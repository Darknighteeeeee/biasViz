  Possibly related to #1506.

Any chance you could put a complete log with -vv somewhere? Ah that makes sense.

onedrive is case insensitive, which means you can only have one of "file.jpg" and "file.JPG".

So when rclone syncs, it will continually replace the files.  You can see it doing it in your log

```
$ grep -i IMG_5746.jpg 1xww 
2017/06/30 17:36:27 DEBUG : IMG_5746.JPG: Size and modification time the same (differ by -336.28768ms, within tolerance 1s)
2017/06/30 17:36:27 DEBUG : IMG_5746.JPG: Unchanged skipping
2017/06/30 17:36:30 INFO  : IMG_5746.jpg: Copied (new)
```

Unfortunately there isn't a solution to this other than to rename your files so they are all different even when made lowercase.

rclone should do a better job warning about this  This message is from an old version of the AWS SDK for go, so it is definitely an rclone problem.

Can you try again with the latest beta from https://beta.rclone.org please? Glad it is solved!  I think this bug may have been fixed already - try the latest beta from https://beta.rclone.org No problems - thanks for updating the issue.  Yes that is what rsync does - it carries on with a non fatal error, giving a non-zero exit code at the end.

What rclone should do is skip directories if they couldn't be read for some reason. This needs a little care as we need to make sure a skipped directory is treated differently from an empty directory, but I don't think that is too hard.  I'm going to close this as I think this is done already in #1494 - please re-open if you disagree!  Could use this, or code like this to read the terminal width: https://github.com/creack/termios/blob/master/win/win.go  Drive duplicating files is an old problem - see #28.  I'm pretty sure there is a bug in drive itself as rclone is not the only app to have this problem.  Even google photos on android duplicates pictures!

Your suggestion of warning when it encounters a duplicate is a good one, and one that is now possible with the new style sync routines, so I shall use this amend this issue to implement that. PS the old sync method did used to warn about duplicate files, so this functionality has gone missing. NB this is also causing a problem [being discussed on the forum](https://forum.rclone.org/t/rclone-sync-bug-files-are-being-erroneously-erased-moved-on-the-remote-causing-subsequent-rclone-syncs-to-copy-them-over-and-over-again-was-repeated-rclone-sync-commands-from-a-read-only-local-fs-copying-the-same-files-over-and-over-again/2252) This will warn about duplicated files

https://beta.rclone.org/v1.36-231-g575e779b/ (uploaded in 15-30 mins)  > refreshing the mod-time on a remote, meaning if the file checksum matches but the mod-time differs, update the mod-time on the remote to match the source

That is exactly what `rclone copy` and `rclone sync` do normally, so I'm unsure what the patch might do, except maybe not copy files if they aren't found?

Hopefully @calisro can attach the modified file or a `git diff `here for me to have a look at! See this discussion.  

https://forum.rclone.org/t/only-update-mod-time-without-copying-file/2295/5

It was literally reintroducing the bug this commit fixed:
https://github.com/ncw/rclone/commit/b52c80e85c7d98b894ccbcfe5604150ad349d611

The difference is that the current rclone code doesn't update the mod time on existing files if the files match.  It would be good to have flag that would refresh the mod times where the files match while doing a sync.  Maybe --freshen-modtimes or something.  The hack also wouldn't copy the files but I don't think that is a necessary feature.   
[operations.txt](https://github.com/ncw/rclone/files/1103985/operations.txt)
 I would say the important effect of the patch to operations.go is to **ignore** hashes - meaning it updates the mod time if only the size matches, not the size and the hash.

Are you uploading to crypt by any chance which doesn't support hashes?
  Thanks :-)  Which version of rclone were you using? Can you try again with the latest beta please?

Were you maybe running two versions of rclone at once?  Thank you for fixing all my typos, spelling mistakes and grammar fails - much appreciated :-)

> There's also this sentence in the “Server Side Copy” section:
> > Server side copies are used with `sync` and `copy` and will be identified in the log when using the `-v` flag. _The may also be used with `move` if the remote doesn’t support server side move._
> It seems wrong (move may be used if move is not supported?), but I don't know what it should say. Maybe it's even correct, just not very clear.

How about this?

Server side copies may be used to implement the `move` command, if the remote does not support server side move directly.  This is done by issuing a server side copy then a delete which is much quicker than a download and re-upload
  I thought I'd fixed that issue... I think it is caused by transferring big files greater than the temp link threshold.

I wonder it is was reintroduced by go 1.8 which changed something in that area.

So what size are the files that give the error? I have an idea what is happening here - can you try using `--acd-templink-threshold 0` and see if that works around the problem?

What I think is happening is that files above a certain size are being redirected - but rclone is supplying the `Authorization` header on redirect.  I fixed this problem for the templink case already so the above might be a work around.  If it does work around the problem I know how to fix it properly. I'm going to re-open this as I need to fix it properly!  Hmm, seems Ok now.  Uptime robot says it has been up for 30 days so probably just a transient bgp thing.  I suggest you checkout [this thread in the forum](https://forum.rclone.org/t/proxy-for-amazon-cloud-drive) for a better auth way.

I'm going to close this since it isn't really a bug report - if you need more help please post in the forum - thanks!  I can't replicate this - it worked just fine when I tried

    rclone mount -vv hubic:default ~/mnt/tmp/

And in another terminal window

    cd ~/mnt/tmp
    wget https://dnd.rem.uz/Advanced%20D%26D%20%28unsorted%29/AD%26D%20Accessory-FR-Volo%E2%80%99s%20Guide%20to%20All%20Things.pdf

Was this just a one-off or can you replicate it at will?

 Hmm, that isn't quite what I see - I see the correct character for the quote symbol not \342%80%99.

What is your locale set to? Is it not UTF-8? I don't understand why curl is saving the file name with % signs in instead of using the UTF-8 character.

I guess I can try saving a file with the name exactly as from your log and see what happens. It has invalid UTF-8 characters in so maybe that is what hubic doesn't like.

I should be able to try this on a Debian 8 VM (I tested on Ubuntu) I managed to recreate this by attempting to copy a file I made like this.  This is the troublesome part of your filename above. 

```
$ python
>>> f =open("/tmp/volo/Volo\342%80%99s", "w")
>>> f.write("hello world")
>>> f.close()
```

This will now give this error if you attempt to copy it into the mount rather than hang.

```
2017/06/26 11:35:03 ERROR : volo/Volo�%80%99s: WriteFileHandle.New Put failed: HTTP Error: 412: 412 Precondition Failed
2017/06/26 11:35:03 ERROR : volo/Volo�%80%99s: WriteFileHandle.Write error: io: read/write on closed pipe
```

The problem is that the filename is not a valid UTF-8 string which hubic will reject with a 412 error.

I'm not quite sure why your wget does what it does, but when I try it onUbuntu 17.04 I get a different (much more sensible!) result with a valid UTF-8 filename

```
$ wget https://dnd.rem.uz/Advanced%20D%26D%20%28unsorted%29/AD%26D%20Accessory-FR-Volo%E2%80%99s%20Guide%20to%20All%20Things.pdf
--2017-06-26 12:04:00--  https://dnd.rem.uz/Advanced%20D%26D%20%28unsorted%29/AD%26D%20Accessory-FR-Volo%E2%80%99s%20Guide%20to%20All%20Things.pdf
Resolving dnd.rem.uz (dnd.rem.uz)... 163.172.43.158
Connecting to dnd.rem.uz (dnd.rem.uz)|163.172.43.158|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 8871505 (8.5M) [application/pdf]
Saving to: ‘AD&D Accessory-FR-Volo\’s Guide to All Things.pdf’

AD&D Accessory-FR-V 100%[===================>]   8.46M  5.14MB/s    in 1.6s    

2017-06-26 12:04:02 (5.14 MB/s) - ‘AD&D Accessory-FR-Volo\’s Guide to All Things.pdf’ saved [8871505/8871505]
```

```
$ ls -l AD*
-rw-rw-r-- 1 ncw ncw 8871505 Jun 24  2016 AD&D Accessory-FR-Volo’s Guide to All Things.pdf
```

```
 dpkg -l | grep wget
ii  wget                                        1.18-2ubuntu1                               amd64        retrieves files from the web
```

Anyway, please find a fixed rclone here

https://beta.rclone.org/v1.36-213-gb3605279/  (uploaded in 15-30 mins)

And please reopen if it doesn't fix it for you.  B2 (and S3, swift, GCS) don't have the concept of folders - they are object stores only. So mkdir does nothing except create the bucket if necessary.

As soon as you copy files in, folders will appear. This effectively means you can't have empty folders on B2.  I think you can do this on Linux:

Rclone copy /x/  y:    -v --stats 5m  2>&1 | egrep "^E|^T|^C"

I know it's not exactly what you want but it would work...... Yes but I do agree with you. I'm going to fix this in #1180 so I'm going to close this as a duplicate.  Please subscribe to that bug for updates!

Thanks  Here is a beta with sftp which can take a key file

https://beta.rclone.org/v1.36-210-gd55f8f04/ (uploaded in 15-30 mins)

Here are the [updated docs](https://github.com/ncw/rclone/blob/master/docs/content/sftp.md) Excellent, thanks for testing :-)  Are you using v3 auth?  If a file is called "http://example.com" in the swift container, then rclone will download it to a directory called "http:" and the file called "example.com".

On subsequent syncs rclone will be looking to see if an object called "http:/example.com" (note 1 slash this time) exists, so it will forever copy these objects.

The problem is that "http://example.com" isn't a valid file name, whereas it is a perfectly valid object name.

Is that the problem you are seeing?  This was recently done for google cloud storage in 022ab4516db76b8b57b60e9e81eeb6be5c491184

@mwitkow you did the changes for GCS service accounts - do you think the same methodology would work for Google drive?  That is great - thanks for updating that :-) When I make the next release which should be quite soon.  There is an issue about this already at #575 - can you add your useful links to that issue please?  The idea of backing up to nntp is both bonkers and brilliant! @SanderM2 I have too much on my plate at the moment, but if someone wanted to have a go I'll provide help.  Hey, that looks amazing!

Some questions for you

  * Do you know if can I get a free account for testing (I'm based in the UK)?
  * I see it is passing most of the tests - well done!  What about the remaining ones? I'm willing to bend the tests to make Qingstor pass, but I'd like all the remotes to be running all the tests.
  * have you tried the tests in `fs` with `go test -v -remote TestQS:`?
  * It says in the docs (translated by google translate) that it is AWS S3 compatible - does the S3 remote not work properly with Qingstor?

And an apology - I'm just about to change the `List` interface, splitting it into a simple `List` which just lists one directory, and a `ListR` interface with does a recursive list if the remote is capable of it. This takes a lot of complexity out of the remotes which is a good thing hopefully!  If you take a look at the S3 remote you'll see how it has changed.
 FYI [Here are the S3 changes](https://github.com/ncw/rclone/commit/8a6a8b9623bea1f600ad15eec2dc74ed37e65110#diff-7db3cb93944d57b2ffc803281c906018) to the List and ListR interfaces OK I'll have a go at registering with QingCloud and see what happens!

> About compatibility, Is qingstor one-way compatible with the S3 interface; This is for S3 users can smoothly migrate to QingStor.

I'm not sure I understand. Would the existing rclone s3 backend work with QingStor?  And if so why would you use this one?

> The list interface change very well I think. I'd be interested to know more. Such as, how do I test it after making a change.

regenerate the *_test.go with `cd fstest/fstests && go generate` and that should have all the tests you'll need in.  There are various issues about keeping a database of files to speed things up.

However you should find rclone is reasonably quick - it has to do a directory listing of each directory at dropbox and one on your local disk.  It can be doing those in parallel - see the `--checkers` flag.  For dropbox, everything rclone needs to do the comparison is in the directory listing.

How many files are you syncing? I plan to do the local database thing eventually :-)

You can already encypt stuff - see https://rclone.org/crypt/
  Thanks for figuring out the problem - saved me lots of hair pulling!

This was introduced by c59a292719ad574357f96f8e19b65b02a6e135f1

I've fixed this in this beta

https://beta.rclone.org/v1.36-174-g1e88f070/ (uploaded in 15-30 mins)
  Are you using `--bwlimit`?

Can you try with the latest beta please?  I've completely re-written the dropbox code to use the v2 API since 1.36! I think what is happening with the chunked uploads is that rclone is reading a chunk off disk then uploading it, so the pause is the read off disk bit.  Or it could possibly be dropbox processing the chunk.

Can you try  `--buffer-size 128M` - this should let the buffer get 1 upload chunk ahead.  Try it bigger and smaller too if it is easy.  Note that it will use 128M for each transfer in progress potentially - so just try this on your 16GB file to start with.

Some of the other remotes have more sophisticated multipart uploads where you can upload the parts in parallel, but [I don't think that is possible with dropbox](https://www.dropbox.com/developers/documentation/http/documentation#files-upload_session-append_v2).  rclone could be making more of an effort to parallelize the loading of chunks and the sending of data.  If the `--buffer-size` experiment is successful then I can have a think about how to do that.

> As you can see there it should return a header called "Retry-After" which should be used to avoid those rate limit warnings.

You are right, rclone doesn't look at Retry-After yet.

Here is what the docs say

> Rate limited responses include a Retry-After header that provides the duration, in seconds, your app should wait before retrying the request to avoid a subsequent rate limit response. Note that this duration is an upper cap on how long an app would wait to guarantee the next request will not be rate limited. A request made sooner may not be rate limited. If you’re finding that waiting the duration provided by the Retry-After header is causing a bottleneck in your app’s execution, you may want to implement your own back-off mechanism wherein you start by waiting a fraction of the Retry-After value before retrying and wait a little longer after each consecutive request that is being rate limited.

Which is in fact exactly what rclone does - it uses an exponential backoff.  So I think seeing `too_many_write_operations` errors is normal.  If rclone retries the same file many times with the same error then rclone's algorithms will need a tweak - have you seen that? > I have just retried with --buffer-size 128M but honestly, it doesn't make any difference. I even tried putting a 8Gb file in ram drive and the problem persists, so it's not a disk-read issue I'm pretty sure...

OK thanks for testing that.

> I also tried with --dropbox-chunk-size 25m and this made the burst/stall network pattern repeat more often so maybe you're right that it's dropbox needing more time to process the request.

We could test this out possibly...  If you run rclone with `-vv --dump-headers` it will show the http transactions as they come and go.  What would be interesting to see is the time between rclone finishing one transaction and starting the next and whether the network bursti finishes before rclone gets the response.

> But then I wonder.. why would it be this slow using the API while their own application can do it a lot faster? Isn't their own application using their own API ? :-)

I'm pretty sure in the past that dropbox's own app didn't use the public API since it could set client_modified, but you couldn't do that in the API.  I don't know what the status is now though.

I saw this [blog post about streaming sync](https://blogs.dropbox.com/dropbox/2014/07/introducing-streaming-sync-supercharged-sync-for-large-files/) - I wonder if that is related.

Any chance you could investigate which API it is using?

> In order to speed things up. I'm not sure if it's possible at all: but maybe we can already start uploading the next chunk while waiting at confirmation of the previous chunk?
Reading the dropbox API docs I don't see a problem with that and maybe we can have it going a lot faster this way? Not sure... Love to hear your thoughts.

That would be the idea solution.  However there are two things that give me pause, one is the name of the method [Append](https://www.dropbox.com/developers/documentation/http/documentation#files-upload_session-append_v2) and the other is this error 

> incorrect_offset UploadSessionOffsetError The specified offset was incorrect. See the value for the correct offset. This error may occur when a previous request was received and processed successfully but the client did not receive the response, e.g. due to a network error.

that said it might be worth an experiment... I asked a quetion in the developer forum about parallel uploading [here](https://www.dropboxforum.com/t5/API-support/Using-upload-session-append-v2-is-is-possible-to-upload-chunks/m-p/225845#M12296) :-( that is a shame.  However rclone can upload lots of stuff at once with `--transfers` which ups the parallelism. Ok thanks for the confirmation. They said the same thing in the forum too.

There is a ticket for onedrive for business. It should be quite simple buy whenever I've tried I get bogged down in Microsoft terminology and needing developer subscriptions which I don't have, so help needed!  The API should just work it is registering the app etc which is the problem.  rclone will retry the whole sync if it finds errors it can't correct by doing its low level retries.

Check the log for error messages.

Try with --retries 1 (which will do 1 retry and 0 retries - badly named I know!) to make it stop after the first cycle if you want to see what is going on. I think what is happening here is the google rate limiting.  Google gives you an initial burst, then log term limits you to quite a small number of transactions per second.

Are you using rclone's client_id or have you made your own?  If you haven't then I recommend you try making one and see if that makes a difference? #1220  only affects the initial setup.  Once its setup and working and all other rclone processes are using that new config, it works just fine.  I've been using it forever.  That function `AcquireSRWlockExclusive` is only available in windows 2008 or Vista and above, however Go should work with WIndows XP+.

Can you try this binary please?

I compiled it without cgo and therefore without `rclone mount`.

[rclone-v1.36-173-g68333d34.zip](https://github.com/ncw/rclone/files/1075001/rclone-v1.36-173-g68333d34.zip)
 @billziss-gh do you have an thoughts about this? Is this related to the WinFSP intiialization? @billziss-gh thanks for looking into this.

I plan to support XP as long as go does.

I don't think we need to support mount on XP though, so would it be possible just to set an unsupported flag in the init which produces an error when mount is called? I'm guessing that wouldn't work unless we dynamically load the AcquireSRWlockExclusive symbol?

Another alternative is I just make a build without mount for XP era Oses and we leave the mount code alone which might be simpler all round. Thanks @billziss-gh that would be perfect :-) Thanks for fixing that @billziss-gh :-)

A beta test which includes those fixes can be found in

https://beta.rclone.org/v1.36-196-g4ce31555/ (uploaded in 15-30 mins)

Please re-open if it doesn't fix the problem.

Thanks

Nick  It should probably only print that message once I think.  Would that be acceptable?  It might actually be easier for rclone to check the mount directory is empty itself.  There are issues already in rclone to add things like cache'd urls and file cache and such.  For example:

https://github.com/ncw/rclone/issues/711
https://github.com/ncw/rclone/issues/897
I think this issue is really a duplicate of multiple other 'issues'.

  Investigate this: I'm beginning to think I've made a mistake normalizing the file names, what I should be doing is normalizing them in the sync routine when rclone is comparing source and destination directory, rather than normalizing them all the time.

Normalizing the file names causes issues with other tools which don't (it doesn't seem to be the expected way of doing things).

Originally from #1472

See also #1515 for normalization problems when using a single file copy (and thus --files-from and all the other filters...)

Note from #62 

> One other thing I noticed regarding the drive code, is for directories ending in one or more spaces. The Google Drive Windows App will replace the ending spaces with "_", because Windows doesn't like directories ending in blank spaces. For Linux, it doesn't matter.

It occurs to me that the filename matching code should transform the input strings using the OS transformation before comparison. so if either src or remote is local on windows, it should transform the input into the Windows safe version before doing the comparison.

Likewise if one of the remotes is case insensitive then it should transform everything into lower case (checking for case folded matches).

Would also need to convert the check routine to use this framework.

So the pipeline of filename transformations might look like

  * -> normalize unicode
  * -> lower case (if dst is case insensitive, or possibly if src | dst case insensitive for bidirectional goodness)
  * -> make Windows safe

And then do the name comparisons in `matchListings`

Could have an optional transform function for each Fs which does lossy translation of unacceptable characters.  Local would do the Windows transforms on windows.  Others might get rid of control characters (or perhaps there should be a default one which does that).  Fses would then use their Transform function when writing objects.

Need to think carefully about

  * copyto/moveto
  * filters - especially single file copies

**Update** after a bit of testing with `rclone info` there are no remotes which complain about denormalized UTF-8.  There are some (eg onedrive, dropbox) which will read denormalized UTF-8 or normalized UTF-8.

Have two types of transform, one for comparison purposes and one for writing the file purposes.  Perhaps these are identical except for the case insensitivity.
  I've treid to replicate this using a long upload.  I failed though - the token got renewed just fine.

Looking at your log it is trying to renew the token with Amazon rather than the proxy... The user agent is `User-Agent: rclone/v1.36` so that probably means you are not using the beta on the linux box.

I suspect if you use the beta on the linux box too it will work fine. `rclone version` or `rclone -V`.  If you got an official build then it will have a correct version number.  If you built it yourself it won't unless you used the Makefile. Glad you got it working.  The official docs changed at some point from /usr/sbin (which is incorrect) to /usr/bin so understandable there was some confusion.  Which SFTP server are you using?  It looks like it is having a problem with something rclone is doing.

Can you also try with the latest beta from https://beta.rclone.org please?  Assuming you are only interested in file IDs, then you could do this by changing the `newObjectWithInfo` in `drive/drive.go` to add just before the return `fs.Debugf(o, "ID = %q", o.id)`.  This might open the door for logging other stuff like the download URL which I know I've been asked for.

Have a play with that and see if that is useful then we can think about how to integrate it properly.  This is almost certainly caused by OS X and its strange decision to store denormalized UTF-8 file names.

You didn't mention OS X in the above, but I suspect that these file names have been through an OS X machine at some point.  The `b` file below has the e acute character in, whereas the `a` file has e then the acute accent combining character which is exactly what OS X does.  So if you store `b` on an OS X file system, it will read back as `a` which has caused a lot of problems with rclone!

```
$ python
>>> a=u"10 - Polonaise in Ab major Op 53 Héroique.flac"
>>> b=u"10 - Polonaise in Ab major Op 53 Héroique.flac"
>>> a == b
False
>>> a
u'10 - Polonaise in Ab major Op 53 He\u0301roique.flac'
>>> b
u'10 - Polonaise in Ab major Op 53 H\xe9roique.flac'
>>> 
```

You can try this flag in the latest beta `--local-no-unicode-normalization` which should help.

I'm beginning to think I've made a mistake normalizing the file names, what I should be doing is normalizing them in the sync routine when rclone is comparing source and destination directory, rather than normalizing them all the time. If you use `--track-renames` then rclone will rename stuff for you rather than copying it.  The source and dest need a compatible hash which may not be available though. I've put the changing the place the normalization is done as an issue here #1477   Thank you for that :-)  That looks like a bug in Hubic, but it is easy enough to work around.

Any chance you could send me the output of `-vv --dump-bodies` for the transaction that produced the error?  Run your command again with `-vv --dump-bodies` should do it. Thanks for posting that.

I've realised that the problem isn't with hubic, it is with your remote string.

You can't copy files to `/` on a hubic remote - you need a name of a bucket first.

So if you do `rclone -vvvv copy b hubic:b`

It should work fine.

The actual error message is crazy though, and I've fixed that here

https://beta.rclone.org/v1.36-170-g2ca477c5/ (uploaded in 15-30 mins)

It will now give the error `Failed to copy: container name needed in remote`  I see these too.  I don't think it is an rclone problem - I think it is a Hubic problem.  Specifically if you try to use anything but the default bucket it seems to take a very long time with lots of errors to create it.  Once it is created it is OK, so I suggest you keep trying.  What needs to be done is make the acd remote fail unless you have put in custom credentials and put a note in the docs about that. > Zenjaba have the credentials for you @ncw if you are willing to setup auth server so ACD would work again.

Wouldn't it just get banned again just like what happened with acdcli again?  I can certainly see rclone continuing to work with ACD for people that have their own credentials because of the low API hits but as soon as a auth server is officially put up, I just see it getting banned again.   > > Zenjaba have the credentials for you @ncw if you are willing to setup auth server so ACD would work again.
> 
> Wouldn't it just get banned again just like what happened with acdcli again?  I can certainly see rclone continuing to work with ACD for people that have their own credentials because of the low API hits but as soon as a auth server is officially put up, I just see it getting banned again.  

That is my concern.  Setting up an auth server is easy, I'm just not sure that I'm allowed by the TOS to use  any other credentials for rclone. Perhaps rclone is setup to either accept custom API credentials and/or an auth server (in the config rather than in the code) and let people do what they want with their creds and there wouldn't be an official association with rclone until such time as Amazon official allowed it again. At least then it would be flexible enough to allow people to use it if they have access to one or the other.....

Edit: I realize this is off topic and prob better in the forums..... @calisro wrote:
> Perhaps rclone is setup to either accept custom API credentials and/or an auth server (in the config rather than in the code) and let people do what they want with their creds and there wouldn't be an official association with rclone until such time as Amazon official allowed it again. At least then it would be flexible enough to allow people to use it if they have access to one or the other.....

I like that idea.  Consequently I have made an oauth server for use with rclone here: https://github.com/ncw/oauthproxy

You'll need the latest beta https://beta.rclone.org/v1.36-165-g52e1bfae/ which has this patch 52e1bfae2a8b67b0056af81eac00a0c394915121 to use it.

I don't have any credentials I can put in the oauth server (I was hoping amazon would supply some) but if someone does have some production cleared credentials, then the oauth server above will work! I have updated the docs in 10d5377ed8fe0136688f5d8fd4df03e73a3047b9

See the forum https://forum.rclone.org/t/proxy-for-amazon-cloud-drive for a working proxy.  Does mediafire have an API?  If so can you post a link? Ah the proper API docs are here: http://www.mediafire.com/developers/core_api/1.5/getting_started/  Can you try the latest beta please to see if it is the same? Also double check the installation of OSXFUSE.

Thanks Does the path "/Users/steven/onedrive" exist?

Can you get any other OSXFUSE file systems to work, eg sshfs?
 Excellent :-)  Very nice docs - thank you :-)  Does rclone recover after the directory cache expires?  I would expect it to.  In which case you could send SIGHUP to reread the mount in these situations. (That is only in the beta I think)  Rclone doesn't currently support syncing the dates on folders.  It will at some point, probably as part of #100   Here's another workaround idea: you can simply chmod the folders on the rclone mount. It is stored locally in memory, so it will be lost once you restart rclone. This would be just one API call though, and probably less prone to race conditions.   That looks perfect - thanks for doing that.  @breunigs this is probably related to your changes. This had an easy fix - please find a beta here:

https://beta.rclone.org/v1.36-154-gb0474022/ (uploaded in 15-30 mins)  Thank you - much appreciated :-)  That looks very useful :-)

I think the best place would be in `docs/content/s3.md` as a new section - say "S3 permissions".  If you could explain in words what permissions are needed and link to your ansible playbook that would be perfect I think.

Fancy sending a PR for that?  This might be possible, but I don't really want to do it as I'm sure it would end up with rclone being in trouble with Amazon again! I'm going to close this ticket now as I don't think this is a good direction for rclone to go in.  That looks fine.

There is now quite a bit of duplicated code in mount/cmount - most of the flags handling which makes me think we should factor it into mountlib.

I plan to use mountlib for serving other filesystem like things like webdav in due course.  That is fun!  [Webassembly suport for go](https://github.com/golang/go/issues/18892) would probably be the best way of approaching this in the long run.

What are your plans for this?  Are you planning to develop this further?  Not a bad idea!  It would probably need to be some other flag used together with --dry-run or instead of it. You know as soon as you do this someone will want a confirmation after each file as well the whole set.  Perhaps 
--confirm each   
--confirm all

You could default to 'all' if someone just types 'confirm'.  I would think that dry-run and this should be mutually exclusive (instead of).   Just my 1 penny.  What is the problem you are seeing?

    2017/05/26 12:22:20 DEBUG : Targem Root/Welcome targem games/Links.xlsx: Sizes differ
.
Is a normal debug message which indicates the size of something has changed and needs transferring.
 Use `rclone dedupe targemmain:` to check for duplicates - this can make this sort of error. Rclone syncing wasn't designed to cope with duplicate file names and file system​s can't cope with duplicate file names so I think the best option is to fix the source.

There aren't any more options which will help.

Rclone dedupe could be cleverer though when there are duplicate directories - it should probably collapse those first.  I tried to reproduce this, but failed, by

  * running rclone config
  * renaming an existing remote
  * quitting rclone config
  * examining the config file to see if the change was correct - it was

What did you do differently to that? I just tried with a space too, but still worked!  Looks good to me :-)  It doesn't appear that the backup-dir switch works with mounts.  It's this something that could be added? I'd like to be able to track and have a backup of all changes occurring in my mounts.  

It also doesn't appear to work with regular local filesystems or local remotes. backup-dir wasn't intended to work with mounts.

What would you want it do do?

Whenever you overwrite a file or delete one it moves the old file to backup-dir would be the obvious choice.  That is pretty much how backup dir works for sync. I was intending on using it exactly like sync/copy works now just like you said. The only thing that just came to mind though it's the fact that a date driven directory structure won't work because it's evaluated at mount time which effectively makes it less useful since multiple files 'backups' will overwrite. I might as well use the gdrive-use-trash switch instead.  



 @ajkis  --drive-use-trash                   Send files to the trash instead of deleting permanently. Closing this as I cant think of a better way to handle it.  Merged - thank you very much :-)  That looks great!

I've reviewed the code and it looks good :-)

I'll merge it now and we can fix up issues as they appear.

Thank you! PS I'd like to make you a committer - can you email me (nick@craig-wood.com) from your preferred email address to discuss!

Thanks

Nick  Hmm, chunked files.

Can you paste a log with `-vv` of uploading a chunked file that you think might have a problem?

Are there retries involved?

Can you check with the latest beta that this is still a problem? Your explanation sounds correct looking at the code.

Which means on average the transfer rate will be what you set, however it will burst to very high while it is sending the chunks.

I agree this isn't ideal especially since B2 chunks are 100MB.

I could put the bandwidth limit on the other side of the buffer, but then that will have the opposite problem if you are copying from a network remote to B2.

Having a bandwidth limiter on both sides of the buffer would work, but that will limit the bandwidth too much since both limiters will be taking from the same pool.

Perhaps two limiters at twice the bandwidth...

I'm sure there is a simple solution, it just hasn't come to me yet! Two pools is the conclusion I came to as well. I'd set them both to bwlimit. I like your idea of bypassing them for local  disk read or write.

I'll have a play with some of those ideas. @SirCrest this is likely the same issue... I'll update the title of this issue accordingly!  What was the sequence of events leading up to that?
 @tynor88 thanks  - that is something to go on :-) I have fixed this in https://beta.rclone.org/v1.36-222-gf3c7e1a9/ (uploaded in 15-30 mins)  This looks good.  I'll fix up the merge and merge it in a little while. I've merged this in ade61fa7564824ed938c69d1940b9989b22c1bc6

I've also merged 0f07b63fd163ce0a6ed3669123ce86d967340708 which converts the old config style which I'll remove eventually.

Thank you very much for doing this, and very thoroughly updating the docs.  This issue has a lot in common with #1296 which is fixed.  You've said you've tried it with the latest beta though.

@yonjah did we miss a QueryEscape in 32e9c86fcdb38d77dc5018f6184e4f3607c714d9 ? Are you sure you tried this with the latest beta - I just attempted to replicate and it seemed to work fine.

```
$ rclone copy -vv space-test onedrive:space-test
2017/05/25 09:18:43 DEBUG : rclone: Version "v1.36-DEV" starting with parameters ["rclone" "copy" "-vv" "space-test" "onedrive:space-test"]
2017/05/25 09:18:44 DEBUG : onedrive: Saved new token in config file
2017/05/25 09:18:45 INFO  : One drive root 'space-test': Modify window is 1s
2017/05/25 09:18:46 DEBUG : One drive root 'space-test': Reading ""
2017/05/25 09:18:46 DEBUG : One drive root 'space-test': Finished reading ""
2017/05/25 09:18:46 INFO  : One drive root 'space-test': Waiting for checks to finish
2017/05/25 09:18:46 INFO  : One drive root 'space-test': Waiting for transfers to finish
2017/05/25 09:18:46 INFO  : file with spaces.txt: Copied (new)
2017/05/25 09:18:46 INFO  : 
Transferred:      6 Bytes (1 Bytes/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:        3.4s
2017/05/25 09:18:46 DEBUG : Go routines at exit 7
2017/05/25 09:18:46 DEBUG : rclone: Version "v1.36-DEV" finishing with parameters ["rclone" "copy" "-vv" "space-test" "onedrive:space-test"]

$ rclone ls onedrive:space-test
        6 file with spaces.txt

$ rclone copy -vv space-test onedrive:space-test
2017/05/25 09:19:14 DEBUG : rclone: Version "v1.36-DEV" starting with parameters ["rclone" "copy" "-vv" "space-test" "onedrive:space-test"]
2017/05/25 09:19:14 INFO  : One drive root 'space-test': Modify window is 1s
2017/05/25 09:19:14 DEBUG : One drive root 'space-test': Reading ""
2017/05/25 09:19:14 DEBUG : One drive root 'space-test': Finished reading ""
2017/05/25 09:19:14 INFO  : One drive root 'space-test': Waiting for checks to finish
2017/05/25 09:19:14 DEBUG : file with spaces.txt: Size and modification time the same (differ by -474.334908ms, within tolerance 1s)
2017/05/25 09:19:14 DEBUG : file with spaces.txt: Unchanged skipping  <<<<<<<<<<<< file not copied again
2017/05/25 09:19:14 INFO  : One drive root 'space-test': Waiting for transfers to finish
2017/05/25 09:19:14 INFO  : 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:                 1
Transferred:            0
Elapsed time:       500ms
2017/05/25 09:19:14 DEBUG : Go routines at exit 7
2017/05/25 09:19:14 DEBUG : rclone: Version "v1.36-DEV" finishing with parameters ["rclone" "copy" "-vv" "space-test" "onedrive:space-test"]
``` Thanks!  If there is a problem with the latest beta can you make a little demo like the one I made to show exactly how to reproduce it. I'm going to close this one as fixed - please re-open if you think otherwise.

Thanks  That folder has single quotes, double quotes and spaces in - all of which should work fine (and are part of the integration tests).

I tried to replicate the problem, but I didn't succeed.

Do you have any other examples of it failing?

Can you attach a log with `-vv` of it going wrong please?

```
$ rclone ls "drive:Issue 1440"
  6624250 "It's Muh Policy"/etcd-v0.4.6-linux-amd64.tar.gz
$ rclone -v copy "drive:Issue 1440" /tmp/Issue-1440
2017/05/24 23:19:23 INFO  : Local file system at /tmp/Issue-1440: Modify window is 1ms
2017/05/24 23:19:23 INFO  : Local file system at /tmp/Issue-1440: Waiting for checks to finish
2017/05/24 23:19:23 INFO  : Local file system at /tmp/Issue-1440: Waiting for transfers to finish
2017/05/24 23:19:25 INFO  : "It's Muh Policy"/etcd-v0.4.6-linux-amd64.tar.gz: Copied (new)
2017/05/24 23:19:25 INFO  : 
Transferred:   6.317 MBytes (1.752 MBytes/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:        3.6s
``` Thanks for the share of the path - that was very helpful.

I've tracked down the problem and fixed it.  The fix also will speed up the nothing to do case of syncing by almost a factor of 2.

Here is the beta

https://beta.rclone.org/v1.36-159-g3fe94482/ (uploaded in 15-30 mins).  I did a quick test with ProFTPd and I can see stuff inside square brackets

```
$ rclone ls TestFTP:brackets
        0 with[brackets]yes/inside
```

So this probably depends on which ftp server you are using.  What is it called?


 Yes this seems to be pureftpd intepreting wildcards.

```
ftp> ls brackets
200 PORT command successful
150 Connecting to port 34975
drwxrwxr-x    3 sftptest   sftptest         4096 May 24 20:51 with[brackets]yes
226-Options: -l 
226 1 matches total
ftp> ls brackets/with[brackets]yes
200 PORT command successful
150 Connecting to port 44283
226-Options: -l 
226 0 matches total
```

Where if we try with wildcards we can see it can see in the directory,

```
ftp> ls brackets/with?brackets?yes
200 PORT command successful
150 Connecting to port 53341
drwxrwxr-x    2 sftptest   sftptest         4096 May 24 20:51 a
-rw-rw-r--    1 sftptest   sftptest            0 May 24 20:50 inside
226-Options: -l 
226 2 matches total
ftp> quit
221-Goodbye. You uploaded 0 and downloaded 0 kbytes.
221 Logout.
```

I wonder if there is some way of turning off wildcard matching...

A quick experiment with curl indicates that it CWD into a directory before listing it, so I could maybe do that.  Super - thanks :-)  For remotes that support it (not many, swift is an example) `rclone lsd` shows the count  already.

I have written (but not merged yet) an [ncdu](https://en.wikipedia.org/wiki/Ncdu) clone for rclone which is probably exactly what you want for an interactive way of answering the question, "what is using all my data?"  Thanks for making the issue - much better for tracking! I've attempted to fix this here - can you have a go please!

https://beta.rclone.org/v1.36-125-g51d2174c/ (uploaded in 15-30 mins)  Anyone fancy updating [the docs](https://github.com/ncw/rclone/blob/master/docs/content/ftp.md) with a bit more help about ports? @sjurtf thanks for your PR I have merged that :-)

> Wouldn't it make sense that the FTP and SFTP remotes handle destination URLs the same way during setup of a new remote?

Yes possibly and given that SFTP has been released and FTP hasn't now is the time to change the FTP if we want to.

Supplying a URL lets you supply a default directory to log in to for FTP.

We could add this to sftp which could be useful, though it is actually a bit of a pain to code as rclone has its own idea of root directory when you do `ftp:/home/user` so I'd be tempted to remove that from the ftp client.

I think it would probably be easier for the users to get a Host, Port, User, Password like the sftp rather than have to construct an FTP URI which as this issue demonstrates isn't familiar to most people. @easy90rider you'll need to put the path on the rclone command line now in the latest beta. Yes you will need to use usual shell escaping rules for spaces. This issue was fixed in #1441 so I'm going to close it now.  @rhummelmose Which FTP server are you testing with?

You might need to put some path in front of `Bitport:` eg `Bitport:/home/username` maybe - or you could put that in the configuration URL.  Could you easily try this on linux or OS X?  I suspect it isn't related to Windows.

This is probably a quirk of the sftp daemon running on your NAS.

Can you attach a log with `-vv` of your test command please? Ok there is something very obviously wrong with the Windows support!  Try this and see if it fixes it.  If not I'll dig further.

https://beta.rclone.org/v1.36-126-ga243ea63/ (uploaded in 15-30 mins)
 That is uploaded now - had to give the builder a kick! Excellent  and many thanks for testing.

I'll close this now.  Can you try running [rclone dedupe](https://rclone.org/commands/rclone_dedupe/) on the underlying google drive (not the egd: but the one its `remote` points to), eg something like

    rclone dedupe --dry-run gd:whatever_you_are_using

Drive is a bit prone to duplicating files that you upload which `dedupe` can fix.  Those would cause the symptoms above I think.  Unlike a normal filing system, drive can have two files with the **same** name!

That might not be the problem, but if you can rule that out first that would be excellent. @DurvalMenezes wrote
> The unencrypted remote my "egd" is built on is indeed called "gd", but what do I replace the "whatever_you_are_using" part with? I would guess I should replace it with the encrypted name for the directory I'm verifying (and getting these issues on), ie, the "REDACTED" in my "egd:REDACTED" rclone sync command line. Is that correct? Or should I just leave it empty (and let rclone dedupe work its away through the entire gd contents)?

You can use the remote that you put in the crypt config to check the whole thing, or point at the subdirectory if you can work out what it is called

> Out of curiosity, WTF could have caused GDrive to create those duplicates? Collisions on the rclone filename encryption? Some kind of internal bug/feature, perhaps interacting weirdly with rclone? Inquiring minds want to know ;-)

I wish I knews - it is one of rclone's oldest bugs #28.  I did a bit of investigation with the help of another user and Google - I think it may be caused by files not appearing in directory listings occasionally, but the jury is out! @alternativesurfer @RomeSilvanus 
I need to see logs with `-vv`.  If you can grep out logs all logs which mention one affected file name and paste them here that would be very useful.  rclone will say why it is uploading and deleting stuff somewhere in the log hopefully!  Also which parameters you are using with rclone and whether you are using crypt @alternativesurfer in your log you are using `rclone move` so deleting the file (locally) after it has been copied (remotely) is exactly what I'd expect to see.

@RomeSilvanus your problem is caused by OS X and denormalised UTF-8.  When you uploaded the files to drive using the Gdrive app on Mac it sent them up with denormalised UTF-8 ([tech details](https://stackoverflow.com/questions/9757843/unicode-encoding-for-filesystem-in-mac-os-x-not-correct-in-python)).

Here are the log messages

```
2017/05/26 20:26:24 INFO  : My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4: Copied (new)
2017/05/26 20:26:24 INFO  : My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4: Deleted
```

Copying those names into python you can see they are different though they appear the same in the browser.

```
>>> a=u"My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4"
>>> b=u"My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4"
>>> a == b
False
>>> a
u'My Little Pony FiM - 6.25 - Die Ru\u0308ckkehr der Wechselponys - Teil 1.mp4'
>>> b
u'My Little Pony FiM - 6.25 - Die R\xfcckkehr der Wechselponys - Teil 1.mp4'
>>> 
```

rclone normalises the file names so you should find it wants to upload each files once only.

However you can use the `--track-renames` flag to avoid rclone uploading the data again - one sync should be sufficient to rename and normalise everything.

@DurvalMenezes The latter could possibly be related to your problem.  Either way can you make a small demo of the problem that I can reproduce locally?  I'm having trouble working out what is going on! @alternativesurfer in your log you are using `rclone move` so deleting the file (locally) after it has been copied (remotely) is exactly what I'd expect to see.

@RomeSilvanus your problem is caused by OS X and denormalised UTF-8.  When you uploaded the files to drive using the Gdrive app on Mac it sent them up with denormalised UTF-8 ([tech details](https://stackoverflow.com/questions/9757843/unicode-encoding-for-filesystem-in-mac-os-x-not-correct-in-python)).

Here are the log messages

```
2017/05/26 20:26:24 INFO  : My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4: Copied (new)
2017/05/26 20:26:24 INFO  : My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4: Deleted
```

Copying those names into python you can see they are different though they appear the same in the browser.

```
>>> a=u"My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4"
>>> b=u"My Little Pony FiM - 6.25 - Die Rückkehr der Wechselponys - Teil 1.mp4"
>>> a == b
False
>>> a
u'My Little Pony FiM - 6.25 - Die Ru\u0308ckkehr der Wechselponys - Teil 1.mp4'
>>> b
u'My Little Pony FiM - 6.25 - Die R\xfcckkehr der Wechselponys - Teil 1.mp4'
>>> 
```

rclone normalises the file names so you should find it wants to upload each files once only.

However you can use the `--track-renames` flag to avoid rclone uploading the data again - one sync should be sufficient to rename and normalise everything.

@DurvalMenezes The latter could possibly be related to your problem.  Either way can you make a small demo of the problem that I can reproduce locally?  I'm having trouble working out what is going on! @alternativesurfer I'd like to see a log with that if possible, or best of all make a small scenario I can reproduce it with.  I haven't had any luck reproducing the problem yet. @alternativesurfer have you got a stale mount open? I just had an idea about this issue.

I wonder if you have duplicate directories on Google drive. Rclone dedupe only detects duplicate files, however duplicate directories could have a similar effect.

    rclone lsd --max-depth 999 remote:

Will list all the directories. A bit of cut, sort, uniq will find any duplicates (not at pc right now so can't provide a recipe)
 @RomeSilvanus glad you fixed it. I guess I could either put a warning in about unnormalised utf-8, or normalise it.  I made an issue here #1457

@DurvalMenezes  can you run this to see if you have any duplicated directories

    rclone lsd remote:path | cut -b 44- | sort | uniq -c | sort -n | grep -v ' *1 '
 @johnl sorry for the delay in replying - what is your rclone command line?  I'm having trouble working out the sequence of events which could cause those log messages unless you are using `rclone move`?

> Might it be related to a large number of files? Swift's object list limit is 10000, so rclone is presumably paging through the object list - perhaps a bug there?

rclone lists in chunks of 1000 (that was 256 until recently)

Can you try again with the latest beta to see if the problem is still there? @johnl Just trying to work out what happened - which version of rclone were you using originally?  Nick, this is wierd.  I'm seeing this bahavior on a LOT of files on google drive.

I ran a sync on the google drive against the non-crypted representation of the crypted files:
```

HS -> rclone sync robgs:/cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/ riosgd:/cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/ --checksum -vv --dry-run 
2017/05/21 08:23:37 DEBUG : rclone: Version "v1.36-49-g5135ff7β" starting with parameters ["rclone" "sync" "robgs:/cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/" "riosgd:/cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/" "--checksum" "-vv" "--dry-run"]
2017/05/21 08:23:40 INFO  : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Modify window is 1ms
2017/05/21 08:23:40 NOTICE: Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Not making directory as dry run is set
2017/05/21 08:23:40 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Reading ""
2017/05/21 08:23:40 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Reading ""
2017/05/21 08:23:40 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Finished reading ""
2017/05/21 08:23:40 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Finished reading ""
2017/05/21 08:23:40 DEBUG : 662mdt2pq5trpjesuu7o3selbg53ppag9cdqchafas87djd1nov6nevc2nak41l7gpf1sevulk7t2: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 1mtcdo455na47j2hp5it4st4n56h9mb6i7s08du2939j8vpo46jb12vo58bf709qad65fmij6m3p4: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 1mtcdo455na47j2hp5it4st4n56h9mb6i7s08du2939j8vpo46jb12vo58bf709qad65fmij6m3p4: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 70eijfg70kvaoe3hcv93ufqiktgem60imn3qtmaatg8rvqdmp3v7cenao9ioh5v8qoip43lcss480: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 70eijfg70kvaoe3hcv93ufqiktgem60imn3qtmaatg8rvqdmp3v7cenao9ioh5v8qoip43lcss480: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 7bsdmjg4vqfs6n6nqdrkrkue2i3fcmssuvlcnb7vd9khscka105f2kkhspc63vbk7ka3pnj5p45j2: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 7bsdmjg4vqfs6n6nqdrkrkue2i3fcmssuvlcnb7vd9khscka105f2kkhspc63vbk7ka3pnj5p45j2: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 7ds7uj3oo0qkt9llm627u9hdle7ulgvn55oghv0ko8vetnpd6m6eqks5v5h1j3bso2g2ks0agu702: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 7ds7uj3oo0qkt9llm627u9hdle7ulgvn55oghv0ko8vetnpd6m6eqks5v5h1j3bso2g2ks0agu702: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 894a5ijahhvi794h8bf9tmb95glsb7n9r9q4jf57t4r55hgfoqhgsmlv7f58r14hi08ip74au25oo: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 894a5ijahhvi794h8bf9tmb95glsb7n9r9q4jf57t4r55hgfoqhgsmlv7f58r14hi08ip74au25oo: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 8bmqr0gj9glvhgfbb1rmbl6drknome3jpmhg2055fl3kb5n8sv5ri9n2i6aif9mf2qkhh9lujc688: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 8bmqr0gj9glvhgfbb1rmbl6drknome3jpmhg2055fl3kb5n8sv5ri9n2i6aif9mf2qkhh9lujc688: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 34ee2h4caqmg2k714so5f1qg6uanun2ehtvta0jv92jlu4jvcvm0s4c55n3cj45ibku9b411ccjtg: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 34ee2h4caqmg2k714so5f1qg6uanun2ehtvta0jv92jlu4jvcvm0s4c55n3cj45ibku9b411ccjtg: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 9952g9p8dl336q9nuv8g6kgbthgbm58p891m18c9sajcslddpnv05himi3kp0t15p67h49ppl22ea: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 9952g9p8dl336q9nuv8g6kgbthgbm58p891m18c9sajcslddpnv05himi3kp0t15p67h49ppl22ea: Unchanged skipping
2017/05/21 08:23:40 DEBUG : aum3ft2hqn1aokp4entmofqercl8q3pjgp15a0ue4fc0to0ljnoq6jl0vrdm2eml5ljkr7d3o7lfq: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : aum3ft2hqn1aokp4entmofqercl8q3pjgp15a0ue4fc0to0ljnoq6jl0vrdm2eml5ljkr7d3o7lfq: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 4vva6e6ooh0h4qtkaao8crb2n5t2n5co8lo09dinnl0drncf7k6847d9qo1tgt9ncbtt0crkd5no0: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 4vva6e6ooh0h4qtkaao8crb2n5t2n5co8lo09dinnl0drncf7k6847d9qo1tgt9ncbtt0crkd5no0: Unchanged skipping
2017/05/21 08:23:40 DEBUG : bke869i1uo33rrv1agt86v2u21p214m33p5vt4s1ukq583t7iuett3n0dj0vdb3p390009gkv5gi2: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : bke869i1uo33rrv1agt86v2u21p214m33p5vt4s1ukq583t7iuett3n0dj0vdb3p390009gkv5gi2: Unchanged skipping
2017/05/21 08:23:40 DEBUG : c5n3215d322s10aua0top2t9g0mli34cjsd8ka8l4nebpr1gmmhbkdtgl0o7rofda731t7t7ndc12: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : c5n3215d322s10aua0top2t9g0mli34cjsd8ka8l4nebpr1gmmhbkdtgl0o7rofda731t7t7ndc12: Unchanged skipping
2017/05/21 08:23:40 DEBUG : eu5cdcpu1fa7oo84copb8771g5knje0r0n36c8i5pkshq0qt31iurhg92n6btc267rftgfgjnst1m: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : eu5cdcpu1fa7oo84copb8771g5knje0r0n36c8i5pkshq0qt31iurhg92n6btc267rftgfgjnst1m: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 2njb1jgptodqflug5rdn7eqcb09sbub0o0lknk1gdt731700iem1omu6mhh9nac9lfitqln9oqpr6: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 2njb1jgptodqflug5rdn7eqcb09sbub0o0lknk1gdt731700iem1omu6mhh9nac9lfitqln9oqpr6: Unchanged skipping
2017/05/21 08:23:40 DEBUG : hg4kil2rg9q1n0kagchia7qpm16fqiagu328vn8q7iqs5c2m1larctccr7o5n9msfmovc1q7o41nk: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : hg4kil2rg9q1n0kagchia7qpm16fqiagu328vn8q7iqs5c2m1larctccr7o5n9msfmovc1q7o41nk: Unchanged skipping
2017/05/21 08:23:40 DEBUG : hhotc9jlhgujg7k5huj3d4h9cusk6gl1ao6crleglv8r2hhkaurou4q3nr7is661hk7d4p29ppvfo: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 1b6nt6ohd9tk0f6qfkc20qisdgubor0vh9m1qphkhcd2akdglg5q8etqsg4l5jonccqospie1mu9u: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 1b6nt6ohd9tk0f6qfkc20qisdgubor0vh9m1qphkhcd2akdglg5q8etqsg4l5jonccqospie1mu9u: Unchanged skipping
2017/05/21 08:23:40 DEBUG : iifo0s4fs029ajucmj9j3k52e9psruj4t62n7dlh0vsbgu1o0p0eumnd41nkck4blt1g54jf8o3jbkkh11qb7tg8fumkp48qc101gdg: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : iifo0s4fs029ajucmj9j3k52e9psruj4t62n7dlh0vsbgu1o0p0eumnd41nkck4blt1g54jf8o3jbkkh11qb7tg8fumkp48qc101gdg: Unchanged skipping
2017/05/21 08:23:40 DEBUG : itmqvijo3hgk99i0lnoup0kg15l6npe0cne225o5corpg3oc7niomknlaqgqaket3bfn3tu1sp5om: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : itmqvijo3hgk99i0lnoup0kg15l6npe0cne225o5corpg3oc7niomknlaqgqaket3bfn3tu1sp5om: Unchanged skipping
2017/05/21 08:23:40 DEBUG : j42gqqct673mj7876i18atmg98hnommeu4t7iun4q3fol567hiflhdeml1vcgj2kgbs2o0vt825lc: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : j42gqqct673mj7876i18atmg98hnommeu4t7iun4q3fol567hiflhdeml1vcgj2kgbs2o0vt825lc: Unchanged skipping
2017/05/21 08:23:40 DEBUG : kl8gnu35o3bqp4iacn2ms574cua4g0r7qs1cl2bh8qtpscq4la5uscjeqciuqif3rglnvrrhffc8a: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : kl8gnu35o3bqp4iacn2ms574cua4g0r7qs1cl2bh8qtpscq4la5uscjeqciuqif3rglnvrrhffc8a: Unchanged skipping
2017/05/21 08:23:40 DEBUG : l98ds5c5cb5e7j0nkeliupmjuvjr46050hphr8etr1kr0arg0sfe928p2pbdnaurc8m2o9mjvav48: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : l98ds5c5cb5e7j0nkeliupmjuvjr46050hphr8etr1kr0arg0sfe928p2pbdnaurc8m2o9mjvav48: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 1e5nute23ucogehejhh505mporkmb0e77qp6op2ndq5h8fg15mbohkpitlea7r12ihmjdd6n2q35a: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 1e5nute23ucogehejhh505mporkmb0e77qp6op2ndq5h8fg15mbohkpitlea7r12ihmjdd6n2q35a: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 662mdt2pq5trpjesuu7o3selbg53ppag9cdqchafas87djd1nov6nevc2nak41l7gpf1sevulk7t2: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 8mbgvguccgpj9kn469lurcmcoburk86tocdaan9vucfu2rh8fjq7gsh9a53kdvoa5cgro866o9aeb126idklhu2dq1kn20psponkn6o: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 8mbgvguccgpj9kn469lurcmcoburk86tocdaan9vucfu2rh8fjq7gsh9a53kdvoa5cgro866o9aeb126idklhu2dq1kn20psponkn6o: Unchanged skipping
2017/05/21 08:23:40 DEBUG : ous2c868h9rttvkq35n441o26vuq820nabn95prdl8v208kt25behlo7sf2uoo4k2t0okofv14hr6dds8lujpfbe6b0tfpoaap3nsk0: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : ous2c868h9rttvkq35n441o26vuq820nabn95prdl8v208kt25behlo7sf2uoo4k2t0okofv14hr6dds8lujpfbe6b0tfpoaap3nsk0: Unchanged skipping
2017/05/21 08:23:40 DEBUG : s8ue57pvenb4q8vb5uhv29a7537u60uhtu1jugm5r743r15vp5jseaj8m8u8a0co0e3bk5htu2ig6: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : s8ue57pvenb4q8vb5uhv29a7537u60uhtu1jugm5r743r15vp5jseaj8m8u8a0co0e3bk5htu2ig6: Unchanged skipping
2017/05/21 08:23:40 DEBUG : v36c9jujrlk55oqrts96dijqkkf3lpum8aetsk66o9ag6flfrrf4p65h45lmtngjpjr64c5576v9o: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : v36c9jujrlk55oqrts96dijqkkf3lpum8aetsk66o9ag6flfrrf4p65h45lmtngjpjr64c5576v9o: Unchanged skipping
2017/05/21 08:23:40 DEBUG : vjhnlmh4clsjh7laio3bafvtv8a4h8644bu6m5hav8a8etfsq047dops0pjjnpb2tpdakg8q1441k: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : vjhnlmh4clsjh7laio3bafvtv8a4h8644bu6m5hav8a8etfsq047dops0pjjnpb2tpdakg8q1441k: Unchanged skipping
2017/05/21 08:23:40 DEBUG : vqjpvrso6dh1pvorpqfuehtcs7bl9hven954h2t5vdp8svsqer0dm6se9fa4aa3ad9fn99ji05oui: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : vqjpvrso6dh1pvorpqfuehtcs7bl9hven954h2t5vdp8svsqer0dm6se9fa4aa3ad9fn99ji05oui: Unchanged skipping
2017/05/21 08:23:40 DEBUG : hhotc9jlhgujg7k5huj3d4h9cusk6gl1ao6crleglv8r2hhkaurou4q3nr7is661hk7d4p29ppvfo: Unchanged skipping
2017/05/21 08:23:40 DEBUG : 11d7cg175tkfj2pf0ecm474eq1976krlrfif4rqvm6nitqkj1b81v1tfd018k1sp1db7g0gkrgc3i: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : 11d7cg175tkfj2pf0ecm474eq1976krlrfif4rqvm6nitqkj1b81v1tfd018k1sp1db7g0gkrgc3i: Unchanged skipping
2017/05/21 08:23:40 DEBUG : pr57kca6irk57ffb5cj62ss216ugfse74gecbqs3l8gjad0uqagisc6f4lfbbcd9rulcdkr9cvlcg: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : pr57kca6irk57ffb5cj62ss216ugfse74gecbqs3l8gjad0uqagisc6f4lfbbcd9rulcdkr9cvlcg: Unchanged skipping
2017/05/21 08:23:40 DEBUG : qqru2ft86bmpgunshgo8u9b9vugv0irtj5c3a86huqohq7ehj1ep6ms1ofeum8embjg39mbi57vbmdpugfufvl2slo7ab8kt524bgn5jdfefnv22v5h7lm602u1egtn1: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : qqru2ft86bmpgunshgo8u9b9vugv0irtj5c3a86huqohq7ehj1ep6ms1ofeum8embjg39mbi57vbmdpugfufvl2slo7ab8kt524bgn5jdfefnv22v5h7lm602u1egtn1: Unchanged skipping
2017/05/21 08:23:40 DEBUG : lkqb5q67h310a9lqib3cog8375m4f6k6v6aqro423u637dmke735spu2bjlled9r2lr5nmr4cfcpu: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : lkqb5q67h310a9lqib3cog8375m4f6k6v6aqro423u637dmke735spu2bjlled9r2lr5nmr4cfcpu: Unchanged skipping
2017/05/21 08:23:40 DEBUG : b4j52bl8eur5pb90nafc54ms5tj0g3hs0rl40fshm2ideiprqqdd834ll829tijo706pplsgpfr0a: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : b4j52bl8eur5pb90nafc54ms5tj0g3hs0rl40fshm2ideiprqqdd834ll829tijo706pplsgpfr0a: Unchanged skipping
2017/05/21 08:23:40 DEBUG : gj1ac4sgumj4egtql0255mivddjfkp7t4eqsemcdcdom7o5vim4gtvq5fav2ib6mq9ht9g6llimka: Size and MD5 of src and dst objects identical
2017/05/21 08:23:40 DEBUG : gj1ac4sgumj4egtql0255mivddjfkp7t4eqsemcdcdom7o5vim4gtvq5fav2ib6mq9ht9g6llimka: Unchanged skipping
2017/05/21 08:23:40 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Reading "bi26l61nr0n7u32atdd3t4u0fo/"
2017/05/21 08:23:40 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Reading "bi26l61nr0n7u32atdd3t4u0fo/"
2017/05/21 08:23:41 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Finished reading "bi26l61nr0n7u32atdd3t4u0fo/"
2017/05/21 08:23:41 DEBUG : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Finished reading "bi26l61nr0n7u32atdd3t4u0fo/"
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/hvg468t5r7fmf5efq7tekhfgblt7864k6bs122tb873ousipqe0g: MD5 = 7bd69e8f1d09cd7f714b0b65141c4233 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/1467vg9ukunr3egp351q5smj7oo4md3qkgqc79nv166radvhbcd0: MD5 = e49bcfe6a7b375dc8f85eb690f1f8c6f (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/67ka2mppteqop7jcknc2e00oc4tlf7di83fmi3r6lt59u9f4kgp0: MD5 = daa9a8e0b5b930d1179e4cddcdd759f3 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/67ka2mppteqop7jcknc2e00oc4tlf7di83fmi3r6lt59u9f4kgp0: MD5 = 2fa17c9ba2ce3b87720a88b57fa44c0c (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/67ka2mppteqop7jcknc2e00oc4tlf7di83fmi3r6lt59u9f4kgp0: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g: MD5 = e62ad6931cd9d7428ead4296bab2a2dd (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/299nrpgvh3j07294fqpdplmp45i8dehoht0cv4dc5fest69oam4g: MD5 = 4ebaded0c642a56d4b5d631887f77b5a (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/299nrpgvh3j07294fqpdplmp45i8dehoht0cv4dc5fest69oam4g: MD5 = eab1b149edf64f1efbc8552b39a245fb (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/a0ga3oa6va95h46i5dtrr1nom8: MD5 = 26c06751d416e6926d62a2b147e8c04c (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/a0ga3oa6va95h46i5dtrr1nom8: MD5 = b5e6f39dbe45846487b1318afa800813 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/a0ga3oa6va95h46i5dtrr1nom8: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/hvg468t5r7fmf5efq7tekhfgblt7864k6bs122tb873ousipqe0g: MD5 = 89a90d00f5543fd19f3b3c78fcf7ff98 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/hvg468t5r7fmf5efq7tekhfgblt7864k6bs122tb873ousipqe0g: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/ks5ml5dfqiadev4t1e29o4ef20g90d1n7188bjqv6rmhjofmngkg: MD5 = f4e7f99163cb0f38c9eb4341d9fd3c71 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/ks5ml5dfqiadev4t1e29o4ef20g90d1n7188bjqv6rmhjofmngkg: MD5 = e305385089d8f1ea5a7aa6d1a406f446 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/ks5ml5dfqiadev4t1e29o4ef20g90d1n7188bjqv6rmhjofmngkg: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/hvg468t5r7fmf5efq7tekhfgblt7864k6bs122tb873ousipqe0g: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/ks5ml5dfqiadev4t1e29o4ef20g90d1n7188bjqv6rmhjofmngkg: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g: MD5 = 914294f32d885c4e77e17c4b335456bf (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/67ka2mppteqop7jcknc2e00oc4tlf7di83fmi3r6lt59u9f4kgp0: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/299nrpgvh3j07294fqpdplmp45i8dehoht0cv4dc5fest69oam4g: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/o2n5g2nkj02ur808514i2m9hms7fidmjjfauekrgr243ud0ljo3g: MD5 = 19bd738b0ce1c20b11d33fc79f309d75 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/299nrpgvh3j07294fqpdplmp45i8dehoht0cv4dc5fest69oam4g: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/a0ga3oa6va95h46i5dtrr1nom8: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/1467vg9ukunr3egp351q5smj7oo4md3qkgqc79nv166radvhbcd0: MD5 = 486dbb2c4b38e2cd3f3cb17a882cf58c (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/1467vg9ukunr3egp351q5smj7oo4md3qkgqc79nv166radvhbcd0: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/oca031civ090of8o9mat0n7jcubo6pd6fou3ovmskui8fogqeosg: MD5 = b3c500c48afab16ebe302d349684b6a2 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/oca031civ090of8o9mat0n7jcubo6pd6fou3ovmskui8fogqeosg: MD5 = 8f93d3eb1f549a50822e677f3c842abe (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/oca031civ090of8o9mat0n7jcubo6pd6fou3ovmskui8fogqeosg: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/le03la6a5jmaj1le14fd6g6pkcp9s420ok1fueuj6jqve9ojaoo0: MD5 = 68f60a661bdda934c2dde2c49e0f13a9 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/oca031civ090of8o9mat0n7jcubo6pd6fou3ovmskui8fogqeosg: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/4o08j8sc5mvtvd23p6laq8knhqlifqvs0ldcr63m85f9te7kn630: MD5 = b733dae8b353dc2f8b8653322d7094e2 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/4o08j8sc5mvtvd23p6laq8knhqlifqvs0ldcr63m85f9te7kn630: MD5 = 7c33bb723d9e04c23c9527959ce509b1 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/4o08j8sc5mvtvd23p6laq8knhqlifqvs0ldcr63m85f9te7kn630: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/sr478fnfdk3aa6boaqu91j8si0: MD5 = 88ce19aa91073dbc669e1c6cc467d1be (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/sr478fnfdk3aa6boaqu91j8si0: MD5 = 5c3e2257563099b2f75dab17a392a333 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/sr478fnfdk3aa6boaqu91j8si0: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/neokklqs45cfa8qa8v3734bcq8mqndqu53eeng4peg8g010ui26g: MD5 = 401f4d6f1c7f7c7cd54053ad067ba517 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/4o08j8sc5mvtvd23p6laq8knhqlifqvs0ldcr63m85f9te7kn630: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/3l3tjsrqgam55red3gn56tf5pnft38srs65vsjnksc11e01upkog: MD5 = 4d8e0846b37f5b90c417066394aa52b0 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/3l3tjsrqgam55red3gn56tf5pnft38srs65vsjnksc11e01upkog: MD5 = 9dee37c48822b9cbd9612637e59f112c (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/3l3tjsrqgam55red3gn56tf5pnft38srs65vsjnksc11e01upkog: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/u431qplbkdvae7kv2ijbfs9qpphhapuk2dh64u0d3jgl3qfs3lm0: MD5 = 400ea31a5f75d86341492a48baa33363 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/u431qplbkdvae7kv2ijbfs9qpphhapuk2dh64u0d3jgl3qfs3lm0: MD5 = 5e7dc4c4fee6325b6cbbdcd5c4062c68 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/u431qplbkdvae7kv2ijbfs9qpphhapuk2dh64u0d3jgl3qfs3lm0: MD5 differ
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/uft2ve9uai26l5js6h94usb0897en10goflmha2smvtduvnp1svg: MD5 = 9b72c19fdc99e044dc908342962446a7 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/uft2ve9uai26l5js6h94usb0897en10goflmha2smvtduvnp1svg: MD5 = 40b2c2799c843a850638af30687fc221 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/uft2ve9uai26l5js6h94usb0897en10goflmha2smvtduvnp1svg: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/3l3tjsrqgam55red3gn56tf5pnft38srs65vsjnksc11e01upkog: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/u431qplbkdvae7kv2ijbfs9qpphhapuk2dh64u0d3jgl3qfs3lm0: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/uft2ve9uai26l5js6h94usb0897en10goflmha2smvtduvnp1svg: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/o2n5g2nkj02ur808514i2m9hms7fidmjjfauekrgr243ud0ljo3g: MD5 = d7e4c59dc04001d6d0f9aaea128b9197 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/o2n5g2nkj02ur808514i2m9hms7fidmjjfauekrgr243ud0ljo3g: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/o2n5g2nkj02ur808514i2m9hms7fidmjjfauekrgr243ud0ljo3g: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/j2782vnnoquq9lmld976e89p64m39e4h88o180kugu4fd5nocs4g: MD5 = 24bb4ff8c8014756cafec0cdc645e232 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/j2782vnnoquq9lmld976e89p64m39e4h88o180kugu4fd5nocs4g: MD5 = 1fa65c12b0e5d294cf5600bea9e41815 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/j2782vnnoquq9lmld976e89p64m39e4h88o180kugu4fd5nocs4g: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/j2782vnnoquq9lmld976e89p64m39e4h88o180kugu4fd5nocs4g: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/rk71vnkub2n6ukakhqdm3rt7quj0jjm03kbctrgrm2v19hreq8tg: MD5 = 70af023854158b3ab9b21608d18e93b8 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/rk71vnkub2n6ukakhqdm3rt7quj0jjm03kbctrgrm2v19hreq8tg: MD5 = f7a580ceee7125398e64dccdc06f3a86 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/rk71vnkub2n6ukakhqdm3rt7quj0jjm03kbctrgrm2v19hreq8tg: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/rk71vnkub2n6ukakhqdm3rt7quj0jjm03kbctrgrm2v19hreq8tg: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/1467vg9ukunr3egp351q5smj7oo4md3qkgqc79nv166radvhbcd0: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/le03la6a5jmaj1le14fd6g6pkcp9s420ok1fueuj6jqve9ojaoo0: MD5 = 508d80bf9d0aae9af00b55e0f37b8f7f (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/le03la6a5jmaj1le14fd6g6pkcp9s420ok1fueuj6jqve9ojaoo0: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/le03la6a5jmaj1le14fd6g6pkcp9s420ok1fueuj6jqve9ojaoo0: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/e0tgcire3mjcvoprqp37b6vv2bdgslh49tgvai42np3rkj1s694g: MD5 = 6f9c87a8e3b97726488f2c4eda9d1f5b (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/e0tgcire3mjcvoprqp37b6vv2bdgslh49tgvai42np3rkj1s694g: MD5 = c5a0f69ae41537a488b43a64f789897f (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/e0tgcire3mjcvoprqp37b6vv2bdgslh49tgvai42np3rkj1s694g: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/e0tgcire3mjcvoprqp37b6vv2bdgslh49tgvai42np3rkj1s694g: Not copying as --dry-run
2017/05/21 08:23:41 INFO  : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Waiting for checks to finish
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/tgd0rp7aga646mui92tjbp9tds3apu56lv7l42uon7a5e76kj7n0: MD5 = 59e0d1748cc2c3bb52c8299ac9a5fbd4 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/tgd0rp7aga646mui92tjbp9tds3apu56lv7l42uon7a5e76kj7n0: MD5 = 8c261f41fad0a6e7b29cbd8b904e0ef0 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/tgd0rp7aga646mui92tjbp9tds3apu56lv7l42uon7a5e76kj7n0: MD5 differ
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/tgd0rp7aga646mui92tjbp9tds3apu56lv7l42uon7a5e76kj7n0: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/sr478fnfdk3aa6boaqu91j8si0: Not copying as --dry-run
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/neokklqs45cfa8qa8v3734bcq8mqndqu53eeng4peg8g010ui26g: MD5 = 12ee1be07925f4c58fa9c1a5d01955f3 (Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4')
2017/05/21 08:23:41 DEBUG : bi26l61nr0n7u32atdd3t4u0fo/neokklqs45cfa8qa8v3734bcq8mqndqu53eeng4peg8g010ui26g: MD5 differ
2017/05/21 08:23:41 INFO  : Google drive root 'cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4': Waiting for transfers to finish
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g: Not copying as --dry-run
2017/05/21 08:23:41 NOTICE: bi26l61nr0n7u32atdd3t4u0fo/neokklqs45cfa8qa8v3734bcq8mqndqu53eeng4peg8g010ui26g: Not copying as --dry-run
2017/05/21 08:23:41 INFO  : Waiting for deletions to finish
2017/05/21 08:23:41 INFO  : 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:                56
Transferred:           20
Elapsed time:        3.9s
2017/05/21 08:23:41 DEBUG : Go routines at exit 7
2017/05/21 08:23:41 DEBUG : rclone: Version "v1.36-49-g5135ff7β" finishing with parameters ["rclone" "sync" "robgs:/cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/" "riosgd:/cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/" "--checksum" "-vv" "--dry-run"]

```


I knew this drive was a complete sync of a very recent clone so was surprised by the need to recopy files.  I selected one to check:

```
rclone md5sum riosgd:cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g 
914294f32d885c4e77e17c4b335456bf  i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g

HS -> rclone md5sum robgs:cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/rheiv1nvivna0s0skafojp9ne4/bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g 
e62ad6931cd9d7428ead4296bab2a2dd  i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g
```

They do in fact differ so that makes sense but im still surprised.  So I download them from within the crypt.  I mapped them to the crypt file using the crypt mapping command:

`2017/05/20 21:07:11 NOTICE: Media/Videos/Series/Californication/.actors/Oliver_Cooper.jpg: Encrypts to "lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/nhcij8dbshqodics7ddemvfbok/i1dt6gb9f46ad1lp26ap7ci0q0/bi26l61nr0n7u32atdd3t4u0fo/i8lujvia3cvuv90fo9o0voatggmsurfoa47ec973g3r9c2v46f4g"
`

```
HS -> rclone copy robgs-cryptp:Media/Videos/Series/Californication/.actors/Oliver_Cooper.jpg /home/robert/a/
HS -> rclone copy riosgd-cryptp:Media/Videos/Series/Californication/.actors/Oliver_Cooper.jpg /home/robert/b/

HS -> md5sum /home/robert/a/Oliver_Cooper.jpg
5528ac071237f0e946d21ab0f2f21a81  /home/robert/a/Oliver_Cooper.jpg

HS -> md5sum /home/robert/b/Oliver_Cooper.jpg
5528ac071237f0e946d21ab0f2f21a81  /home/robert/b/Oliver_Cooper.jpg
```
When I download the files the md5sum is the same.  They are identical files.  This is no a one-off case.  I've done a full sync on the two google drives crypt volumes and a very very large number of files are being replaced as if they are different when I do it against the crypt representation but for each case when I check the files within the crypt they are identical.  Any ideas? 






 more....  I have thousands like this:
```

✓ robert [~] $
HS -> grep lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/9bou635g4nb6o0bmo31ppffus4/8jvagk09qa9ilvh97d6inc0ta6s78tan8752rv7s4nln1j3jojds2vmi2aq26fq1tsa5vuegt1gq6/bi26l61nr0n7u32atdd3t4u0fo/u2sekqcqctt3h6dip49llc1mno out1
2017/05/20 21:09:21 NOTICE: Media/Videos/Movies/Assassins.Creed.(2016).720p.bluray/.actors/Coco_König.jpg: Encrypts to "lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/9bou635g4nb6o0bmo31ppffus4/8jvagk09qa9ilvh97d6inc0ta6s78tan8752rv7s4nln1j3jojds2vmi2aq26fq1tsa5vuegt1gq6/bi26l61nr0n7u32atdd3t4u0fo/u2sekqcqctt3h6dip49llc1mno"
✓ robert [~] $

HS -> rclone md5sum riosgd:cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/9bou635g4nb6o0bmo31ppffus4/8jvagk09qa9ilvh97d6inc0ta6s78tan8752rv7s4nln1j3jojds2vmi2aq26fq1tsa5vuegt1gq6/bi26l61nr0n7u32atdd3t4u0fo/u2sekqcqctt3h6dip49llc1mno 
49c305f6c997b7b4bc6ef9105090813d  u2sekqcqctt3h6dip49llc1mno
✓ robert [~] $
HS -> rclone md5sum robgs:cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/9bou635g4nb6o0bmo31ppffus4/8jvagk09qa9ilvh97d6inc0ta6s78tan8752rv7s4nln1j3jojds2vmi2aq26fq1tsa5vuegt1gq6/bi26l61nr0n7u32atdd3t4u0fo/u2sekqcqctt3h6dip49llc1mno 
0c273c6b9c2dca6f429eadf4e34bbff7  u2sekqcqctt3h6dip49llc1mno
✓ robert [~] $
HS -> rclone copy 'robgs-cryptp:Media/Videos/Movies/Assassins.Creed.(2016).720p.bluray/.actors/Coco_König.jpg' /home/robert/a/
✓ robert [~] $
HS -> rclone copy 'riosgd-cryptp:Media/Videos/Movies/Assassins.Creed.(2016).720p.bluray/.actors/Coco_König.jpg' /home/robert/b/
✓ robert [~] $
HS -> diff '/home/robert/a/Coco_König.jpg' '/home/robert/b/Coco_König.jpg'
✓ robert [~] $
HS -> md5sum '/home/robert/a/Coco_König.jpg'
93cd2a98d2bf75b972431bc97ddccf36  /home/robert/a/Coco_König.jpg
✓ robert [~] $
HS -> md5sum '/home/robert/b/Coco_König.jpg'
93cd2a98d2bf75b972431bc97ddccf36  /home/robert/b/Coco_König.jpg
✓ robert [~] $


HS -> rclone copy robgs:cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/9bou635g4nb6o0bmo31ppffus4/8jvagk09qa9ilvh97d6inc0ta6s78tan8752rv7s4nln1j3jojds2vmi2aq26fq1tsa5vuegt1gq6/bi26l61nr0n7u32atdd3t4u0fo/u2sekqcqctt3h6dip49llc1mno /home/robert/a
✓ robert [~] $
HS -> rclone copy riosgd:cloudp/lp08934q11u6a5gslo2ddds5b4/hkitmqcr303hk1a33m19ttdq24/9bou635g4nb6o0bmo31ppffus4/8jvagk09qa9ilvh97d6inc0ta6s78tan8752rv7s4nln1j3jojds2vmi2aq26fq1tsa5vuegt1gq6/bi26l61nr0n7u32atdd3t4u0fo/u2sekqcqctt3h6dip49llc1mno /home/robert/b
✓ robert [~] $
HS -> diff /home/robert/a/u2sekqcqctt3h6dip49llc1mno /home/robert/b/u2sekqcqctt3h6dip49llc1mno
Binary files /home/robert/a/u2sekqcqctt3h6dip49llc1mno and /home/robert/b/u2sekqcqctt3h6dip49llc1mno differ
✗ robert [~] $
HS -> md5sum /home/robert/a/u2sekqcqctt3h6dip49llc1mno
0c273c6b9c2dca6f429eadf4e34bbff7  /home/robert/a/u2sekqcqctt3h6dip49llc1mno
✓ robert [~] $
HS -> md5sum /home/robert/b/u2sekqcqctt3h6dip49llc1mno
49c305f6c997b7b4bc6ef9105090813d  /home/robert/b/u2sekqcqctt3h6dip49llc1mno
✓ robert [~] $
HS -> 

```

It is really wierd that I can decrypt them and they are identical but leaving them crypted they have different md5sums even on my local files system.  
 Did you sync from crypt -> crypt?

I suspect that caused this.  When rclone encrypts a file it chooses a [random nonce](https://rclone.org/crypt/#file-encryption) which will make the representations of the files different if you copied them from crypt -> crypt.  The random nonce is an important part of keeping the files secure

I recommend that people copy encrypted remotes by copying the underlying unencrypted remotes - this takes less CPU and the hashes will match.

Does that make sense? Yes. It does. Thanks.    I'm going to close this issue now - for the resolution see https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/  That looks simple enough.

It does change the oauth for all the other cloud storage remotes - have you checked it works with them?
 I've tested this on the other remotes and it works fine, so I'm going to merge.

Thank you very much :-)  If you do

    rclone copy . --include "Filename.S01E*.mkv" Remote:

that should have the same effect.

Not quite as convenient, I agree but it should work.  That does seem strange!

Can you get a log with `-vv` of it happening?

It might be that the directory listing thread is having lots of retries at b2 maybe. This is probably related to  #1443  I think.

Because the chunks for large files are buffered in memory...

I'll check the code.

I'll still like to see those logs if possible.  This will require another flag.  You'd have to use --no-traverse and this flag --no-check-destination (say).

--no-check-destination would have to not run NewObject (just leave dst as nil) in `fs/sync.go` here

```go
func (s *syncCopyMove) runRecursive() error {
...
		if s.noTraverse {
			var err error
			dst, err = s.fdst.NewObject(remote)
			if err != nil {
				dst = nil
				if err != ErrorObjectNotFound {
					Debugf(src, "Error making NewObject: %v", err)
				}
			}
		} else {
```

There should be another place it needs changing too for the --old-sync-method but I couldn't find it in 1 minute!

 Fancy patching rclone and see if that helps and work that up into a PR?  I put a [post in the forum](https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/2314) which I'll update as and when I hear more info. @yonjah wrote
> You updated that amazon have banned since the secret are kept in code
Does that mean the fail to implement the oAuth2 protocol regarding native app shared secret ?
https://tools.ietf.org/html/draft-ietf-oauth-native-apps-07#section-8.9

I've not seen that before, but it hits the nail on the head - thanks for pointing that out.

Yes, amazon are treating the client ID/ client secret as confidential even for native apps.

> We had this discussion regarding g drive in #309 before . Having an external server might cause more issues than it actually solves and the only situation where it might make sense is if Amazon has security flaws in the way the implement oAuth2.0

I don't really want to run an auth server and personally think it is the wrong approach.  However if it gets rclone back online then I'll go with it.

I've yet to hear back from Amazon though. I'm going to close this issue now - for the resolution see https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/  That looks perfect - thank you :-)  I have some emails out to contacts at Amazon asking what has happened.  They are on the west coast of the US I think, so they won't be in to work for a few hours...

@mdubash interesting info - thanks @geekcroft yes I have.  I'd like confirmation though. Yes. If look in the manage apps section in cloud drive, rclone is no longer listed just like acdcli was removed. This looks like permanent or at least long term ban now.... I put a [post in the forum](https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/2314) which I'll update as and when I hear more info. @uncuervo - you just need 1 person as the 5 user minimum isn't enforced now. @uncuervo - only stating what is accurate for folks at this moment so they can make informed decisions as your statement isn't correct "now" :) @uncuervo  - That's what I'm personally using:

http://imgur.com/Sgf7LqR - my GD

http://imgur.com/GTIbUJc - my Billing

http://imgur.com/B70aFVG - Unlimited @poom - few months now. All the business plans have this and you just have to move a domain over. It took me a few minutes to get it setup.  @SirJMD that is terrible.  I've always had really good performance from gd.  Must be the end point your connected to. @SirJMD I just maxed my 150 megabit connection with a single transfer to gd. Perhaps there is a way to change your end point.   I'm going to close this issue now - for the resolution see https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/  That is looking really good :-)

Some small stuff
* Can you link the commit message to #801
* Can you split the vendor commit into a separate commit "vendor: add xyz for azure"?
* Fix the build! (I think you've missed actually committing the stuff in vendor)

I presume you've seem the docs on how to [write a new backend](https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md#writing-a-new-backend)? You've got to "unit tests" by the look of it - if you generate them for Azure, then they will guide you through making it 100% compliant with the rest of rclone.

Excuse my ignorance but are there different types of Azure storage?  This just addresses blobs - will there be a requirement for people for the other types of storage? I'm wondering whether it might be better called "azureblob" but happy to go with your recommendation.

Anyway, great stuff - carry on :-)
 Looking forward to seeing your changes!

Happy for you to keep force pushing this PR if that makes a good workflow for you.

The tests are quite fussy, so let me know if you need any help getting them to pass. How are you doing with this?

I have to make an apology - I've just changed the List interface, splitting it into a simple List which just lists one directory, and a ListR interface with does a recursive list if the remote is capable of it. This takes a lot of complexity out of the remotes which is a good thing hopefully! If you take a look at the S3 remote you'll see [how it has changed](https://github.com/ncw/rclone/commit/8a6a8b9623bea1f600ad15eec2dc74ed37e65110#diff-7db3cb93944d57b2ffc803281c906018)

  All 429 errors for me too.

I'll email my contacts at amazon and see what they have to say. > It's annoying because they've disabled new signups to the ACD API, otherwise the logical step for us would be to create our own API keys!

Yes you used to be able to create your own keys and and use them on your own account before having the app whitelisted (which is the hard part). Yes. If look in the manage apps section in cloud drive, rclone is no longer listed just like acdcli was removed.  This looks like permanent or at least long term ban now....  I'm going to close this issue now - for the resolution see https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/  I think this is probably caused by the eventual consistency of the listings for acd.

Sometimes after uploading a file it won't be in the file listings for a while.  I wonder whether partial info is in the file listings at the point md5sum is run.

You can debug with `-vv --dump-bodies` to look see what amazon actually sends.
 I've tracked this down...

The problem is due to the way rclone interprets the command line.  When you say `rclone md5sum acd:/testfile.txt` rclone doesn't know if `testfile.txt` is a directory or a file.

rclone has a really convoluted method of working out whether it is a directory or a file which just happens to go wrong when amazon declares the file is missing after it previously declared it was there.

So, yes this is a bug and I'll fix it when I rationalise the working out whether `testfile.txt` is a directory or a file code.

This may take a release or two!  > 2017/05/16 18:38:39 ERROR : series/file.mkv: ReadFileHandle.Read error: low level retry 1/10: unexpected EOF

rclone will retry those errors as the `retry 1/10` bit indicates.  That is probably what is causing the slow start if you are getting a lot of them.

`Unexpected EOF` is caused by the stream from amazon dropping unexpectedly for some reason.  It is worth doing the simple things like rebooting your router to see if you see any improvements.

Using `--buffer-size 128M` (say) will help a lot with this - it allows rclone to read ahead in the stream so it will have time to remake the connection if it drops before you notice.

I'm not sure about `--acd-templink-threshold` - I don't think it will help, but it doesn't hurt to try!  That looks really interesting.  I have a build pipeline set up, but it is low overhead to have circleci building stuff too so I'll merge this and we can have a play with it - thanks! Build status here: https://circleci.com/gh/ncw/rclone No I didn't do anything special  What problem are you trying to solve?  And on which OS? I'm guessing OS X...

The normalization was added because OS X stores its file names an unormalized UTF-8 for some reason.  It probably isn't necessary for Windows / Linux.

I'm not sure whether the cloud providers store unnormalized UTF-8 - I know some do (eg S3/ACD) but I imagined when I put that feature in that some might not.

I think a flag to selectively disable is a good idea - it would work well when syncing to crypt.

This would need to be a flag in `local/local.go` in the `cleanRemote` function to stop the `	name = norm.NFC.String(name)` line running.

The flag should then be called `--local-no-unicode-normalization` or maybe `--local-no-unicode-nfc`.

Fancy having a go? This should be built into

https://beta.rclone.org/v1.36-122-gbe5b8b8d/ (in 15-30 mins)  It is always a possibility.  However I've had various email conversations with the amazon developers in the past and I don't think they would do that without a dialogue first. I was distressed to see on that thread a discussion of how to use rclone's keys to restore service.  Please don't do this - it will likely lead to rclone being banned in the same way. > Edit 2: Now that stuff on ACD is encrypted using rclone, I'm worried that we won't be able to retrieve and decrypt the backup. :(

Provided you can retrieve the files somehow, you can decrypt them with rclone locally very easily. > You are right. But it would still require downloading the whole directory just to get one file, right?
Because we will no longer be able to rclone lsl remote-crypted:directory

Assuming that there existed another tool that could mount acd, then you could set it up so the above would work just fine. @Rufflewind wrote
> No, https://tensile-runway-92512.appspot.com/src is acd_cli’s default authentication server. Amazon’s investigation indicates that it’s buggy and can sometimes leak tokens to the wrong user.

Just so everyone is clear, rclone doesn't use an external authentication server - it is all done on your computer.  That is why the signup for Amazon Drive (and all the oauth providers like Google etc) is a little awkward.

I didn't want to have the responsibility of other people's credentials going through my server so I deliberately crafted rclone so that wasn't necessary.  Your credentials never leave your computer! I think maybe rclone's quota has been reduced.  I'm contacting Amazon to find out what is happening. @Webreaper I understand your inclination for make believe right now but those customer support exchanges are useful and pertinent to the issue, _not speculation by any means_: these are official company reps. Several users have reported similar exchanges already so they are credible. Now, is the ban written in stone? I hope not but in the meantime those tech support comments are informative about the current situation regarding this issue. @Webreaper it's common for people to post similar reports about the same thing on any given issue in order to confirm it so why should this issue be any different? It's expected that that will happen here too. Tech support comments are only inaccurate (not misleading: they are all saying the same thing right now) if the situation changes in the future, in the meantime and until we get further answers they are giving us _the company line_ which is useful and informative for this issue at this time. @Webreaper should've unsubscribed before asking users to stop posting useful information. And I do hope you're right and it _ends up_ being inaccurate information but in the meantime it is not: it's the company line and pertinent to this issue. @gordan-bobic Amazon almost never denies a refund so this is hardly indicative of anything. @ncw hasn't received an answer from ACD team yet so removing anything ACD related from rclone is premature. His most recent tweet related to the issue: https://twitter.com/njcw/status/865583044870189056 I'm going to close this issue now - for the resolution see https://forum.rclone.org/t/rclone-has-been-banned-from-amazon-drive/ @marvinpinto there is an update in the first post, you don't have to scroll the entire thread.  I have merged that as it gets sftp working again in master - thank you!

What do you think about doing it properly?  Storing the host key in the config file and checking it?

It shouldn't be too difficult.  This seems to be the problematic part of the log

@breunigs - you wrote this bit of code I believe - any ideas?

```
2017/05/15 10:21:58 DEBUG : .bup/config.lock: Dir.Rename to ".bup/config"
2017/05/15 10:21:59 DEBUG : .bup/config.lock: Move: quick path rename failed: HTTP code 409: "409 Conflict": response body: "{\"code\":\"NAME_ALREADY_EXISTS\",\"logref\":\"9095b131-3947-11e7-b266-fde87a655d7c\",\"message\":\"Node with the name config already exists under parentId UZpZ4f0QSkuO-ZjPaM4S-A conflicting NodeId: YBm7WlwAQ_iIr8oxnEX6Bw\",\"info\":{\"nodeId\":\"YBm7WlwAQ_iIr8oxnEX6Bw\"}}"
2017/05/15 10:21:59 DEBUG : .bup/config.lock: Could not directly rename file, presumably because there was a file with the same name already. Instead, the file will now be trashed where such operations do not cause errors. It will be restored to the correct parent after. If any of the subsequent calls fails, the rename/move will be in an invalid state.
2017/05/15 10:22:01 DEBUG : .bup/config.lock: Updating file with .bup/config 0xc420552540
2017/05/15 10:22:01 DEBUG : .bup/config: Dir.Rename renamed from ".bup/config.lock"
```  Can you post an `rclone lsl` of the local directory an an `rclone lsl` of  the remote directory `Fullmetal Alchemist - The Sacred Star of Milos (2011)` please?

If you try `rclone check` on that directory what does it say?  24 hour ban from Google.  That's all You'll have to post a bit more of the log with `-vv`.  Do you see any low level retries before the error?  A cronjob is the only way to achieve this at the moment.

Getting rclone to loop is part of #1119 - please add your support to that ticket if you'd like to see this built in to rclone.  Or even better send a PR :-)

Thanks  When i've had this happen, they were listed as duplicates.  I found it would never recover but if I deleted BOTH versions of the duplicates, i didnt see the problem anymore.   You can use the rclone dedupe to determine if the ones you are having issues with have duplicates.  Even if they don't, try deleting the singular version before the sync and then resync again to verify the problem is gone.   Excellent suggestion from @calisro :-) 

If it doesn't bear fruit, then please paste some logs with `-vv` which will show the exact reasons rclone wanted to copy the files.  Already an issue for this.  https://github.com/ncw/rclone/issues/502 @FernandoMiguel  please add your support to #502 - I'm going to close this as a duplicate.  Note that rclone measures its bandwidth limits in kByte/s and MByte/s not not bit/s

Using `--bwlimit 0.3M` gives you `Starting bandwidth limiter at 307.199kBytes/s`

Which should be approx 2.5 Mbit/s.  Not sure why you are seeing nearly 3 times that.

What happens if you try using `--bwlimit 37k` which should get you around 300kBit/s? Thanks for investigating further.

The sharp cutoff makes me think this might be to do with the timer or sleep resolution of FreeBSD.

Which processor are you running?

I can send you some binaries to try. I presume you are running 64 bit - if not I'll have to remake the binaries...

Can you post the results of what happens when you run this binary please?

https://www.craig-wood.com/nick/pub/timer-test

It should print something like this

```
time.Sleep(1s) actually slept for 1.000089761s
time.Sleep(500ms) actually slept for 500.201545ms
time.Sleep(250ms) actually slept for 250.153946ms
time.Sleep(125ms) actually slept for 125.16933ms
time.Sleep(62.5ms) actually slept for 62.704354ms
time.Sleep(31.25ms) actually slept for 31.35135ms
time.Sleep(15.625ms) actually slept for 15.832816ms
time.Sleep(7.8125ms) actually slept for 7.963973ms
```

And if you try running this version of rclone with `-vv`  to do your copies to b2 it will print lots of messages like this

https://www.craig-wood.com/nick/pub/rclone

2017/05/09 17:20:55 DEBUG : bwlimit sleep 0s
2017/05/09 17:20:55 DEBUG : bwlimit sleep 0s
2017/05/09 17:20:55 DEBUG : bwlimit sleep 100ms
2017/05/09 17:20:55 DEBUG : bwlimit sleep 100ms

I'd be interested in a sample of what those look like.

Can you also try copying to a different provider to see if you get the same problem?  (Eg sftp/google drive)

Thanks No that all looks normal which is good.

I tried it on a FreeBSD VM and it worked fine with using --transfers 32, --bwlimit 0.3M to b2.

I compared the rates printed by rclone (in Bytes/s) to the speeds shown in `iftop` (in bits/s - divide by 8 to get bytes/s) and they matched quite closely for `--bwlimit 0.3M` and `--bwlimit 0.1M`.

Can you send me a log of  the file transferring with `-vv --stats 10s` please.

Can you also check the bandwidth with `iftop` so we can both be using the same tools?

Thanks Are you still trying it on the same large >100MB file?

 That is good to hear!  I'll close this then, but please re-open if necessary.  You need to set your [GOPATH](https://golang.org/doc/code.html#GOPATH) - all go packages need that.

I'd suggest

```
mkdir ~/go
export GOPATH=~/go
go get -u -v github.com/ncw/rclone
```

Like it says in the [installing from source](https://rclone.org/install/#install-from-source) section in the docs. The error

> package fmt: unrecognized import path "fmt" (import path does not begin with hostname)

Indicated you've set GOROOT I think.  Do `unset GOROOT` and all should be fine hopefully.  This isn't a supported way of building rclone.

The recommended way of installing rclone from source is documented [in the install docs](https://rclone.org/install/#install-from-source).

You can use the makefile to build rclone, but you need to set it up as a go project first, in the right filesystem heirachy and with GOPATH set.  > but there seem to be another download attempt each time rclone reports a seek:

That is how seeking is currently implemented.  When you do a seek, rclone will re-open the file with a `Range` request seeking to the position you asked for.

rclone does not as yet buffer read files, leading to the above.

There are various efforts underway to improve this, including a LRU cache for file segments which would fix this, so watch this space!  Can you try the latest beta please - this is likely fixed already.

Thanks  Firstly note that those are only warnings - likely things will still work.

This is the first report of this problem I've seen.

If your clock is badly out then the oauth protocol doesn't work very well.  I can't remember how long b2's initial grant is, but if it is 24hr then you won't notice until 24-5 hours time....

So firstly, is the time correct on your computer?  rclone is suggesting it is ~5h out.  Likely this is a timezone thing, so is the timezone set correctly on your computer?

The other explanation would be that FreeBSD treats timezones somehow differently.

What happens if you try setting your time from a remote server with rdate? (or ntp?) Well done for tracking it down and thanks for reporting back.  Could it be  
 --buffer-size int                   Buffer size when copying files. (default 16M)

contributing to linux filesystem cache?  file system cache would get flushed if needed anyway by linux.  Well.  that parameter was only introduced in 1.36.  So my point was that it defaults to use 15M per.  Turn it off and see what happens.  It didn't buffer at all in 1.35.
--buffer-size 0

https://forum.rclone.org/t/rclone-v1-36-released/1383 How many files are there in the sync?

Your graph is of swap usage - did you actually measure the memory usage of rclone?

Here is my basic memory logger.  Save it in a file called `memusage` and then run `memusage rclone` and it will log the memory usage once per second.  You might want to change the sleep to 60 if you are running it for ages.

```
#!/bin/sh
# Monitor memory usage of the given command

COMMAND="$1"

if [ "$COMMAND" = "" ]; then
   echo "Syntax: $0 command"
   exit 1
fi

headers=""
while [ 1 ]; do
    ps $headers -C "${COMMAND}" -o pid,rss,vsz,cmd
    sleep 1
    headers="--no-headers"
done
``` @lickdragon wrote
> Sadly, this was hot the fix.

Do you mean setting `-buffer-size 0` was the solution? Or wasnt?

> I'm trying to git bisect this problem.  I would like to download the
precompiled binary.  How does one determine the "commit number"?

The commit numbers are shown in the [git log](https://github.com/ncw/rclone/commits/master).  Note it is only the first few (6?) digits of the full commit hash in the zip file names.

@unnfav is probably right though the easiest way will be to go through the available binaries at https://beta.rclone.org and bisect from that list.  If I look at my project ID quotas.  For the past 30 days the highest I had was 365 queries per 100 seconds.

On my 2nd project ID (different account) the highest I had there was 500 Q/100s over a 30 day period.

EDIT: I guess you're right though.  What happens is the client hits the rate limits and then throttles before retrying which levels it out pretty quickly because everything sits in this "Rate limited, sleeping for 2.210826867s (2 consecutive low level retries)" state and throttles itself.  Perhaps increasing that number would in effect lower the error rates and raise efficiency without actually lowering performance because we're just getting throttled anyway due to 403s.  I have noticed that my error rates are always pretty high (greater than 20%) even on a simply lsl and I typically don't use too many checkers/transfers. @azgul  wrote:
> Just a question, what is it for the rclone project? It defaults to 1,000 requests/100s per user, so 10 requests per sec.

That is a long term average, but yes that is correct.

Rclone is set to `Queries per 100 seconds per user	1,000` so 10 per second as you said.

> Currently minSleep in the drive implementation is set to 10 ms which implies that the quota for users is 10,000/100s (10x as many as a fresh project has). Is that correct or could it be why so many people are hit with soft bans for exceeding user quota?

It is a possibility.  I set the quotas based on people doing file transfers which use less transactions per second.  The theory being you need a burst during the directory comparisons then not a lot during the actual file transfers.

> So if this theory is correct it would be trivial to increase minSleep to 100ms and never get rate limited on a user basis. I'll be testing this over the weekend.

Worth a try!

@jaketame wrote
> So it could be that @ncw has requested a rate increase for QPS hence the lower minSleep = 10 but for people using there own client id then this is too high and could be why people are getting API bans rather than download bans on syncing.

rclone has 1200 QPS now I think.  It is quite easy to get them to raise the limit to 100 QPS, above that it harder!  > Since switching from rclone v1.35 to v1.36, it now makes no output at all on stdout or stderr, though appears to otherwise be syncing.

I changed the defaults for rclone do be less noisy to bring it into line with rsync.

This confused a lot of people, so I apologise!

Using the `-v` flag will bring back the previous behavior more or less.  Can you post a log with `-vv --dump-headers` please? That looks like an IO timeout.  Does it work from your web browser?

When I try

```
curl --head 'http://bookdl.online/' -H 'User-Agent: rclone/1.36-DEV'
```

It works fine.  How does that work for you on the system you are running rclone on?  What are you trying to acheive?

Google drive will preserve mime types when copying from remote to remote.  Can you post a log with `-vv` please? The error is

> 2017/05/03 06:30:06 ERROR : Attempt 20/20 failed with 0 errors and: error reading source directory ".DocumentRevisions-V100": failed to open directory "/Volumes/music/.DocumentRevisions-V100": open /Volumes/music/.DocumentRevisions-V100: permission denied

Which can be worked around with a filter as @ajkis suggested :-)

If you have having trouble with the filters then post them here and we can help you debug them - they should definitely work with b2. The above said, I intend to make rclone continue after that sort of error which is what rsync does, and just report an error at the end.  The command hangs.  That is normal.  put a '&' on the end. and ls the directory.

rclone mount -vv gd: /mnt/gd &
ls /mnt/gd
  There is already a section on installation with [snap in the installation instructions](https://rclone.org/install/).

This is replicated in MANUAL.md

If you'd like to send patches then send them against `docs/content/install.md` as the website and MANUAL.md are both built from there. Since I haven't heard from you, I'm going to close this for the reasons above.  rclone does multiple transfers within the same ssh connection which is part of the sftp spec.

Which sftp server are you using?  This isn't a problem with any SFTP servers I've tried so far.

It might be that rclone needs to adapt to the server somehow... I need to either know which sftp server it is, or have access to it for a bit to make progress with this! @sjurtf wrote
> Multiple ssh connections (like the Filezilla client does) would solve this problem, and some others related to rate limiting and TCP + latency issues.

So one ssh connection per transfer? Seems inefficient as the ssh handshake is quite expensive.  BTW How do you know the Filezilla client does this?

ssh is a transport which can have multiple streams within it.  By default rclone will do `--transfers` streams simultaneously within the ssh transport.  This works extremely well with the ssh servers I've tried, but obviously not with the server that is the subject of this ticket!

> This is a problem related to the sftp client rclone uses

I would say it is a problem with the server not accepting multiple transfers down the same ssh connection like it is supposed to.  As I said, there may be some adaption rclone should be doing though which it isn't at the moment.

> has an open issue for this in the stfp go client git. (pkg/sftp#158).

That issue is to do with speed of transfers and is related to rclone issue #1158 - it might well be that opening multiple ssh connections would fix that issue.

> I haven't looked at the code, but it might be a significant rewrite to support transfers over multiple connections, but since they have not yet closed the issue they might be open for it.

The place to do that would be in rclone I think.  rclone could have a pool of opened ssh connections it could use - it wouldn't be too tricky.
  Thank you for doing this - it is a great addition to data security for the FUSE filesystem.

> I'm not sure how to write decent tests for the error cases, i.e. when comparing the hash sum fails. The only thing I can think of would be meddle with the internal state of some of the objects. When I edit the code so that hash comparison always fails, we get the desired behaviour: input/output errors both during reads and writes.

Yes rclone is missing a test framework for that at the moment.

Perhaps an optional interface for Local objects which would corrupt a bit of data that the test could check for and use if found.

rclone should really have a set of test *Object for this purpose...

I'm going to merge this as-is and if we come up with a better test we can add it later!  Sorry for the confusion!  > I don't know if that intentional for reference that rclone could detect to find which is encrypted version.

Yes that is right.

It is important that rclone can manage to detect its own files.  There are two nulls after rclone which I will use for file versioning eventually, then a header - all of which could be detected.  The underlying format for the blocks, nacl secretbox might also have some recognisable features (I'm not sure).

Every encryption system I've seen uses some kind of header...  Thank you very much :-)  Re SIGHUP:
From [the signal docs](https://golang.org/pkg/os/signal/#hdr-Windows) I think only Interrupt is supported under Windows.  Anyway the whole of fuse is compiled out for windows at the moment.

SIGHUP usually means reload the config file so I wonder whether SIGUSR1 might be a better choice.  SIGUSR2 means kick the bandwidth limiter at the moment too.

 > Regarding reviewing non-squashed patches: I am curious, how do you review usually? Github has the "Files Changed" tab, which essentially works like a squashed view. Locally, something like git diff master... should do the trick (the three dots compare it to when it was branched off, not the current state of the master branch). Regardless, I will squash in the future.

I don't think I ever clicked on that tab before - thanks for teaching me a new trick :-)  From previous revision control systems all the way back to patches to a mailing list I've always reviewed individual diffs and I guess I just carried that forward to github!

> However, do you remember why d.items = nil was needed when renaming a directory? It is there since this feature was supported, but the test I wrote for this corner case passes either way. I also tried with a shell, which keeps the reference to the current directory without a traversal. They also behave the same, as far as I can tell.

It was there to get rid of any items in a directory.  Each rclone Object contains within it the full path in the remote.  This is great for syncing etc, but for a file system when you rename the directory you need to invalidate all those objects because they now have the wrong paths in them.  Directory listings etc will work without `d.items = nil` but if you try to read any of the data it will likely say object not found (on bucket based remotes it will certainly (eg swift/s3/b2), on drive, acd I guess it may not!).

I'll just note that I've been refactoring the underlying directory tree / nodes into [mountlib](https://github.com/ncw/rclone/tree/cmount/cmd/mountlib) (currently in the [cmount](https://github.com/ncw/rclone/tree/cmount) branch) so I can re-use it with a different fuse library ([cgofuse](https://github.com/billziss-gh/cgofuse)) which can be made to work under windows.  I'd also like to use the mountlib to serve webdav also.

Anyway at some point this means I'll need to merge what you've done into the mountlib (that isn't a problem - I've done that sort of thing lots of times before!).

I'd appreciate your feedback on mountlib if you have a moment!  At some point I'd like to make some tests for mountlib itself rather than going through FUSE but I haven't done that yet! BTW there is a golint message to fix!

```
/home/travis/gopath/src/github.com/ncw/rclone/cmd/mount/dir.go:75:1: comment on exported method Dir.ForgetAll should be of the form "ForgetAll ..."
``` That looks great thank you :-)
 I forward ported this to the cmount branch - can you give it a quick once over and see if it looks sane? add408c9edb7954d2d749981fb516de31bdcbe53

Thanks  That looks very good - thank you.  Nice - well done.

I think we need a test suite.  I was thinking of a tests directory with some index.html files from different webservers and some expected output from them.  What do you think? >  I don't like test files, i think we can utilize some variables with text inside it for caddy nginx and apache

Sounds fine to me!

> But sorry i'm bit lazy for tests, so it slowly =(

:-)  Mine is mounting ok with that version.   Not saying there isn't a problem but things seem okay here.

rclone -V
rclone v1.36-41-gb651784β
 The `--timeout` flag is an idle timeout, so if whatever you are doing might pause reading the data then you want to set the timeout bigger than that pause.

Looking at your log, those timeouts all appear to be Amazon misbehaving :-(  This appears at first glance very similar to the `-u, --update` flag.  Does that flag work for your use case?

If not can you describe how the semantics of --check-new and --update differ?

Thanks

Nick No worries, rclone has a **lot** of options!  Plex will only read data as fast as it needs to which will average at the bitrate of the video.

The problems seem to be show by the `read tcp 100.96.1.15:60754->52.1.173.13:443: i/o timeout` errors.

These are caused by `--timeout 5s` in your flags.

The timeout is an idle timeout so that means that if data isn't being fetched for 5s then close the connection and re-open.  I suggest you remove this to go back to the default of `30s`.  You did the right thing - all those other files are auto generated.  Thanks for the fix.  Thanks :-)  Nice one!  Thanks for fixing.  This should be fixed by

https://beta.rclone.org/v1.36-35-gdd968a8/ (uploaded in 15-30 mins)

  ACD doesn't support mod times.  What you get instead is the uploaded time.  So when you copied from acdUK to acdDE the mod times will have changed.

Does that explain it? acd doesn't support mod times, and rclone knows that when copying from acd to drive it shouldn't rely on the mod times, so it won't update the mod time if it is different.

However when rclone copies the file for the first time it does use the mod time as it really doesn't have anything else to go on.  You can't really until the metadata items are addressed and the directory structure can be cached:

https://github.com/ncw/rclone/issues/1337  @felixbuenemann 
> Transfers to google drive simply hang with no output from rclone, this even happens with server side copies, where rclone might show a file as checking or transferring forever.

I'm not seeing this with a very brief test.

Can you try running with `-vv --dump-bodies` (you probably tried this already though).

If no joy with that then try `strace` and see if that gives any clues.

Is it some kind of networking problem?  DNS resolution hanging? That sounds plausible...

You can adjust `/etc/gai.conf` (in linux - I expect there is an OSX equivalent) if you'd prefer to use IPv4 addresses if available.

If it happens again an IPv6 traceroute to the drive servers would tell you whether it is the problem or not.
  That isn't the tidiest code I've ever seen, but I've merged it :-)  There is an issue about that here: #497 - can you add your support to that and close this one please.  I've squashed and merged that - thank you very much.

I note that we don't need `owners` if we aren't using `--drive-auth-owner-only` and the owner seems to be quite voluminous - what do you think about sending another PR which conditonally adds `owners` to the string if that flag is in use? Re-opening this for test comments! Whoops wrong ticket!  > Is that some sort of latest "improvement" to mute the output and introduce -v?

Yes.  The aim was to make rclone behave more like rsync which is quiet unless you add `-v`.  It has caught a  lot of people out though, so apologies!  That error message didn't really say what went wrong...

Did you try `See "systemctl status var-lib-snapd-snap-core-1577.mount" and "journalctl -xe" for details.` to see if that gives anything more?

@Dedsec1 any ideas?
 i dont use Arch that often but i would suggest your running as root when installing snaps 
  I think this is mostly a duplicate of #675 - can you add your comments about LZ4 to that ticket and close this one?  What are those files called?  Do they have any funny unicode or other characters in them? 

Do the files download properly if you use the web interface? Can you post a log with `-vv` of the problem happening please?  Can you run rclone with `-vv --dump-bodies` - this wil generate a huge amount of logs!  However if you could attempt to capture the http request and the 429 response that would be very useful.  (Note the http requests have an ID you can match up with the http responses). According to the [b2 docs](https://www.backblaze.com/b2/docs/calling.html) this is what a 429 error means

> 429 TOO MANY REQUESTS - B2 may limit API requests on a per-account basis.

So what is probably happening is that you are blowing your quota of requests.

Is this a free B2 account?  They have a fixed quota of requests.

This might be related to #1277 in which case you should find the temporary work-around there works. I wonder if backblaze have put some new API limiting in place... ...However the `--old-sync-method` flag would probably fix it for you.  I'd be happy to receive a PR for this if you wanted to help?  When filing an issue, please include the following information if possible as well as a description of the problem.  Make sure you test with the latest beta of rclone.

    https://beta.rclone.org/
    https://rclone.org/downloads/

If you've just got a question or aren't sure if you've found a bug then please use the [rclone forum](https://forum.rclone.org/) instead of filing an issue.

> What is your rclone version (eg output from `rclone -V`)
v1.36
> Which OS you are using and how many bits (eg Windows 7, 64 bit)
UBUNTU 16.04
> Which cloud storage system are you using? (eg Google Drive)
Amazon Drive
> The command you were trying to run (eg `rclone copy /tmp remote:tmp`)
I use "/home/plex/library/scripts/rclone mount secret: /home/plex/library/.acd &" to mount the amazon drive. secret: is tied to a folder within my amazon drive
> A log from the command with the `-vv` flag (eg output from `rclone -vv copy /tmp remote:tmp`)
plex@ZZeus:~/library/scripts$ /home/plex/library/scripts/rclone mount secret: /home/plex/library/.acd -vv
2017/04/18 22:33:02 DEBUG : rclone: Version "v1.36" starting with parameters ["/home/plex/library/scripts/rclone" "mount" "secret:" "/home/plex/library/.acd" "-vv"]
2017/04/18 22:33:03 DEBUG : pacer: Rate limited, sleeping for 287.113937ms (1 consecutive low level retries)
2017/04/18 22:33:03 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:04 DEBUG : pacer: Rate limited, sleeping for 549.16732ms (2 consecutive low level retries)
2017/04/18 22:33:04 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:04 DEBUG : pacer: Rate limited, sleeping for 3.632969758s (3 consecutive low level retries)
2017/04/18 22:33:04 DEBUG : pacer: low level retry 3/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:04 DEBUG : pacer: Rate limited, sleeping for 5.331776148s (4 consecutive low level retries)
2017/04/18 22:33:04 DEBUG : pacer: low level retry 4/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:08 DEBUG : pacer: Rate limited, sleeping for 13.183117216s (5 consecutive low level retries)
2017/04/18 22:33:08 DEBUG : pacer: low level retry 5/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:13 DEBUG : pacer: Rate limited, sleeping for 24.480279449s (6 consecutive low level retries)
2017/04/18 22:33:13 DEBUG : pacer: low level retry 6/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
^C
plex@ZZeus:~/library/scripts$ sudo vi mount.sh
plex@ZZeus:~/library/scripts$ /bin/fusermount -uz /home/plex/library/.acd
plex@ZZeus:~/library/scripts$ /bin/fusermount -uz /home/plex/library/media
plex@ZZeus:~/library/scripts$ /home/plex/library/scripts/rclone mount secret: /home/plex/library/.acd -vv
2017/04/18 22:33:40 DEBUG : rclone: Version "v1.36" starting with parameters ["/home/plex/library/scripts/rclone" "mount" "secret:" "/home/plex/library/.acd" "-vv"]
2017/04/18 22:33:41 DEBUG : pacer: Rate limited, sleeping for 666.145821ms (1 consecutive low level retries)
2017/04/18 22:33:41 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:41 DEBUG : pacer: Rate limited, sleeping for 235.010051ms (2 consecutive low level retries)
2017/04/18 22:33:41 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:41 DEBUG : pacer: Rate limited, sleeping for 287.113937ms (3 consecutive low level retries)
2017/04/18 22:33:41 DEBUG : pacer: low level retry 3/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:42 DEBUG : pacer: Rate limited, sleeping for 4.54916732s (4 consecutive low level retries)
2017/04/18 22:33:42 DEBUG : pacer: low level retry 4/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:42 DEBUG : pacer: Rate limited, sleeping for 7.632969758s (5 consecutive low level retries)
2017/04/18 22:33:42 DEBUG : pacer: low level retry 5/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:46 DEBUG : pacer: Rate limited, sleeping for 21.331776148s (6 consecutive low level retries)
2017/04/18 22:33:46 DEBUG : pacer: low level retry 6/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:33:54 DEBUG : pacer: Resetting sleep to minimum 20ms on success
2017/04/18 22:34:16 DEBUG : pacer: Rate limited, sleeping for 760.398084ms (1 consecutive low level retries)
2017/04/18 22:34:16 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:16 DEBUG : pacer: Rate limited, sleeping for 263.669287ms (2 consecutive low level retries)
2017/04/18 22:34:16 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:17 DEBUG : pacer: Resetting sleep to minimum 20ms on success
2017/04/18 22:34:17 DEBUG : pacer: Rate limited, sleeping for 884.491574ms (1 consecutive low level retries)
2017/04/18 22:34:17 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:17 DEBUG : pacer: Rate limited, sleeping for 1.414458836s (2 consecutive low level retries)
2017/04/18 22:34:17 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:18 DEBUG : pacer: Rate limited, sleeping for 3.211445515s (3 consecutive low level retries)
2017/04/18 22:34:18 DEBUG : pacer: low level retry 3/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:19 DEBUG : pacer: Resetting sleep to minimum 20ms on success
2017/04/18 22:34:19 DEBUG : pacer: Rate limited, sleeping for 838.182873ms (1 consecutive low level retries)
2017/04/18 22:34:19 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:19 DEBUG : pacer: Rate limited, sleeping for 1.472644968s (2 consecutive low level retries)
2017/04/18 22:34:19 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:20 DEBUG : pacer: Resetting sleep to minimum 20ms on success
2017/04/18 22:34:22 DEBUG : pacer: Rate limited, sleeping for 910.584091ms (1 consecutive low level retries)
2017/04/18 22:34:22 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:22 DEBUG : pacer: Rate limited, sleeping for 539.11079ms (2 consecutive low level retries)
2017/04/18 22:34:22 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:23 DEBUG : pacer: Resetting sleep to minimum 20ms on success
2017/04/18 22:34:23 INFO  : Encrypted amazon drive root 'ZZeus': Modify window not supported
2017/04/18 22:34:23 DEBUG : Encrypted amazon drive root 'ZZeus': Mounting on "/home/plex/library/.acd"
2017/04/18 22:34:23 DEBUG : Encrypted amazon drive root 'ZZeus': Root()
2017/04/18 22:34:24 DEBUG : : Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:24 DEBUG : : Dir.ReadDirAll
2017/04/18 22:34:24 DEBUG : : Reading directory
2017/04/18 22:34:24 DEBUG : amazon drive root 'ZZeus': Reading ""
2017/04/18 22:34:25 DEBUG : amazon drive root 'ZZeus': Finished reading ""
2017/04/18 22:34:25 DEBUG : : Dir.ReadDirAll OK with 5 entries
2017/04/18 22:34:25 DEBUG : Tv Shows: Dir.Lookup
2017/04/18 22:34:25 DEBUG : Tv Shows: Dir.Lookup OK
2017/04/18 22:34:25 DEBUG : Tv Shows: Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:25 DEBUG : Movies: Dir.Lookup
2017/04/18 22:34:25 DEBUG : Movies: Dir.Lookup OK
2017/04/18 22:34:25 DEBUG : Movies: Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:25 DEBUG : Sports: Dir.Lookup
2017/04/18 22:34:25 DEBUG : Sports: Dir.Lookup OK
2017/04/18 22:34:25 DEBUG : Sports: Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:25 DEBUG : Stand Ups: Dir.Lookup
2017/04/18 22:34:25 DEBUG : Stand Ups: Dir.Lookup OK
2017/04/18 22:34:25 DEBUG : Stand Ups: Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:37 DEBUG : : Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:37 DEBUG : : Dir.ReadDirAll
2017/04/18 22:34:37 DEBUG : : Dir.ReadDirAll OK with 5 entries
2017/04/18 22:34:37 DEBUG : : Dir.Attr valid=1m0s ino=0 size=0 mode=drwxrwxr-x
2017/04/18 22:34:39 DEBUG : Movies: Dir.ReadDirAll
2017/04/18 22:34:39 DEBUG : Movies: Reading directory
2017/04/18 22:34:39 DEBUG : pacer: Rate limited, sleeping for 853.353331ms (1 consecutive low level retries)
2017/04/18 22:34:39 DEBUG : pacer: low level retry 1/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:39 DEBUG : pacer: Rate limited, sleeping for 324.778273ms (2 consecutive low level retries)
2017/04/18 22:34:39 DEBUG : pacer: low level retry 2/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:40 DEBUG : pacer: Rate limited, sleeping for 3.138149956s (3 consecutive low level retries)
2017/04/18 22:34:40 DEBUG : pacer: low level retry 3/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:40 DEBUG : pacer: Rate limited, sleeping for 693.774911ms (4 consecutive low level retries)
2017/04/18 22:34:40 DEBUG : pacer: low level retry 4/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:44 DEBUG : pacer: Rate limited, sleeping for 14.183515637s (5 consecutive low level retries)
2017/04/18 22:34:44 DEBUG : pacer: low level retry 5/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:44 DEBUG : pacer: Rate limited, sleeping for 30.821866378s (6 consecutive low level retries)
2017/04/18 22:34:44 DEBUG : pacer: low level retry 6/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:34:58 DEBUG : pacer: Rate limited, sleeping for 33.342231109s (7 consecutive low level retries)
2017/04/18 22:34:58 DEBUG : pacer: low level retry 7/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")
2017/04/18 22:35:29 DEBUG : pacer: Rate limited, sleeping for 35.797652072s (8 consecutive low level retries)
2017/04/18 22:35:29 DEBUG : pacer: low level retry 8/10 (error HTTP code 429: "429 Too Many Requests": response body: "{\"message\":\"Rate exceeded\"}")








My issue is that after mounting, I can see the folders inside, I try to go inside one of those folders and run an ls though and that's where these last low level retries came from. I'm curious if this is an amazon issue or an rclone issue, and what the cause of this could be and what a fix for this issue might be. Also, after mounting rclone if trying to play any media through it I get these same errors.  I've seen this happening the past couple days when watching certain shows, although it's only temporary and will just cause the plex to stop playing for a minute or 2.  Now it's permanent. Thanks for the help! It's happening on anything I can test with via the API. 

acd_cli
ARQ

also generate the same errors.

Same post here too-> https://www.reddit.com/r/DataHoarder/comments/664hbt/acd_upload_rate_limited_even_though_i_havent_used/ As I noted above, it isn't just rclone, it's all API at this point.  I contacted amazon, the devs came back and told me they want me to send them my logs from my amazon drive web gui... not sure if I want to do that. They should know if something changed in their platform and give me a proper answer. @francesco2013  - Where did you see that ? @francesco2013  - so no facts, just your opinion? @francesco2013 - lots of posts also in regards to a glitch with the API. 

Fact: Expandrive still works as a 3rd party app btw. I'm sure there are more but that's one that I personally confirmed as I've got that mounted on Windows box now copying over to my GDrive.

 Arq is also a commercial app. It's really all guessing until something is announced or released officially from amazon. My point was that commercial or non-commercial, you have some 3rd party apps that are working. @francesco2013  - fyi, working fine for me now via rclone.  This is fixed, no point in keeping open, was clearly not an issue with rclone anyhow I closed this, how is this being commented on?  @hotfloppy 
> it looks like when i copy the same folder to external drive (/media/gdrive), it failed.
> but, if i copy to local disk (/tmp), it worked just fine.

How is your external drive formatted?  I suspect it is formatted for windows (vfat).  Windows file systems can't have `:` in the file names so I suspect that is the problem.

If you were to do the copy on windows it will substitute the `:` for `_`, but linux doesn't do that substitution as it (wrongly) expects all file systems to be able to write a `:` in the name.

You could rename the files on drive, or copy them to /tmp and rename them before copying them to /media/gdrive. @ErAzOr2k 
> Don't forget a slash after :
> wrong: gdrive:Documents
> correct: gdrive:/Documents

Actually those two are identical - none of the remotes except for `local` and `sftp` care about leading `/`.
  How does that actually work?  What type of file is created and what is in it?  I suspect this intercepted by the google drive client somehow.  Use -v flag to show some more progress.

Or when I've done #1180 you can just use --stats. I've fixed this by adding a new flag, use `--stats-log-level NOTICE` to see the stats without using `-v`

https://beta.rclone.org/v1.36-219-gaa204864/ (uploaded in 15-30 mins)  subprocess.call ? There isn't a python library for rclone I'm afraid.  So `os.system` or better as @calisro suggested use the subprocess module.  I'd probably use `subprocess.check_call` in fact (but hey, I contributed that to python a long time ago ;-)  I received this from the google drive team

> We have increased the queries per second to your project ID, you will be able to see the increase shortly. However, it is strongly recommended that you use the fields parameter so that our backends don't have the load of providing information that is not needed. Our documentation is at https://developers.google.com/drive/v3/web/performance#partial-response.

The Go API has `Fields()` calls which can be given a list of strings.  These need to be the strings as used in the JSON api as listed in the v2 docs.

It should be a straightforward exercise to add calls to `Fields()` in the drive API and run through the integration tests to make sure nothing is broken. 

The most important place to add this is in the listing routine, and the reading metadata.

I know that some of the other APIs (OneDrive springs to mind - there may be others) can also do this.
 Sounds great mate I have merged @azgul patch in - thank you :-)

https://beta.rclone.org/v1.36-33-g7d9faff/ (uploaded in 15-30 mins)

I tried both sorts of integration tests and it worked fine for me - it could do with some real world testing too and some performance testing.

I did a rough before and after test like this

```
$ rclone ls drive:1000files -vv --dump-bodies 2>z-after
$ rclone-v1.36 ls drive:1000files -vv --dump-bodies 2>z-before
$ grep -v DEBUG z-before  | wc
 100991  187367 3719012
$ grep -v DEBUG z-after  | wc
  37967   70302 1227481
```

So the bytes used was reduced from 3.5M to 1.2M which is a reduction to 33% of the original size.
 Next update from @azgul - don't ask for owner info if not needed

https://beta.rclone.org/v1.36-34-g0d6e1af/ (uploaded in 15-30 mins) @dfunkt That is reproducible before and after that commit?

What would be most useful is a dump with `-vv --dump-bodies` of your `lsd` command before and after.  You can email that to me if you want subject "rclone issue 1346" if you want as it will have lots of personal info in. Here is the fix for the missing files, and it should fix your size problem too @ajkis 

https://beta.rclone.org/v1.36-35-gdd968a8/ (uploaded in 15-30 mins)

Thanks @azgul  @felixbuenemann This probably merits its own issue.

> It looks to me like rclone is not trying to adhere to the rate limiting by throttling request to a certain rate, but instead works as fast as it can until it gets rate limited and then uses the pacer for exponential back-off, however that doesn't seem to work well with google drive's rate limiting.

rclone will limit requests to 100/s normally with drive.  If it gets a 403 then it does exponential back-off.  As far as I know this is the correct way of working with drive - why doesn't it work well for you?

You can experiment with changing the `minSleep` parameter in `drive/drive.go` and see what difference that makes.

Are you using your own credentials or rclones?
 The fields parameter is now working properly, so I'm going to close this issue now.  A check with `passwd` on linux indicates it writes its prompt to stderr, unless I did something stupid.

```
$ passwd USER
Changing password for USER.
(current) UNIX password: 
passwd: Authentication token manipulation error
passwd: password unchanged

$ passwd USER >/dev/null
(current) UNIX password: 
passwd: Authentication token manipulation error
passwd: password unchanged

$ passwd USER 2>/dev/null
Changing password for USER.

$ 
```

The [library rclone uses for reading the terminal prompt](https://godoc.org/golang.org/x/crypto/ssh/terminal#ReadPassword) reads from stdin at the moment.  That could easily be `/dev/tty` instead.  However I'm really struggling to work out how this could be done in a cross platform way!

There is already quite a good work-around for having to enter the password for rclone commands - set the environment variable.  I have this in a little shell script

```shell
#!/bin/echo Source this file don't run it

read -s RCLONE_CONFIG_PASS
export RCLONE_CONFIG_PASS
```

I'm verging on thinking that this PR is a net improvement.  I think using `/dev/tty` is probably too difficult to do cross platform, and is bound to cause problems to existing users.

Any futher thoughts @sweharris ? I've merged this - thank you for your contribution :-)  >  seems like Google now enforce a data transfer quota for server side copy or something has changed in the way server side copy is implemented

That would seem the likely conclusion.  Are you using your own credentials or rclone's?  Thank you for the fix :-)  I think it might be worth exploring what this DB would be used for exactly.

I'd like something which would help with
  * #949 - local caching of hashes
  * #897 - cache a remote hierarchy so it could them be kept up to date with the changes API

My original thinking was to cache serialized versions of the `Object` keyed on path.  This would mean adding some new methods to the `Fs` to serialize and unserialize them.  This is essentially "caching the remote file id" as you mention above, just in a more generic fashion.

Then when rclone does a list operation, it can query the db instead of the remote as required, unserializing objects as it goes along.

[ I also have a plan to remove the root of the Fs, and have this be handled by a higher layer.  This would help with this - the `remote:path` stuff will need care with the `path`. ]

The DB design above has quite a few more things in - what issues are they relevant to?  I can't replicate this on Ubuntu 16.10.

When I CTRL-C the terminal is reset properly.

Which terminal are you using?

Do you have to use sudo to see the problem?  That is a fault of the parser which parses the remote name off the front....

You can work around this by making a remote of type "local" then using that.

I should probably improve the parse so it recognises backslashes or something like that, or maybe make it so it doesn't parse a /, so you could then write ./test:directory
 Actually I just noticed that the ./ workaround works right now, so 

    ./rclone -vv copy ./test:directory "acd-data:.tmp/test:directory/"

should work fine I've documented the workaround in the referenced commit.  This looks good.  Will merge in a bit!  (Sorry have been a bit overloaded after my return from vacation!) I've merged this - thank you very much :-)  There is an issue for this here #254

Those two links are very useful - especially the step by step in the second link of how to make a onedrive for business app work.

Can you add them to that issue please?  I'm afraid that getting rclone to ask about individual files doesn't fit within the architecture of rclone.

However if you are worried about data loss, use the `--backup-dir` option which will save any files that get overwritten into a different directory.  You've set a buffer size of 128M - how many open files did you have?  Each open file will use 128M of memory. > When you say every open file will take 128MB of memory, does this also apply to opened files which have a size <128MB?

No the buffer can only be the max size of the file, rounded up a bit.

> When I'm right setting transfers to 20 should limit the count of open files to 20? So there's a max of 20*128MB which could be used by rclone?

If you were using rclone sync/copy/move then that would be correct, but it isn't under rclone's control using rclone mount how many open files there are.  I think @yonjah is probably correct - try checking the actual internet usage with a different tool No probs!  I saw a few similar issues in regards to lookup failures and open files, but I don't think this is related back to a ulimit with the number of files open as I have a big number on my system:

```
felix@plex: ~$ ulimit -n
65536
```

It seems that if ACD is unavailable due to whatever reason, in this case it looks like it went offline and wasn't returning, the mount went into a loop and just opened up quite a number of sockets until if finally hit my ulimit and errored out. 

Does that make sense?

> What is your rclone version (eg output from `rclone -V`)
felix@plex: ~$ rclone -V
rclone v1.36-20-g2fd86c9β
> Which OS you are using and how many bits (eg Windows 7, 64 bit)
root@plex:~# lsb_release -a
No LSB modules are available.
Distributor ID:	Debian
Description:	Debian GNU/Linux 8.7 (jessie)
Release:	8.7
Codename:	jessie
> Which cloud storage system are you using? (eg Google Drive)
ACD
> The command you were trying to run (eg `rclone copy /tmp remote:tmp`)
rclone mount
> A log from the command with the `-vv` flag (eg output from `rclone -vv copy /tmp remote:tmp`)
```
Apr 12 00:25:40 plex rclone[17911]: Movies/This.Is.the.End.(2013)/This.Is.The.End.1080p.mp4: File.Open failed: open for read: Get https://co
ntent-na.drive.amazonaws.com/cdproxy/templink/WkqL9DhvS2mvcH8DvFbSdxqqDsVy0TJq7w3_VvoSUo038XJVg: dial tcp: lookup content-na.drive.amazonaws
.com on 192.168.86.1:53: no such host
Apr 12 00:25:41 plex rclone[17911]: Movies/This.Is.the.End.(2013)/This.Is.The.End.1080p.mp4: File.Open failed: open for read: Get https://co
ntent-na.drive.amazonaws.com/cdproxy/templink/WkqL9DhvS2mvcH8DvFbSdxqqDsVy0TJq7w3_VvoSUo038XJVg: dial tcp: lookup content-na.drive.amazonaws
.com on 192.168.86.1:53: no such host
Apr 12 00:25:41 plex rclone[17911]: Movies/Underworld.Rise.of.the.Lycans.(2009)/Underworld.Rise.Of.The.Lycans.1080p.mp4: File.Open failed: o
pen for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/wkTScp5h8N9nTXDXPTky_-vJOZH-e2IYjWZB8hMV-bw38XJVg: dial tcp: looku
p content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 12 00:25:41 plex rclone[17911]: Movies/Underworld.Rise.of.the.Lycans.(2009)/Underworld.Rise.Of.The.Lycans.1080p.mp4: File.Open failed: o
pen for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/wkTScp5h8N9nTXDXPTky_-vJOZH-e2IYjWZB8hMV-bw38XJVg: dial tcp: looku
p content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 12 00:25:41 plex rclone[17911]: Movies/Saw.II.(2005)/Saw.II.1080p.mkv: File.Open failed: open for read: Get https://cd-na-prod-content.s3.amazonaws.com/IG6HV0CTQQKP57XT5PYDCA8ZZDFF05768DCMAYRDI96NKMNK89?response-content-disposition=attachment%3B%20filename%3D%22q59076l1arbo5sbn9lq0ob87ljgml0sq03b2609fjupjrlvqrie0%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=FQoDYXdzEJ3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDB9cphFCzuI8PZQPPCK3AzVLWMyYBjWaFlJcLUZxGPzqOjZ2L6oZU1bHvYfJ73Gj2DHzA27kkPp%2FHy%2B%2FNMvl%2BPMBrqMeN7tPvKuqMhk%2BUj%2F4xv7HNlP2PTl7YMwNRbkpiHgbG1Po4DLjNFJirnbCFzK%2BD2AZOSymOjNbLKd9jDPiiNFct6xEnceF%2F7RVkBKbCUKYo2Kmv51rs38CvxhdRfWGjQAtk2jjl5rAAZdFmEQqoW5dJWPWDJFB3OC6vtH3XpxFuE3jIqCmL52o4SNgjx6Dc4%2BUE2G6k21F4XzJoHmm%2B793zhOHMFZ3JvABrnvwZLwDyNf9xiJuFpVJ%2BO4dxbyilLYjzvTIP%2Bes4FW0HuH1gdU7bRA5bYZp1kbG9i1JZLTXNh3WPumSul5dUaa%2FpD7aEUr9yBU%2BI3WALbAZllrxosrmDyEL2OTsijSmMeJpvZa%2FPJKjh4%2Bjo3hDz%2B029V2JvPulPPPEtAzpLqBBMXG%2FYcsIiPX3QI1RGM7N4fDnUL0acFssDnvb1Wq%2B16G1RZZGIogZE7el1lm0eLjmf2S58Rgig%2Fwa07cCbJ7PsngBXdElz%2BT0SyQI%2F99BXEL5vebt6uukcRoogsG2xwU%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170412T042537Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAJ6ICQSCRAQ7KPRLA%2F20170412%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=1f5c49753a1708d54b5a23812867f342764107f46222da0c657e0febe6d026a4: dial tcp: lookup cd-na-prod-content.s3.amazonaws.com on 192.168.86.1:53: dial udp 192.168.86.1:53: socket: too many open files
```
 Hmm. Odd. Had something similar this morning just before I woke up and saw this:

```
XS5Q3dWKADZKkTlrMz5gsJGXM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 13 06:13:47 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E07.Bluray-720p.mkv: ReadFileHandle.Read error: low level retry 10/10: Get https://content-na.drive.amazonaws.com/cdproxy/templink/EEiW1EqpKw91ygYuhYXS5Q3dWKADZKkTlrMz5gsJGXM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 13 06:13:47 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E07.Bluray-720p.mkv: ReadFileHandle.Read error: Get https://content-na.drive.amazonaws.com/cdproxy/templink/EEiW1EqpKw91ygYuhYXS5Q3dWKADZKkTlrMz5gsJGXM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 13 06:13:47 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E07.Bluray-720p.mkv: ReadFileHandle.Read error: low level retry 1/10: file already closed
Apr 13 06:13:47 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E07.Bluray-720p.mkv: ReadFileHandle.Read error: low level retry 2/10: Get https://content-na.drive.amazonaws.com/cdproxy/templink/EEiW1EqpKw91ygYuhYXS5Q3dWKADZKkTlrMz5gsJGXM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 13 06:13:48 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E07.Bluray-720p.mkv: ReadFileHandle.Read error: low level retry 3/10: Get https://content-na.drive.amazonaws.com/cdproxy/templink/EEiW1EqpKw91ygYuhYXS5Q3dWKADZKkTlrMz5gsJGXM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 13 06:13:48 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E07.Bluray-720p.mkv: ReadFileHandle.Read error: low level retry 4/10: Get https://content-na.drive.amazonaws.com/cdproxy/templink/EEiW1EqpKw91ygYuhYXS5Q3dWKADZKkTlrMz5gsJGXM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host
Apr 13 06:14:01 plex rclone[6096]: TV_Ended/Torchwood/Torchwood.S01E08.Bluray-720p.mkv: File.Open failed: open for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/wqaAjz0Fo3wxzXENDShgvbog93tNBYhDNSFu8pU3OsM38XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: dial udp 192.168.86.1:53: socket: too many open files
```

I've been finishing up a pretty lengthy plex scan as it is done now so it seems I had a blip in internet access, which caused the too many open files.

I logged in and checked and really saw very limited number of TIME_WAITs and no where near 65k files open. Ended up just bouncing the system to bring it back. `dial tcp: lookup content-na.drive.amazonaws.com on 192.168.86.1:53: no such host` is caused by your router `192.168.86.1`  failing to resolve that name into an IP address.

I'm guessing that these piled up until you ran out of sockets `dial udp 192.168.86.1:53: socket: too many open files`

Which machine is `192.168.86.1`?  Is it the same machine?
 My router that is what servers my DNS so that's what I'm guessing is the 53s for the DNS lookups. Just to clean things up, I decided to remove the router DNS from my linux box and just pointed to Google DNS for now as that should be fine. I'll see if the issue crops back up but I do have DNS monitoring on my router and didn't see any issues. My router DNS forwards to OpenDNS so I'm feel pretty confident in saying DNS didn't have an issue as I have multiple monitoring points for that.

I'll let it run again for a few days and see if the issue pops back up. Anything you'd recommend I try to capture? Hmm.

Same issue even with Google DNS:

```
3: i/o timeout
Apr 18 11:17:59 plex rclone[4330]: Movies/LEGO.Movie,.The.(2014)/The.LEGO.Movie.1080p.mp4: File.Open failed: open for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/PSgiQj5w96XAIm8rxmFH6BBcohd5KhoV6I1qcTuuWE838XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 8.8.4.4:53: no such host
Apr 18 11:17:59 plex rclone[4330]: Movies/LEGO.-.Star.Wars.The.Padawan.Menace.(2011)/LEGO.Star.Wars.The.Padawan.Menace.720p.mp4: File.Open failed: open for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/hxMzkU3UASSoDlvU7HnH9LqORoKBCK8YFmcHlF_XUY438XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 8.8.4.4:53: no such host
Apr 18 11:17:59 plex rclone[4330]: Movies/LEGO.-.Star.Wars.The.Padawan.Menace.(2011)/LEGO.Star.Wars.The.Padawan.Menace.720p.mp4: File.Open failed: open for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/hxMzkU3UASSoDlvU7HnH9LqORoKBCK8YFmcHlF_XUY438XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 8.8.4.4:53: no such host
Apr 18 11:17:59 plex rclone[4330]: Movies/LEGO.-.Star.Wars.The.Padawan.Menace.(2011)/LEGO.Star.Wars.The.Padawan.Menace.720p.mp4: File.Open failed: open for read: Get https://content-na.drive.amazonaws.com/cdproxy/templink/hxMzkU3UASSoDlvU7HnH9LqORoKBCK8YFmcHlF_XUY438XJVg: dial tcp: lookup content-na.drive.amazonaws.com on 8.8.4.4:53: no such host
Apr 18 11:17:59 plex rclone[4330]: Movies/Les.Misérables.(2012)/Les.Misérables.1080p.mp4: File.Open failed: open for read: Get https://cd-na-prod-content.s3.amazonaws.com/UC87DJ0J7YJROHPUGDGCAPYEFCMO5TGOWJKZ31RY3H6GL7NV1S?response-content-disposition=attachment%3B%20filename%3D%22o831altalaaoahbp3u9eqjdo1m2seihsvti158gf2219se0i6nv0%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=FQoDYXdzEEcaDH38G8kYZARjZZbKXCK3A6bnAAGqFwN2tnrspVlQrv2gkS1hojUlXniTmZkzFItaNwEo0VD6GefqD%2BcH0gtyISUw6JnivC99mq3ZqjH%2FaIFTPTdQSZzP4RhlfCocDZSMJ5PaO15fVamzVW5HW8jS6XLZW5BZvUjn%2B3%2F3WN9uQCxFVPcPLyNsxWTu%2F%2FPEXB0X8HtocfmJ3jp5R3l5vAN0rwqDW85i9dFPbBv194%2BBx1QIHiKpAVmUMrRBjGmCJwhnh95PnxFxaO%2FKbilhJfcfLnjtSdPEuK8%2BFZImu3Z1NSopaYqKZPr5ub2kKNCSVBm3vBHLO9%2FfaMunC4YTZA6zOp2VmVy%2Fg1WMZpTxDG1UdsDhq4iIei9AUcuSX61z3IXdDea5%2F1wrJl%2FoMxQaRMH3%2BNuxCTPFtbjDzOzq3j0r0eloBmonTmoV7MiKUILoAO4e0%2BY6YWYthJ7zfiw%2FL5drSA4VwsTDMFslJtbsWQebIPgITNG6S%2BYkEfA1BxgSCQeK%2BVgjwoBOeBmpdvSFFEvsu8PZt5Ep0XDN9y8EXEyLYUPlHPT81EtOcxVYmaUItycFUa747xxTnlLvq8spjObmH9Est9weGRYooLjYxwU%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170418T151755Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAI4LJHUBZE26X6R5Q%2F20170418%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=432a5ecee01de357256fe1b94cfdbeceb58a7826ced3c4b0bcc5e2cd310eb099: dial tcp: lookup cd-na-prod-content.s3.amazonaws.com on 8.8.8.8:53: dial udp 8.8.8.8:53: socket: too many open files
``` Have you tried using `lsof` to see which processes have sockets open?  It looks like to me that something has a lot of sockets open somewhere! I'm pretty sure it's rclone spawning them off as that's the only thing that's impacted. I'll have to wait to see if the error occurs again and I can check the rclone process before I terminate it.

I hardly get any TCP sockets open as well. Last hour:

http://imgur.com/VPejp6K

 I use netdata and capture sockets at least for netdata.

I can see the sockets climb until I killed it.

http://imgur.com/fkccUFZ I'm trying to figure out what these "sock" connections are as well:

```
rclone  21578 felix  248u  sock                0,7      0t0  748129 can't identify protocol
rclone  21578 felix  249u  IPv4             757075      0t0     TCP 192.168.86.30:33842->ec2-52-45-43-200.compute-1.amazonaws.com:https (ESTABLISHED)
rclone  21578 felix  251u  sock                0,7      0t0  746813 can't identify protocol
rclone  21578 felix  252u  sock                0,7      0t0  745984 can't identify protocol
rclone  21578 felix  253u  IPv4             756133      0t0     TCP 192.168.86.30:59725->ec2-52-86-107-164.compute-1.amazonaws.com:https (ESTABLISHED)
rclone  21578 felix  255u  IPv4             758000      0t0     TCP 192.168.86.30:45786->ec2-54-164-62-76.compute-1.amazonaws.com:https (ESTABLISHED)
root@plex:~# lsof -p 21578 | grep sock
rclone  21578 felix    1u  unix 0xffff88040c686080      0t0  590458 socket
rclone  21578 felix    2u  unix 0xffff88040c686080      0t0  590458 socket
rclone  21578 felix    3u  unix 0xffff88003645f0c0      0t0  584389 socket
rclone  21578 felix    5u  sock                0,7      0t0  661909 can't identify protocol
rclone  21578 felix    7u  sock                0,7      0t0  664757 can't identify protocol
rclone  21578 felix    8u  sock                0,7      0t0  660110 can't identify protocol
```

I can see the IPV4 stuff to my ACD drive, but not sure what those other 247 sockets are:

```
root@plex:~# lsof -p 21578 | grep sock | wc -l
247
``` Can you post the output of `lsof -c rclone` please?

Try the same thing for plex also?

I suspect plex has zillions of files open - for each of these rclone will have a socket open. I'll attach them as 2 files. Plex really doesn't have a lot of open files. Even on a library scan it just checks file size/stamps and I may have an analyze going on now but that works one at a time on a file.

My rclone user is "felix"
My plex user is "plex"





[lsof-plex.txt](https://github.com/ncw/rclone/files/930431/lsof-plex.txt)
[lsof-rclone.txt](https://github.com/ncw/rclone/files/930432/lsof-rclone.txt)
 And I also did downgrade to 1.3.6 as well. I'm guessing somewhere along the way that a socket isn't being closed. Here is a 30 day snapshot and you can see the ramp up.

http://i.imgur.com/loXSKcS.png Can you use lsof to see if there are any files open on the mount? [hints](https://unix.stackexchange.com/questions/3109/how-do-i-find-out-which-processes-are-preventing-unmounting-of-a-device)

That lsof-rclone.txt is puzzling - why all the "can't identify protocol" sockets? Maybe that is the fuse protocol...

> I'm guessing somewhere along the way that a socket isn't being closed. Here is a 30 day snapshot and you can see the ramp up.

Suspicious...  Note that it could be a file that is open on the rclone mount which isn't being closed, rather than rclone not closing something... My ACD drive is acting up and I can't get a mount going currently on my home Linux box nor my mac so I'll have to test a little later. I had some rate exceeded messages so I think I need to let it sit a bit :) I'm still getting 429s from everywhere. I'll try later tonight I guess. I had this happen a few times yesterday again and I'm still seeing a ton of connects via the lsof and in this case, only 1 file open that plex is analyzing.

```
root@plex:~# lsof /media
COMMAND  PID USER   FD   TYPE DEVICE   SIZE/OFF                 NODE NAME
Plex    8212 plex   49r   REG   0,38 7622372074 15236750637832222691 /media/Movies/Veronica.Mars.(2014)/Veronica.Mars.1080p.mp4
root@plex:~# lsof -c rclone
COMMAND  PID  USER   FD   TYPE             DEVICE SIZE/OFF    NODE NAME
rclone  6643 felix  cwd    DIR                8,1     4096       2 /
rclone  6643 felix  rtd    DIR                8,1     4096       2 /
rclone  6643 felix  txt    REG                8,1 14671328 3951564 /usr/bin/rclone
rclone  6643 felix    0r   CHR                1,3      0t0    1028 /dev/null
rclone  6643 felix    1u  unix 0xffff88040b4d7400      0t0   35183 socket
rclone  6643 felix    2u  unix 0xffff88040b4d7400      0t0   35183 socket
rclone  6643 felix    3u  unix 0xffff880037314bc0      0t0   34087 socket
rclone  6643 felix    4u  0000                0,9        0    7666 anon_inode
rclone  6643 felix    5u  sock                0,7      0t0   48411 can't identify protocol
rclone  6643 felix    6r   CHR                1,9      0t0    1033 /dev/urandom
rclone  6643 felix    7u  sock                0,7      0t0   42884 can't identify protocol
rclone  6643 felix    8u  sock                0,7      0t0   43626 can't identify protocol
rclone  6643 felix    9u  sock                0,7      0t0   42922 can't identify protocol
rclone  6643 felix   10u  sock                0,7      0t0   43995 can't identify protocol
rclone  6643 felix   11u  sock                0,7      0t0   44925 can't identify protocol
rclone  6643 felix   12u   CHR             10,229      0t0   12506 /dev/fuse
rclone  6643 felix   13u  sock                0,7      0t0   46753 can't identify protocol
rclone  6643 felix   14u  sock                0,7      0t0   49246 can't identify protocol
rclone  6643 felix   15u  sock                0,7      0t0   46004 can't identify protocol
rclone  6643 felix   16u  sock                0,7      0t0   46839 can't identify protocol
rclone  6643 felix   17u  sock                0,7      0t0   49261 can't identify protocol
rclone  6643 felix   18u  sock                0,7      0t0   49311 can't identify protocol
rclone  6643 felix   19u  sock                0,7      0t0   41954 can't identify protocol
rclone  6643 felix   20u  sock                0,7      0t0   46864 can't identify protocol
rclone  6643 felix   21u  sock                0,7      0t0   47537 can't identify protocol
rclone  6643 felix   22u  sock                0,7      0t0   45814 can't identify protocol
rclone  6643 felix   23u  sock                0,7      0t0   45840 can't identify protocol
rclone  6643 felix   24u  sock                0,7      0t0   41914 can't identify protocol
rclone  6643 felix   25u  sock                0,7      0t0   50234 can't identify protocol
rclone  6643 felix   26u  sock                0,7      0t0   45867 can't identify protocol
rclone  6643 felix   27u  sock                0,7      0t0   46927 can't identify protocol
rclone  6643 felix   28u  sock                0,7      0t0   48556 can't identify protocol
rclone  6643 felix   29u  sock                0,7      0t0   48359 can't identify protocol
rclone  6643 felix   30u  sock                0,7      0t0   48074 can't identify protocol
rclone  6643 felix   31u  sock                0,7      0t0   51362 can't identify protocol
rclone  6643 felix   32u  sock                0,7      0t0   47748 can't identify protocol
rclone  6643 felix   33u  sock                0,7      0t0   50660 can't identify protocol
rclone  6643 felix   34u  sock                0,7      0t0   49461 can't identify protocol
rclone  6643 felix   35u  sock                0,7      0t0   57855 can't identify protocol
rclone  6643 felix   36u  sock                0,7      0t0   52353 can't identify protocol
rclone  6643 felix   37u  sock                0,7      0t0   54336 can't identify protocol
rclone  6643 felix   38u  sock                0,7      0t0   48832 can't identify protocol
rclone  6643 felix   39u  sock                0,7      0t0   52657 can't identify protocol
rclone  6643 felix   40u  sock                0,7      0t0   48845 can't identify protocol
rclone  6643 felix   41u  sock                0,7      0t0   51583 can't identify protocol
rclone  6643 felix   42u  sock                0,7      0t0   55381 can't identify protocol
rclone  6643 felix   43u  sock                0,7      0t0   50805 can't identify protocol
rclone  6643 felix   44u  sock                0,7      0t0   50840 can't identify protocol
rclone  6643 felix   45u  sock                0,7      0t0   50028 can't identify protocol
rclone  6643 felix   46u  sock                0,7      0t0   55423 can't identify protocol
rclone  6643 felix   47u  sock                0,7      0t0   55680 can't identify protocol
rclone  6643 felix   48u  sock                0,7      0t0   54979 can't identify protocol
rclone  6643 felix   49u  sock                0,7      0t0   55026 can't identify protocol
rclone  6643 felix   50u  sock                0,7      0t0   51061 can't identify protocol
rclone  6643 felix   51u  sock                0,7      0t0   50018 can't identify protocol
rclone  6643 felix   52u  sock                0,7      0t0   55061 can't identify protocol
rclone  6643 felix   53u  sock                0,7      0t0   61722 can't identify protocol
rclone  6643 felix   54u  sock                0,7      0t0   55118 can't identify protocol
rclone  6643 felix   55u  sock                0,7      0t0   51816 can't identify protocol
rclone  6643 felix   56u  sock                0,7      0t0   57440 can't identify protocol
rclone  6643 felix   57u  sock                0,7      0t0   53007 can't identify protocol
rclone  6643 felix   58u  sock                0,7      0t0   63923 can't identify protocol
rclone  6643 felix   59u  sock                0,7      0t0   59423 can't identify protocol
rclone  6643 felix   60u  sock                0,7      0t0   62480 can't identify protocol
rclone  6643 felix   61u  sock                0,7      0t0   60859 can't identify protocol
rclone  6643 felix   62u  sock                0,7      0t0   61732 can't identify protocol
rclone  6643 felix   63u  sock                0,7      0t0   58310 can't identify protocol
rclone  6643 felix   64u  sock                0,7      0t0   64164 can't identify protocol
rclone  6643 felix   65u  sock                0,7      0t0   61810 can't identify protocol
rclone  6643 felix   66u  sock                0,7      0t0   59074 can't identify protocol
rclone  6643 felix   67u  sock                0,7      0t0   57134 can't identify protocol
rclone  6643 felix   68u  sock                0,7      0t0   58189 can't identify protocol
rclone  6643 felix   69u  sock                0,7      0t0   66739 can't identify protocol
rclone  6643 felix   70u  sock                0,7      0t0   73797 can't identify protocol
rclone  6643 felix   71u  sock                0,7      0t0   57237 can't identify protocol
rclone  6643 felix   72u  sock                0,7      0t0   61955 can't identify protocol
rclone  6643 felix   73u  sock                0,7      0t0   60359 can't identify protocol
rclone  6643 felix   74u  sock                0,7      0t0   63361 can't identify protocol
rclone  6643 felix   75u  sock                0,7      0t0   68013 can't identify protocol
rclone  6643 felix   76u  sock                0,7      0t0   66230 can't identify protocol
rclone  6643 felix   77u  sock                0,7      0t0   77866 can't identify protocol
rclone  6643 felix   78u  sock                0,7      0t0   74905 can't identify protocol
rclone  6643 felix   79u  sock                0,7      0t0   75939 can't identify protocol
rclone  6643 felix   80u  sock                0,7      0t0   71221 can't identify protocol
rclone  6643 felix   81u  sock                0,7      0t0   72089 can't identify protocol
rclone  6643 felix   82u  sock                0,7      0t0   84091 can't identify protocol
rclone  6643 felix   83u  sock                0,7      0t0   72225 can't identify protocol
rclone  6643 felix   84u  sock                0,7      0t0   76859 can't identify protocol
rclone  6643 felix   85u  sock                0,7      0t0   71444 can't identify protocol
rclone  6643 felix   86u  sock                0,7      0t0   76905 can't identify protocol
rclone  6643 felix   87u  sock                0,7      0t0   75983 can't identify protocol
rclone  6643 felix   88u  sock                0,7      0t0   73244 can't identify protocol
rclone  6643 felix   89u  sock                0,7      0t0   76114 can't identify protocol
rclone  6643 felix   90u  sock                0,7      0t0   80918 can't identify protocol
rclone  6643 felix   91u  sock                0,7      0t0   77881 can't identify protocol
rclone  6643 felix   92u  sock                0,7      0t0   76205 can't identify protocol
rclone  6643 felix   93u  sock                0,7      0t0   76238 can't identify protocol
rclone  6643 felix   94u  sock                0,7      0t0   73405 can't identify protocol
rclone  6643 felix   95u  sock                0,7      0t0   79935 can't identify protocol
rclone  6643 felix   96u  sock                0,7      0t0   77909 can't identify protocol
rclone  6643 felix   97u  sock                0,7      0t0   72621 can't identify protocol
rclone  6643 felix   98u  sock                0,7      0t0   77693 can't identify protocol
rclone  6643 felix   99u  sock                0,7      0t0   73679 can't identify protocol
rclone  6643 felix  100u  sock                0,7      0t0   77750 can't identify protocol
rclone  6643 felix  101u  sock                0,7      0t0   79951 can't identify protocol
rclone  6643 felix  102u  sock                0,7      0t0   80376 can't identify protocol
rclone  6643 felix  103u  sock                0,7      0t0   82414 can't identify protocol
rclone  6643 felix  104u  sock                0,7      0t0  102913 can't identify protocol
rclone  6643 felix  105u  sock                0,7      0t0   97606 can't identify protocol
rclone  6643 felix  106u  sock                0,7      0t0   78532 can't identify protocol
rclone  6643 felix  107u  sock                0,7      0t0   87804 can't identify protocol
rclone  6643 felix  108u  sock                0,7      0t0   88954 can't identify protocol
rclone  6643 felix  109u  sock                0,7      0t0   85988 can't identify protocol
rclone  6643 felix  110u  sock                0,7      0t0   93482 can't identify protocol
rclone  6643 felix  111u  sock                0,7      0t0   92942 can't identify protocol
rclone  6643 felix  112u  sock                0,7      0t0   94111 can't identify protocol
rclone  6643 felix  113u  sock                0,7      0t0   97818 can't identify protocol
rclone  6643 felix  114u  sock                0,7      0t0   98981 can't identify protocol
rclone  6643 felix  115u  sock                0,7      0t0   99000 can't identify protocol
rclone  6643 felix  116u  sock                0,7      0t0   96938 can't identify protocol
rclone  6643 felix  117u  sock                0,7      0t0   97960 can't identify protocol
rclone  6643 felix  118u  sock                0,7      0t0   99117 can't identify protocol
rclone  6643 felix  119u  sock                0,7      0t0   99744 can't identify protocol
rclone  6643 felix  120u  sock                0,7      0t0   97034 can't identify protocol
rclone  6643 felix  121u  sock                0,7      0t0   96113 can't identify protocol
rclone  6643 felix  122u  sock                0,7      0t0   99129 can't identify protocol
rclone  6643 felix  123u  sock                0,7      0t0   99175 can't identify protocol
rclone  6643 felix  124u  sock                0,7      0t0   98036 can't identify protocol
rclone  6643 felix  125u  sock                0,7      0t0   99215 can't identify protocol
rclone  6643 felix  126u  sock                0,7      0t0   98103 can't identify protocol
rclone  6643 felix  127u  sock                0,7      0t0  103530 can't identify protocol
rclone  6643 felix  128u  sock                0,7      0t0  100689 can't identify protocol
rclone  6643 felix  129u  sock                0,7      0t0  101539 can't identify protocol
rclone  6643 felix  130u  sock                0,7      0t0  101529 can't identify protocol
rclone  6643 felix  131u  sock                0,7      0t0  103430 can't identify protocol
rclone  6643 felix  132u  sock                0,7      0t0  103811 can't identify protocol
rclone  6643 felix  133u  sock                0,7      0t0  109942 can't identify protocol
rclone  6643 felix  134u  sock                0,7      0t0  106937 can't identify protocol
rclone  6643 felix  135u  sock                0,7      0t0  106947 can't identify protocol
rclone  6643 felix  136u  sock                0,7      0t0  103166 can't identify protocol
rclone  6643 felix  137u  sock                0,7      0t0  102146 can't identify protocol
rclone  6643 felix  138u  sock                0,7      0t0  105010 can't identify protocol
rclone  6643 felix  139u  sock                0,7      0t0  102302 can't identify protocol
rclone  6643 felix  140u  sock                0,7      0t0  108042 can't identify protocol
rclone  6643 felix  141u  sock                0,7      0t0  103213 can't identify protocol
rclone  6643 felix  142u  sock                0,7      0t0  107469 can't identify protocol
rclone  6643 felix  143u  sock                0,7      0t0  114803 can't identify protocol
rclone  6643 felix  144u  sock                0,7      0t0  108720 can't identify protocol
rclone  6643 felix  145u  sock                0,7      0t0  110746 can't identify protocol
rclone  6643 felix  146u  sock                0,7      0t0  106412 can't identify protocol
rclone  6643 felix  147u  sock                0,7      0t0  107877 can't identify protocol
rclone  6643 felix  148u  sock                0,7      0t0  110772 can't identify protocol
rclone  6643 felix  149u  sock                0,7      0t0  113760 can't identify protocol
rclone  6643 felix  150u  sock                0,7      0t0  110806 can't identify protocol
rclone  6643 felix  151u  sock                0,7      0t0  111837 can't identify protocol
rclone  6643 felix  152u  sock                0,7      0t0  113863 can't identify protocol
rclone  6643 felix  153u  sock                0,7      0t0  109956 can't identify protocol
rclone  6643 felix  154u  sock                0,7      0t0  109965 can't identify protocol
rclone  6643 felix  155u  sock                0,7      0t0  109981 can't identify protocol
rclone  6643 felix  156u  sock                0,7      0t0  113781 can't identify protocol
rclone  6643 felix  157u  sock                0,7      0t0  114037 can't identify protocol
rclone  6643 felix  158u  sock                0,7      0t0  108140 can't identify protocol
rclone  6643 felix  159u  sock                0,7      0t0  106455 can't identify protocol
rclone  6643 felix  160u  sock                0,7      0t0  112952 can't identify protocol
rclone  6643 felix  161u  sock                0,7      0t0  108413 can't identify protocol
rclone  6643 felix  162u  sock                0,7      0t0  110921 can't identify protocol
rclone  6643 felix  163u  sock                0,7      0t0  112179 can't identify protocol
rclone  6643 felix  164u  sock                0,7      0t0  110311 can't identify protocol
rclone  6643 felix  165u  sock                0,7      0t0  112440 can't identify protocol
rclone  6643 felix  166u  sock                0,7      0t0  113195 can't identify protocol
rclone  6643 felix  167u  sock                0,7      0t0  113446 can't identify protocol
rclone  6643 felix  168u  sock                0,7      0t0  115177 can't identify protocol
rclone  6643 felix  169u  sock                0,7      0t0  115234 can't identify protocol
rclone  6643 felix  170u  sock                0,7      0t0  115138 can't identify protocol
rclone  6643 felix  171u  sock                0,7      0t0  109301 can't identify protocol
rclone  6643 felix  172u  sock                0,7      0t0  114374 can't identify protocol
rclone  6643 felix  173u  sock                0,7      0t0  109367 can't identify protocol
rclone  6643 felix  174u  sock                0,7      0t0  118794 can't identify protocol
rclone  6643 felix  175u  sock                0,7      0t0  114445 can't identify protocol
rclone  6643 felix  176u  sock                0,7      0t0  118865 can't identify protocol
rclone  6643 felix  177u  sock                0,7      0t0  117970 can't identify protocol
rclone  6643 felix  178u  sock                0,7      0t0  111259 can't identify protocol
rclone  6643 felix  179u  sock                0,7      0t0  116315 can't identify protocol
rclone  6643 felix  180u  sock                0,7      0t0  118940 can't identify protocol
rclone  6643 felix  181u  sock                0,7      0t0  114494 can't identify protocol
rclone  6643 felix  182u  sock                0,7      0t0  118007 can't identify protocol
rclone  6643 felix  183u  sock                0,7      0t0  115334 can't identify protocol
rclone  6643 felix  184u  sock                0,7      0t0  117650 can't identify protocol
rclone  6643 felix  185u  sock                0,7      0t0  118218 can't identify protocol
rclone  6643 felix  186u  sock                0,7      0t0  111430 can't identify protocol
rclone  6643 felix  187u  sock                0,7      0t0  111459 can't identify protocol
rclone  6643 felix  188u  sock                0,7      0t0  115574 can't identify protocol
rclone  6643 felix  189u  sock                0,7      0t0  123977 can't identify protocol
rclone  6643 felix  190u  sock                0,7      0t0  122000 can't identify protocol
rclone  6643 felix  191u  sock                0,7      0t0  120027 can't identify protocol
rclone  6643 felix  192u  sock                0,7      0t0  125031 can't identify protocol
rclone  6643 felix  193u  sock                0,7      0t0  122118 can't identify protocol
rclone  6643 felix  194u  sock                0,7      0t0  124998 can't identify protocol
rclone  6643 felix  195u  sock                0,7      0t0  121260 can't identify protocol
rclone  6643 felix  196u  sock                0,7      0t0  125114 can't identify protocol
rclone  6643 felix  197u  sock                0,7      0t0  121361 can't identify protocol
rclone  6643 felix  198u  sock                0,7      0t0  121339 can't identify protocol
rclone  6643 felix  199u  sock                0,7      0t0  126223 can't identify protocol
rclone  6643 felix  200u  sock                0,7      0t0  121629 can't identify protocol
rclone  6643 felix  201u  sock                0,7      0t0  122612 can't identify protocol
rclone  6643 felix  202u  sock                0,7      0t0  121639 can't identify protocol
rclone  6643 felix  203u  sock                0,7      0t0  121560 can't identify protocol
rclone  6643 felix  204u  sock                0,7      0t0  122797 can't identify protocol
rclone  6643 felix  205u  sock                0,7      0t0  124847 can't identify protocol
rclone  6643 felix  206u  sock                0,7      0t0  123659 can't identify protocol
rclone  6643 felix  207u  sock                0,7      0t0  124797 can't identify protocol
rclone  6643 felix  208u  sock                0,7      0t0  124818 can't identify protocol
rclone  6643 felix  209u  sock                0,7      0t0  123639 can't identify protocol
rclone  6643 felix  210u  sock                0,7      0t0  123673 can't identify protocol
rclone  6643 felix  211u  sock                0,7      0t0  129180 can't identify protocol
rclone  6643 felix  212u  sock                0,7      0t0  120749 can't identify protocol
rclone  6643 felix  213u  sock                0,7      0t0  121769 can't identify protocol
rclone  6643 felix  214u  sock                0,7      0t0  123824 can't identify protocol
rclone  6643 felix  215u  sock                0,7      0t0  131131 can't identify protocol
rclone  6643 felix  216u  sock                0,7      0t0  133125 can't identify protocol
rclone  6643 felix  217u  sock                0,7      0t0  123856 can't identify protocol
rclone  6643 felix  218u  sock                0,7      0t0  127901 can't identify protocol
rclone  6643 felix  219u  sock                0,7      0t0  133152 can't identify protocol
rclone  6643 felix  220u  sock                0,7      0t0  132223 can't identify protocol
rclone  6643 felix  221u  sock                0,7      0t0  129416 can't identify protocol
rclone  6643 felix  222u  sock                0,7      0t0  133172 can't identify protocol
rclone  6643 felix  223u  sock                0,7      0t0  125790 can't identify protocol
rclone  6643 felix  224u  sock                0,7      0t0  129475 can't identify protocol
rclone  6643 felix  225u  sock                0,7      0t0  134254 can't identify protocol
rclone  6643 felix  226u  sock                0,7      0t0  132384 can't identify protocol
rclone  6643 felix  227u  sock                0,7      0t0  131277 can't identify protocol
rclone  6643 felix  228u  sock                0,7      0t0  125864 can't identify protocol
rclone  6643 felix  229u  sock                0,7      0t0  129688 can't identify protocol
rclone  6643 felix  230u  sock                0,7      0t0  128708 can't identify protocol
rclone  6643 felix  231u  sock                0,7      0t0  131441 can't identify protocol
rclone  6643 felix  232u  sock                0,7      0t0  134515 can't identify protocol
rclone  6643 felix  233u  sock                0,7      0t0  128776 can't identify protocol
rclone  6643 felix  234u  sock                0,7      0t0  130493 can't identify protocol
rclone  6643 felix  235u  sock                0,7      0t0  132760 can't identify protocol
rclone  6643 felix  236u  sock                0,7      0t0  129896 can't identify protocol
rclone  6643 felix  237u  sock                0,7      0t0  130519 can't identify protocol
rclone  6643 felix  238u  sock                0,7      0t0  128940 can't identify protocol
rclone  6643 felix  239u  sock                0,7      0t0  131585 can't identify protocol
rclone  6643 felix  240u  sock                0,7      0t0  132948 can't identify protocol
rclone  6643 felix  241u  sock                0,7      0t0  129957 can't identify protocol
rclone  6643 felix  242u  sock                0,7      0t0  136348 can't identify protocol
rclone  6643 felix  243u  sock                0,7      0t0  133536 can't identify protocol
rclone  6643 felix  244u  sock                0,7      0t0  134988 can't identify protocol
rclone  6643 felix  245u  sock                0,7      0t0  133070 can't identify protocol
rclone  6643 felix  246u  sock                0,7      0t0  131743 can't identify protocol
rclone  6643 felix  247u  sock                0,7      0t0  138275 can't identify protocol
rclone  6643 felix  248u  sock                0,7      0t0  136519 can't identify protocol
rclone  6643 felix  249u  sock                0,7      0t0  139340 can't identify protocol
rclone  6643 felix  250u  sock                0,7      0t0  131800 can't identify protocol
rclone  6643 felix  251u  sock                0,7      0t0  133719 can't identify protocol
rclone  6643 felix  252u  sock                0,7      0t0  138443 can't identify protocol
rclone  6643 felix  253u  sock                0,7      0t0  133734 can't identify protocol
rclone  6643 felix  254u  sock                0,7      0t0  137509 can't identify protocol
rclone  6643 felix  255u  sock                0,7      0t0  135669 can't identify protocol
rclone  6643 felix  256u  sock                0,7      0t0  136840 can't identify protocol
rclone  6643 felix  257u  sock                0,7      0t0  138552 can't identify protocol
rclone  6643 felix  258u  sock                0,7      0t0  130915 can't identify protocol
rclone  6643 felix  259u  sock                0,7      0t0  139534 can't identify protocol
rclone  6643 felix  260u  sock                0,7      0t0  133848 can't identify protocol
rclone  6643 felix  261u  sock                0,7      0t0  136897 can't identify protocol
rclone  6643 felix  262u  sock                0,7      0t0  139758 can't identify protocol
rclone  6643 felix  263u  sock                0,7      0t0  130967 can't identify protocol
rclone  6643 felix  264u  sock                0,7      0t0  140409 can't identify protocol
rclone  6643 felix  265u  sock                0,7      0t0  135798 can't identify protocol
rclone  6643 felix  266u  sock                0,7      0t0  134096 can't identify protocol
rclone  6643 felix  267u  sock                0,7      0t0  134110 can't identify protocol
rclone  6643 felix  268u  sock                0,7      0t0  140069 can't identify protocol
rclone  6643 felix  269u  sock                0,7      0t0  142458 can't identify protocol
rclone  6643 felix  270u  sock                0,7      0t0  142479 can't identify protocol
rclone  6643 felix  271u  sock                0,7      0t0  140120 can't identify protocol
rclone  6643 felix  272u  sock                0,7      0t0  140189 can't identify protocol
rclone  6643 felix  273u  sock                0,7      0t0  138154 can't identify protocol
rclone  6643 felix  274u  sock                0,7      0t0  140171 can't identify protocol
rclone  6643 felix  275u  sock                0,7      0t0  140250 can't identify protocol
rclone  6643 felix  276u  sock                0,7      0t0  142634 can't identify protocol
rclone  6643 felix  277u  sock                0,7      0t0  142644 can't identify protocol
rclone  6643 felix  278u  sock                0,7      0t0  141497 can't identify protocol
rclone  6643 felix  279u  sock                0,7      0t0  145410 can't identify protocol
rclone  6643 felix  280u  sock                0,7      0t0  145454 can't identify protocol
rclone  6643 felix  281u  sock                0,7      0t0  142835 can't identify protocol
rclone  6643 felix  282u  sock                0,7      0t0  141598 can't identify protocol
rclone  6643 felix  283u  sock                0,7      0t0  140633 can't identify protocol
rclone  6643 felix  284u  sock                0,7      0t0  141600 can't identify protocol
rclone  6643 felix  285u  sock                0,7      0t0  147509 can't identify protocol
rclone  6643 felix  286u  sock                0,7      0t0  140689 can't identify protocol
rclone  6643 felix  287u  sock                0,7      0t0  143612 can't identify protocol
rclone  6643 felix  288u  sock                0,7      0t0  143191 can't identify protocol
rclone  6643 felix  289u  sock                0,7      0t0  143106 can't identify protocol
rclone  6643 felix  290u  sock                0,7      0t0  143056 can't identify protocol
rclone  6643 felix  291u  sock                0,7      0t0  143076 can't identify protocol
rclone  6643 felix  292u  sock                0,7      0t0  141671 can't identify protocol
rclone  6643 felix  293u  sock                0,7      0t0  143714 can't identify protocol
rclone  6643 felix  294u  sock                0,7      0t0  146714 can't identify protocol
rclone  6643 felix  295u  sock                0,7      0t0  145758 can't identify protocol
rclone  6643 felix  296u  sock                0,7      0t0  144627 can't identify protocol
rclone  6643 felix  297u  sock                0,7      0t0  147730 can't identify protocol
rclone  6643 felix  298u  sock                0,7      0t0  146802 can't identify protocol
rclone  6643 felix  299u  sock                0,7      0t0  140768 can't identify protocol
rclone  6643 felix  300u  sock                0,7      0t0  148491 can't identify protocol
rclone  6643 felix  301u  sock                0,7      0t0  147864 can't identify protocol
rclone  6643 felix  302u  sock                0,7      0t0  147784 can't identify protocol
rclone  6643 felix  303u  sock                0,7      0t0  148575 can't identify protocol
rclone  6643 felix  304u  sock                0,7      0t0  146963 can't identify protocol
rclone  6643 felix  305u  sock                0,7      0t0  144954 can't identify protocol
rclone  6643 felix  306u  sock                0,7      0t0  140867 can't identify protocol
rclone  6643 felix  307u  sock                0,7      0t0  148527 can't identify protocol
rclone  6643 felix  308u  sock                0,7      0t0  146984 can't identify protocol
rclone  6643 felix  309u  sock                0,7      0t0  148601 can't identify protocol
rclone  6643 felix  310u  sock                0,7      0t0  148623 can't identify protocol
rclone  6643 felix  311u  sock                0,7      0t0  143855 can't identify protocol
rclone  6643 felix  312u  sock                0,7      0t0  145361 can't identify protocol
rclone  6643 felix  313u  sock                0,7      0t0  145292 can't identify protocol
rclone  6643 felix  314u  sock                0,7      0t0  149659 can't identify protocol
rclone  6643 felix  315u  sock                0,7      0t0  148088 can't identify protocol
rclone  6643 felix  316u  sock                0,7      0t0  149144 can't identify protocol
rclone  6643 felix  317u  sock                0,7      0t0  144328 can't identify protocol
rclone  6643 felix  318u  sock                0,7      0t0  151809 can't identify protocol
rclone  6643 felix  319u  sock                0,7      0t0  144340 can't identify protocol
rclone  6643 felix  320u  sock                0,7      0t0  141268 can't identify protocol
rclone  6643 felix  321u  sock                0,7      0t0  152688 can't identify protocol
rclone  6643 felix  322u  sock                0,7      0t0  150764 can't identify protocol
rclone  6643 felix  323u  sock                0,7      0t0  150852 can't identify protocol
rclone  6643 felix  324u  sock                0,7      0t0  149367 can't identify protocol
rclone  6643 felix  325u  sock                0,7      0t0  151958 can't identify protocol
rclone  6643 felix  326u  sock                0,7      0t0  150917 can't identify protocol
rclone  6643 felix  327u  sock                0,7      0t0  150931 can't identify protocol
rclone  6643 felix  328u  sock                0,7      0t0  151046 can't identify protocol
rclone  6643 felix  329u  sock                0,7      0t0  150962 can't identify protocol
rclone  6643 felix  330u  sock                0,7      0t0  150979 can't identify protocol
rclone  6643 felix  331u  sock                0,7      0t0  149947 can't identify protocol
rclone  6643 felix  332u  sock                0,7      0t0  151064 can't identify protocol
rclone  6643 felix  333u  sock                0,7      0t0  152322 can't identify protocol
rclone  6643 felix  334u  sock                0,7      0t0  153703 can't identify protocol
rclone  6643 felix  335u  sock                0,7      0t0  156696 can't identify protocol
rclone  6643 felix  336u  sock                0,7      0t0  153768 can't identify protocol
rclone  6643 felix  337u  sock                0,7      0t0  150161 can't identify protocol
rclone  6643 felix  338u  sock                0,7      0t0  152313 can't identify protocol
rclone  6643 felix  339u  sock                0,7      0t0  152346 can't identify protocol
rclone  6643 felix  340u  sock                0,7      0t0  153844 can't identify protocol
rclone  6643 felix  341u  sock                0,7      0t0  151393 can't identify protocol
rclone  6643 felix  342u  sock                0,7      0t0  157726 can't identify protocol
rclone  6643 felix  343u  sock                0,7      0t0  154056 can't identify protocol
rclone  6643 felix  344u  sock                0,7      0t0  157152 can't identify protocol
rclone  6643 felix  345u  sock                0,7      0t0  155234 can't identify protocol
rclone  6643 felix  346u  sock                0,7      0t0  159943 can't identify protocol
rclone  6643 felix  347u  sock                0,7      0t0  157193 can't identify protocol
rclone  6643 felix  348u  sock                0,7      0t0  159310 can't identify protocol
rclone  6643 felix  349u  sock                0,7      0t0  153469 can't identify protocol
rclone  6643 felix  350u  sock                0,7      0t0  159437 can't identify protocol
rclone  6643 felix  351u  sock                0,7      0t0  159505 can't identify protocol
rclone  6643 felix  352u  sock                0,7      0t0  160451 can't identify protocol
rclone  6643 felix  353u  sock                0,7      0t0  160912 can't identify protocol
rclone  6643 felix  354u  sock                0,7      0t0  160467 can't identify protocol
rclone  6643 felix  355u  sock                0,7      0t0  157308 can't identify protocol
rclone  6643 felix  356u  sock                0,7      0t0  160965 can't identify protocol
rclone  6643 felix  357u  sock                0,7      0t0  157329 can't identify protocol
rclone  6643 felix  358u  sock                0,7      0t0  160691 can't identify protocol
rclone  6643 felix  359u  sock                0,7      0t0  162933 can't identify protocol
rclone  6643 felix  360u  sock                0,7      0t0  162880 can't identify protocol
rclone  6643 felix  361u  sock                0,7      0t0  162942 can't identify protocol
rclone  6643 felix  362u  sock                0,7      0t0  157547 can't identify protocol
rclone  6643 felix  363u  sock                0,7      0t0  165238 can't identify protocol
rclone  6643 felix  364u  sock                0,7      0t0  161487 can't identify protocol
rclone  6643 felix  365u  sock                0,7      0t0  162119 can't identify protocol
rclone  6643 felix  366u  sock                0,7      0t0  162144 can't identify protocol
rclone  6643 felix  367u  sock                0,7      0t0  166212 can't identify protocol
rclone  6643 felix  368u  sock                0,7      0t0  166246 can't identify protocol
rclone  6643 felix  369u  sock                0,7      0t0  161781 can't identify protocol
rclone  6643 felix  370u  IPv4             168197      0t0     TCP 192.168.86.30:59265->ec2-52-73-211-240.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  371u  IPv4             162394      0t0     TCP 192.168.86.30:54789->ec2-52-7-40-173.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  372u  sock                0,7      0t0  167515 can't identify protocol
rclone  6643 felix  373u  IPv4             162353      0t0     TCP 192.168.86.30:56779->ec2-52-7-115-83.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  374u  IPv4             166441      0t0     TCP 192.168.86.30:57995->ec2-52-22-33-148.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  375u  IPv4             162364      0t0     TCP 192.168.86.30:57425->ec2-54-172-68-21.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  376u  IPv4             162379      0t0     TCP 192.168.86.30:40436->ec2-52-4-187-176.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  377u  IPv4             164738      0t0     TCP 192.168.86.30:40461->ec2-52-4-187-176.compute-1.amazonaws.com:https (CLOSE_WAIT)
rclone  6643 felix  378u  IPv4             167875      0t0     TCP 192.168.86.30:42365->ec2-34-195-188-75.compute-1.amazonaws.com:https (ESTABLISHED)
rclone  6643 felix  379u  IPv4             163772      0t0     TCP 192.168.86.30:41956->ec2-52-72-159-114.compute-1.amazonaws.com:https (ESTABLISHED)
rclone  6643 felix  380u  IPv4             166644      0t0     TCP 192.168.86.30:34585->ec2-54-172-239-173.compute-1.amazonaws.com:https (ESTABLISHED)
```

Something definitely is not closing a file / socket out as I rebooted this morning at ~630ish and it's a slow climb of sockets..

http://i.imgur.com/2lDinft.png

I can't think of what else to collect to help pinpoint the issue :( I used the morning to just do a complete reinstall of my linux box and I can't get the issue to reproduce. I did a full scan and I don't see any of those unknown sockets anymore.

I close the issue out as something system specific that wasn't playing nice. Thanks for the help. I spoke too soon.

So I did a complete fresh install of another distro this morning on Fedora 25. Reinstalled everything. 

Output from lsof has the same running off connections that don't match
```
[root@plex ~]# lsof -c rclone
COMMAND   PID  USER   FD      TYPE             DEVICE SIZE/OFF     NODE NAME
rclone  15592 felix  cwd       DIR                8,2     4096 13111780 /home/felix/scripts
rclone  15592 felix  rtd       DIR                8,2     4096        2 /
rclone  15592 felix  txt       REG                8,2 14671328  3943129 /usr/bin/rclone
rclone  15592 felix    0r      CHR                1,3      0t0     1028 /dev/null
rclone  15592 felix    1u      CHR              136,0      0t0        3 /dev/pts/0
rclone  15592 felix    2u      CHR              136,0      0t0        3 /dev/pts/0
rclone  15592 felix    3u     unix 0xffff9057c0383c00      0t0   437077 type=DGRAM
rclone  15592 felix    4u  a_inode               0,11        0    11590 [eventpoll]
rclone  15592 felix    5u     sock                0,8      0t0   463414 protocol: TCP
rclone  15592 felix    6u     sock                0,8      0t0   452192 protocol: TCP
rclone  15592 felix    7u     sock                0,8      0t0   465884 protocol: TCP
rclone  15592 felix    8u     sock                0,8      0t0   463320 protocol: TCP
rclone  15592 felix    9u     sock                0,8      0t0   472245 protocol: TCP
rclone  15592 felix   10u     sock                0,8      0t0   465967 protocol: TCP
rclone  15592 felix   11u      CHR             10,229      0t0    17036 /dev/fuse
rclone  15592 felix   12u     sock                0,8      0t0   468996 protocol: TCP
rclone  15592 felix   13u     sock                0,8      0t0   466235 protocol: TCP
```

My point point shows zero file activity:
```
[root@plex ~]# lsof /media
[root@plex ~]#
```

I haven't hit the out of sockets issue yet, but I can't imagine the socket build is something I"m doing. My previous install was Debian 8. I thought I had some correlation with io timeouts to sockets, but it doesn't seem one for one. If I just scan my plex library, doesn't seem to create the issue.

If I run something that opens and closes files, I can recreate it easily like if I do an analyze. I kinda gave up on ACD at this point and just moved to GDrive for now. I will close it out as I'm not trying to test any further as I kept getting the issue and the ACD stability has been too annoying.  Noted - thanks!
  I've merged this now - thank you very much  Nice idea!

I think that would be best implemented as a separate command, say

    rclone copyurl https://example.com/file.tar remote:File/file.tar

Yes rclone does need to know the size of files in advance, however for a lot of remotes, you can just use 0 and it will work.  This doesn't work with b2 for instance.  You mostly likely will get a Content-Length as you suggest though.  For a remote which support directory moves, your operation is perfectly safe.

For a remote which doesn't it has to move each individual file which might fail and need a retry which is where the trouble starts...

I need to spend some time thinking about the corner cases!  This issue has come up before (can't find the issue though!).

I like your idea of lanes - it would have to be implemented something like that.  > Yea I thought tests are necessary. I'll try to write them but I might need some pointers.

The best place to look in rclone is the [buffer tests](https://github.com/ncw/rclone/blob/master/fs/buffer_test.go).  Note how easy it is to make special purpose io.Readers to test certain things.

I'd make 3 test functions, one to test New, one for Seek and one for Read.  I'd then try to cover all the code paths using the [go coverage](https://blog.golang.org/cover) tool to make sure I didn't miss anything.

> Should I also use this reader with gdrive and maybe other endpoints that are currently reading the whole chunk ?

Anything which is doing chunked uploads is a candidate which included gdrive and b2.  I think we should make sure it is rock solid with onedrive first though.

> I don't have a gdrive account (or any other endpoint than onedrive) setup so I wont be able to run integration tests on them

Luckily you can make free accounts for gdrive and b2 very easily - that is what I use for the integration tests. I think the code and the tests look good so I'm going to merge this now - thank you very much for your patience.

You use `require` when you don't want the test to continue past that point and `assert` when you do.  It doesn't make a lot of difference, but is useful to `require` an error to be nil, before `assert` ing that the object returned does this that or the other. > probably github just over eagerly interrupted my first comment to this merge

yes that is what happened!  I've re-opened the issue.  Yes this is a problem with SFTP and the local file system...  rsync uses temporary names to get around this problem so rclone should probably adopt that too..  @sweharris fancy having a look at this? which beta was this problem introduced in? @sweharris Are you going to send a PR for the fix?

Option 4 seems like a reasonable choice to me.

Can you make sure you include a test which fails on "Avatar The Last Airbender" and is fixed by your patch?

As for existing data that has been uploaded, a short python script should be able to rename things shouldn't it? This has been merged and will be in this beta

https://beta.rclone.org/v1.36-23-ge1647a5/ (uploaded in 15-30 mins)  I see... I presume you are talking about this rsync option?

       -R, --relative
              Use  relative  paths. This means that the full path names speci‐
              fied on the command line are sent to the server rather than just
              the  last  parts  of  the filenames. This is particularly useful
              when you want to send several different directories at the  same
              time. For example, if you used this command:

                 rsync -av /foo/bar/baz.c remote:/tmp/

              ...  this would create a file named baz.c in /tmp/ on the remote
              machine. If instead you used

                 rsync -avR /foo/bar/baz.c remote:/tmp/

              then a file named /tmp/foo/bar/baz.c would  be  created  on  the
              remote machine, preserving its full path.  These extra path ele‐
              ments are called "implied directories" (i.e. the "foo"  and  the
              "foo/bar" directories in the above example).
  @dogear42 did you set up a password for ssh when you did the `rclone config` originally?  If so then it should have used that rather than an agent.

I just tried this beta https://pub.rclone.org/v1.36-22-g06ea13a-ssh-agent%CE%B2/ with termux on my nexus 6 using sftp with a password and it worked fine.

Can you try that?  rclone doesn't change the go default which is to make one OS thread per CPU.  goroutines are then sheduled over these OS threads - there can be many more goroutines than OS threads.  OS threads are also made for certain kinds of IO (eg disk IO).

So rclone should already be using the maximum amount of CPU it needs.

You can change the number of OS threads rclone creates initially by setting the [GOMAXPROCS](https://golang.org/pkg/runtime/) environment variable.  I think you are unlikely to see any performance changes if you do that, but I would be interested in reports if you do notice a difference.

> The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code; those do not count against the GOMAXPROCS limit.  Yes that sounds right - see 6f8501e9a1b38cdd0aaee752d325dcc6552ef750 for an example.

PR much appreciated!

Thanks  Sorry, I have been away - back now!

You can find a beta to fix #1296 in that issue.

My plan is to phase out --old-sync-method eventually, but not until I'm sure we don't need it any more. Now that #1296 is done I'm going to close this.  I have fixed this now - thanks for reporting

https://beta.rclone.org/v1.36-22-gbc25190/ (uploaded in 15-30 mins)  This is a bug in the golang.org/x/crypto/ssh/terminal library I think... - I have reported it as such here: https://github.com/golang/go/issues/19909

@azureblaze work-around is a good one though.  I was trying to do something similar with nginx through the use of the try_files module where I could make fault tolerance across multiple mounts and this was immediately brought as an issue.  It would be helpful if listing of files would break if the mount is in an error state.  The question I guess would be how to determine if it was in that state.  Perhaps some heart beat between the mount and the provider. Hmm, it is quite difficult to determine that a mount is broken.  The providers aren't good at saying - this is broken now, go away, they just give errors on the transactions, which might be OK if you retry them. Slippery slope.  If someone configured 60 transfer threads and quota is saying to allow 12 at that time then you'll get a lot of errors but the mount is working fine...

I've also had issue where threads are not doing any work but other than are fine.  In this case the mount is partially working.  My nginx server is hung on some threads but others are streaming.    > Issue 1 rClone does not copy subdirectories

rclone does copy subdirectoreis by default.  However it doesn't copy **empty** directories - is that your problem?

> issue 2 How to exclude certain directories

`--exclude "/thumbnails/**"` this will exclude thumbnails and everything in it.  If you have thumbnail directories not at the root, then you want `--exclude "thumbnails/**"`.  However I suspect you are missing the leading `.`, so what you want is `--exclude "/.thumbnails/**"`  This would be reasonably easy to fix...  The code in question is

https://github.com/ncw/rclone/blob/master/crypt/crypt.go#L143

Fancy having a go? This will be in

https://beta.rclone.org/v1.36-26-g82b8d68/ (uploaded in 15-30 mins!)  Which version of rclone are you using, which remote (with crypt?) and which OS are you running it on?

This might just be the provider aborting the connection for some reason of its own.  It could of course be a bug with seeking and errors - it is quite complicated that code!
  The command `rclone size` does already output in human readable form, eg

```
Total objects: 578218
Total size: 110.891 GBytes (119068486230 Bytes)
```

Do you mean in other output like `rclone ls`?  This would have to go in as part of #349 the v2 API.

It is annoyingly non standard!  It isn't either md5sum or an sha1sum or even an sha256sum  it is an sha256sum of 4 MB blocks, all concatenated then the sha256sum of those.

However it will do perfectly for upload and download integrity checking. Dropbox v2 API has just hit master with content hashing

Please try this beta

https://beta.rclone.org/v1.36-144-g178ff62d/ (uploaded in 15-30 mins) Bit of trouble building: beta now here: https://beta.rclone.org/v1.36-146-g71028e0f/  That looks great - thanks :-)

This is a recurring theme - there are quite a few issues this - eg #1181.  I think a more general purpose solution might be necessary in the future - maybe a writeback cache or something like that.  That is a nice idea.  It is a bit complicated to implement as the wait for the upload logic is in the amazon cloud drive module, wheras the restriction of the number of transfers is in the general copying layer of rclone.
  I've merged that - thank you very much for fixing that..  Seeking is slow because rclone has to close the existing stream, and re-open it using a `Range:` header.

There is a test script in `cmd/mount/seek_speed.go` which you can use to test the seek speeds.

Here is me running it on a mounted encrypted acd disk

You can see seeks are taking on average 2 seconds for me.

```
$ go run seek_speed.go ~/mnt/tmp/bigfile
2017/03/31 13:16:42 Reading 170511 from 150060860
2017/03/31 13:16:43 Reading 459963 from 139102236
2017/03/31 13:16:47 Reading 643462 from 1819658102
2017/03/31 13:16:48 Reading 578220 from 1654194378
2017/03/31 13:16:49 Reading 280484 from 1769279246
2017/03/31 13:16:49 Reading 350895 from 810238284
2017/03/31 13:16:55 Reading 614385 from 967797739
2017/03/31 13:16:55 Reading 300314 from 1621112450
2017/03/31 13:16:57 Reading 285845 from 1112190248
2017/03/31 13:16:57 Reading 625634 from 1595138255
2017/03/31 13:16:58 Reading 67802 from 336933638
2017/03/31 13:16:58 Reading 401042 from 1216435296
2017/03/31 13:16:59 Reading 445739 from 2034103843
2017/03/31 13:17:04 Reading 85240 from 1090106652
2017/03/31 13:17:10 Reading 923639 from 2185605778
2017/03/31 13:17:10 Reading 1011576 from 1289722284
2017/03/31 13:17:11 Reading 686863 from 1219131873
2017/03/31 13:17:15 Reading 756044 from 235146038
2017/03/31 13:17:22 Reading 82935 from 73103248
2017/03/31 13:17:23 Reading 543634 from 1037027997
2017/03/31 13:17:27 Reading 180882 from 502230445
2017/03/31 13:17:31 Reading 402243 from 1873257165
2017/03/31 13:17:31 Reading 670611 from 1972506627
2017/03/31 13:17:33 Reading 735972 from 463388514
2017/03/31 13:17:34 Reading 1025113 from 1917938230
2017/03/31 13:17:35 That took 52.400670551s for 25 iterations, 2.096026822s per iteration
```

 I have a sneaking suspicion that Amazon are penalizing range requests also as the above test used to run faster.  The service does run though phases of working slower and faster. I'll close it out as I can't find anything rclone related and seems to be amazon. I made the switch to Emby to fix my repeated scans so life is good at this point.  This should happen already on remotes which support modtime: https://rclone.org/overview/#features

Which is everything except ACD and dropbox.
  I haven't quite managed to replicate your exact problem, but there is clearly something weird going on!

```
$ rclone copy -v Dir onedrive:Dir
2017/03/30 10:40:36 INFO  : One drive root 'Dir': Modify window is 1s
2017/03/30 10:40:36 INFO  : One drive root 'Dir': Waiting for checks to finish
2017/03/30 10:40:36 INFO  : One drive root 'Dir': Waiting for transfers to finish
2017/03/30 10:40:37 INFO  : Sub Dir/test.txt: Copied (new)
2017/03/30 10:40:37 INFO  : 
Transferred:      6 Bytes (3 Bytes/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:        1.9s
$ rclone lsd onedrive:'/Dir/'
          -1 2017-03-30 09:40:38         0 Sub Dir
          -1 2017-03-30 09:40:39         1 Sub+Dir
```

If I run the integration tests they fail with lots of errors like this

```
	            	expected: []string{"file name.txt"}
	            	received: []string{"file+name.txt"}
```

So this indicates that something has changed at OneDrive, since I ran the integration tests for the 1.36 release and they worked fine then.

Needs more investigation!

 @yonjah - have you got time to look at this?
 @yonjah excellent work finding the problem :-)

go1.8 is not a problem - rclone builds with go1.8 normally.  However I'm trying to keep it working with pre go1.8 too.

What we need for this is a backported PathEscape for pre go1.8.  It is possible get the equivalent of path escape with a bit of trickery using a url.URL I think...  Something like this: https://play.golang.org/p/IPYL2oINdD would be a backwards compatible way of doing it.

 @yonjah that is surely related - well spotted! Sorry have been away - back now!  Will merge the PR soon and post a beta here for you all to try. Please find the beta with this fix in here

https://beta.rclone.org/v1.36-21-ge3a4132/ (uploaded in 15-30 mins) Thanks for testing.

@traynier thanks for the suggestion - I will mull it over...  You could do this in two ways

1. Make GPG do all the encryption/decryption of the data
2. Get GPG involved in the key handling, but still use crypt for the encryption

Option 1 is a lot more work, probably a new remote, and it would undoubtedly mean losing some features like seeking.

Option 2 is less work and might be useful.  You'd encrypt the key to the remote using gpg, then rclone would call gpg to decrypt it when needed, using your gpg agent or getting you to type in a password. rclone can open files part of the way through to allow `rclone mount` to seek files.  If there was a pgp remote then this wouldn't be possible as as far as I know, pgp wasn't designed to be used like this.  The API looks quite capable.  I don't see any documentation on what an object is though - can it store arbitrary data?  The easiest fix for this would be for rclone to take a --lower-case flag (say) so it lowercased all file names in and out.

Do do it properly would be ok if the files were always in the same directory, so if you had

/path/to/file
/path/to/FILE

that would be possible, but

/path/to/file
/path/TO/FILE

would be quite hard.

Any of those options useful?  I've merged that - thank you very much!

I've updated rclone.org - the auto generated docs will have to wait for the next release.  Something like --max-transfers would be possible.  However rclone works very asynchronously so getting it exactly right might be hard!  The handling of the config file has improved from 1.34, but I'll check that this case is handled properly.

I should probably change the config file handling to

* read config
* write config.new
* delete config.old if it exists
* rename config config.old
* rename config.new config
 It refreshes the auth token.  `rclone ls` will dump everything just fine.

You can also use

    rclone ls --include "*keyword*" remote:
  Can you paste the logs from rclone mount please? That error means that the name couldn't be resolved which is usually a temporary networking error.  You need to set `--low-level-retries` if you want to control the retry of individual operations.  It is 10 by default.

> (What I cannot figure out is what --acd-upload-wait-per-gb 60m exactly does. Does it wait 60 minutes to see if the file is there or does it keep checking and is the 60m per GB a maximum/timeout?)

It is a maximum timeout - if the file appears before then it will carry on.  I've merged this - thank you very much!

Do you think you could do a follow on PR with some docs in docs/content/drive.md please?  The fix doesn't look too complicated - it uses the small module https://github.com/xanzy/ssh-agent to make sure the windows code does the right thing.

I made a test on a branch here - fancy giving it a go on windows?

https://pub.rclone.org/v1.36-22-g06ea13a-ssh-agent%CE%B2/ @kubark42 Did it work? I've merged this to master as it nearly got lost!

https://beta.rclone.org/v1.36-136-g5455d34f/ (uploaded in 15-30 mins)

Please re-open the ticket if it doesn't work properly.  I'm just about to fix this, so watch this space! Use --old-sync-method in the mean time, but I've pushed my changes you'll use --fast-list. Please try using this beta with the --fast-list flag.  It will still use more memory that without but it will definitely use less transactions.

https://beta.rclone.org/v1.36-185-g64662bef/ (uploaded in 15-30 imins)  ACD supports range requests (as to nearly all the remotes) so this is certainly possible.

I'd need to make some extra internal interfaces so that the destination remote (eg local disk) could support ranged writes, or failing that the rclone core would need to buffer the chunks in memory which is probably undesirable.

I don't think this would help for streaming though as the whole point would be to download from different parts in the file simultaneously, whereas for streaming you are really only interested in the data at one given point (ie the place you are watching).  Thanks for the feedback, always nice to hear from happy users :-)  I unpacked the Movies1.zip and then ran

```
$ rclone -vv copy Movies1 TestSftp:Movies2
2017/03/24 10:10:16 DEBUG : rclone: Version "v1.36-05-g216499d" starting with parameters ["rclone" "-vv" "copy" "Movies1" "TestSftp:Movies2"]
2017/03/24 10:10:16 INFO  : sftp://sftptest@localhost:22/Movies2: Modify window is 1s
2017/03/24 10:10:16 INFO  : sftp://sftptest@localhost:22/Movies2: Waiting for checks to finish
2017/03/24 10:10:16 INFO  : sftp://sftptest@localhost:22/Movies2: Waiting for transfers to finish
2017/03/24 10:10:16 INFO  : Trying New Things 1/More Long Trying New Things/Exciting New Things That Are Long/New Bitmap Image.bmp: Copied (new)
2017/03/24 10:10:16 INFO  : Trying New Things 1/More Long Trying New Things/Exciting New Things That Are Long/New Text Document.txt: Copied (new)
2017/03/24 10:10:16 INFO  : Trying New Things 1/More Long Trying New Things/Exciting New Things That Are Long/New Rich Text Document.rtf: Copied (new)
2017/03/24 10:10:16 INFO  : 
Transferred:      7 Bytes (31 Bytes/s)
Errors:                 0
Checks:                 0
Transferred:            3
Elapsed time:       200ms
2017/03/24 10:10:16 DEBUG : Go routines at exit 14
2017/03/24 10:10:16 DEBUG : rclone: Version "v1.36-05-g216499d" finishing with parameters ["rclone" "-vv" "copy" "Movies1" "TestSftp:Movies2"]
```

Which didn't replicate the problem...

So it might be Windows specific, or it might be to do with your SFTP server.  Which SFTP server are you using?  Can you get me as many details about that as possible? This is almost certainly fixed by a243ea63538fb4598ffb85b971ed9e14826cf0e8

So the latest from https://beta.rclone.org should fix it.

If not, please reopen.

Thanks  Perfect - thank you :-)  Hey Nick, 

not sure if you meant to shut off the verbose output in 1.36 by any chance, seems when i run any command without -v i dont see anything when i try rclone sync, copy, move, just wanted to let you know incase you missed something before you pushed out 1.36 

Thanks 

Dedsec1 Yes, this is working as intended to make rclone behave more like rsync.  Use copyto: http://rclone.org/commands/rclone_copyto/  Nice one!

The CI failed because you need to check the error return from `bufReader.Seek(0, io.SeekStart)`.  Realistically it is never going to fail so `_ = bufReader.Seek(0, io.SeekStart)`.  Also you'll need to use the numerical equivalent to io.SeekStart as it was only defined in go1.7 (I think).

Did you run the integration tests for onedrive?  See: https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md#testing I've merged that now - thank you very much  What would you want - biggest / smallest?  Amazon service level seems to get better and worse like the weather!  I don't know of anything changing in particular though.  This looks like the config file has got corrupted somehow.

What does it look like?  Can you give me some idea (without pasting the secret token stuff!)

If you remake the amazon remote does it start working?
 I'm going to close this issue now since we have put in workarounds to stop the config file getting corrupted.  Could you make me a container which reproduces this issue that I can get from the docker hub?  With some instructions on how to fetch, run and reproduce please?

I've done quite a bit of searching and while I see lots of reports of this issue, I don't see a definitive fix.  If you can make it reproducible then I can investigate, and send it upstream to the go developers if appropriate.

Thanks

Nick I'm going to close this until we see it again :-)  You'll need to use `-v` to see the stats at the moment.  I'm investigating further in #1180 so I'll close this issue.  https://github.com/ncw/rclone/issues/580 I'm going to close this as a duplicate of #580 - please add your support there.  That looks like a bug!  You can find docs for the cat command here

http://rclone.org/commands/rclone_cat/

And check here

http://rclone.org/commands/rclone_check/  Drive supports md5sum - I'm not sure why we would want to store the checksum on a property given that?  @nagualcode open a new issue with a log with -vv please  I suspect there is something scanning the mount.  

Use `lsof | grep Mounts/acd` to try to find out what. Thanks for working that out  > I'm getting a an issue where the upload is getting reset on large files 15gb+ it looks like a connection reset. Any setting I need to add to keep the connection open due to large file size?

Connection resets can be caused by Amazon, your ISP, your router or any bit of networking equipment.

Most likely suspect is your router, though pfsense is very reliable I hear.  Is there another box the pfsense router connects to?

Some ISPs have been known to kill long running TCP connections... @diamondsw BTW rclone writes this warning if it detects a too big file

    "Warning: file %q may fail because it is too big. Use --max-size=%dM to skip large files.  Can you post a log with some of the errors please?
 Glad you got it working.  I think this is probably OS buffering.  When rclone closes the file the OS writes the buffer to disk - this effectively pauses rclone because you are using --transfers=1

If you try --transfers=2 (or higher) then you should see that rclone can be downloading a different file in the mean time while the OS is writing its buffer.

Does that help?  I've merged that - thank you!  Fantastic sketches!

> The current logo looks a little outdated.

I think that is the nicest thing anyone has ever said about the logo ;-)

While I can draw and paint, I really am hopeless at graphic work so your help is much appreciated!

Here are the sketches I originally made when trying to think of logos - maybe that will give you some inspiration

![2013-07-01-200551](https://cloud.githubusercontent.com/assets/536803/24075947/12f17410-0c1e-11e7-8d05-6916c192b77d.jpg)

Of the ones you've done I like the 3rd one the best :-)

Of the current logo, I like the blue colour in the background the most. > Am sorry, If it came out wrong. I didn't meant anything negative :)

No offense taken!  You were perfectly polite :-) I was just making a joke at my own expense for designing the original logo.

> I am using the colors from the previous logo. From your sketches I understood that you are looking for the text rclone in the logo and cloud icon. In the previous explorations I am playing more with the letters r and c.

The text rclone was my original idea but I couldn't make it work. I like your ideas for the r and the C very much.

> Based on your sketches I have come up with few more variations of the logo; If you like any of them we can explore that more. Since you like the 3rd one in the initial image, I did few more variations of it.

They are fab!  I think 6 is a very clever idea with the arrows round the cloud.

I really like your concept of the rC in the cloud.  3.1 has great simplicity and is about square.  I like 3.2 and 3.3 too, but I wonder whether including the text rclone takes away from it?

Maybe you could somehow combine the cloud arrows of 6 with 3.1 ?

What do you think? I like 7 but the letters are too hard to read.  specifically the 'R'.  I'd choose 6 if 7 couldn't be made more legible. :)  Nice work. Sorry for the delay in responding - been a bit hectic!

I love seeing your ideas - fantastic work :-)  I think they are all amazing!  I like the clarity of 6.  7 is a great logo - somewhat abstract but recognisable and I think 6 would work very well as a wide logo.

I thought it would be useful for you to see where the logo is used at the moment

  * [main website](http://rclone.org/) - logo and favicon
  * [github](https://github.com/ncw/rclone/) - logo
  * [forum](https://forum.rclone.org/) - logo along the top
  * [G+](https://plus.google.com/+RcloneOrg)

So some of those would suit a wide image like 6.1 very well and some would suit a squarer image like 6/7.

Though the "rclone" text in 6 will be unreadable if it is very small (eg a favicon), so maybe 7 would be a better choice for that.
 6.3 looks great.  I like 6.2 second.   Sorry for the delay in replying - I've been away.

I like your idea of just using the cloud icon as the favicon and the longer logo incorporating the cloud icon - nice!

Of the ones you've done - I like 6.2 as the width of the line in the cloud matches that of the font.  That said I don't really like that font - I think it looks better with thicker lines as in 6.3 or 6.4. I like the font in 6.4 - what would the cloud look like with lines that think I wonder?

Do you think the logo would look better with the letter and the cloud all the same colour? 6.5 looks great.  Sharp. Great work - I like 6.5 - the angles on the font give it the feeling of arrows pointing your data about!  The two colours look good too.

I did some scribbles in my notebook (which I don't have with me at the moment) with actual arrows at the end of the font strokes, but I don't think it worked very well.  In Copy, check to see if can read more bytes from the reader after the copy has finished.

If this is the case, then make an error - "file size changed while copying" and abort the copy.

This would have detected #902 also.  Are there some docs somewhere as to what rclone would have to do?  You need to get some logging in there to find out what is happening

    rclone move /root/plexorigin/tv_shows Google:/plex/tv_shows >>/tmp/rclone.log 2>&1

At a guess it will be because rclone can't find the home directory because HOME isn't set...  Nice one, thank you!

Can you update the docs (in cmd/mount/mount.go) to describe what happens on the signals please?
 Thanks!  You don't need to update the man page - it is automatically made - I'll drop that chunk from the commit.

I've merged this in 788b6ce821fceff747238e1de5501f4d284abefb - thank you very much for your contribution :-)  Not a bad idea... Why do you need it?  > I was pretty sure that RCLONE invalidated the directory cache one a remote change (copy, move, mkdir, etc)...

If you make the change through the mount it will, otherwise you'll have to wait for the --dir-cache-time  That is a good idea.  Do other FUSE tools do this do you know?

What should rclone do if the mount is in use?  Just do a `fusermount -u -z` and let the user deal with the consequences? rclone mount now does this :-)  I've noticed this too!  Glad it is all sorted :-)  Glad you got it working!  It would be possible with a bit of patching if someone wanted to have a go  rclone downloads files from drive using the downloadUrl which is just a straight forward https fetch, but it does need authorization (I just tried it without).

I don't immediately see a way to generate a templink in the api docs - do you?

[api docs](https://developers.google.com/drive/v2/reference/files)
 Caching the metadata is part of #897  @felixbuenemann is that in the v3 API?  I've only looked at the v2 API  You can use --crypt-show-mapping for this by listing all the files and grepping for the encrypted path - would that work for you? Neither would the forward...

I could do this with an extra command, something like

```
rclone cryptname encrypt crypt: plaintext
rclone cryptname decrypt crypt: ciphertext
```

I'm just not sure it is worth the effort since this isn't an often used option and there is a (kludgey) work-around.

I'd be prepared to be convinced otherwise though!  Any help much appreciated :-)  I've fixed this in

http://beta.rclone.org/v1.35-173-g986a285/ (uploaded in 15-30 mins)

Let me know if it doesn't work for you!

Thanks

Nick Thanks for testing :-)  Interesting idea to use a substitution cipher over an alphabet which is guaranteed to be acceptable for file names. I like the longer path segments!

Some thoughts about your implementation
  * It is a bit ASCII centric - this isn't going to do much for Japanese filenames!
  * I've tried not to make valid extensions on encrypted file names so we don't get content type guessing by the provider.
  * it is possible for dir to be 0 thus not obfuscating the file name at all
  * this should probably be keyed on a bit of the key material otherwise all users using it will have the same file mappings
  * some remotes (eg ACD) are case insensitive OK here are some thoughts about generalising the algorithm to non ASCII

Let's define some unicode character ranges
  * '0'-'9'
  * 'a'-'z'
  * 'A'-'Z'
  * 0xA1-0xFF (http://jrgraphix.net/r/Unicode/00A0-00FF)
  * 0x100 -0x1FF
  * 0x200 -0x2FF
  * ...

So when you come across a character first find out which group it is in.  Then use the length of that group to work out the rotation.

For the >=0x100 groups you would treat each one as a block of 256 chars, so the length would be 256 and you would map back into that block.  So for 0x73A you would use a length of 256 and map to (say) 0x712.

**However** you would only rotate the character if both the character **and** its destination were both printable: unicode.IsPrint](https://golang.org/pkg/unicode/#IsPrint)

This will encode just about every unicode character and avoid the awkward ones and will have the property that it doesn't increase the UTF-8 encoding length which is probably important for size limits of file names.

Hmm, this has the downside that if unicode make some new characters, it could change the definition of IsPrint.  I don't think this is likely though as I think unicode define entire ranges as printable or not.

----

Anyway this is shaping up well - can you add tests and docs please?  (You only need to edit docs in docs/content/crypt.md) > Hmm, how does case sensitivity work with these unicode character sets? Could we end up with a rotation that maps lowercase to uppercase and so doesn't really obfuscate at all?

A good question!  I don't know the answer - I'm not sure it is even defined in the [ACD API docs](https://developer.amazon.com/public/apis/experience/cloud-drive/content/nodes)

A bit of experimentation might be needed!

Drobbox and OneDrive are also case insensitive.

> And I'm not sure how we'd handle filenames that have characters from multiple ranges (eg "A1\x0130\x0230") which would have potentially three different distance calculations.

Each character would have an individual distance calculation is what I was thinking.
 > The problem is that we need to store the distance in the filename so that we can de-obfuscate (eg for "rclone ls"). That's why it's just an obfuscation (easily reversed) and not an encryption.

I don't think it is necessary to store the distance in the file name is it?

Given the use of the key material you could do without the rotation depending on the file name data. Then you just use the same algorithm you use in `obfuscateSegment` and derive the distance when you need it?

The distance you derive would be a 32 bit integer (say), which you would then modulo the different lengths for each character. You could add one to it each character or multiply it by some prime to keep things a bit mixed up.

> Having a different distance for each character would blow the filename length limits more than "standard" does :-)

Indeed!
 >  Now if the new character is not valid we can't just store it untouched because it's possible the unrotate would still create a good character (two potential mappings charX->charY; charY->charY; what do we unrotate charY to?)

I think the solution to this is only do the rotation if valid(srcChar) && valid(dstChar) and do this exact same check in the unrotation valid(dstChar) && valids(srcChar).  I think that works doesn't it?

So given X -> Y under a given rotation

| valid(Y) | valid(X) | rotate |
| -------- | -------- | -------- |
| false | false | false |
| false | true | false |
| true | false | false |
| true | true | true |
 
When we unrotate we are mapping Y -> X so the same truth table applies.

So if either X or Y is invalid we will never rotate or unrotate them. That is a nice clear argument and I'm convinced you are right. 

So if we go with the escape char, we need to pick one which allowed on all cloud systems and oses. Is ! Allowed on windows file names? 

I'm not too worried about the Unicode case folding as it will be rare and I'm not sure the remotes case fold Unicode or just ascii so I think we should ignore it.  Looks like you succeeded! Looking good :-)

I just ran the integration tests with the obfuscate name encryption and they passed which is a good sign!  You can pull this commit 87617bd2247f51af4a950094a180ca7af43bce75 into your branch if you want (or I'll do it later)

Can you squash your commits and do a force push, then I'll do a full code review.

Nearly there!

Nick I have merged this - thank you very much :-)

An excellent first contribution - hopefully there will be many more!

Thanks

Nick  Can you check you are using the latest beta please - that error text doesn't exist in the code any more. Excellent :-)  Things passed to an `--include` filter are shell globs, so can be `*.jpg` or `file[123].txt`.  `[` and `]` are metacharacters.

You probably want to use  `--files-from` which just reads file names from a file without interpreting them as a glob.
  A PR would be much appreciated!  I've managed to confirm this.

It doesn't seem to need a space in the filename.  This seems to go wrong on google drive too
```
$ rclone -vv copyto hello.txt secretacd:non-exist-dir/hello2.txt
$ rclone ls secretacd:non-exist-dir
2017/03/13 12:07:47 Failed to ls: directory not found
$ rclone lsl --max-depth 1 acd:
       54 2017-03-13 11:51:05.755000000 gttofhvjcn6226aka4trpppv4k
```

This also goes wrong without crypt

```
$ rclone -vv copyto hello.txt acd:non-exist-dir/hello2.txt
2017/03/13 12:09:14 DEBUG : rclone: Version "v1.35-169-g320c53e" starting with parameters ["rclone" "-vv" "copyto" "hello.txt" "acd:non-exist-dir/hello2.txt"]
2017/03/13 12:09:15 INFO  : amazon drive root 'non-exist-dir': Modify window not supported
2017/03/13 12:09:15 DEBUG : hello.txt: Couldn't find file - need to transfer
2017/03/13 12:09:16 INFO  : hello.txt: Copied (new)
2017/03/13 12:09:16 INFO  : 
Transferred:      6 Bytes (5 Bytes/s)
Errors:                 0
Checks:                 0
Transferred:            0
Elapsed time:        1.1s
2017/03/13 12:09:16 DEBUG : Go routines at exit 12
2017/03/13 12:09:16 DEBUG : rclone: Version "v1.35-169-g320c53e" finishing with parameters ["rclone" "-vv" "copyto" "hello.txt" "acd:non-exist-dir/hello2.txt"]
$ rclone ls acd:non-exist-dir
2017/03/13 12:09:25 Failed to ls: directory not found
$ rclone lsl --max-depth 1 acd:
        6 2017-03-13 12:09:15.951000000 hello2.txt
```
 This should fix all the above!  If it doesn't, please re-open.

http://beta.rclone.org/v1.35-174-g9b07d32/ (uploaded in 15-30 mins) > runtime: failed to create new OS thread (have 2 already; errno=22)

Looks like the container is limiting the number of threads?  rclone can potentially use lots of threads. @tcf909 can you create a new issue about this please?

Can you describe exactly how you are running the binary in the ticket too please?  Do you see this as being a separate app to rclone?  Or an option or command for rclone?

If it isn't a separate app, then it should probably be a command `rclone webui` say.

To be part of rclone it will need to not use any C components which will rule SQLite out unfortunately. > When I use flags for the server, such as --port or --host, where should I put those? In rclone/cmd/webui/webui.go with the option? Or rclone/fs/config.go?

In `cmd/webui/webui.go` then they will be specific to that command.

I've been looking at boltdb for some other purposes in rclone, so that would be a good choice.  JSON files would be fine too.  I haven't designed the rclone API to be re-used, except from within rclone.  It is possible though - I often write little scripts embedding rclone.

What I'd like to do is fix #633 - you can see some code there.

That would enable you to call rclone from within your binary.

With a bit of thought, it would be possible to make an API so you could do what you want...
  @orlandordiaz rclone copies directory to directory, so to do what you want use

    rclone copy dir1 src:dir1
 rclone doesn't work like cp it works like rsync which syncs directories to directories.

So you need to name the remote directory in the copy, so `copy dir1 remote:src/dir1` if I've understood your example correctly.  You'll need a beta for this to work - see beta.rclone.org - the 1.36 release with this in is imminent!  How repeatable is this?  Can you get it do do it whenever you like?

I tried to reproduce it but I couldn't yet.

----

This looks like `o.Object` was nil in `crypt/crypt.go`

```go
// Remote returns the remote path
func (o *Object) Remote() string {
	remote := o.Object.Remote() // o.Object nil here --------------------
	decryptedName, err := o.f.cipher.DecryptFileName(remote)
``` I think this will probably fix the panic - the rename will fail instead.  Can you give it a test please?

http://beta.rclone.org/v1.35-164-g488353c/ (uploaded in 15-30 mins)

If it doesn't can you attach a log with `-vv` please?  Thanks Thanks for testing :-)  Mixing --include and --exclude on the command line is a bit hit and miss due to the command line parser! This is mentioned in the docs: http://rclone.org/filtering/#repeating-options

Either covert them all to --filter, eg 

    --filter '- Archivio/**' --filter '+ *.[Jj][Pp][Gg]' 

Or use --filter-from which is probably what I'd do  The integration tests I run before a release have pointed this up too!

I see what you mean about the monitor returning 500 with a valid JSON blob.  That is unexpected!  Will have to read the API docs again. I've fixed this here

http://beta.rclone.org/v1.35-165-gf046c00/ (uploaded in 15-30 mins)

Let me know if it works for you! I fixed the parsing of the async status too in

http://beta.rclone.org/v1.35-167-g0faf827/ (uploaded in 15-30 mins) @traynier thanks for testing.

Can you put the above in a new issue please?

I'm having some problems with onedrive and the integration tests which are probably related to the above!  There are a few more of these if you fancy updating them
```
docs/content/about.md:  * Microsoft One Drive
docs/content/changelog.md:    * One Drive
docs/content/changelog.md:    * One Drive
docs/content/changelog.md:      * Add support for Microsoft One Drive
docs/content/commands/rclone.md:  * Microsoft One Drive
docs/content/docs.md:  * [Microsoft One Drive](/onedrive/)
docs/content/onedrive.md:One drive supports SHA1 type hashes, so you can use `--checksum` flag.
docs/content/overview.md:| Microsoft One Drive    | SHA1    | Yes     | Yes              | No              | R         |
docs/content/overview.md:| Microsoft One Drive    | Yes   | Yes  | No [#197](https://github.com/ncw/rclone/issues/197) | No [#197](https://github.com/ncw/rclone/issues/197)    | No [#575](https://github.com/ncw/rclone/issues/575) |
docs/layouts/chrome/navbar.html:                    <li><a href="/onedrive/"><i class="fa fa-windows"></i> Microsoft One Drive</a></li>
``` Are you going to update this PR? I'm happy to leave this one open until you are ready - no rush! I've updated those extra ones too and merged your commit - thank you very much for your contribution.  There is an issue about this here: #337  Just re-edit your remote and refresh the re-authorize now that you have it under the under ID. ```
└─> rclone config
Current remotes:

Name                 Type
====                 ====
xxxxxx

e) Edit existing remote
n) New remote
d) Delete remote
r) Rename remote
c) Copy remote
s) Set configuration password
q) Quit config
e/n/d/r/c/s/q> e
Choose a number from below, or type in an existing value
xxxxxxx
remote> 2
--------------------
xxxxxxx
--------------------
Edit remote
Value "client_id" = "xxxxxxx"
Edit? (y/n)>
y) Yes
n) No
y/n> n
Value "client_secret" = "xxxxxxxxx"
Edit? (y/n)>
y) Yes
n) No
y/n> n
Remote config
```
**_Already have a token - refresh?
y) Yes
n) No
y/n> Y_**

 check to make sure you have the client id set in your rcline.conf.  I had a problem where it wouldn't save if you tried to refresh your token immediately after you added the clientid.  I had to not refresh it the first time through and then go back in and refresh it.
 Yep. It's got to be a bug.  I had the same issue too. I had to quickly save my Comp file and then replace it shortly thereafter again @calisro A bug in rclone or drive?  If rclone what do you think the problem is? Rclone...  It seems to be either two rclone process clobbering the config file or the order to which a single rclone process writes to the config.  In my case I found that when adding a client id to an existing remote that when I walked through the config setup and added the client Id that if refreshed the auth in that same work flow it would succeed but not write the client id!  I found I had to go through the setup without refreshing the auth and exit then go back in and refresh and test the auth.

It is entirely possible I had other rclone process that wrote to the config in between though. Perhaps it would make sense to separate transient auth tokens from the config so that there would be less clobbering? That would also help with losing the config data when space was at zero in the filesystem.    Check this page out: http://rclone.org/remote_setup/  What happens if you do

```
$ env | grep ssh
```

Do you see something like this?

```
SSH_AUTH_SOCK=/tmp/ssh-Sj7JaoojWnEb/agent.3807
```

Do you see your keys if you do `ssh-add -l`?

I wonder If I'm using an outdated way of getting the agent [see blog post](http://blog.ralch.com/tutorial/golang-ssh-connection/) Thanks for working that out.  There must be a special macOS way of doing this...

A bit of searching [brings up this thread](https://github.com/lionheart/openradar-mirror/issues/15361) which has several work-arounds in it, probably the best is described [in this blog post](http://joshbuchea.com/revert-ssh-agent-behavior-pre-macos-sierra/) Would it be helpful if rclone could use a key file directly without using ssh-agent? I meant as a config option you would give a path to a key file, and rclone would load that instead of using the ssh-agent.  rclone could even copy the key file into the config file.

> How do tools like ssh, git, and rsync work? Do they load ssh-agent themselves, and make a best guess as to where the SSH keys are?

They would expect an ssh-agent to be running @kubark42 
> So I did a simple test to try to better understand how this works. When I kill-all ssh-agent and run ssh foo.bar, it loads ssh-agent. And when I open up another terminal, it uses the same ssh-agent. git does the same. So could rclone also launch ssh-agent and get keys in the same manner?

I wonder where that behaviour of launching ssh-agent comes from because it certainly isn't standard ssh behaviour on linux.  I launch my ssh-agent once and I have to type in a very long password - it would be might annoying to have to do that on every rclone invocation!

I think this must be something to do with apple's integration of ssh keys into the secure keychain - that is what the `ssh-add -K` does in @howardm 's writeup above.  So I guess ssh-agent is just started as a shim for the secure keychain.  This must be OS X specific behaviour...

My theory is that this is done by the standard library.  When I cross compile rclone (which I do for the releases) it doesn't use the stdlib so you don't get this behaviour.  However if you compile rclone from source directly on OS X you will get this behaviour.

Can anyone confirm that?  If you need help compiling rclone from source there are some [brief instructions in the docs](http://rclone.org/install/#install-from-source).

If that is the case then I can look at getting the travis build to build the OSX binary... @kubark42 thanks for trying the rclone built from source. So after you've done `ssh-add -K` it then works correctly, starting and stopping an ssh-agent - is that right?  That sounds like it is working properly.

re Windows: interesting!  Can you make a separate issue with that please.  And from a brief look a the code, I think the terraform fix will work for rclone too. I've updated [the docs](https://github.com/ncw/rclone/blob/master/docs/content/sftp.md#ssh-agent-on-macos) with a combination of @diamondsw  and @howardm 's suggestions.

I've implemented ssh key support in #1494 

So I'm not sure what else I can do - any ideas? I'm going to close this now as the docs are updated and we now have ssh key support.  I searched other issues but I wasn't 100% sure if they were similar enough to bump them.  So here is a new one that can be tied together.  

I have two mounts.  1 GDrive and 1 ACD.  I push the same content with rclone itselv to both via mounts.  I get lots of errors on both.  The errors are more plentiful than the successes. :)

So the path looks like this:

local volume-> rclone copy /data/Media1 /data/Media3  where Media1 is local content and Media3 is a gdrive crypt mount.
local volume-> rclone copy /data/Media1 /data/Media2  where Media1 is local content and Media2 is a ACD crypt mount.

This is an example of the log output from ACD:

```
2017/03/06 07:30:02 INFO  : Videos/Movies/Movie1/Movie1.nfo: Copied (new)
2017/03/06 07:30:03 INFO  : Videos/Series/Series2/Season.1/.actors/Jason_Watkins.jpg: Copied (new)
2017/03/06 07:30:07 NOTICE: Videos/Series/Series3/Season.2/Series3.S02E03.720p.HDTV.x264-AVS[PRiME]/Torrent Downloaded From 1337x.to.txt: Removing partially written file on error: close /data/Media2/Videos/Series/Series3/Season.2/Series3.S02E03.720p.HDTV.x264-AVS[PRiME]/Torrent Downloaded From 1337x.to.txt: input/output error
2017/03/06 07:30:07 ERROR : Videos/Series/Series3/Season.2/Series3.S02E03.720p.HDTV.x264-AVS[PRiME]/Torrent Downloaded From 1337x.to.txt: Failed to remove partially written file: remove /data/Media2/Videos/Series/Series3/Season.2/Series3.S02E03.720p.HDTV.x264-AVS[PRiME]/Torrent Downloaded From 1337x.to.txt: no such file or directory
2017/03/06 07:30:07 ERROR : Videos/Series/Series3/Season.2/Series3.S02E03.720p.HDTV.x264-AVS[PRiME]/Torrent Downloaded From 1337x.to.txt: Failed to copy: close /data/Media2/Videos/Series/Series3/Season.2/Series3.S02E03.720p.HDTV.x264-AVS[PRiME]/Torrent Downloaded From 1337x.to.txt: input/output error
2017/03/06 07:30:07 NOTICE: Videos/Series/Series4/Season.2/Series4.S02E10.720p.webrip.h264-thumb.jpg: Removing partially written file on error: close /data/Media2/Videos/Series/Series4/Season.2/Series4.S02E10.720p.webrip.h264-thumb.jpg: input/output error
2017/03/06 07:30:07 ERROR : Videos/Series/Series4/Season.2/Series4.S02E10.720p.webrip.h264-thumb.jpg: Failed to remove partially written file: remove /data/Media2/Videos/Series/Series4/Season.2/Series4.S02E10.720p.webrip.h264-thumb.jpg: no such file or directory
2017/03/06 07:30:07 ERROR : Videos/Series/Series4/Season.2/Series4.S02E10.720p.webrip.h264-thumb.jpg: Failed to copy: close /data/Media2/Videos/Series/Series4/Season.2/Series4.S02E10.720p.webrip.h264-thumb.jpg: input/output error
2017/03/06 07:30:08 NOTICE: Videos/Series/Series4/tvshow.nfo: Removing partially written file on error: close /data/Media2/Videos/Series/Series4/tvshow.nfo: input/output error
2017/03/06 07:30:08 ERROR : Videos/Series/Series4/tvshow.nfo: Failed to remove partially written file: remove /data/Media2/Videos/Series/Series4/tvshow.nfo: no such file or directory
2017/03/06 07:30:08 ERROR : Videos/Series/Series4/tvshow.nfo: Failed to copy: close /data/Media2/Videos/Series/Series4/tvshow.nfo: input/output error
2017/03/06 07:30:08 NOTICE: Videos/Series/Series4/Season.2/Series4.S02E03.webrip.h264-thumb.jpg: Removing partially written file on error: close /data/Media2/Videos/Series/Series4/Season.2/Series4.S02E03.webrip.h264-thumb.jpg: input/output error
2017/03/06 07:30:08 ERROR : Videos/Series/Series4/Season.2/Series4.S02E03.webrip.h264-thumb.jpg: Failed to remove partially written file: remove /data/Media2/Videos/Series/Series4/Season.2/Series4.S02E03.webrip.h264-thumb.jpg: no such file or directory
2017/03/06 07:30:08 ERROR : Videos/Series/Series4/Season.2/Series4.S02E03.webrip.h264-thumb.jpg: Failed to copy: close /data/Media2/Videos/Series/Series4/Season.2/Series4.S02E03.webrip.h264-thumb.jpg: input/output error
2017/03/06 07:32:51 INFO  : Videos/Series/Series5/Season.6/Series5.S06E04.720p.hdtv.h264.mkv: Copied (new)
2017/03/06 07:34:58 INFO  :
Transferred:   3.713 GBytes (12.675 MBytes/s)

```

These things retry and sometimes they are successful and sometimes they are not.  But overall there a LOT of errors/retries.  

This is hte log from the mount itself
```
2017/03/06 07:30:07 ERROR : ....... : WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9ea92930-0268-11e7-ad9f-43c113caf993\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 07:30:07 ERROR : ....... : Dir.Remove error: no such file or directory
2017/03/06 07:30:07 ERROR : ....... : WriteFileHandle.Release error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9ecd2bf3-0268-11e7-99a8-477cb4e7d4b9\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 07:30:07 ERROR : ....... _HIDDEN~: WriteFileHandle.Release error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9eedfaae-0268-11e7-a6cf-5d645af872ff\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 07:30:07 ERROR : ....... : WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9f0a0daa-0268-11e7-9179-fbbb7ecb6809\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 07:30:07 ERROR : ....... : Dir.Remove error: no such file or directory
2017/03/06 07:30:08 ERROR :....... : WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9f28b982-0268-11e7-ba9f-795965b51193\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 07:30:08 ERROR : ....... : Dir.Remove error: no such file or directory
2017/03/06 07:30:08 ERROR :....... : WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9f1690d3-0268-11e7-8b59-6db6d047a573\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 07:30:08 ERROR : ....... : Dir.Remove error: no such file or directory
2017/03/06 07:38:24 ERROR : ....... : WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"c639fc9a-0269-11e7-8580-85f63a4c128e\",\"message\":\"Rate exceeded\",\"code\":\"\"}"

```


I realize these errors are out of rclone's control but something seems fishy with how poorly the mount performs errorwise compared to direct rclone copies to ACDgdrive.  The actual BW usage is great.  Its just the failed partial transfers retries thats the problem.  I'd say 3/4 of my bandwidth is wasted with half uploaded files and then removals/tries sometimes ultimately failing and sometimes succeeding within the 3 retry allowances.  

I can get a debug output if it would help.  This just happened to be what I logged with regular logging for now.
 To add a little more to this.  I just reran it and I had the same issues but after failing twice it never continued yet stats continued.  I've seen my scripts hanging before but I wasn't entirely sure.  Now I can see it happen in the log.  I verified the source /data/Media1 is fine, mounted and still working.
```

2017/03/06 11:14:25 ERROR : Attempt 1/3 failed with 1 errors and: close /data/Media2/Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: input/output error
2017/03/06 11:14:25 INFO  : .unionfs/Videos/Series/SIX/Season.1/SIX.S01E08.720p.BluRay.x264-DEMAND[ettv]/.actors/Barry_Sloane.jpg_HIDDEN~: Copied (new)
2017/03/06 11:14:27 INFO  : Local file system at /data/Media2: Waiting for checks to finish
2017/03/06 11:14:27 INFO  : Local file system at /data/Media2: Waiting for transfers to finish
2017/03/06 11:15:03 INFO  : 
Transferred:   3.401 GBytes (11.608 MBytes/s)
Errors:                 0
Checks:            169302
Transferred:            8
Elapsed time:        5m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv: 62% done, 13.854 MBytes/s, ETA: 23s

2017/03/06 11:16:08 NOTICE: Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: Removing partially written file on error: close /data/Media2/Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: input/output error
2017/03/06 11:16:08 ERROR : Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: Failed to remove partially written file: remove /data/Media2/Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: no such file or directory
2017/03/06 11:16:08 ERROR : Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: Failed to copy: close /data/Media2/Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: input/output error
2017/03/06 11:16:08 ERROR : Attempt 2/3 failed with 1 errors and: close /data/Media2/Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: input/output error
2017/03/06 11:16:10 INFO  : Local file system at /data/Media2: Waiting for checks to finish
2017/03/06 11:16:10 INFO  : Local file system at /data/Media2: Waiting for transfers to finish
2017/03/06 11:20:03 INFO  : 
Transferred:   3.725 GBytes (6.357 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       10m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 410104h59m9s

2017/03/06 11:25:03 INFO  : 
Transferred:   3.725 GBytes (4.238 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       15m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 11:30:03 INFO  : 
Transferred:   3.725 GBytes (3.178 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       20m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 11:35:03 INFO  : 
Transferred:   3.725 GBytes (2.543 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       25m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 11:40:03 INFO  : 
Transferred:   3.725 GBytes (2.119 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       30m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 11:45:03 INFO  : 
Transferred:   3.725 GBytes (1.816 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       35m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 11:50:03 INFO  : 
Transferred:   3.725 GBytes (1.589 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       40m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 11:55:03 INFO  : 
Transferred:   3.725 GBytes (1.413 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       45m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:00:03 INFO  : 
Transferred:   3.725 GBytes (1.271 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       50m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:05:03 INFO  : 
Transferred:   3.725 GBytes (1.156 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:       55m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:10:03 INFO  : 
Transferred:   3.725 GBytes (1.059 MBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:      1h0m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:15:03 INFO  : 
Transferred:   3.725 GBytes (1001.410 kBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:      1h5m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:20:03 INFO  : 
Transferred:   3.725 GBytes (929.881 kBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:     1h10m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:25:03 INFO  : 
Transferred:   3.725 GBytes (867.890 kBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:     1h15m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:30:03 INFO  : 
Transferred:   3.725 GBytes (813.646 kBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:     1h20m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:35:03 INFO  : 
Transferred:   3.725 GBytes (765.784 kBytes/s)
Errors:                 0
Checks:            253957
Transferred:            8
Elapsed time:     1h25m0s
Transferring:
 * ...eason.6/Homeland.S06E07.720p.hdtv.h264.mkv:  0% done, 0 Bytes/s, ETA: 0s

2017/03/06 12:40:03 INFO  : 
Transferred:   3.725 GBytes (723.188 kB
```


mount log
```
2017/03/06 11:10:09 ERROR : .unionfs/Videos/Series/SIX/Season.1/SIX.S01E08.720p.BluRay.x264-DEMAND[ettv]/.actors/Barry_Sloane.jpg_HIDDEN~: WriteFileHandle.Release error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"5b825cf5-0287-11e7-a72a-19cdf1f8b2c1\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 11:11:57 ERROR : Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"9bde0ae4-0287-11e7-a287-29e642bf8c6f\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
2017/03/06 11:16:08 ERROR : Videos/Series/Homeland/Season.6/Homeland.S06E07.720p.hdtv.h264.mkv: WriteFileHandle.Flush error: HTTP code 429: "429 Too Many Requests": response body: "{\"logref\":\"315b3ce0-0288-11e7-ba5a-9b746851e4c3\",\"message\":\"Rate exceeded\",\"code\":\"\"}"
```

 I'm pretty sure the problem is the mount itself.I actually tried to run another sync on that rclone mount and it seems to be hanging and not doing anything.  Interestingly though, I can browse the mount and even cat a nfo files.  It seems to be 'partially' working.  In order to actually sync, I had to unmount and remount it.  More interesting is when I unmounted it with fusermount, i had to lazy umount (umount -l) to get it to stop.  I then had to manually kill the rclone mount command:
```

└─> ps -ef | grep rclone
robert   22537 28191  0 13:29 pts/30   00:00:00 grep --color=auto rclone
root     29950 29945  1 10:58 pts/30   00:02:27 /usr/sbin/rclone -v --log-file /data/log/rmount-acd.log --config=/home/robert/.rclone.conf mount robacd-cryptp:Media/ /data/Media2 --stats 0 --allow-other --checkers=32 --transfers=32 --no-modtime --timeout 15m --max-read-ahead 1024k --default-permissions --gid 1002 --uid 118 --umask 002 --dir-cache-time 48h
┌[ ✓ robert [/data/bin] $
└─> sudo kill 29950
[sudo] password for robert: 
┌[ ✓ robert [/data/bin] $
└─> /data/bin/cloudfs.sh: line 32: 29950 Terminated              /usr/sbin/rclone -v --log-file /data/log/rmount-acd.log --config="/home/robert/.rclone.conf" mount robacd-cryptp:Media/ $ACD_RCLONE --stats 0 --allow-other --checkers=32 --transfers=32 --no-modtime --timeout 15m --max-read-ahead 1024k --default-permissions --gid $gid --uid $uid --umask $UMASK --dir-cache-time $CACHE

``` The main problem is that uploading files via a mount doesn't do retries.  That won't be possible until #711 is in.

That should solve the unreliability of uploads (cross fingers!)

I'd recommend uploading stuff with rclone copy, not via the mount until then. Then i can close this in favor of #711 rather than having yet another ticket.  @ncw Shouldn't the lack of I/O be timed out at 5 minutes rather than the hanging?  I realize there are issues with retries but my main issue here is the hang condition. The lack of IO is probably caused by 

    --acd-upload-wait-per-gb duration   Additional time per GB to wait after a failed complete upload to see if it appears. (default 3m0s)

So for your 3.75 GB file rclone waits 11 minutes for it to appear. Yes but I've had it wait for 48 hours with no I/O.  It nevers stops.... Even with the failures I would expect it to stop at somepoint.  It seems that when rclone is pointed at a local filesystem (which is actually a rclone mount) it will wait indefinately for the filesystem to return a "okay". It should timeout. The timeouts only apply to network interactions, if you do a local <-> local there will be no timeouts :-(

However the mount should be doing timeouts.  It is possible there is a bug somewhere - can you make me a reproducer? That makes sense.  It reproduces constantly.  I can get you what you need.  Do you need a debug output from the mount?  Anything else? I'll set up whatever you think will help on the mount. It happens for both Amazon and Google Drive so it isn't specific to a cloud type A series of commands to execute to make it happen on my machine would be perfect! ```
 rclone -V
rclone v1.35-162-gb2a4ea9β
```

The problem is intermittent.  I can't reproduce it on demand but it does reproduce consistently enough.  Lowering the retries helps make it happen more frequently.  I've decreased the transfers also just to make it easy to identify but more transfer threads means a higher chance of happening more frequently obviously.

Create a mount:
` rclone -vvv --log-file ~/p.log --checkers 1 --transfers 1 mount robacd-cryptt:/ ~/p --retries 1 --timeout 1m&`

Sync to the mount:
`rclone -vvv --stats 30s --log-file ~/s.log --checkers 1 --transfers 1 copy /data/Media1 ~/p`

Once we do that, we'll see a number copies and at some point we'll see a failure (which is somewhat expected at this point and those aren't really my primary concern with this ticket).  When we see the failure attempts exceed the retries (one in this case but i've increased mine to 10 on my machine), the copy operation will sit there and hang waiting on the mount to error.  It seems that on the mount that copy thread is 'stuck' after a failure.

In the 'stats' in the copy log we'll see at least one file just stop transferring.  Look at 'Transferred:   134.298 MBytes'  like this.  It will never change and will continue indefinitely.  
```

2017/03/16 12:29:55 NOTICE: AudioBooks/1989 - A Time to Kill/Chapter 01.mp3: Removing partially written file on error: close /home/robert/p/AudioBooks/1989 - A Time to Kill/Chapter 01.mp3: input/output error
2017/03/16 12:29:55 ERROR : AudioBooks/1989 - A Time to Kill/Chapter 01.mp3: Failed to remove partially written file: remove /home/robert/p/AudioBooks/1989 - A Time to Kill/Chapter 01.mp3: no such file or directory
2017/03/16 12:29:55 ERROR : AudioBooks/1989 - A Time to Kill/Chapter 01.mp3: Failed to copy: close /home/robert/p/AudioBooks/1989 - A Time to Kill/Chapter 01.mp3: input/output error
2017/03/16 12:30:04 NOTICE: AudioBooks/1989 - A Time to Kill/Chapter 02.mp3: Removing partially written file on error: close /home/robert/p/AudioBooks/1989 - A Time to Kill/Chapter 02.mp3: input/output error
2017/03/16 12:30:04 ERROR : AudioBooks/1989 - A Time to Kill/Chapter 02.mp3: Failed to remove partially written file: remove /home/robert/p/AudioBooks/1989 - A Time to Kill/Chapter 02.mp3: no such file or directory
2017/03/16 12:30:04 ERROR : AudioBooks/1989 - A Time to Kill/Chapter 02.mp3: Failed to copy: close /home/robert/p/AudioBooks/1989 - A Time to Kill/Chapter 02.mp3: input/output error
2017/03/16 12:30:08 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 03.mp3: Copied (new)
2017/03/16 12:30:11 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 04.mp3: Copied (new)
2017/03/16 12:30:12 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 05.mp3: Copied (new)
2017/03/16 12:30:13 INFO  : AudioBooks/The Subtle Art of Not Giving a F ck by Mark Manson.m4b: Updated modification time in destination
2017/03/16 12:30:13 DEBUG : AudioBooks/The Subtle Art of Not Giving a F ck by Mark Manson.m4b: Unchanged skipping
2017/03/16 12:30:13 DEBUG : AudioBooks/find her.mp3: Modification times differ by 9469h52m27.916930185s: 2016-02-15 21:35:28.507069815 -0500 EST, 2017-03-16 12:27:56.424 -0400 EDT
2017/03/16 12:30:14 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 06.mp3: Copied (new)
2017/03/16 12:30:17 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 07.mp3: Copied (new)
2017/03/16 12:30:18 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 08.mp3: Copied (new)
2017/03/16 12:30:20 INFO  : AudioBooks/1989 - A Time to Kill/Chapter 09.mp3: Copied (new)
2017/03/16 12:30:21 INFO  : 
Transferred:   134.298 MBytes (1.119 MBytes/s)
Errors:                 4
Checks:                 3
Transferred:            8
Elapsed time:        2m0s
Checking:
 * AudioBooks/find her.mp3
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 3.902 MBytes/s, ETA: 2s

2017/03/16 12:30:51 INFO  : 
Transferred:   134.298 MBytes (916.788 kBytes/s)
Errors:                 4
Checks:                 3
Transferred:            8
Elapsed time:       2m30s
Checking:
 * AudioBooks/find her.mp3
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 540.338 kBytes/s, ETA: 18s

2017/03/16 12:31:21 INFO  : 
Transferred:   134.298 MBytes (763.993 kBytes/s)
Errors:                 4
Checks:                 3
Transferred:            8
Elapsed time:        3m0s
Checking:
 * AudioBooks/find her.mp3
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 73.072 kBytes/s, ETA: 2m13s

2017/03/16 12:31:24 INFO  : AudioBooks/find her.mp3: Updated modification time in destination
2017/03/16 12:31:24 DEBUG : AudioBooks/find her.mp3: Unchanged skipping
2017/03/16 12:31:51 INFO  : 
Transferred:   134.298 MBytes (654.854 kBytes/s)
Errors:                 4
Checks:                 4
Transferred:            8
Elapsed time:       3m30s
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 9.882 kBytes/s, ETA: 16m27s

2017/03/16 12:32:21 INFO  : 
Transferred:   134.298 MBytes (572.998 kBytes/s)
Errors:                 4
Checks:                 4
Transferred:            8
Elapsed time:        4m0s
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 1.336 kBytes/s, ETA: 2h1m45s

2017/03/16 12:32:51 INFO  : 
Transferred:   134.298 MBytes (509.332 kBytes/s)
Errors:                 4
Checks:                 4
Transferred:            8
Elapsed time:       4m30s
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 185 Bytes/s, ETA: 15h0m21s

2017/03/16 12:33:21 INFO  : 
Transferred:   134.298 MBytes (458.399 kBytes/s)
Errors:                 4
Checks:                 4
Transferred:            8
Elapsed time:        5m0s
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 25 Bytes/s, ETA: 110h57m44s

2017/03/16 12:33:51 INFO  : 
Transferred:   134.298 MBytes (416.728 kBytes/s)
Errors:                 4
Checks:                 4
Transferred:            8
Elapsed time:       5m30s
Transferring:
 * ...Books/1989 - A Time to Kill/Chapter 10.mp3: 29% done, 3 Bytes/s, ETA: 820h30m56s

```

Once that happens, I can't even terminate the rclone copy with a 'ctrl-c'.  I must kill it "-9" and then it is left defunct.  






 Yes, mine is consistent in the failures.  Even right now I can see the hang.  Other threads process fine but slowly they all hang up.

```

2017/03/29 08:54:22 INFO  : 
Transferred:   1.414 GBytes (4.825 MBytes/s)
Errors:                 9
Checks:              5274
Transferred:           58
Elapsed time:        5m0s
Checking:
 * Videos/Movies/Aladdin.(1992).720P.BRRip.x264/Aladdin.(1992).720P.BRRip.x264.nfo
Transferring:
 *                    Pictures/2017/DSC_0121.JPG:  4% done, 0 Bytes/s, ETA: 170479h45m23s
 *                    Pictures/2017/DSC_0122.JPG:  7% done, 0 Bytes/s, ETA: 43852h20m37s
 * ...ason.2/Blindspot.S02E16.720p.hdtv.h264.mkv: 77% done, 2.499 MBytes/s, ETA: 1m16s
 * ...tury.Women.(2016).720p.bluray.h264.aac.mp4: 79% done, 2.507 MBytes/s, ETA: 1m16s

2017/03/29 08:55:43 ERROR : Videos/Series/Blindspot/Season.2/Blindspot.S02E16.720p.hdtv.h264.mkv: Failed to copy: chtimes /data/Media2/Videos/Series/Blindspot/Season.2/Blindspot.S02E16.720p.hdtv.h264.mkv: no such file or directory
2017/03/29 08:55:45 INFO  : Videos/Movies/20th.Century.Women.(2016).720p.bluray.h264.aac/20th.Century.Women.(2016).720p.bluray.h264.aac.mp4: Copied (new)
2017/03/29 08:55:47 INFO  : .unionfs/Videos/Series/Game.of.Thrones/Season.6/Game.of.Thrones.S06E03.1080p.HDTV.x264-BATV[ettv]-thumb.jpg_HIDDEN~: Copied (new)
2017/03/29 08:55:48 INFO  : Videos/Movies/20th.Century.Women.(2016).720p.bluray.h264.aac/.actors/Annette_Bening.jpg: Copied (new)
2017/03/29 08:59:22 INFO  : 
Transferred:   2.859 GBytes (4.879 MBytes/s)
Errors:                10
Checks:              5275
Transferred:           61
Elapsed time:       10m0s
Transferring:
 *                    Pictures/2017/DSC_0121.JPG:  4% done, 0 Bytes/s, ETA: 1217378h48m37.349638144s
 *                    Pictures/2017/DSC_0122.JPG:  7% done, 0 Bytes/s, ETA: 1707882h59m7.715026944s
 * ...le.Lies.S01E05.1080p.webrip.h264.dd5.1.mkv:  9% done, 2.501 MBytes/s, ETA: 34m7s
 * ...on.5/Bates.Motel.S05E05.720p.hdtv.h264.mkv: 67% done, 2.494 MBytes/s, ETA: 1m51s

2017/03/29 09:01:35 INFO  : Videos/Series/Bates.Motel/Season.5/Bates.Motel.S05E05.720p.hdtv.h264.mkv: Copied (new)
2017/03/29 09:04:22 INFO  : 
Transferred:   4.324 GBytes (4.919 MBytes/s)
Errors:                10
Checks:              5275
Transferred:           62
Elapsed time:       15m0s
Transferring:
 *                    Pictures/2017/DSC_0121.JPG:  4% done, 0 Bytes/s, ETA: 0s
 *                    Pictures/2017/DSC_0122.JPG:  7% done, 0 Bytes/s, ETA: 0s
 * ...eason.2/Billions.S02E05.720p.hdtv.h264.mkv: 48% done, 2.506 MBytes/s, ETA: 2m59s
 * ...le.Lies.S01E05.1080p.webrip.h264.dd5.1.mkv: 23% done, 2.499 MBytes/s, ETA: 28m45s

```

These two lines will never end and never abort.  The only thing that can be done is kill -9 on the sync and the calling shell script. 
 *                    Pictures/2017/DSC_0121.JPG:  4% done, 0 Bytes/s, ETA: 0s
 *                    Pictures/2017/DSC_0122.JPG:  7% done, 0 Bytes/s, ETA: 0s

The problem seems to be in the mount itself since the mount is the one exceeding the retries and hten hanging my rclone sync that I have pointing to the mount.   @djsecrist  Is your issue with the mount or a sync directly?  This issue is with the mount and hangs after the retries are expired.

@ncw Is there anything I can get you further to help?  The issue is pretty consistent and hte test case should be able to demonstrate it but im happy to provide anything to help you.
  That wouldn't be hard...

I guess you could use ETags also...
  What file operations are you doing via mount?  Are you doing seeking?

I've found acd performance to be very variable even with just using `rclone copy`.  I'm unclear exactly what OpenDirectIO does for fuse - can you find some docs?  I don't know whether it has the same limitations as O_DIRECT for instance.

It would be relatively easy to implement as an option -just another flag for Open.  I'm using the new beta and i'm seeing some errors intermittantlly in once of my processes that hasn't changed for a long time.  (same commands/remote).  This just started in one of hte new betas.  I see it on logs as far back as v1.35-137-ge2f0feeβ but i don't have logs going back further.

v1.35-153-g1cc58e4β

```
rclone --bwlimit '07:00,6M 23:00,10M' '--include=/*/*/*' move -v /data/cams robacd:/cams/
...
...
...
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x6ff3eb]

goroutine 3304 [running]:
github.com/ncw/rclone/fs.(*asyncReader).Read(0xc426e1cea0, 0xc427472000, 0x8000, 0x8000, 0xc400000008, 0xc427253d70, 0x40d5f8)
        /home/travis/gopath/src/github.com/ncw/rclone/fs/buffer.go:132 +0x4b
github.com/ncw/rclone/fs.(*Account).read(0xc42730e4d0, 0x7f3bbd8551f0, 0xc426e1cea0, 0xc427472000, 0x8000, 0x8000, 0x0, 0x0, 0xc427253e28)
        /home/travis/gopath/src/github.com/ncw/rclone/fs/accounting.go:437 +0xa5
github.com/ncw/rclone/fs.(*Account).Read(0xc42730e4d0, 0xc427472000, 0x8000, 0x8000, 0x0, 0x0, 0x0)
        /home/travis/gopath/src/github.com/ncw/rclone/fs/accounting.go:467 +0x160
io.(*multiReader).Read(0xc425cd19c0, 0xc427472000, 0x8000, 0x8000, 0x8000, 0x0, 0x0)
        /home/travis/.gimme/versions/go1.8.linux.amd64/src/io/multi.go:26 +0xcf
io.copyBuffer(0x11a9740, 0xc425cd1d60, 0x11a9600, 0xc425cd19c0, 0xc427472000, 0x8000, 0x8000, 0x0, 0x0, 0x0)
        /home/travis/.gimme/versions/go1.8.linux.amd64/src/io/io.go:390 +0x100
io.Copy(0x11a9740, 0xc425cd1d60, 0x11a9600, 0xc425cd19c0, 0x17, 0x11a9740, 0xc425cd1d60)
        /home/travis/.gimme/versions/go1.8.linux.amd64/src/io/io.go:360 +0x68
github.com/ncw/rclone/vendor/github.com/ncw/go-acd.(*NodesService).putOrOverwrite.func1(0xc420658070, 0xc425eef2c0, 0x55, 0xc4250ee8d0, 0xc426ec8d20, 0xc420233812, 0x17, 0x11a9600, 0xc425cd19c0)
        /home/travis/gopath/src/github.com/ncw/rclone/vendor/github.com/ncw/go-acd/nodes.go:649 +0xf2
created by github.com/ncw/rclone/vendor/github.com/ncw/go-acd.(*NodesService).putOrOverwrite
        /home/travis/gopath/src/github.com/ncw/rclone/vendor/github.com/ncw/go-acd/nodes.go:654 +0x3d2

``` Thanks for reporting this.

It looks like a mistake in the buffer logic I introduced recently...

Looks like I'll have to push back the release until I've reproduced it and fixed it :-( I believe I have fixed this - would appreciate you testing it though.

http://beta.rclone.org/v1.35-162-gb2a4ea9/ (uploaded in 15-30 mins)

Please re-open if it doesn't fix the problem!

Thanks

Nick I'll give it a go. I haven't had a recurrence.  So that's good.  What happens if you do

    rclone ls d:/test

What do you see there?  Excellent debugging - thanks.

What if you try with the `--old-sync` flag? I expect that will work. I'd like to find out why it isn't working... (I'm planning to remove the --old-sync flag eventually).

I've tried to replicate the problem but I haven't been able to.

How did you create that directory structure?  Could you make a bat script which creates the structure for me?

Is your disk in some kind of special encoding?  Or are some of the unicode characters not normalised properly?  That might explain it... I've managed to replicate the problem with the help of your .bat file - thanks!  I'll post more when I've worked out what is going on and have a beta for you to try! Looks like this **is** a unicode normalisation problem.

What is happening is that the directory objects (unlike the file objects) don't contain the un-normalised path.

This is going wrong because rclone now recurses through the directories in the sync code, rather than letting the remote do it.

There are two ways of fixing this

  1. store the un-normalised path in the directory object
    * change all the listing code to take a directory object (or be a directory object method)
  2. store a cache of un-normalised to normalised listings.
    * we do the equivalent of this using the dircache infrastructure
    * the other way would be a better way of doing this in general and would get rid of the need for dircache (mostly)

Either way that is quite an involved fix :-(

Is this something to do with the `chcp 65001` in your batch file.  What does that do?

This is also going to be a serious issue for macOS which loves un-normalised unicode.


```
C:\Users\Dev>x:\go\src\github.com\ncw\rclone\rclone.exe -vv copy t2 t3 --retries 1
2017/03/07 16:48:07 DEBUG : rclone: Version "v1.35-DEV" starting with parameters ["x:\\go\\src\\github.com\\ncw\\rclone\\rclone.exe" "-vv" "copy" "t2" "t3" "--retries" "1"]
2017/03/07 16:48:07 INFO  : Local file system at \\?\C:\Users\Dev\t3: Modify window is 100ns
2017/03/07 16:48:07 NOTICE: Local file system at \\?\C:\Users\Dev\t2: Cleaned "-r?'a´o¨" -> "-r'áö"  #### here is rclone normalising the path
2017/03/07 16:48:07 NOTICE: stat error is: GetFileAttributesEx \\?\C:\Users\Dev\t2\-r'áö: The system cannot find the file specified.
2017/03/07 16:48:07 INFO  : Local file system at \\?\C:\Users\Dev\t3: Waiting for checks to finish
2017/03/07 16:48:07 INFO  : Local file system at \\?\C:\Users\Dev\t3: Waiting for transfers to finish
2017/03/07 16:48:07 ERROR : Attempt 1/1 failed with 0 errors and: error reading source directory "-r'áö": directory not found
2017/03/07 16:48:07 Failed to copy: error reading source directory "-r'áö": directory not found

C:\Users\Dev>dir t2\-r'áö ##### normalised unicode here
 Volume in drive C has no label.
 Volume Serial Number is 5CD1-D250

 Directory of C:\Users\Dev\t2

File Not Found

C:\Users\Dev>dir t2
 Volume in drive C has no label.
 Volume Serial Number is 5CD1-D250

 Directory of C:\Users\Dev\t2

07/03/2017  15:47    <DIR>          .
07/03/2017  15:47    <DIR>          ..
07/03/2017  15:50    <DIR>          -r?'a´o¨  ##### un-normalised unicode here
               0 File(s)              0 bytes
               3 Dir(s)   6,261,989,376 bytes free

C:\Users\Dev>
```
 I've fixed this here

http://beta.rclone.org/v1.35-176-g5355881/ (uploaded in 15-30 mins)

Please re-open if you are still having problems!
 thanks for testing :-) So it works with Windows now, but not with Linux is that correct?

As it is the same problem I'll open this issue again! Can you so what you did before and make some shell commands I can run to reproduce?  I plan to hook into the google drive changes API eventually (got some re-org to do first) which should help.  I see no way to do that without a redownload/upload.    > Could mount be enhanced to handle upload same way rclone copy/move would do eg never fail and keep retrying :).

That is what is required, but it is going to take making a local copy of the file which I'll do in #711  Do you mean in the stats output? The one which tells you which files are transferring?  Or in the log lines that print what has been copied? Is this a duplicate of #1206 ? Do you mean the same as #1206 ? Can you attach a screenshot of the output you would like to change? OK, that is what is being proposed in #1206 so I'll close this one in favour of that one.

Can you subscribe to that issue?

Thanks  Yes this would imply building a complete map of transfers first which would potentially use a lot more memory. With rclone's new sync method it will order the transfes by name on a per directory basis - is that helpful?

so when transfering any given directory, it will start the transfers in alphabetical order for that directory.  Rclone may be looking at up to `--checkers` directories at once though.  I believe dedupe needs to have the same name, same path, sane md5.  Your names are different so they are different.  I don't think this is an rclone problem.  It is possible you've hit some sort of quota and giving it a rest and trying again later might help.  Or it might just be ACD going through one of its unreliable phases.  Just a side-bar in the meantime.  you can do this then back that file up.  

getfacl -R / >permissions.facl

setfacl --restore=permissions.facl


 I think this may happen at some point, but it will involve quite a lot of metadata plumbing...

@calisro nice work-around Setfacl --restore should be there.  Check the man page  I've merged this in 84d4d7f9d9dd5b8bf990b289c63bebe8251a3489 - thank you very much!

I shuffled stuff about so that the message comes before the manual configuration.  The current scheme of `http://localhost.rclone.org:53682/` seems to work.

Is that because I added it in the past and the rules have now changed?

Do you have a pointer to any docs stating the rules?

I just looked in the log - I added that scheme specifically for onedrive so I'm reluctant to change it without some official docs!

 OK thanks for doing a bit of research.

I think I can add a second redirect URL to the onedrive app so I can keep the old rclone versions working.

I'll have to do a bit of testing with old and new version. I've added a second redirect URL to rclone and tested it works with the old and new methods, so I've merged this now in 8350544092f746e367c3b8a7243b22491e6c461e - thank you very much :-)

  Yes, this is because I haven't implemented setting attributes yet.  It should be straight forward enough though. Copy the data up using `rclone copy` or `rclone sync` instead which has much better retry logic and will preserve the mod time right now. This has been done in this beta

http://beta.rclone.org/v1.36-06-g4dc030d/ (uploaded in 15-30 mins) I can't take the credit it was entirely @breunigs 

@breunigs - fancy taking a look? @ms-89 thanks for testing.  The rate limit exceeded unfortunately are a fact of life with drive uploads.  This will get fixed for uploads when they are cached on disk first which is planned in #711  When we get round to doing WebDav in #580 - this should make Owncloud work  Yes this is a good idea.  I think it is probably covered in #711 though.  You can use the new cryptcheck command in the latest beta for checking checksums on crypted filesystems.

----
rclone cryptcheck checks a remote against a crypted remote.  This is
the equivalent of running rclone check, but able to check the
checksums of the crypted remote.

For it to work the underlying remote of the cryptedremote must support
some kind of checksum.

It works by reading the nonce from each file on the cryptedremote: and
using that to encrypt each file on the remote:.  It then checks the
checksum of the underlying file on the cryptedremote: against the
checksum of the file it has just encrypted.

Use it like this

    rclone cryptcheck /path/to/files encryptedremote:path

You can use it like this also, but that will involve downloading all
the files in remote:path.

    rclone cryptcheck remote:path encryptedremote:path

After it has run it will log the status of the encryptedremote:.

 @azureblaze the info from the docs is above.  I'm planning on a full release this weekend which will update the docs on the website.

It doesn't mean --checksum will work, no, it is a specialised mode for checking only.  Actually `--retries 1` should really be called `--tries 1` so it will only try the once. That is a `--low-level-retry` Sure...

`--low-level-retries` are used to retry individual actions immediately.  Stuff that goes wrong all the time.

`--retries` are used to retry the entire sync/copy/move if there was an error. If it exhausts the low level retries it will count that file as errored, log a message and move on.

When it gets to the end of the sync if there are errors and it has retries left it will retry the whole sync.  You probably need the fuse package installed.  @Dedsec1 are there dependencies missing from the snap? Seems like it should depend on fuse? well i added the fuse-support plug so hopefully that should do the trick, of course iam testing this before pushing the updated snap file out to the main repo.. also make sure your doing sudo snap install rclone --classic because by default ubuntu snaps are set to the strict tag so they dont have full access to an system unless you allow it to do so. ok new update, for now i have created an apt-get install option for rclone using the free service packager.io . all it does is monitors your github repo and automatically builds the deb package if changes are made. so to make it short and simple, packager.io does all the monitoring and building the packages so it would be an perfect fit for users install rclone. i can confirm it works fully with mount as well. @Dedsec1 re apt-get install: interesting - can you describe that further - maybe in a new issue? my guess, since it would be something to do the sandbox feature that snapd has when an snap is installed onto an system. i was looking at making an Appimage for rclone that has no limits from what i have been reading so far. so my next project might be that since it works across many dist's as well.   If you want debug, you'll need -vv now.  -v only shows files transferred. The stats coming at a regular interval is supposed to be like a heartbeat for rclone so you know it is still alive.  I think #1032 covers this too? OK I put that one on the 1.37 milestone and I'll close this one.  I'll take a look and see if I can help - thanks for the heads up!  > I need sync like command but first remove from dest all files that absent from source

sync with --delete-before will do that

It will remove all files in dest that are absent from the source

> and after that cleanup source

Not sure I understand what you mean here?  rclone never touches the source directory.
 If by cleanup local you mean delete everything, then `rclone move` will do what you want.  You can use `--delete-before` with that if you want. Yes `--delete-after` is the default and what I'd recommend using.  I think this is all to do with the shell.  The problem I think is the use of " within the quotes

This should work

```
STD_SWITCHES="--stats 5m
\"--config=$RCLONECONF\"
--transfers=16 --checkers=32
--one-file-system
\"--bwlimit=07:00,6M 23:00,10M\""
```

Note that when you do `rclone --bwlimit="07:00,6M 23:00,10M"` it is the shell parsing the quotes, not rclone.

So your original command passed the quotes to rclone which it doesn't parse - it is expecting the shell to do that.  There is a reason for this...

It is because under the hood rclone move and rclone copy are just rclone sync with a few tweaks, and sync syncs two directories together.

You can also use moveto and copyto which work slightly more like mv and cp.

I do kind of agree with you, but I'm not going to change the semantics of move and copy now though - I think that ship sailed a while ago! > I created a folder at the root of a crypt remote called the_test. Inside this one I have another one called test_within. I want to move the_test (including its subdirectory) inside a folder called fixed.

You want this

    rclone moveto myCryptRemote:/the_test myCryptRemote:/fixed/thetest
 @unnfav yes that is right, you need the full path to the file/dir in the source **and** destination  Thanks for your detailed bug report.

This happens because of the async closing of the file - see the `test: WriteFileHandle.Release OK` after the directory read.

I'll try to think of a way to fix this.

Ideally I'd kill off the async file close, but I haven't figured out how to do that with FUSE.

Note that it will probably work if you do this (ie don't use a 0 length file)

    echo "hello" > mnt/test ; rm mnt/test @rafal-krypa that is Amazon being unreliable and rclone not retrying the upload (because in the general case it needs to buffer it to disk).    The unreliability I'm going to fix in #711.  I've changed the defautls of rclone to be less verbose, like rsync.

So by default rclone won't output very much at all - only if stuff goes wrong.

With -v it will just show the transfers

With -vv it will show the full debug @doonze I hear your impassioned plea!  I'll think about how I could make --stats appear without -v.. @doonze wrote
> Or does the new beta write a stats block when done anyway?

should do yes.

I like the idea about the auto updating stats block - fancy making a different issue about that?  I'll use this one to track changing --stats to show without -v if possible.

 I've fixed this by adding a new flag, use `--stats-log-level NOTICE` to see the stats without using `-v`

https://beta.rclone.org/v1.36-219-gaa204864/ (uploaded in 15-30 mins)  You'll find the release builds (the betas and the release) are built like that.

However if you build with `go install` or `go build` I don't think there is any way to get the git version into the build.

I changed the Makefile in f7252645ba8cd1844a20cad982c8fc2d9dc3c035 so that if you just run `make` you'll get an rclone with the correct version number.
  If you want to hack on rclone, then you need to hack on it at `$GOPATH/src/github/ncw/rclone` - just use git remote to set your origin to your own fork on github.

Then use git branches to select between the versions of code you are working on - no copies of the source tree needed.

Don't worry about go get stomping on stuff - it doesn't stomp on stuff which isn't on the master branch I don't think.
 Working in `$GOPATH/src/github/ncw/rclone` is the go way.  You **can** do it differently, but go is opinionated about things like that so you'll be swimming against the current :-(

You can have different directories with code in, but you should be changing GOPATH when you swap between them.

I've had similar arguments with the go tooling over the years (eg with symlinks) but for an easy life I've switched to doing things the go way.

I find working with branches a lot more convenient that having lots of directories full of random changes.  The discipline of having to check stuff in (even as a WIP FIXME commit) makes you describe what you are doing and helps when you come back to it later.

If you are working on rclone, you should be using git to update it rather than go get.  I'd recommend keeping a clean master branch then doing your work in feature branches which you then rebase to master.

I hope that helps

Nick > So it is proper (as far as go development) to have sub-packages/dependencies that live within an application refer to an effectively absolute rather than a relative location, so that they appear as though they are external to the application code even when they aren't? That seems so strange to me.

Yes that is the recommended way.  You can use relative imports but they are frowned upon and don't work properly in some circumstances, so people don't.

[Here is a reddit discussion](https://www.reddit.com/r/golang/comments/2588lk/relative_imports_do_or_dont/) on the topic which is interesting.

Go has a reputation for opinionated tooling and this is one of those areas.

PS install the goimports tool into your editor save hook and you will never type an import line again  That is bad luck :-(

Note that you need `-vv` to see all the low level retries now as they are DEBUG messages.

Only certain errors get retried - ones that are considered to be temporary.  Obviously that one didn't get considered to be temporary (or maybe didn't).

It should do a high level retry when the sync has finished and try again.  However this will involve uploading the entire file again.

Now because this file is uploading in chunks, there might be an argument that all errors should be retried, not just "retriable" ones.

Anyway I will add "broken pipe" to the list of retriable erros (not sure why go wasn't classifying it as that anyway - maybe that is a bug in go).  This probably could be fixed with a bit work in the local file system.  > Currently these delete existingdir (if it is empty) and rename file to that name, or if existingdir is not empty, it fails with the following error:

I've fixed that in 79e3c67bbdb0722fd211a67f8a7c5c034a655a15

The rest I'll do in 1.37  With the latest beta you can use the name of the remote, eg `drive` instead of the number.

I really need to be able to update these docs automatically - there are 11 remotes and their numbers keep changing!

In the mean time any takers to send a PR? @kubark42 this release has been going on to long - I'd forgotten that!  3c87a0d0dcc822ae53b65edd1878e5e1f18ab5a6 was the commit - it is probably out of date though since the sftp remote got added after that  This is fine, but you need to pass it every time you use it.  rclone is complaining that you didn't give it a command.

So

    rclone --drive-chunk-size=131072k copy xzy remote:subdir

will work

    rclone --drive-chunk-size=128M copy xzy remote:subdir

will also work

 You need to create the remote first with `rclone config`  I'll have a go at this after 1.36 is out the door.  Fixed - thank you!  If you make this change, then you can use `-vv --dump-bodies` and you can see clearly what is wrong

```diff
diff --git a/serve/serve.go b/serve/serve.go
index a1e8df2..96dd87f 100644
--- a/serve/serve.go
+++ b/serve/serve.go
@@ -78,12 +78,7 @@ func NewFs(name, root string) (fs.Fs, error) {
 		port = "80"
 	}
 
-	tr := &http.Transport{
-		MaxIdleConns:       10,
-		IdleConnTimeout:    30 * time.Second,
-		DisableCompression: false,
-	}
-	client := &http.Client{Transport: tr}
+	client := fs.Config.Client()
 
 	_, err := client.Head(host + ":" + port)
 	if err != nil {
```

    rclone ls -vv --dump-bodies serve:atomic

```
2017/02/23 13:24:33 DEBUG : HEAD /atomicatomic HTTP/1.1
```

 > Does i need something specific for mount command?

I don't think so

> Also - does it possible to force when mount to readonly mode 

`--read-only` flag for mount will do it

> also if i have two servers or more that serve identical content, does it possible to add to fs.Config.Client ability to round-robin each endpoint or do failover?

If you put both the IPs into the DNS then the transport will round robin them I think.

> and what happening when remote when request arrived not responding, but after one second works fine - does rclone mount supports reconnection ability or it fails with transport endpoint not connected?

The mount will retry failed opens

 >  i'm fix all my issues. So by providing http endpoint i can list and mount any path

Nice one!

Do you want me to merge this as a sort of special purpose remote?

If so can you squash all the commits and separate the vendor changes from code changes?

It needs some docs, and some tests too ideally, though the normal integration tests won't work if the remote can't read and write.

I think we should probably rename it to be the `http` remote which makes more sense than calling it the `serve` remote.



 Sorry have been busy with home life (wife been in hospital)!  Also trying to get the 1.36 release out the door!

> build failed for strange reasons in travis with go 1.8.

It looks like the goimports check is failing... Just format the code with goimports.

> Can you suggest me or write tests ? What i need? Does i need t spawn http server and connect it to this remote ?

In an ideal world you'd use the auto-generated tests that all the other remotes have.  However that isn't going to work since your remote is read-only :-(

The easiest way of testing will be with https://golang.org/pkg/net/http/httptest/

You then use that to test each individual part of your remote - a simple example here: https://gist.github.com/cespare/4992458

 Apologies for the delay - been very busy with home and work life.

I've been having a go with this prior to merging into master.

It works very well for your test URL, but when I tried it with some other examples
  * https://www.craig-wood.com/nick/pub/
  * http://beta.rclone.org/

The results were mixed, including adding `./` in the remote names, and including the URLS with `&` in.

That makes me think that this module needs a test suite.  It should be able to simlulate various different types of web server (have some stored output from various web browsers to make sure the parsing of the page works properly).

The parsing also needs to be stricter so it includes `http://example.com/file` but not `http://example.com/file&size=43` or `http://example.com/subdir/file/`

What do you think about adding a test suite and trying to make it more generic?

I merged this into a branch "http" along with some fixes from me - send future PRs against that branch.

PS you can test new remotes like this

```RCLONE_CONFIG_HLS_TYPE=http RCLONE_CONFIG_HLS_ENDPOINT=http://beta.rclone.org/ rclone lsl hls:``` Just show you know I've merged this to master now along with tests and docs.

Thank you very much for contributing it, and sorry it has taken me such a long time.

-- Nick  > It would be great if stats in logs would show us the amount of data transferred in kb/mb and not just %

Something like this?

    movies/Alien (1979)/Alien (1979).mp4: 5MB/10MB 0% done, 1.702 MBytes/s, ETA: 17m3s

>  If average transfer per file could be showed would be awesome as well so we get general idea if cloud is preforming well enough for streaming eg we know that at least 20Mibs+ we need for full 1080p and now with buffer its hard to monitor ... maybe current buffer value of the transfer

That figure `1.702 MBytes/s` is the average transfer for that file.  Did you mean something different?
 I see what you mean :-)  Ouch `rclone copy` shouldn't be deleting anything!  That needs fixing ASAP.

Assuming I fix that, If I try your example with `rclone sync` then both the syncs give the same result `bin/rclone` is deleted.  That is what I would expect as the source and the destination overlap.

```
$ mkdir -p /tmp/foo/bin
$ rclone copy $GOPATH/bin/rclone /tmp/foo/bin
$ cd /tmp/foo/bin
$ rclone sync . .. -vv --include rclone
2017/02/22 10:42:14 DEBUG : rclone: Version "v1.35-DEV" starting with parameters ["rclone" "sync" "." ".." "-vv" "--include" "rclone"]
2017/02/22 10:42:14 INFO  : Local file system at /tmp/foo: Modify window is 1ns
2017/02/22 10:42:14 INFO  : Local file system at /tmp/foo: Waiting for checks to finish
2017/02/22 10:42:14 INFO  : Local file system at /tmp/foo: Waiting for transfers to finish
2017/02/22 10:42:14 INFO  : rclone: Copied (new)
2017/02/22 10:42:14 INFO  : Waiting for deletions to finish
2017/02/22 10:42:15 INFO  : bin/rclone: Deleted
2017/02/22 10:42:15 INFO  : 
Transferred:   22.275 MBytes (190.006 MBytes/s)
Errors:                 0
Checks:                 1
Transferred:            1
Elapsed time:       100ms
2017/02/22 10:42:15 DEBUG : Go routines at exit 4
2017/02/22 10:42:15 DEBUG : rclone: Version "v1.35-DEV" finishing with parameters ["rclone" "sync" "." ".." "-vv" "--include" "rclone"]
```


```
$ rclone copy $GOPATH/bin/rclone /tmp/foo/bin
$ rclone sync . .. -vv --delete-before --include rclone
2017/02/22 10:42:47 DEBUG : rclone: Version "v1.35-DEV" starting with parameters ["rclone" "sync" "." ".." "-vv" "--delete-before" "--include" "rclone"]
2017/02/22 10:42:47 INFO  : Local file system at /tmp/foo: Modify window is 1ns
2017/02/22 10:42:47 INFO  : Waiting for deletions to finish
2017/02/22 10:42:47 INFO  : Local file system at /tmp/foo: Waiting for checks to finish
2017/02/22 10:42:47 INFO  : Local file system at /tmp/foo: Waiting for transfers to finish
2017/02/22 10:42:47 INFO  : bin/rclone: Deleted
2017/02/22 10:42:47 INFO  : Local file system at /tmp/foo: Waiting for checks to finish
2017/02/22 10:42:47 INFO  : Local file system at /tmp/foo: Waiting for transfers to finish
2017/02/22 10:42:47 INFO  : 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:                 1
Transferred:            0
Elapsed time:          0s
2017/02/22 10:42:47 DEBUG : Go routines at exit 4
2017/02/22 10:42:47 DEBUG : rclone: Version "v1.35-DEV" finishing with parameters ["rclone" "sync" "." ".." "-vv" "--delete-before" "--include" "rclone"]
$ 
 I've fixed the copy deleting files with --delete-before issue here

http://beta.rclone.org/v1.35-132-g6b0f2ef/ (uploaded in 15-30 mins)

Please re-open the ticket if there is still a problem.

Thanks for reporting  That looks like a regression - thanks for reporting.

It seems like the depth 1 listing code in dropbox is broken:

```
$ rclone ls --max-depth 1 dropbox:Photos
2017/02/22 09:28:41 Failed to ls: path "/Photos/XXXX" is not under root "/Photos/"
```

I'll post a beta for you to try when I've figured out what is going on. I've fixed this in the beta below

    http://beta.rclone.org/v1.35-131-g12aa03f/ (uploaded in 15-30 mins)

Let me know if it works for you - re-open the ticket if it doesn't

Thanks I think that is probably caused by #1103 which I haven't fixed yet. The reason why it is different between v1.35 and the latest beta is that rclone uses a different method of listing the files.

I'm going to re-open this issue and think some more on how to fix it. I think I have fixed this here.  Would you mind trying the sync again and see if it is fixed for you?

http://beta.rclone.org/v1.35-138-g527099a/ (uploaded in 15-30 mins)

Thanks

Nick Thanks for testing :-)  You probably want `--files-from` to make a list of exactly which files you want.  These don't need escaping. > But my wife says I can't find the peanut butter in the pantry a lot either.

Hehe - mine too ;-)  Not a bad idea.  Not sure how easy it would be though as rclone relies on the config file being set up to parse the remote string.  There are secure links for downloads on the downloads page...

However I want to make everything secure and I plan to do this quite soon :-) I've now migrated all the rclone sites onto their own server running caddy. Letsencrypt have provided certs so everything is https now!

Let me know if I've missed any links. @marvwatson - thanks I - I did that.  There were a few more in the README as well. @marvwatson - I see your PR fixing them - thanks :-)  Do you mean making an `http` remote which looks at standard nginx or apache directory listing?

I got part of the way through writing `rclone serve` to serve a remote via http.  My plan was to make this so you could optionally upload files too.  This could serve to such a remote...

 I think we are talking about the same thing.

What I'd like to do is 
  * make an `rclone serve` command which can serve an rclone remote over http
  * make an rclone remote which can read files from `rclone serve`
    * this could in addition read files in a read only fashion from nginx/apache directory listing (that is what you are talking about) I only have code for the serve bit of it.  I pushed that to the `serve` branch if you want to have a go.

To do the remote bit of it will be a bit more work... Yes it can take any rclone remote  Can you try again?  There have been problems with this in the past, but I think it takes a bit of time for the new account to be synced to amazon.com or something like that.

As far as I know there isn't a separate oauth for any of the non .com Amazon's.  I know that amazon.co.uk works and also amazon.de.  I made an issue https://github.com/pkg/sftp/issues/158 to ask for some help with this. I've been unable to replicate the speedup with FileZilla - was the tests you did with the same files from the same server in the same direction?

Were you uploading or downloading?  Yes the application_id and user_id are optional.  It is possible to get your own but quite a difficult process, so just use rclone's by leaving them blank.  Can you add your support to issue #497 please?  The more subscribers I see to an issue, the more popular I know it is!

Thanks
  > --update flat out just solves the problem though, because when the torrent finishes it'll be a newer file on my harddrive than my last backup and it'll upload it. The only reason I didn't close my own issue is that --update does not work as stated in the documentation, in that --update does not check size (maybe it's not supposed to but documentation says it does).

Glad update solves the problem.

I just had a look at the source, and update does work exactly as the docs state

> -u, --update
> 
> This forces rclone to skip any files which exist on the destination
> and have a modified time that is newer than the source file.
> 
> If an existing destination file has a modification time equal (within
> the computed modify window precision) to the source file’s, it will be
> updated if the sizes are different.

So the size is only checked *if* the times are *equal*.

I was following rsync when I wrote that flag, and here is what rsync's man page states

> -u, --update
>
> This forces rsync to skip any files which exist on the
> destination and have a modified time that is newer than the
> source file.  (If an existing destination file has a
> modification time equal to the source file’s, it will be
> updated if the sizes are different.)


As for the original problem I would probably try to solve the problem
a different way, having an incoming directory and move the things out
of the incoming directory when satisfied.  You can use
`--track-renames` to make sure the move means it doesn't need to be
uploaded again.  What you are after is this flag from rsync

```
        -l, --links                 copy symlinks as symlinks
```

rclone has all the bits and bobs to implement that now using its MimeType calls.

A bit of research indicates that the correct type should be `inode/symlink`.

I thought there was a ticket about this, but apparently not!  @baro77 Try the steps in [this link](https://support.globalsign.com/customer/portal/articles/1353318-view-and-or-delete-crl-ocsp-cache) that has helped other mac users in the past.

@hashbackup you might need to update the ssl root certificates (that would be ca-certificates package on ubuntu).

I wonder if google have changed their certificates somehow recently... @baro77 rclone uses the go TLS implementation which was written from scratch in go.

I don't really understand what is going on - whether there is a problem with the TLS library with OS X.  

It might be that this issue is relevant: https://github.com/golang/go/issues/14514

In which case compiling rclone yourself would fix the problem.  I'd do it for you but I don't have a mac :-(

If you want to have a try compiling rclone, then you'll need to install `go` and `git`
  * [install go](https://golang.org/doc/install)
  * [install git](https://git-scm.com/download/mac)
  * run go `get -u github.com/ncw/rclone`
  * find the rclone binary in ~/go/bin/rclone
 @baro77  I should probably re-open the go bug (or create a new issue) as it is definitely the same problem.

Let me know how it goes! It does seem very strange that it only happens some of the time.

Maybe someone is intermittently man-in-the-middling your connection?

Or maybe there is a mis-configured server in the cluster with a dud certificate.

If you do with `-vv --dump-headers` likely you will see which server it is actually connecting to. Great debugging.

This seems most likely that it is an intermittent problem with rclone contacting Avast.  Can you turn it off and confirm that?

Also can you try building rclone from source (see above) and try that too? How did it go - did an rclone built from source fix the problem?
 That is good to know that rclone built on the mac is OK.

That probably means that something like golang/go#14514 is relevant.  If the native build is OK but the cross compiled build isn't.

I don't know why it should be intermittent though - that is strange!
 To build a specific version

  cd $GOPATH/src/github.com/ncw/rclone
  git checkout v1.35
  go install

should do it After you've done the `go get` try this commands above

```
cd /Users/baro/go/src/github.com/ncw/rclone
git checkout v1.36
go build
```

That should build a binary in the current directory @brandur - yes #1041 will only apply to 1.36 and later!  This is a duplicate of #1018 - can you add your support to that issue please.  I've merged that - thank you very much and well spotted.

Unfortunately the test framework isn't up to the task of injecting an error to actually test that code :-(

Something I need to fix.  Just run the copy without `--ignore-existing` - copy is intelligent enough to figure out files are already present. Can you try the latest beta, and post a log with `-v` of rclone copying files you think it shouldn't.  It will say exactly why rclone is copying the files. If the sizes are different then the file needs transferring.  So rclone thinks it needs to update that file because it is definitely different.

Can you confirm the sizes are different?

You can use `--ignore-existing` which should stop that file being transferred if you are sure you don't want it updated.  That is what the --stats flag does pretty much.

The individual file stats have 10s averages, the overall ones don't though.  Thanks for finding that.

However there is a ticket about mega.nz already #163 so can you add your support to that please?

Thanks

Nick  > Should rclone automatically refresh the tokens as needed?

Yes definitely.

> Any idea why this is not working?

Incorrect time could be a reason...

Does rclone print `"Time may be set wrong - time from %q is %v different from this computer"` in its log?

Look at the token generated in rclone.conf - does the expiry time look sensible?

> Not sure if there is anything else I need to set up in the Google Developer Console or anything like that?

I don't think so...
 Can you try the latest beta please? (See the downloads page).  I don't think it will change anything, but we will be looking at the same versions then.

> Should rclone log when it's going to try and do a refresh or anything?

Under debug logging `-vv` you should see `Saved token in config file` in the log

> I start getting the unauthorized at:
> 2017/02/16 13:46:10
> 
> That is 9 seconds before the refresh should be happening. But then I keep getting unauthorized errors well past the expiry time and it never refreshes the tokens.

That is suspicious...  The oauth library will refresh tokens 10 seconds before they are due to be refreshed...

So it is like the refresh token didn't work

if you run rclone with `-vv --dump-headers` you get to see all the gory transactions.  Can you make a log with the latest beta and run it until it goes wrong then post it somewhere please?  Or if you don't want it to make it public, email it to me nick@craig-wood.com with subject "rclone issue #1139" > The client_id and the client_secret parts were blank.

If you leave them blank they will use rclone's built in credentials.

> Could it be that rclone couldn't write to the config file properly?

Could it be that you had two rclone's running at once?  rclone updates the config file when the token is refreshed.  This would require rclone to know the total storage of the remote which it doesn't at the moment.

You can do 2 with `rclone size remote:` I'll re-open this so we can do this some time!  I think for rclone a list of error codes like rsync would probably work best.

Then I can assign specific errors codes..

I'd probably want to keep it simple to start with.  Rclone can classify some errorrs - these are the categories used at the moment, which I could map to exit codes...
 
  * 0 success
  * 1 Syntax or usage error
  * 2 Error not otherwise categorised
  * 3 Directory not found
  * 4 File not found
  * 5 Temporary error (one that more retries might fix) (Retry errors)
  * 6 Less serious errors (like 461 errors from dropbox) (NoRetry errors)
  * 7 Fatal error (one that more retries won't fix, like account suspended) (Fatal errors)

How would that work for you? OK the above would be relatively easy!  I agree that this should be consistent - I suspect I've missed a test from the test suite so the remotes have differing behaviour.

  cryptcheck does --low-level retries, but not --retries at the moment.

I'll make it do retries too. @Jdogzz what error does it quit with? @Jdogzz Try increasing `--low-level-retries`.  You might have got yourself a ban though.  Also look at the log with `-vv` just before it exits - hopefully you see the low level retries.  Check out the ulimit suggestions in this issue #1111  Looks odd I agree! > no error was reported in the exit code for a src file that doesn't exist

This is now fixed.

> rclone assumed the thing being transferred was a directory and left one behind in the target

The assumption that the thing being transferred was a directory is the way rclone works, however I've fixed the spurious directory creation in f88300a153a63a08abd026f20dc93da3b76cafc9

  The command would need to take your crypt remote as a parameter, so it would need to be something like

    rclone cryptdecode crypteremote: filename1 filename2 filename3

Good idea!  I built those binaries using go tip (the latest beta should work though) with these commands.

    GOOS=linux GOARCH=mipsle go build

and 

    GOOS=linux GOARCH=mips go build

To build the two versions.

Attached is the result of this

```
 GOOS=linux GOARCH=mipsle go build
 mv rclone rclone-mipsle
 GOOS=linux GOARCH=mips go build
 mv rclone rclone-mips
 zip -9 rclone-v1.35-95-g5419292-mips.zip rclone-mipsle rclone-mips
```

[rclone-v1.35-95-g5419292-mips.zip](https://github.com/ncw/rclone/files/773747/rclone-v1.35-95-g5419292-mips.zip)
 I'm using ubuntu 16.04 amd64 to build.

Try building it with this command on a single line

    GOOS=linux GOARCH=mipsle go build

That should work  I found some API docs here: https://spideroak.com/apis/partners/web_storage_api

The docs don't seem to be complete though - they don't say how to upload files.

There is an opensource client which probably has more details!  If you can somehow get the swift credentials for hubic, then you can just set up a swift remote with them and not use the oauth flow.  Is that what you mean? The way you get the swift credentials is to do the oauth bit, and use the API. Yes rclone will grab the token and refresh it when it expires so you only have to log in once.  Not sure what is going on here - this is a panic from the go standard library.

A bit of searching suggests maybe cancelling a request twice causes this, though I don't think that is anything that rclone ever does.

Is it reproduceable the bug?

Can you  @PiscisSwimeatus 

I had a quick chat about this issue on the go developers list.  This should never happen according to the core developers, and I agree after looking at the code.

It clearly did happen which means there was some kind of memory corruption in rclone.  That can be caused by a race condition, however I tested rclone for a long time uploading things to ACD to try to trigger it with no success.  So if it is a race condition it doesn't happen very often :-(

The other alternative is that there was an actual memory corruption on your computer.  Have you tried memtest86 on it recently?  Or it could be soft error which are [suprisingly common](http://www.zdnet.com/article/dram-error-rates-nightmare-on-dimm-street/).  Anyway that really is clutching at straws.

So I agree with you that we should close this for the moment, but please re-open if it happens again!  Can you retry with the latest beta please?  (See downloads page).

I think this problem may be fixed already The latest version may still get stuck, but it should always carry on correctly and not truncate files or give IO errors. Thanks.

The pauses are caused by some kind of network error, and rclone having to reconnect.  I'm not sure exactly why though, but it could be amazon, or it could be a local networking problem.  Have you checked your router has the latest firmware? Very strange!  Something happening at Amazon most likely.

When you do the transfer with `rclone copy` can you see if you see a pause with that too? OK so the pause is not to do with `rclone mount` specifically.  What does the corruption look like - is it just the config file but mixed up (that is what I'm expecting)? @PiscisSwimeatus sorry I missed that.  Yes that looks like it was part of the config file. I think this should be fixed by 9cede6b372002a739cca8d5db589c61c6d229bba so I'm going to close this - let me know if it re-occurs!  Hi ncw

i wanted to make it easier for people to install rclone on different types of Linux distros, so i went ahead and setup an Ubuntu snap for rclone. you can install rclone by doing the following command listed below. luckily snaps can be deployed on other distro's as well such as Redhat, CentOS, Debian etc. currently i have my launchpad account pulling from my fork automatically using git and building an new snap each time an change is  detected. so you could add my snapcraft.yaml file to the main repo for rclone, then you could  update the version in the file when an new version is pushed out.  then i can have launchpad pointed at the main repo for rclone if you would like.

https://github.com/Dedsec1/rclone/

https://github.com/Dedsec1/rclone/blob/master/snapcraft.yaml

sudo snap install rclone --classic

Regards

Dedsec1 That looks great!

Do you think you could send a pull request with

  * the snapcraft.yml
  * some updates to the docs (you only need to edit docs in `docs/content` the others are auto generated) describing how to configure snap and install rclone

Then I can integrate the changes :-)

Thanks

Nick sure thing, i did create an PR as well, just have to edit the docs. docs has been created https://github.com/Dedsec1/rclone/blob/master/docs/content/rclone%20snap.md I've merged your PR now thank you!

I've got some questions about it though...

Are you willing to maintain the snap going forwards?  If so I'd like to put some details for you in the install.md (or you could send a PR).

Can you point your builder towards the official rclone source now?

Is the snap builder checking out a release or is it just using the latest commit?

And do I need to update the version number in the snapcraft file when I make new releases? I presume so... sure thing, i dont mind maintain the snap going forward, the only thing i have to do is change the version number in the yaml file each time. i already pointed the auto builder for your git repo so Ubuntus servers will do the rest when it comes to building. Great.  Can you give me a URL to point to the builder for the docs? here is the repo i created on my launchpad account, https://launchpad.net/~deathero291/+snap/rclone
as of right now i have it setup to pull any changes from your repo every few hours.   @Dedsec1 any chance you could look at @joewashear007 s comments?  I think fixing them would probably fix #1188 i will have to look into this since fuse might have different deps that it needs to run. but it should be possible @Dedsec1 do you have an opinion on this? I'm going to close this for the moment as a snap is created.

However I would much appreciate PR to update the snap or the docs.  There have been several requests for rclone to

  * loop for N loops or perhaps indefinietly
  * quit after a certain length of time

There should probably be a delay between loops too. @ajkis I think there is an issue about this somewhere too!  It is a good idea, just not particularly easy ;-)  I've fixed this already - try the latest beta :-)

See #825

The fix will be in the 1.36 release  Looks like I introduced that bug quite recently in bd29015022ab627a644c26b3b61a3a44f3268210

Here is a beta with a fix

http://beta.rclone.org/v1.35-80-g40c0298/ (uploaded in 15-30 mins)

Please re-open if it doesn't fix it!

Thanks

Nick  This is almost certainly rclone's handling of files vs directories.

When you open a remote `remote:file` rclone doesn't know whether you are referring to a file or a directory, so it has to query the remote to find out.

I think that is the second request.  Note that it is different to the 3rd and 4th because it uses `mimeType='application/vnd.google-apps.folder'`  wheras the 3rd and 4th use `mimeType!='application/vnd.google-apps.folder'`

So I think the queries are

1. Find the root directory
2. See if the path supplied is a directory
3. It isn't so get details of the file so we know the remote points to a file
4. the actual listing

3&4 are duplicated, but rclone doesn't cache the object it finds in 3.
 Requests 1-3 are part of setting up the remote.  Only 4 is part of the ls.

It could be done more efficiently certainly but would involve a bit of caching and optimisation for somthing which isn't the common case.  Nice idea.  I deliberately didn't implement all the rsync rules to keep things simple.  However protect sounds like a good next one to do.

Do you fancy having a go?  You have to do the check the other way round on the crypted remote and grep for the file name you want

    rclone lsl --crypt-show-mapping encryptedgdrive: 2>&1 | grep m8fnghbhatmesjafpqr4uf953f8q6va267jlau5s4cs16lgeb9bg

Anyway it looks like you have some duplicate files (which is a problem with google drive).  Try this - it will fix them for you interactively.

    rclone dedupe gdrive:crypt

 Sync doesn't deal with duplicates unfortunately. They don't happen very often though luckily.  I'm going to close this now as I think we have got to the bottom of it (the duplicates).  > 2017/02/08 17:11:48 Mark_Masters.zip.003: Upload error detected but didn't finish upload: Post https://content-na.drive.amazonaws.com/cdproxy/nodes?suppress=deduplication: write tcp 192.168.88.238:55296->52.1.75.19:443: wsasend: An existing connection was forcibly closed by the remote host. ("HTTP status UNKNOWN")

That looks like it could be your ISP, your router or your firewall closing connections.

That probably explains why you are having problems.

Check the firmware on your router is probably the most productive line of attack (I've had lots of reports from rclone users of routers flaking out when rclone pummels them with data!).  This is probably a consequence of 9d362589233e50f9ca8befd3e9ef7f01b08e408b

@kynikos - fancy taking a look at this? @AzureBlaze how did you set the environment variable?  I can't seem to set it without it being immediately expanded! Eg

```
X:\go\src\github.com\ncw\rclone>set RCLONE_CONFIG=%HOMEPATH%\Documents\rclone.co
nf

X:\go\src\github.com\ncw\rclone>set
[snip]
RCLONE_CONFIG=\Users\Dev\Documents\rclone.conf
[snip]
```

I'll try your patch @kynikos when I've worked that out. OK - that is strange! That version doesn't contain the patch.  As far as I can see the difference between 77 and 79 shouldn't affect that code at all.
 OK.  I'll close this for the moment - please re-open if you get this problem again and can reproduce it reliably.

Thanks

Nick  The ulimit suggestion by @Coornail sounds like the right one, unless `rclone mount` has a file descriptor leak.

How many open files are there at once?

What happens if you use `lsof` on the rclone mount process - do you see the number of file descriptors continuously rising?

It is possible that the application you are using isn't closing its files properly also - you can check that with lsof too. Cross fingers! :-)  Could platforms work over the network and they are unreliable because of it.

However rclone does lots of retries, so works around errors like the above.

What did rclone print at the end of the transfer?
 The occasional `Failed to copy: upload failed: Failed to grab locks for 253673584: lock held by connection 21020332. ` is to be expected.

However continuous corrupted on transfer messages aren't.

Where were you copying from and to?  Were the files being modified while being copied? @jxs714 in #1099 the corrupted files were caused by your exclude of `/sys/` not working.  I pointed out how to make it work in that ticket.

If you can correct your excludes hopefully the corrupted messages will disappear.

You might find the `-x` flag useful too

    -x, --one-file-system                   Don't cross filesystem boundaries.
  I have noticed the same thing - that sometimes ACD transfers are much slower and sometimes much faster.  I conjecture they are connecting to different bits of infrastructure.

I've never seen that big a difference though, only ever about 2x.  What would you think if I put an easy to grep INFO, DEBUG, TRANSFER in the log - would that be enough? Great!  I'm planning on re-vamping the logging for 1.36 and I'll put that in. I have reworked the logging - please have a go with it in this beta

http://beta.rclone.org/v1.35-87-g006227b/ (uploaded in 15-30 mins)

Here are the bits from the docs

## Logging ##

rclone has 4 levels of logging, `Error`, `Notice`, `Info` and `Debug`.

By default rclone logs to standard error.  This means you can redirect
standard error and still see the normal output of rclone commands (eg
`rclone ls`).

By default rclone will produce `Error` and `Notice` level messages.

If you use the `-q` flag, rclone will only produce `Error` messages.

If you use the `-v` flag, rclone will produce `Error`, `Notice` and
`Info` messages.

If you use the `-vv` flag, rclone will produce `Error`, `Notice`,
`Info` and `Debug` messages.

You can also control the log levels with the `--log-level` flag.

If you use the `--log-file=FILE` option, rclone will redirect `Error`,
`Info` and `Debug` messages along with standard error to FILE.

If you use the `--syslog` flag then rclone will log to syslog and the
`--syslog-facility` control which facility it uses.

Rclone prefixes all log messages with their level in capitals, eg INFO
which makes it easy to grep the log file for different kinds of
information.
 I messed up with that beta - try this one instead http://beta.rclone.org/v1.35-92-g18c75a8/ (uploaded in 15-30 mins)
 @AzureBlaze I'm taking my lead from rsync here which prints almost nothing unless you pass `-v`.  Now `-v` will just print useful stuff!  The .lock file isn't directly in the "Windows Defender" directory, so you need this I think

    Windows Defender/**/*.lock Sorry, my eyes deceived me there - you are right!

What happens if you do this - does it list the lock file? (Send the output to a file and search the file.)

    rclone --exclude "Windows Defender/*.lock" ls "C:/"

Can you paste the contents of your `Exclude.txt` and also what happens when you do

    rclone --exclude-from Exclude.txt --dump-filters
 I see the problem

In your exclude file you don't want the "quotes" around

    "Windows Defender/*.lock"

You want

    Windows Defender/*.lock

You should remove all the " from that file.  I don't think there is a problem with rclone, I think it is amazon being unreliable...

Keep trying and it should come good in the end.  Amazon is quite unreliable uploding large files.  When they fail, rclone will try again - hence the 97GB transferred but only 36 GB uploaded.

> Once a file is uploaded should it appear instantly on my Amazon Drive?

yes

> If an upload stops due to my internet disconnecting, what happens?

You'll have to start the upload again.

> If it says 97GB was transferred then does that mean that 97GB of files are on Amazon Drive or could it mean that 97GB of files were partially transferred but failed to upload due to network disconnecting?

See above for the most likely explanation - Amazon being unreliable with big files.

> How can I access a log to see what happened to the missing files?

Use `--log-file` to store the log somewhere.  Use `-v` for more detail. @holygamer 
> I know Amazon's own software can pause and resume uploads but does it also resume uploads that fail due to the network disconnecting for a minute or so?

Yes it can, but it uses an unpublished API unfortunately :-(  Some work had been done to work out what it is doing, but it isn't ready for use in rclone yet. >  Amazon Drive's desktop software fails after a few hours with this message: "File Name Conflicts". All files have unique names so that makes no sense. Only a few files were uploaded.

That is caused by uploading a file that has already been uploaded without checking it already exists.  rclone has that problem too sometimes, though much less recently.  Sounds like you want --delete-excluded - try with --dry-run first! 

The warnings about symonds won't affect anything.  @hkbakke no worries!  I think this is the same issue as #693 and #691

It is to do with dropbox not always returning the directories with the same case.

I could do with some help fixing this - it requires a new approach to the dropbox internals.

Probably what someone should do is re-write the dropbox remote using the v2 API and fix this issue along the way! I may have fixed this in #1165 - would you mind testing with this beta and let me know what happens please?

http://beta.rclone.org/v1.35-138-g527099a/  Excellent - thanks for testing.  This is a good idea in theory...

However it is a little tricy to implement
  * it would probably require a new checkcrypt command
  * crypt would need to implement an optional interface to return the hash of the encrypted file
  * crypt would need to implement another optional interface to crypt with a nonce and checksum a file @ajkis 
rclone verifies MD5SUMs in normal operation too - just not with crypt I'm going to try to squeeze this in to the 1.36 release so people can check for corruptions. @ajkis No it is purely an integrity check I've implemented rclone cryptcheck in this beta please give it a go!

http://beta.rclone.org/v1.35-91-g01c747e-cryptcheck/ (uploaded in 15-30 mins)

Here are the docs

----

rclone cryptcheck checks a remote against a crypted remote.  This is
the equivalent of running rclone check, but able to check the
checksums of the crypted remote.

For it to work the underlying remote of the cryptedremote must support
some kind of checksum.

It works by reading the nonce from each file on the cryptedremote: and
using that to encrypt each file on the remote:.  It then checks the
checksum of the underlying file on the cryptedremote: against the
checksum of the file it has just encrypted.

Use it like this

    rclone cryptcheck /path/to/files encryptedremote:path

You can use it like this also, but that will involve downloading all
the files in remote:path.

    rclone cryptcheck remote:path encryptedremote:path

After it has run it will log the status of the encryptedremote:.

Usage:
  rclone cryptcheck remote:path cryptedremote:path [flags]
 Correction: this is the beta http://beta.rclone.org/v1.35-92-g18c75a8/ (uploaded in 15-30 mins) @calisro strange!

If you download the files using rclone copy, do you see the 4 file differences or only the 1?

Were there any other log messages when you did the rclone cryptcheck? As far as I can see this is done for the 1.36 release.  If there are any outstanding problems - please open a new issue. @Saviq no - I'm afraid not.  Doing it with a flag would probably be a good idea.
 
I propose using the flag `--skip-unreadable` which would mean that a file or directory that rclone can't read would be skipped.  This would be implemented by the local fs, but could in theory be implemented by others such as sftp.

In general you don't want it though as if you were syncing a directory, then it became unreadable for some reason, it would get deleted on the remote.  I just checked the code - yes you are right!

I'll fix this. @balazer You should definitely win a prize for noticing that - it has been like that since the very first version of rclone! I've fixed this in http://beta.rclone.org/v1.35-102-gb52c80e/ (uploaded in 15-30 mins)  > Constant 429 too many requests errors

That is Amazon I'm afraid... rclone does lots of retries so it will get there in the end.

> • Some files will just not copy and give the following error: corrupted on transfer: sizes differ SMALLSIZEFILE vs BIGGERSIZEFILE

```
2017/02/03 16:12:03 amazoncloudlog.txt: corrupted on transfer: sizes differ 1412 vs 4683
2017/02/03 16:12:03 amazoncloudlog.txt: Removing failed copy
```

These look like files which are being written to while rclone is running.

> • Excludes file isn't being considered/used

You need to use `/sys/**`  if you want to match `/` in the path (which you do)

> • Progress isn't displaying in command line.

Use the `--stats` command to change the rate of progress printing, or `-v` for line by line account of what is happening. > The file issue still occurs with other files where they say corrupted on transfer as well. How can i at least solve this issue with other files?

The other files looked like there were in /sys which hopefully the `/**` change will stop uploading.

> Also, any way to limit the amount of files to transfer at a time or limit the upload speed so that 429's dont occur?

You can try reducing `--transfers` and/or using `--bwlimit`.  Not sure either will help, but worth a go!  Not being able to rename directories is #954 - I suggest you subscribe to that issue.  I'm due to implement that for 1.36.

The panic is new - I'd like to fix that.  Is it reproducable?

Cut/Paste in the finder is equivalent to a rename - is that right? 

It looks like you were trying to copy or move a file.

Here is an attempt to fix it!

http://beta.rclone.org/v1.35-75-g381b845/ (uploaded in 15-30 mins)
 > Interestingly enough copy / paste in place (via finder -- which is actually duplicating a file) fails:
>
> (copy / paste to another folder fails as well)

It looks like you got unlucky with the upload failing with 429 error

Uploading files via `rclone mount` is much less reliable than `rclone copy` or `rclone sync` because it can't retry.

Once I've done #711 this will make it much more reliable - until then I recommend uploading using `rclone copy` etc not by copying to the mount. @tcf909 I think we've fixed the problem in this issue, but #711 remains!  This is as I expected - rclone doesn't attempt to preserve the more of the file.

However at for most users it is unexpected and I'd like to fix it eventually!  Unfortunately the meaning of chunk size varies from remote to remote and there are lots of different constraints, so a general purpose implementation isn't possible.

Which remote are you interested in particularly?  Sorry this fell of my radar for a bit.

Reluctantly I've come to the conclusion that we've taken a wrong turn and that a `--json` flag is the wrong thing to do.  We should have a separate command instead.

I'm really sorry to change direction on you.  It is the way I work - I rip stuff out and redo if I think I've gone the wrong way, but I realise that it can be frustrating for external contributors, so please accept my apologies.

Here is my thinking.

If a user wants JSON then they want to output objects, rather than different encodings of different human readable lists.  The `--json` flag makes a bit of a mess of the existing code and each of those uses of JSON should really have extra tests leading to quite a lot of extra code complexity.

So I think we should make a new command called `lsjson` which
  * only outputs JSON
  * lists both files and directories
  * if `--hash` is specified includes any hashes that are available
    * this can be expensive so we don't want to do it by default
  * We could also add MimeType in there too.

I'd really like to be making the output from a standardised JSON struct that we can document.  It will become part of the rclone API so it needs to be documented, standardised and have tests.

I think the output struct should look something like this (so It can be marshalled directly by the go code)

```go
type ObjectJSON struct {
    IsDir bool // true for directories, false for files
    Path string // the remote
    ModTime string // need to output this in a JSON friendly format, eg time.RFC3339Nano
    Size int64
    Hashes map[string]string `json:",omitempty"` // map of hash names to hash value
    MimeType string `json:",omitempty"` // if known
}
```

A bit of care is needed with the [JSON time format](http://stackoverflow.com/questions/10286204/the-right-json-date-format) - Go's [time.RFC3339Nano](https://golang.org/pkg/time/) should be the right format I think.

To do this we'd need to write a new `ListJSON` functinon which calls `ListFn`. It should then serialise an ObjectJSON for each file or directory.

We can then write tests for ListJSON in the same way as the other list tests.

I'm sorry to mess you about :-(  I think it is important we get this right though as I can see lots of people building upon it.

Let me know if you want to do this, or whether you'd like me to.
 @Wundark thanks for your understanding.  I'll have a go at this in time for the 1.36 release - I'll post updates in #1063

Looking forward to seeing a web UI!  I've merged this - thank you very much for your contribution :-)  Unfortunately crypt remotes don't support MD5SUMs yet.

> I'm guessing that the md5sum feature is designed to use the remote storage provider's own metadata, but when an file is encrypted remotely you would get the md5sum of the remote encrypted file, not the original decrypted version?

100% correct.

I need a place to put the encrypted MD5SUM as metadata on the objects which I haven't had time to do yet.

It does mention it here in the docs:

http://rclone.org/crypt/#modified-time-and-hashes  Looks like this is supported in the s3 go module [WithS3UseAccelerate](https://godoc.org/github.com/aws/aws-sdk-go/aws#Config.WithS3UseAccelerate)

Fancy sending a PR? @mcianfrocco Sorry I shouldn't have used the abbreviation.  Yes I'd love to see a pull request for this feature if you have time to make one.

Thanks

Nick  Vendoring is the way to keep rclone's dependencies stable.

I don't like it particularly, but it is the best solution so far.

> I'm use new fs with own connection, why rclone uses
> storageURL := fs.ConfigFile.MustValue(name, "storage_url") ?

That is so you can override it - in normal use it will be blank and fetched from the server as you said.
 Can you post the complete script somewhere (a gist maybe) then I can have a go with it? You need a call to 

    fs.LoadConfig()

Early in your main - that should fix it! > Nice! How can I modify default after loadconfig?

You can change the `fs.Config` structure which is defined [here](https://github.com/ncw/rclone/blob/master/fs/config.go#L184)  >  I have some files that their extension changed from being lowercased to capitalized.
I'm not sure if this was some bug that was fixed in previous issue of rclone or a bug that existed in OneDrive and was fixed

rclone expects that if it uploads a file in a particular case, then the remote will keep that case.  I don't think that is an unreasonable expectation even for case insensitive remotes like OneDrive.

According to some brief tests onedrive still behaves like that - if I upload FILE.JPG it keeps it like that in the listings and doesn't change it to FILE.jpg

So is it possible that these got renamed locally?

You can try a latest beta with --track-renames if you want the detecting files have moved feature! > That seemed to be working and the log indicated many files that were renamed
from ####.MP4 to ####.mp4 unfortunately after every move rclone also indecated
deleting the original file (####.MP4) and because OneDrive is case insensitive it just deleted the file.
I'm not sure if the actual rename succeed in changing the name to lowercase before it was deleted or not since this happened almost instantly.

Foo, that is annoying and unexpected! I made an issue to fix it #1094

> Can rclone run on windows ? not sure what's going with windows maybe it should also do it with the local fs.

Yes and agreed.

  Nice idea - thanks!

I also keep meaning to make `rclone config file` and `rclone config show` to show the location of the config file and dump it...  No rclone doesn't have a system to do that.

I could imagine checking a file for existence and creating it, deleting it at the end.

It is a bit racy though.

Do you have any ideas? The file with timestamp and heartbeat idea could work.  It relies on the time being correct on the client (but rclone checks that anyway).

Fancy making a PR?
 @vtolstov 
> As i understand this can be generic config option suitable for all storages..

yes I would have thought so.  See [the forum](https://forum.rclone.org/t/moving-the-contents-of-a-folder-to-the-root-directory/914/4)

```
rclone move crypt:/folder crypt:
```

> and I get: Attempt 1/3 failed with 0 errors and: can't move files on overlapping remotes

Is this being more conservative that it needs to?  I'm going to close this for the moment - can you resubmit when you've got something which uses it (eg opendrive).

Thanks

Nick_  I'll just note that the sftp support scheduled for release in 1.36 works really well with C14 (thanks for the test account!).  Something weird is going on there certainly!

`Can't find parent directory for "somepath/"` is a parsing the remote error.

As for why the trailing `/` makes a difference - I think that is the same code...

I'll take a look... I've fixed this in this beta

http://beta.rclone.org/v1.35-136-g30e97ad/ (uploaded in 15-30 mins)

Your command will now give this error

```
$ rclone --retries 1 -vv moveto testfile somepath/
2017/02/22 21:13:20 "somepath/" is a directory
```

which should hopefully be a hint that you need to put a leaf name on the end. I've fixed the overwrite of a directory you mentioned - find that in 79e3c67bbdb0722fd211a67f8a7c5c034a655a15

> I just built rclone from master including through checkin e2f0feef3cf4ff5a15312b5f02e5f7e71ad7ecd8 and I'm still getting the error:

I can't replicate that?

```
$ touch testfile
$ mkdir somepath

$ rclone --retries 1 -v moveto testfile somepath
2017/02/25 11:11:37 INFO  : Local file system at /tmp/b: Modify window is 1ns
2017/02/25 11:11:37 ERROR : Attempt 1/1 failed with 0 errors and: "somepath": is a not a regular file
2017/02/25 11:11:37 Failed to moveto: "somepath": is a not a regular file

$ rclone --retries 1 -v moveto testfile somepath/
2017/02/25 11:11:46 "somepath/" is a directory
```

> Normal semantics for filesystems (and rsync) are that moving to a directory destination should work, so it seems to me like the fix you propose of warning that it's a directory and still failing to do what is expected isn't really the desired result. I think most people would expect moves to a directory to work just fine.

At the moment `moveto` either takes

    rclone moveto file newfile
    rclone moveto dir newdir

If any of the arguments don't exist it will be treated as a directory, and trailing slashes should make no difference.

I think you'd like

    rclone moveto file existingdir
    rclone moveto file existingdir/

to work, which it could - can you make a new issue for that please and I'll have a look at it for 1.37.

Thanks @eharris good news - thanks!  This looks like it is large files on acd problem. When uploading large files sometimes acd takes the file but gives an error. 

Rclone then waits a configurable length of time to wait for the file to appear - that is what you are seeing. 

For big files this is a long time. See the acd upload wait parameter. 

I've had other reports that uploading big files to acd isn't working well at the moment too. I don't think it is rclone's fault though.  > Thanks for such a quick reply. I see that the acd upload wait defaults to 3m0s per GB. Have you heard of any desirable wait times that have proven fruitful? I'm going to attempt --acd-upload-wait-per-gb 10m0s.

The 3m was based on extensive testing and supposed to be a figure that will always work... However things change :-(

> So now I'm getting wrong file size matching with the longer wait.
It seems that ACD is just so unreliable. I may take to the forums to discuss reliable providers.

ACD does appear to have got less reliable over time.  Especially with big files....
  I've merged that - thank you very much.  A great first contribution :-)  Did the mount succeed in the first place?  Check

```
$ mount | grep rclone
local:/tmp/a on /home/ncw/mnt/tmp type fuse.rclone (rw,nosuid,nodev,relatime,user_id=502,group_id=502)
```
  Check that your fuse.conf has `user_allow_other`.

```
$ cat /etc/fuse.conf
# /etc/fuse.conf - Configuration file for Filesystem in Userspace (FUSE)

# Set the maximum number of FUSE mounts allowed to non-root users.
# The default is 1000.
#mount_max = 1000

# Allow non-root users to specify the allow_other or allow_root mount options.
user_allow_other
```

If that doesn't work, you'll get more help in the forum.  Well done for tracking that down - that is a clearly nonsensical series of events.

What do you think should happen here?

I would kind of expect an extension-less file to overwrite a google doc, but it clearly doesn't work properly!  

To fix newObjectWithInfo() shouldn't find the google doc without extension at all - that would be consistent with the ListDir code - the extensionless Google Docs are never output. That would then cause a file with the same name as the google document to be created which would probably turn out OK as far as rclone is concerned and would probably be OK with the user.

Maybe it should be giving an error instead?  That might be more what people expect.

What do you think the correct outcome is?  Once we've worked out that then we can work out how to get there!  I don't 100% understand your setup, but this

> When I perform an upload I use "sudo rclone copy /home/plex/.local/* acd:Zeus" This ensures all of my category folders get uploaded.

means that you are uploading unencrypted files to acd doesn't it?

You will probably be better off asking on the forum about this Glad you sorted it!  > "Failed to sync: error listing: Google drive root 'Documents': couldn't list directory: googleapi: Error 401: Invalid Credentials, authError"

I suspect you filled in 

```
client_id = 
client_secret = 
```

You need to leave these blank. > Nope, I left them blank, as prompted. It took me to a web page where I gave rclone permission.

Ah, OK

> It detected many duplicate files. It did something for 2h 44m before putting out the error.

I wonder if the token expired.

Do you know what it was doing when it went wrong?  Try with the -v flag. @isaiah36 
I would say that is working properly.  A few userRateLimitExceeded is perfectly normal - it is just google telling rclone to slow down.

The sync failed because of this

    2017/01/27 17:40:35 Attempt 1/3 failed with 0 errors and: error listing source: Local file system at \\?\C:\Users\Isaiah\My_Documents: directory not found

So apparently the directory you were trying to sync doesn't exist.

The fact it did a complete remote traversal before telling you is in #1067 which should be fixed shortly.  Move will only delete the source **after** the copy has completed successfully.

Move will retry things in the same way as copy, so if some files fail to move, then they will be moved on the next run.

> Is there a chance move action could loose files entirely in the above scenario?

Move is designed not to lose files so (barring bugs) it should do what you want.
  Any volunteers?  Let me know if you need any help. Assuming there isn't a go module implementing the API, then I would first translate the API datastructures into go structures (see `onedrive/api/types.go` for an example).

I'd then use the `rest` module to interface with the API.

If you look at the `onedrive` remote you can see a remote done in that style.  I'm guessing that it is a directory based remote (not bucket based like s3) so that is probably a good choice to look at.

Also take a look at the docs in [making a new backend](https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md#writing-a-new-backend).  Yes you are right...

However this will be fixed by #517 which is in progress :-)
 I have merged this to master now - see here for a beta:

http://beta.rclone.org/v1.35-74-g48cdedc/

Any feedback much appreciated
 rclone could check that the source directory exists before starting the sync, but that would slow things down in the general case because it needs another transaction.  It will get an error as soon as it starts to use the directory which in the new code is quite quick.

I'm planning on rejiggling the internals of the remotes which will give me a chance to do this properly so I'll put it on the list!  > Add a column for DirModTime support

Do you mean remotes where rclone will read the dir mod time if it is available (like drive, acd but not s3, swift)?

> Add a column for DirCopy support (DirMove is already present)

I don't think anything supports `DirCopy` - at least rclone doesn't support calling it.  There is a `DirMove` column.

> Add a column for Folder support (see #100)

What do you mean by Folder support?  Remotes which actually support the creation of folders like acd, google drive but not s3, swift?  This is probably the same list as DirModTime if I'm understanding you correctly.

> Add a column for indicate support of some form of arbitrary metadata support

Rclone doesn't support any sort of arbitrary metadata at the moment.  Are you wondering what the remotes could support in the future?
 OK I'll see what I can do.  Any help much appreciated so any answers you can find - please place in the ticket.

The metadata bit might be complicated - there are restrictions, etc.  But the total length of the metadata supported would be a useful thing to know - and if that would fit an encrypted name+hash then that solves  a lot of the crypt problems.  What does rclone do with them at the moment?  (try with the -v flag).

Is this just a question of adding a mapping for `application/vnd.google-apps.script+json` exports?

See: https://github.com/ncw/rclone/blob/master/drive/drive.go#L60

Not sure about the parent vs the contents - how do you find the names of the contents if you can't see them?

 Probably what I'd do next is

    rclone -v --dump-bodies copy Gdrive:PowerMute .

Which will dump the http transactions - that should tell you something!  Are files being left behind on the remote, or only directories?

rclone doesn't remove directories at the moment - use `rclone rmdirs` for that.  Which output are you interested in? That would be relatively easy. Fancy giving it a go? In fs/operations.go examine the ListLong function.

You'd want to write a variant of that and call it from cmd/ls/ls.go when a --json flag is set.

Something like that! @Wundark 
I would encode each line at a time, so at the start print a `[` then encode each line then print a `]` at the end. @mh-cbon interesting idea.  That is a whole new level of JSON-ness - I've interpreted this issue as being about just having a version of ls which outputs JSON I've implemented a new command lsjson in this beta - let me know what you think.

https://beta.rclone.org/v1.36-185-g64662bef/ (uploaded in 15-30 imins)  Looks interesting...

Is it something you have to install yourself, or are there public providers?  This is almost certainly SSL performance.

Go re-implemented its own crypto - it doesn't use OpenSSL.  Unfortunately it means that some crypto is a lot slower and AES on ARM is one of those cases.

Possible solutions:
  * it is possible to use OpenSSL with go - it isn't straight forward though
  * use any hardware support for crypto (eg AES instructions)
  * re-write the AES routines in assembler (something I keep meaning to do)
  * persuade SSL to negotiate a faster cipher (eg salsa20)

Related issue: #1013 Can you try this binary which I compiled with go tip?

This might pick up SALSA20 and be a lot faster.  It might just be a bit faster (say 20%) because the compiler improved.  Or it might be no difference...

[rclone-v1.35-42-g9fdeb82-arm-go-tip.zip](https://github.com/ncw/rclone/files/727832/rclone-v1.35-42-g9fdeb82-arm-go-tip.zip)
 Note that I've put in a PR for go porting the openssl AES routines to go.  This doubles the performance again for me on RPi3 - hopefully that will be in go 1.9.

https://go-review.googlesource.com/c/38366/
 The normal rclone betas are now built with go 1.8.3 which should have those performance improvements  I've done this in this beta

http://beta.rclone.org/v1.35-103-gdac4bb2/ (uploaded in 15-30 mins)
  I will do this once #517 has landed. I've done this in this beta

http://beta.rclone.org/v1.35-77-gc0ad29c/ (uploaded in 15-30  mins)
  A lot of people is injured with daily google API bans because of the amount of request done by scanning apps like plex, infuse, renamers, etc.

So, as I said, I thought to include a flag in Drive mounting, to avoid 24h bans from google API, scan will be slower, or a bit less performance, but I prefer it that get a daily ban, or maybe you can mount the drive with the flag to scan, and when you have finished, mount it normally. I consider it a need for people who have big libraries or who do continous scans in their libs.

I thought in a flag to control seeks/reads/requests/gets per second ,like,

--drive-request-by-second  (or --drive-limit-requests)

that's because I've been banned from google API 4 times in 5 days scanning my library (scanning 1 folder with 950 items I was banned in less than 10 minutes (with checkers 1 transfers 1). 

So a little request queue, limiting the requests by second, would be useful too with plex or scanning file apps. I don't know if it will help, but I think It'll do because the api limits, maybe a beta to try how api responses to that limit could be done

Regards This problem/feature is commented on issues:
#485 
and
#972   What does this say?

    rclone ls encfs:

How did you get the encrypted files up to amazon in the first place?

What does your config file say - can you the `encfs` section (remove the passwords)? So if you do `rclone ls acd:media/encfs` you see lots of encrypted looking file names?

But if you do `rclone ls encfs:` you see nothing?

Try the latter command with `-v`, so `rclone ls -v encfs:` and it may debug something useful.

It is possible that the password/password2 is wrong in which case rclone won't see any of the files.  And use it to decide which remotes to run with -subdir in test_all.sh
  Fancy sending a PR? It should be quite straight forward.  Which version of rclone is that from? make vars in the source directory and it will tell you. 

 I think I fixed that very recently in #973 No worries - I'd rather have two bug reports than none!   Try the command without the & 

Then when it is running you can put it into the background with CTRL-Z and then type bg. 

Or you can set an environment variable with the password in - see the docs for more info.   This error leads me to think your fuse installation might be broken.  Can you try updating it?  Maybe try a new kernel?

```
2017/01/19 12:06:35 mount helper error: fusermount: mount failed: No such device
``` @Massaguana if you want to test fuse you could try sshfs which your distro probably has a package for.

You then do `sshfs hostname: mountpoint` where hostname is somewhere you can ssh to (maybe the the same server).  It is certainly an easier feature to implement if rclone doesn't have to persist a db.  However rclone would have to do some of that work as I guess you don't want rclone dropping the cache if you restart your video player for instance so the file gets closed and then opened again. @flixajki wrote
> what did you change to improve seeks and how did that influence on existing buffering ?

There have been lots of incremental improvements to seek.  One of the main ones was getting it to work under crypt without opening the file twice for each seek.

There is discussion of more buffering here: #1043 this wouldn't buffer the whole file though, just do read-ahead. @gordan-bobic wrote
> Actually, a small amount of RAM buffering will make a substantial difference. IIRC, the (AFAICT undocumented) throttling limit is 2,000 requests per hour from ACD. 

Interesting info!

I'll just note that if you are streaming from ACD, then rclone only makes one request, then one request for each seek - it doesn't need to keep making requests as long as you access the data sequentially, ACD just streams the data.

> But the key point is that the average request size is tiny, probably in the 2-3MB range. If we can bump this up by a factor of 10, the entire problem goes away while still only using an amount of RAM small enough that a more complex full file prefetch solution isn't needed.

So this isn't the case - the average request will be for the entire size of the file.

I'll add the buffering in  #1043 first - then we can take a view on how much difference a more complicated scheme buffering to disk would make.  This is a similar, but slightly different idea to #1014

My plan there was for each remote to grow a "root" parameter rather than make an alias type of remote. Creating aliases would be useful for rclone's internals too sometimes (eg `--backup-dir` takes two remotes, where it should only really be one).  If you don't have a : in your remote name, it refers to a local path

    rclone mount foo bar

This will cause rclone to mount the directory in the current directory called "foo" onto the directory "bar".  Due to the way rclone works, if "foo" doesn't exist, then it doesn't matter - it will be created if you copy files into the mount.

So that is expected behaviour. 

However I could make rclone mount check the directory exists which might make more sense - it would then exit with an error "directory not found". OK I'll leave rclone how it is until I get more requests!

Thanks  Good idea.

The changelog for rclone is hand made by me at each rclone release - I don't want to do that for the betas.  However I could put the git log in which is what I generate the changelog from. This is done in

http://beta.rclone.org/v1.35-128-g86cc9f3/ (uploaded in 15-30 mins)

I made the links as per your suggestion and put a git-log.txt file in the zip Foo, I see I've put the files in the wrong place! Second attempt: http://beta.rclone.org/v1.35-129-g980cd5b/ (uploaded in 15-30 mins)  Are these log files in the source?  It sounds like you want to exclude them from the sync.

What file name do they have?  If you paste some examples I can help you construct a filter you can use with sync or delete. Rclone itself doesn't create log files. I don't understand where those log files are coming from. Is it a setting on your Dropbox account, or some other piece of software doing it? 

Could you post a bit of one of the logs (redact sensitive file names if you want) so I can try to identify it?  Try doing

    rclone ls --include "XYZ*"

and if that lists the right files then you can do

    rclone delete --include "XYZ*"
  The problem looks like the fact that rclone mount doesn't support fsync

```
2017/01/17 22:45:50 fuse: <- Fsync [ID=0x4e Node=0x3 Uid=0 Gid=0 Pid=3133] Handle 0x1 Flags 0
2017/01/17 22:45:50 fuse: -> [ID=0x4e] Fsync error=EIO
```

which causes

```
  File "/usr/lib/python3/dist-packages/borg/platform.py", line 10, in sync_dir
    os.fsync(fd)
OSError: [Errno 5] Input/output error
```

`rclone mount` can't support fsync - it isn't something that any cloud storage systems support.  But I suppose it could just return without an error.

I can give that a go relatively easily and post a beta.

I don't guarantee that will make borgbackup work though - the file system that rclone presents has quite a lot of limitations (like you can't read and write a file)...
 I've implemented fsync here.  Can you give it a go please?  If there are other things borgbackup doesn't like about rclone mount then please open another issue with logs - Thanks - Nick

http://beta.rclone.org/v1.35-47-g29c6e22/ (uploaded in 15-30 mins) Thanks for working that out - I'll have a go for directories too! OK here is a beta for you to try (untested!)

http://beta.rclone.org/v1.35-58-g8a11da4/ (uploaded in 15-30 mins) @kconat glad that is working :-)

Unfortunately rclone can't retry the file without buffering it to disk first (that is the way fuse works).

There are some issues about this #1050 and #711 so you don't need to make a new one.

> write request just stalls until borgbackup decides to give up and terminates

What should happen is borgbackup gets an error when writing (or on the close)

Here is what happens when I try copying files with cp and I get one of those errors

    cp: failed to close '/home/ncw/mnt/tmp/xxxx.jpg': Input/output error
  I'll just note there is a FAQ about this: http://rclone.org/faq/#rclone-gives-x509-failed-to-load-system-roots-and-no-roots-provided-error

It doesn't have `apt install ca-certificates` as a solution - do you think it should?  I'm pretty sure that ca-certificates is part of the base install for Ubuntu - not sure why it is missing from your container install.  We are some people that we use cloud storage to save some media files and bluray backups, and use plex (or other) to play our media BD. I've noticed the bad performance on plex _direct play_ (specially when we use acd), but the "good performance" when plex creates a "buffer" of X minutes when _transcode/direct stream_. (no freezes and fast start)

I don't know if it's possible because maybe apps do requests that can't be cached, but I'm gonna try to explain it.

There is possible to create a "read-buffer" when media is played in mount options? 
Don't know if this will affect to other type of file read/analyze media method, so I leave the experts to decide if it's viable to implement. (maybe we could limit buffers only to some media types mkv,avi....)

I suggest to create 3 values in **mounting** options to improve media playing and avoid freezes when play media:

1. **initial-read-buffer**   (in megabytes.....etc. this will be the initial buffer, until this buffer is full, the media won't play media files (mkv, avi, mp4, mpeg, etc), when initial buffer is full, the media will start the play)
2. **total-read-buffer** (the total buffer to use when a media is read, so you will be using this buffer, and while it will be fulfilled downloading from cloud)
3. **file-size-usebuffer**  (use read buffer when the file is >= tan 2G per example)



This maybe could be useful to people who use plex, emby, or direct play from the mount

Regards



 Thanks for your ideas.

I have experimented with buffering and it does make a difference, but it severely affects the seek performance.

rclone streams the data from acd so provided you don't seek, having lots of buffering works well.

However if you do seek, then rclone has to throw all the buffering away and re-open the https connection using a range request.  If you do lots of seeks (like some media players do at the start) rclone ends up downloading a lot of stuff that is thrown away which kills the performance.

What I should probably do is put the buffer option back and set a flag to control it.  2) and 3) of your list would be easy to do.  Not sure about 1) - that would be very detrimental to seeking. Nice, I hope to try it soon.


I have tought too about implement a request queue in mount options to avoid API bans (some apps do a lot of requests in a short period of time, like plex (and don't matter if you put checkers at 1 and transfers at 1)). Or a requests/s feature ¿?. Or maybe that buffer make rclone a bit slow, but it avoids api bans with that loss of performance in seek

Thanks for your hard work @scoopydude2002 auto_cache sounds like a really good idea.

Looks like this is implemented in libfuse (which rclone doesn't use) rather than the kernel, so rclone can't use it directly :-( @flixajki:  Pleople like us have plex servers on fast connections but, ACD sometimes doesn't take the whole bandwidth to download a file.

I consider initial buffer is important only on ACD (slower init than drive). 

when you play media from acd, the initial connection (30segs) never goes above 20-22mbps (I have 300/300), in the first 30 segs, so I always have a freeze at init of playing. So, the initial buffer I mention is to start play while the rest of the buffer is loading (if the flag could be customizable, you could put it as you like avoiding freezes)

The buffer written in disk instead of ram I consider a need with apps like plex, emby, infuse, etc, because you stream more than 1 media and you can run out of ram memory.


@ncw: **Another thing that would be useful is to add some flag to control seeks/reads/requests/gets per second, because I've been banned from google API 4 times in 5 days scanning my library (scanning 1 folder with 950 items I was banned in less than 10 minutes (with checkers 1 transfers 1). So a little request queue, limiting the requests by second, would be useful too with plex or scanning file apps**  
 @NeoPhyTe-x360 re requests / second.  Can you make another issue about that please?  Link it to #485 which is a related idea.  It would be relatively easy to implement I think by limiting it in the http transport... OK, try this beta.  You can control the size of the buffer for each transfer with `--buffer-size`.  It may be a disaster for seeking - let me know!

    http://beta.rclone.org/v1.35-98-g9a9d098/ (uploaded in 15-30 mins) For those in doubt, `--buffer-size` takes a number followed by `k`, `M`, or `G`.  IIf you leave the number out it is in `k`.  This is the same as all the other [SIZE parameters](http://rclone.org/docs/#options) I've just realised that I wasn't re-applying the buffer after a seek...

So can you try this beta please

http://beta.rclone.org/v1.35-101-gf15c6b6/ (uploaded in 15-30 mins) OK here is what I hope is a final tweak (for 1.36 at least) on buffer sizes.

This does a soft start, which greatly improves seeking, and fixes the mis-use of the seek interface.  Together those changes bring it back to 1.35 performance, but with buffering.

http://beta.rclone.org/v1.35-113-g033d1eb/ (uploaded in 15-30 mins) @ncw latest beta link doesn't work, so I can't try v1.35-113 release. Can you reupload?

EDIT: downloading 116 release (they aren't ordered by release number so I didn't see the release at the bottom)

Thanks ncw 117 release tested a bit. For the moment direct play (from samsung tv-> plex<-acd) is freezing after a 10 seconds from play, and not restarting (plex), I don't know if buffer is working because sometimes it's not playing anything after a minute (keeps loading but not playing anything).

mount:
rclone mount amazon: /media/amazon --allow-non-empty --allow-other --no-check-certificate --buffer-size 128M --max-read-ahead 128M --transfers 4 --dir-cache-time 30m --checkers 12 --uid 128 --gid 135 --umask 000 &


EDIT: without the flag happens the same, starts the play 10 seconds, and then freeze forever, no restart
 @endiz  tested without these other flags but the same happens.

I see connection downloading at 36MB/s in the monitor but the play sometimes doesn't start (128mb buffer at 300mbps doesn't take more than a few seconds to fill I think). When it starts (maybe 5/10 times, the other times directly don't play anything), it freezes after 10-30 segs, and won't play anymore @endiz tried 384M but the same happens (now I'm trying 98 release, and it takes more time to became frozen (2-3min) in the first play, in the second play freeze happens at 10 seconds. Not play after the freezes

Now I'm gonna try with --no-seek flag to see what happens

Tried 116,117,114 seeking enabled and buffers from 128mb to 384mb and the same result, long times waiting, freezes and no restarts, etc.

EDIT:
@98release first play with --no-seek --buffer-size 128M, no freezes after 5min ("fast" start)
@98release second play without restart mountpoint, same flags, took a bit more to start, 1 freeze after 30 seconds, but play restarted, no more freezes after 2-3min from cut. Gonna try more buffer size

EDIT2:
@98release first play with--no-seek --buffer-size 512M, no freezes after 5 min (takes about 1 minutes to start playing)


EDIT3:
@117release first play with --no-seek --buffer-size  512M, no freezes after 5min (faster start than r98 with same flags)
@117release second play --no-seek -buffer-size 512M, network error after 1min loading
@117release third play  --no-seek -buffer-size 512M, 4 min loading and anything played for the moment
More problems playing with r117 than r98 (not in the first try, but yes on the rest)

EDIT4: 
@99release with 64mb buffer and no seek for the moment no freezes on first, no freezes on second play, and starting "fast", gonna try more
3th and 4th play fine too on r99  64mb buffer no seek mountpoints
Tomorrow I'll try r99 with seeking

EDIT5: Tested r99 without --no-seek flag, and the performance playing is worse than with the flag added, buffer is working as I see the ram increased, the problem sometimes is that don't matter if you enable buffer or not, freeze is shown (but restarted in r99), maybe it could be the "initial" amount of connections to acd in the first play? 

@ncw , I'm seeing that on the first play after mount, the performance is almost perfect, but the rest of the plays after that, are very very slow to load, and sometimes not loading anything. With --no-seek I see a bit improve. r98 takes a bit more to play, but it's playing fine 
r99 is fine for the moment on all plays, noticed some freeze of 1seg but at 10minutes or more and not on all plays (72mb buffer atm) @ncw , noticing the rclone is maintaining in memory the buffer for about 1-2min, so maybe the problems comes from there.  (on r99, the other new releases are working very bad testing with plex)
If you start to read a file, buffer fills in memory and play fast, you stop the file copy/read, and the buffer stills there for about 1-2minutes, so, if you play the second one in the first 5minutes, the play starts very very slow. The problem is that's not ocurring every time, only sometimes.

Could be a problem related with IO writting? or simultaneous reads/copys? It is possible to writte buffer in disk (/tmp or somewhere) instead on kernel buffer? (I don't know where is written now, but I don't see anything new in /tmp)

For the moment r99 with this is working fine 90% of times I play something with some sporadic freeze of 1seg (in 2/10 plays): 	rclone mount amazon: /media/amazon --allow-non-empty --allow-other --no-check-certificate --no-seek --buffer-size 100M --max-read-ahead 128M --transfers 15 --dir-cache-time 30m --checkers 12 --uid 128 --gid 135 --umask 000 &




EDIT: For the moment, tested the above and working fawlessly. Will test that release for the moment all the week. (tested to direct play from remote plex and working fine too) @NeoPhyTe-x360 - thanks for all your testing.

> I'm seeing that on the first play after mount, the performance is almost perfect, but the rest of the plays after that, are very very slow to load, and sometimes not loading anything

Playing the same file or a different one?

Can you do some tests with the latest beta? http://beta.rclone.org/v1.35-123-g6e0e1ad/

I'd like to see -123 with `--buffer-size 128M` (say) and `--buffer-size 0` and also a comparison with 1.35.

I think one of the problems with testing with ACD is that it is very variable in performance.  Sometimes it doesn't even deliver enough bandwidth for streaming so if you could repeat each test as many times as possible that would be useful.
 @ncw 
> Playing the same file or a different one?

Different ones but the same size more or less (to avoid if buffer is saved somewhere)

Gonna test new version, later I'll edit the post
All test are being done doing direct play on a samsung tv (not transcoding or stream)

r123 @ 128M buffer with --no-seek (1000-1500kbps bitrate files):
-First play loading takes a bit more than r99 with the same flags (30 segs more), after a minute 1 freeze of 1-2seg but replay is fine, no more freezes during 5-6 min playing
-Second play takes the same time to run (1minute~), no freezes during 5-6 min
-Third play takes a bit less time to play but nothing remarkable (maybe depends on acd initial speed I think), no freezes

r123 @ 256M with --no-seek (1000-1500kbps bitrate files):
-First play was very fast (less than 15segs :S :S), but frozen after 20seg (30seg freeze), replay is fine, but other froze after 20-30 seg, restart and no more frozes
I'm seeing it's depeding a lot of the "first" connections to amazon, sometimes connection is fully at start, and sometimes the conn is about 2mbps for about a minute (frozes frozes, slow start etc).
-Second one, very fast start with no freezes during 5-6min
-third, same, fast start, no freezes

r123 @128M (1000-1500kbps bitrate files):
-First play was fine on speed, but froze after 30 segs, it happens the same as older betas without no-seek flag, once freeze has happened it takes a lot to restart (for the moment I'm about 2 minutes on the freeze waiting the restart), edit: movie not resumed after 4-5 minutes frozen.
-Second play, movie is not loaded after 3-4 minutes waiting.
-Third play, finally movie played, but waited a lot (2 min~~), more waiting than with no-seek flag, I'm playing the same 5-6 movies, so movies are ok.
-fourth play, not played after a few minutes, don't know what is doing seeking, but is doing something here

r123 @ 0M (1000-1500kbps bitrate files):
First play was fine, 30-40 segs to load. No freezes
Second play the same.
720p:
Third I played a 720p file, start was fine, but froze after 30seconds, and restarted after other 30 secs. All the test I did yesterday with r99 was 720p files, so maybe I redo all tests tomorrow with 720p files. (yesterday with no-seek and r99 no frozes at all in 720)
Fourth (720p), good start, but froze after 1 minute~~, restarted 15-secs after, no more freezes



I remember in 1.35 did some tests with acd and direct play 720p's: the play started fine, but always I had only ONE cut in 30-40secs, after that, no more freezes.
....will do more tests in a few hours



If I have time tomorrow I'll do some tests to with google drive @NeoPhyTe-x360 thanks for testing.  That is a huge amount of variability :-(  I suspect ACD is introducing the variability, so testing with Google Drive would be very interesting.

Are you using the `--no-seek` flag?  I would hope that it would work well without the `--no-seek` flag. Using the `--no-seek` flag might be the cause of some of the freezing but not restarting if the player can't rewind the stream.  Not sure how the player will respond to the errors it gets back when it tries to seek. I'm going to close this issue now as I think the general consensus is that the changes work reasonably well.

There is undoubtedly more work to do though with buffering! @ncw thanks for your hard work. I was very busy this weeks, so I could't try more than I tried that day. I will try more once I have a bit time, so I haven't used plex neither this time.

Thanks  Can you post a log with -v of the problem 

What does 

    rclone ls remote:

And

    rclone lsd remote: --max-depth 100

Say?

Is there some sort of file in the directory tree that rclone doesn't see but does appear on the web interface?  I saw a suggestion somewhere (can't find it now) that this was caused by trashed files

I just tried it and that replicates the problem exactly.

```
$ rclone ls drive:a_file
$ rclone rmdirs --retries 1 drive:a_file
2017/01/27 21:07:42 : Failed to rmdir: directory not empty: []*drive.ChildReference{(*drive.ChildReference)(0xc420410300)}
2017/01/27 21:07:42 Attempt 1/1 failed with 1 errors and: directory not empty: []*drive.ChildReference{(*drive.ChildReference)(0xc420410300)}
2017/01/27 21:07:42 Failed to rmdirs: directory not empty: []*drive.ChildReference{(*drive.ChildReference)(0xc420410300)}
```

I'm not sure how to fix this though!  Any ideas? @gustavorochakv you are right!

I've adjusted the drive code to trash the directory if it has any trashed files in (and no non trashed files).  Otherwise the directory will be trashed or deleted as normal depending on `--drive-use-trash` flag.

Here is a beta with the fix

http://beta.rclone.org/v1.35-104-g33c2873/ (uploaded in 15-30 mins)  @4getit wrote
> The name --drive-chunk-size suggests that this has effect on Google Drive only.

That is correct

> Is --drive-chunk-size supposed to work for ACD as well?

No.

> If not, shouldn't there be an --acd-chunk-size parameter?

ACD uploads aren't chunked in the same way so no chunk size parameter.

I think Amazon may limit total uploads speeds anyway, so you might not be able to get it to go faster.

  I think this is a duplicate of #923 ?

If so could you add your support to that issue please?

Also if you have any ideas about how rclone would implement it, please add there!  I think this a duplicate of #40 which is scheduled for the next release.  Yes you are right there is no mention of `-u` in the [darwin docs](https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man8/umount.8.html)  Good idea.

Not too tricky - fancy having a go? That would be great > Just to see if I got it clear: @okaresz is talking about ignoring Google Docs when syncing/copying files from one place to another or just about "deleting" Google Docs files if they are not present in the source location?

Ignoring them.  I'd imagine a flag `--drive-no-docs` which would stop rclone reporting the google docs in listings. @okaresz wrote
> can you help me pointing out what doc files should I update for this new cmd line switch? :) I don't see which md files are auto-generated, or whether I should build/update the website as well with hugo, etc... (I haven't found any documentation about documentation :D)

You are right, I should write some docs on how the docs are built!

You should only need to edit `docs/content/drive.md` - everything else is auto created from that.

I make the full docs, the man page, website when doing a release. I added a section in the docs about writing docs!  20c033b48442dfd866074aee6a0f2dd1e6a2abb6 Thanks for doing this @okaresz 

A beta with this feature is here

http://beta.rclone.org/v1.35-54-gff8f11d/ (uploaded in 15-30 mins)
  Nice idea.  This could be done as part of #637 What if there was an upgrade path from standard encryption to enhanced encryption - would that satisfy you? (One that didn't involve downloading and uploading!)  I'm sorry, I thought I replied to this, but I must have forgotten to press submit or something :-(

Travis is complaining about you not checking the error from `os.MkdirAll`

The patch itself looks OK, but I'm concerned about backwards compatibility - I don't want rclone to stop working for people with their config in the old place.

 >Thanks, I've rewritten the patch to be backwards compatible, i.e. it uses ~/.rclone.conf if it already exists, otherwise it looks for ~/.config/rclone/rclone.conf or creates it if it doesn't exist.

Looks good :-)

> I think I've properly checked the error from os.MkdirAll

I'd have probably done a `log.Fatalf("Failed to make config directory %q: %v",xdgcfgdir, err )` if the `os.MkdirAll` failed rather than carry on.

> but make check is failing now? Can you spot what is wrong in the code?

make check is complaining that you haven't run `go fmt` on your code.

I get my editor to run that on a save hook.  (Actually I use goimports which writes the imports lines, but same idea).

----

Here is a thought I had in the reply I forgot to send that I just remembered

In [the XDG spec](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) it says

> $XDG_CONFIG_HOME defines the base directory relative to which user specific configuration files should be stored. If $XDG_CONFIG_HOME is either not set or empty, a default equal to $HOME/.config should be used.

So maybe it should be defaulting to `$HOME/.config` even if the variable isn't set?  This answer [on superuser.com agrees](http://superuser.com/a/425712/97813)

On my ubuntu laptop I have lots of XDG vars but not XDG_CONFIG_HOME, but I do have a .config with lots of stuff in it.

Thoughts?

```
$ env | grep XDG
XDG_VTNR=7
XDG_SESSION_ID=c2
XDG_GREETER_DATA_DIR=/var/lib/lightdm-data/ncw
XDG_MENU_PREFIX=xfce-
XDG_SESSION_PATH=/org/freedesktop/DisplayManager/Session0
XDG_SEAT_PATH=/org/freedesktop/DisplayManager/Seat0
XDG_CONFIG_DIRS=/etc/xdg/xdg-xubuntu:/usr/share/upstart/xdg:/etc/xdg:/etc/xdg
XDG_SESSION_TYPE=x11
XDG_SEAT=seat0
XDG_SESSION_DESKTOP=xubuntu
XDG_DATA_DIRS=/usr/share/xubuntu:/usr/share/xfce4:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop:/usr/share
XDG_RUNTIME_DIR=/run/user/502
XDG_CURRENT_DESKTOP=XFCE
``` > I've made some more changes, I agree with defaulting to $HOME/.config.

Great

> I'm only wondering if it's consistent to exit with log.Fatalf if os.MkdirAll fails, when the default behavior of the program in all the other cases (also before these patches) is to use a .rclone.conf file in the current directory.

I'm happy to go with what you suggest.

> Then I can't make the tests pass on AppVeyor (Windows): I've tried with 3 additional patches, but to no avail... :( Especially I was very confident that test3 would pass, even though it wouldn't have been a definitive solution, but no way... Maybe you have more experience and can tell what's wrong?

Looks like I broke the Windows build :-(  WIll fix shortly - then you'll need to rebase and force push. I've fixed the windows build in 9fdeb823282e5b6af943b62a644af932c0bcc7ba - really sorry about that! > No worries, thanks for the fix, but now are the Travis/Linux builds that fail with "An error occurred while generating the build script", is it still something on your end?

Travis end I think.  I restarted those builds - will check back in a bit... I've tested that and merged it thank you very much!

I updated the docs to reflect that we use `.config/rclone/rclone.conf` even if `XDG_CONFIG_HOME` isn't set - I think that must have been from a previous revision.

I'm glad you've done this - I keep being nagged by a colleague at work about this (Arch user also) and this will make him very happy!

Thanks

Nick  This issue came up before in #34 - and there we decided that `--drive-use-trash` was a reasonable workaround - that doesn't seem to be working for you though?

What error message does it give?  Exactly which version are you using? Multiple --exclude flags is a relatively recent feature. 

Have you tried with the latest beta? 

Post the output of --dump-filters also.  I think if you upgrade to 1.35 (which seems to be in debian unstable - not sure about testing) then this should be fixed.

I had no idea rclone had been packaged for debian - thanks for letting me know!

I'll close this - please reopen if necessary.

Thanks

Nick  Nice idea.  Something like it has been in my mind but it has never made it to an issue.

Your analysis is spot on.  Prefilling the cache using the sync listing routines should be relatively easy.

Some thought needs to be given to when to do it.

Initially to keep it simple, it might be best to do it initially, and on the same frequency as the `--dir-cache-time` maybe. This issue will get fixed as part of #897 I think...  Looks interesting! @eharris very useful list - thank you  > It would appear that rclone checks what files are in local and remote locations only at the beginning of the sync. It doesn't seem to check again just before uploading the file.

That is correct.

I don't usually recommend using rclone from two places simultaneously.

However, doing a metadata refresh and check just before uploading a file would be relatively cheap compared to uploading a file.  It could be an option maybe?  I haven't been able to reproduce this.

How big is the file you are copying?  How often does the cp go wrong?

Can you share the rclone log (redact it if necessary) showing the 429 error and what happened before and after?

What should have happened is that rclone should have returned an error on the 429 error and cp should have failed.  I see!  I'm going to fix this as part of the changes I'm making to simplify the remotes in 1.37  I don't really want to change the meaning of the `--checksum` flag like that.  In the docs it says

> Normally rclone will look at modification time and size of files to see if they are equal. If you set this flag then rclone will check the file hash and size to determine if files are equal.

I think checking the modification time as a last resort would be surprising for the user.

>  HB doesn't know whether a remote supports checksums or not, and can't easily determine this.

Perhaps this is the thing we should be working on?

rclone should be able to generate the tables in: http://rclone.org/overview/ in some programmatic format quite easily, or maybe just the values for a given remote.

What would work for you here?  To link things which work with rclone

Eg
  * https://github.com/rhummelmose/rclonesyncservice

Any suggestions should have a desired link text and a link.

See also [forum thread](https://forum.rclone.org/t/suggestions-for-third-party-tools-which-use-rclone-or-enhance-rclone) @hashbackup do you want to put hashbackup here? @hashbackup - can you do me a link + link text for the page please? @brandur nice one :-) @mmozeiko 
> I've made simple cross-platform GUI for rclone: https://mmozeiko.github.io/RcloneBrowser/
Works on Windows, macOS and GNU/Linux.

That looks amazing! I'll certainly put that on the page.

It would be interesting to know if there are things you'd like rclone to do to make your life easier.  This relies on there being a stable FUSE for windows.  There have been movements in that direction but I don't think we are there yet.  It also would require working from go which makes things a little trickier. @billziss-gh - WinFsp is a very impressive project.  I read about your difficulties with undocumented APIs and debugging - quite a challenge!

Unfortunately I'm not much of a Windows programmer - I did a fair amount of Win32 programming mostly to port cross platform programs to windows, but I've never done windows first development.  I've tried to keep the non cross platform code in rclone to the absolute minimum.

Actually rclone uses the higher level `path` interface that `bazil/fuse` provides not the lower level inode based interface.  rclone is very much path based internally.

I think the path of least resistance for rclone would be to use the C based FUSE compatible api and use cgo to interface with it.

That would probably mean making an alternative mount command which used cgo to interface with libfuse, and re-use the internals of the current mount command.
 @billziss-gh wrote:
>  I could not find a path based API for bazil/fuse.
I expect you are right and I'm confused about what exactly is meant by the path based interface.

cgofuse looks great!  I had a look through the code - very impressive for a first go program :-)

I had a go with it on linux and a brief smoke test with it worked (I didn't try fstest or anything like that, just a bit of ls).

I'll send you a PR with some fixes I had to make to get it to compile.  I suspect you have symlinks in your GOPATH or something like that.

As for rclone, I guess what is needed is a new mount command to use cgofuse - the internals of mount need factoring out into a package, then rclone could use cgofuse directly, and hopefully WinFsp in due course.

I already have an alternate mount system (not released yet) using the other go fuse implementation [hanwen/go-fuse](https://github.com/hanwen/go-fuse) so that internal refactoring is overdue.

Looks very promising :-) @billziss-gh wrote
>  Feel free to make any suggestions for improvement. In particular I am wondering if the way I am mapping C pointers to Go interfaces and back (in [handle.go](https://github.com/billziss-gh/cgofuse/blob/master/fuse/handle.go)) is the best way of doing things.

That code will certainly work - it is very conservative.

I'd have probably just used an index into a slice rather than the result of C.Malloc(1) - it is an opaque pointer after all.  You'd need to change the unsafe.Pointer to a uintptr though as unsafe.Pointers are supposed to point to valid memory.

All those type assertions `.(FileSystemInterface)` are ugly though, so I'd probably make two versions one specialized for `FileSystemInterface` and the other for`FileSystemHost`.

> This is just me resisting the way Go package paths work. I basically do git clone repo in the go/src directory rather than go get repo.
> 
> My main problem with go get repo is that I do not fully undestand the src/github.com/user/repo scheme. What if, for example, I move the cgofuse repo to bitbucket.org? Are users of cgofuse expected to change their github.com imports to bitbucket.org imports?
> 
> Still if this is the way it has to be done in order to play in the Go ecosystem that's fine.

I felt the same way when starting go.  Go is very opinionated about some things and this is one of them, so it is best to do it the Go way otherwise you'll be fighting a losing battle!  Yes if you move your source code repo you need to change the code :-(  There are ways of making a vanity url you can use, but it turns out not to be much of a problem in practice.

> Cgofuse is now ported to OSX, Linux and Windows (WinFsp). The OSX and Linux ports are being tested using fstest. I have not completed fully testing the Windows port yet.

Great - well done :-)

> Once Windows testing is complete, I will set up CI through Travis for Linux and AppVeyor for Windows. I note that although Travis supports OSX, it does not do so for Go. Any other CI suggestions for Go on OSX?

I test rclone on OSX via Travis.  Check out [rclone's .travis.yml](https://github.com/ncw/rclone/blob/master/.travis.yml) - I test all go version on linux, but only the latest on OS X.

> One final note: Cgofuse is currently licensed under the GPLv3. I will probably change it to a more liberal license to ease adoption, likely the BSD.

:-)

> Cgofuse is a simple layer over the C FUSE layer. It does not have any complicated code paths. The only potential problems are in how marshaling is done between the C and Go worlds or if some Go object that gets passed to C gets garbage collected. If there are any such remaining problems, we should be able to catch them with some testing.

I expect you've seen [the docs on pointer passing](https://golang.org/cmd/cgo/#hdr-Passing_pointers) which has a section on runtime flags you can set to do more checking.

Also note that `go vet` does some checking of `unsafe.Pointer` use.

...

Next step for me to experiment with cgofuse and rclone :-)
 > I probably made this too generic because I was not sure how I would end up using it. But I am wondering: is there a performance penalty to using type assertions? Or is that suggestion more of a stylistic issue?

Using an interface is effectively a pointer lookup, so same penalty as a virtual method in C++.  Doing a type assertion is more expensive as it involves scanning a table for type information.  However it is pretty quick...  [This SO question indicates it is 5x slower than the direct interface call](http://stackoverflow.com/questions/28024884/does-a-type-assertion-type-switch-have-bad-performance-is-slow-in-go) - I think it depends on how many possible types there could be though.

> Fantastic. Note that I made a couple of changes to the public API:

Noted!

I've been factoring the current mount code into a library so I can re-use it with cgofuse.  I've made cgofuse work as far as directory listings which means I'll probably get the rest working over the weekend :-)

I haven't tried building on Windows yet, but I'll have a go with your instructions in a bit.
 I have put the first version (not quite finished) of cgofuse for rclone in the [cmount](https://github.com/ncw/rclone/tree/cmount)  branch.  It has a `cmount` command which works just like the [mount](https://rclone.org/commands/rclone_mount/) command except it uses libfuse under linux/osx and WinFsp under Windows.  I haven't tried building it for Windows yet though so that might not actually work yet!

Got to go to bed now - will try building and testing it with Windows tomorrow!

PS This project https://github.com/karalabe/xgo looks like an excellent way of doing a cross compile build for Windows... @billziss-gh thanks for your hackpatch!

I've added that in in a cross platform way in 3fe64e44e41e7ec02027c1c124c34c9938e3e33b - thank you!

I didn't add this chunk as I didn't seem to need it, unmount happened when I CTRL-C the rclone process.  Am I missing something?

```patch
@@ -260,7 +262,7 @@ func Mount(f fs.Fs, mountpoint string) error {
 	}
 
 	// This isn't needed under Windows as it gets unmounted by the cgofuse
-	if runtime.GOOS != "windows" {
+	if true || runtime.GOOS != "windows" {
 		sigChan := make(chan os.Signal, 1)
 		signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
```
 For anyone who would like to play with mount on Windows here is a binary.  You'll need [WinFSP](https://github.com/billziss-gh/winfsp/releases/tag/v1.0) installed.  Run it with `rclone cmount remote:path X:` where X: is an unused drive letter.

It likely has a lot of bugs still so don't use it for anything you care about!

[rclone-v1.36-48-g3fe64e4-cmount-windows.zip](https://github.com/ncw/rclone/files/981888/rclone-v1.36-48-g3fe64e4-cmount-windows.zip)

I built this against WinFSP v1.0 - I probably should have used 1.1Beta... @vampywiz17 you'll need to compile it on Windows as it needs cgo.  I installed the mingw compiler, the WinFsp dev and copied the include files into the mingw include directory.  There is a binary above if you just want to have a go with it. @billziss-gh have fixed up uid.gid in dc0f9087463e42e2dfcd6d42f92a869498f53f48

> if you have any ideas on how to streamline the Windows build for cgofuse (and consequently rclone) I will be happy to modify cgofuse. At some point I added a Makefile, but removed it when I found out that Mingw-builds does not come with make! I also thought about adding a winbuild.bat batch file to avoid having to set Mingw PATH and CPATH, but somehow it felt "wrong" to do something like this on Go.

I'm not sure of the best way...  Normally go builds take no configuration at all which is kind of relaxing once you've got used to it!  [go-sqlite3](https://github.com/mattn/go-sqlite3) might be interesting to study.

I'm using this little .bat file to compile rclone on Windows (thanks for the hint about `CPATH`).

```bat
@echo off
echo Setting environment variables for mingw+WinFsp compile
set GOPATH=X:\go
set PATH=C:\Program Files\mingw-w64\i686-7.1.0-win32-dwarf-rt_v5-rev0\mingw32\bin;%PATH%
set CPATH=C:\Program Files\WinFsp\inc\fuse
```

TBH the whole mingw install seems a bit of a mess, and given that Windows doesn't seem to have standard places for compilers or include files, trying to do anything other than show a bat file like the above as an example is probably too hard.

As for making a release build for rclone I'll try to cross compile it if possible so it can fit in with the rest of the builds, failng that I'll build it on AppVeyor.

I had a look at the rclone.exe binary and it doesn't seem to depend on a WinFsp DLL.  Squinting at the code confirms that it loads the WinFsp DLL dynamically.  So this means I can make the Windows release include the cmount code but not require WinFsp to be installed unless mount is needed.  Is that right?

@sbr481 here is a new version with the fix for the uid, gid as suggested by @billziss-gh - it works for write in my very brief testing.

[rclone-v1.36-50-gdc0f908-cmount.zip](https://github.com/ncw/rclone/files/983223/rclone-v1.36-50-gdc0f908-cmount.zip)




 Try this version - note that you need to use `rclone mount` now (not `rclone cmount`)

[rclone-v1.36-58-g824c66e-cmount.zip](https://github.com/ncw/rclone/files/984274/rclone-v1.36-58-g824c66e-cmount.zip)

If you are still getting the same error (I didn't when I tried it) then run the mount with `-vv` and post the relevant logs please!

Note that if you are asking for properties on a directory then it will need to scan the whole directory which may take some time and network bandwidth. @nicko88 wrote
> Does this new mount command in these builds here support parameters like --max-read-ahead ?

It should support all the same parameters @billziss-gh wrote:
> Yes, that is correct. The intent was to give maximum flexibility and not make WinFsp a hard requirement for rclone on Windows.

Perfect :-) @sbr481 wrote:
>  'mkdir' still throws as output 'incorrect function' after creating directory (this 'really' is a problem only if launching a recursive copy, because makes to stop copying when needs to create a directory in the recursion).

I haven't managed to replicate this - can you give me a screenshot of the command you used and the result.

Also the exact command line you are using for rclone and the type of remote.

I have managed to replicate using `copy` to overwrite a file giving access denied messages. @billziss-gh wrote

> "Incorrect function" is likely the error message for ERROR_INVALID_FUNCTION. This can be produced for a variety of reasons on Windows, but in our case the primary culprit is the Windows kernel status code STATUS_INVALID_DEVICE_REQUEST. This can also be produced if someone returns -ENOSYS from the FUSE layer. @ncw do you return -ENOSYS anywhere in the latest version?

Ah,.. Yes... I haven't implemented every FUSE callback, so if the `fuse.FileSystemBase` returns `-ENOSYS` for some things then rclone will.  I've probably forgotten something important!

@sbr481  Passing the `--debug-fuse` flag to rclone will set `-o debug` which should show which

> EDIT: tested with a normal remote (Google Drive without crypt):
· Same problems using 'mkdir' or 'md' (output: incorrect function). But directories are made.
· Same problems using 'copy' (output: access denied), and file is created only (without content), with 0 bytes.
It seems is nothing related to crypted or not crypted remote. Same behaviour in both remote types.

I seem to be able to make directories just fine using google drive (no crypt).  I wonder if this is because I'm using Windows 7 for my tests?

> @ncw: does rclone pass -o uid=-1,gid=-1 to WinFsp-FUSE or some other value? For example, I am not sure the value -o uid=4294967295,gid=4294967295 will work.

rclone doesn't pass uid or gid at all, it sets them in the Attrs returned.


 Here is the latest version

* Statfs: reduce max size of volume for Windows
  * this should make the volume size be 15.9 TB not 0!
* allow extra options to pass to fuse with -o
  * for testing
* Add -o uid=-1 -o gid=-1 for Windows/WinFsp
  * this should fix the permissions issues - thanks @billziss-gh 
* implement no-ops for Fsync, Chmod, Chown, Access, Fsyncdir and stop using fuse.FileSystemBase
  * this should fix one of the Mkdir issues - the "incorrect function one"  thanks again @billziss-gh 
* Add function tracing
  * this should provide enough logging to track down the other Mkdir issue
  * I'm not quite sure what is causing it so logs needed please!

[rclone-v1.36-66-g2cb5a60-cmount.zip](https://github.com/ncw/rclone/files/986787/rclone-v1.36-66-g2cb5a60-cmount.zip)

Testing in CMD and the cygwin shell (it should work in both) appreciated!

@vampywiz17 wrote
> What do you think guys, what he prob. the GUI operations? (copy, move is extremely slow) on CMD, it is really fast, but the GUI command buggy now. you need some log?

The GUI seems to do this approximately 35 times after copying a file which won't be helping the performance. I have no idea why!

```
2017/05/09 18:22:18 DEBUG : /zz: Getattr: fh=0xFFFFFFFFFFFFFFFF
2017/05/09 18:22:18 DEBUG : /zz: >Getattr: errc=0
2017/05/09 18:22:18 DEBUG : /zz: Getattr: fh=0xFFFFFFFFFFFFFFFF
2017/05/09 18:22:18 DEBUG : /zz: >Getattr: errc=0
2017/05/09 18:22:18 DEBUG : /zz: Open: flags=0x0
2017/05/09 18:22:18 DEBUG : /zz: >Open: errc=0, fh=0x2803
2017/05/09 18:22:18 DEBUG : /zz: Flush: fh=0x2703
2017/05/09 18:22:18 DEBUG : /zz: >Flush: errc=0
2017/05/09 18:22:18 DEBUG : /zz: Release: fh=0x2703
2017/05/09 18:22:18 DEBUG : zz: ReadFileHandle.Release closing
2017/05/09 18:22:18 DEBUG : /zz: >Release: errc=0
```
 Here is the latest code

[rclone-v1.36-74-g0a020f88-cmount.zip](https://github.com/ncw/rclone/files/992512/rclone-v1.36-74-g0a020f88-cmount.zip)

Changes are

* on read only open of file, make open pending until first read
  * This fixes a problem with Windows which seems fond of opening files just to read their attributes and closing them again.
* Wait for mountpoint to appear on Windows before declaring mounted
* pass --FileSystemName under windows
* implement --fuse-flag to pass commands to fuse library directly
  * Useful for `--fuse-flag -h` to see exactly which options the library supports.
* fix openFile leak

@vampywiz17 wrote
> i try again the --buffer size flag, but it FC, if it will arrive the pre-set buffer size. I attach the log:

That is out of memory (as is your other log).

Try again with the latest beta - I fixed a couple of things which could have caused that.

If not please send a log!

@sbr481 wrote
> As vampywiz said, --read-only does not work.

I haven't fixed that yet, but I've received a steer from @billziss-gh  as to the best way

@billziss-gh wrote
> It is actually possible to mount a file system as a network drive by using the special WinFsp-FUSE option: --VolumePrefix=\server\share

You should now be able to pass that option to rclone like this

    --fuse-flag --VolumePrefix=\server\share

You can alsu use `--fuse-flag -h` which shows you exactly what options you can pass.

@billziss-gh wrote about rclone not supporting read/write files
> I wanted to mention this because it took me by surprise at first. I think it might be worthwhile to consider implementing caching functionality for rclone akin to the Andrew File System (again as discussed elsewhere). This would solve many of the problems and it can perhaps be done in a way that solves (or minimizes) the problems with eventual consistency in cloud storage systems.

Yes! I plan to spool uploads to disk quite soon.  The main reason is to make uploads more reliable.  Cloud storage systems being connected over the Internet are quite unreliable (in filesystem terms).  It is easy to retry read operations, but write operations are impossible to retry as none of the cloud storage systems have a ranged PUT (some of them do do resume, but rclone doesn't support that yet).  Getting the file onto local storage would then mean uploads can be retried at leisure.

A full solution like AFS would involve downloading files so they could be r/w modified - I'd like to get this far eventually :-)
 I found a major memory leak in the pending open code!  This should also fix really slow transfers.  Here is a fixed version

[rclone-v1.36-89-g6352557e-cmount-rebase.zip](https://github.com/ncw/rclone/files/995583/rclone-v1.36-89-g6352557e-cmount-rebase.zip)

 @sbr481 wrote
> It seems like it crashes because it starts to use a lot of RAM, at least in my tests; always crash when arrives near 850 MB. I've observated how it starts to grow it's consumption, and always crash when arrives near to that amount of RAM.

Ah, I see I've been compiling rclone as a 32 bit binary which explains the 850 MB limit.

Here is a 64 bit binary for you to try

[rclone-v1.36-90-ga8f76175-cmount-rebase-amd64.zip](https://github.com/ncw/rclone/files/995610/rclone-v1.36-90-ga8f76175-cmount-rebase-amd64.zip)

 @vampywiz17 excellent!

> I found a another bug.
> 
> If i create a new .txt file, open it, write something and i try to save it, it say that no write rights. But i able to delete/rewrite the file. I attach the log.

The error is this from the log

    2017/05/12 11:33:56 ERROR : /Sorozatok/Új szöveges dokumentum.txt: Can't open for Read and 
Write

This is a known limitation of the linux/OSX version too.  It will get fixed by caching files locally #711 > Do you know about the 1.36-90 mount problem?

That is the 64bit version.  I haven't tried it at all (don't have a 64bit VM) so I wouldn't be suprised if it doesn't work.  What did it do? @vampywiz17 thanks!

I forgot to enable CGO when cross compiling....

Try this version (again untested by me!) built with

```
X:\go\src\github.com\ncw\rclone>go env
set GOARCH=amd64
set GOBIN=
set GOEXE=.exe
set GOHOSTARCH=386
set GOHOSTOS=windows
set GOOS=windows
set GOPATH=X:\go
set GORACE=
set GOROOT=C:\Go
set GOTOOLDIR=C:\Go\pkg\tool\windows_386
set GCCGO=gccgo
set CC=gcc
set GOGCCFLAGS=-m64 -mthreads -fmessage-length=0 -fdebug-prefix-map=C:\Users\Dev
\AppData\Local\Temp\go-build370856747=/tmp/go-build -gno-record-gcc-switches
set CXX=g++
set CGO_ENABLED=1
set PKG_CONFIG=pkg-config
set CGO_CFLAGS=-g -O2
set CGO_CPPFLAGS=
set CGO_CXXFLAGS=-g -O2
set CGO_FFLAGS=-g -O2
set CGO_LDFLAGS=-g -O2
```

[rclone-v1.36-91-g91ac31c4-cmount-rebase.zip](https://github.com/ncw/rclone/files/995850/rclone-v1.36-91-g91ac31c4-cmount-rebase.zip)

 A bit more reading  convinced me that what I was trying to do by cross compiling 64 bits on 32 will never work :-(  You can't do cgo while cross compiling without a lot more work.

I managed to build rclone using `xgo -targets windows/* .`  I had to copy the WinFsp header files into `cgofuse/fuse` to get it to compile which isn't a good long term solution as it breaks all the other builds!

This has a 32 bit and 64 bit binary which should both have a `mount` subcommand.

[rclone-v1.36-93-gfafd0512-cmount-rebase-xgo.zip](https://github.com/ncw/rclone/files/997910/rclone-v1.36-93-gfafd0512-cmount-rebase-xgo.zip)

@billziss-gh interesting idea on binary only packages.  It would solve part of the problem - rclone would still have to be compiled with cgo though which would mean a cross compile environment was needed.  I'm not sure it is worth doing just for not having to install the WinFsp headers.

If we were to squeeze the WinFsp development parts into the form [as described in the xgo docs](https://github.com/karalabe/xgo#cgo-dependencies) that would fix the cross compile issues.  It would presumably be quite easy to make a configure which did nothing and a make install which copied the header files into the right place.

I've been pushing commits to the `cmount-rebase` branch (which I shall force push so beware!) in preparation for merging it back to master.  I want to get mountlib back into master as people keep sending me patches I have to forward port!


 @billziss-gh wrote
> After a bit of experimentation I got AppVeyor to produce Windows binaries for both x86 and x64 architectures: https://ci.appveyor.com/project/billziss-gh/cgofuse/build/135/artifacts

Nice!

You need to add `package fuse` to the two `.go` files BTW.

Relaxing the cgo build constraint I get this cross compiling on linux

```
cmd/cmount/fs.go:12: import /home/ncw/go/pkg/windows_386/github.com/billziss-gh/cgofuse/fuse.a: object is [windows 386 go1.8 X:framepointer] expected [windows 386 go1.8.1 X:framepointer]
g
```

So it is nearly working, and it is going to need a build for each go compiler version which is annoying.

(short pause to build go1.8)

and I get this

```
/opt/go/go1.8.0/pkg/tool/linux_amd64/link: cannot open file /opt/go/go1.8.0/pkg/windows_386/runtime/cgo.a: open /opt/go/go1.8.0/pkg/windows_386/runtime/cgo.a: no such file or directory
```

Which is kind of what I was expecting. It is probably not impossible to build that cgo.a.

However that is the problem xgo solves. I think it is probably less effort if we think of a way of getting the WinFsp headers into xgo.  I can do this by copying them in as part of a build step and deleting them afterwards which isn't too bad, or we can make an xgo "cross platform" package.

> Thinking more about this it might be worthwhile trying to integrate xgo into a Travis CI build for cgofuse, so that we get a single zip with all architectures in it.

Yes that would work.

Hmm, I wonder if you can[ run docker on Travis](https://docs.travis-ci.com/user/docker/)... Yes. Have to use the non-container build but that would be OK if a bit slower. The xgo container is massive though!

```
$ docker images
REPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE
karalabe/xgo-latest           latest              0532a33f402b        4 weeks ago         4.322 GB
```

I think the optimal solution might be to build the windows builds on Appveyor (provided I can figure out how to build the 32 bit version **and** the 64 bit version) and push them up separately.  Each release is a separate zip for each os/arch so that would work ok without too much work.

Anyway in summary I think there are 2 possibilities for automated builds
  * Use xgo on Travis
  * Build the windows builds on Appveyor, and the rest on Travis (like they are at the moment)

Using xgo to build locally is convenient for me running on linux. I've been working on the build system...

Can someone with a win64 system have a go with this?  I managed to produce it direct from Appveyor.  Still haven't figured out how to build a 32 bit version on a 64 bit platform :-(  Probably installing a 32 bit go and using a 32 bit C compiler will do it.  I need to test on my 64 bit VM, but I had to restore it from a backup of 3 years ago and it has a LOT of updates to install!

https://beta.rclone.org/test/v1.36-98-g78b7fff4/rclone-v1.36-98-g78b7fff4-windows-amd64.zip

@billziss-gh wrote
> Interesting that cgo is still needed. Is it because a program that contains Cgo libraries requires to be linked with the native linker?

I'm not sure.  It is certainly missing the cgo.a library which probably could be sourced.

@vampywiz17 I can't really see what is going on in that log.  Is it possible you had lots of files open - from the file handle numbers it looks like you did.  If you are using `--buffer-size 1G` then each open file can potentially use 1G of memory (though only if the file is that big).  Can you try agian with much smaller `--buffer-size` and see if that makes a difference? @billziss-gh thanks for your help - your appveyor examples have been really useful.  I have finally managed to make travis and appveyor build windows, osx and linux versions with a great deal of messing around.  The os x / linux builds I am building without cmount and the windows build with.

I'd like to make the cgofuse fit into the xgo framework in due course, but having travis and appveyor doing the builds is good enough for the moment.

I've now merged the cmount branch to master so beta builds will appear.  Here is the latest.

https://beta.rclone.org/v1.36-98-g7ee3cfd7/

I'm going to go through this thread and pick up bugs to fix, after that I'm going to close the issue as it has got rather long!  So sort of like a RAID1 array for remotes.
  I don't see this on linux, so I think it is probably an OS X specific problem.

Here is a version of rclone build with go tip - does that help?

[rclone-v1.35-23-ga7d8ccd-osx-go-tip.zip](https://github.com/ncw/rclone/files/698394/rclone-v1.35-23-ga7d8ccd-osx-go-tip.zip)

Another thing you could try is build rclone on your OS X machine locally.  That will be able to use the native SSL libraries which may make a difference. @garyo that seems strange - `--no-check-certficate` shouldn't do anything for just rclone help.  Did you have any other parameters?  A 460 error from dropbox means "Restricted Content".  That means that dropbox thinks the file has a virus, or contains copyright material or something like that.  [Search on Dropbox 460](https://www.google.co.uk/search?q=dropbox%20460&rct=j) and you'll see lots of stuff about it.

You can download it using the web interface though?  I suspect this is because on ARMv8 go doesn't use the built in AES instructions.

There is an issue about it here: https://github.com/golang/go/issues/18498 - fixing that will bring the speed back to openssl speeds.  It looks like some progres has been made on it - it might be in go 1.9.

Which hardware are you using?

If you try doing curl or wget to an https address on the ARMv8 board, how much CPU does that use for a given amount of throughput?

There is a project to use openssl from go here: https://github.com/spacemonkeygo/openssl - it requires a patched net/http which makes it a bit tricky to install. > A better implementation in Go still wouldn't immediately address the hardware accelerator/coprocessor offload

Are there other crypto accelerators built in to your board other than the ARMv8 AES instructions which I believe are a standard part of ARMv8?

> via the /dev/crypto interface, which OpenSSL can do.

Are you using linux? According to wikipedia [/dev/crypto was never merged in mainline](https://en.wikipedia.org/wiki/Crypto_API_(Linux)) linux.

It looks like a great idea and go could directly target it if available... If you wanted to have a go with https://github.com/spacemonkeygo/openssl

Then it would be relatively easy, you'd just need to add this in `fs/http.go` in the `Transport` method

```go
t.DialTLS = func(network, address string) (net.Conn, error) {
        return openssl.Dial(network, address, context.TODO(), 0)
}
```

There might be a bit more stuff, but that plus [the docs](https://godoc.org/github.com/spacemonkeygo/openssl#Dial) should give you a start.

I'm unlikely to do this by default with rclone as adding dependencies to C libraries breaks the cross compilation, but it could be set up so you could supply a build tag to make it work quite easily. If someone would do the same thing for the AES instructions and upstream that to the go project that would solve all the problems for ARMv8!  I've merged this - thank you very much.

I squashed the commits and added  a commit message for the first commit since it seemed to have got mixed up somehow.


Do you fancy sending me a followup PR with
  * a bit more documentation in cmd/obscure.go - you could add the docs you wrote above
  * maybe if you don't supply an argument it could read the password using `fs.GetPassword()`

Probably best if you [revert your master branch to sync with rclone's master branch](http://stackoverflow.com/questions/1628088/reset-local-repository-branch-to-be-just-like-remote-repository-head), and [start a feature branch](https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches) for the above.

Thanks

Nick  This has been merged in d4c923a5ccadb4f189ce2f501afbfc0ca0bf263e  I recently merged something which will allow you to configure rclone through environment variables - see #616 - does that help?

I'm not sure how you would script the oauth login - how do you see that working?  > One thing I'm not sure that I got right: I'm not sure if the goroutines that weren't first to get the new error will "fail fast" when the first error gets set. That said, I'm not sure that it really matters.

Once the error gets set the other goroutines will abort.  It isn't instant but it should be quite quick. I've merged this in 3b1e0b66bbf78024baf06d51c3de837135ef95e2 - thank you very much for your contribution :-)

I rebased and I edited the commit message slightly to take out the questions since we've resolved them - I hope you don't mind.

If you have any 3rd party tools using, or compiling with rclone that you'd like to share I'd love to have them in #1019

Thanks

Nick  The error `readdirent: input/output error` is encfs returning EIO while reading a directory.  Can you run encfs with more logging and have a look at its logs - that might give you a clue?

> I am wondering if it is timing out on directories containing lots of files, as since it is mounted over the network very large folders might experience latency. I didn't see any relevant settings on the sync command.

rclone doesn't have any timeouts while directory listing.  You say it is mounted over the network.  Is that with NFS?  It might be worth trawling through the NFS logs too.

It might be worth seeing if you can reproduce the problem with find - using an operation that needs to stat each file, say

    find /Users/gryphon/Encrypted -size +1G

See also: possibly related #364 which I never got to the bottom of, but I was convinced it was an encfs problem.
  ...in such a way rclone could use it with an "rclone" remote.

Perhaps allow it to be served over ssh or some other socket too. This is actually a very very interesting idea.  I saw someone mention 'rclone serve' in the forum.  If this was available, I can see a very basic app created on the android platform, for example, where it would keep rclone serve http/ftp/whatever up and running in the background and we could then stream/access our files via http.  Imagine being able to modify the http css as well via the app and we have a inbuilt application now to via/access all our rclone data whether it be via crypt or noncrypt.

Right now, im forced to not crypt some content so that I can use Amazon's app to access those files remotely (surveillance videos).  With something like this, I can crypt everything.  

  That is very useful thank you.

What would you think about updating this commit to update other remotes too? I've squashed and merged those - thank you very much for doing that.  I'll probably do this with a flag `--crypt-show-mapping` which in combination with `-v` will show the mapping of crypted files to unencrypted files whenever they are listed.  So this could be used with `rclone ls` or `rclone sync` etc. I've just implemented this - please find the beta here

http://beta.rclone.org/v1.35-46-g390f3cf/ (uploaded in 15-30 mins) @DurvalMenezes that will get sorted out when I make the release.  Unfortunately the betas come out with the old docs due to the way they are made.  I should probably work out how to fix that!  Thanks for reporting. @DurvalMenezes :-)  I've merged that - thank you very much :-)  I've just fixed this - try the latest beta  Note that this is metioned in #230 also  I think this is related to #739
 I'm currently re-working the logging.

Rather than allow sending different logs to different files, I'm going to put a tag on the front of each log message, NOTICE, INFO, DEBUG, ERROR which will make grepping the log files trivial.
 I have reworked the logging - please have a go with it in this beta

http://beta.rclone.org/v1.35-87-g006227b/ (uploaded in 15-30 mins)

Here are the bits from the docs

## Logging ##

rclone has 4 levels of logging, `Error`, `Notice`, `Info` and `Debug`.

By default rclone logs to standard error.  This means you can redirect
standard error and still see the normal output of rclone commands (eg
`rclone ls`).

By default rclone will produce `Error` and `Notice` level messages.

If you use the `-q` flag, rclone will only produce `Error` messages.

If you use the `-v` flag, rclone will produce `Error`, `Notice` and
`Info` messages.

If you use the `-vv` flag, rclone will produce `Error`, `Notice`,
`Info` and `Debug` messages.

You can also control the log levels with the `--log-level` flag.

If you use the `--log-file=FILE` option, rclone will redirect `Error`,
`Info` and `Debug` messages along with standard error to FILE.

If you use the `--syslog` flag then rclone will log to syslog and the
`--syslog-facility` control which facility it uses.

Rclone prefixes all log messages with their level in capitals, eg INFO
which makes it easy to grep the log file for different kinds of
information.
 I messed up with that beta - try this one instead http://beta.rclone.org/v1.35-92-g18c75a8/ (uploaded in 15-30 mins)  I'm sorry you are having problems with this.  I think this probably isn't related to #677 - practically any error in the crypt data transfer will cause that error message.

Can you perform a test for me?  I'd like you to see if you can download the encrypted version of a file, then try to decrypt it locally.

To work out which file it is, you'll need to find out what it is called.  The easiest way of doing this is to find the directory

    rclone -v ls egd:REDACTED/REDACTED/REDACTED/REDACTED.mp4

This will print a messagae like this

    2017/01/08 12:10:20 Encrypted Google drive root 'h0fgn6vkka25jj9fhu5roml6pg/rv5l12j0nanu5af9ivc97ofdk8/gtu1leijh9rlm05pcjtusqop2g': Modify window is 1ms

That will be the directory that the file is in.  You can find exactly which one it is by matching the sizes in the web interface.

Once you've identified the file, can you try downloading it with rclone copy (using the non encrypted mount).  Say to a directory called /tmp/downloaded

If you then edit your config file with a text editor and duplicate your `egd` remote calling it `elocal` say and change the remote line to be `remote = /tmp/downloaded`, this will let you decrypt things locally.

You can then use `rclone ls elocal:` and if you can try `rclone copy elocal:REDACTED.mp4 /tmp/redownload` and see what happens there.

This will enable me to see whether
  * the file is corrupted on google drive
  * or rclone is having a problem downloading the file (which would also give that error).

Can you also try downloading the ecnrypted file with the web intereface and see if it is the same as the file you download with `rclone copy`.

Other questions
  * is it only large files you see this problem on?
  * what does rclone check say?

I note that #854 would be useful here. If you have the original file, another useful check would be to rclone copy it into the elocal: and then compare that file with the one you downloaded from drive - (size and md5sum).  Ach, I forgot about the padding...  Yes your guess of the file of size 262060813 is correct

Calculation using python

```
>>> divmod(262060813-32,(65536+16))
(3997, 49437)
>>> 3997*65536+49437-16
261996813
```

> No problem -- every piece of software more complex than "helloworld" has issues, and in this case, no one can assert that the error is indeed on rclone: the data could have been corrupted on GDrive itself.

Yes that is the first thing we need to do - work out where the corruption is happening.

> About the only thing I can say is that it has not been corrupted in my servers: I use ZFS (which has built-in checksums) for everything, and all the machines involved in the above transfers have ECC RAM (my only 'computers' without ECC RAM are my laptop, my mediacenter and my smartphone -- and I plan on getting an ECC-based laptop ASAP as they are now somewhat available).

We have similar levels of paranoia about data loss!  You'd probably enjoy one of my other programs [stressdisk](https://github.com/ncw/stressdisk) if you haven't seen it already.

> OTOH, is there any particular reason for the rclone -v output not including each file's encrypted full path+filename? If not please let me know and I will open an issue for it, as I think it would make these kinds of determinations much easier.

I agree that would be useful.  Probably what I'd so is make the name mappings show if you do an `ls -v` - how does that sound?  If you wanted to make an issue about it that would be great.

> 16 hashes could not be checked

Can you try it over the whole remote?  I'm concerned as to whether `rclone check` will notice those missing files you reported.

> I see what you mean. Even better would be an rclone-equivalent of a "md5sum -c" (perhaps a "-c" option to "rclone md5sum"?), where I supply a remote path and a local .md5 file (eg, one generated using "md5sum * >file.md5" or equivalent), and then rclone calculates the MD5SUM for every remote file mentioned in the .md5 file and checks against the MD5SUM contained in that file and prints the appropriate message just like a local "md5sum -c" (or one run against a "rclone mount" on the remote) would.

That is effectively what `rclone check` does at the moment - however crypt doesn't support hashes.  With the addition of #854 it would work for crypted remotes too

A `-c` option to `rclone md5sum` is a good idea too.
 @DurvalMenezes - thanks for all your testing and for making the issues
> I think that this corroborates the above: the encrypted file that is now stored on GDrive is already corrupted (ie, in a state where rclone can't decrypt it), it is not being somehow modified by rclone during "download".

Yes that is my reading of it.

> So it seems that, after the "RCLONE\000\000" header, the files are utterly different.

Thinking about it further, that is what I'd expect as they have different randomly generated nonces - so that wasn't a particularly useful test, sorry :-(

I thought of another test which might shed some light...

If you could `rclone cat egd:REDACTED/REDACTED/REDACTED/REDACTED.mp4 > tmp.mp4` or `rclone cat elo:REDACTED.mp4 > tmp.mp4` if you prefer and see how big the resulting output is that would be very useful.

That will tell us which block to look at to see the corrupted data in the encrypted file you downloaded.

The output file will be some multiple of 65536 long.  So take the file size and divide it by 65536 call it N.

This corresponds to a block at offset 32+N*(65536+16).  It would be interesting to see what the data in the encrypted version of the file for the next 64k looks like.  If it doesn't look like random data then there will be a clue there.

If we see a problem at a `--drive-chunk-size` boundary it is something to do with the multipart uploading.
 Try that with the `-q` flag - `-v` writes to stdout by default

    rclone -q cat elo:REDACTED.mp4 | wc -c
 Hmm so a corruption after only 5MB of data - that isn't what I was expecting - I thought it would be corrupted at 8MB boundaries to do with the chunked uploading.

It would be worth eyeballing the 64k chunk to see if there is anything you recognise in it (you could run strings on it) but likely ent is correct.

I've attached a version of rclone which will print errors when it receives corrupted blocks and fill them with 0x55 and carry on.

If you try the rclone cat with this version, how many error messages do you get? And how much of the file is uncorrupted?

[rclone-ignore-crypt-errors.zip](https://github.com/ncw/rclone/files/694646/rclone-ignore-crypt-errors.zip)

I'm going to try the upload process with the race detector and see if there is a problem there. > I don't think this is what you expected, tho ;-)

Ha! No.  Will try again in the morning (it's late here now) . OK, I had a better idea...

I've attached a version of rclone which has a fixed nonce.  **DO NOT USE THIS FOR ANY OTHER PURPOSE!**

The nonce should be the once from your original encrypted file from the first hexdump above (it is possible I chose the wrong hexdump - I wasn't 100% sure).

You can then use this rclone to perform that experiment again, copying the original file into elo:.  This time the files should be identical except for any corruptions.  Assuming I haven't messed something up you should be able to generate the corrupted and the original hex dumps and diff them which hopefully will reveal something interesting! @DurvalMenezes here is the actual test file - sorry I forgot to attach it!

[rclone-fixed-nonce.zip](https://github.com/ncw/rclone/files/704889/rclone-fixed-nonce.zip)

@4getit Cause unknown as yet, but likely only drive. @4getit wrote
> I'm still struggling to keep my encrypted remotes copies the same as my remotes.
I noticed that when I run sync several times on an unchanged remote, it keeps going and thinks that different files have changed.
> 
> Now I noticed that it happens every time after the rate limiter kicks in.

The rate limiter is a fact of life when uploading to google drive.  I think it unlikely the two are related.  I've noticed you get the most rate limiting from google when you are doing the directory scanning.  Immediately that stops it will start to upload files.

If you want to email me a complete log with `-v` and with subject "log from @4getit from rclone issue 999" I'll take a look at it. ( nick@craig-wood.com )
 @jrarseneau and @4getit  Are you using the latest beta?

If you have a corrupted file with the latest beta when you do an `rclone copy` of the file is it always corrupted?  (Try rclone copy multiple times)

Is it the same size as the original (use rclone ls)? @jrarseneau So that file is definitely corrupted.  Can you download the unencrypted version?  If you see earlier in the thread you'll see how to find the unencrypted version.  I'd be really interested in a hex dump of the last 1k of the file say.  I have a suspicion that this was caused by #902 which manifests itself copying from one network remote to another.  If it was, then you'll see a repeating pattern in the last 1k of data.

@4getit - thanks for testing.  I see you were doing a network copy too.

@DurvalMenezes were you doing a network copy when you uploaded the files to drive?
 @jrarseneau 
I've just implemented --crypt-show-mapping (see #1004 ) which will help when working out which decrypted file is which encrypted file.

http://beta.rclone.org/v1.35-46-g390f3cf/ (uploaded in 15-30 mins) @jrarseneau 
> Quick Q. if/when this get identified, the only solution to files currently corrupted on Gdrive will be to completely purge the library and re-sync? Or will there be a way to identify which ones are corrupt?

One way would be to try to download all the files - the corrupt ones will fail with this error.

Another might be using #854 to download the files and check the checksums will show which files are corrupt (this won't take storage space).

It it does turn out that these files were corrupted by the over-the-network-remote->crypted-remote transfer, then I might be able to make a tool to identify files which were corrupted like that.  That will depend on what you find though! @jrarseneau Last 1k in hex `tail -c 1024 file | hexdump -C`

Provided @DurvalMenezes says the corruptions were seen after a transfer from network -> crypted gdrive, I'm pretty sure this was caused by #902

I had an idea on how to detect corrupted files - I'll alter the `rclone cat` command and make an `rclone tail` command.  Reading the last byte of every crypted file will be enough to detect corruption introduced by #902.  So I imagine you'd so something like `rclone tail -c 1 remote: > /dev/null` and look at the errors produced.  This error can only have happend for files bigger than `--drive-upload-cutoff` I think (8M by default) so you could add `--min-size 8M` in there to speed things up. See #819 for tail. @jrarseneau thanks for testing.  I was expecting a repeating pattern, but after doing a bit of testing I think I was mistaken...

The tail test I've suggested above is quite a good confirmation I think too - I'll implement that soon and post a beta here. @jrarseneau 

In your first step you decrypted and then encrypted - this will change the md5sum.

If you were to sync the `acd:Media` to `gd:Media` then this test would work.

In a sec I'll post how to check for corruptions using `rclone cat` Now that I've done #819 you can use this beta to check for corruptions

http://beta.rclone.org/v1.35-76-gd091d4a/

Use it like this

     rclone cat --tail 1 --stats 1m --discard encryptedRemote:

This will run through every file in the `encryptedRemote:` and read and discard the last byte of the file.  If corruptions were introduced with #902 this will detect them. 

Reading the last byte of a crypted file isn't as trivial as it sounds!  First it has to read the header and starting nonce, then calculate the nonce of the last block, read the last block (up to 64k), decrypt and only then output the last byte of the decrypted block.

If it filnds a corrupted file you should get an error level log with the file name and the `failed to authenticate decrypted block` error and the errors will count up.

The `--stats 1m` is optional.  Increasing the number of `--checkers` will make it run faster. 

I would be very interested to hear your results and in particular how many and how big any corrupted files are.
 @naeloob 

The lines like `* REDACTED/REDACTED/REDACTED/REDACTED.jpg` are caused by `--stats 1m` - it is just what it is doing at the moment.  Remove `--stats 1m` if you don't want to see them.

> And one line like :
2017/02/08 09:11:14 REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/REDACTED.log: Failed to open: unexpected EOF

That might just be a network error - it would be interesting to try that on its own and see if the error is repeatable

    ./rclone cat --tail 1 --stats 1m --discard RemoteCrypt:REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/REDACTED.log

 > Tested again 5 times with that file and another one that failed too.
> Always the same result : Failed to open: unexpected EOF

OK, that isn't the error I was expecting, but it isn't entirely unexpected...

What happens if you try to copy that file back with rclone copy?

    ./rclone copy RemoteCrypt:REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/REDACTED.log /tmp/download @naeloob - thanks for that.  I'll have to study exactly what is going on there.  Does the file have the expected length when you use `rclone ls` on it? @naeloob

Are they both log files which may be written to while rclone is running?

That would explain the sizes differ errors.

That explains the UnexpectedEOF error.

I would have expected to see `Failed to remove failed copy:` messages about that file too.

Can you check that rclone didn't duplicate that file

    run rclone dedupe  RemoteCrypt:REDACTED # on the directory the file is in.
 @naeloob 
> Yes, It could be that the files were written while the rclone copy.

OK

> "Failed to remove failed copy" appears twice on the log, but referencing other files.
> Rclone dedupe just says "Looking for duplicates using interactive mode." and exit on both paths.

So they aren't duplicated which is good.

@jrarseneau 
> I installed the new beta and ran the command. I got 990 unexpected EOF

Could those have been files which were being written to while rclone was uploading?

Did you see any other types of error?

Those files are likely corrupted - you can try downloading them individually and see if you get any difference.

I don't know why they are giving Unexpected EOF errors though - I was expecting "failed to autheticate decypted block" errors. @jrarseneau try copying a few files down if you can.  If not then these files are corrupted and you'll need to re-upload them :-( You can use --files-from with a list of the files and then `rclone delete --files-from file_list remote:`

Then use `rclone sync` to fill in the gaps.

> This is disappointing though and I'd like to know why so many files got corrupted

Me too.

I'm pretty sure it was #902 though.  I'll need to think through where the UnexpectedEOF comes in though.

> Rclone really needs a way to hash check files on crypt, kind of like hash check, upload file, hash check validate and then move on.

I think what you are asking for is #1102 @DurvalMenezes 
> could "rclone cat --tail" be doing anything that plain "rclone cat" doesn't, and that could justify a ban?

What it does is use range requests to seek to the end of the file.  It might be that google is somehow fussy about that. It will also be a lot quicker than not transferring the data so maybe it just runs faster... @DurvalMenezes looks like it has leaked a goroutine per file which is unexpected...

```
$ rclone cat --tail 1 --discard . --stats 10s -v
2017/02/09 11:00:03 rclone: Version "v1.35-DEV" starting with parameters ["rclone" "cat" "--tail" "1" "--discard" "." "--stats" "10s" "-v"]
2017/02/09 11:00:03 Local file system at /home/ncw/go/src/github.com/ncw/rclone: Modify window is 1ns
2017/02/09 11:00:03 
Transferred:   4.762 kBytes (58.399 kBytes/s)
Errors:                 0
Checks:                 0
Transferred:         4913
Elapsed time:          0s
2017/02/09 11:00:03 Go routines at exit 4917
```

Gah, a stupid mistake....

Have fixed in

http://beta.rclone.org/v1.35-79-g50e190f/ (uploaded in 15-30 mins)

Thanks for reporting!

> What could have happened? Perhaps the excessive number of goroutines (over 44K seems a lot for just 48 checkers). This machine has 6GB RAM plus 6GB swap, and at the time the above was being run, less than 250MB (0.25GB) total was in use... :-/

For big files (> 10MB or so) rclone allocates 16 MB of buffer to speed up the transfer.  What was happening is that rclone was allocating those buffers but not using them, so using lots of virtual memory but not much real memory.  You'd only need 750 files bigger than 10 MB to use 12 GB of virtual memory.

Fixing the go routine leak has fixed the memory usage, but I've fixed the overeager allocation of buffers too so it now runs using very little memory.


 @Stonedestroyer 
> My old backup directory transferred fine using the tail test, but my new one seems to be affected

How did you transfer your new backup directory?  I'm guessing it is an encrypted google drive and the contents was transferred from another encrypted non-local place (eg ACD, not the local disk)

> But this shouldn't affect mount and normal rclone copy transfers without encryption?

It shouldn't no.  I think the corruption was introduced under quite specific conditions - you had to be transferring from a nonlocal crypted remote and there had to be a certain sort of network error. > I got a EOF last time but running with your new version, everything was local to gdrive. Been running for 5 minutes and no errors so far

rclone will retry --low-level-retries times, then signal a retry of the whole sync.  In the case of `rclone cat` no higher level retries are done. > Alright tried newest beta, after command is done nothing happens. It just goes back to cmdline. Does that mean there was no errors?

Quite likely.

What was your command line?
 What is in the log file?  It should show any errors if found and some stats if the command took longer than 1m to run. > But I take it that is fine and just shows transfers.

Yes that is right. @Durval yes those all look like network or overload errors. Reducing the number of checkers will help I would have thought.  @DurvalMenezes That looks like some sort of ban if it is failing on everything.  It is a shame the error message isn't a bit more specific as to what kind of 403 error you got. @DurvalMenezes don't know - ask on the forum? I've implemented rclone cryptcheck (see #1102) in this beta which gives an alternative way of checking that a crypt remote is intact.

http://beta.rclone.org/v1.35-91-g01c747e-cryptcheck/ (uploaded in 15-30 mins)

Here are the docs

----

rclone cryptcheck checks a remote against a crypted remote.  This is
the equivalent of running rclone check, but able to check the
checksums of the crypted remote.

For it to work the underlying remote of the cryptedremote must support
some kind of checksum.

It works by reading the nonce from each file on the cryptedremote: and
using that to encrypt each file on the remote:.  It then checks the
checksum of the underlying file on the cryptedremote: against the
checksum of the file it has just encrypted.

Use it like this

    rclone cryptcheck /path/to/files encryptedremote:path

You can use it like this also, but that will involve downloading all
the files in remote:path.

    rclone cryptcheck remote:path encryptedremote:path

After it has run it will log the status of the encryptedremote:.

Usage:
  rclone cryptcheck remote:path cryptedremote:path [flags]
 Correction: this is the beta http://beta.rclone.org/v1.35-92-g18c75a8/ (uploaded in 15-30 mins) I'm going to close this now as the underlying issue was fixed by 2abfae283cd6128cf36bb31bffad1c818e82bbb4 @DurvalMenezes yes that fix is in.  Are you copying over the network from a crypted drive?  If so that will be the fix. @DurvalMenezes thanks for the confirmation and let me know if you do see any errors that were definitely copied from the local filesystem. @DurvalMenezes I'd be tempted to grep out the failing files from the log and put them in a form I could pass to --files-from.  You could then use the -I flag to unconditionally upload them. For what it is worth, I just ran this on my google drive remote that I created as a clone of another google drive remote and I had zero errors:

rclone -v cat --tail 1 --stats 1m --discard  zonegd-cryptp:

no ban either.  I'll also run this on the source I created.  The source was created with copy/sync from a local share.  

Both remotes are crypted.

EDIT:  oh its marked fixed.  :(  Thats what I get for reading on my phone!!!   Smaller than yours.  3TB with 96,000 files. Also I use my own client id so I can track my quotas in the API dashboard.  I think this is probably a duplicate of #711 - what do you think?  Provided the reader isn't seeking, rclone will read the files using streaming, so that will be just one request.

There is a certain amount of read-ahead - I did try adding more but it then can interfere with the seeking.

I need to revisit this! > The access pattern exhibited by media streaming server such as plex is such that they open a file, read a chunk at the required position and close the file. The read the next chunk and close the file, repeat. AFAICT

Ok that is interesting.  Have you verified that by looking at the rclone mount logs?  Nice idea.

I think this could be achieved by detecting the name of the binary as you say then doing textual manipulation of the `-o parameters` to make the as per what rclone normally requires.

note putting rclone in the background is #723 I'll try to do this for the 1.37 release...

Or unless anyone else wants to have a go?  Bad luck!

I'm not clear on exactly what happened though
> note: I made an error, and copied the folder REPORTS (from '/P2/A/REPORTS/') inside another place 'P' where it was already existing with the same content. Too bad for me as it appears to me that it deleted my "root/.rclone.conf" file to 0 byte. ;(

Was "root/.rclone.conf" in that directory tree anywhere?

Is it possible your disk got full at one point?  Then when rclone attempted to update the config file with a new token it wrote it out a 0 bytes?

BTW http://rclone.org/remote_setup/ has a procedure for doing remote setups which you might find easier!  Nice idea - any Windows developers reading this that would like to have a go? Ask any developers you know to see if they could help :-)  I've merged that - thanks for fixing :-)  > I was using the --delete-before flag! So I am afraid it somehow interprets the bucket is "empty" and wipes the local sync folder.

If there are errors when listing the bucket it should stop and not delete anything.

Can you attach a log with `-v --dump-headers` of this happening please?  Running with `-v` should reveal what is happening.

You might try increasing this

     --acd-upload-wait-per-gb duration   Additional time per GB to wait after a failed complete upload to see if it appears. (default 3m0s)
 > Post https://content-eu.drive.amazonaws.com/cdproxy/nodes?suppress=deduplication: read tcp 172.20.0.8:42096->52.214.55.243
:443: read: connection reset by peer ("HTTP status UNKNOWN")

That is a network error, so --acd-upload-wait-per-gb probably won't help here.

That is one of
  * amazon closing the connection
  * your router closing the connection
  * your ISP closing the connection

I don't know which of those is more likely but it might be worth investigating your router (reboot it, upgrade its firmware)

I guess it could also be SmartOS LX zone (which I know nothing about!).  Nice idea.  Fancy having a go? Ok here is an outline of what you need to do...
  * in fs/config.go make a new flag (or flags) and stuff them into the Config struct
  * in fs/sync.go in the `run` method, you need to break out of the `Do the transfers` loop when your limits are reached.  Curernt transfers will finish neatly to completion if you do that, and there may be some objects in the pipeline, but that is the neatest way to do it.

Then write a test or two, do the docs and you are done! > Do you think checking these two values in the run loop would cause any perceivable slow down?

No.  rclone is limited by network speed and disk speed, not by CPU.

> Figuring out how to adapt the Bytes, MB, GB conversions that are in place already for the bandwidth limits to the total size limit. I'm not sure if it's best to copy the existing as is, and create a whole new process for my variables, or add logic to the existing code to handle both at once. Still thinking on the best way to handle this.

You shouldn't need to make anything special here - look at a `SizeSuffix` config variable - that does MB/GB parsing for you into an int64. > I'll have to copy the format for the time limit, got to convert minutes, hours, and days to seconds. I'll likely just adapt the bytes conversion to a base 60 for time. Unless it's already in a package 

You can use a time.Duration for this, eg

    cmd/mount/mount.go:	commandDefintion.Flags().DurationVarP(&dirCacheTime, "dir-cache-time", "", dirCacheTime, "Time to cache directory entries for.")
  Eeek!  Will look at it tomorrow. It should be easy to fix though.

Thanks for testing. OK I fixed that hopefully - I've put it in a fix-track-renames branch - can you have a go with that and see what you think?

Thanks

Nick Can you work out why it is slower? 

I note that it could be optimized further by only calculating the hashes of files which have a size which might match - that is probably the difference.

The trouble with your implementation is it has an [O(N^2) loop in](https://github.com/bep/rclone/blob/renames/fs/sync.go#L345), so if you, say, moved all your photos (10000 say) to a subdirectory and did a sync, then rclone would be doing 10000x10000 times around the loop

Will investigate some more tomorrow - past my bed time now, so probably none of the above makes any sense ;-) Firstly I'd like to say thank you for your contribution.

I'm sorry you feel that I wiped out your code. I overstepped the mark in changing the code without discussing my concerns about it with you first.  Please accept my apologies.

Yes I am probably too picky about the code, and I'll try to take your comments on board for future pull requests.  I definitely haven't mastered the art of being an open source maintainer so I'm always willing to learn.

Thanks for your feedback

Nick  The bash autocompletion script is provided by the cobra library - I suggest you add your support to this issue: https://github.com/spf13/cobra/issues/107

re the compile problems - Which go version are you trying?

Did you try `go get -u github.com/ncw/rclone` ?

The travis builds are all succeeding at the moment so it is probably an updating rclone problem, or using an old go version. I've put this into the "Known Problems" milestone.

I subscribed to the issue too!

> P.S. Thank you for the great product!

You are welcome!  I've merged that now - thank you very much - very nice feature :-)  I just had a look at the code.  The corrupted hashes are in order src, dst so from

    2017/01/02 21:41:53 Attempt 1/3 failed with 1 errors and: corrupted on transfer: MD5 hash differ "53a0e5a5db26301f97d390a51155cae7" vs "cd6c3d2dc4e91101b5000357dfd96e73"

"53a0e5a5db26301f97d390a51155cae7" is the hash that google thinks it is

"cd6c3d2dc4e91101b5000357dfd96e73" is the hash that we get when downloading - which agrees the web interface.

So for some reason google has the checksum wrong on these objects.

`--ignore-checksum` which I'm implementing in #793 would help you here.


 #863  is the same problem.

I'll try to get #793 done soon! Can you try this which implements the --ignore-checksum command?

http://pub.rclone.org/v1.35-15-g82742f5-ignore-checksum%CE%B2/ I've merged this to master for the 1.36 release

Here is the beta - http://beta.rclone.org/v1.35-62-g9d331ce/ (uploaded in 15-30 mins)  I've not seen that before...

I've always found it a little unsatisfactory that rclone uses the existence or not of an MD5 to check whether something is a file or not but I didn't find anything better.

This is what it says [in the drive v2 api docs](https://developers.google.com/drive/v2/reference/files)

> md5Checksum	string	An MD5 checksum for the content of this file. This field is only populated for files with content stored in Drive; it is not populated for Google Docs or shortcut files.
	
> fileSize	long	The size of the file in bytes. This field is only populated for files with content stored in Drive; it is not populated for Google Docs or shortcut files.

So maybe changing `case item.Md5Checksum != "":` to `case item.Md5Checksum != "" || item.FileSize > 0:` would be a fix for you.

I made a binary with that fix in for you here http://pub.rclone.org/rclone-v1.35-11-g13bfe06-980-drive-files.zip

Can you give it a go?

Thanks

 Thanks for testing - I'll merge tomorrow.

Do you think that an unknown object should cause the sync to fail? That would be the consequence of incrementing the errors.  Or do you think it should just be a higher level log?  I'm tending towards the latter - what do you think? I've merged this to master now, here is the beta

http://beta.rclone.org/v1.35-60-g28f9b9b/ (ready in 15-30 mins)  Thanks for that!

I've dropped the changes in `vendor` as those aren't really my code!

Merged in 5894c02a346a511b516a8463be083edccc77dd36

Thanks for your contribution

Nick  Copy with --min-age to acd copies files that it shouldn't.

The problem is trying to do a copy/sync which relies on modification times to a remote which doesn't support them.

The filter (`--min-age`) is applied to the destination directory traversal combined with ACD not supporting modification times causes the problem.

If ACD supported mod times then this wouldn't be a problem.

However it isn't obvious that the copy relies on modtime working on the destination.

I could produce a warning here which might not be a bad idea.

Here is an idea for a work-around

   * for a remote which doesn't support modtime used as the destination of a `sync`, `copy` or `move`
   * and the user has specified age based filters (eg `--min-age`, `--max-age`)
   * disallow it as a destination for `sync` - that just isn't going to work
   * for `move` and `copy` ignore the age based filters on the destination


See [the forum](https://forum.rclone.org/t/copy-and-delete-excluded-issues/455/6) for full discussion. @ajkis 

From the forum

> So I have been seeding for two more days and wondering why my total file count hasn't been moving, it has been re-uploading the same files because of the --min-age 14d applying to the destination. @naeloob if you read the forum thread you should see what the problem is.

rclone applies the filter to the source and destination (if it didn't do that rclone sync would delete files it shouldn't).  However since the modtimes are wrong on ACD rclone doesn't see that files are uploaded and uploads them again.

The work-around above will fix the problem. @naeloob 
> Just a small difference, you say to ignore it only on remotes which not support modtime.
As i dont see any problem on ignore it for any remote, i suposse you will search the best/simple implementation.

You may be right - with the new syncing mechanism, it will make very little difference so leaving it out for all remotes is probably a good idea.  Thanks for writing that up - you have a lot of good points.

I was thinking that this sort of think should be implemented as part of #637 - I hadn't considered it to be separate from the encryption until now.

The philosophies of rclone are
  * files should be stored in a way that other tools can retrieve them (eg web interface etc)
  * any metadata should be ignorable by other tools
  * rclone shouldn't need any extra local data other than the config file

You'll notice that encryption crossed the first of those lines, hence my thinking to stuff it into #637 where we're already in unique tool territory.

I think it is a very interesting idea to split out the "extra metadata" bit of #637. Thought that has some special requirements such as needing to map file names to UUIDs (or something like that). 

The file name is a pain point that this could solve.  All the remotes have different file name length requirements special characters, case sensitivity etc...  Which is related to the crypt requirement to obscure the file names.

The easiest way to implement this in rclone would be to implement an overlay remote (like crypt), or maybe even as part of crypt (bringing us back to #637).

I wouldn't want rclone to be writing per directory metadata by default.

I will mull over some more - thanks I don't think anyone is working on this at the moment.  Please open a new issue so we can discuss exactly how this would work - there are a lot of related issues!  Your traffic leak is data being downloaded from S3.

rclone isn't downloading the actual data as you are copying it from local to s3.

Looking at your logs, these ones look like then could be the problem (though you'd have to add them up to see if it comes to the right amount)

> ca2******a0116 bucket [28/Dec/2016:06:29:16 +0000] x.x.x.230 arn:aws:iam::53****10:user/myuser 5104483EDC6FF5AD REST.GET.BUCKET - "GET /bucket?delimiter=&marker=43055%2F55aba977cc05c_14-9-1-1.mp4.jpg&max-keys=1024&prefix= HTTP/1.1" 200 - 345385 - 120 119 "-" "rclone/v1.33" -
> ca2******a0116 bucket [28/Dec/2016:06:29:17 +0000] x.x.x.230 arn:aws:iam::53****10:user/myuser 1138E425D56F6EF8 REST.GET.BUCKET - "GET /bucket?delimiter=&marker=46057%2F55b53148cec53.mp4&max-keys=1024&prefix= HTTP/1.1" 200 - 345952 - 100 99 "-" "rclone/v1.33" -
> ca2******a0116 bucket [28/Dec/2016:06:29:17 +0000] x.x.x.230 arn:aws:iam::53****10:user/myuser 86D269EED8315CD2 REST.GET.BUCKET - "GET /bucket?delimiter=&marker=4430%2F5521c01a214cf.mp4&max-keys=1024&prefix= HTTP/1.1" 200 - 345574 - 97 96 "-" "rclone/v1.33" -

These are rclone listing the bucket to work out which files need to be transferred.

I'm guessing that a) you have a lot of files and b) you are running rclone quite often.  In which case the directory listings could add up to a lot of bandwidth.

If this is the problem then there are a number of things you can do.

If you know you've got new files (maybe they come into a new directory first) then you could copy these up with an `rclone copy` command with the `--no-traverse` flag or the `--files-from` flag.  This will mean rclone doesn't need to do a listing of the remote directory.

Or if the files arrive in a directory tree you are syncing, then you could do something like this to copy only new files `rclone copy --no-traverse --max-age 10m /src s3:bucket` then every now an again do an `rclone sync` to tidy up any deleted files.

If you describe your upload process a bit more, I can help you design a way of using rclone more efficiently. Sounds like one of the solutions I posted above should work for you.  Do those make sense?  There are two places you can download via https, both linked on the downloads page..

  * https://github.com/ncw/rclone/releases/tag/v1.34
  * https://downloads-rclone-org-7d7d567e.cdn.memsites.com/

that said I do plan to convert rclone.org into https very soon!  @arcimboldo This looks almost ready to go.  Did you solve the problems with the vendor directory?  if you can do that then travis will start telling you about stuff you need to fix.

Which FTP server have you been testing against BTW? @arcimboldo how are you doing with this?  Need any help?

Regards

Nick @arcimboldo want me to merge what you've done so far? I'm testing this in the ftp branch - will merge shortly :-) > hi, were you able to merge it? It looks like master changed enough so that
it doesn't properly merge, and I happen to be home sick today, so if you
want I can at least fix the merging.

I fixed up the bitrot in a separate branch.  You can have a go with [it in the ftp branch](https://github.com/ncw/rclone/tree/ftp).

It is working quite well - it passes all the tests :-)  There is a bit more work to be done to make a connection pool for the FTP login connections - that will speed it up greatly. ...I'm just leaving this PR open until I have merged the ftp branch to master so I don't forget! I have merged this now :-)

See https://beta.rclone.org/v1.36-114-gcdacf026/ (in 15-30 mins) @arcimboldo thank you very much for contributing this.  I think I've managed to get all the kinks out of it and make it nice and fast - I'd be interested to see what you think!  This appears to have been caused by an acd.Node without an Id...

What sequence of operations lead to this - can you provide a sequence which I can use to reproduce? I've figured this one out - it should be fixed in this beta

http://beta.rclone.org/v1.35-26-gaa62e93/ (uploaded in 15-30 mins) @PiscisSwimeatus thanks for testing.  Sorry late to the party!  I looked at the drive console for rclone.  I can see that rclone isn't approaching its global queries per second limit (I got google to raise that to 500 QPS in November 2016).

From my analysis of the error codes it seems to be directory listings that are causing the problems..

Can anyone reproduce the problem just using `rclone ls` commands?  Or is it something specific to what plex is doing? I'm still getting banned of api. I have a custom id+secred, and in api website, the requests don't go over 200/100s, so maybe is not a problem related with total amount of requests.

My library is about 23Tb. Huge but the total requests are not the problem I think, I've done 1881 requests in a period of 6h, and never I went over 100-200 by100s (max 2-3req/s). In less than 3h from 10am I get banned updating library
![api](https://cloud.githubusercontent.com/assets/349675/21982738/18dbd8a8-dbee-11e6-844f-7a65d122e2f3.png)


This is the mount command I'm using now: 
`rclone mount google: /media/gdrive --allow-non-empty --allow-other --max-read-ahead 200M --dir-cache-time 30m --checkers 3 --umask 000 --drive-use-trash &
`

What could we do to try more? @ncw   Please give a sequence of commands to Replicate the problem - thanks!  I haven't managed to reproduce this.  Can you still reproduce it?

> So perhaps this problem was a compound between something weird having happened in the past and a bug causing rmdir to not stick via the fuse mount?

That seems plausible!

If you can't still reproduce the issue then we should probably close it.  Assuming these are big files (> 8MB) then #968 will fix this problem.  Thanks for the helpful bug report and links.

This is probably a job for the v2 API conversion #349 

However it wouldn't be too difficult to fix now...

The fix needs to be done here: https://github.com/stacktic/dropbox/blob/master/dropbox.go#L734

Fancy having a go? Dropbox v2 API has just hit master!

Can you retry this with the v2 API please?

https://beta.rclone.org/v1.36-144-g178ff62d/ (uploaded in 15-30 mins) Bit of trouble building: beta now here: https://beta.rclone.org/v1.36-146-g71028e0f/ I think this is probably fixed by the V2 API which you can find in a current beta.  Please re-open if not!

Thanks  The filtering works on matching the whole path of the objects.
  * A `*` matches anything but not a `/`.
  * Use `**` to match anything, including slashes (`/`).

So if you want everything in `/a` you need to use `/a/**`
  Nice idea.  I'm not 100% sure what is going on from that log.

Can you run it without the `-q` flag and with the `-v` flag and post another log please? This is likely caused by a very long filename.

The log, rather unhelpfully doesn't say which file it is.

Can you post the contents of your `--filter-from` and at least I can help you with the filter lines to stop it transferring?
  Looks like you might have a file name that is very long.  Remember crypt expands file names by approx 60%...

I think the limit on file names might be 256 characters on windows - can you test and see if that is correct?  So the file name you are trying to crypt will be greater than 160 characters.

Does that make sense?  > I have no idea why the CI tests fails

That was me - I broke them.  If you rebase then they should be fixed again.

> I have added test coverage for the non-error cases

Super.

> Looking at the general coverage, I would say that the test framework needs some support for "error simulation" that lies outside of this PR

Yes you are right.  The tests can be used as integration tests against an actual remote, but rclone could do with some more unit-y tests with a framework for injecting errors etc.

> I will squash all the commits once we agree that this is fine

Excellent.  You could leave the .gitignore one as a separate commit (which should probably have an newline on the end).

> I have also done a fair amount of deletions / renaming on my photo collection today, and the syncing has performed nicely.

:-)

Can you add some docs for the new flag please?  The options are documented in `docs/content/docs.md` in alphabetical order.

Thanks

Nick > I will. But a tip if you don't already know. Cobra can generate markdown doc of your commands. See output example here

I'm using that to generate the docs here http://rclone.org/commands/ .  The docs for the options are limited to a single sentence though (AFAIK), so I like to have a section in the docs for each one as they often need explaining in depth.

cobra FTW ;-) > One question. I noticed this when I renamed a folder with lots of files the other day. In my head this will trigger a massive amounts of renames when we should probably be able to just rename the folder. Or maybe that's just possible for local fs?

That depends on whether the remote implements `DirMove` or not...

> That may be an optimization to add later perhaps ...

Possibly, though there are rather a lot of corner cases involved!

Speaking of optimizations, we are iterating through the src and dst map making an O(n^2) job of renaming the files.  We could instead make a hash with all the checksums from the src (say) and iterate the dst to see if any match, then call equal for the final confirmation.  That would trade off O(n) space to make it O(n) ish.  Probably an optimzation for later too.

I think there is only one thing missing - I think you might have missed this comment

> Tracking renames relies on both the source and dest having a common hash. You can test for this with

```go
	common := fsrc.Fs().Hashes().Overlap(fdst.Fs().Hashes())
	if common.Count() == 0 {
		// no hashes in common so cantMove
                canMove = false
                // this should probably have a log too
	}
``` Thanks for that.  I'm going to merge this after I've released 1.35.  I think there are some optimisations to make but I want to get the release out the way.

Thanks

Nick I've merged this now (except for your last commit which I decided to do in a different way).

Thank you very much for your contribution - a great feature, and sorry it has taken me so long to merge - I've not had much free time over the last couple of weeks.

Thanks again

Nick  The Equal predicate checks to see whether two files are the same by checking size, modtime and hash in various combinations depending on the flags set by rclone.

The updateOlder flag is 

    -u, --update                            Skip files that are newer on the destination.

So are you saying that the `--update` flag should skip if the files are `Equal` too? That might be OK, but in the case of using the `--checksum` flag that doesn't check the modtimes at all so that could end up updating the mod time to an older one which isn't what `--update` implies.

Is that what you mean?

In the code for Equal - you'll see a branch which updates the mod time on the destination if the hashes match.
 I think you are correct.  There is a corner case which is worrying me though...

If `--checksum` is set, then `Equal` will not update the modtime of the destination file.  This will mean that after the sync the destination file will not have the correct modification time.  If you are using root to copy something to a fuse filing system you need --allow-root and you need to allow that in /etc/fuse.conf

I tried the above on ubuntu 16.10 and it worked fine for me.

Which OS are you using?

If --allow-root + /etc/fuse.conf fix doesn't work, can you post the result of

    strace cp testing.txt /storage/acd/
 This is what I think is happending.  When you do a normal cp not through encfs it opens the output file write only.  You can see that in the strace

> 2016/12/18 14:51:17 fuse: <- Open [ID=0x1f Node=0xe Uid=0 Gid=0 Pid=1880] dir=false fl=OpenReadWrite
2016/12/18 14:51:17 fuse: -> [ID=0x1f] Open error=EIO: can't open file - writer failed

However by the time that has gone through encfs it has become a ReadWrite.  What encfs appears to do is create the file then open it read write.

> 2016/12/18 14:51:17 fuse: <- Open [ID=0x1f Node=0xe Uid=0 Gid=0 Pid=1880] dir=false fl=OpenReadWrite
> 2016/12/18 14:51:17 fuse: -> [ID=0x1f] Open error=EIO: can't open file - writer failed

rclone can't open files read/write it can only open them read or write, not read and write - that is why you are getting the EIO.

I should put a debug in to make that a little clearer...

So it doesn't look like to me that that could ever work with encfs...

Did you have a version of rclone where that did work?  If so which version was that?  I think this is a duplicate of #221 - can you add your support to that issue please.

You can also use SIGUSR2 to turn the bwlimit on and off in the mean time.  I usually look for rsync for guidance for this kind of thing.  In this case rsync has

    --max-delete=NUM        don't delete more than NUM files

which would be straightforward to implement.  Can you run with the `-v` flag and post the log please?

Have you set  `--acd-upload-wait-per-gb` to a very large value? OK - glad you've sorted it.  I'll close this now.  Can you write a sequence of commands to Replicate please.  I'm unable to replicate this.

Can you paste the command line you are using?

Can you cut and paste (or screenshot) a terminal session where you replicate the problem?

Thanks

Nick Ah, I see what happened I think....

If in your test you reverse the order in the setup

```
rclone mkdir gdenc:/test
rclone mkdir gdenc:/test/dir1test

rclone mount gdenc:/ ~/gdenc > ~/debug.txt
```

do you see the same effect? I've managed to work out what is going on - it was a missing bit of functionality from rclone mount.  Here is a beta whch should work

http://beta.rclone.org/v1.35-18-g35a6436/ (uploaded in 15-30 minutes)

Please re-open if it doesn't!  localhost.rclone.org should resolve to 127.0.0.1 which it does when I try it.  Not sure why it wouldn't.

What happens if you open a shell/cmd window and type `ping localhost.rclone.org`?

You could try replacing `localhost.rclone.org` with `127.0.0.1` in the browser URL bar.

If all else fails you can copy the config file as detailed here: http://rclone.org/remote_setup/#configuring-by-copying-the-config-file Excellent.  Not sure why that was necessary - some DNS strangeness!  Will close now :-)  In the TODO section of the mount docs you'll see this is noted there: http://rclone.org/commands/rclone_mount/#todo

I'll re-purpose this issue into adding it as an enhancement. Note to self: Make the DirMove interface take a parameter so we get from, to? I've finally managed to complete this - it took a lot longer than I expected due to various caching issues :-(

http://beta.rclone.org/v1.35-107-gef604f6/ (uploaded in 15-30 mins)  This is a duplicate of #651 - can you add your support to that issue please?

Thanks  Yes, use the `copy` verb and it will do what you want - copy never deletes anything on the destination.

You can test first with --dry-run to see exactly what it does.  If you use the `-v` flag you'll get more info on the progress of the transfer.

I'm going to close this now as I think the original question is answered.  I've made a small modification which makes rclone report the actual modification times of directories as read from the remote.  Since ACD has a concept of directory and these have times then this should fix the problem for you.  Don't use `--no-mod-time` though (it doesn't buy you anything with ACD) and will confuse plex.

http://beta.rclone.org/v1.34-64-g13b705e/ (uploaded in 15-30 mins)
 @flixajki so does that help having stable directory times? Great - I'll mark this one as closed then.  I'd propose implementing it like this.

Use a simple Key/Value database - [bolt](https://github.com/boltdb/bolt) looks like the leading contender here.  Using an actual DB rather than a text file will mean we don't have to load the entire database into memory and we can update it as we go along.

Use the absolute value of the paths as a key - this will mean that you can use the cache for any local file accesses, not just ones from a specific directory.

As the values we need to store
  * modification time
  * sha1sum
  * md5sum

I'd serialize these into JSON probably.

We'd use this DB
  * when reading a Hash we'd look in the DB first
  * when we've calculated a hash we'd stuff it in the DB

Potential issues
  * garbage collection - if you move a lot of files about there will be files in the DB which don't have a corresponding file - I haven't thought of a mechanism for removing these.  It might not be a big problem though.
  * concurrent use. - bolt (and most key value databases) only allow one writer at a time to the db so you'd only be able to use one rclone processes at once.
  * is bolt available on all the platforms rclone builds for @jediry @monroe-74 thanks for thinking about this.

I think the garbage collection would be solved for the time being with some extra docs.  If necessary rclone could grow an extra command to help deal with it.  The compact on shutdown is a reasonable idea, but even scanning a huge directory tree can take some time.  We'll have to see how big the DBs become, but pessimistically 1k per file.  So for 1M files the DB becomes 1GB.  So if there are 100k stale files that is not going to be much of a problem.

As for concurrent access, rclone could wait until the db was available rather than stop with an error.  It could print a message every 60s while it was waiting.  This would mean that backups would proceed in an orderly fashion.  boltdb takes a lot of care to not allow concurrent access so you wont get corruption.

I could solve the concurrent access to the db problem like this if necessary
  * the file you pass in as the cache rclone only reads from - multiple readers are allowed if read only
  * rclone writes to a temporary db
  * when rclone has finished the sync it
    * waits for any other users of the main db to finish
    * opens the main db r/w
    * updates any changes

Using multiple DB's is a good idea.  You could fill it up from one run then copy it and use it in another.

> Is it fairly cheap to simultaneously compute the SHA1 and MD5

The main cost is IO so it is pretty much free computing the SHA1 if you are already computing the MD5.  That is what rclone does at the moment anyway.

> Specifically, in addition to being a non-relative path, I think the path needs to be canonicalized to remove this-directory (./) and parent-directory (../) sequences, superfluous '/' sequences (e.g., /mnt///path -> /mnt/path), and also should resolve symbolic links.

Absolutely (pun intended ;-) > and also should resolve symbolic links.

rclone ignores symbolic links at the moment - that needs a little bit of extra thought...  Those "409 Conflict" errors are might be because you have files with names in the same case.  Eg `Front.jpg` and `front.jpg` - can you check that?  They sometimes happen because of ACD's eventual consistency and will be fixed by a retry.

> Deletions have been failing again. It started shortly after my last issue with failing deletions, but I hadn't gotten around to reporting it.

I don't see anything about deletiions in the log above
 > I wish I could just forget about this backup but there is something up with it often.

rclone does its best to help you with lots of retries, at different levels, but sometimes it just can't!

> Problem solved.

Excellent!  rclone -v tells you the default location of the config. You can also use --config.  rclone -h I meant!   Have you been uploading a lot of stuff?  It might be that you've used your quota up.  If you wait 24h it should all be back to normal.

The quota is invisible to the user, but that is what rclone users have figured out. @canonlp is this working again now? It would be worth checking out your router - rebooting or upgrading the firmware. Rclone works the network quite hard and I had multiple reports of people needing to do that.  @canonlp wrote:
> Notice it only transferred 256 MB and won't transfer any more. The speed will also drop to 0. Unless BOTH my accounts are rate limited, I'm not sure what is really going on. The server I tried this on just now is a new one that hasn't been uploading to Gdrive at all

The bandwidth stats are slightly misleading here as it hasn't actually transferred 256MB to google, only to an internal buffer to upload to google.  So it looks like your transfers aren't progressing at all.  You will see it shoot up to 256M with very fast transfer rates then gradually sink down again.

I fixed this behavior for chunked uploads for ACD and I should do the same for drive - see #968

I tried with the default `--drive-chunk-size` and with 256M and they both worked fine here.

Is it possible that you just aren't waiting long enough for the chunk to be uploaded?  How fast is your internet connection?  I've fixed this in: http://beta.rclone.org/v1.34-65-g215fd2a/ (uploaded in 15-30 mins)

Please re-open if you see any issues with it.  Timeouts happen - it is a fact of the internet.  I suggest you remove the setting of low level retries and leave it at the default - that will catch the timeouts and retry them - that is what it is for.

I couldn't see your log - it didn't seem to be attached.  @diamondsw 

what would you like rclone to do?

Come to a halt the moment this sort of error is detected?  Or just not retry?

Either is possible by wrapping the error message in a FatalError or a NoRetryError. @maciozo yes a system like that would be possible, and I and others are thinking about the whole metadata issue!  @jpat0000  Can you send a pull request with this in please?

I'd probably prefer to use the time the mount started rather than the 0 epoch time if that would work for your requirements.

Maybe with an optional flag to set the time to something specific.

If you can send a PR with what you've done with or without those changes then I can get it integrated and we can iterate the design. @flixajki  wrote
> Is there a specific reason why we dont have actual times since they are clearly supported by Amazon Cloud ?

You'll get times that the file was uploaded if you don't use --no-mod-time and it is no slower with ACD so I'd use that.

Or are you seeing something different? All the folder times will be set to the time you mounted the drive, however the file times should be constant.  That is what I see.

Does plex look at the folder date stamps?

```
$ rclone mount acd:test ~/mnt/tmp/
$ ls -l ~/mnt/tmp/
total 5242881
-rw-rw-r-- 1 ncw ncw 5368709121 Nov 20 14:00 5GB
drwxrwxr-x 1 ncw ncw          0 Dec 13 10:57 newsubdir
```

However

```
$ rclone lsl acd:test
5368709121 2016-11-20 14:00:44.915000000 5GB
```


 I've made a small modification which makes rclone report the actual modification times of directories as read from the remote.  Since ACD has a concept of directory and these have times then this should fix the problem for you.  Don't use `--no-mod-time` though (it doesn't buy you anything with ACD) and will confuse plex.

http://beta.rclone.org/v1.34-64-g13b705e/ (uploaded in 15-30 mins)
  Rclone stores the token which changes every hour in the config file - that is why it rewrites it.   None of those things are errors - it looks like rclone thinks it worked successfully. Did rclone exit with an error (do `echo $?` after it ran to see)?

> I have mixed up the sync order (rclone sync source :destination) which did a sort of sync and then finished.

:-(

Was the destination empty when you did that?  If it was then rclone quite likely deleted all your local files. That is why it says in the manual to try `--dry-run` first.

So what does `rclone ls source:` say and `rclone ls dest:` say?

It would be helpful if you post a log and the actual commands you are using.

Also try adding the `-v` flag to see what is going on.  Interesting! I would say this is a good use of rclone so it should work properly for this. 

What to the rclone logs say - can you post some of those? Logs with -v would be most helpful.  

What you could do is kill -QUIT the rclone process when it gets stuck and post the enormous traceback that will generate to stderr. If it is a livelock then that will tell us where it is hopefully. 

This process might be CPU limited (I'm not certain though)  - you are doing two encrypted https links plus rclone's own encryption. However I wouldn't expect it to grind to a halt. 

When it gets stuck is it doing something in particular? Uploading big file or small files? What to the rclone stats say at this point? 

Is that all the strace program printed? Looks very suspicious that it has livelocked...  I'm pretty sure this is the same issue as #902 

I think I've worked out what is happening here. I managed to reproduce it once, and it gave me enough of a clue.

Can you try this beta please and let me know how you get on:

http://beta.rclone.org/v1.35-41-g2abfae2/ (uploaded in 15-30 mins)

Thanks

Nick @diamondsw excellent - thanks for testing  After a bit of research, I don't think you can use read ahead to raise the value supplied by the kernel (which is 128k on linux), so no, it probably isn't working as expected.

However I've put in some in memory buffering  which reads up to 16M of the file before it is used which should help a lot in the latest beta: http://beta.rclone.org/v1.34-51-gcb9f1ee/ - fancy giving it a go? I put the stats printing in as part of fixing the bwlimit for mount which makes mount the same as the other commands. I can disable it if everyone hates it though...   Using 1.34 on a Mac.
When copying a large directory, had a panic crash after a few hours.

```
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0x136596]

goroutine 273 [running]:
panic(0x5fe1a0, 0xc420010080)
	/usr/local/go/src/runtime/panic.go:500 +0x1a1
github.com/ncw/rclone/local.(*Object).Size(0xc4207180c0, 0x0)
	/home/ncw/Code/Go/src/github.com/ncw/rclone/local/local.go:540 +0x26
github.com/ncw/rclone/fs.Copy(0x9c4d80, 0xc420069500, 0x9c5500, 0xc420118500, 0x9c5380, 0xc4207180c0, 0x0, 0x0)
	/home/ncw/Code/Go/src/github.com/ncw/rclone/fs/operations.go:242 +0xfdb
github.com/ncw/rclone/fs.(*syncCopyMove).pairCopier(0xc4205f6000, 0xc4205e6480, 0x9c4d80, 0xc420069500, 0xc4205f6088)
	/home/ncw/Code/Go/src/github.com/ncw/rclone/fs/sync.go:251 +0x2b5
created by github.com/ncw/rclone/fs.(*syncCopyMove).startTransfers
	/home/ncw/Code/Go/src/github.com/ncw/rclone/fs/sync.go:349 +0x108 
```

 The crash was here indicating that `o.info` was nil.  There shouldn't be any code paths which allow `o.info` to be nil, so that is a bit of a puzzle.

It is as if os.Lstat returned nil, nil which it shouldn't really, but would be easy to check for.

Can you reproduce this problem?  Might something else have been changing that large directory (removing, renaming fles)? Anything else that might be helpful?

```go
// Stat a Object into info
func (o *Object) lstat() error {
	info, err := os.Lstat(o.path)
	o.info = info
	return err
}
``` Nothing has changed in the directory. BTW the files are on a NAS mounted using SMB
I was able to replicate the issue with the same commande. It follows another error:
Failed to calculate src hash  How do you reproduce this?

I'm guessing something to do with moves - in which case try

http://beta.rclone.org/v1.34-60-g2656a0e/ (uploaded in 15-30 mins) or just update your git repo if you prefer.  I've re-worked the move code.

If it still doesn't work, then can you make a repro for me so I can make it go wrong here.

Thanks I think this is very likely fixed by #973 

You can try this here

http://beta.rclone.org/v1.35-26-gaa62e93/ (uploaded in 15-30 mins)  > thank you for this great tool!!!

:-)

> So the question is, does rclone require all of the S3 permission just to sync/copy files to a single bucket?

I doubt it needs all the permissions, but I don't know very much about S3 permissions!

I looked at: http://docs.aws.amazon.com/AmazonS3/latest/dev/using-with-s3-actions.html

I would have thought PutObject and and ListBucket would be sufficient for upload purposes.  You can run with --dump-headers to give you more of a clue as to what rclone was doing when it gets a 403 error.

If you fancied writing this up and adding it to the docs when you figure it out, I'd be grateful :-)  I think this is probably the same as #784

I think you've configured the crypt to point at itself.

The crypt needs to point at the underlying unencrypted remote.

Does that make sense?

I'm planning to warn about this in #927  Try the latest beta - it fixes all those issues and adds the `moveto` command which can be used to rename things.  I should probably put this in the docs...

Make a file - I called mine ~/bin/set-rclone-password with this in

```
#!/bin/echo Source this file don't run it

read -s RCLONE_CONFIG_PASS
export RCLONE_CONFIG_PASS
```

Then when you want to unlock rclone for a while do

```
. ~/bin/set-rclone-password
```  I recommend you use `rclone copy` to upload stuff - the fuse system doesn't do any sort of retries...

> So I managed to solve this problem. I thought that this could be a valid solution:
> unionfs-fuse -o allow_other,cow /mnt/local-data/=RW:/mnt/acd/=RO /mnt/Merged/
> ... and periodically:
> rclone sync /mnt/Merged/ acd_e:

Use `rclone copy /mnt/local-data/ acd_e:` would be a better solution I think. I would script the `rclone copy` but only do the `rclone sync` you originally suggested by hand after running it with `--dry-run` first.  Run with -v and --dump-bodies and post the results. You might need to edit sensitive data out before posting. 

Hopefully that will give a clue.  Thanks for investigating

> The region setting is getting ignored when choosing the v1 auth url

v1 auth doesn't support regions so that isn't too surprising

> when not specifying region at all and one container exists in IAD, all other containers in other regions are ignored

The swift library docs say about region "Region to use eg "LON", "ORD" - default is use first region (v2,v3 auth only)".  I think that explains what is happening.

rclone & the swift library will only speak to the endpoints for a single region at a time, so it is likely you will need two rclone configs, one for each region.

I'm not sure how cyberduck shows all the containers - is there a universal endpoint - I don't know!
  What would rclone need to do to support this? Set a life cycle policy? @hashbackup thanks for the writeup - very interesting.

So it looks like S3 Infrequent Access would be the way to go.  It looks like that would be easy to add - what do you think?  You can make a workflow like this already using rclone's built in crypt support.

Why do you particularly want to use gpg?  @davidstrauss 

So instead of reading the encryption keys from the rclone config file, rclone would read them from the gpg agent - is that right?

Any idea what the gpg command line would look like for that?

I could add an option to rclone which would run a command to get the key material quite easily.

Though if you can run a command to get key material then you can (using a recent beta) set environment variables with the keys in before running rclone quite easily.  If you had an encrypte remote called `eremote` then you could set `RCLONE_EREMOTE_PASSWORD` and `RCLONE_EREMOTE_PASSWORD2` and take the `password` and `password2` lines out of the config.  How about a web page?  Would that work for you?  rclone could serve a simple auto refreshing web page with some stats about the current transfer in. Doing web pages from go is super easy...

I'd make another option called `--stats-page` which took `:port` or `host:port` which would show some read only stats while rclone was running.  The first version would probably just show exactly what the stats printer does.  I don't know why this happens, but it does :-(  If you look back through rclone's issues you'll see lots about duplicate files on google drive.

There is the `rclone dedupe` command to help (and if you try the latest beta you can use `rclone rmdirs` to remove all empty directories)

What I should probably do is make a mode for `rclone dedupe` to merge those duplicated directories.  It isn't trivial to do unfortunately.

The duplicated directories are invisible to normal rclone copy etc, it can cope with them just fine.  But `rclone mount` has a problem with them.

You could also fix this by moving things about in the google drive web interface if there aren't too many of them. > Is the content of both duplicate folder copied to the computer if I use rclone copy Remote:Folder Localdrive:folder ?

You'll get all the contents of the duplicate folders merged which is probably what you want.

I'm going to close this now as it is covered in #28 #360 (and others!)  Not sure I understand what you mean - can you give an example please? A midnight commander mode would be fun yes.  I think now-a-days that sort of interface is called a TUI (text/terminal user interface) - @scj643 is that what you mean?

The idea of being able to navigate a remote in your terminal I like. 

This is a great cross platform library for building TUIs : https://github.com/nsf/termbox-go which I have used before.   Would the ability to set arbitrary headers on uploaded files work for you?  Eg #59  @Cr0iX you can't create your own client id with ACD easily :-(  The bug is here

```go
// Size returns the size of an object in bytes
func (o *Object) Size() int64 {
	return int64(*o.info.ContentProperties.Size)
}
```

Thanks for reporting! Will fix and post a beta. I've fixed this in http://beta.rclone.org/v1.34-39-g0579867/ (will be uploaded in 15-30 mins)

Thanks for the report!  I think this is probably an amazon issue - is it working now?  That is only a warning - where are you syncing to? @joeytwiddle can you paste some logs?  I've merged that - thank you.  I reworded the commit messages slightly.

Much appreciated

Nick.  Can you get a log with `-v` of this happening? No there doesn't appear to be anything out of the ordinary there!

Did the `rclone mount` process die completely at that point?  Is it possible it wrote stuff to stderr which you didn't capture? I'm going to assume you got this working since I haven't heard from you for a while.  checksum isn't supported over crypt - see #637 for the ultimate solution.

Also see #854

Modtime should be supported with Yandex though, can you give log of it not working please with `-v` please? i confirm that this is a bug.

I've fixed it in 2756900749a37d450a1c31fab6386c027333844e

Here is a beta with the fix: http://beta.rclone.org/v1.34-37-g2756900/ (uploaded in 15-30 mins)

Please re-open if there is any problems.  Looks perfect - I've merged that - thank you very much.  This is almost certainly #682 - that is you've got one rclone running while you are configuring it.

Does that make sense?  Can you try the latest beta please?

How are you writing to the mount?  rsync, cp or something else? I'm going to assume that since I haven't heard from you, you got this working  Likely this is because you've hit drive's rate limits.  If you want a bit it will start working again.  This sounds like it is something to do with sudo and job control. 

Two things you could try... 

1) sudo -i then run the rclone command without sudo

2) try the rclone mount without sudo - that should work Which user did you use to configure rclone?  Either run the mount using that user, or reconfigure using the plex user. I'm going to assume you got this working.  If not, I suggest you ask on the forum for help.  I think this is a duplicate of #825 which I've attempted to fix already - try the latest beta and see if that helps.

Please comment in #825 whether it works for you or not!

I'm going to close this one now.  I think this is unlikely to be an rclone problem.  Did your internet connection go to pieces at that moment?  Or maybe Amazon had a problem.

Can you reproduce the problem? This has also been discussed [on the forum](https://forum.rclone.org/t/rclone-stalls-on-acd-upload-done-0-bytes-s-eta-0s-and-must-be-restarted/727/12)

I think the problem is copying from ACD to ACD so I'll change the title in a moment.

I'll try to replicate! I think I've worked out what is happening here.  I managed to reproduce it once, and it gave me enough of a clue.

Can you try this beta please and let me know how you get on:

http://beta.rclone.org/v1.35-41-g2abfae2/ (uploaded in 15-30 mins)

Thanks

Nick @peixotorms it is probably some kind of networking problem.  If it happens again use netstat to work out where rclone is connecting to and then use traceroute to see what the path looks like to those IPs. I'm going to close this now as the issue is fixed by 2abfae283cd6128cf36bb31bffad1c818e82bbb4  I think this is a duplicate of #254

Can you subscribe to that one please.

Thanks

Nick  Is this still a problem with the latest beta?  I think this divides into two parts

  1. cache the drive structure - this is covered in #180 with the idea of manual commands to resync the cache.
        * This would work with any remote.
        * it should probably cache the names from the root of the remote regardless of the path specified
  2. update the structure using the changes method
        * this would require a new optional method for remotes to implement

The first of these would fix the immediate problem, but with a manual scanning step required.

Combined with the second that would be the perfect solution.

I've been thinking about a metadata caching scheme for local here #949 which could possibly be combined with this.

@IKShadow wrote:
> I support this as long as its optional, still have the bad taste with acd_cli and database corruptions :)

It would absolutely be optional.

@scoopydude2002 wrote:
> Which database type are you planning to use? Please consider SQLite

I'd love to use SQLite, but it is written in C.  Using any C in rclone would mean a whole heap of ugliness in the build and packaging which I'm trying to avoid.  The go equivalent would be [ql](https://github.com/cznic/ql)

What I require is a multi-user key-value store.  The go equivalent is boltdb which is almost perfect...

 @jtl5770 ACD has a [changes API](https://developer.amazon.com/public/apis/experience/cloud-drive/content/changes) which I haven't studied in detail, but it looks like it might do the job! @felixbuenemann I agree 100% with you that a local DB fed by the changes is the way to go for rclone in general, not just mount.

I want to get rid of the concept of `root` out of the remotes first which will simplify them and will make this a lot easier .

I then imagine an optional interface to poll for changes on the remote which can be used to update a local database.  There are a few things that need to be thought out, like serializing objects

Thanks for looking up the APIs :-) I'd like to fit it into the next release which should be in a couple of months, but that might be over optimistic! I think #1346 will also help with this issue for google drive  > What is your rclone version (eg output from `rclone -V`) 1.33

> Which OS you are using and how many bits (eg Windows 7, 64 bit) Windows 10 64 bit

> Which cloud storage system are you using? (eg Google Drive) none

> The command you were trying to run (eg `rclone copy /tmp remote:tmp`) rclone f:\ nounc g:\

> A log from the command with the `-v` flag (eg output from `rclone -v copy /tmp remote:tmp`) the command just waits for more input

 There doesn't seem to be a verb in the command line you posted, eg `copy`?

So you probably want `rclone copy f:\path\subpath\ g:\path\subpath\`

Is that the problem?  If not then cut and past the actual command an error message please.

Thanks
 @ncw woops that was a typo. It's rclone sync  f:\ nounc g:\
 @eygraber Can you cut and paste (or screenshot) the actual command and the output please with the `-v` flag - thanks!
 @ncw figured out the issue. On Windows you need to prepend `\\` to use other drives. So the command was `rclone sync \\f:\ \\g:\`. Actually scratch that. That just created a directory called `:h_` at the root of the drive I was on. What would the correct command be to copy or sync between 2 local drives? This is mentioned in the docs: https://rclone.org/docs/#windows

The correct syntax should be:

rclone sync f:\ g:\

  Do you need rclone to show non zero info on `df`?

```
$ rclone mount acd: ~/mnt/tmp/
$ df -h ~/mnt/tmp/
Filesystem      Size  Used Avail Use% Mounted on
acd:            0.0K  0.0K  0.0K    - ~/mnt/tmp
```

I'll have to make some stuff up to go in there - I don't want to do a full filing system scan to read the info.  What is important?  That there is some free space? That there is some space used?  I could make it return 1 MB used and 1 PB free or something like that.

What is the error that samba returns?

Note to self:  This requires implementing the statfs interface

``` go
type FSStatfser interface {
    // Statfs is called to obtain file system metadata.
    // It should write that data to resp.
    Statfs(ctx context.Context, req *fuse.StatfsRequest, resp *fuse.StatfsResponse) error
}
```
 ...also what does acd_cli return for `df`?
 I've fixed this to return a static output

Find it in this beta http://beta.rclone.org/v1.34-23-gc41b67e/ (will be uploaded in 15-30 mins)

```
$ df /mnt/tmp/
Filesystem         1K-blocks  Used     Available Use% Mounted on
acd:           1099511627776     0 1099511627776   0% /mnt/tmp
$ df -h /mnt/tmp/
Filesystem      Size  Used Avail Use% Mounted on
acd:            1.0P     0  1.0P   0% /mnt/tmp
$ df -i /mnt/tmp/
Filesystem         Inodes IUsed      IFree IUse% Mounted on
acd:           1000000000     0 1000000000    0% /mnt/tmp
```
  The sort of errors in your log happen. ACD gives all sorts of errors and rclone retries them.  Sometimes this involves retrying the entire sync.  Hopefully what happened is rclone retried the sync and everything was fine.

My first suggestion would be to retry the sync and see if it completes this time.
  I've merged that - thank you very much :-)

PS I'd like to get the ACD integration tests working again - did you have some patches for that too?
  You might want to check out using one of these

```
export GODEBUG=netdns=cgo   # force cgo resolver
```

The default with the precompiled binaries is the go resolver.  I'm not 100% sure trying to use the cgo resolver will work with the precompiled binaries so you might have to compile from source to try that.

This issue looks like essentially what you are reporting: https://github.com/golang/go/issues/17448

If so it is scheduled for go 1.9 fix which should be out in ~7 months.

You can also try configuring your preference in `/etc/gai/conf` (might not work with the go resolver).
  Nice idea!
 @jschwalbe This would involve tweaking the sync engine in `fs/sync.go`.  I'd control this with an additional flag - `--track-renames` say.

If this flag was set, in [run()](https://github.com/ncw/rclone/blob/master/fs/sync.go#L415) I'd make sure both source and destination were traversed first (like the deleteBefore option).  Once traversed I'd do a separate phase doing the renames, adjusting the destination map on successful rename, then carry on doing the sync as usual.  This would need a unit test or two as I try to keep the core syncing routines fully covered.

Not the easiest first project, but it is of relatively narrow scope so not too bad.

Note: not all the remotes can do a server side rename (or a server side copy + delete) so you probably would want to ignore this option on those remotes as the move primitive would download and upload the file which would be less efficient than just uploading the new one. 

If you want to spend a bit of time studying go then [the go tour](https://tour.golang.org) is an excellent place to start.
 @jschwalbe no worries!  There are some easier issues if you want to help with something else.  Take a look through https://github.com/ncw/rclone/milestone/22 and see if anything catches your eye - or any other issue!
 @bep that would be great!

This will involve adjusting fs/sync.go which is quite intricate, but it is fully tested.

What I'd do is make sure that the source map and dest map get loaded into memory in the `run` method (which will make it incompatible with `--no-traverse` - you want to follow the `--delete-before` code path).  When both maps are loaded then do the rename detection in a separate pass, removing things from the src and dst map when you do a successful rename.  Then fall through into the sync/copy/move proper.

The actual moves you want to do with the `Move` function.  This will fall back to download/upload if the remote doesn't implement `Move` or `Copy` which isn't ideal so if the remote doesn't support `Move` or `Copy` you probably want to ignore the `--track-rename` flag completely. @bep let me know if you need any more pointers. Nick  Please find a beta with this in here: http://beta.rclone.org/v1.35-07-gf1221b5/ (uploaded in 15-30 mins) @Techrocket9 yes this made it into the release.  Unfortunately crypt doesn't support hashes :-( @olanmatt yes that is correct, `--track-renames` doesn't work with encrypted remotes :-(  @davidjgraph what happens if you use `rclone config` with the drive option with a team drive? Does that work now? Does anyone want to work on this?

I don't have access to a team drive unfortunately... @jahands I've sent you an email with my gmail! @jahands thanks for that - it is working fine!

A bit of poking around discovers
  * I'll need to update the drive API package to get access to team drives
  * I'll need to [list](https://developers.google.com/drive/v2/reference/teamdrives/list) the team drives to get their ids (which is also the id of their top level folder)
  * once I have their ids then rclone should work as normal (though might need to set some extra flags in the [files list method](https://developers.google.com/drive/v2/reference/files/list)

How do you think this should be used from within rclone?

Using the handy graphic above, rclone uses the contents of "My Drive" above.  I could add a flag, say `--drive-team-drive NAME` which means that it will use the team drive with that name instead.

Or instead of a flag it could be an item in the config that you set the name of the team drive you want to use.  I think this might make the most sense - it makes each team drive a different remote which would make it easy to copy between remotes.

Alternatively I could make a kind of virtual file system making a Pseudo Directory "Team Drives" and within that a pesudo directory for each team drive the API lists.  That goes against the way the drives are organised in reality (see pic above) which is bound to lead to trouble!

Thoughts? @gustavorochakv good point! Here is the first attempt at supporting Team Drives.  To use it configure a google drive remote - during the config you will be asked if  you want it to be a team drive.  It will then show a list of the first 100 team drives which you can pick a number or type in a team drive ID.

The integration tests pass when run on a team drive which is good, but there may be other things which don't work - feedback much appreciated!

This isn't in master (so not in the the betas) yet - I'd like a bit of feedback before I merge it to master.

https://pub.rclone.org/v1.36-155-gcf29ee8b-team-drive%CE%B2/

 Hmm, just realised I messed up the integration test so it works less well than I'd hoped! Will post an update soon. @timwsuqld  Ah, that is a bug - it should retry those errors!  Just try the config process again.  No you shouldn't need to specify your own client id and secret, but you can if you want. OK here is a better beta which does pass the integration tests :-)  This should be mostly working.  I've done retries for reading the team drive list and figured out why it wasn't working properly before!

https://pub.rclone.org/v1.36-156-ge1f0e0f5-team-drive%CE%B2/

I'm not 100% sure the syncing is working properly, In my tests it uploaded my test folder, then immediately wanted to upload some more files.  Possibly related to #1431 I got this error when I was configuring it:

```
Log in and authorize rclone for access
Waiting for code...
Got code
2017/06/02 10:45:29 ERROR : Failed to save new token in config file: section 'XXXXXX' not found
Configure this as a team drive?
y) Yes
n) No
y/n> y

```
Seems to be working.  I copied a folder to test things out.  and one error was odd.  Over and over it tried to copy 0B_raumxsbgcgYk9oOGhSSmZjcjg which I have no idea what that actually is..... 
```
./rclone copy XXXXX:/XXXXX/XXXXX/XXXXX  XXXXX:/XXXXX/XXXXX/XXXXX -vv
2017/06/02 10:50:13 DEBUG : rclone: Version "v1.36-156-ge1f0e0f5-team-driveβ" starting with parameters ["./rclone" "copy" "XXXXXX:/XXXXXX/XXXXX/XXXXX" "XXXXXX:/XXXXX/XXXXX/XXXXX" "-vv"]
2017/06/02 10:50:14 INFO  : Google drive root 'XXXXXX/XXXXX/XXXXX': Modify window is 1ms
...
...
2017/06/02 10:51:11 DEBUG : 01_20170602004337000.MP4: Sending chunk 109051904 length 6653388
2017/06/02 10:51:12 ERROR : 01_20170602004337000.MP4: Failed to copy: googleapi: Error 404: File not found: 0B_raumxsbgcgYk9oOGhSSmZjcjg, notFound
2017/06/02 10:51:12 DEBUG : 01_20170601235309000.MP4: Sending chunk 75497472 length 8388608
2017/06/02 10:51:14 DEBUG : 01_20170601235309000.MP4: Sending chunk 83886080 length 8388608
2017/06/02 10:51:14 INFO  : 
Transferred:   460.333 MBytes (7.542 MBytes/s)
Errors:                 1
Checks:                 0
Transferred:            0
Elapsed time:        1m1s
Transferring:
 *                      01_20170601235309000.MP4: 17% done, 1.814 MBytes/s, ETA: 3m47s
 *                      01_20170602001421000.MP4: 29% done, 2.151 MBytes/s, ETA: 2m43s
 *                      01_20170602002850000.MP4: 25% done, 1.617 MBytes/s, ETA: 3m23s
 *                      01_20170602005739000.MP4: 25% done, 574.274 kBytes/s, ETA: 10s

2017/06/02 10:51:16 DEBUG : 01_20170602002850000.MP4: Sending chunk 109051904 length 8388608
2017/06/02 10:51:17 DEBUG : 01_20170602001421000.MP4: Sending chunk 150994944 length 8388608
2017/06/02 10:51:18 DEBUG : 01_20170601235309000.MP4: Sending chunk 92274688 length 8388608
2017/06/02 10:51:20 INFO  : 01_20170602005739000.MP4: Copied (new)
2017/06/02 10:51:21 DEBUG : 01_20170602001421000.MP4: Sending chunk 159383552 length 8388608
2017/06/02 10:51:23 DEBUG : 01_20170602001421000.MP4: Sending chunk 167772160 length 8388608
2017/06/02 10:51:23 INFO  : 01_20170602005845000.MP4: Copied (new)
2017/06/02 10:51:24 DEBUG : 01_20170602001421000.MP4: Sending chunk 176160768 length 8388608
2017/06/02 10:51:25 INFO  : 01_20170602005933000.MP4: Copied (new)
2017/06/02 10:51:26 DEBUG : 01_20170602001421000.MP4: Sending chunk 184549376 length 8388608
2017/06/02 10:51:27 DEBUG : 01_20170602010017000.MP4: Sending chunk 0 length 8388608
2017/06/02 10:51:27 DEBUG : 01_20170601235309000.MP4: Sending chunk 100663296 length 8388608
2017/06/02 10:51:29 DEBUG : 01_20170602002850000.MP4: Sending chunk 117440512 length 8388608
2017/06/02 10:51:29 DEBUG : 01_20170602010017000.MP4: Sending chunk 8388608 length 8388608
2017/06/02 10:51:31 DEBUG : 01_20170601235309000.MP4: Sending chunk 109051904 length 8388608
2017/06/02 10:51:33 DEBUG : 01_20170602002850000.MP4: Sending chunk 125829120 length 8388608
2017/06/02 10:51:33 DEBUG : 01_20170602010017000.MP4: Sending chunk 16777216 length 8388608
2017/06/02 10:51:34 DEBUG : 01_20170602001421000.MP4: Sending chunk 192937984 length 8388608
2017/06/02 10:51:36 DEBUG : 01_20170602010017000.MP4: Sending chunk 25165824 length 8232141
2017/06/02 10:51:36 DEBUG : 01_20170602002850000.MP4: Sending chunk 134217728 length 8388608
2017/06/02 10:51:36 DEBUG : 01_20170601235309000.MP4: Sending chunk 117440512 length 8388608
2017/06/02 10:51:37 ERROR : 01_20170602010017000.MP4: Failed to copy: googleapi: Error 404: File not found: 0B_raumxsbgcgYk9oOGhSSmZjcjg, notFound
2017/06/02 10:51:38 INFO  : 01_20170602010715000.MP4: Copied (new)
2017/06/02 10:51:39 DEBUG : 01_20170602001421000.MP4: Sending chunk 201326592 length 8388608


```

I'm having a lot of failures though with that repeated message.  Pretty wierd:
```

Transferred:   8.482 GBytes (8.034 MBytes/s)
Errors:                23
Checks:                 0
Transferred:            8
Elapsed time:       18m1s
2017/06/02 11:08:06 DEBUG : 01_20170602041239000.MP4: Sending chunk 520093696 length 5736165
2017/06/02 11:08:06 DEBUG : 01_20170602035337000.MP4: Sending chunk 260046848 length 8388608
2017/06/02 11:08:07 DEBUG : 01_20170602033554000.MP4: Sending chunk 486539264 length 8388608
2017/06/02 11:08:08 ERROR : 01_20170602041239000.MP4: Failed to copy: googleapi: Error 404: File not found: 0B_raumxsbgcgYk9oOGhSSmZjcjg, notFound
2017/06/02 11:08:08 DEBUG : robgs: Saved new token in config file
2017/06/02 11:08:09 DEBUG : 01_20170602040217000.MP4: Sending chunk 243269632 length 8388608
2017/06/02 11:08:10 DEBUG : 01_20170602035337000.MP4: Sending chunk 268435456 length 8388608
2017/06/02 11:08:11 DEBUG : 01_20170602042157000.MP4: Sending chunk 0 length 8388608
2017/06/02 11:08:12 DEBUG : 01_20170602042157000.MP4: Sending chunk 8388608 length 8388608
2017/06/02 11:08:14 DEBUG : 01_20170602033554000.MP4: Sending chunk 494927872 length 8388608
2017/06/02 11:08:14 DEBUG : 01_20170602035337000.MP4: Sending chunk 276824064 length 8388608
2017/06/02 11:08:14 INFO  : 
Transferred:   8.482 GBytes (8.034 MBytes/s)
Errors:                23
Checks:                 0
Transferred:            8
Elapsed time:       18m1s
Transferring:
 *                      01_20170602033554000.MP4: 96% done, 1.558 MBytes/s, ETA: 11s
 *                      01_20170602035337000.MP4: 54% done, 1.592 MBytes/s, ETA: 2m24s
 *                      01_20170602040217000.MP4: 49% done, 1.651 MBytes/s, ETA: 2m33s
 *                      01_20170602042157000.MP4:  3% done, 3.872 MBytes/s, ETA: 2m4s

2017/06/02 11:08:15 DEBUG : 01_20170602040217000.MP4: Sending chunk 251658240 length 8388608
2017/06/02 11:08:16 DEBUG : 01_20170602033554000.MP4: Sending chunk 503316480 length 8388608
2017/06/02 11:08:18 DEBUG : 01_20170602035337000.MP4: Sending chunk 285212672 length 8388608
2017/06/02 11:08:18 DEBUG : 01_20170602033554000.MP4: Sending chunk 511705088 length 8388608
2017/06/02 11:08:18 DEBUG : 01_20170602042157000.MP4: Sending chunk 16777216 length 8388608
2017/06/02 11:08:19 DEBUG : 01_20170602033554000.MP4: Sending chunk 520093696 length 4266793
2017/06/02 11:08:20 DEBUG : 01_20170602040217000.MP4: Sending chunk 260046848 length 8388608
2017/06/02 11:08:20 ERROR : 01_20170602033554000.MP4: Failed to copy: googleapi: Error 404: File not found: 0B_raumxsbgcgYk9oOGhSSmZjcjg, notFound
2017/06/02 11:08:22 DEBUG : 01_20170602043157000.MP4: Sending chunk 0 length 8388608
 2017/06/02 11:08:23 DEBUG : 01_20170602043157000.MP4: Sending chunk 8388608 length 8388608
2017/06/02 11:08:24 DEBUG : 01_20170602035337000.MP4: Sending chunk 293601280 length 8388608
2017/06/02 11:08:24 DEBUG : 01_20170602043157000.MP4: Sending chunk 16777216 length 8388608
2017/06/02 11:08:25 DEBUG : 01_20170602042157000.MP4: Sending chunk 25165824 length 8388608
2017/06/02 11:08:26 DEBUG : 01_20170602040217000.MP4: Sending chunk 268435456 length 8388608
2017/06/02 11:08:27 DEBUG : 01_20170602043157000.MP4: Sending chunk 25165824 length 8388608
2017/06/02 11:08:29 DEBUG : 01_20170602043157000.MP4: Sending chunk 33554432 length 8388608
2017/06/02 11:08:29 DEBUG : 01_20170602035337000.MP4: Sending chunk 301989888 length 8388608
2017/06/02 11:08:30 DEBUG : 01_20170602040217000.MP4: Sending chunk 276824064 length 8388608
2017/06/02 11:08:30 DEBUG : 01_20170602042157000.MP4: Sending chunk 33554432 length 8388608
2017/06/02 11:08:32 DEBUG : 01_20170602043157000.MP4: Sending chunk 41943040 length 8388608
2017/06/02 11:08:33 DEBUG : 01_20170602035337000.MP4: Sending chunk 310378496 length 8388608

``` Hmm, something is not quite right. I haven't seen a specific problem with media but I have seen the uploads just disappearing.  I'll investigate some more and post another beta. I must have missed something in the team drive docs...

If someone could capture the http transaction which causes the file not found error that would be useful. Use -vv and --dump-bodies (or --dump-headers if that isn't practical) Thanks @timwhite that was very helpful.  I missed a  teamdrive change in the upload chunked files code.  This beta fixes that so it should fix the "File not Found" problem.

https://pub.rclone.org/v1.36-156-gca76b3fb-team-drive%CE%B2/ @Qwatuz I don't think you are using the latest version - can you try this one: https://pub.rclone.org/v1.36-156-gca76b3fb-team-drive%CE%B2/ (note that the 156 is the same, but the ID is different). Excellent - thanks for testing everyone.

I'll merge this to master so it will be in the general betas from now on.

@gfrewqpoiu wrote
>  Only issue left is that during first configuration it always states Failed to save key for NICKNAME, no such section or something like that.

I've made a new issue about that here #1466 as it affects all the oauth remotes.  The message is harmless though.

The next beta will be here: https://beta.rclone.org/v1.36-158-ga5cfdfd2/ (uploaded in 15-30 mins).
  I'm going to close this as a duplicate of #965  Note that rclone measures bandwidth in MByte/s whereas line speeds are usually quoted in Mbit/s.

35 MByte/s is about 280 Mbit/s so I think it is probably doing OK!
  Note that if you haven't disabled them the rclone stats show which files are transferring. You can change the frequency of them with the --stats flag. 

Does that help?   The code adaptively increases it so that it can upload something without too many parts - what problems are you seeing with that?
 I see.  You could add a new flag `--s3-min-part-size` say which defaults to the current value and leave the increasing the part size algorithm alone.
  I've merged that - thank you very much for your contribution :-)
  From my investigations ` --max-read-ahead` is pretty useless - the kernel caps it at 128k which is the default value so I can't see adjusting it dynamically being helpful.

Please re-open if you have some evidence otherwise!  @jdahmen Go for it!  If you need help then ask on the forum is a good plan.  pflags now supports multiple flags so we can have multiple copies of each flag.

This still won't make it as flexible as rsync where you can have include, exclude, include, exclude - this will be parsed as include, include, exclude, exclude but it is probably worth doing. I have fixed this in http://beta.rclone.org/v1.34-60-g2656a0e/ (uploaded in 15-30 mins)  Hi,

I was trying to write to a acd mount point and kept getting the following type of errors `WriteFileHandle.Flush error: Post https://content-eu.drive.amazonaws.com/cdproxy/nodes?suppress=deduplication: http: ContentLength=486 with Body length 57546611`. I found https://github.com/ncw/rclone/issues/813 which explained the problem as 

> This is because Amazon need to know the size of the file in advance for the upload, but we don't know it until the end.

Can you explain this problem in some more detail please because i also read https://github.com/ncw/rclone/issues/669 which seems to suggest that the file size isn't required in advance. 

Looking through the acd API documentation it does mention [here](https://developer.amazon.com/public/apis/experience/cloud-drive/content/restful-api-getting-started) that the size **is** required.

> Content-Length	Specifies the length of the message in the body; without the headers. This is a required parameter when the body of the request in not empty.

Fingers crossed that write support is possible on `mount` to allow use of common tools/scripts to shift data into acd like `cp` or `rsync` etc..

Out of curiosity... in the fuse code when you receive the request would it not be possible to use the target file name that your about to create/write via rclone and scan the associated local source path for the matching file and stat the file size to fulfill acd requirement to specify size on upload ? Or is that a bit willy wonka thinking ?

Great project, thanks for all your hard work. 
 Just some more info which i noticed and seems odd

If i try to cp the below local file to rclone mount point

```
$ stat .viminfo 

File: ‘.viminfo’
Size: 5895          Blocks: 16         IO Block: 4096   regular file
Device: fc01h/64513d    Inode: 441488      Links: 1
```

I get the below error which seems to suggest it detects the file size correctly as `5895` and then fails because of a miss-match between the actual size and the one send in http header `Error: http: ContentLength=430 with Body length 6325`

```
2016/11/13 17:12:27 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/11/13 17:12:27 .viminfo: WriteFileHandle.Write len=5895
2016/11/13 17:12:28 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/11/13 17:12:28 HTTP RESPONSE (req 0xc4200d05a0)
2016/11/13 17:12:28 HTTP/1.1 200 OK
Cache-Control: no-store
Connection: keep-alive
Content-Type: application/json
Date: Sun, 13 Nov 2016 09:12:28 GMT
Pragma: no-cache
Server: Server
Vary: User-Agent
X-Amzn-Requestid: 4cf3a4e1-a981-11e6-8878-XXXXXXXXXXXX

2016/11/13 17:12:28 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/11/13 17:12:28 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/11/13 17:12:29 HTTP REQUEST (req 0xc4200d0b40)
2016/11/13 17:12:29 POST /cdproxy/nodes?suppress=deduplication HTTP/1.1
Host: content-eu.drive.amazonaws.com
User-Agent: rclone/v1.34-DEV
Content-Length: 430
Authorization: XXXX
Content-Type: multipart/form-data; boundary=231ee268c16bb03cde80501ecc73f18e585163d95af8e8a48c3ae0a38b4b
Accept-Encoding: gzip

2016/11/13 17:12:29 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/11/13 17:12:30 .viminfo: WriteFileHandle.Write OK (5895 bytes written)
2016/11/13 17:12:30 .viminfo: WriteFileHandle.Flush
2016/11/13 17:12:30 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/11/13 17:12:30 HTTP RESPONSE (req 0xc4200d0b40)
2016/11/13 17:12:30 Error: http: ContentLength=430 with Body length 6325
2016/11/13 17:12:30 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/11/13 17:12:30 .viminfo: Error detected after finished upload - waiting to see if object was uploaded correctly: Post https://content-eu.drive.amazonaws.com/cdproxy/nodes?suppress=deduplication: http: ContentLength=430 with Body length 6325 ("HTTP status UNKNOWN")
2016/11/13 17:12:30 .viminfo: Giving up waiting for object - returning original error: Post https://content-eu.drive.amazonaws.com/cdproxy/nodes?suppress=deduplication: http: ContentLength=430 with Body length 6325 ("HTTP status UNKNOWN")
2016/11/13 17:12:30 .viminfo: WriteFileHandle.Flush error: Post https://content-eu.drive.amazonaws.com/cdproxy/nodes?suppress=deduplication: http: ContentLength=430 with Body length 6325
2016/11/13 17:12:30 .viminfo: WriteFileHandle.Release nothing to do
```
 Nice explanation @breunigs 

I note that the [bit of code you linked to](https://github.com/ncw/rclone/blob/master/amazonclouddrive/amazonclouddrive.go#L593-L597) is a work around for uploading 0 length files.  If the Put() method could be changed so it could upload 0 length files then uploading would work via FUSE.

The `Put` method in go-acd was written by me and it seems very complicated - perhaps there is a better way...
 OK many thanks for the helpful info.. helped me understand. 

@breunigs Looking forward to testing the write support when it drops :) 

For rsync i'd try something like this, just for file transfer functionality. Using `--size-only` should work around the time stamp issue and be accurate enough for my requirements. 

`rsync -v --inplace --size-only --whole-file --info=progress2 /path/to/src_files rclone_fuse_mnt/`
 This should be fixed in http://beta.rclone.org/v1.34-22-g0b562bc/ (will be uploaded in 15-30 mins)

Please re-open if it doesn't work for you.
  I made a version of rclone with the race detector enabled (also compiled with go-tip).

http://pub.rclone.org/rclone-v1.34-13-gd8b7156-race.zip

Can you run this with

```
rclone --log-file race.log  mount ...
```

Don't run it with `-v` or `--debug-fuse` to keep the logs quiet.

If you give it a thrash with plex to see if you can make it print something in race.log!  If you can get it to print a race condition then I'll be able to fix it most likely!

Note the race detector does slow down rclone a bit.

Thanks

Nick
 I haven't got to the bottom of this yet.

If anyone would like to send me logs of it happening with the latest beta and `-v --debug-fuse --dump-headers` - email them to nick@craig-wood.com with subject "rclone logs for issue 873" I would be grateful! @toomuchio How did you do that with curl?  I haven't seen the log yet? I believe I have worked out what is going here...

Can you try this beta please - it should return an "unexpected EOF" error instead of the "failed to authenticate block" error.

While on its own this isn't very significant, it will allow me to retry the connection which will fix this issue properly.  So if you could have a go with this beta and let me know what happens.

http://beta.rclone.org/v1.34-62-gec0916c/ (uploaded in 15-30 mins)

> crypt: return unexpected EOF instead of Failed to authenticate decrypted block
> 
> Streams which truncated early with an EOF message would return a
> "Failed to authenticate decrypted block" error.  While technically
> correct, this isn't a helpful error message as it masks the underlying
> problem.  This changes it to return "unexpected EOF" instead.

The rest of rclone knows it should retry such errors.
 @toomuchio I think that is the same problem [as here](https://forum.rclone.org/t/rclone-v1-34-60-g2656a0e-panic-in-handler/425/2) which I thought was a nil retry error, but looking closely at the traceback I'm not so sure...

Can you reproduce that problem? @toomuchio I've made a fix for that - here is a beta (which includes the other changes): http://beta.rclone.org/v1.34-63-g8083804/ (uploaded in 15-30 mins). Here is a new beta which should retry connection fails.  This should be the final piece to fix this issue.

http://beta.rclone.org/v1.34-70-g43c5309/ (uploaded in 15-30 mins)

I have removed the async buffering for the moment which was causing problems with seek - quite likely this was the cause of @nummersyv problems.

I've got a few more bugs to fix then I'll be releasing v1.35 so any testing much appreciated! OK Fixed that!

http://beta.rclone.org/v1.34-71-g4482e75/ (uploaded in 15-30 mins)

>  golang seems to filled with bugs on debian testing atm

What version of Go are you using?  You'll need 1.5 or later to compile rclone.  The error message should say that though. @nummersyv I had a look at your log.  It appeared to contain several seeks and no errors, so I don't think the problem is rclone seeking.  Maybe it is the video player not liking the delay on seek.  Did you try seeking then waiting for a while to see if the player would sync?

@scoopydude2002 thanks for the update.  I'll put the buffering back in eventually when I've worked out how to fix it properly! Thanks for all your testing.  I'm going to mark this one as fixed and include it in the 1.35 release :-)  I have had a look over the code - looks very nice :-)

I don't think you've generated or run [the unit and integration tests](https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md#writing-a-new-backend)

On that page are some more things that need to be done for making a new Fs too.

I'm happy to do some of those things (eg docs) but can you do the testing parts please? You'll find lots of corner cases in the tests!

If you could squash this into two commits, one for your actual code, and another for all the vendor stuff that will make it easier to review.

Thanks for a super contribution - looking forward to the next version :-)
 HI!  Just wondering if you are working on the next update of this? @jackschmidt I'm going to merge this to a branch and add tests in time for the 1.36 release.  Hopefully that is OK with you!

-- Nick @jackschmidt 
> That works. I don't have time to work on it.

No problem - thank you for your contribution - much appreciated :-)  Super -thank you very much!  Merged :-)
  OK I'll close this one, then you can open a new one when you have all the details.
  I see what you are getting at I think.  Did you have a specific use case in mind?
 >  At each touch, I have to re-enter the password for the config… not an incentive to keep an appropriately complex password. :)

Did you know you can put the password in an environment variable?

Put this into a file (called `set-rclone-password` say)

``` sh
#!/bin/echo Source this file don't run it

read -s RCLONE_CONFIG_PASS
export RCLONE_CONFIG_PASS
```

Then source it before you want to use rclone

```
$ . ~/bin/set-rclone-password 
$ echo $RCLONE_CONFIG_PASS
potato
```
 I've put the shell magic for password reading in the docs now, so I'm going to close this issue  I read from https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html that $XDG_CONFIG_HOME is probably what is required.

> $XDG_CONFIG_HOME defines the base directory relative to which user specific configuration files should be stored. If $XDG_CONFIG_HOME is either not set or empty, a default equal to $HOME/.config should be used.

That would fulful the request I've had in the past to get the config file into the `.config` dir.
 Does anyone know what macOS does...  Is the XDG convention sensible here?
 This is now done!  See this beta

http://beta.rclone.org/v1.35-44-g8068ef9/ (uploaded in 15-30 mins)  I'm sorry to hear that :-(

Just so I've got things straight
- You used rclone 1.33 to upload files to drve using `rclone sync`
- Somewhere in the directory tree there was a junction point to another drive
- When you upgraded to 1.34 you ran the upload again, however
- rclone ignored the junction point and deciding all the files were missing deleted all the files under it on Google Drive

is that correct?

I'm sorry that was an unexpected consequence of ignoring Junction points.  I didn't think rclone could read  junction points at all, but I'm obviously wrong about that.

What behaviour do you think rclone should have with Junction points?
 What would you think about an option to not skip junction points?

As far as I understand it junction points are kind of like symlinks under unix.  `rsync` skips symlinks by default and so does `rclone`, but there is an option to allow `rsync` to follow symlinks.  I'm planning to implement that option `-L` in #40 for the next release.  It would seem to make sense to me to make `-L` mean follow junction points under Windows.

You'd then restore the v1.33 behaviour with `rsync -L`.

BTW How do you make your junction points?  I need to learn how to do it so I can understand them better!
 `-L` is now implemented, so I'm closing this issue.  That would depend on what info rclone gets from FUSE I suspect.  I'll have a think...
  This is a duplicate of #762 most likely.

I suspect you are using crypt where the file names can become too long to fit in amazon's limits.
  You can use standard linux tools to help like this (put in a shell script).

```
renice 19 $$
ionice -c 3 -p $$
rclone ....
```

What is the command line of the rclone command you are using?
 :-(  I have had lots of horrible things go wrong with NFS in the past so I can't say I trust it any more!
  So you don't have any duplicate files any more...

This looks like there has been a data corruption somewhere - Google's MD5 hash they stored when they stored the picture doesn't match the MD5 hash rclone calculates when it downloads the picture.

If I implement the `--ignore-checksum` flag from #793 then you would be able to download the files at least and see how corrupted they are.  It might be worth trying to download them via the web interface and have a look at them.
 Can you try this which implements the --ignore-checksum command?

http://pub.rclone.org/v1.35-15-g82742f5-ignore-checksum%CE%B2/ I've merged this to master for the 1.36 release

Here is the beta - http://beta.rclone.org/v1.35-62-g9d331ce/ (uploaded in 15-30 mins)  rclone already support s3 public buckets like this.  The question would be whether the other cloud providers let you use their APIs on public shared docs.

> re there any plans to include a way to make rclone behave more like rsync in the way it actually syncs

rclone syncs many things concurrently which rsync doesn't hence getting things out of order.  If you run with `--checkers 1 --transfers 1` you'll get behaviour more like rsync at the expense of lots of speed.
  I put that in to fix #689

Yes I agree with you about the engineer who came up with `Icon\r` files!

I could put a flag in to allow files with control characters - is that worth doing?  I think they should be disallowed by default.
 Thanks for letting me know that you can store `\r` on ACD - that is a surprise!

As for the actual `Icon\r` files, I believe they are 100% resource fork which won't have been backed up properly by rclone anyway.

I'll close this issue for now, but it will remain in the archives should anyone else have a similar experience!
  I've fixed that in the go-acd repo here: https://github.com/ncw/go-acd/commit/a3c3df8bf8c4a3b8c5c0e10ec88e914c68ffa977

You can find an rclone beta with this change in http://beta.rclone.org/v1.34-03-g34e7ca9/ (uploaded in 15-30 mins)
 Yes that is the change from %v to %q.  I'd say that is working as intended - %q will escape any `\n` that get in the output for instance.  Does that cause you a problem?
 @diamondsw this will be in the next release -it isn't in 1.34 yet.  Strange - It works for me, I just tried it now.

Did you tell photos to show your pictures in Drive?  Go to https://photos.google.com/settings and make sure the `Show Google Drive photos & videos in your Photos library` setting is set.

Assuming you did that already...

Does `rclone ls  'googledrive:Google Photos'` work?  I suspect it will fail in the same way.

If it does fail in the same way can you do

```
 rclone -v --dump-bodies --retries 1 ls  'googledrive:Google Photos' --log-file output.log.txt
```

Or if it doesn't do

```
 rclone -v --dump-bodies --retries 1 copy  'googledrive:Google Photos' /path/to/wherever --log-file output.log.txt
```

and post (or email it to me privately at nick@craig-wood.com if you prefer) the output.log.txt please.

Thanks

Nick
 Turns out this is  google/google-api-go-client#139

I've sent instructions via email on which image to delete to fix it!
 Looks like google have fixed the original bug, so come the next update of the vendor directory this should be fixed too! No, I don't think so....

I'll update the vendor directory and post a beta here for you to try if you want? I've updated the vendor directory, so hopefully you should find this fixed in the latest beta

http://beta.rclone.org/v1.35-117-g8ec57d1/ Google photos are a bit prone to having duplicates - I suggest you run the `rclone dedupe` command [check the docs first](http://rclone.org/commands/rclone_dedupe/) and remove the duplicates, then try the sync again. @idcmp great :-)

I'll close this issue now.  > If that helps, a similar behavior happens using acd_cli.

That probably means it isn't rclone's fault.

It might be your ISP doing traffic shaping, or there might be packet loss on the link.

It would be worth taking a trawl through rclone's logs to see if it is having to do retries to fetch more stuff from amazon.
 It might be worth you looking at [this thread](https://forum.rclone.org/t/acd-fuse-mount-still-not-working-with-plex/170/12) on the forum where setting `--max-read-ahead` is discussed and see if that helps.
 OK I'll close this issue then.  If you find it is rclone's fault then please re-open!
  So does using `rclone ls acde:` work ok?

Are you using 32 bit ARM or 64 bit?

Can you try the beta's here: http://beta.rclone.org/ and try to find one which works and then work out where it stops working?  It might be that the update of the fuse library before the v1.34 release broke things.
 @zordiack is there an old mount running somewhere? `mount | grep rclone` will show you, you can then use `fusermount -z -u` to unmount it.  If that fails reboot.
 No problems - glad you got to the bottom of it :-)
  A quick thought... Try using ACDE: instead of ACDE:/ in your mount command line. 
 I note also that you've used two different remotes `ACDE:` and `acdenc:` in your tests.

What do you get if you do

```
rclone -q ls --max-depth 1 ACDE:
```

Hopefully that will show the single file you see in your mount.  That is effectively what mount does anyway.

Note that `rclone lsd` doesn't show files, so your `rclone lsd ACDE:` isn't ever going to show that file.

For your final test you seem to be using `rclone lsl acdenc:` which is a different remote.
  Make an option to download the file and calculate missing checksums on the fly.

This would be useful for crypt mounts in particular and for `rclone md5sum` and `rclone sha1sum`

See [the discussion on the focum](https://forum.rclone.org/t/so-is-there-any-way-to-clean-up-those-empty-left-behind-directories/114/15)

Ideas for what the flag might be called

  * `--calculate-checksum-if-missing`
  * `--calculate-missing-checksums`
  * `--calc-missing-checksums`
  * `--calc-missing-chksum`
  > Here's the rebased patch to add moving to amazon drive. It needs this patch to work: ncw/go-acd#2, which is also why the CI fails.

I've put some comments on that PR for you.  Like that patch, can you rebase this one rather than create a merge commit please?

I've reviewed this patch too.

Can you rebase onto `ncw:acd-move` branch, fixup, squash where appropriate and (force) push please?

> I'm fairly certain this patch can be easily extended to also support DirMoving, since files and directories are essentially the same to AmazonDrive API wise. I will have to look into that though and will be able to work on this next weekend at the earliest. If you keep the PR open I will add the patches here, otherwise I'll open a separate PR.

That was my thinking that DirMove would be very similar.  If it is really easy add it to this PR, otherwise a new one.

> If you're generally ok with the changes let me know, and I can create a new squashed PR.
> 
> I fixed a couple of bugs and noticed that the DirCache was not properly updated. Right now it simply flushes the whole cache on move, although a more fine grained clean up would be possible: While I can dc.Put(newPath, oldFileId), I cannot remove the old dc.cache[path] = id association, for which I cannot see any option. So I guess the DirCache would need an extension like Delete(path string) and/or DeleteInv(id string). Delete + Put would be enough for Amazon's case, but it does make the API rather fragile. Not sure if it's worth the effort, so I'd say let's wait and see if there are any performance issues with flushing the cache.

Yes I see what you mean.  Refilling the dircache is reasonably expensive (especially if you are accessing files many directories deep) so it would be nice not to flush it if we don't have to.

Feel free to leave it like it is and we can sort it out later - I'd rather have correct and slow than no feature or incorrect and fast ;-)
 > It appears, that z.txt, which was recently moved from trhE5BtJTYKGCFuHAVUBtg to EfaOMydZQSaJdJv0MrdsMg still appears in the query for children of trhE5BtJTYKGCFuHAVUBtg. So, I guess this might be a caching issue on Amazon's side? Yaaayy…

Yes that is the "eventually consistent" listings.  You'll find the tests are full of workarounds for that - acd has been more trouble than all the other remotes put together here!

> @ncw I see two options here:
> - filter in listAll to only pass the nodes whose parents actually include the one queried for
> - make listAll find the directory and use GetChildren instead, but I'd have to check if latter is cache-bug free

Am I correct in thinking this is an issue which only really affects the tests?  If so the option I usually use is to retry the listings the tests a few times until they give the answer we want, or given an error if they don't.  For instance check out [this lovely bit of code](https://github.com/ncw/rclone/blob/master/fstest/fstest.go#L147).

Your first option seems sensible and low cost too.  As for your second option, I doubt `GetChildren` will be any better.
 @breunigs is this ready for another review or did you want to work on it more?
 In the interest of keeping the momentum going I've done a bit of squashing and rebasing and a couple of minor fixes to make the linter pass and merged this to master.  Thank you for your excellent work.

> Arguably, retrieving metadata could be moved into go-acd, either as separate method or as an option to FolderFromId and NodeFromId. It's only ever used once in DirMove. Let me know what is the most idiomatic option here, and I'll adapt the patch.

Yes I agree that `FolderFromID` and `NodeFromID` would be a good idea in go-acd.  In fact I wrote the latter this week while experimenting with resume.  I've deleted it again though!

> I couldn't get the CheckListing tests working for DirMove. All work fine, except the last one that checks if newRemote is empty again. No amount of sleeping before the check will make it pass. However, if I sleep 30s directly after each DirMove, before doing any checkListing calls the tests now pass. I left it at that, see below for my working theory as to why this happens.

I'd like to get the tests working if we can.  The ACD tests are the least reliable tests though!

>  I didn't know how to optionally call fs.dirCache.ResetRoot on file systems that implement it. Not sure if it's worth the effort, either.

The way to do that would be to make another optional interface for fs.Fs - say `FlushCache()` then implement that for the remotes which use the dircache.  You'd then use a type assertion to discover whether the interface is available or not and call it if it was.

If you just want to try something out then define the extra public method on amazon cloud `Fs.FlushCache()` then do this when you want to use it - if it works I'm happy to flesh it out from there.

``` go
  if do, ok := f.(interface { FlushCache() }); ok {
    do.FlushCache()
  }
```
  Use [sync errgroup from x repo](https://godoc.org/golang.org/x/sync/errgroup) to re-write and simplify some internal parts of rclone such as

  * directory listing routines in the remotes
    * when source list breaks can cancel destination list
  * directory scanning routines in fs
  * sync/copy/move in operations

See also the [article about the feature](https://www.oreilly.com/learning/run-strikingly-fast-parallel-file-searches-in-go-with-sync-errgroup)  The main go compilers only support `linux/mips64` and `linux/mips64le` 

For MIPS32 you'd have to use gccgo which I don't have much experience with.

Fancy working out how to do it?
 Here would be the place to start: https://github.com/golang/go/wiki/GccgoCrossCompilation
 It shouldn't need any sort of porting, just building the right cross compiler.

I think go 1.9 (out in about 9 months) may have native support for MIPS32.
 Unexpectedly (to me anyway) mips32 support will be in go 1.8 which is in beta right now.  I attempted to compile a version for you to try but failed due to: https://github.com/golang/go/issues/18192  - I'm sure it will work soon though! I'm pretty confident that we'll have rclone building for mips32 quite soon. I don't know exactly what hardware is supported by the go runtime though.  OK the upstream golang/x/sys repo is fixed (swift work Go team!) so here are some binaries for mips and mipsle built with go 1.8beta1

  * mips - http://pub.rclone.org/rclone-v1.34-49-ge79a5de-mips.zip
  * mipsle - http://pub.rclone.org/rclone-v1.34-49-ge79a5de-mipsle.zip

Check out [how to install root certs also](http://rclone.org/faq/#rclone-gives-x509-failed-to-load-system-roots-and-no-roots-provided-error)

I'd be interested to know if these work!

rclone definitely uses floating point maths for the stats calculations BTW.

Do I need to build both mips and mipsle builds?  Are they both in use? @alecuba16 thanks for the link! Now that go 1.8 has been released I've integrated linux/mips and linux/mipsle into the release.  Please find them in this beta.  This will be in the 1.36 release.

http://beta.rclone.org/v1.35-119-g69a15ae/ (uploaded in 15-30 mins)  Yes if there were errors in `rclone sync` then it will not delete any files from the source.  I borrowed this idea from rsync and it is a good one to stop you losing your data when the network is flakey.

Look at the errors rclone logs and work out why you are getting errors - maybe they are something you can fix like permissions, or maybe you need to increase `--retries`.

In @davidafuller01 's case it looks like rclone can't open the source directory "myPictures".  The error message isn't very helpful here - I'll try to improve it!

In #642 I'm going to add an `--ignore-errors` flag which will allow you to run deletions locally if you really want to even if there were errors - click subscribe on that issue if you are interested in it.
 @davidafuller01 I've improved the error message for that - in this beta: http://beta.rclone.org/v1.33-107-gff41b0d/
 @davidafuller01 

> @ncw http://beta.rclone.org/v1.33-107-gff41b0d/ seems to be working better. It's not throwing the same errors. Did you change something besides the error messages?

Yes there are quite a lot of changes from 1.33 in that version

> Are the changes you made included in the new v1.34?

Yes they are.
 I'm assuming this is all good now :-)  Excellent idea - I have the same problem when people sent me logs!  It would probably only appear with the -v flag though.
 This is done in

http://beta.rclone.org/v1.35-128-g86cc9f3/ (uploaded in 15-30 mins)
  Using onedrive on MacOS Sierra with rclone 1.33.
Especially with folders containing a lot of files (and especially with large ones like movies or software), I can't complete the copy because I get some errors.
Some logs : 
```
2016/11/01 11:07:41 Movies/Toy Story Toons_ Hawaiian Vacation/Toy Story Toons_ Hawaiian Vacation (HD).m4v: Uploading segment 94371840/234431669 size 10485760
2016/11/01 11:07:48 Couldn't decode error response: invalid character '<' looking for beginning of value
2016/11/01 11:07:48 pacer: Rate limited, increasing sleep to 20ms
2016/11/01 11:07:48 pacer: low level retry 1/10 (error 503 Service Unavailable: )
2016/11/01 11:07:48 pacer: Reducing sleep to 15ms
2016/11/01 11:07:48 Mobile Applications/AIR RACE 1.74 1.ipa: Cancelling multipart upload
2016/11/01 11:07:48 pacer: Reducing sleep to 11.25ms
2016/11/01 11:07:48 Mobile Applications/AIR RACE 1.74 1.ipa: Failed to copy: Put https://api.onedrive.com/up/eyJSZXNvdXJjZUlEIjoiNzlGNDY1NEQxQkE0MDc1MyE0MzM2OSIsIlJlbGF0aW9uc2hpcE5hbWUiOiJBSVIgUkFDRSAxLjc0IDEuaXBhIn0/3mH_bu4oxlLxXb9HHmyTv_gpPPfgNWBMQnWRN83pBuVJrqGrZPMv9hdLyyIp8N0Wbl/eyJOYW1lIjoiQUlSIFJBQ0UgMS43NCAxLmlwYSJ9/3wYBV0iMr46yENesCJ0xvNTVpK2CQAtivBMe_RuO6iU24CkO4DJRfbBiH3T1YFKC1XJ1pKDKKpjKWLDv15JlJpnBJAnvaxH-DhJQhQIO_belAAzkDg_k50zPnDYPW0uUu5RM24Z1qthsIRZYbp_AxGF5KW_cZdClPA8Jdsetfc4n9ex7krcxpFoseIbCMyTXnW5FFTd9/0M7UuLA_4xLSy6s5h8_d3dvtuISUlDTd667JBIFMlES0C5gpm0JKT2fWGRRqyKd1hBVHx5B2XVGuimAYQukJaFQ1Fy3hMGF3wlXJ4BCWnBYgrZTrRvStQkewadl1ksSPSWyCuNBJY-Dj9sSsn327JA-m5_IesAzE7v7dthwrrrUUC-jnp983rE0L0D2BBxiZiR-HWgmQ/ti3UGU2vktTY_89tA2cautyH-jcwFCMpO3y1E5OqFCcMsHRFbPfWeUsTwBrY34YRPq3EKn0A: http: ContentLength=10485760 with Body length 0
2016/11/01 11:07:48 Mobile Applications/AIR RACE 1.74 1.ipa: Removing failed copy
2016/11/01 11:07:49 pacer: Reducing sleep to 10ms
2016/11/01 11:07:49 Mobile Applications/AIR RACE 1.74 1.ipa: Failed to remove failed copy: invalidRequest: API not found
```

 Another type of issue linked to this seems to be related to the length of the file name. Example:

```
2016/11/04 09:18:59 Music/Les Enfoirés/La Boîte À Musique Des Enfoirés/2-01 La Mode_ Le Sens De La Vie _ Call Me Maybe _ C'est Beau La Bourgeoisie _ Ai Se Eu Te Pego _ Et Alors _ Gangnam Style _ À La Pêche Aux Moules (Chanson Enfantine).mp3: Failed to copy: invalidRequest: Bad Argument
2016/11/04 09:18:59 Music/Les Enfoirés/La Boîte À Musique Des Enfoirés/2-05 Un Collège Au Théâtre_ Les Yeux d'Elsa _ T'es OK _ Heureux Qui Comme Ulysse _ C'est Ma Terre _ Tirade Du Nez _ Mignon, Mignon _ On Ne Badine Pas Avec L'amour _ À Ma Place _ La Langue De Chez Nous.mp3: Failed to copy: invalidRequest: Bad Argument
```

I tought  this limitation of path length was gone in one drive.
 Can you try v1.34 please?

And if you could capture some logs of the things going wrong with `--dump-headers` that would be very useful - that should hopefully tell us what is going on.

As for lengths - those look rather short to be causing a problem. It might be that this is a file name encoding problem fixed in 1.34.
 Nick, first tests show improvement. the error Failed to copy: invalid request seems gone. But the errors pacer: low level retry seems still to be there. I will post more info tomorrow.
 A few low level retries is normal.  Can you post some logs?
 command line:

> /Users/fbastok/Downloads/rclone-v1.34-osx-amd64/rclone -v --dump-headers sync "/Users/Shared/iTunes Media" onedrive-backup:Backup/iTunesMedia 2>Desktop/rclone_errors.txt >Desktop/rclone_verbose.txt
> See attached file for the logs
> [rclone_errors.txt](https://github.com/ncw/rclone/files/582714/rclone_errors.txt)
> [rclone_verbose.txt](https://github.com/ncw/rclone/files/582715/rclone_verbose.txt)
 @fbastok I looked rclone_errors.txt and I see this eror

```
2016/11/10 07:55:30 Music/Les Enfoirés/La Boîte À Musique Des Enfoirés/2-01 La Mode_ Le Sens De La Vie _ Call Me Maybe _ C'est Beau La Bourgeoisie _ Ai Se Eu Te Pego _ Et Alors _ Gangnam Style _ À La Pêche Aux Moules (Chanson Enfantine).mp3: Failed to copy: invalidRequest: Bad Argument
2016/11/10 07:55:30 Music/Les Enfoirés/La Boîte À Musique Des Enfoirés/
Music/Les Enfoirés/La Boîte À Musique Des Enfoirés/2-05 Un Collège Au Théâtre_ Les Yeux d'Elsa _ T'es OK _ Heureux Qui Comme Ulysse _ C'est Ma Terre _ Tirade Du Nez _ Mignon, Mignon _ On Ne Badine Pas Avec L'amour _ À Ma Place _ La Langue De Chez Nous.mp3: Failed to copy: invalidRequest: Bad Argument
```

Do these files always fail to transfer?  I tried transferring a file with that name and directory structure and it worked fine.  What is the original name on your hard disk?

in rclone_verbose.txt I couldn't see any problems.  I saw a few retries from 502 gateway  errors but these are normal.
 Yes, they always fail to transfert. It's the original name on the hard drive
the full directory:
/Users/Shared/iTunes Media/Music/Les Enfoirés/La Boite À Musique Des Enfoirés/2-01 La Mode_ Le Sens De La Vie _ Call Me Maybe _ C'est Beau La Bourgeoisie _ Ai Se Eu Te Pego _ Et Alors _ Gangnam Style _ À La Pêche Aux Moules (Chanson Enfantine).mp3
 @guilhermebf let me try that and I will post the results
 Command line:
/Users/fbastok/Downloads/rclone-v1.34-osx-amd64/rclone -v --ignore-size --dump-headers sync "/Users/Shared/iTunes Media" onedrive-backup:Backup/iTunesMedia 2>Desktop/rclone_errors.txt >Desktop/rclone_verbose.txt

Some errors:

> Errors:                 0
> Checks:              9829
> Transferred:           84
> Elapsed time:     15m1.3s
> Transferring:
> - ...19 Tableau _L'Eurovision Des Enfoirés_.mp3: 23% done. avg:   38.8, cur: 2364.3 kByte/s. ETA: 14s
> - ...Enfoirés/18 Tableau _Le Train Fantôme_.mp3: 57% done. avg:   73.7, cur: 1164.3 kByte/s. ETA: 12s
> - ...és/20 Tableau _Une Etrange Expo Photo_.mp3: 35% done. avg:   40.3, cur: 4023.4 kByte/s. ETA: 4s
> - ...és/21 Les Conseils Musicaux De Michaël.mp3: 31% done. avg:   42.1, cur: 8941.9 kByte/s. ETA: 2s
> 
> 2016/11/20 09:38:42 Music/Les Enfoirés/Au rendez-vous des Enfoirés/20 Tableau _Une Etrange Expo Photo_.mp3: Failed to copy: Put https://api.onedrive.com/up/eyJSZXNvdXJjZUlEIjoiNzlGNDY1NEQxQkE0MDc1MyE1MDA1NCIsIlJlbGF0aW9uc2hpcE5hbWUiOiIyMCBUYWJsZWF1IF9VbmUgRXRyYW5nZSBFeHBvIFBob3RvXy5tcDMifQ/3mukilpSox_6cuvtrjFWBz4IxU9-y6380LSzsb4UVQ_yNnwzWyVzY6czxC6B_4_Mhu/eyJOYW1lIjoiMjAgVGFibGVhdSBfVW5lIEV0cmFuZ2UgRXhwbyBQaG90b18ubXAzIn0/3wW7rIzJinu-cQDBOfWZctNCQhPEjqfzeXcapVlwv8735Bp5dkKHin6DDM7ZDFygBoFUTVnbt5oJ92uFreLFV4jEDTroeykhW7EwVIWrvllNiutSv-BkGNYJsdfHhCvXcZj6MfucpXn8i4fdLAA-DE6JNv9HiZtcN1sqPhYKKErRiRDHvI4CuQleMFAc_ElTvf4gmb-5/SqIAhqqwmBfOyFtSKb5_uniHYNjualaAmBaImPDWGZhVdvHLeXoXc96Z8WxC_is0FTwJ0wH7OUpgZa0Jt9YlgrHrwQ_A71hAgvfK4n7n-PexUwnZs3ulnHmIwSMW4dfeV21rFeDvP_rX8ueg8t0F946jID4VffuiqEVogK6aBda9J3CUZKcEbve5EqvWO72lCT2Q2nzH/f2ekYbriiBhlXpisyB39Gg8szyQr9zn7vO1eja28w3fyRkjSq-QXMcf1soL6kCSMaT2zLNBg: read tcp 192.168.1.243:50241->204.79.197.213:443: read: network is down
> 2016/11/20 09:38:42 Music/Les Enfoirés/Au rendez-vous des Enfoirés/18 Tableau _Le Train Fantôme_.mp3: Failed to copy: Put https://api.onedrive.com/up/eyJSZXNvdXJjZUlEIjoiNzlGNDY1NEQxQkE0MDc1MyE1MDA1NCIsIlJlbGF0aW9uc2hpcE5hbWUiOiIxOCBUYWJsZWF1IF9MZSBUcmFpbiBGYW50w7RtZV8ubXAzIn0/3mEdPnbYI0ulDBhf6TZzC0VQ9VsPIljeMwUFw7IcqJvjkZ2Kg9C6kN7cJk43snf3C_/eyJOYW1lIjoiMTggVGFibGVhdSBfTGUgVHJhaW4gRmFudMO0bWVfLm1wMyJ9/3wqSDXpV8UJhdV-rh01rP59fMfc5rVmdl7PEArivi2WlbNxCo6a37DQy7vgpLMowIVuMrFdntOFObnS7VdU8DecEGBDw5tTbBqwUQ7Fb34YQQ93dtw4dntAz1yUE8SdxD5O5eH-SUDPFa0IbF-DFxJ_aXSqRHi6q2nM6R7vyI685AEh7I7sVsOYdt0AeGhiGCrYWUTpn/JsHHlyqR4RJjsIm0zYPP0ADIYi9uSTIZ6pv-oqCnWayDxOOT4fhpo5-K-mF1X25ODaomZd4qjUSPhdl3a3JP2-LKB-dBwZQBT9ojg4jIRJG3aVY8gqcrUDhCcc_DZxAs5LGtUqrWCCo2k1Sxia4--wOlXSOOM3t5iLpMNKWS4ov-PdgIcIApmTLxehWsv5o5cQKBm4kd/FSBfgBzc3MrZnBrhh3AnnZBV24AwCtshRZavFBiNTHlsqHfuOev5xT5fmgAVmP-3fUj2nVeg: read tcp 192.168.1.243:50244->204.79.197.213:443: read: network is down
> 2016/11/20 09:39:25 Music/Les Enfoirés/La Boite À Musique Des Enfoirés/2-01 La Mode_ Le Sens De La Vie _ Call Me Maybe _ C'est Beau La Bourgeoisie _ Ai Se Eu Te Pego _ Et Alors _ Gangnam Style _ À La Pêche Aux Moules (Chanson Enfantine).mp3: Failed to copy: invalidRequest: Bad Argument
> 2016/11/20 09:39:25 Music/Les Enfoirés/La Boite À Musique Des Enfoirés/2-04 La Soirée Diapos_ Le Coup De Soleil _ Regarde-Moi Bien En Face _ À Présent, Tu Peux T'en Aller _ Rien Que De L'eau _ C'est Dit.mp3: Failed to copy: invalidRequest: Bad Argument
> 2016/11/20 09:39:27 Music/Les Enfoirés/La Boite À Musique Des Enfoirés/2-05 Un Collège Au Théâtre_ Les Yeux d'Elsa _ T'es OK _ Heureux Qui Comme Ulysse _ C'est Ma Terre _ Tirade Du Nez _ Mignon, Mignon _ On Ne Badine Pas Avec L'amour _ À Ma Place _ La Langue De Chez Nous.mp3: Failed to copy: invalidRequest: Bad Argument
> 2016/11/20 09:39:27 
> Transferred:   744.799 MBytes (793.324 kBytes/s)
> Errors:                 5
> Checks:              9860
> Transferred:           85
> Elapsed time:     16m1.3s
 I'm not having much luck reproducing this.  Can you make a log with `-v --dump-headers` too please and post the relevant section here, or email the whole lot to me at nick@craig-wood.com if you prefer with the subject "rclone log for issue 846"  - thanks
 Attached rclone_versbose and rclone_errors, using --dump-headers.
Command line is the one above.
[rclone_errors.txt](https://github.com/ncw/rclone/files/605773/rclone_errors.txt)
[rclone_verbose.txt](https://github.com/ncw/rclone/files/605774/rclone_verbose.txt)

 Back to the first issue I had. I tried to upload one file at a time.
The error "Couldn't decode error response: invalid character '<' looking for beginning of value" cancel the upload and so it never completes.
I get this always at some point during the upload:

> Transferring:
>  * ...g.2006.FRENCH.720p.BluRay.x264-AiRLiNE.mkv: 14% done. avg:  919.1, cur: 1082.1 kByte/s. ETA: 1h0m26s
> 
> 2016/11/22 10:37:26 Camping 2006 FRENCH/Camping.2006.FRENCH.720p.BluRay.x264-AiRLiNE.mkv: Uploading segment 671088640/4689143360 size 10485760
> 2016/11/22 10:37:36 Camping 2006 FRENCH/Camping.2006.FRENCH.720p.BluRay.x264-AiRLiNE.mkv: Uploading segment 681574400/4689143360 size 10485760
> 2016/11/22 10:38:19
> Transferred:   660 MBytes (865.315 kBytes/s)
> Errors:                 0
> Checks:                26
> Transferred:            0
> Elapsed time:       13m1s
> Transferring:
>  * ...g.2006.FRENCH.720p.BluRay.x264-AiRLiNE.mkv: 14% done. avg:  874.3, cur:   80.5 kByte/s. ETA: 13h28m1s
> 
> 2016/11/22 10:38:46 Couldn't decode error response: invalid character '<' looking for beginning of value
> 2016/11/22 10:38:46 pacer: Rate limited, increasing sleep to 20ms
> 2016/11/22 10:38:46 pacer: low level retry 1/10 (error 503 Service Unavailable: )
> 2016/11/22 10:38:46 pacer: Reducing sleep to 15ms
> 2016/11/22 10:38:46 Camping 2006 FRENCH/Camping.2006.FRENCH.720p.BluRay.x264-AiRLiNE.mkv: Cancelling multipart upload
> 2016/11/22 10:38:46 pacer: Reducing sleep to 11.25ms
> 2016/11/22 10:38:46 Camping 2006 FRENCH/Camping.2006.FRENCH.720p.BluRay.x264-AiRLiNE.mkv: Failed to copy: Put https://api.onedrive.com/up/eyJSZXNvdXJjZUlEIjoiNzlGNDY1NEQxQkE0MDc1MyE0NzQzNTkiLCJSZWxhdGlvbnNoaXBOYW1lIjoiQ2FtcGluZy4yMDA2LkZSRU5DSC43MjBwLkJsdVJheS54MjY0LUFpUkxpTkUubWt2In0/3mszD8tbSGYdg5xb5uA-WDmIzA2zSH3RsYqykhV1Dcl99Yf8bHNpB6DUz2wnuVaW1Y/eyJOYW1lIjoiQ2FtcGluZy4yMDA2LkZSRU5DSC43MjBwLkJsdVJheS54MjY0LUFpUkxpTkUubWt2In0/3wd3g4W3t9JBfq5_mY8cgnSc5_nu-5HaP0y6t7TZi_rbUZYajHJWE0jz7ddXOmK7KqlS2C41QSpWfkjbQ_5MOH-eogv3OoPawEri2nzsfdxhEEjFrpRiPZOPntPYcDYhMX-MvsOT7i_BGHvIH-iwq9Soi1enbCrHRGT_jyUcbvJBEAmSffu9cIM7BWRJXjFDe9NJN2Qx/rNJFItwhYWVx34GOR3eZjwAcihZN4VEDQKHsMvX699ANR9xdZP33Twfidj6n4G6geDpehsCdWRhY0oEyW9pkwgJggg30VEd7FdrFWJH5HTjrcttHBKSWhBGdvjFdVPvkD7ytF5Ori3BRc7Tv4gexCDiW1YO8UPAWXBvUyLfGnLhomTihih-TNTCCZLSjymKAA7tNuYoX/dAKtZbYmm4fc1whImk0reeWvrWZapO9V08RuArQpicxiYO-wL4-6Sc903-SYm2oe7v5zISQg: http: ContentLength=10485760 with Body length 0 I was able to upload the file Campingxxx.mkv above after a few tries. So it seems not to always happen. Won't be easy to fix.
the next 5 movies I tried to upload didn't work. The same error as above I replicated the same behavior in Linux (linux mint 18). Same errors. @yonjah you've been looking at onedrive stuff recently - what do you think of this?  I've merged this in 1c912de9cc058515468bfaa9bbca6b6b4325efd9 - thank you very much for working that out :-)
 @wikt0r I don't know.  I've had reports from other people that they are having trouble uploading big files - it might just be growing pains for ACD.  The individual file transfer stats show the transfer rate of the individual files averages over a short period.

I could add a short time period transfer rate to the main stats easily enough though.
  Interesting idea - does YouTube have an API for doing that?
  I've given the CI a kick so hopefully all will pass.

The CI doesn't check against all the remotes, but I run a rull integration test before making a release.  It takes quite a long time (~60 mins) and often needs retrying so isn't really suitable for a CI test.

If you cd to the `fs` directory and run `go test -v -remote TestAmazonCloudDrive:` it will run the integration tests for ACD.

I've spent the evening chasing race conditions down so I'll have to leave review of your patch until tomorrow!
 I've rebased and merged this into the `acd-move` branch - thanks :-)
  What it looks like is `"sync" "/Volumes/music/3archives"` disappeared after the first retry.

Is that a network mount?  Could it have disappeared?  You could try mapping it to a drive letter and see if that helps - I think mounts are more likely to stick around like that.
 Ah sorry, I thought you were on Windows!  Let me know if you find anything.
  I think this would be hard to implement the way rclone is structured.  I'll leave it as an enhancement though just in case I or anyone else has an idea about how to implement it.
  Thanks for writing this up.
 @mwitkow you did the work in #444 - would the same approach work here?  If so fancy adapting it?  Sounds interesting...  Any chance you could dig up the API docs for this?
  rclone doesn't delete empty directories at the moment - see #100 

Note that rclone doesn't do any deletions in the presence of errors.
  Can you post the logs from the mount command `rclone mount egoogle: /home/user/Google/ --allow-other --dir-cache-time=2m &` - that should show what is going on.
  Thank you @breunigs for your contributions.  I think @toomuchio has indicated that he is out of time, so do you want to carry on?

I've merged this into the `acd-move` branch along with #842 - @breunigs can you rebase your work onto this branch and submit me a pull request with your changes against this branch please?  If you could also submit a PR to the go-acd repo?
  Will be great if rclone will have some stats as output.

So we can do for example `rclone --stats` h or `--stats d`

Stats for uploaded count of files and uploaded mb/gb/tb (i can do upload in one day about max 3-5TB of data xD so thats why TB need)

I know there are tons of apps like `vnstat` but whats about rclone "own stats"?

Thx, keep the great work :)
 rclone will print stats like this after each run, or at regular intervals with the `--stats` option.   These will scale up to TB too!

```
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:                 0
Transferred:            0
Elapsed time:          0s
```

is that what you need?  rclone doesn't accumulate stats over different runs though.
 I mean just if i do for example `rclone --stats d` it will print stats for whole day not stats when running (sync or move or copy etc...)

Probably output like this:
`Transferred: 3.25 TB | 
Files: 512000`

Thx :)
  Try the latest beta - I've improved this considerably!

http://beta.rclone.org/v1.33-85-g6846a1c/
  I suspect this is (at least partly) #I suspect this is (at least partly) #824 - fuse mounted acd can't write at the moment
 Glad we've got to the bottom of this.  I'll close this in favour of #824 
  Make a new rclone command which will remove empty directories - based on http://pub.rclone.org/rclone-delete-empty-dirs.py3

As it stands this isn't very efficient as it makes a new remote for each directory deleting.

Examine whether to change the Rmdir/Mkdir calls to take a directory parameter.

Factored from #100
 Beta here: http://beta.rclone.org/v1.34-25-gf3365dd/  To answer your initial question

> rclone move acde:atestfile acde:hrm
> 
> A new folder called hrm is created and atestfile is put in there, instead of just renaming the file to hrm.
> Is that intended? Seems quite odd.

That is intended behaviour.  I agree it is a bit odd!  I'm going to make new rclone commands `cp` and `mv` which will work more like people expect. See #227

@toomuchio thanks for your PR - I'm a bit behind with rclone stuff, but I'll get to it soon!

I'm going to close this issue as I think the remaining things are represented well elsewhere.
  Checksum isn't supported on crypt. I think I updated the docs with that in recently (so it isn't on the website yet). 

It will be when I do the high security name encryption mode. 
  Exactly how big is file1 in your log above?

@Coornail 

> As of reproducing it, I'm not entirely sure.
> 
> I would upload a moderate sized file (100 megabytes?) to an encrypted amazon cloud folder and try to seek to different positions in it.

I uploaded a 2 GB file to acd+crypt and ran a seek testing program I wrote which did a random seek, read a random amount of data and compared it against a local copy of the file.  That didn't find any errors 200 times in a row :-(

Can send a complete unmodified log please?  Email it to me at nick@craig-wood.com if you don't want it to be public.

Thanks

Nick
 Thanks for the log.

Can you send one with `--debug-fuse` too please?

> Edit: I'm not sure, but it's possible that multiple processes opened the file to read. Could that be an issue?

It might be something like that.  How do the multiple processes open the file? Is one forked from the other?
 @Coornail thanks for the log - I have spent some time studying that but haven't come to a conclusion yet.

Can you send me a log with `-v --debug-fuse --dump-headers` in please?  This will be quite big and have `Authorization:` headers in you should strip out before sending eg `grep -v Authorization: <dirty-log >clean-log`.

I've also posted a version of rclone with the race detector built in.  rclone runs a bit slower like that, but it will produce a traceback if it detects any races which would indicate more missing locking.  If you can run that with `-v --debug-fuse` and send me the log I would be grateful (the race detector will write to stderr I think so make sure you capture that).

http://pub.rclone.org/rclone-v1.33-85-g6846a1c-linux-amd64-race.gz

Thanks for your help and hopefully we'll get to the bottom of this shortly!
 Thanks for the log.  It looks like there are no races which is good :-)

What appears to be happening is every now and again rclone does a `Range:` request, but acd replies with a normal `200 OK` instead of a `206 Partial Content`

```
2016/10/28 20:51:04 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/10/28 20:51:04 HTTP REQUEST
2016/10/28 20:51:04 GET /cdproxy/nodes/XXXX/content HTTP/1.1
Host: content-na.drive.amazonaws.com
User-Agent: rclone/v1.33-DEV
Range: bytes=786165168-

2016/10/28 20:51:04 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
```

and the response was

```
2016/10/28 20:51:06 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/10/28 20:51:06 HTTP RESPONSE
2016/10/28 20:51:06 HTTP/1.1 200 OK
Content-Length: 359274160
Cache-Control: public, max-age=7200
Connection: keep-alive
Content-Disposition: attachment;    filename=XXXXX
Content-Type: application/octet-stream
Date: Fri, 28 Oct 2016 20:51:06 GMT
Etag: "3aa5a31aeeca48405be931b18aa754dd-69"
Server: Amazon-Cloud-Drive
X-Amzn-Requestid: 550bfe99-860d-4794-b650-44b0fa256b1c

2016/10/28 20:51:06 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
```

So reading from the start of the file not seeked like rclone asked for.

I've put in a work-around for this in this beta.

http://pub.rclone.org/v1.33-92-g881740e-acd-range-fix%CE%B2/

Ideally what I'd like to see is `Error expecting HTTP response 206 but got 200 - retrying` in the logs and it all works properly :-)

Have a go and see what happens - I suggest recording the full log with `-v --debug-fuse --dump-headers` and sending it to me.

Cross fingers!
 With the hints in your logs I managed to reproduce the problem with a program which did lots of simultaneous seeking.  I managed to get the race detector to fire using go tip and it showed me where the problem was.  Here is the commit message.

```
crypt: Fix data corruption caused by seeking

The corruption was caused when the file was read to the end thus
setting io.EOF and returning the buffers to the pool.  Seek reset the
EOF and carried on using the buffers that had been returned to the
pool thus causing corruption when other goroutines claimed the buffers
simultaneously.

Fix by resetting the buffer pointers to nil when released and claiming
new ones when seek resets EOF.  Also added locking for Read and Seek
which shouldn't be run concurrently.
```

I've committed this to a branch and made a beta from it - can you have a go with it please?

http://pub.rclone.org/v1.33-92-g4bcea27-seek-race-fix%CE%B2/

If it works for you too I'll merge it to master.

This turned out to be a complicated problem to debug - that is the danger with highly concurrent programs like rclone, but once I could reproduce it locally I knew I'd crack it!

The ACD 200 vs 206 fix was a complete red-herring, alas caused by my mis-reading of the logs!  I made a change to help with this in the future 5c967810ed15def4f5235cef7d58f26516f8bc5d

Thank you for your help debugging this, and I really hope this fixes it for you!

PS You were spot on with your original comment: `I believe the issue is related to the frequent seeking in the file, possibly a missing lock.` :-)
 OK I've merged the above in: http://beta.rclone.org/v1.33-101-g154e91b/ (ready in 15-30 mins)

I didn't see the logs yet so look forward to seeing them :-) 
 Thanks for the logs.

I see the problem - looks straight forward for once!

rclone attempted to seek 20 bytes beyond the end of the file to 371155456 however the file was only 371155436 bytes long.  Amazon responsed with 200 OK and the whole file from the beginning (which is technically an error - it should have given an error here).

The decrypted file is 371064828 bytes long

FUSE asked rclone to seek to position 371068928 in the decrypted stream which is 4100 bytes beyond the end of the stream.

This is a legit thing for fuse/plex to do - rclone needs to check it isn't being asked to seek beyond the end of a file.  I'll have to lookup the exact behaviour required though.

I think that is the problem - I'll double check in the morning!

I should double check that rclone is passing the correct sizes of everything about too.

The length of the decrypted file is 4 bytes short of a major hex boundary which may be significant...
 I've fixed this now in: http://beta.rclone.org/v1.33-105-gd1080d5/ (uploaded in 15-30 mins) in changeset 64b5a76bec1a49cf74355f8bc939988649424b91

Can you give this a test please and if it still isn't working re-open the ticket!

Thanks for all your testing
  At the moment when seeking a file > 10 GB we request the templink each time which is less efficient than it could be.  The templinks live for a certain length of time so find out how long and cache them for that long.
 @jkaberg thank you for digging that up - very useful :-) Don't know why they don't link to that in the API docs where you actually get a templink!
  Interesting idea - not sure how I'd implement it.

It is the kind of thing I might expect the go name resolver to do for me...

A [little experiment](https://play.golang.org/p/j3XWDSzO6G) indicates that it returns IPs in random order which is good...

```
$ ./LookupIP content-na.drive.amazonaws.com
"content-na.drive.amazonaws.com": 54.88.149.237
"content-na.drive.amazonaws.com": 54.85.147.124
"content-na.drive.amazonaws.com": 54.87.96.13
"content-na.drive.amazonaws.com": 54.84.119.41
"content-na.drive.amazonaws.com": 54.210.228.88
"content-na.drive.amazonaws.com": 54.210.230.197
"content-na.drive.amazonaws.com": 54.210.230.84
"content-na.drive.amazonaws.com": 54.84.77.98
$ ./LookupIP content-na.drive.amazonaws.com
"content-na.drive.amazonaws.com": 54.85.147.124
"content-na.drive.amazonaws.com": 54.87.96.13
"content-na.drive.amazonaws.com": 54.84.119.41
"content-na.drive.amazonaws.com": 54.210.228.88
"content-na.drive.amazonaws.com": 54.210.230.197
"content-na.drive.amazonaws.com": 54.210.230.84
"content-na.drive.amazonaws.com": 54.84.77.98
"content-na.drive.amazonaws.com": 54.88.149.237
$ ./LookupIP content-na.drive.amazonaws.com
"content-na.drive.amazonaws.com": 54.87.96.13
"content-na.drive.amazonaws.com": 54.84.119.41
"content-na.drive.amazonaws.com": 54.210.228.88
"content-na.drive.amazonaws.com": 54.210.230.197
"content-na.drive.amazonaws.com": 54.210.230.84
"content-na.drive.amazonaws.com": 54.84.77.98
"content-na.drive.amazonaws.com": 54.88.149.237
"content-na.drive.amazonaws.com": 54.85.147.124
```

Another [little test](https://play.golang.org/p/91imBybSI7) indicates that Dial chooses a different IP each time

```
$ ./dial content-na.drive.amazonaws.com:443
"content-na.drive.amazonaws.com:443": 52.73.248.252:443: <nil>
"content-na.drive.amazonaws.com:443": 52.72.145.30:443: <nil>
"content-na.drive.amazonaws.com:443": 52.73.155.106:443: <nil>
"content-na.drive.amazonaws.com:443": 52.72.168.2:443: <nil>
```

So I would kind of expect rclone to work like that already.

It might be worth trying this before running rclone.

```
export GODEBUG=netdns=go    # force pure Go resolver
```

Also check with `netstat -tnp` to see which addresses rclone does connect to.

When I try it

```
$ netstat -tnp | grep rclone
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 192.168.1.149:47218     54.76.70.53:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53986     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:56176     54.72.49.85:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53964     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:56172     54.72.49.85:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:37940     205.251.243.243:443     ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53992     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:38328     54.77.75.54:443         ESTABLISHED 8824/rclone     
tcp        0    709 192.168.1.149:38332     54.77.75.54:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53996     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:54000     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:47216     54.76.70.53:443         ESTABLISHED 8824/rclone     
tcp        0    709 192.168.1.149:38334     54.77.75.54:443         ESTABLISHED 8824/rclone     
tcp        0    709 192.168.1.149:56174     54.72.49.85:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:38326     54.77.75.54:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53988     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53990     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:56178     54.72.49.85:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:56180     54.72.49.85:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:38330     54.77.75.54:443         ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53998     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:54002     54.239.34.70:443        ESTABLISHED 8824/rclone     
tcp        0      0 192.168.1.149:53994     54.239.34.70:443        ESTABLISHED 8824/rclone     
```

I see that rclone does indeed connect to lots of different IPs
 I think rclone is already doing random IPs with `export GODEBUG=netdns=go` so I'm going to close this as it would be very difficult for me to change the built in behaviour.
  I see what has happened.  You started your upload at `2016/10/25 20:49:40` and got the first `401 expired_auth_token` message at `2016/10/26 20:50:30` so almost exactly 24 hours later.

According to [the b2 api docs](https://www.backblaze.com/b2/docs/b2_authorize_account.html)

> An authorization token to use with all calls, other than b2_authorize_account, that need an Authorization header. This authorization token is valid for at most 24 hours.

So rclone either needs to get a new authorization token when it gets an  `401 expired_auth_token` error, or it needs to fetch one at regular intervals.
 restart the script would be a work-around yes.

I'll fix soon and post a beta for you to try here.
 I've attempted to fix this in this beta: http://beta.rclone.org/v1.34-04-g4105da2/ (will be uploaded in 15-30 mins)

Can you have a go with it and see if it fixes this issue for you please?

Thanks

Nick
 OK, thanks for the report.  Will try another fix attempt and post it here in due course! I managed to replicate this with a test flag `--b2-test-mode expire_some_account_authorization_tokens` and I've fixed the problem.

Please find it in this beta

http://beta.rclone.org/v1.35-36-ga4bf22e/ (uploaded in 15-30 mins)

Let me know how you get on.

Thanks

Nick Good news! Thanks for testing.  @marrie Thanks for testing too. Cross fingers! @bshiner wrote
> Sync running for the last 125 hours.

Excellent!

I'll close this issue now - thanks everyone for testing.  It will be in the official 1.36 release.
  Thanks for making a ticket about this.  I agree it needs to be done, but it might not be easy!  The two approaches are
- play with the uploading code for acd / b2 to see if it can work without knowing the length in advance - probably won't work
- cache the upload on disk first (as in #711) - will be more reliable but has several disadvantages
 @breunigs just wondering when you said 

> no seeking write support in AmazonDrive

 ... can't `Content-Range` be used to provide both random file read/write access ? So in same way that `rclone upload` already allows resume... can't that be retro fitted with magic to allow overwrite feature ? 



  How exactly did you cause the crash?  Were you playing a video?  If so with which player?

Can you reliably cause this crash?  If so can you post the whole log please?

Note to self: the crash was caused by the reader being nil in crypt.Decrypter.Read. How did it get to be nil is the question?  Perhaps a race between Read and Seek.
 Thanks for the logs.

Examining them closely I can see that reads are being issued from mutiple threads simultaneously.  Which lead me to a bit of missing locking in the FUSE module.

Here is a beta with the fix - let me know if it works: http://beta.rclone.org/v1.33-83-g8710741/ (will be uploaded in 15-30 mins)
 @toomuchio excellent news - thanks for testing
 @calisro does that fix your issue too?
 Excellent - I'm going to close this now
  You can use the `--min-age` and `--max-age` flags from [the filtering](http://rclone.org/filtering/) commands.  These filter on modified date.

Is that what you want, or do you want to filter on accessed date?
 I'm assuming that was what you wanted - if not, please reopen!  Thank you very much for that - much appreciated.

I've merged that in 6fd5ef2d99e6528e911e45585b850e2bf0dd17a4
  Amazon Drive had the same problem which was fixed in 2ebeed67531263294542834ea8edc7bb14334b09

I can add the same fix to onedrive.

I should probably factor the logic out and stick it in oauthutil then use it in all the oauth based remotes...
 I've attempted to fix this here.  Please have a test and re-open the ticket if it doesn't work!

Thanks Nick

http://beta.rclone.org/v1.35-52-g7470255/ (uploaded in 15-30 mins) OK I'll look at this some more... @yonjah 's change has been merged now which should fix this issue - find it here:

https://beta.rclone.org/v1.36-13-gd1787b5/ (uploaded in 15-30 mins)

Thank you very much @yonjah for fixing this.  Ultimately the cloud storage systems all support some variant of [http range](https://tools.ietf.org/html/rfc7233).  They don't all support all the features though if I remember rightly.  A sort of minimum subset would be to return a section of a file.

I could see this translating into flags for rclone

```
--head N      - fetch the first N bytes of the file
--tail N      - fetch the last N bytes of the file
--range N-N   - fetch from bytes N to M in the file
```

Which would all be mutually exclusive.

I think all the cloud storage systems could support that.

That isn't exactly what you asked for - is it useful enough?
 The easiest way to write this would be to make the flags just apply to the `rclone cat` command.

Here is a quick runthrough of what you would need to do

You would put the flag handling in [cat.go](https://github.com/ncw/rclone/blob/master/cmd/cat/cat.go) see [mount.go](https://github.com/ncw/rclone/blob/master/cmd/mount/mount.go) as to how to add flags to a specific command.

You'd then decode the flags into a [RangeOption](https://github.com/ncw/rclone/blob/master/fs/options.go#L24) which you would pass to `fs.Cat` as an extra parameter, and you'd change the implementation of `fs.Cat` to take that extra parameter and pass it to the [`o.Open()` call](https://github.com/ncw/rclone/blob/master/fs/operations.go#L1053)

You'd then write some docs and a test in `fs/operations_test.go` for the extra parameter for Cat.
 tail would be particularly useful for detecting corruptions introduced by #902 in #999 so I'm going to try to get `rclone head` and `rclone tail` into 1.36. Yes it would work on crypt too. I don't think range is too hard either.  I've implemented this in the following beta

http://beta.rclone.org/v1.35-76-gd091d4a/ (uploaded in 15-30 mins)

It should work for all remote types.

```
You can use it like this to output a single file

    rclone cat remote:path/to/file

Or like this to output any file in dir or subdirectories.

    rclone cat remote:path/to/dir

Or like this to output any .txt files in dir or subdirectories.

    rclone --include "*.txt" cat remote:path/to/dir

Use the --head flag to print characters only at the start, --tail for
the end and --offset and --count to print a section in the middle.
Note that if offset is negative it will count from the end, so
--offset -1 --count 1 is equivalent to --tail 1.

Usage:
  rclone cat remote:path [flags]

Flags:
      --count int    Only print N characters. (default -1)
      --discard      Discard the output instead of printing.
      --head int     Only print the first N characters.
      --offset int   Start printing at offset N (or from end if -ve).
      --tail int     Only print the last N characters.

```  This might need a new concept in the remote to fix properly - a Chroot call...
  No there isn't any docs on the config file.  There is a very straight forward mapping from the auto configurator to what goes in the config file so it isn't too hard to figure out, but you are right there aren't any docs on it.

Fancy writing some?
 @AzureBlaze @qinwf I keep meaning to do that, so I've made an issue about it #850 - please click the subscribe button to be notified of changes there!
 Note that you can now (if you use the latest beta) set an environment variable "RCLONE_DRIVE_USE_TRASH=true` which is a step in the right direction. @AzureBlaze I beleive it should work.  How are you setting the environment variable?

You can see the difference in the help

before

```
$ rclone -h | grep trash
      --drive-use-trash                   Send files to the trash instead of deleting permanently.
```

after

```
$ RCLONE_DRIVE_USE_TRASH=true rclone -h | grep trash
      --drive-use-trash                   Send files to the trash instead of deleting permanently. (default true)
$ 
```

I tried this on Windows with

```
set RCLONE_DRIVE_USE_TRASH=true
```

And it worked for me. @AzureBlaze it is scheduled for the 1.36 release so only in beta at the moment.  Glad you made it work.  It could be nginx usage patterns I guess...

If you run the mount with `-v` and take a look at the output what do you see.  Maybe you could post some logs here?

I have sped up the seeking for crypt+fuse which you can try here: http://beta.rclone.org/v1.33-79-g77b975d/ which might help.
  I think I just fixed that bug in #813 - try this beta: http://beta.rclone.org/v1.33-79-g77b975d/

(I see you compiled from source - in that case do a `go get -u -v github.com/ncw/rclone`)
 This was fixed in #813 so I'm going to close this one too.
 Great! 
  I think this is an alignment of a 64 bit variable problem. From sync.Atomic docs

> On both ARM and x86-32, it is the caller's responsibility to arrange for 64-bit alignment of 64-bit words accessed atomically. The first word in a global variable or in an allocated struct or slice can be relied upon to be 64-bit aligned.

I've attempted to fix this in this beta

http://beta.rclone.org/v1.33-78-gc464cc6/ (will be uploaded in 15-30 mins)

Please test and report back!

Thanks

Nick
 Thanks for testing - glad that worked!

The next log is a different problem!  The problem being is that uploads don't work for amazon drive via `rclone mount` :-(

This is because Amazon need to know the size of the file in advance for the upload, but we don't know it until the end.  This could be fixed by buffering the file to disk for uploads (#711).

I've put a note to that effect in the docs 77b975d16fe2f52cb55302c69a6b1618a000d152
 @kraiz yes you are right. Someone made an issue for this at #/824 so click the subscribe button there to receive notifications.

I'm going to close this now as the main issue is fixed.
  Were there any other logs earlier in the log with low-level retries in?  Could you upload the complete log file maybe?

rclone should always retry a 503 error so what I suspect happened is that a previous chunk timed out and it had to wait for the intermediate chunks in order to show you that message, so the actual error is further back in the log.
 Something looks broken there!  `low level retry 1/1` is wrong it should say `low level retry 1/10`\- I can see you haven't set `--low-level-retries` though though...

Can you try again with the latest beta please?

http://beta.rclone.org/v1.33-79-g77b975d/
 @marrie Your logs show the same issue as #825

@8BitAce the `x509: certificate signed by unknown authority` can only really be a backblaze problem if it got better

I'm going to close this now as I think we've dealt with the issues (or they are in other issues)
 I can't control the `503 service_unavailable` - you'll get it when backblaze is too busy.  rclone clould cope with that by slowing down and retrying and upload everything eventually.  If it doesn't then you can run it again.
  You need this in your `/etc/fuse.conf` in order to use `--allow-other`

```
# Allow non-root users to specify the allow_other or allow_root mount options.
user_allow_other
```

Not sure that is the problem though - the segmentation fault looks strange.

Does it work without the `--allow-other`?
 This area of code has been moving quite quickly in recent days!  I've fixed two major bugs one of which may be yours.  Can you try the latest beta: http://beta.rclone.org/v1.33-85-g6846a1c/ and see if that fixes the segmentation fault?
 This error `Fatal error: failed to mount FUSE fs: fusermount: signal: segmentation fault` means that rclone tried to do a FUSE mount and the kernel didn't understand and blew up.

I think there probably isn't anything rclone can do about this.

I don't know whether you can upgrade the kernel in the synology, or check out other people using FUSE mounts on it?
 I'm going to close this as I haven't heard back from you so I hope it is all working now!  Thanks for thinking about this and apologies for not replying sooner.  Changing the bwlimit dynamically is a popular feature request for rclone.

Signals as a method for controlling it are problematic though as they don't work on Windows (& Plan 9) as the failed appveyor build shows.

```
fs\accounting.go:47: undefined: syscall.SIGUSR2
```

Now this could be fixed with a few [build tags](https://golang.org/pkg/go/build/#hdr-Build_Constraints) (nice [blog about conditional compilation](http://dave.cheney.net/2013/10/12/how-to-use-conditional-compilation-with-the-go-build-tool))

However what would you think about implementing #221 instead which would vary bwlimit by time of day?  that would fix the problem for most people and be cross platform portable? Would that satisfy your use case?
 > Aha! That's why appveyor was failing then! I was curious about that. Looking at the logs, it didn't appear to be anything related to my changes, and I was puzzled.

It originally blew up with something random like a timeout fetching something, but I re-ran the build today so your confusion is understandable!

> What about adding conditional compilation and time of day for this feature? I could try fixing this issue and implementing #221.

If you want to do that, then that would be fine.  I suggest you make this PR work for the conditional compilation needed then do a separate one for #221

> I think it's also important to have a "tactical" way to turn bwlimit on and off. Would you have any suggestions on portable ways to do this? Being a Unix/Linux guy, I don't know what methods are available to Windows (signals are clearly a problem). Any ideas?

I have to admit to not really being a windows dev either though I have done some windows dev stuff in the past (with win32 api!).

Doing IPC opens a whole can of worms in the general case...  Probably the easiest way would be for rclone to listen on a localhost:socket for commands if you supply a `--rpc host:port` parameter then make a separate rclone command `rclone rpc bwlimit 10k` to interact with the running rclone.  There are some security implications doing it like that, so a named pipe might be better.

I suggest you just ignore windows for the time being - I want to solve the IPC thing in a general way for tickets like #771 and some other ideas I've got.
 I've merged that in cc4f5ba7ba5bbebb31ad4f2f3c8d28e13613cc3f (after rebase).

After a bit of testing I realised that the build was broken for some builds, and the race detector warned me that the update of the token bucket was racey so I fixed those things in e65059e431642d497bc0b134270cd3032d87f790

Thank you very much for your contribution :-)
 @DurvalMenezes 

> Plan 9?! Wow, just noticed that rclone builds are routinely available for it. Say (if it's not too OOT), is Plan 9 ready for anything but a lab setting? Just curious here, been following it for a long time but never thought I would see it become usable.

Some of the core go developers use plan 9 or at least hack on it.  No idea how well it works though!  Go makes doing cross compilations really easy so I did it for plan 9 just because it was easy!

> But anyway, I would like to make a suggestion: Windows (since the Win16 days) has supported an IPC mechanism called "named pipes"

There are some ideas along the lines you suggested in #771

> Also, this would open up the door for more sophisticated "run-time behavior changing" for rclone; for example, I frequently need to restart rclone to change --transfers and --checkers

That is a nice idea... Fancy making an issue about it?
  The problem I suspect is that ACD doesn't support renames: https://github.com/ncw/rclone/issues/122

I don't recommend you upload files with the mount - use rclone copy for that as it will be a lot more reliable.
 I'm going to close this as #835 covers the rename issue.
  This will fix that

```
nohup rclone mount encryptedremote: /path/to/mount --allow-other </dev/null >/dev/null 2>&1 &
```

rclone mount doesn't daemonise itself at the moment.  It probably should, or at least have an option to do so.

In [this thread on the forum](https://forum.rclone.org/t/mounting-acd-with-decryption-for-reading/64/11) Buba_metola posted a recipe for using rclone mount with systemd.

You could also run it in a screen which is cheap and easy!
  Rclone tries very hard to maximise bandwidth usage. However this can cause problems which can be alleviated with the --bwlimit flag. 

That said 14 kBytes/s isn't exactly fast so it looks like there is something else going on. 

Where do you see the packet loss with mtr - is it at your router or further up the network?

Try scp some files to and from the pi to your laptop to see what sort of speed you get.

My guess would be something up with your local network. You could try rebooting your router and see if that helps. If it doesn't give the pi the same treatment. 

It would also be worth taking a look at dmesg to see if there is anything in there from the network interface. 

Hope something in the above is useful! 

Nick 
 > I didn't do a file transfer to my laptop (also on the same router and only 2 hops, but considering that the SSH typing suddenly becomes 'far' better once rclone is killed, I am not hoping much here either.

I meant try the bit file transfer without rclone running and see if you get the same effect.
 OK - glad it is working!
  I haven't deliberately done anything to that issue. It does seem to have disappeared though :-(

I'll ask github support about it. 

I'll see if I can dig up the problem and post it here in a moment. 
 Here is a reconstruction of what the problem is...

First upload a file with a UTF-8 character to a bucket

```
$ rclone -q ls dreamhost:utf8-test
        6 complex character £100.txt
```

Now try a server side copy

```
$ rclone copy dreamhost:utf8-test dreamhost:utf8-test2 --retries 1
2016/10/19 22:55:29 S3 bucket utf8-test2: Waiting for checks to finish
2016/10/19 22:55:29 S3 bucket utf8-test2: Waiting for transfers to finish
2016/10/19 22:55:29 complex character £100.txt: Failed to copy: NoSuchKey: 
    status code: 404, request id: tx000000000000003b63946-005807ebd1-10bb00c9-default
2016/10/19 22:55:29 Attempt 1/1 failed with 1 errors and: NoSuchKey: 
    status code: 404, request id: tx000000000000003b63946-005807ebd1-10bb00c9-default
2016/10/19 22:55:29 Failed to copy: NoSuchKey: 
    status code: 404, request id: tx000000000000003b63946-005807ebd1-10bb00c9-default
```

Now with http debug

```
$ rclone copy -v --dump-bodies dreamhost:utf8-test dreamhost:utf8-test2 --retries 1
```

The relevant transaction is

```
2016/10/19 22:56:40 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/10/19 22:56:40 HTTP REQUEST
2016/10/19 22:56:40 PUT /utf8-test2/complex%20character%20%C2%A3100.txt HTTP/1.1
Host: objects-us-west-1.dream.io
User-Agent: rclone/v1.33-DEV
Content-Length: 0
Authorization: SNIP
Date: Wed, 19 Oct 2016 21:56:40 UTC
X-Amz-Copy-Source: utf8-test%2Fcomplex+character+%C2%A3100.txt
X-Amz-Metadata-Directive: COPY
Accept-Encoding: gzip

2016/10/19 22:56:40 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/10/19 22:56:40 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/10/19 22:56:40 HTTP RESPONSE
2016/10/19 22:56:40 HTTP/1.1 404 Not Found
Content-Length: 225
Accept-Ranges: bytes
Content-Type: application/xml
Date: Wed, 19 Oct 2016 21:56:40 GMT
X-Amz-Request-Id: tx000000000000003b51cb0-005807ec18-10b45ddf-default

<?xml version="1.0" encoding="UTF-8"?><Error><Code>NoSuchKey</Code><BucketName>utf8-test2</BucketName><RequestId>tx000000000000003b51cb0-005807ec18-10b45ddf-default</RequestId><HostId>10b45ddf-default-default</HostId></Error>
2016/10/19 22:56:40 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
```

However if you try the exact same thing against S3 it works just fine.

```
2016/10/19 23:03:01 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/10/19 23:03:01 HTTP REQUEST
2016/10/19 23:03:01 PUT http://s3.amazonaws.com/utf8-test2/complex%20character%20%C2%A3100.txt HTTP/1.1
Host: s3.amazonaws.com
User-Agent: rclone/v1.33-DEV
Content-Length: 0
Authorization: snip
X-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
X-Amz-Copy-Source: utf8-test%2Fcomplex+character+%C2%A3100.txt
X-Amz-Date: 20161019T220301Z
X-Amz-Metadata-Directive: COPY
Accept-Encoding: gzip

2016/10/19 23:03:01 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/10/19 23:03:01 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/10/19 23:03:01 HTTP RESPONSE
2016/10/19 23:03:01 HTTP/1.1 200 OK
Content-Length: 234
Content-Type: application/xml
Date: Wed, 19 Oct 2016 22:03:02 GMT
Server: AmazonS3
X-Amz-Id-2: SAkNeGyzpGulA++DE6QqVHAaHqzn2tpwVyjqBa2rL/jO3ZigeG3jGihiLLkEXQ816g0MVYIjwqc=
X-Amz-Request-Id: 0D8ECC97657B88F4

<?xml version="1.0" encoding="UTF-8"?>
<CopyObjectResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><LastModified>2016-10-19T22:03:02.000Z</LastModified><ETag>&quot;b1946ac92492d2347c6235b4d2611184&quot;</ETag></CopyObjectResult>
2016/10/19 23:03:01 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
```

I think the problem is the `X-Amz-Copy-Source` which according to the [AWS copy docs](http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) is supposed to be URL encoded, however I conjecture that CEPH isn't decoding it as such - hence the key not found error.

> x-amz-copy-source
> 
> The name of the source bucket and key name of the source object, separated by a slash (/).
> 
> Type: String
> 
> Default: None
> 
> Constraints:
> 
> This string must be URL-encoded. Additionally, the source bucket must be valid and you must have READ access to the valid source object.
> 
> If the source object is archived in Amazon Glacier (storage class of the object is GLACIER), you must first restore a temporary copy using the POST Object restore. Otherwise, Amazon S3 returns the 403 ObjectNotInActiveTierError error response.
 Just noticed that #586 is still in the milestone: https://github.com/ncw/rclone/milestone/18 so looks like some sort of data corruption at github :-(
 Issue #586 is back.  Github support restored it for me.  They didn't explain what happened or why it went away though!
 I'm going to close this now #586 is back
  I'm going to close this issue now as we've got to the bottom of it!
  rclone could listen on a signal to do this quite easily so you would `killall -SIGHUP rclone` maybe.
 I've put it on the list for 1.37 @chmercesmoreira nice idea, but not straight forward to implement as it is two different instance of rclone running so one would have to find the other which is non-trivial... @breunigs it seems to me the best approach might be to send a PR to bazil/fuse/fs with the extra functionality rclone needs.  Perhaps a flush this path method and return nodes flushed? How can I test this?  Is there a small doc/comment on how to use it?  I'm happy to test it.

Ah. Found it. :)

## Directory Cache ###
 +
 +Using the ` + "`--dir-cache-time`" + ` flag, you can set how long a
 +directory should be considered up to date and not refreshed from the
 +backend. Changes made locally in the mount may appear immediately or
 +invalidate the cache. However, changes done on the remote will only
 +be picked up once the cache expires.
 +
 +Alternatively, you can send a ` + "`SIGHUP`" + ` signal to rclone for
 +it to flush all directory caches, regardless of how old they are.
 +Assuming only one rlcone instance is running, you can reset the cache
 +like this:
 +
 +    kill -SIGHUP $(pidof rclone)
 +

 btw, you have a mis-spelling  "+Assuming only one **rlcone** instance is running" Have fixed the typo thanks @breunigs Reading above, it looks like we can do partial flushes of specific paths?  How would we invoke that or is that part not complete?  Here is an experimental  fix for that error.  It will produce a different error instead of the spurious `failed to authenticate decrypted block` - something to do with the connection breaking would be my guess.

http://pub.rclone.org/v1.33-81-gef17f90-readfull-fix%CE%B2/

I'm on the trail of the underlying error, but I need to confirm I'm going in the right direction, so any testing appreciated.
 @jkaberg useful log thanks. I see you are using seeks which lead me into finding a data corruption problem with crypt+seek which I've now fixed.  Have a go with http://beta.rclone.org/v1.33-81-g9d2dd2c/ (will be uploaded in 15-30 minutes) and tell me if that makes a difference.
 After looking at your logs, I have an idea... Are the files which don't play all bigger than 10 GB?  Can you try playing a file < 10GB (say 5GB) and compare with one > 10GB?

(10GB is the templink threshold which means these files are being fetched a different way - maybe that way doesn't support Range requests or something like that.  I can't test myself as my Internet connection has gone to pieces :-( )
 Great, looks like we are getting somewhere :-)

What I need now is someone to run `rclone mount` with `-v` and `--dump-headers` and try to play a bigger than 10 GB video and post those logs.  **Important** Make sure you remove the `Authorization:` lines from the log with eg `grep -v Authorization: <dirty-log >clean-log` before posting.
 @calisro The errors I see in your log all look like `read tcp 10.100.1.195:59112->54.165.17.201:443: use of closed network connection`

Can you open a new issue with that log please, and also put a description of your setup with nginx - if I can easily reproduce the problem locally it will be much easier for me to fix.
 @jkaberg I can see what is happening there is that rclone tries to download the file with a `Range:` header using the templink.

This gets redirected to download directly via S3 as it is bigger than 10GB, however rclone doesn't put the `Range:` header back on at that point so it downloads from the beginning which isn't what the crypto expects, hence the bad password error.

Luckily I've already written code to do that in `rest.clientWithHeaderReset` so it will be a question of gluing A onto B and it should work.

I'll post a beta in a bit for you to try...
 Here is a beta with that fix: http://beta.rclone.org/v1.33-82-g5986953/ (will be uploaded in 15-30 mins)
 Excellent :-) 
 @calisro would probably be a good idea to confirm that the bactrace you see from `rclone mount` is the same as the on in that ticket.
 I found a bit of missing locking in the FUSE module which can also produce this error message. Here is a beta with the fix - let me know if it works: http://beta.rclone.org/v1.33-83-g8710741/ (will be uploaded in 15-30 mins)
 @Coornail probably the best thing to do is to try to work out what is causing the issue in such a way I could reproduce it, then put that into a new issue, with a full log with -v from the latest beta.

Thanks 

Nick 
 I'm going to close this one now as this issue is fixed.
  Is that the equivalent of S3?  Can you link the API docs?
 The Azure SDK looks complete and supported:  https://godoc.org/github.com/Azure/azure-sdk-for-go/storage

Anyone fancy having a go at this?  Try latest beta maybe, also try adding quotes into `rclone sync "L'amour c'est mieux à deux"`
 What file system are you reading the files off? HFS? or other?

There have been several problems with OS X and non ascii characters in the past - OS X stores its files in a very strange encoding on disk!

Does the file name look correct if you do

```
rclone ls acd:path
```

or is it just the warning message which is wrong?
 Ah that makes sense now. I fixed that same bug in rclone #623

There is more explanation on that ticket, but it is due to the way HFS stores its file names. 
 Yes they should normalise the UTF8 before uploading. Some cloud storage systems like Google drive do that for you which was causing a similar problem to yours for Mac users. The best solution is to always normalise the filrnames before uploading - that is what rclone does anyway! 
  I made it myself for ACD, and working great! it is auto-unmount and auto-mount every 5min. via cron. All you need is setup your paths ...

https://github.com/ncw/rclone/issues/790#issuecomment-253769752
 I think this is a duplicate of #680 which I'm planning to fix soon! However @scriptzteam solution may work for you in the mean time.
 @jope2000 thanks.  If you click the subscribe button on the other issue, you'll get a notification when it is updated (eg when I post a beta with a fix).
 > that might be an option if you are just dealing with small files.
> However, what if you are in the middle of accessing a larger file and your cron re-mounts the drive?

use flashfxp or some ftp client when accessing mounted folder, ftp protocol support resume of failed download ;) so you will start downloading where you end, so not the whole file again ;)
  That message is a generic this command failed message produced here.  So when `rclone XXX` with error YYY, it prints `Failed to XXX: YYY`

https://github.com/ncw/rclone/blob/master/cmd/cmd.go#L188

The other part of the error is made here

https://github.com/ncw/rclone/blob/master/fs/operations.go#L564
 check needs to return an error so rclone knows to return an exit code.

It needn't be that error though...

Interestingly this is also being discussed on the forum: https://forum.rclone.org/t/pipe-into-text-or-grep-not-working/58/5

Where the suggestion is that rclone should be writing the output of check to stdout.  In that case then I should probably change the error returned to be a bit more generic.  Though in your diff it is an error so probably go to stderr (and hence ErrorLog).
 @ebridges the Check function returns an error either if something went wrong, or a difference was found.  The final nil is if everything was OK and no differences were found.

I'd be happy to receive a rewording of the error message if you can come up with something :-)
 @ebridges the point of the `rclone check` command is to ask the question - are all my files uploaded correctly.  The unix convention in this case is to return an error if everything isn't OK.  This means you can script `rclone check` command and do something if there is something wrong.  For instance the unix `grep` command works in a similar way - if it finds something it doesn't return and error, but if it doesn't is does return an error even though the grep command ran correctly.
  This is a duplicate of #97 - can you subscribe to that issue please to show interest?

Thanks

Nick
  I don't think the bandwidth limiting is working at all with rclone mount!

Do you agree - what if you try a really small `--bwlimit` do you get the same speed?
 Here is a test version of rclone where bwlimit should work with rclone mount

http://pub.rclone.org/v1.34-39-g7be5724-fuse-buffer%CE%B2/ I've merged this to master now: http://beta.rclone.org/v1.34-48-gc24da0b/ (will be uploaded in 15-30 mins)  I'm going to fix this in #227

In the mean time you can use the steps that durval made in the forum to find the encrypted name then do the rename in the web interface.

https://forum.rclone.org/t/how-to-efficiently-move-dir1-dir1-to-dir1-amazon-cloud-drive-encrypted/54/4
 Some time in the future yes!
 @pedrosimao you should find both of those things are fixed in the latest beta.  In particular server side moves are now supported on acd, and there is the `moveto` command which can be used to rename stuff. @pedrosimao I can confirm server side directory moves on acd+crypt seem to be broken...

Will fix and post a beta for you to try @pedrosimao try this http://beta.rclone.org/v1.34-60-g2656a0e/ (uploaded in 15-30 mins)

that should fix it hopefully! Hopefully that is all fixed!  Please re-open if not.  Good idea.
  I would say you shouldn't be seeing the `corrupted on transfer: sizes differ 6036 vs 17556` messages at all. It might be worth adding the flag `--no-gzip-encoding` to see if that makes a difference.

Can you copy the files to a local directory without the error message?  And how about from the local directory to minio?
 > I identified that only this bucket (assets) presents this error, and to my luck I pick exactly this one to start with.

Is there a problem with that bucket do you think?  Or a problem with rclone?
 Ah I see!

An `--ignore-checksum` command which disabled the post copy checksum check would fix this for you.
 I'm going to open this again, so I actually implement the feature!
 Can you try this which implements the --ignore-checksum command?

http://pub.rclone.org/v1.35-15-g82742f5-ignore-checksum%CE%B2/ I've merged this to master for the 1.36 release

Here is the beta - http://beta.rclone.org/v1.35-62-g9d331ce/ (uploaded in 15-30 mins)  What has probably happened is that some files have needed to be uploaded again.  You can see this in the log - use `-v` for more info.

There is a problem with uploading very large files to ACD - I suggest you try this beta: http://pub.rclone.org/v1.33-66-g2e20a0f-acd-timeout%CE%B2/ which has a fix designed to work around exactly that problem.
 @Stonedestroyer  You can't upload files bigger than 50GB - there is a bit in the docs about it with a workaround: http://rclone.org/amazonclouddrive/#limitations  `x509: certificate signed by unknown authority` means that there is something wrong with the SSL certificates.

It is possible onedrive messed up temporarily, or maybe there is something wrong with the certificates on your computer?

What happens when you go to https://api.onedrive.com/ - you should receive a JSON error message - if you get a certificate warning error from your computer then something is up with it.

It might be something to do with this: https://twitter.com/globalsign/status/786505261842247680?lang=da
see http://www.theregister.co.uk/2016/10/13/globalsigned_off/ for a better overview.
 Sorry just read that it was b2 not onedrive!

Try https://api.backblazeb2.com/b2api/v1/b2_authorize_account in your browser.  You should get a message about a missing token, not a security error.
 Can you try this url please?  https://api.backblazeb2.com/b2api/v1/b2_authorize_account
 You could try the steps for your OS here (I think this is likely the cause of the problem)

https://support.globalsign.com/customer/portal/articles/1353318-view-and-or-delete-crl-ocsp-cache
  So i using latest ubuntu 16.04.1 LTS and latest stable rclone 1.33

My question is if i copy from my old server file in /root/.rclone.conf into new server on the same location will it works? Or do i need re-auh ?
 Not works - Failed to create file system for "secret:/TEST": didn't find section in config file
 ```
./rclone config
No remotes found - make a new one
n) New remote
s) Set configuration password
q) Quit config
n/s/q>

```
 Aha it works i just needed to copy it from /root into /home/user/.rclone.conf
 FYI there is an FAQ about this: http://rclone.org/faq/#can-i-copy-the-config-from-one-machine-to-another
  I presume you have some quotes around that?

```
rclone sync -v d:\ e:\backup --exclude "/System Volume Information/**" --dry-run
```

I would have thought that should work...

You can test quickly by using `rclone size d:\ --exclude "/System Volume Information/**"`

Trying to reproduce with the latest beta on a Windows VM I'm getting this

```
X:\rclone>rclone ls c:\
2016/10/18 02:09:24 failed to open directory "\\\\?\\c:\\": open \\?\c:\: The system cannot find the path specified.
```

Which looks like it might be a different bug - do you see that?

It would be worth testing with the latest beta of rclone from http://beta.rclone.org
 I submitted a [go bug #17500](https://github.com/golang/go/issues/17500) for the `failed to open directory "\\\\?\\c:\\": open \\?\c:\: The system cannot find the path specified.` problem.
 That go bug has been fixed now, so I'm going to close this issue.  403 errors are normal as part of using google drive - they are drive's way of telling rclone to slow down.  rclone should slow down and drive be happy with that.

Can you run rclone with the `-v` flag and attach the output please?

Thanks

Nick
 Drive has lots of internal resource limits - maybe you tripped one of those and had to wait for it to reset.
 @spencercharest no - google don't publish them so we are left guessing!  If I'm reading that correctly you have a file for directory name which is 137 AES blocks long, 2176 characters according to the traceback.

That seems unlikely since I think most linux file systems are limited to 255 character file names.

I made an rclone binary which contains an extra log message which looks like this

```
2016/10/12 11:03:22 encryptSegment "Makefile" length 8
```

Can you give it a go and tell me what it says immediately before the crash?

http://pub.rclone.org/rclone-v1.33-63-gace1e21-crypt-log.gz

Thanks
 What have you got in your crypt config? It looks like it is somehow pointed at itself? 

When using crypt you have one remote say acd: which you point at unencrypted acd, then you make a second crypt remote called encryptedacd: with the remote parameter 'acd:secret'. 

Have you got it set up like that? 
 No problem. Glad we got to the bottom of it. 

Good suggestion to put a check in as I'm planning to make more overlays like crypt. 
  The github mirror doesn't support IPv6 either :-(

Does your IPv6 network not have an IPv4 gateway?
 I see!  I work at Memset so I'll see what is going on with IPv6 support :-)
 I've moved the downloads onto its own server now so this should be accessible via IPv6.  It is possible the DNS hasn't finished propagating yet so if it doesn't work immediately wait an hour and try again!  I use acd with .co.uk and it works just fine.

What symptoms do you see?
 I just checked the docs for ACD - there is absolutely no mention of non `.com` domains anywhere!

Looking at it more closely I see that it does take me to amazon.com (where I have an account with the same password) when I login.

If I try changing the initial oauth URL thus

``` patch
@@ -53,7 +53,7 @@ var (
    acdConfig = &oauth2.Config{
        Scopes: []string{"clouddrive:read_all", "clouddrive:write"},
        Endpoint: oauth2.Endpoint{
-           AuthURL:  "https://www.amazon.com/ap/oa",
+           AuthURL:  "https://www.amazon.co.uk/ap/oa",
            TokenURL: "https://api.amazon.com/auth/o2/token",
        },
        ClientID:     rcloneClientID,
```

Then I get this when I try to use it

```
We're sorry!
An error occurred when we tried to process your request. Rest assured, we're already working on the problem and expect to resolve it shortly.

Error Summary
400 Bad Request
Unknown client_id
Request Details
client_id=amzn1.application-oa2-client.6bf18d2d1f5b485c94c8988bb03ad0e7
redirect_uri=http%3A%2F%2F127.0.0.1%3A53682%2F
response_type=code
scope=clouddrive%3Aread_all+clouddrive%3Awrite
state=e64fdde...336
```

So maybe I have to register rclone for amazon.co.uk? Though you don't seem to be able to register any new apps for ACD unless you have an invite for the new system :-(

Does the password and email address match for your `.com` and `.fr` accounts - that is the only thing I can think of trying at the moment.
 @jody-frankowski it is worth checking to see whether you have exceeded your allowance also - the web interface is an easy way of doing that.
 @jody-frankowski thanks for working that out - I'll put something in the docs
  I see what you mean...

I have considered making a release branch with fixes like that on to get a 1.33.1, 1.33.2, 1.33.3 which would have just the essential bug fixes in from the 1.33 release.

Is that the kind of thing you mean?

Which distro are you packaging for?
 Very soon!

For your purposes I need to use one of the vendoring solutions for go otherwise the build won't be deterministic.  I've made a new issue about this #816 and will try to do this for the next release.  This will fix the issue permanently!
 @krieghof it is indeed!  Will close now  When i want to copy file from server to home pc i got 500 OOPS: lseek, via total commander
 Trying the beta is a good idea if you have't already.  Not sure how you managed to use total commander - over the network?
 i just pointed via total commander to mounted folder over ftp, and its 1.33 no beta :)
 Okay tried rclone-v1.33-64-gbc414b6β-linux-386 and confirming it works, the error lseek gone.

But now the mount still not updating only when i stop mount and remount again ...
 Try http://beta.rclone.org/v1.33-79-g77b975d/ which will refresh the mount every 5 minutes by default or how often you want with `--dir-cache-time`
  The error id

```
 dial tcp: lookup pod-000-1010-11.backblaze.com on 8.8.8.8:53: dial udp 8.8.8.8:53: i/o timeout
```

Which means that your dns couldn't be contacted or didn't know the answer. 

This can happen if your link is very busy - did the file transfers start? Or is there something else running using bandwidth? 
 Does rclone ever get going or are the Dns errors transient? 
  Speed tests will report in 12 MBit/s wheras rclone reports as 1.5 MByte/s which is approx 15 Mbit/s - so maybe you are actually doing OK?

You could also try the latest beta: http://beta.rclone.org/v1.33-52-gaedad89/ which has some ACD fixes.
 No worries!
  Are those figures without using rclone mount? If not can you try using rclone directly without FUSE and report the timings? 
 Thanks for the tests.

I'm not sure what is going on here!

Observations suggest it is most likely it is something to do with the crypt code and seeking but I haven't figured it out yet!

It could also be something that has changed (eg the fuse library I'm using) - I'll try to narrow it down more.
 Good results thanks. How are you measuring the speed while seeking? Maybe I need to write some benchmarks to formalise that. 
 I have fixed this in : http://beta.rclone.org/v1.33-69-gd8d1102/ (will be uploaded in 15 mins)

There was a bug in the seek implementation which was causing it to seek on every read which really killed the performance.

I only figured out what was going on by playing videos from ACD with vlc - so thank you all for your tips.
  @justusiv good thinking.  That was merged to master so will be in the latest beta: http://beta.rclone.org/v1.33-55-g98804cb/

(Not a windows expert so help much appreciated!)
 I'm going to assume this is fixed now - please re-open if not.  `--allow-other` is what you need so that users other than the user that did the mounting can read the files.  Not having that will give you the permissions denied errors when accessing files with root.

Maybe you need `--allow-root` too?

What do the permissions look like - do an `ls -l` of some files you are having a problem with and post them here...
 You don't need the execute bit for files only directories. 

Have you tried adding user_allow_other to /etc/fuse.conf? You need that in order to make allow other and allow root work. 
 It would be worth re-testing with the latest beta: http://beta.rclone.org/v1.33-52-gaedad89/ which does definitely support seek.
 Excellent :-)
  Saving and loading state would be hard and different for each provider, so I'm reluctant to go down that route.

What would be easier would be to run an rclone process you can send commands to somehow.  rclone would have to cache the `remote:` objects between invocations to be of any use.

Imagine for a moment that rclone opened a socket (or maybe you could pipe stuff on stdin), one rclone command per line.  Would that be the kind of thing that would be useful? How would rclone return errors?

It would be helpful to have a bit more detail about what exactly are in those `rclone copy [source] [destination]` commands. Do they go both ways upload and download?  Is it files or directories you are transferring? What about the paths do they change much?

An alternative to making a command line interface might be to make a simple API, simple enough that you could WGET or CURL it from a script.

PS what does git-annex-remote-rclone do?  I'm not familiar with git-annex.
 I had an idea to refine the above...

Make a command called `rclone client`.  This would have some subcommands

```
rclone client start_server [opts]
rclone client stop_server [opts]
rclone client run [opts] -- rclone command
```

The opts would be:

```
--addr host:port - where to bind to - defaults to localhost:19384 (say)
--auth string - string that must be supplied as a password to use the server- defaults to ???
```

These should be usable as environment variables ideally also

Or maybe rclone should be communicating over stdin and stdout to make it private?

Might want another command to say you are going to be re-using a particular remote which would help rclone working out what to cache.

```
rclone client cache_remote [opts] remote:path
```

`rclone client start_server` would start a server in the background running on the port you gave. It would implement some sort of http API - the details of which are irrelevant.

So you would use it like this

```
rclone client start_server
rclone client run -- copy /path/to/dir remote:
rclone client run -- copy remote: /path/to/dir
rclone client run -- delete remote:path
rclone client stop_server
```

Things to think about
- the logs should probably be streamed back from the server to the client, failing streaming they need to be returned somehow.
- working out how to cache the remotes is tricky because of the path after the remote
  - might want to implement a reset root command? which would take a remote: and change the path it was attached to. This might kill the directory caching though...
- the internals of rclone will only allow one command to be run at once (there is too much global state at the moment) so need to enforce that
- might want rclone to choose a random port and password and return these as strings that a shell could intepret (like ssh-agent does)
  After a bit of investigation I can see that the rclone code is working as expected, what is unexpected is that the templinks look different for big and small files.

For a 1GB file I get a templink like this

```
https://content-eu.drive.amazonaws.com/cdproxy/templink/IuJ...
```

Wheras for a 10GB file I get a templink like this

```
https://cd-eu-prod-content.s3-eu-west-1.amazonaws.com/DJE...
```

Interestingly the 1GB file downloaded at 25 MBytes/s wheras the 10 GB file downloaded at 44 MByte/s which is why I guess you were experimenting with that value.

PS thanks for pointing out `iftop` to me - not seen that tool before!
  I think this is the same issue as #610 which I'm planning to fix for the next release.
 I've fixed this in http://beta.rclone.org/v1.33-55-g98804cb/ (it will take 15 mins to appear).  Have a go and re-open the ticket if there is a problem.

Thanks

Nick
  I've just committed a fix for #638 to master which may fix this: 

http://beta.rclone.org/v1.33-49-gde80a54/ 

Note that I don't have a mac so haven't tested very much of the fuse code on OS X.  I'd be interested if you might want to contribute anything to the docs on how to set it up on OS X.
 Wow,  that is a lot slower! 

I wonder why? 

Can you try a plain cp rather than an rsync and compare the versions? 

A step by step in markdown would be perfect. I can wedge that in the docs or you can send me a pull request whichever you prefer. 
 Thanks for that and it is what I expected.  Note that seeking is really slow for `rclone mount` - and I suspect rsync does quite a bit of seeking.

In http://beta.rclone.org/v1.33-51-gf45b3c8/ I added a `--no-seek` flag - can you see what difference that makes?

See [the donations page](http://rclone.org/donate/) for the other thing!
 Thank you very much for the docs - they look great.  I've edited them a bit to take the current version numbers out: See http://rclone.org/install/ - does that still work OK?

If you could do a paragraph on how to install FUSE for macOS as well that would be very useful :-)  Maybe there is a doc I could link to?

I messed up the build - here is the new beta: http://beta.rclone.org/v1.33-52-gaedad89/
 @spencercharest 

> On this newest beta both --no-seek and no flags are both very slow. Less than 200 Kb/s.

:-( I'll try to find out what is going on...
 Is this still a problem with rclone 1.35? @spencercharest excellent news!  I'll close this isse then!  I think this is the same issue as #680 which I'm planning to fix for the next release.
 Hope for the fix going live soon :)
  This is to do with the revamp of the timeout code in 0cb9bb3b54397ff2dd7a563194d1ec98ca9cedaa

The not turning timeouts off if 0 is a bug.

What I think is happening in your case is the read timeout is firing even though writes are going through just fine.

I'll confirm that and give the logic a little tweak, then post a beta for you to try.
 http://pub.rclone.org/v1.33-41-g90aee8c-fix-timeout%CE%B2/
 That is a beta for you to have a go with - I think it is fixed!
 That looks like a different problem!

I set the `ResponseHeaderTimeout` to the `--contimeout` value which is 1 minute by default.  I've seen lots of issues with Amazon being slow to process big files after upload, but them appearing in the web UI later. 

Perhaps I should be setting it to the `--timeout` value instead.  If you try with `--contimeout 5m` does it succeed?

```
    // ResponseHeaderTimeout, if non-zero, specifies the amount of
    // time to wait for a server's response headers after fully
    // writing the request (including its body, if any). This
    // time does not include the time to read the response body.
    ResponseHeaderTimeout time.Duration
```
 Thanks for testing. 

I'll fix on this issue don't worry about opening a new one. 

The speculation is that it is something to do with calculating the hash, yes. 
 I've fixed this now - find the beta in http://beta.rclone.org/v1.33-56-gd42b386/ (in about 15 minutes)
 @felixbuenemann thanks for testing 
  What you are talking about is detecting renames.  For remotes which support a hash it would possible.
  This is a known problem. See issue #100
 what about we will have something in rclone like this:

`rclone move ...` will got option `rclone move --delete ...` possible?
 @scriptzteam I'm not sure I understand what you mean - move will delete the files but won't delete the empty directories until #100 is fixed.
 yes i mean the prefix `--delete` will be used as to remove empty dirs or `--delete-empty`
  It is possible this got fixed as part of 0cb9bb3b54397ff2dd7a563194d1ec98ca9cedaa - so can you give a recent beta a try, eg http://pub.rclone.org/v1.33-31-g9ea20ba%CE%B2/
 None of the above errors say `x509: certificate signed by unknown authority` which is what the `--no-check-certificate` prevents.

What are you trying to connect to?  Have you tried using `-v` and `--dump-bodies` to debug what is going on?
 Did you figure this out? Ok!  ```
Received error: failed to make directory: HTTP code 400: "400 Bad Request", reponse body: {"message":"1 validation error detected: Value 'BLABLAlooooNG.....ENCRYPTED' at 'name' failed to satisfy constraint: Member must have length less than or equal to 280"}
 - low level retry 1/10
```

is there a ways to skip that?
 You could probably put a filter rule in.

if you did `--exclude="????...????*"` where you need about 175 `?` symbols then that will exclude any files which are too long after crypt.  You'd need another one for directories too, so I'd probably put them in an `--exclude-from` file with these two lines in

```
???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????*
???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????*/
```

Sorry that isn't very elegant but it is the best I can come up with at the moment!

Which remote storage system are you using?  The rules for name lengths vary for all of them.
 Its Amazon Cloud Drive with CRYPT to be exact :)

i just did for now this -> `--low-level-retries 1`
 @camjac251 the right solution is #637 which will solve this problem properly.  Until then there isn't a nice solution :-(
 That really is very ugly isn't it!

I'll make a new flag `--max-name-length` which you can use to filter this explicitly
  The source and the destination don't overlap in your example, one is on the local disk, the other is on acd so it should work fine.

If you did

```
rclone move acd:Main acd:/Main/Sub
```

Then they would overlap
 It will use the same logic as sync or copy - by default it will overwrite if it is different, otherwise it will just delete the source.
 I think you should find this is fixed in the latest beta: http://beta.rclone.org/v1.35-19-g499766f/ for overlapping remotes on ACD.  There isn't a way to do this directly with rclone at the moment

You could script it like this though...

```
rclone copy/sync .....  2>&1 | split -l 1000 - rclone.log
```

Don't use `--log-file` in the above.

Which will split the output of rclone into files with 1000 lines in each.  Use `split --help` for more options.
 Glad you got it working :-)
  Unfortunately rclone doesn't know how much disk it needs until it has done the sync.

It would have to do a pre-pass which would slow things down, rather like using `--delete-before`.  Is there a way for deleting of source after upload complete on acd?

Thx!
 thx, i read in docs this "Specifying --delete-after will delay deletion of files until all new/updated files have been successfully transfered.", means it delete souce or destination? since i need to remove source
  Great idea. It should be reasonably easy to implement in rclone's dial functions (fs/http_new.go and fs/http_old.go).

If you fancy giving it a go I'll write up exactly how? 
 Here is what the SSH manual says

```
IPQoS   Specifies the IPv4 type-of-service or DSCP class for connections.  Accepted values are “af11”, “af12”, “af13”, “af21”, “af22”, “af23”, “af31”, “af32”, “af33”, “af41”,
             “af42”, “af43”, “cs0”, “cs1”, “cs2”, “cs3”, “cs4”, “cs5”, “cs6”, “cs7”, “ef”, “lowdelay”, “throughput”, “reliability”, or a numeric value.  This option may take one or two
             arguments, separated by whitespace.  If one argument is specified, it is used as the packet class unconditionally.  If two values are specified, the first is automatically
             selected for interactive sessions and the second for non-interactive sessions.  The default is “lowdelay” for interactive sessions and “throughput” for non-interactive ses‐
             sions
```

And they are defined thus in the source code

``` c
/*
 * Definitions for IP type of service (ip_tos)
 */
#include <netinet/in_systm.h>
#include <netinet/ip.h>
#ifndef IPTOS_LOWDELAY
# define IPTOS_LOWDELAY          0x10
# define IPTOS_THROUGHPUT        0x08
# define IPTOS_RELIABILITY       0x04
# define IPTOS_LOWCOST           0x02
# define IPTOS_MINCOST           IPTOS_LOWCOST
#endif /* IPTOS_LOWDELAY */

/*
 * Definitions for DiffServ Codepoints as per RFC2474
 */
#ifndef IPTOS_DSCP_AF11
# define        IPTOS_DSCP_AF11         0x28
# define        IPTOS_DSCP_AF12         0x30
# define        IPTOS_DSCP_AF13         0x38
# define        IPTOS_DSCP_AF21         0x48
# define        IPTOS_DSCP_AF22         0x50
# define        IPTOS_DSCP_AF23         0x58
# define        IPTOS_DSCP_AF31         0x68
# define        IPTOS_DSCP_AF32         0x70
# define        IPTOS_DSCP_AF33         0x78
# define        IPTOS_DSCP_AF41         0x88
# define        IPTOS_DSCP_AF42         0x90
# define        IPTOS_DSCP_AF43         0x98
# define        IPTOS_DSCP_EF           0xb8
#endif /* IPTOS_DSCP_AF11 */
#ifndef IPTOS_DSCP_CS0
# define        IPTOS_DSCP_CS0          0x00
# define        IPTOS_DSCP_CS1          0x20
# define        IPTOS_DSCP_CS2          0x40
# define        IPTOS_DSCP_CS3          0x60
# define        IPTOS_DSCP_CS4          0x80
# define        IPTOS_DSCP_CS5          0xa0
# define        IPTOS_DSCP_CS6          0xc0
# define        IPTOS_DSCP_CS7          0xe0
#endif /* IPTOS_DSCP_CS0 */
```
  I've merged the acd doc fix in 7cf6fe220948641e28c1d7e00cf458427b556c69

I've fixed the move command so I dropped the other commit.

Thanks for your contribution :-)
  Nice idea thanks.

If you had a list of applications which use a 0 sized `dir\` object to mean a directory that would be very helpful.

Note this is related to #14 which discussed s3fs
  Is your web browser running on the Proxmox machine? If not then follow the instructions here: http://rclone.org/remote_setup/
  I think this is a bug - `move` should move the files and if an identical file exists in the destination then the source file should be deleted.  This is what would happen if it was copied

Here is a demo of the problem.

```
$ mkdir /tmp/a /tmp/b
$ echo hellp >/tmp/a/test.txt
$ rclone -q copy /tmp/a /tmp/b
$ rclone move -v /tmp/a /tmp/b
2016/10/01 17:08:33 rclone: Version "v1.33-DEV" starting with parameters ["rclone" "move" "-v" "/tmp/a" "/tmp/b"]
2016/10/01 17:08:33 Local file system at /tmp/b: Modify window is 1ns
2016/10/01 17:08:33 Local file system at /tmp/b: Using server side directory move
2016/10/01 17:08:33 Local file system at /tmp/b: Server side directory move failed - fallback to file moves: can't copy directory - destination already exists
2016/10/01 17:08:33 Local file system at /tmp/b: Waiting for checks to finish
2016/10/01 17:08:33 test.txt: Size and modification time the same (differ by 0s, within tolerance 1ns)
2016/10/01 17:08:33 test.txt: Unchanged skipping
2016/10/01 17:08:33 Local file system at /tmp/b: Waiting for transfers to finish
2016/10/01 17:08:33 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:                 1
Transferred:            0
Elapsed time:          0s
2016/10/01 17:08:33 Go routines at exit 3
$ rclone -q ls /tmp/a
        6 test.txt
```
 I've fixed this in this beta: http://pub.rclone.org/v1.33-30-g945f49a%CE%B2/

If you find any problems then please re-open the ticket!
 Arg - don't use that beta - I brown paper bagged it!  I'll post another in a moment
 http://pub.rclone.org/v1.33-31-g9ea20ba%CE%B2/
  Can you send a log with `-v` please?

625% done does seem very strange.

Can you do `rclone ls /path/to/Song1.mpg` and check that the size rclone reports is the same as you see in windows explorer?

Have you got anthing else installed which might act on music files?
 That sounds quite plausible - good thinking. 

What rclone does is it reads the length of the file the same way Explorer would. But when it is reading a file it will read as many bytes as the OS gives it, not just the length from the directory listing. That should always be the same number of bytes, but maybe drive pool is causing it not to be somehow.

It might be worth contacting them about it and see what they say. I'd be happy to talk to a technical contact there if you can find one to help. 
 @drashna Great, glad to hear that you are on the case. Let me know if you want anything from me. 
  I don't think this is anything particularly to do with copying from one cloud store to another.

It looks like this is a problem with onedrive.

Does it work if you include `--no-check-certificate` ?

I suspect it is a mis-configuration by the onedrive team which they will fix quickly - so does it still do it?
 Is this working now? I'm going to assume this is fixed now - if not please re-open!  I should probably make an ARMv7 release as well as the ARMv6 which is the default... 

In the meantime you should be able to compile from source 
 I compiled a version from source with

```
GOOS=freebsd GOARCH=arm GOARM=7 go build
```

Can you see if it works?

http://pub.rclone.org/rclone-v1.33-28-gf2eeb43-freebsd-armv7.zip
 Note that ARMv6 code runs fine on ARMv7 processors under linux.  It is a limitation freebsd (or the go runtime running under freebsd - I'm not sure which) that ARMv6 code doesn't run on ARMv7 processors.

https://golang.org/src/runtime/os_freebsd_arm.go

That said an ARMv7 version would almost certainly run faster on the new raspberry Pi.

ARMv8 is known as arm64 and I've added that to the release process already - you can find a beta here, though I think the Raspberry Pi runs in ARMv7 mode by default.

http://pub.rclone.org/v1.33-24-gbfe6f29%CE%B2/

I'll try to figure out a sensible way of adding ARMv7 to the release process.
  I'm surprised you are getting that error under linux. You get that error if there is something wrong with the user setup. 

It might be a symptoms of running under cron. 

Exactly which Linux distribution are you using? 

The error is harmless and I should probably disable it if you pass in the --config option. 
 Here is the code

``` go
// Find the config directory
func configHome() string {
    // Find users home directory
    usr, err := user.Current()
    if err == nil {
        return usr.HomeDir
    }
    // Fall back to reading $HOME - work around user.Current() not
    // working for cross compiled binaries on OSX.
    // https://github.com/golang/go/issues/6376
    home := os.Getenv("HOME")
    if home != "" {
        return home
    }
    ErrorLog(nil, "Couldn't find home directory or read HOME environment variable.")
    ErrorLog(nil, "Defaulting to storing config in current directory.")
    ErrorLog(nil, "Use -config flag to workaround.")
    ErrorLog(nil, "Error was: %v", err)
    return ""
}
```

If you supply `--config` you can ignore the error message.

What user is the crontab running under? 

Exporting HOME will get rid of the error message but it doesn't explain why `user.Current()` didn't work in the first place.

As a normal user, if you `unset HOME` do you get the same error when you run rclone?
 Did you manage to resolve this? I should probably not give that error if `--config` is set. That would involve a bit of gymnastics but it could be done.

> I'm not sure why I'm getting it since no one else has reported it, it must be a special case in my environment 

The root cause of the problem is that `user.Current()` didn't work. A quick squint at the go source code tells me that it is never going to work compiled without cgo if $HOME is unset.

```
$ export CGO_ENABLED=0
$ unset HOME
$ go build 
$ ./rclone version
2017/01/10 16:00:44 Couldn't find home directory or read HOME environment variable.
2017/01/10 16:00:44 Defaulting to storing config in current directory.
2017/01/10 16:00:44 Use -config flag to workaround.
2017/01/10 16:00:44 Error was: user: Current not implemented on linux/amd64
2017/01/10 16:00:44 Config file ".rclone.conf" not found - using defaults
rclone v1.35-DEV
```

So setting $HOME is the correct thing to do in this case as I don't plan to start building with CGO_ENABLED=1
 @benamira set the `--config` flag for rclone in the script.  Take a look [at this thread](https://forum.rclone.org/t/rclone-mount-on-startup-with-systemd/360/2) on the forum I'm going to close this now since the original issue is sorted.  There is an issue about that here #97

Can you click the subscribe button on that issue to be notified of changes, and for me to guage interest.

I'm closing this one as a duplicate.

Thanks

Nick
  I just checked - those both are v1.33 - did you get an old cached version maybe?
  According to the [drive api docs](https://developers.google.com/drive/v2/web/about-auth)

> All requests to the Google Drive API must be authorized by an authenticated user. Google Drive uses the OAuth 2.0 protocol for authenticating a Google account and authorizing access to user data. You can also use OAuth 2.0 or Google+ Sign-in to provide a "sign-in with Google" authentication method for your app.

So I think that is a no unfortunately.

Amazon S3 does support anonymous use of the API though (and so does rclone).
  I just tried rclone v1.33 on my FreeBSD 10.1 VM and it seems to work OK.

Are you sure you downloaded the right package amd64 vs 386 vs arm ? That is what the error "Exec format error" suggests.  Or maybe it got corrupted somehow so you could try re-downloading it.

> I can also run rclone without arguments and have it display v1.26 which I thought was equally strange.

Is there an old rclone in your path somewhere?
 Hmm there is another issue about the downloads being wrong here #745

Assuming that wasn't you too under a pseudonym, I wonder if there is a problem with the CDN I'm using... 

Did you do the downloading under freebsd? What tool did you use? 
 @adrianmace 

What do you get if you do this?

```
$ cd /tmp/
$ wget 'http://downloads.rclone.org/rclone-v1.33-freebsd-amd64.zip'
--2016-10-01 17:37:46--  http://downloads.rclone.org/rclone-v1.33-freebsd-amd64.zip
Resolving downloads.rclone.org (downloads.rclone.org)... 77.73.5.145
Connecting to downloads.rclone.org (downloads.rclone.org)|77.73.5.145|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3657436 (3.5M) [application/zip]
Saving to: ‘rclone-v1.33-freebsd-amd64.zip.1’

rclone-v1.33-freebs 100%[===================>]   3.49M  3.78MB/s    in 0.9s    

2016-10-01 17:37:47 (3.78 MB/s) - ‘rclone-v1.33-freebsd-amd64.zip’ saved [3657436/3657436]
```

```
$ md5sum rclone-v1.33-freebsd-amd64.zip
43e9f2fa785f154b0ea243b04eb08279  rclone-v1.33-freebsd-amd64.zip
```

```
$ unzip rclone-v1.33-freebsd-amd64.zip
$ md5sum rclone-v1.33-freebsd-amd64/rclone
07a660c90472909bac8baaa7a47de7ef  rclone-v1.33-freebsd-amd64/rclone
```

```
$ strings rclone-v1.33-freebsd-amd64/rclone | grep v1.33
v1.33
$ 
```
 I'm going to assume this is fixed now since I haven't had any other complaints about it - please reopen if not  @lanrat What ACD seems to be saying is that you have a directory called "under the covers" and another called "Under the Covers".  rlone treats these as separate directories, but ACD treats them as the same, hence the conflict error.

Can you check if acd_cli uploaded the directory as "under the covers" maybe?  That would explain the problem
 @javier-gotham2 you seem to have a similar problem with "windows 10" and "WINDOWS 10".  I'd guess you are using windows so that is unlikely to be actually the case.

Can you check what the directory is actually called - and also sending a log with -v would be helpful.
  Can you attach a log with the `-v` flag please?
 Do you see the file with `rclone ls yandex:` after you get one of those errors?  Or is it only visible in the web interface?
 It looks like Yandex refused to take the upload which rclone can't really do anything about other than retry.

  Looking at your log, it looks like those `.JPG` files are causing the problem.

If you do

```
rclone ls --include "*.JPG" remote:path
```

rclone will show you the JPG files.  rclone is always case sensitive.

If happy with the list you can then do

```
rclone delete --include "*.JPG" remote:path
```
  There isn't a way of doing that at the moment.  What would you like to see - a `--log-transferred` flag or something like that?
 Yes something like that seems sensible.

At the moment there are 3 log levels
- errors only `-q`
- info (no flag)
- debug `-v`

We'd need to slip a fourth one in, say
- errors only `-q` `--log-level errors`
- info (no flag) `--log-level info`
- transfers `--log-level transfers`
- debug `-v` `--log-level debug`

The transfers level should log the Copied lines that you get from `-v` and any deleted lines, but nothing else (no skipping etc)
 After a bit of thought about this, I think that I should take rclone back to its rsync roots and make -v only show file transfers and do this

- errors only `-q` `--log-level errors`
- info (no flag) `--log-level info`
- transfers `-v` `--log-level transfers`
- debug `-vv` `--log-level debug`

At the moment transfers and debug are combined into one level.  I'd split them and into transfers I'd put any files which were

  * uploaded
  * downloaded
  * renamed
  * server side copied
  * server side moved
  * deleted

Everything else would go into the debug level which I'd make a shortcut of -vv to use.

I'm also proposing to send all logs to stderr by default (at the moment the -v log goes to stdout).

Being able to send the logs to [syslog](https://golang.org/pkg/log/syslog/) would be extremely useful, so I propose adding a --syslog optionc (especially for rclone mount) and in that case the rclone log levels should probably match up with syslog levels

  * LOG_EMERG      system is unusable
  * LOG_ALERT      action must be taken immediately
  * LOG_CRIT       critical conditions
  * LOG_ERR        error conditions
  * LOG_WARNING    warning conditions
  * LOG_NOTICE     normal, but significant, condition
  * LOG_INFO       informational message
  * LOG_DEBUG      debug-level message

I'd propose making the 4 levels match up like this

- errors only `-q` `--log-level errors` - LOG_ERR
- info (no flag) `--log-level info` - LOG_NOTICE
- transfers `-v` `--log-level transfers` - LOG_INFO
- debug `-vv` `--log-level debug`  LOG_DEBUG

As for @calisro's suggestion of prefixing the log messages with an identifier - I'm not sure about that.  It could be an option?

 @calisro 
> Is there a reason why you'd default to stderr vs stdout?
Sorry that was a typo!  Have corrected the above post - logs will go to stdout!

I think all the things in your LOGFILTER would be in the debug level so you'd need `-vv` to see them.

Not sure about "File found with same name but different case" - is that a debug or an info?  It is currently an info.  Why do you see that in your transfers?

> What are you intending on sending to INFO (default) versus the transfers/debug? I'm just trying to see how that matches up to my pruning.

I plan to leave info (default) level pretty much as is.  That doesn't receive a lot of logs normally anyway.

The transfers level will match up with what you want to see I think - that is my intention.
 I have reworked the logging - please have a go with it in this beta

http://beta.rclone.org/v1.35-87-g006227b/ (uploaded in 15-30 mins)

Here are the bits from the docs

## Logging ##

rclone has 4 levels of logging, `Error`, `Notice`, `Info` and `Debug`.

By default rclone logs to standard error.  This means you can redirect
standard error and still see the normal output of rclone commands (eg
`rclone ls`).

By default rclone will produce `Error` and `Notice` level messages.

If you use the `-q` flag, rclone will only produce `Error` messages.

If you use the `-v` flag, rclone will produce `Error`, `Notice` and
`Info` messages.

If you use the `-vv` flag, rclone will produce `Error`, `Notice`,
`Info` and `Debug` messages.

You can also control the log levels with the `--log-level` flag.

If you use the `--log-file=FILE` option, rclone will redirect `Error`,
`Info` and `Debug` messages along with standard error to FILE.

If you use the `--syslog` flag then rclone will log to syslog and the
`--syslog-facility` control which facility it uses.

Rclone prefixes all log messages with their level in capitals, eg INFO
which makes it easy to grep the log file for different kinds of
information.
 I messed up with that beta - try this one instead http://beta.rclone.org/v1.35-92-g18c75a8/ (uploaded in 15-30 mins)  Are the files on the CIFS mount owned by the user using rclone?  rclone is probably trying to update the modtimes on files that user doesn't own.

Check the files on the mount, eg `ls -l /mnt/ntserver/TestDeploy/boma/Functions` to see which user owns them.
 That looks like the right solution :-)
 I see you made another issue about the info printing (good idea)

So I'll close this one now
  I think this is a good idea. To encrypt a file, you'd make a session key, encrypt that with the public key then store that blob in the file header.  To decrypt you'd read the blob out of the file header, decrypt with private key then use the session key to decrypt the rest of the message.
  What you really want is biderectional sync: #/118

I'm going to close this as a duplicate - can you press the subscribe button on that issue and you'll get notifications of updates there.
  I see what you mean.
- You have an existing bucket with some existing files in.
- You've setup a crypt remote to point to that bucket.
- rclone sync isn't deleting the existing files

The problem is that the crypt filesystem doesn't see files that aren't crypted - they don't exist as far as it is concerned!

You'll see if you do `rclone ls RemoteCrypt:path` you won't see any of the existing files.

That explains why `rclone sync` doesn't delete them as for the `RemoteCrypt:` they don't exist.

As for how you fix this....

Assuming your underlying remote for S3 is called `remote:` and you have file name encryption off

  rclone ls --exclude "*.bin" remote:bucket

This should show you all the non encrypted files - check them carefully then do

  rclone delete --exclude "*.bin" remote:bucket

Of if you just want to move them out the way

  rclone move --exclude "*.bin" remote:bucket remote:newbucket
  @pkIAIK Excellent suggestion!

Another thing you could try would be to reduce the overall bandwidth slightly - I'd try `--bwlimit 12M` to limit to 12 Mbytes per second.  This might help by allowing some bandwidth for the TCP acks to come back.
 Are you happy with the transfer now?  Do you still need this issue open?  Good explanation by @DurvalMenezes though it isn't Amazon guessing the content type from the name, it is rclone (via the go runtime). 

I could add an interface to the object which could return the content type. The local fs would carry on working like it does (or maybe on OS X it would read the content type somehow from the OS) - the others will know the content type already so as return it. This would fix the original issue. I could then list content typed in listings etc. 
 I made a little test to see how it might work: http://pub.rclone.org/v1.33-28-gdb2307f-content-type%CE%B2/

Let me know if it does!
 Ah, and the code looked so good ;-) 

I'll have another go and post another beta tomorrow if I have time. 

Thanks for testing. 
 This should work a bit better...

http://pub.rclone.org/v1.33-28-g2dee7bc-content-type%CE%B2/
 I've merged this to master - please re-open if for some reason it doesn't work for you:

http://pub.rclone.org/v1.33-30-g945f49a%CE%B2/
 @seabre 

I've fixed this in 

http://beta.rclone.org/v1.35-155-g10e532b/ (uploaded in 15 - 30 mins)

Thanks for reporting  It looks like this is the same issue as #689 - you have a file called "Icon" with a carriage return on the end.

I'll post a beta when I've done a fix for it.

In the mean time you could add `--exclude 'Icon?'` to your command line to not sync those files.
 When there is an upload failure rclone deletes the upload so you don't get a partial upload (something I'm going to modify in #559). If it didn't get uploaded then the delete fails.
 No need to worry :-)
  Big files upload their parts in parallel and take more than one upload slot which may well explain why some files make no progress for a while - their upload slots are being used by big files uploading.  I should probably have a separate pool for the big file upload chunks then they wouldn't starve the incoming files.

It might be better to have a separate flag for controlling the concurrency of the large file uploads, as each chunk will take 96MB of memory.  If not set it could default to the number of transfers.  This means that by default B2 might transfer twice `transfers` things at once.

I need to see that error message to work out exactly what is going on - I'm not expecting any sort of timeout - and a full log with `-v` would be very helpful.  Attach it here, or if you are worried about the contents email it privately to nick@craig-wood.com .  Put the issue number in the title would be helpful if you do email it.
 @dpedu I've had a look through the log you posted.  I can see clearly the problem you mean.  I think if I do a fix similar to my first comment then it will fix the issues.

I think the issue is mostly cosmetic though - the files not being transferred - the ones you see at 0% will get done eventually.

The errors I see in your log seems to be all backblaze being overloaded at various points in the transfer - `Failed to copy: c001_v0001000_t0021 is too busy (503 service_unavailable)`.  There isn't a lot rclone can do about that except retry the uploads which it will do eventually (see the `--retries` flag).  The log you sent didn't show a retry so I guess it hadn't quite finished yet.

So on a retry rclone should retry those 18 files, hopefully successfully.

@marrie 

> Waht I normally do is rerun knowing that it won't copy the stuff already there hoping that the larger stuff I do have will copy over.
> I'd like to see a log with `-v` if possible.

It might be that you need to increase the value of `--low-level-retries` if big files are failing to upload - that will mean rclone will try harder - the log will show you though.
 I've implemented this here: http://beta.rclone.org/v1.33-67-gb7875fc/ (may take 15 mins to be uploaded)
  Glad it is sorted - will close now  If you go to the amazon drive web page, then click the menu in the top right corner - go to "Account Settings" then under "Manage Third Party Apps", click on manage settings, you'll get to a page where you can remove rcone from your account.

You'll need to go through the registration process if you want to use it again.
 Yes I'm afraid that is an Amazon limitation.
  To get that message you need to have two files with the same name and different case.  This can be n the same directory
- `Music/R.E.M/Dead Letter Office/Folder.jpg`
- `Music/R.E.M/Dead Letter Office/Folder.jpg`

Or two different directories like this
- `Music/R.E.M/Dead Letter Office/Folder.jpg`
- `Music/r.e.m/Dead Letter Office/Folder.jpg`

The error is only a warning and won't affect you with an encrypted file system with encrypted names.  It is there for transfers to from Windows / OS X or unencrypted ACD where it would be a problem.
 Yes you are right, the crypted file system is case sensitive, even when running on a case insensitive remote -something I should probably put in the docs somewhere.
  You can put fuse mounts in the fstab - at least that is what the docs say, but I haven't tried it.

There may be a problem with that because rclone mount doesn't run in the background yet.
 Let me know if it works or not!
  It is something I've been thinking about but have never written into an issue.

An `rclone serve` command would be the way forwards which might serve stuff over http/HTTPS or maybe would run over SSH. It could be quite simple...

You could have a go with minio (see rclone.org/s3 ) which runs an s3 server locally and works with rclone.
 I put the serve idea in a new ticket #/1007 so I'm going to close this one  rclone traverses directories in a somewhat non-deterministic (random) fashion.  The way the architecture of rclone works (it streams everything) I can't easily do what you ask.

Here are some ideas...

Assuming you normally run

```
rclone sync --checksum /path/to/source acd:dest
```

you could use

```
rclone copy --max-age 1d --checksum /path/to/source acd:dest
```

to freshen up any files which were modified in the last day.  (Don't use `rclone sync` here you don't want this deleting any files.) Then finish this off with a

```
rclone sync --size-only /path/to/source acd:dest
```

to copy anything missed and delete deleted files.  Once a month you could run

```
rclone sync --checksum /path/to/source acd:dest
```

to check if it has been working properly.  You can also use

```
rclone check /path/to/source acd:dest
```

for this purpose.

#626 would help with this too.
  This would rely on the remote supporting server side copy.  It would be similar to the --track-renames flag.  The mount command doesn't run in the background. So stick an & on the end of the command and it will behave like you are expecting.

I will emphasize this in the docs as it is different to most fuse mounts. However it is hard getting go stuff to run in the background.

I should probably spend some more time trying to figure that out!
 @robdidop I've moved this up to a feature request so I can actually put rclone mount in the background like it is supposed to be! This looks like a good library

https://github.com/sevlyar/go-daemon

Also need to add `-f` or `--foreground` to not daemonise.
  That should say `--min-size 1b`.

I've never used the s3 management console. What exactly gets created? Is it a 0 byte file? What is its content type?
 I'd probably prefer to "fix" this with a bit of documentation.

Fancy supplying some?  Amazon Drive could support Move and MoveDir but doesn't at the moment.  This would use the PATCH calls for nodes and dirs.

I don't think Amazon Drive can support Copy thought.
 I've merged this into #122
 @eharris this will be updated when I make the next release which is now 20 days overdue, but RealLife	™ keeps getting in the way ;-)  Great idea - I've also found it really annoying emptying 1000s of items from the trash! Probably why I have 7,500 items in there at the moment!

There is a ticket about `--backup` and `--backup-dir` in #98 

There is also some related stuff in #18
 I note that this would require #721 (which I just made) also.
 I believe this is, for the most part, addressed. Closing this in favor of https://github.com/ncw/rclone/issues/98  Yes #100 is the master ticket for this which will get fixed, we are getting closer!
  Good list thanks. Will add modules and I see I missed nlinks also. 
  Not sure what to do here - I don't see how rclone can do anything with a file that is changing.  You can't (in general) change files that have been uploaded to cloud storage systems. I'm going to close this as I don't think there is a solution :-(  Sorry I haven't commented on this - it had somehow escaped my attention!

I am interested in building OS packages - it is one of those things I've never quite found the time for.

To be part of a major distro, I need to make rclone build repeatably from source #816.  Once I do that I could upload rclone to launchpad and that would make .deb builds for ubuntu (and debian based distributions).  I don't know very much about centos/rpm based distros though - is there something similar to a ppa for Centos?

I see your solution repackages the released zip files which is a great idea, but won't fly with the distro vendors.

> Would a package added to the artifact download be ok in your eyes, similar to the current tarballs, then the user would manage the repository management steps required (eg. upload to Satelite channel, creation of Ubuntu launchpad instance)

Not quite sure I understand what you mean.  Do you mean adding the .deb/.rpm files to the download page - that sounds good to me.

In an ideal world as part of the build process I'd build .deb/.rpm packages too.

> How about an account in PackageCloud.io, allowing users to easily install the package including repo setup and GPG signing? I don't mind chipping in to pay on this if it got popular enough, would help me a lot

I see they do open source projects - maybe they would do it for free?  I'm not keen on the rclone project incurring fees if I can avoid it.  I've asked them

> How do you feel about fpm abstraction vs raw dsc/specfile?

Ultimately if the upstream distros package rclone then they will do their own specfiles, so fpm seems like a reasonable solution in the mean time.

I think your pull request is missing some bits to actually do the build isn't it? Can you add those and then I'll give it all a try!
 packagecloud.io have agreed to give rclone a free account under these conditions, which seem reasonable to me

> In exchange, we ask folks on our open source accounts to link back to us on their project webpage / project README. You can find our logo here: https://packagecloud.io/brand-guidelines

So we are all go!
 Sorry, I've dropped the ball on this pull request!

Do you still want to merge it?

rclone has gained a couple of extra packaging options since you submitted this - a snap and a packager.io build (which I'm not totally happy with).

If so can you rebase it please?  There are quite a few conflicts in the docs...

At minimum if you could send me a pull request with details of the brew formula for OS X that would be great!
  Ah I see.  I think this is a doc issue rather as you aren't the first person to have this problem. You need to put a : after the remote name,  so `remote = acd:` otherwise you are just copying files to a local directory called acd. 

I should probably get the crypt remote to warn you or ask for confirmation if you enter a remote without a ":". 
  This will probably help: http://rclone.org/faq/#rclone-gives-x509-failed-to-load-system-roots-and-no-roots-provided-error

I suspect you'll need to update the ssl roots - judging by the error message you have some already. 

You can also use --no-check-certificate which is insecure. 
 No problems. The error message you got wasn't exactly the one in the FAQ so don't feel bad about not finding it! 
  Use the `--acd-upload-wait-time` flag to reduce that time limit.

It really shouldn't be waiting on a 429 error though - I need to fix that.
 I've fixed this here: http://pub.rclone.org/v1.33-19-g83ba597%CE%B2/

Please re-open if you find any problems with it
 @felixbuenemann I think if amazon give you a 429 the file hasn't been uploaded properly - what makes you think otherwise?
 @felixbuenemann the code without the patch above does exactly that - it waits for any kind of error provided that rclone has uploaded all the data.

I think maybe getting the timing more accurate and reverting the above patch might be the best solution.

I'll re-open this to remind me to think about it.
 OK here is my latest attempt to fix this problem.  See below for changed docs.

http://pub.rclone.org/v1.33-66-g2e20a0f-acd-timeout%CE%B2/

Any feedback very welcome, with suggestions for the default parameter values.  In my tests I successfully uploaded a range of file sizes up to 15 GB.  The tests weren't successful before I introduced the extra 2m sleep.

#### --acd-upload-wait-time=TIME, --acd-upload-wait-per-gb=TIME, --acd-upload-wait-limit=TIME

Sometimes Amazon Drive gives an error when a file has been fully
uploaded but the file appears anyway after a little while.  This
happens sometimes for files over 1GB in size and nearly every time for
files bigger than 10GB. These parameters control the time rclone waits
for the file to appear.

If the upload took less than `--acd-upload-wait-limit` (default 60s),
then we go ahead an upload it again as that will be quicker.

We wait `--acd-upload-wait-time` (default 2m) for the file to appear,
with an additional `--acd-upload-wait-per-gb` (default 30s) per GB of
the uploaded file.

These values were determined empirically by observing lots of uploads
of big files for a range of file sizes.

Upload with the `-v` flag to see more info about what rclone is doing
in this situation.
 I've merged this to master here: http://beta.rclone.org/v1.33-76-g5b83270/ (will be uploaded in 15-30 mins)

Let me know if you find any problems and I'll re-open the ticket if you do!
  This could help with #707. 
  @calisro nice walk through. I test crypt like this so it is definitely supported. 
  It would be reasonably easy to do this. 

I'm not  in favour of checking all the time though, Id make an update command probably. 

I don't know what I'd do about updating the docs though - rclone knows where the user installed the binary but not the docs. Maybe I'd separate the two... 
  Seems sensible! Will do. 
 I have fixed this here: http://pub.rclone.org/v1.33-19-g83ba597%CE%B2/

Please re-open if you find any problems with it
  This should be possible for read only files and would likely make the fs a lot more compatible 
 As far as I know all the underlying cloud storage systems support http range requests which could be used to build seeking on. It would be quite a lot of work to add it though (rclone supports lots of cloud storage systems!). 

I'd probably make an Object.OpenRange(start, end) which has would return an io.ReadCloser. This could maybe be built on an Object.OpenWithHttpHeaders which could come in useful for other things... 

Another alternative is to buffer the file to disk and use that to support seeking which would have some advantages but the major disadvantage that seeking to the end would require downloading the whole file. 
 Have a go with this beta which implements http seeking: http://pub.rclone.org/v1.33-17-g7b80335-fuse-seek%CE%B2/

I can now play and skip around videos with rclone mount.

Note that seeking doesn't work over crypt yet (it will eventually just need a bit more brainpower than I have right now to work out how to do it!).  It does work over all the other remotes though.
 Thanks for testing.  Note that seeks will be slow as they have to re-open the connection to the server.  Will examine your log a bit later!
 Lots to think about thanks for your useful tests. Looks like I probably need to increase some buffering in fuse. Will examine the source if the competition some more! 
 @justusiv That would be the interesting if it isn't too much bother, thanks.
 I've committed the seek stuff to master now - see http://beta.rclone.org/v1.33-49-gde for a beta with it in.
 Yes it does include crypt support as you discovered. And thanks for updating the URL! 
 Try: http://beta.rclone.org/v1.33-81-g9d2dd2c/ which has a number of significant fixed for seeking in, especially with crypt.
  If you don't want deletions use rclone copy rather than rclone sync. Copy never deletes anything. 

--max-delete sounds like a good addition. 
  Thank you very much for doing that  - have merged :-)
  Yes crypt should retry files under control of the `--retries` and `--low-level-retries` flags as normal.

That particular error `Failed to copy: failed to authenicate decrypted block - bad password?` I think is caused by a broken network connection rather than an actual corruption.
 That error will probably be a high level retry. 

Yes I will fix the misleading error. 
 The misleading error has now been fixed so I'm going to close this.  I think this is probably the same issue as #682 - did you have rclone running for drive when you made the config for hubic?

If you make the configs while rclone isn't running the config won't disappear.  I'll fix the problem properly in #682
 Great.  I'll do the fix in #682 - please re-open if you find anything new.

Thanks
  Now we are using go1.7 there are more architectures we could be building for.  linux/arm64 is quite an important one - there may be others I've missed.

To add a new build architecture it is a matter of editing `cross-compile.sh` and `docs/content/downloads.md.in`
 Excellent!  Arm64 Linux builds will be part of rclone's normal build now. 
  rclone shouldn't do head requests on all the objects, it will only do it on zero sized objects which might be dynamic large objects (to read their size correctly). Those 3 HEAD requests are taking over 30 seconds which I think has to be hubic's fault!

From the code

``` go
    // Note that due to a quirk of swift, dynamic large objects are
    // returned as 0 bytes in the listing.  Correct this here by
    // making sure we read the full metadata for all 0 byte files.
```

In general hubic does seem incredibly slow when I try to use it.  I'm getting about 30 kByte/s ~ 300 kBit/s upload from our 1 Gbit/s connection!  Doing an `rclone lsl` (so HEAD request for each file) on a bucket with 114 files takes 37s.  If I try that on a different swift cluster it returns in 400 ms.

> I hoped to use rclone because I have some reliability issues with the tools which mount FUSE FS, but why is it so slow? This is basically unusable, sync will take forever (killed it after 50 minutes, whereas my normal rsync to svfs or hubicfuse completes in less than 30) and of course it doesn't cache the dir structure locally between runs. Is there any way to speed it up?

IMHO You would be much better using rclone directly instead of using rsync over a fuse mount. To get max performance, experiment with the number of `--checkers` and `--transfers`. rclone does lots of retries and concurrent operations which the rsync/FUSE combo won't, making it much more reliable. You can skip using the modtime (so skip a HEAD request) by using --checksum or --size-only.
 > "content_type": "application/directory" <- these can't be dynamic large objects, right? Seems to be a potential optimization to skip that HEAD request if the object type is a directory. When there's lots of directories, this will bring a huge speed improvement.

Great idea.  I made an issue with that #703 - please subscribe to that and I'll post a beta in due course.

> > You would be much better using rclone directly instead of using rsync over a fuse mount.
> 
> That's what I wanted to do, except that currently rsync over svfs or hubicfuse FUSE mounts performs way faster than rclone directly (except some reliability problems).

I'm surprised!  Can you do a time comparison for me? Make sure that svfs/hubicfuse has finished syncing its buffers at the end of the test. (Ha, just noticed that I contributed the rename a file code to hubicfuse or to its ancestor  cloudfuse!)

> Is there really a need to HEAD request for modtime? In my dump "last_modified" fields are returned as a part of the bulk JSON list.

The last_modified field is the time the object was uploaded.  rclone tries to preserve the modification time of the file so it stores it in some metadata which requires a HEAD request to retrieve.
 I made a fix for #703 - here is a beta with the fix in: http://pub.rclone.org/v1.33-11-g346d4c5%CE%B2/

Maybe you could try a benchmark now?

> > rclone tries to preserve the modification time of the file so it stores it in some metadata which requires a HEAD request to retrieve.
> 
> Is there an option to not do that? Currently I'm using the --size-only mode with rsync, as what I am storing is a media file archive, and those files don't get their content modified, ever.

It is free to store the mtime when creating the object.  Use the `--checksum` or `--size-only` flags to stop rclone checking it when doing a sync.
 Excellent, that looks more like it :-) 
 This has been released now so I'm going to close thos.  It should work. Can you supply some more details? Which remote, what crypt settings and the command line you used to mount. Thanks! 
 I just tried an encrypted remote on ACD and it worked as I expected.

Can you try `rclone ls EACD:` - does that show files? If not then the `rclone mount` won't work.

What exactly did you put in the config of the crypt?  Here is what mine looks like for comparison

```
[secretacd]
type = crypt
remote = acd:secret
filename_encryption = standard
password = SNIP
password2 = SNIP
```

and here is a sample run

```
$ rclone -q ls secretacd:
 36700160 ubuntu-13.04-mini-netboot-amd64.iso
      131 MD5SUMS.xubuntu-15.10-desktop-amd64.iso
      131 MD5SUMS.xubuntu-13.04
$ rclone -q ls acd:secret
      179 pjacqd7vm8q6eulc3k09u0lvk6aldh8l6t3f60rn0jffjocatlv2so43qjriiggr2kqv4a537kaj6
      179 91k62k7em7t7abi5on8t3hj3pek41lg9vvra8tlk9nj3lb9hf3gg
 36709152 q0fiipdq5vdor6jbbovsj08ug2bir2cteo4b899qnl069b8c84lgr77jn8s2vmfevb7romnjr9ahi
$ rclone mount secretacd:/ ~/mnt/tmp/ &
[2] 25236
$ ls -l ~/mnt/tmp/
total 0
-rw-r--r-- 1 root root      131 Sep  5 18:28 MD5SUMS.xubuntu-13.04
-rw-r--r-- 1 root root      131 Sep  5 18:28 MD5SUMS.xubuntu-15.10-desktop-amd64.iso
-rw-r--r-- 1 root root 36700160 Sep  5 18:30 ubuntu-13.04-mini-netboot-amd64.iso
$ fusermount -u ~/mnt/tmp/
```

Note that you don't need the leading and trailing slashes - I don't think they affect anything though.
 No problems! I think I'll get rclone to check the remote names for back slashes. 
  Nice one! `x/syscall` didn't exist when I started rclone!

Can you try `make cross` to see if this builds for all platforms? (You'll need to install gox) 

If it does you could try fiddling with the build constraints to fix #363 too. 
 ... Or rather to fix the last comment on #363
 I've merged the above - thanks!

I've explained what I meant in a new issue #698 which if you've got time to look at would be fantastic!

Also I made a new issue about supporting linux/arm64 in #699 which you might want to look at too if you have time.

Thanks for your contribution

Nick
 PS Just saw you are the author of Termux - fantastic app thanks - I use it most days on my android phone, and build rclone with it regularly! :-)
  This is probably the same issue as #691 which probably means dropbox have changed something in their API in a subtle way. I haven't found anything about it though. 

It might be that swapping to the Dropbox v2 API is the right solution - #349
  I've managed to replicate this with Windows 7

```
X:\rclone>rclone.exe ls C:\Users\Dev
     5052 .rclone.conf
     4883 rclone.conf
  2628754 Downloads/rclone-v1.12-windows-386.zip
  4005419 Downloads/rclone-v1.27-windows-386.zip
      402 Favorites/desktop.ini
      580 Links/desktop.ini
      431 Links/Desktop.lnk
2016/09/07 20:42:30 failed to open directory "\\\\?\\C:\\Users\\Dev\\Application
 Data": open \\?\C:\Users\Dev\Application Data: Access is denied.
```

Using dir to take a look is a bit more helpful

```
X:\rclone>dir /ah "C:\Users\Dev"
 Volume in drive C has no label.
 Volume Serial Number is 5CD1-D250

 Directory of C:\Users\Dev

04/05/2011  14:29    <DIR>          AppData
04/05/2011  14:29    <JUNCTION>     Application Data [C:\Users\Dev\AppData\Roami
ng]
04/05/2011  14:29    <JUNCTION>     Cookies [C:\Users\Dev\AppData\Roaming\Micros
oft\Windows\Cookies]
04/05/2011  14:29    <JUNCTION>     Local Settings [C:\Users\Dev\AppData\Local]
04/05/2011  14:29    <JUNCTION>     My Documents [C:\Users\Dev\Documents]
04/05/2011  14:29    <JUNCTION>     NetHood [C:\Users\Dev\AppData\Roaming\Micros
oft\Windows\Network Shortcuts]
07/09/2016  20:51           786,432 NTUSER.DAT
07/09/2016  20:51           230,400 ntuser.dat.LOG1
04/05/2011  14:29                 0 ntuser.dat.LOG2
04/05/2011  15:16            65,536 NTUSER.DAT{6cced2f1-6e01-11de-8bed-001e0bcd1
824}.TM.blf
04/05/2011  15:16           524,288 NTUSER.DAT{6cced2f1-6e01-11de-8bed-001e0bcd1
824}.TMContainer00000000000000000001.regtrans-ms
04/05/2011  15:16           524,288 NTUSER.DAT{6cced2f1-6e01-11de-8bed-001e0bcd1
824}.TMContainer00000000000000000002.regtrans-ms
04/05/2011  14:29                20 ntuser.ini
04/05/2011  14:29    <JUNCTION>     PrintHood [C:\Users\Dev\AppData\Roaming\Micr
osoft\Windows\Printer Shortcuts]
04/05/2011  14:29    <JUNCTION>     Recent [C:\Users\Dev\AppData\Roaming\Microso
ft\Windows\Recent]
04/05/2011  14:29    <JUNCTION>     SendTo [C:\Users\Dev\AppData\Roaming\Microso
ft\Windows\SendTo]
04/05/2011  14:29    <JUNCTION>     Start Menu [C:\Users\Dev\AppData\Roaming\Mic
rosoft\Windows\Start Menu]
04/05/2011  14:29    <JUNCTION>     Templates [C:\Users\Dev\AppData\Roaming\Micr
osoft\Windows\Templates]
               7 File(s)      2,130,964 bytes
              11 Dir(s)   6,889,426,944 bytes free

X:\rclone>dir /ah "C:\Users\Dev\Application Data"
 Volume in drive C has no label.
 Volume Serial Number is 5CD1-D250

 Directory of C:\Users\Dev\Application Data

File Not Found

X:\rclone>
```

So maybe rclone needs to be doing something different with windows junction points...

I note that `filepath.Walk` uses `Lstat` on each file name read like this

``` go
func readDirNames(dirname string) ([]string, error) {
    f, err := os.Open(dirname)
    if err != nil {
        return nil, err
    }
    names, err := f.Readdirnames(-1)
    f.Close()
    if err != nil {
        return nil, err
    }
    sort.Strings(names)
    return names, nil
}
```

rather than using `Readdir(1024)` like we do at the moment which stats each file.

This might fix #614 also.

I'll make a test with this a bit later and if it works I'll post a beta for you to try.
 Here is a beta with a potential fix: http://pub.rclone.org/v1.33-8-ga02273e%CE%B2/

Let me know how you get on!

Thanks
 I'll merge this to master now.
  Not sure exactly why.  Probably because FUSE is using lots of small buffers...

I'll investigate next time I'm looking at the FUSE code.
 Give this a try - I put some more buffering in as an experiment

http://pub.rclone.org/v1.34-39-g7be5724-fuse-buffer%CE%B2/ I've merged this to master now: http://beta.rclone.org/v1.34-48-gc24da0b/ (will be uploaded in 15-30 mins)  Your files seem to have a carriage return on the end of them which is really odd!

All the failing files are called `Icon^M`.  Here is an explanation of what those are: http://superuser.com/questions/298785/icon-file-on-os-x-desktop/298798

Backblaze (nor any other cloud provider) don't let you store a file with a control character in which is why they don't upload and why so many errors.

I think if it wasn't for those files then your sync would be working quite well.  There are always a few errors with cloud storage systems, but rclone will retry the sync and get there in the end.

So I think what I should do is substitute control characters (0x00-0x1F and 0x7F) in local.go `cleanRemote` (perhaps to %XX) this will make the sync complete properly.  Note that it won't back up the contents of the Icon files though as these are stored in the resource fork and rclone only copies the data fork.

I'll post a beta for you to try in due course here.
 ...in the mean time you could add `--exclude 'Icon?'` to your command line to not sync those files.
 I've fixed that in: http://beta.rclone.org/v1.33-71-ga02edb9/ (uploaded in 15 mins)

I decided to ignore the files with control characters in which seems perfect for this use case.
 It has appeared now. Sometimes Travis takes a bit longer her than 15 minutes. 
 The files with control chars weren't being backed up before. Those Icon files are the only examples I've found. 
  Looks like you are using a pre go 1.5 compiler. Check the output of

```
go version
```

You should have got a compile error from `cmd/versioncheck.go` about a missing function `Go_version_1_5_required_for_compilation` - maybe that check isn't working properly any more.

Can you try with a later go compiler?
 You can always use one of the prebuilt versions at rclone.org/downloads 
 Eg http://downloads.rclone.org/rclone-v1.33-linux-arm.zip
 This is fixed with vendoring now.  localhost.rclone.org is set to resolve to 127.0.0.1

what do you see if you do this (assuming you are using Windows, if using Linux/OS X then use `ping6` instead of `ping -6`)

```
ping localhost.rclone.org
ping -6 localhost.rclone.org
```

Which really should work, unless maybe your computer is IPv6 only?

I added an IPv6 address to localhost.rclone.org of `::1` too.

Does that help?
 Also it might be that you don't have the localhost interface set up.  You'll need to tell me which OS you are using if you want help with that!
 Did you make this work? I've changed the onedrive code to use  "http://localhost:53682" so this should be fixed now.

Please try https://beta.rclone.org/v1.36-41-gb651784/ and re-open if the issue is still present.  It looks like a total of 8 MBytes of files were transferred.

What do you see when you do `rclone ls secret:flibble/etc`

How are you looking to see whether files are transferred or not? What do you see on the web interface?

And in the definition of your crypt remote did you chose a bucket, eg `b2:bucket` as the docs recommend?  If you didn't then rclone will have created a new funny named bucket (lots of lower case letters and numbers) to store `flibble` in. That might be confusing things.
 Ah, so you put as your remote in `remote` rather than `remote:` or `remote:bucket`.  I'll try to make the docs a bit clearer here.  Maybe I can get the crypt to give a warning (though I do use crypt a lot to and from local disk for testing).

I'm glad you've got to the bottom of it, and I'll leave this ticket open to remind me to put some more stuff in the docs.
 I've fixed this with this text

```
Remote to encrypt/decrypt.
Normally should contain a ':' and a path, eg "myremote:path/to/dir",
"myremote:bucket" or maybe "myremote:" (not recommended).
```
  This is probably some error at the end of uploading the file to amazon which I'd quite common. I really need to see a log with -v to be certain of what is happening around the time when it goes from 100% to 0%. 
 That command looks good.  rclone writes continuously to the log file.  If you were on unix you would `tail` the file to see what is in it.  Not sure what the equivalent for windows is - sorry!
 There should be a FILE.log in the directory where you started rclone from. 
 > low level retry 1/1 (error HTTP code 500: "500 Internal Server Error", reponse body: {"message":"Internal failure"}

Is amazon erroring after receiving the file.

There isn't a lot rclone can do after that other than retry the file.  Hmm, the crypt remote doesn't support any kind of checksums (it can't as the underlying remote will store the checksum of the encrypted file) so effectively the `--checksum` option will be equivalent to `--size-only`.  This is something that will get fixed as part of #637 most likely.  Unfortunately Amazon Drive doesn't support modification times so,

I need to put this in the docs!

Note that the files themselves have a very strong authenticator so when you decrypt them you'll detect any corruptions.  This doesn't help for using `--checksum` though.
 @lsching17 good point.  I made an issue about this #700 - if you subscribe to it you'll get updates for it.
 I documented the checksums in 75e5e593854442ade4a48570e4d5bc4de0a4f5e3 and we've got #700 for the checksum command.
  This looks like an interaction between go's name resolution code and systemd. Go doesn't use the libc interface it will query 127.0.0.53 as if it was a real nameserver.

What happens if you do `dig  www.googleapis.com @127.0.0.53` does that work?  Try several times and see if you  get the same result.

I'm wondering if this is a bug in `systemd-resolved` - maybe systemd/systemd#3981 ?
 There was a fix to dns resolution in the latest go point release 1.7.1

https://github.com/golang/go/issues/16865

I'll compile a binary for you with it and you can see if that helps. 
 Try this beta and let me know what you think:

http://pub.rclone.org/v1.33-7-g54fdc68%CE%B2/
 OK thanks for trying.

Is it easy for you to try older versions of systemd-resolved to see if any of them work with rclone?
 I see!

I'm inclined to think this is a bug in this very new feature of `systemd-resolved` then.

Can you report it to them too and see what they think?
 Just noticed that there was a DNS resolver issue bug fixed in Go 1.7.1. Can  you try the latest beta which was compiled with Go 1.7.3 and see if that helps?

http://beta.rclone.org/v1.33-73-geca9e8e/
 :-(  - thanks for testing.
 Is there an easy way for me to replicate this myself?  Maybe you could point me at a virtualbox image for the OS you are using, or give me instructions on how to build one?
  Yes you are correct, this is a problem.

I'll get the oauth module to read in the config file first before changing it - that should narrow the window for bad things to happen greatly.
 I've fixed this problem.  There is a beta here: http://beta.rclone.org/v1.34-73-g1b2dda8/ (uploaded in 15-30 mins) for you to try if you want.  Yes rclone checks md5/sha1 where available for upload and download. 

Bwlimit is a global limit per rclone for upload and download combined. Note that not all the traffic is limited, eg directory listings only data transfers. 
 When rclone uploads a file it computes the checksum as it goes. When it is uploaded it queries the cloud provider for what it thinks the checksum is and checks they match. 

For download it asks the cloud provider for the checksum then downloads the file computing the checksum on the fly and checks it at the end. 
  Yes I need to put some metadata caching timeouts in you are right. I'll add it to the list. 

Thanks 

Nick 
 I've fixed that in: http://beta.rclone.org/v1.33-71-ga02edb9/ (uploaded in 15 mins)

With a new flag `--dir-cache-time` which has a default of 5 minutes.  It seems to work well in my testing.

@talisto re manual trigger of cache flush - I suggest you make another ticket with about that.
 Thx!
  Your only option is to install rclone at the moment.  PC is straight forward.  You can use Termux on android to compile and run rclone on Android...
  Thanks for the report. Will investigate further early next week when I'm back from vacation.
 I've fixed this - here is a beta with the fix in.

http://pub.rclone.org/v1.33-5-gb3d0848%CE%B2/

Please re-open the ticket if you find a problem with it.
 @ivy I fixed this again in 1d42a343d25b4ae6263fe2046c681f54ebee53dc - if you try the latest beta you should find it works fine.  Where is the encryption coming in? Are you using rclone's encryption too? If not are you using another encryption layer somewhere?

I'm trying to figure out where this error comes from: "failed to authenticate decrypted block - bad password?" 
 What it looks like to me is that the files are being truncated on download - amazon, or your ISP or your router is killing the connections.

Because rclone decrypts the file on the fly, when it gets an unexpected end of file, it will attempt to decrypt the last incomplete block and you'll get that "failed to authenticate decrypted block" error.

rclone should just retry the file though.

I'm not 100% satisfied with that explanation though... Can you send me a complete log (with the stats output too) so I can see the progress?  Email it to nick@craig-wood.com if you want to keep it private.

Thanks

Nick
 Any news on the log?
 Thanks for the log. I had a go at reproducing this without any luck.

What I suspect is that some external agent (Amazon, your ISP, your firewall, your PC) is closing connections prematurely and the error in decryption is masking the other error.

Could you try adding `--stats 1s` say, and see whether you can get it to do it again and get some idea of whether you think the file was at 100% or not when the error happened.
 Thanks for the log @DurvalMenezes 

I'm pretty sure that the problem is caused by some internet glitch interrupting the download, but rclone is reporting the wrong error.

I'll try and reproduce this in a test case
 Here is an experimental  fix for that error.  It will produce a different error instead of the spurious `failed to authenticate decrypted block` - something to do with the connection breaking would be my guess.

http://pub.rclone.org/v1.33-81-gef17f90-readfull-fix%CE%B2/

I'm on the trail of the underlying error, but I need to confirm I'm going in the right direction, so any testing appreciated.
 I think that probably wasn't the problem :-(  I've found a data corruption problem with crypt+seek which I've now fixed.  Have a go with http://beta.rclone.org/v1.33-81-g9d2dd2c/ (will be uploaded in 15-30 minutes) and tell me if that makes a difference.
 I found a bit of missing locking in the FUSE module which can also produce this error message. Here is a beta with the fix - let me know if it works: http://beta.rclone.org/v1.33-83-g8710741/ (will be uploaded in 15-30 mins)
 You should find this issue is completely fixed in v1.34 so I'm going to close this issue.  If you are still having problems, then please open a new issue with logs of the problem.
 @DurvalMenezes - could you try the test with: http://beta.rclone.org/v1.34-75-gcbfec0d/ ?  Thanks, Nick @dcplaya thanks - that is what I was hoping to see.  Unexpected EOF is caused by a networking problem somewhere. @DurvalMenezes Are you trying the latest rclone?  1.35 should do, or the latest beta?  That should work better.

You need to look in the log from rclone to see what is really happening (with -v)  This is being fixed in #665. I'm away at the moment - I'll merge it early next week. You could try that fix in the meantime. 
  Nice idea.
  You can compile rclone on Android using Termux and use it from the command 
line there.

I'm not currently building releases for Android though so you'll have to 
build it yourself.
 The ARM linux one doesn't have the right android bits in in.  android != linux especially when it comes to system services like DNS.

You can try this one: http://pub.rclone.org/rclone-android-arm-v1.32.gz

Or build one from source with Termux.
 I built the current code with termux and uploaded the result - try this.  It should work better than the linux-arm version.

http://pub.rclone.org/rclone-android-arm-v1.33.gz

I should probably write some instructions on how to do this - it isn't too hard.
  I'm away at them moment with limited Internet connection. I'll study this 
early next week.
 I've fixed this in this beta

http://pub.rclone.org/v1.33-6-g2eaac80%CE%B2/

Please re-open if it still causes problems
  Looks very nice. Will merge when I get back home early next week. 

Thanks 

Nick 
 I've merged that now (squashed and rebased) - very nice code :-)

Thank you very much for your contribution.

Here is a beta with it in: http://pub.rclone.org/v1.33-22-g7227a26%CE%B2/
  Is there any precedence for this in a different tool? I'm a great one for standards to follow if there are any!
 I think this feature would complicate rclone's internals too much, sorry, so I'm going to close this issue.  My plan for this is to implement #98 then implement a seperate higher level `rclone backup` command which would do a backup with ability to see previous backup snapshots up to some limit.

Each time you ran the `rclone backup` command you'd get a new snapshot.

I will spec it out a bit later
  I'm away at the moment - will investigate further next week. 

However I have a nasty feeling that uploads don't work with ACD via fuse because they need to know the file size in advance. 
 Yes I've confirmed this - you can't upload files to ACD with `rclone mount`. :-(  This is rather complicated to fix unfortunately - it might just be easier it I add a flag so rclone mount uses a temporary file for the upload.
 This should be fixed in http://beta.rclone.org/v1.34-22-g0b562bc/ (will be uploaded in 15-30 mins)

Please re-open if it doesn't work for you.
  A fine idea :-)
 @Fmstrat @sammcj It supports ssh-agent - you can just add your key to the agent can't you?  No passwords required... @pierrehenrymuller I think you'll find the sftp support in the latest beta should work perfectly for this  I think this is a duplicate of #637 where discussion of a higher security file name encryption is ongoing.  I'd appreciate your input there - thanks
  I'm away at the moment - will review next week. Did you try the integration 
tests? If not I'll give them a go next week. They will fail for definite if 
the escaping is wrong!

Thanks

Nick
 The main thing is does go test work in the cloud storage directory (and 
Google drive). You'll have to make a remote called TestGoogle CloudStorage 
IIRC for it to run all the tests.

You can then use that remote with the -remote flag to the fs tests.
 This fails the integration test :-(

```
=== RUN   TestObjectOpen
--- FAIL: TestObjectOpen (0.38s)
        Error Trace:    fstests.go:533
    Error:      Received unexpected error "bad response: 404: 404 Not Found"
```

This is trying to open the file "file name.txt" - removing the space from the file name causes the tests to pass which indicates to me that the file name escaping isn't working properly.

I've fixed this in 0c6990bc9505ac1c9602be3c41592a9a95d4b92f after realising that that whole complication isn't needed any more.

Thanks for your pull request, I've decided not to merge it though and I've gone with and alternative fix.
  Rclone is supposed to do that automatically.

However amazon drive's directory listings are very eventually consistent 
which make it hard.

Do you have lots of files which differ only by case? Eg FILE .TXT and 
file.txt that can cause the same problem as AD is case insensitive.
 Sound fine, but check you got the case exactly the same. If you didn't 
rclone will think they are different files and amazon will give that error.
 I'm going to close this now - please re-open if you find out more
  @hailthemelody if you try the latest beta, you'll find it has sftp support which works over ssh.  Most servers which support ssh seem to support sftp over ssh.  It doesn't support it yet, but it is a great idea. 

What I want to do is make any of the command line options for a remote work in the config file too. Thus would need a little bit of reorganisation but isn't too tricky. 
  I'm away at the moment with limited Internet (GPRS!) will study your issue 
when I get back next week.
 I looked through your log file and I didn't see any errors?  Did you log stdout and stderr to different places?  (By default the output for `-v` goes to stdout whereas the log messages go to stderr.)  You can use the `--log-file` option to get them in the same place.
 Hopefully `rclone dedupe` solved your problem.  If not please re-open!  I'm away at the moment. I'll investigate when I get back.
 Yes this is a duplicate of #653 - please subscribe to that issue for updates.
  Will have a think on the best way forward. I think there is an https domain name I could advertise. 

Note you can also download the releases from github which will be https.
 You can find the downloads with https here: https://downloads-rclone-org-7d7d567e.cdn.memsites.com/

And also here: https://github.com/ncw/rclone/releases/latest
  Though I might change the code which displays the passwords which would probably help too..
  It looks like it is working to me. 

It copied a single file  fsdfsdfds into the remote. 

Note that the directory path "test" has been encrypted to 0p5ohk94qrcijn5dqodttaag80 which is as it should be. 

If you do

```
rclone ls secret: 
```

What do you see? Hopefully you'll see "test/fsdfsdfds" in the output. 

If you done want the "test" part encrypted then you need to make a new Crypt with that in the remote. 
 No problems. I should probably attempt to make this clearer in the docs. 

I'm going to reopen this ticket to remind me. 
  Great idea. I'd like read only too. Have any suggestions for fuse apps with a good set of command line options I can look at? 
 On 28 August 2016 7:27:30 a.m. RXWatcher1 notifications@github.com wrote:

> https://github.com/yadayada/acd_cli/blob/master/acd_cli.py
> 
> I believe thats a complete mount option.

Thanks

> The allow-other is something I could really use. It seems like the mounts 
> are owned by root:root and not the user who executed the mount command. Is 
> that normal?

I didn't have any problems with Ubuntu 16.04 making and using the mounts.

Not at computer until next week so that is all I've got right now!
 Here is a beta with as many fuse flags as I could find. See `rclone mount -h` to see them!

http://pub.rclone.org/v1.33-13-g7be4c3a-fuse-flags%CE%B2/
 Yes I thought `suid` was probably not sensible on a cloud fs since rclone doesn't support custom permissions at all.

`use_ino` would mean rclone would have to make up inode numbers - might as well get fuse to do it so I think that option isn't useful.
 Thanks for testing - I'll merge this to master now.
  You can copy the config file and it will work just fine even on two machines at once. 

Rclone will refresh the token once it expires. 
  I'll have to study how enctyptfs works and see if this is possible. 
 It is a good idea. When I'm back at my desk I'll investigate how hard it would be. I  deliberately didn't follow encfs when making crypt as the crypto it uses isn't best practice any more, however a compatibility mode is a good idea if it isn't too much work. 
  Rclone definitely grabs new tokens. Check the time on your computer is set correctly - that can throw it off. 
 Glad you've found the problem. You shouldn't need to set client_id and client_secret in normal operation unless you've registered your own Amazon Drive app with Amazon. If you leave them unset rclone will substitute it's own. 

Have you got your own client_id and client_secret ? If not what did you put in? I'm amazed it worked at all if you didn't put a valid client_id  id in, though it would explain some bug reports.  Either way I'd like to get to the bottom of what is happening. 

Can you explain further what you mean by this

> when I refreshed the token manually (my ACD US security profile is whitelisted).

How did you manually refresh the token and what does it mean to have your security profile whitelisted - do you mean as part of the Amazon app registration process? It is a while since I did it for rclone and the details are hazy! 

Thanks 

Nick 
 Ah OK I understand now - thanks for explaining!

> While I'm glad I got it working, because I have so much data to migrate, I'm also wondering if this is possibly something you want me to test further.. Would it be of use to you if I put the client_id/client_secret entries back in and see if this is reliably reproducible?

As far as I know you are the first person to try this.  I think it should work, but it obviously doesn't so it would be interesting to get to the bottom of it :-)
  Just do a `rclone mkdir provider2:/data` first and then `--dry-run` will have a directory tree to work on and show you something sensible.
 Hopefully that answered your question
  Firstly rclone should always return a non zero exit code if there was an error, however...

The default behaviour of rclone would be to retry the whole operation 3 times or until successful.  So what I guess happened is that the first try failed but the second succeeded - hence the non-0 exit code.

... and because you are using -q you won't be seeing the "Attempt %d/%d failed with %d errors" about to retry errors in the log. You'll see these without -q.

Instead of using -q you could use `--stats=0` for a little bit more log but not much and you'll get to see the retries.

Maybe the retry logs should be visible with -q since there will already have been a report of the error.

Note that with -q mode you'll still get all error level reports.  It is a bit unfortunate (and unexpected) that that error message is a massive blob of html - that is Google's API doing something unexpected!
 Glad that is sorted! 

I'm going to use this issue to remind me to fix the logs with -q to shoe the retries
 I've fixed this here: http://pub.rclone.org/v1.33-19-g83ba597%CE%B2/

Please re-open if you find any problems with it
  I can't immediately replicate this - Can you make me a sequence of commands which will replicate the problem?
 Hmm. Strange. Is it a particular file that is the problem maybe? 

Also double check  the time and timezone are correct on your VPS as if they are wrong that can cause problems with token refresh. 
  Thanks - looks good. Will merge when I get back from vacation. 
 I merged 7cf6fe220948641e28c1d7e00cf458427b556c69 which fixed this - I forgot your PR was first - sorry!
  I've fixed this in the beta below.

Note that this has the unfortunate consequence that all B2 uploads with crypt will be buffered to disk first.  This is because B2 require the SHA1 of the object before it is uploaded.  They are planning to allow you to supply it afterwards in an update of the API which will fix this.

http://pub.rclone.org/v1.33-2-g140a3d0%CE%B2/
 Thanks for confirming that.

When uploading large files ( > `--b2-upload-cutoff`) the file won't be buffered to disk.  The individual chunks are buffered in memory as they were before.
  rsync has the `--ignore-errors` option which means delete even if there are I/O errors.  This woud be useful sometimes.

See #624 for discussion.
  You can't rename a remote at the moment no, but I think it is a good idea and not too tricky.

In the meantime editing the config file is your best bet.
 I've done this (along with copy) - find it in

http://beta.rclone.org/v1.35-50-g2192805/ (uploaded in 15-30 mins)  Yes you are absolutely right.  I think I'll put an example in there too, something like

```
Remote path to encrypt/decrypt (eg remote:path/to/dir)
remote> 
```
 I've clarified the above in d83074ae05e8ec79ebca745c76da86894180204c and fe53caf99777944b7498423f9e7318c05e82c1c9
  I see what happened - everything was going swimmingly until something else opened the file and closed it again.

This is most likely a bug as I had problems with that particular bit of code in development.

```
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Open
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Open OK
2016/08/25 11:30:06 fuse: -> [ID=0x4b8] Read 4096
2016/08/25 11:30:06 fuse: <- Read [ID=0x4b9 Node=0x2 Uid=1000 Gid=1000 Pid=6468] 0x1 4096 @0x4af000 dir=false fl=0 lock=0 ffl=OpenReadOnly
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Open
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Open OK
2016/08/25 11:30:06 fuse: -> [ID=0x4b9] Read 4096
2016/08/25 11:30:06 fuse: <- Flush [ID=0x4ba Node=0x2 Uid=1000 Gid=1000 Pid=6496] 0x1 fl=0x0 lk=0x6d3730d5a34b8c1a
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Flush
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Flush OK
2016/08/25 11:30:06 fuse: -> [ID=0x4ba] Flush
2016/08/25 11:30:06 fuse: <- Read [ID=0x4bb Node=0x2 Uid=1000 Gid=1000 Pid=6468] 0x1 4096 @0x4b0000 dir=false fl=0 lock=0 ffl=OpenReadOnly
2016/08/25 11:30:06 [GJM] Planetarian ~Chiisana Hoshi no Yume~ - 01 [575CC989].mkv: ReadFileHandle.Open
2016/08/25 11:30:06 fuse: -> [ID=0x4bb] Read error=EIO: Attempt to use closed file handle
2016/08/25 11:30:06 fuse: <- Read [ID=0x4bc Node=0x2 Uid=1000 Gid=1000 Pid=6468] 0x1 4096 @0x4b0000 dir=false fl=0 lock=0 ffl=OpenReadOnly
```

Thanks for the report - will attempt a fix and put a beta for you to try here.
 Have a go with this beta and see if it fixes the problem: http://pub.rclone.org/v1.33-17-g7b80335-fuse-seek%CE%B2/

Note that seeking doesn't work over crypt yet so it might not work for you yet!
  Follow on from #219 - see there for lots of ideas

Implement higher security file name encryption and overcome the file name length problem.

Perhaps
- create an index file for each directory which maps uuids to name and metadata (mod time, sha, md5 etc)
- put the same metadata in each file (encrypted) so that the index files can be rebuilt if they get corrupted.
- files named with a uuid - look up the name in the index
- perhaps flatten the structure which would obscure directory trees also.
- could also prepend some random padding to obscure file sizes
- could also use this to segment large files which would be very useful
 @romusz  yes you can just terminate the read - I think that will work with all the remotes.  It will break the underlying ssl connection though which would otherwise be re-used by a subsequent request and remaking ssl connections is reasonably expensive, but if it saves downloading the whole file it is a big saving.

The index could be kept locally... I'm not terribly keen on the idea though, however the index could be rebuilt if you need to backup from scratch.
 @rourke750 can you make a separate issue about the path thing please with some steps to reproduce? The integration test check that works but there are a lot of combinations and I may have missed something. 

Confusingly the passwords it shows you when it summarises the config for the remote are the encrypted ones. I should probably fix this, so can you make another issue about this too please. 

Thanks 

Nick 
  Rclone will only convert google docs - was the original xlsx a google doc?  If you want to convert it to a google doc right click on it and select Open with ￼ Docs (or Sheets or Slides).
 No problems!
 Sorry I misunderstood. 

Rclone will ignore any  export formats if it can just download a file. 

I suspect your xlsx files are both files and have export formats - not a combination I've seen before. 

I'll try to reproduce when I get back from vacation and if I can't I'll get you to send me more info. 
 Can you run this command

```
rclone ls -v --dump-headers --dump-bodies --log-file issue-636.log  drive:GDocs
```

replace `drive:GDocs` with a path to a folder with some docs that you are having trouble with then email me the file `issue-636.log` (to nick@craig-wood.com ) - I'll analyse it.. Let me know the names of the files you are having a problem with.  If you stick the issue number in the subject then that will help me :-)

You can remove any `Authorized:` line from the file if you like to remove your login information. 
 OK, no worries!

I'll close this then, but do re-open if more info comes to light.

Thanks

Nick
  Great!

I've written a few comments inline.

Also can you write something about the new flag in the s3 docs please?

Thanks

Nick
 Normally you would do it like this

```
git checkout master
git pull
git checkout your-branch
git rebase master
```

However you've used the master branch, so I think you'll need to do

```
git pull --rebase
```

Then use this to squash the commits (you'll need to read up on exactly how it works)

```
git rebase -i commit_number
```

Use `git log` to pick the commit number before you started making modifications.

You then need to push it with `--force` to the remote branch.

However I'm happy to do that if you don't feel confident doing it.

I was hoping for an update to `docs/content/s3.md` describing the new flag if you've got time?
 Something strange happened to your docs update - it looks like it got lost in a merge, so sorry I didn't see it.

I rescued the docs update from the diff.

This turned into rather a tricky merge so I have done it and committed it to master in bc414b698dd3acea3762a908f41612f451e36f68

Sorry for the delay, and thank you very much for your contribution :-)
 Thanks @radek-senfeld - I've merged that in cb40511807a03e922bd3fdd477fa41191bbfa62a - I managed to miss the notification when it came in so apologies for the delay!
  I think @zeshanb is right - google don't seem to be offering `odp` as an export type for presentations, only `pptx`.

Note that ods appears twice in the table because there are two subtly different spellings of the mime type for it.

When I query a presentation for export formats I get this - so no ODP :-(

``` javascript
  {
   "source": "application/vnd.google-apps.presentation",
   "targets": [
    "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    "application/pdf",
    "text/plain"
   ]
  }
```
 No problems - sorry we can't fix this one!
  Well spotted.  I'll fix that up, thanks
 This will appear on the website after I release 1.33 - thanks for spotting this.
  Thanks for looking at this.

To finish the feature you would need to
- add a config section - find out the possible values for the acl and let the user choose them in `rlcone config`
- update the docs `docs/content/s3.md` - update the config walk through and add a new section about acl
- I think `perm` should be renamed `acl`
- Search for issues which you could close by implementing this (there is at least one) and mark your commit with `fixes #xxx`

>  but subsequent sync doesn't seem to check/fix existing ones

rclone doesn't sync any metadata except the modification time so I would think this is probably out of scope for rclone. The closest thing might be to make sure `SetModTime` updates the acl properly, then you tell the user to touch any files which need their permissions changing and do a sync.
 PS I'm happy to copy edit any docs
 Excellent!

If you could run the code through gofmt (that is why the build is failing) and squash all into one commit, I'll merge - thanks :-)
 Looks absolutely perfect - well done!

I've merged it (I rebased it as I'd committed something in the mean time).

It occurs to me that there should probably be a command line option to set this too (`--s3-acl` would be the normal name) to override the default. Fancy having a go at that?  If you do then update your master branch and start from there with a new pull request. 

FYI when you removed the "FIXME" gofmt wanted to reformat the bit of code that it came out of so it was your change that caused the problem.  Most go developers install [goimports](https://godoc.org/golang.org/x/tools/cmd/goimports)
 as a pre-write hook in their editor which makes life very easy
  Are you updating the same file over and over again?

The 409 conflict suggests it is this propblem: http://docs.openstack.org/developer/swift/overview_large_objects.html#retrieving-a-large-object

So I might need to update the manifest after an update.

Can you send a log with -v of the whole upload please?  Attach it as file.

Which cloud provider are you using? Is it vanilla openstack or ceph?
 Can you answer the questions in the above?

Thanks

Nick
  I've fixed this already hopefully!  See the comments in #605 and the beta I posted at the end.

http://pub.rclone.org/v1.32-20-g2ebeed6%CE%B2/
 This fix is now in the v1.33 release.
  I quite like the idea of having a cache for a remote, so you could have a cache for the local file system or a cache for the remote filesystem or both.

It would be tricky to implement though!
  This is a good idea, but it is a duplicate of #497 which I've renamed to be easier to find!

Can you subscribe to that ticket please?
  I think an exclude should fix it like this,

```
rclone -v --exclude "/.sync/**" sync Dropbox:Folder/ ~/www/Folder/
```

should do it which means exclude an object called ".sync" at the sync root (hence the leading `/`) and all the stuff under it - the `/**`.

`--exclude .sync` excludes any **file** called sync anywhere in the hierarchy.
 I think 460 means that that file possibly has a virus - if you contact dropbox they can help you apparently.

The not deleting files if there was an io error is a safety feature.

I note that rsync has the `--ignore-errors` option which means `delete even if there are I/O errors` - I could implement that.
 I've put the ignore errors into #642 if you want to subscribe to that.

I'm going to close this ticket now in favour of that one.
  Can you paste me the result of this done in the terminal please

```
ls "Archived Music/Into It. Over It_/Standards/" | grep "Who You Are" | od -bc
```

I can then take a look at what encoding OSX is using on the disk.

What type of filesystem are you reading from?  Is it HFS+ or something else?

I think what I need to do is store the filesystem name that OSX thinks it is if it is different to the normalised name which everything else needs.

Background reading:
- [stack overflow](http://stackoverflow.com/questions/6153345/different-utf8-encoding-in-filenames-os-x)
- [apple tech docs](https://developer.apple.com/library/mac/qa/qa1173/_index.html)
 Yes this is normalization problems.  Here is what it says on your filesystem

```
>>> print '=\xcc\xb8'
≠
```

Which is an = sign followed by a 'COMBINING LONG SOLIDUS OVERLAY' (U+0338)

And here is what it is after normalisation

```
>>> print '\xe2\x89\xa0'
≠
```

Which is a single unicode symbol 'NOT EQUAL TO' (U+2260)

This is definitely a bug.  I'm going to put it in the release after next as I'm going to have to borrow a mac to fix it!
 As part of fixing #628 I may have fixed this issue too.

Can you try this beta please? http://pub.rclone.org/v1.32-29-gbbccf4a%CE%B2/

Thanks

Nick
 Excellent - thanks for testing.  This will be in the v1.33 release.
  Nice idea.  Can anyone get me a free account?  I'd need one that doesn't stop working after 30 days as rclone is continuously tested.
  The PR looks good - thanks - will merge in due course.

I broke the windows build a while back and haven't fixed it yet (naughty me) so not your fault.
 Thank you for the PR. I actually received advanced notice of this from B2 but it was under NDA so I couldn't make an issue about it so forgot - thanks for doing it for me :-)
  This is probably related to #218 .

The `+` being translated to ` ` means that it is to do with URL encoding going wrong.

> when OneDrive replaces invalid characters like ~ with ～

It is rclone doing those replacements. I suspect the reason why they don't display is that your terminal is not displaying UTF-8 characters.  rclone expects everything to be using UTF-8.
 I've fixed this in the beta

http://pub.rclone.org/v1.32-34-g037a000%CE%B2/

Which will be part of the v1.33 release

Please re-open if you find any problems with it!
  Currently the key generation from the user's password is done with sha256 for the config file. It has pre and post strings so isn't vulnerable to rainbow table attack, but using sha isn't best practice any more as it isn't computationally expensive enough. 

We should upgrade to using scrypt with salt we store in the encrypted config file (or maybe reuse the IV for the secret box encryption). This can be done transparently to the user. We'll need to continue to read the old format though. 

Also document the format
 We are using sha256 as a key derivation function.  [scrypt](https://godoc.org/golang.org/x/crypto/scrypt) was purposed designed as a key derivation function, to turn user input into a variable number of random looking bits in a hard to do way (see the `keyLen` parameter in the docs).  So you would do `scrypt(scrypt.Key(password, salt, 16384, 8, 1, 24)` to directly generate the 24 byte key for secretbox.

From what I read scrypt is considered more secure than bcrypt - using more memory means it is more FPGA/ASIC resistant.

An alternative key derivation function would be [pbkdf2](https://godoc.org/golang.org/x/crypto/pbkdf2) which I haven't investigated in depth - that might be a better choice than scrypt.

I believe that we can afford to run scrypt at the default parameters even on a puny NAS - it uses 16 MB of RAM according to its benchmark.  I don't really want to have tune-able parameters on the key derivation function though - that would just complicate things.

bcrypt, pbkdf2 or scrypt would all make better key derivation functions than sha256 and we are lucky to have an implementation of each in the golang.org/x/ repository. In the interest of not bloating rclone with too much crypto it would be nice to use the same key derivation function here and in the upcoming file encryption.  I've used scrypt there and it was very useful in generating the 80 bytes of key material needed, but it isn't released yet so I could change it to something else.
 ;-)

I take your point about the memory usage and security.

[OWASP's advice is interesting](https://www.owasp.org/index.php/Password_Storage_Cheat_Sheet)

It rates them like this
- Argon2 when it becomes available. Argon2 is the winner of the password hashing competition and should be considered as your first choice when solid implementations are available.
- PBKDF2  when FIPS certification or enterprise support on many platforms is required;
- scrypt where resisting any/all hardware accelerated attacks is necessary but support isn’t.
- bcrypt where PBKDF2 or scrypt support is not available.

There is a pure go argon2 implementatino but it has too many warnings on it for me to want to use it!  I don't want to use cgo in rclone.

pbkdf2 also has that arbitrary key length output and has the advantage of being somewhat industry standard (PKCS #5 v2.0, RFC 2898, recommended by NIST).  What do you think of that?
  I'll certainly consider it - thanks for the suggestion.
  It isn't possible at the moment.  For S3 you can read the auth info from the environment or iam but you still need a config file.

It is a good idea though.  Perhaps the config file could be passed as an environment variable in some form?
 This is now done.  Please see this beta

http://beta.rclone.org/v1.35-13-ge1a49ca/ (uploaded in 15-30 mins)

Here are notes from the docs

Environment Variables
---------------------

Rclone can be configured entirely using environment variables.  These
can be used to set defaults for options or config file entries.

### Options ###

Every option in rclone can have its default set by environment
variable.

To find the name of the environment variable, first take the long
option name, strip the leading `--`, change `-` to `_`, make
upper case and prepend `RCLONE_`.

For example to always set `--stats 5s`, set the environment variable
`RCLONE_STATS=5s`.  If you set stats on the command line this will
override the environment variable setting.

Or to always use the trash in drive `--drive-use-trash`, set
`RCLONE_DRIVE_USE_TRASH=true`.

The same parser is used for the options and the environment variables
so they take exactly the same form.

### Config file ###

You can set defaults for values in the config file on an individual
remote basis.  If you want to use this feature, you will need to
discover the name of the config items that you want.  The easiest way
is to run through `rclone config` by hand, then look in the config
file to see what the values are (the config file can be found by
looking at the help for `--config` in `rclone help`).

To find the name of the environment variable, you need to set, take
`RCLONE_` + name of remote + `_` + name of config file option and make
it all uppercase.

For example to configure an S3 remote named `mys3:` without a config
file (using unix ways of setting environment variables):

```
$ export RCLONE_CONFIG_MYS3_TYPE=s3
$ export RCLONE_CONFIG_MYS3_ACCESS_KEY_ID=XXX
$ export RCLONE_CONFIG_MYS3_SECRET_ACCESS_KEY=XXX
$ rclone lsd MYS3:
          -1 2016-09-21 12:54:21        -1 my-bucket
$ rclone listremotes | grep mys3
mys3:
```

Note that if you want to create a remote using environment variables
you must create the `..._TYPE` variable as above.

### Other environment variables ###

  * RCLONE_CONFIG_PASS` set to contain your config file password (see [Configuration Encryption](#configuration-encryption) section)
  * HTTP_PROXY, HTTPS_PROXY and NO_PROXY (or the lowercase versions thereof).
    * HTTPS_PROXY takes precedence over HTTP_PROXY for https requests.
    * The environment values may be either a complete URL or a "host[:port]" for, in which case the "http" scheme is assumed.
 Yes that was an consequence of the new logging levels that the fact you can have `-vv` which is the same as `-v=2`.  Thanks for pointing it out! @vtolstov not currently.  If you want to see them all you can do this

```patch
diff --git a/fs/flags.go b/fs/flags.go
index 8b221ba..05a9ecb 100644
--- a/fs/flags.go
+++ b/fs/flags.go
@@ -244,6 +244,7 @@ func optionToEnv(name string) string {
 // sets the default from the environment if possible.
 func setDefaultFromEnv(name string) {
        key := optionToEnv(name)
+       log.Printf("env for %q is %q", name, key)
        newValue, found := os.LookupEnv(key)
        if found {
                flag := pflag.Lookup(name)
```  There is a framework for dealing with this now and it should be easy enough to do.
  Are they all .zip files maybe?  I wonder if you have some extension to Windows which is treating these in a special way which confuses rclone.

Can you run this program with the full path to one of those files and send me the output please?

http://pub.rclone.org/stat-windows-386.zip

Eg

```
C> stat.exe "KeePass-2.34-Portable.zip"
```

Thanks
 Hmm, those mode flags indicate that that zip file is a symlink

Looking at the go source I see

``` go
    if fs.sys.FileAttributes&syscall.FILE_ATTRIBUTE_REPARSE_POINT != 0 {
        m |= ModeSymlink
    }
```

And according to the msdn docs

FILE_ATTRIBUTE_REPARSE_POINT 1024 (0x400): A file or directory that has an associated [reparse point](https://msdn.microsoft.com/en-us/library/windows/desktop/aa365503%28v=vs.85%29.aspx), or a file that is a symbolic link.

A bit more digging [reveals this](http://answers.microsoft.com/en-us/windows/forum/windows_7-performance/what-is-a-reparse-point-can-anyone-reveal-the/17b9b457-6c8a-4e83-a445-e603011a6b95) - the second answer says

> I also use 64-bit Windows 7 and generally love it, but was stumped for awhile with this same backup problem. Apparently reparse points are related to or associated with backup restore points in some manner. I had moved files to a new physical disk and partition I installed recently. Backups of the moved files were failing with the 0x81000037 error. I followed the steps below and was able to get a successful backup.
1. Open Windows Explorer and right click the drive letter where the files are stored that cannot be backed up. Click Properties.
2. From the General tab click "Disk Cleanup".
3. On the Disk Cleanup window, switch to the More Options tab.
4. In the System Restore and Shadow Copies section, click "Clean up..."
5. Click Delete

> This initially made me nervous about possible file deletion, so before testing I made another copy of the files to a different partition - a backup of my backup I guess. However this process apparently only deletes restore point information associated with files on a given partition. Once that was done, backups worked fine.

So you could try that and see if it improves things.

I could also ignore the ModeSymlink on windows which may or may not do the right thing, I'm not sure!
 Interesting! When I get back from vacation I'll send you a build which ignores the junction point and we can see what happens. 
 Here is a beta with a potential fix: http://pub.rclone.org/v1.33-8-ga02273e%CE%B2/

Let me know how you get on!

Thanks
 Did this work?  (Note to self - this is in the `ingore-junction-points` branch!)
 Thanks for checking - I'll merge this to master now
  No, rclone doesn't limit your file upload size.  It is possible Amazon have raised the limit - let me know if you succeed with files > 50GB.
 Checking the log out, each time where it managed to upload 100% of the file, it gave a "500 Internal Server Error" which I think is Amazon's way of saying it isn't supported :-( 
 Amazon have been strangely coy about publishing file size limits.  50GB is the limit most talked about though.  Try asking customer support rather than API support.
 I'm going to close this now as I think we've established that 50G is still the limit.
  I suspect this is the issue discussed in [the rclone local docs](http://rclone.org/local/) - see the filenames section, namely that you have non UTF-8 file names.

This is easy to fix with convmv though.
 @trolley that is probably a different problem - part of the never ending OSX file name normalisation saga (sigh!). Can you make a new issue please and if you could cut any paste the filename as 'ls'  sees it too then I'll compare the Unicode code points. Thanks 
 @austinginder Great - glad it worked! Though it looks like that filename has really been through the works.  I suspect the sequence `â%80%93` actually represents the utf-8 sequence `\xe2\x80\x93` which is Unicode Character 'EN DASH' (U+2013) or `–`.
  Ha!  That looks like a bug ;-)  Will fix as part of the beta - thanks.
 This is fixed now: see http://pub.rclone.org/v1.32-20-g2ebeed6%CE%B2/
  Your analysis is spot on and I agree with you.

The reason rclone works as it does is so that it can re-use the sync machinery for single file copies.

However it ocurrs to me that these could be done in a much simpler way if just copying a single file - I don't know why I didn't think of that before!
 Potentially all operations which makes sense on a single file can use this (eg `delete`)
 I've fixed this in http://beta.rclone.org/v1.33-55-g98804cb/ (it will take 15 mins to appear).  Have a go and re-open the ticket if there is a problem.

Thanks

Nick
  Are you letting users use rclone directly or are you scripting it for a specific task.  If the latter then you should probably run the rclone job as a separate unix user (or even root) so the user won't be able to read any of the config.
 So running rclone as root (say) doing something simple like `rclone lsd remote:` to refresh the token, then editing the rclone.conf removing the secret stuff except for the token and copying that .rclone.conf into the users home directories.  That should work just fine I would have thought.  If the users are doing really long transfers which span the hour of the token lifetime then rclone won't be able to refresh it though.
 I hope that was of some help!
  I suspect this has been caused by #451 and 7c01bbddf8a7ccde05554633b0165f3711d9fdc4.

This means that it will only affect accented characters as you suspect.

What you will find is that when you look very carefully at the filenames they are different but they appear the same.  Fixing that was the point of #451.  How they got there is to do with OS X file systems - for some reason they store unnormalised UTF-8 in them - rclone used to upload them without normalizing them (which was bad).

So I think what rclone did was correct, however that leaves you with a whole pile of duplicate files.

I would have hoped that if you let the sync continue to the end it will delete the unnormalized file names - does it?

If it doesn't then I can extend `rclone dedupe` to do it quite easily - I don't think it will do the right thing at the moment though.
 Did it finish OK?
 Great glad it finished.  rclone doesn't remove empty folders yet (see #100) so that is a success I think!
  Sure

```
rclone ls remote:path > output.txt
```

Should do something like that. 
 Hope that helps!
  I haven't found a way to avert this yet, sorry :-(  This has come up recently in #601 - if you look in there you'll see links to the amazon developers forum where this issue has been discussed.

There seem to be two types of errors
- 408 REQUEST_TIMEOUT
  - 504 GATEWAY_TIMEOUT

There is speculation on the developers forum that this is caused by hashing on the remote end before the file becomes visible, but the file has actually been uploaded OK.

What I could do as a workaround is if I get one of those errors, wait for a bit (say for 1 minute) and check to see if the file is actually uploaded.  If it is then don't return an error, otherwise return the gateway error.

I've made a patch with this idea for you to try.

 http://pub.rclone.org/rclone-v1.32-16-gf18e289.zip

If you could upload a log with -v of how it works that would be much appreciated.

Thanks

Nick
 Here is a full build for all architectures

http://pub.rclone.org/v1.32-16-gf18e289%CE%B2/
 @isaiah36 Would really like to see a log with -v if possible.  I can't do anything to fix the restarting before 99% that really is Amazon's problem :-(
 Here is a new attempt at a fix.  May fix the partial uploads, but may explode as it isn't very well tested.

http://pub.rclone.org/v1.32-18-gaffc483%CE%B2/
 OK here is another attempt, a bit better tested:

http://pub.rclone.org/v1.32-18-gaffc483%CE%B2/
 With correct URL this time: http://pub.rclone.org/v1.32-18-g29ab7ea%CE%B2/ 
 @Conroman16 thanks for that and for your analysis.

Looks like maybe it could do with waiting for longer than a minute for the file to appear. I don't know what to think about that 500 error - maybe it did get uploaded properly and #559 would fix it.
 I've reworked the patch again with the following changes.  On a failed upload we
- detect if we uploaded the file to 100% - if we didn't do nothing
- do this for any upload error (so might get round the 500 error in @Conroman16 log)
- increase timeout to 2 minutes - timeout might need to vary with file size?
- made timeout configurable with `--acd-upload-wait-time` - perhaps set it large when uploading huge files?

Here is the beta: http://pub.rclone.org/v1.32-18-gf7562dc%CE%B2/
 I've merged this to master and it will be in the v1.33 release - thanks for your help testing.
  I suspect this is the same problem as #601 which unfortunately is an Amazon problem which I can't fix :-(

If you supply a log with `-v` I can confirm.

> I removed the file from the directory completely but the retry is carrying on merrily on its way! Does rclone cache the whole file before uploading? 

No rclone doesn't cache the file.  On unix you can delete an open file and the processes which have it open will carry on just fine.  Windows prevents you from deleting open files at all.

> Is there a command modification that will tell rclone not to retry failed uploads?

Checkout the `--retries` flag and also the `--low-level-retries` flag.
 I posted a potential fix for this in #606 if you fancy giving it a go.
 Here is a new attempt at a fix.  Note that it only stands a chance if your file gets to nearly 100% - if it is cut off much less then that then the only hope is to retry it.

http://pub.rclone.org/v1.32-18-gaffc483%CE%B2/
 OK here is another attempt, a bit better tested:

http://pub.rclone.org/v1.32-18-gaffc483%CE%B2/
 With correct URL this time: http://pub.rclone.org/v1.32-18-g29ab7ea%CE%B2/ 
 I've reworked the patch again with the following changes.  On a failed upload we
- detect if we uploaded the file to 100% - if we didn't do nothing
- do this for any upload error (so might get round the 500 error)
- increase timeout to 2 minutes - timeout might need to vary with file size?
- made timeout configurable with `--acd-upload-wait-time` - perhaps set it large when uploading huge files?

Here is the beta: http://pub.rclone.org/v1.32-18-gf7562dc%CE%B2/
 You want --acd-upload-wait-time="5m" 
 I've had a look at that thanks.  It looks like the token refreshing is working, but rclone didn't wait long enough for objects to appear.

Can you try the latest beta (see above)?

Thanks for testing - Nick
 I've merged this to master and it will be in the v1.33 release - thanks for your help testing.

Please re-open if you see any problems.

Here is a beta with the latest code in: http://pub.rclone.org/v1.32-20-g2ebeed6%CE%B2/
  If we don't do that then we get left with the hide markers which show in the web interface.

```
$ mkdir /tmp/cleanup-test
$ echo one > /tmp/cleanup-test/file1.txt
$ rclone -q sync /tmp/cleanup-test b2:cleanup-test
$ rclone --b2-versions -q ls b2:cleanup-test
4 file1.txt
$ rm /tmp/cleanup-test/file1.txt
$ rclone -q sync /tmp/cleanup-test b2:cleanup-test
$ rclone --b2-versions -q ls b2:cleanup-test
4 file1-v2016-08-02-133808-000.txt
$ rclone -v cleanup b2:cleanup-test
2016/08/02 14:39:08 file1.txt: Not deleting current version (id "4_z9f821d93605dab7b5f580e1b_f106f5a6bb6795c41_d20160802_m133842_c001_v0001031_t0029") "hide"
2016/08/02 14:39:08 file1.txt: Deleting (id "4_z9f821d93605dab7b5f580e1b_f106823c0648d1986_d20160802_m133808_c001_v0001008_t0000")
$ rclone --b2-versions -q ls b2:cleanup-test
$
```

However if I look in the b2 interface I can see that it says "file1.txt (hidden)"

So on cleanup we need to delete the current file if it is in status `hide` too.
 This is fixed now it will be in the v1.33 release

Here is a beta with the fix: http://pub.rclone.org/v1.32-22-gb4f2ada%CE%B2/
  An interesting idea - thanks.

My original suggestion in #111 was a little less wide ranging in that when you `Put` a `fs.Object` the swift remote could then do a type assertion to a `swift.Object` and if successful it would know how to read the metadata out of it.

Defining an interface for Metadata is taking this one step further and is an interesting idea. Most of the remotes (with a notable exception of dropbox) can store metadata in one form or another.

So I guess here you are just seeing what it would look like on the local file system.

`github.com/davecheney/xattr` doesn't use cgo which is good, however it isn't supported on Windows so you'll have to abstract that to another file you conditionally compile.  Take a look at `redirect_stderr.go`,  `redirect_stderr_unix.go` , `redirect_stderr_windows.go` in the root if you want an example. That is needed to fix the windows (AppVeyor) build.

One thing to think about... Is `rclone copy` or `rclone sync` going to guarantee that the metadata is synced properly too? If so the core syncing algorithms will need a bit of work.  If we are just saying that the metadata will get read when the file is copied (which might be acceptable) then what you've got is fine.
 For proper syncing you'd need a `Set` method too in the interface, or a separate interface I suppose.

Splitting the PR is probably a good idea anyway - I'd really like to see some more implementations for other Fses before merging though.

> In addition, what would you think of having rclone.go split so that most of its source code, and its "redirectStderr" dependency, are moved outside the "main" package? I'm implementing a custom fs that I'm plugging into rclone. Today, I need to copy rclone.go and import my own fs just after importing _ "github.com/ncw/rclone/fs/all", which is a bit ugly.

I see what you mean...

If rclone main was just calling a function called main in a different package, then you could do what you want.

Ideally go would support dynamic loading of modules and we'd be able to load fses at run time.

Can you make an issue about it please?   But don't send a PR as I'm in the middle of completely reworking it to use cobra!
 I've finished the cobra re-org and shuffled the internals of rclone into `cmd` in 0a7b34eefc63e38400e362887b956373832e5523.  I left rclone.go importing the fs so you can choose to import which ones you want.  Hopefully that refactor will work for you.
 I'm going to close this now as it was only a proof of concept - look forward to the next iteration :-)
  The file progress on B2 isn't working properly at the moment. You can see your file is being Uploaded by the 428 kBytes/s at the top of the status line. 

If you just wait for it to finish it should be fine. 
 I'm going to re-open this to remind me to fix the B2 status on uploads as it is confusing a lot of people!
 I've fixed this in the beta

http://pub.rclone.org/v1.32-34-g037a000%CE%B2/

Which will be part of the v1.33 release

Please re-open if you find any problems with it!
  I think this is a problem at Amazon's end...

If you check out the [developer forum](https://forums.developer.amazon.com/spaces/87/Cloud+Drive.html) you'll see various relevant threads ([one by me](https://forums.developer.amazon.com/questions/22563/408-request-timeout-errors.htm)!).
 Yeah, I guess so. The problem has definitely gotten worse lately. Thanks for pointing me to the forum.
 [Here is the thread](https://forums.developer.amazon.com/questions/22563/408-request-timeout-errors.html) I missed the last character off the URL when pasting it!

I can't decide if it is a new API or just a wrap of the existing API for android & IOS but I sent off a request for an invite and I'll let you know!
 I posted a potential fix for this in #606 if you fancy giving it a go.
 I'll try this over the weekend and see how it works!
 Sorry, but would it be possible to build a version for Windows x64? My home server runs Windows Server 2012 R2.
 See #606
 Note to self - maybe combine this fix with #559
 Looks like the problems on Amazon's end are getting worse. From looking at the log, there have been several 504 errors mid-transfer, as opposed to at the end. 

The patch seems to have mostly worked on the occasion where the error came at the end. It waited, then looked again and found the file with the correct size and indicated that it was returning with no error. However then it threw this error

`Received error: %!v(PANIC=runtime error: invalid memory address or nil pointer dereference)`

And started the file all over again.

I'll upload the entire log on Monday so that there's an entire weekend's worth of data to pore through, but as of this point I've uploaded 175 GB worth of data and managed to successfully complete one (1) file. Something is very wrong on Amazon's end.

Edited to add: I'm not seeing any of these problems on smaller files. Even files up to 3GB or so upload and complete smoothly.
 I think there might be a bug in the patch.

Here's what happens after any file reaches 100% but then gets a timeout error:

```
Timeout error detected - waiting to see if object was uploaded correctly: HTTP code 504: "504 GATEWAY_TIMEOUT", no response body
Object found with correct size - returning with no error
pacer: Rate limited, sleeping for 150.761883ms (1 consecutive low level retries)
pacer: low level retry 1/1 (error <nil>)
Received error: %!v(PANIC=runtime error: invalid memory address or nil pointer dereference) - low level retry 2/10
Removing failed copy
Resetting sleep to minimum 20ms on success
```

I think it's seeing that it's uploaded correctly and is the correct size, but then it throws the PANIC=runtime error and decides it's a bad copy, deletes it, and starts over from the beginning again.
 Here's the full log from this weekend's attempt. 
[test.txt](https://github.com/ncw/rclone/files/407133/test.txt)
 Very useful log, thanks.

I can see that if I hadn't made a mistake this would have worked 15 times and not worked 23 times which is not bad.

The time for a file to appear after the TIMEOUT error was from 0-15 seconds which is useful information.

At `2016/08/06 04:04:3` something interesting happened.  Your token expired and at the same time all 6 of your uploads died, none of them near 100%.  Then for 10 minutes or so ACD timedout all your uploads - in fact most of the errors seemed to happen about here.

I see what happened there - because all 6 of the upload threads were busy, the token didn't get renewed in a timely fashion. The previous token renewal was at `2016/08/06 01:55:51` so 2 hours and 10 minutes before. The tokens normally only last for one hour.

I'll have a go at fixing that too and that should make things much more reliable.

I've fixed the mistake in the original patch (hopefully!) - here is a new one with the fix for the token timeout also (not very well tested!).

http://pub.rclone.org/v1.32-18-gaffc483%CE%B2/
 OK here is another attempt, a bit better tested:

http://pub.rclone.org/v1.32-18-gaffc483%CE%B2/
 I'll give this latest build a spin throughout the week and let you know the results!
 With correct URL this time: http://pub.rclone.org/v1.32-18-g29ab7ea%CE%B2/ 
 I've reworked the patch again with the following changes.  On a failed upload we
- detect if we uploaded the file to 100% - if we didn't do nothing
- do this for any upload error (so might get round the 500 error)
- increase timeout to 2 minutes - timeout might need to vary with file size?
- made timeout configurable with `--acd-upload-wait-time` - perhaps set it large when uploading huge files?

Here is the beta: http://pub.rclone.org/v1.32-18-gf7562dc%CE%B2/
 Sorry missed your message - you could try `--acd-upload-wait-time 10m` to wait 10 minutes. I don't know the optimum size though.
 I've merged this to master and it will be in the v1.33 release - thanks for your help testing.

Please re-open if you see any problems.

Here is a beta with the latest code in: http://pub.rclone.org/v1.32-20-g2ebeed6%CE%B2/
 Thanks again for addressing this issue so promptly and effectively!
  How did you compile rclone?  I found rclone worked perfectly when I compiled it on my phone with Termux which I believe has some patches for the Go runtime to integrate better with Android.
 Of course! I (stupidly) didn't think that someone would try running that on Android. I should probably do an android release too.  I wonder if go 1.7 has an Android OS build target yet... 
 I think if you then try `go install github.com/ncw/rclone` it will work.

I put my build here for you: http://pub.rclone.org/rclone-android-arm-v1.32.gz if you want to give it a go!
 I'm going to close this for now as I'm not really supporting rclone on Android right now, but I'm glad we've made it work to some extent!
  Can you run rclone as administrator? 
 I'm afraid I'm not  windows expert. Can any one help? 
  The [swift library rclone uses](http://github.com/ncw/swift)  (also by me) tries the API key and the password to be backwards compatible with rackspace - so there should have been an attempt with the password just before the one you saw fail.  I'm not sure why this didn't get logged though, but I'm pretty sure it happens like this

```
2016/07/27 18:17:33 HTTP REQUEST
2016/07/27 18:17:33 POST /v2.0/tokens HTTP/1.1
Host: auth.storage.memset.com
User-Agent: rclone/v1.32
Content-Length: 101
Content-Type: application/json
Accept-Encoding: gzip

{"auth":{"passwordCredentials":{"username":"XXX","password":"XXXX"},"tenantName":"XXX"}}
```

rclone has worked in the past with OVH.

I think you want your config file `~/.rclone.conf` to look like this for a standard openstack v2 auth (put the actual values in though!).  Unfortunately the swift library doesn't read auth from the environment - it really should now but it was written before that was a standard.

```
[ovh]
type = swift
user = $OS_USERNAME
key = $OS_PASSWORD
auth = $OS_AUTH_URL
tenant = $OS_TENANT_NAME
```
 Glad you made it work! Fancy sending a pull request with a bit of OVH specific documentation?
 Actually I'm going to add a bit to the docs

```
Rclone gives Failed to create file system: Response didn’t have storage storage url and auth token

This is most likely caused by forgetting to specify your tenant when setting up a swift remote.
```

Something like

```
This may also be caused by specifying the region when you shouldn't have (eg OVH).
```

What do you think of that?
 I've distilled these into the docs now
  Sorry I missed this issue when you posted it.

I've not seen the `No uploads available in vault 0001013 (503 service_unavailable)` error which seems to be the root cause of your problem.  It looks like backblaze is too busy.

I can see I've made a mistake and capped the retry time to 2 seconds - if rclone had been backing off exponentially there then hopefully it would have got an upload URL.

I'll set the max time to 5 minutes and post a beta for you to try.

You can also try increasing `--low-level-retries`
 Here is a beta with the upper pause time set to 5 minutes from 2 seconds.

http://pub.rclone.org/v1.32-4-g57f8f1e%CE%B2/
 I'm going to close this now as it seems to work :-)

You would need to set `--low-level-retries=20` to make sure of waiting an hour (checking every 5 minutes). `--low-level-retries=10` will only wait just under 15 minutes.
  Sorry I missed this issue when you posted it.

Looks strange!

Can you try with the `-v` flag and see if rclone logs anything else.

Did rclone finish with any sort of error?

Thanks

Nick
 I suspect this is related to golang/go#16570 which looks like the fix will land in go 1.7 which should be released very soon.

I compiled a version of rclone with latest development go compiler (which will shortly be released as 1.7)

http://pub.rclone.org/rclone-v1.32-4-g57f8f1e-macOS-Sierra-fix.zip

Can you give that a go and see if it fixes it please?

Thanks

Nick
 Great - thanks for testing.

The next version of rclone v1.33 will be built with go 1.7 so this will be fixed in that release.
 go 1.7 has been released and has the fix in hopefully!

Here is a beta: http://pub.rclone.org/v1.32-20-g2ebeed6%CE%B2/

I'm going to close this now - please re-open if it isn't fixed.

Thanks
  Strange!  If you can reliable reproduce it then please re-open.

Thanks

Nick
  This would be difficult to achieve with the architecture of rclone at the moment.  The two remotes (source swift and destination swift) know nothing about each other and have no way to communicate something like chunk sizes.

You could sync the _segments container and remake the manifest I suppose.

Why is this causing you a problem?
  --checksum is a parameter,  not an operation, so you can not run it separately anyway. It seems like the command you write does what you want. Is there any problem with it?

Note that --checksum will read the files on both source and destination, which will probably not be much faster than a delete + copy. 
 > But right now I'm unsure if I can do both --checksum & copy... and if I need anything else in the commandline?

It looks fine to me.  You could drop the `--checksum` if you want - rclone always checks the checksums of files it transfers which is probably what you are after, rather than checking the checksums of all files.

> basically I want to have rclone check if there's any changes before the copy command.. no need to do any copying if there's no changes..

That is exactly what the `copy` verb does.
 I'm going to close this now as I think we've answered the question.  Please re-open if you think otherwise!
  Thanks for posting that.

I can see the problem (it is a new bit of code) - I'll fix it for the next release.

The problem is that it got more than one Fatal Error and signaled it more than once in here

``` go
// This checks the types of errors returned while copying files
func (s *syncCopyMove) processError(err error) {
    if err == nil {
        return
    }
    s.errorMu.Lock()
    defer s.errorMu.Unlock()
    switch {
    case IsFatalError(err):
        close(s.abort)
        s.fatalErr = err
    case IsNoRetryError(err):
        s.noRetryErr = err
    default:
        s.err = err
    }
}
```
 That is fixed in this beta: http://pub.rclone.org/v1.32-23-gb5faaf7%CE%B2/

and will be in the v1.33 release.

Thanks for reporting.

Nick
  Does it have a public API somewhere?
  You can run as many rclones as you like.

Note that Amazon/Google rate limit you on your user details so you might find running multiple copies slows all of them down.

The only potential problem is that when rclone refreshes the token for Amazon Drive it will write it back to the config file. I don't think this will cause a problem with multiple rclones writing to it though, you'll get the result of the last one to write to it.
  I see what you mean.  A nice idea!
  Rclone should have refreshed it's tokens and carried on. 

Judging from the fact you only had 9 errors the sync worked pretty well. 

Any chance you could get me a log with -v of this happening? 

You should upgrade to 1.32 there is a bug with B2 in 1.31 BTW. 
 @nemobis a log with `-vv` with the latest beta is what is needed to track down problems - thanks!  If you look at the `Transferred:` line in your screenshots, you can see it is quite consistently at about 1 MByte/s which is the limit of your connection.

I can see why the transfer speeds for the individual files look odd though.  For B2 we buffer 96 MByte chunks into memory and send them, retrying if necessary.  This means that we read the file very quickly (hence the very high transfer speeds at the start) and by the end of the file we've read everything so the transfer speed indicator drops to 0.

This display problem we've noticed before but I can't find an issue for it.

If you look at the log you sent for the file `_Mondsee_cloudlapse/MVI_6037.MOV` you can see it sends a 96MB chunk about every 6 minutes from 10:34 to 12:54 which is a transfer rate of about 2.4 MBit/s.  this is almost exactly 1/4 of your 10 Mbit/s upload which is what I'd expect as rclone uploads 4 files at once by default.

What is wasting bandwidth is when rclone has to cancel an upload - there were two of these in the log you sent me

```
2016/07/16 10:49:27 (2016-06-13)_Mondsee_storm/MVI_0142.MP4: Cancelling large file upload due to error: Error reading uploaded data: Read timed out (400 bad_request)
```

And 

```
2016/07/16 11:39:11 (2016-07-05)_Mondsee_cloudlapse/MVI_6036.MOV: Cancelling large file upload due to error: Post https://pod-000-1029-08.backblaze.com/b2api/v1/b2_upload_part/4_z2c4731fb0384db29583e081c_f201f4c3a5ded21cd_d20160716_m083443_c001_v0001029_t0018/0007: dial tcp: lookup pod-000-1029-08.backblaze.com on 192.168.1.1:53: no such host
```

The first looks like some kind of internal backblaze error, or maybe rclone didn't supply the data quick enough due to network congestion. A 400 error indicates the request was incorrect somehow and you don't want to retry these.

The second looks like a local problem on your network - your router `192.168.1.1:53` failed to look up the name  `pod-000-1029-08.backblaze.com`.  This might be due to network congestion also.

You might find if you limit the speed to slightly slower than your connection it works better - it might help with those errors. You could try `--bwlimit 900k`

I've realised this is the same issue as #553
 That looks all to be working fine - it shows rclone getting an error and retrying.

Why don't you try my `--bwlimit 900k` suggestion?
 You have lots of upload - I explained above that the 0kbs reading isn't real. Limiting the bandwidth will help with the errors you are getting. 
 @isaiah36 That is probably an amazon problem, but I can't see anyone talking about it on the developers forum.  If it continues to happen can you make a new ticket with a log using the `-v` flag of it happening please?
 The upload statistics are now fixed for b2 (along with quite a few other things in) v1.33.
  Interesting... I just tried a test with s3 and it worked fine.

```
$ echo hello > /tmp/"complex character £100.txt"
$ rclone copy /tmp/"complex character £100.txt" s3:utf8-test
$ rclone copy s3:utf8-test  s3:utf8-test2
2016/07/15 15:27:57 S3 bucket utf8-test2: Waiting for checks to finish
2016/07/15 15:27:57 S3 bucket utf8-test2: Waiting for transfers to finish
2016/07/15 15:27:58 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:          9s
```

However if I try the same test with dreamhost

```
rclone copy /tmp/"complex character £100.txt" dreamhost:utf8-tes
rclone -v --dump-bodies --retries 1 copy dreamhost:utf8-test  dreamhost:utf8-test2
```

I get the same error as you

```
2016/07/15 15:29:45 complex character £100.txt: Failed to copy: NoSuchKey: 
    status code: 404, request id: tx000000000000000019ea9-005788f359-e20c137-default
2016/07/15 15:29:45 Attempt 1/1 failed with 1 errors and: NoSuchKey: 
    status code: 404, request id: tx000000000000000019ea9-005788f359-e20c137-default
2016/07/15 15:29:45 Failed to copy: NoSuchKey: 
    status code: 404, request id: tx000000000000000019ea9-005788f359-e20c137-default
```

Here is the troublesome HTTP transaction

```
2016/07/15 15:29:45 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/07/15 15:29:45 HTTP REQUEST
2016/07/15 15:29:45 PUT /utf8-test2/complex%20character%20%C2%A3100.txt HTTP/1.1
Host: objects-us-west-1.dream.io
User-Agent: rclone/v1.32
Content-Length: 0
Authorization: DELETED
Date: Fri, 15 Jul 2016 14:29:45 UTC
X-Amz-Copy-Source: utf8-test%2Fcomplex+character+%C2%A3100.txt
X-Amz-Metadata-Directive: COPY
Accept-Encoding: gzip

2016/07/15 15:29:45 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/07/15 15:29:45 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2016/07/15 15:29:45 HTTP RESPONSE
2016/07/15 15:29:45 HTTP/1.1 404 Not Found
Content-Length: 75
Accept-Ranges: bytes
Content-Type: application/xml
Date: Fri, 15 Jul 2016 14:29:45 GMT
X-Amz-Request-Id: tx000000000000000019ea9-005788f359-e20c137-default

<?xml version="1.0" encoding="UTF-8"?><Error><Code>NoSuchKey</Code></Error>
2016/07/15 15:29:45 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
```

What I suspect is that Dreamhost (and thus CEPH) isn't URL decoding the `X-Amz-Copy-Source` header properly in the request. It says in the [s3 COPY docs](http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) that "This string must be URL-encoded."

I put that URL encoding in to fix #315 which was a very similar problem with S3.

The [CEPH docs on COPY](http://docs.ceph.com/docs/hammer/radosgw/s3/objectops/) don't mention the URL encoding so I suspect this is a bug in CEPH being out of compliance with the S3 spec.

It is possible that it is [CEPH issue 8202](http://tracker.ceph.com/issues/8202) but I'm not 100% certain.

I'll submit a ticket with dreamhost and see what they say!
 I received a very helpful response from Dreamhost support

> Thanks for the info.  We are actually already looking into something
> similar.  The DreamObjects system runs on Ceph, and we are waiting for a
> new release to drop really soon so we can verify whether it has a fit in
> it for this issue or not.  I will add a comment to our internal tracker
> to update that github issue as well, and can keep your ticket open to
> notify you if you wish as well, when we have the new release tested and
> pushed and can test it.
 Tested just now, still not working :-(
 Tested again - still not working.
 Tested after dreamhost upgrade but not change unfortunately.
  That looks like a crash in the Go runtime while it is doing garbage collection.

rclone doesn't use cgo or unsafe so it is unlikely to be an rclone problem unless there is some weird race condition... However I can't see an bug against the go runtime which fits this.

Can you try http://pub.rclone.org/rclone-v1.31-2-g41917eb-go1.7rc1.zip which is 1.32 but compiled with the latest not quite released yet go compiler + runtime.

I could also send you a version with the race detector compiled in which would be useful to know if this is an rclone problem.

@klauspost anything spring to mind?
 It does yes.
 Thanks for testing.

Go 1.7 should be out by the time I release v1.33 so the fix should be in then.
 I'm using go 1.7 now it has been released.

Here is a beta compiled with 1.7: http://pub.rclone.org/v1.32-22-gb4f2ada%CE%B2/

Please re-open this ticket if you see this problem again!

Thanks
  I've fixed this in 1.32 which I released specially to cure this problem :-) 
  Make exponential backoff work exactly as per [google specification](https://developers.google.com/drive/v2/web/handle-errors#exponential-backoff).
 I need to do this before Google will increase rclone's rate limits any further.
  I can see I've messed that up!

Can you try this beta for me please?

http://pub.rclone.org/rclone-v1.31-2-g41917eb.zip

Thanks

Nick
 Thanks for testing! I'm going to put a new release out with that fix in ASAP!
   Getting a filelist from the google api can be slow - especially if you have lots of files.  You can time how long it takes by doing `rclone size dave_kr8:`.  It might be interesting to compare the local speed also with `rclone size /media/dave.kr8@drive.google.com/`.

If you do those two measurements then it will tell you where the problem is.

> hmmm, my nas is running on a ssd, and i use the noatime option to mount my filesystem. maybe this is the problem i am facing???

Unlikely I would have thought

> but it's the same result if i use --checksum oder --size-only command. or is the copy command not using the options?

`copy` does obey those options, yes.

Do the files get their modification time updated when they are placed in the drive?  If so then you can use this which might speed things up

You can use this to see if that looks likely

```
rclone --max-age 1d --no-traverse lsl  dave_kr8:
```

And then this for syncing

```
rclone --max-age 1d --no-traverse copy dave_kr8: /media/dave.kr8@drive.google.com/ --low-level-retries 20 --retries 20
```

Note that you'll need the latest rclone v1.31 (just released) to use the --no-traverse flag.

Note also that you can watch the progress of the directory scanning if you use the `-v` flag.
 Google seem to be rate limiting rclone at the moment #560 which might be related.  So I think that the speed of the sync is being limited by google rather than anything else....

You are only listing 5k objects - that should take no time at all!
 Interesting thanks.

I'll write more about rclone rate limits in #560 which I've seen you've subscribe to and I'll close this one.
  Not currently.  It might help with #382 also. I'll put it on the list :-)
 @J0hnsen why don't you do a pull request and finish it off.   That looks very cool! I see it has an API, but I'm not sure it has enough features to make a good rclone integration.  I'll put it on the list!
  When filing an issue, please include the following information if
possible as well as a description of the problem.

> What is your rclone version (eg output from `rclone -V`)
> rclone v1.30
> 
> Which OS you are using and how many bits (eg Windows 7, 64 bit)
> Windows 7, 64 bit
> 
> Which cloud storage system are you using? (eg Google Drive)
> Backblaze b2
> 
> The command you were trying to run (eg `rclone copy /tmp remote:tmp`)
> rclone sync D:\backup\Personal\ remote:NoSyuPersonalBackUP --transfers 100 --checksum -n --retries 50 -v --log-file NoSyuPersonalBackUP_%ldt%.log
> 
> A log from the command with the `-v` flag (eg output from `rclone -v copy /tmp remote:tmp`)
> 2016/07/10 21:56:38 video/20150830 MLSS 2015 Video.zip: Hash differ
> 2016/07/10 21:56:38 video/20150830 MLSS 2015 Video.zip: Not copying as --dry-run

Hello
I try to sync the local files to backblaze b2 with rclone. Most of the files are copied correctly, but some files occur weird errors.

The error messages of the files are 'Hash differ'.
So, I checked the hash value of the files manually by 'rclone sha1sum' operation.
The results are as below:

> E:\rclone-v1.30-windows-amd64>rclone.exe sha1sum "D:\backup\Personal\video\20150830 MLSS 2015 Video.zip"
> dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f  20150830 MLSS 2015 Video.zip
> 2016/07/10 21:40:15
> Transferred:            0Bytes (0Byte/s)
> Errors:                 0
> Checks:                 1
> Transferred:            0
> Elapsed time:       50.9s
> 
> E:\rclone-v1.30-windows-amd64>rclone.exe sha1sum "remote:NoSyuPersonalBackUP/video/20150830 MLSS 2015 Video.zip"
> dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f  20150830 MLSS 2015 Video.zip
> 2016/07/10 21:41:06
> Transferred:            0Bytes (0Byte/s)
> Errors:                 0
> Checks:                 1
> Transferred:            0
> Elapsed time:        2.2s
> 
> E:\rclone-v1.30-windows-amd64>rclone.exe size "D:\backup\Personal\video\20150830 MLSS 2015 Video.zip"
> Total objects: 1
> Total size: 5.271G (5659957144 bytes)
> 2016/07/10 21:43:00
> Transferred:            0Bytes (0Byte/s)
> Errors:                 0
> Checks:                 0
> Transferred:            0
> Elapsed time:           0
> 
> E:\rclone-v1.30-windows-amd64>rclone.exe size "remote:NoSyuPersonalBackUP/video/20150830 MLSS 2015 Video.zip"
> Total objects: 1
> Total size: 5.271G (5659957144 bytes)
> 2016/07/10 21:42:45
> Transferred:            0Bytes (0Byte/s)
> Errors:                 0
> Checks:                 0
> Transferred:            0
> Elapsed time:        2.8s

The local and remote files have same sha1 value, and the size of the files are also same. But, when I run the sync, it shows 'hash differ'.

I am checking the source code, and test it. If I found the bug or any enhancement, I will commit the code.

Thanks
Yours sincerely
 I suspect this was fixed in e0aa4bb4923237585dc301f41b1f799c1b7cd564 as part of #533 

Have a go with the latest beta: http://pub.rclone.org/v1.30-28-g2a1d4b7%CE%B2/

Thanks

Nick
 Thank you!
I run the latest beta program, and it works fine.

In fact, to avoid the issue #533, I also made own rclone following the instruction (http://rclone.org/install/) at 2016/07/09, and tested. But the program produces same error. So I wrote this article.
I think my installation have a problem. I re-check the process.

Thanks
Yours sincerly
 @NoSyu thanks for testing.  Not sure what went wrong with your own compile - the code should have been up to date on github.  Did you do `go get -u -v github.com/ncw/rclone/...`?
 @ncw I re-download and recompile the code, and test it. The result is OK, no problem.
Sorry for my mistake.
 @NoSyu Thanks for checking!
 Sorry for re-opening this issue, because this problem appears at v1.32 in Windows amd64 version.
Here I bring the same file - 20150830 MLSS 2015 Video.zip as an example.

To check the problem in more detail, I add a code to print the hash of source file and destination file when the 'Hash differ' message is printed based on c2f6decb9cbd8b8bf504fdac76a9f51d288832fd commit in the CheckHashes function in fs/operations.go file.

The result is as below (First one is the source file in local disk and second one is the destination file in B2):

> 2016/07/24 12:00:49 video/20150830 MLSS 2015 Video.zip: dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f
> 2016/07/24 12:00:49 video/20150830 MLSS 2015 Video.zip: 9562fd50e2d953c889c95423c8357970fba44f8c
> 2016/07/24 12:00:49 video/20150830 MLSS 2015 Video.zip: Hash differ
> 2016/07/24 12:00:49 video/20150830 MLSS 2015 Video.zip: Not copying as --dry-run

After that, I run the 'sha1sum' operation, and the result is not the same above.

> E:\rclone>rclone.exe sha1sum "remote:NoSyuPersonalBackUP/video/20150830 MLSS 2015 Video.zip"
> dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f  20150830 MLSS 2015 Video.zip
> 2016/07/24 12:32:26
> Transferred:      0 Bytes (0 Bytes/s)
> Errors:                 0
> Checks:                 1
> Transferred:            0
> Elapsed time:        2.6s

I also check the hash in B2 webpage.

> large_file_sha1: dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f   

I think this is a bug. Please check the issue one more time.

Thank you
 I think this is a different issue you are right.

So what you seem to be saying is when you do `rclone --checksum copy src b2:` the checksums don't match with the one being read from b2: being the incorrect one.

Can you try this for me which should attempt to copy just the one file (with `--dry-run`) that is causing a problem.  It will make a lot of log messages into `log1.txt` - can you remove the `Authorization:` lines and attach them to a post (or email them to me if you prefer).

```
rclone.exe --dry-run -v --dump-bodies copy  --include "/20150830 MLSS 2015 Video.zip" "D:\backup\Personal\video" "remote:NoSyuPersonalBackUP/video/" --log-file log1.txt
```

Can you also do the same with this command and `log2.txt`

```
rclone.exe -v --dump-bodies sha1sum "remote:NoSyuPersonalBackUP/video/20150830 MLSS 2015 Video.zip" --log-file log2.txt
```

That should tell me exactly what is going on.

Thanks

Nick
 Correction - make those `--dump-headers` into `--dump-bodies` (have edited the post)
 I re-re-re test this issue, and the problem appears only when I set the path to root as below:

> rclone sync D:\backup\Personal\ remote:NoSyuPersonalBackUP -n --transfers 32 --checksum --retries 1000 -v --log-file NoSyuPersonalBackUP_%ldt%.log

When I set the path to the specific directory, then it works fine.

> rclone sync D:\backup\Personal\video remote:NoSyuPersonalBackUP/video -n --transfers 32 --checksum --retries 1000 -v --log-file NoSyuPersonalBackUP_%ldt%.log

The result of the last operation is as below:

> 2016/07/24 18:07:38 20150830 MLSS 2015 Video.zip: dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f
> 2016/07/24 18:07:38 20150830 MLSS 2015 Video.zip: dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f
> 2016/07/24 18:07:38 20150830 MLSS 2015 Video.zip: Size and SHA-1 of src and dst objects identical
> 2016/07/24 18:07:38 20150830 MLSS 2015 Video.zip: Unchanged skipping

I also check it with v1.32, and there is no hash differ with the last execution.

I will try to the operations as you mentioned, and check the results.

Thank you
 I wonder if on b2 you have two objects with differing case, eg

```
remote:NoSyuPersonalBackUP/video/20150830 MLSS 2015 Video.zip
remote:NoSyuPersonalBackUP/Video/20150830 MLSS 2015 Video.zip
```

That might explain what is going on.
 I run the code which has `-dump-bodies` command. However, there is no error in both cases.
As I mentioned https://github.com/ncw/rclone/issues/578#issuecomment-234771753, the error only appears when I set the root path. 
So I run the `-dump-bodies` command to root path. It takes long time. I will update the information ASAP.
 OK thanks.
 Thank you for suggsting the reason of the issue. However it's not.

I check the filename as below:

> E:\rclone>rclone ls "remote:NoSyuPersonalBackUP/video/20150830 MLSS 2015 Video.zip"
> 5659957144 20150830 MLSS 2015 Video.zip
> 2016/07/24 20:56:48
> Transferred:      0 Bytes (0 Bytes/s)
> Errors:                 0
> Checks:                 0
> Transferred:            0
> Elapsed time:        3.6s
> 
> E:\rclone>rclone ls "remote:NoSyuPersonalBackUP/Video/20150830 MLSS 2015 Video.zip"
> 2016/07/24 20:56:58
> Transferred:      0 Bytes (0 Bytes/s)
> Errors:                 0
> Checks:                 0
> Transferred:            0
> Elapsed time:        3.1s
> 
> E:\rclone>rclone ls "D:\backup\Personal\video\20150830 MLSS 2015 Video.zip"
> 5659957144 20150830 MLSS 2015 Video.zip
> 2016/07/24 20:57:53
> Transferred:      0 Bytes (0 Bytes/s)
> Errors:                 0
> Checks:                 0
> Transferred:            0
> Elapsed time:           0
> 
> E:\rclone>rclone ls "D:\backup\Personal\Video\20150830 MLSS 2015 Video.zip"
> 5659957144 20150830 MLSS 2015 Video.zip
> 2016/07/24 20:58:00
> Transferred:      0 Bytes (0 Bytes/s)
> Errors:                 0
> Checks:                 0
> Transferred:            0
> Elapsed time:           0

![qqqq](https://cloud.githubusercontent.com/assets/2724705/17083699/6871f38a-51d2-11e6-9286-8a967c32598c.png)

The location of the file are in `video` directory in both local and remote.

I also re-check it with `ls` operator to B2 remote root path, and there is only one `20150830 MLSS 2015 Video.zip` file in the bucket.
 OK thanks for checking
 Here I attached the snipped logs which contain the `20150830 MLSS 2015 Video.zip`. 

> {
>       "action": "upload",
>       "contentLength": 5659957144,
>       "contentSha1": null,
>       "contentType": "application/octet-stream",
>       "fileId": "4_zcc94124fb690a616565d071f_f2004db74d996abe3_d20160723_m171923_c001_v0001004_t0013",
>       "fileInfo": {
>         "large_file_sha1": "dd494d689cceac5e3e62e17f8a1ae2ab67eeea4f",
>         "src_last_modified_millis": "1467909807201"
>       },
>       "fileName": "video/20150830 MLSS 2015 Video.zip",
>       "size": 5659957144,
>       "uploadTimestamp": 1469294363000
>     },
> 
> 2016/07/24 21:17:57 video/20150830 MLSS 2015 Video.zip: Hash differ
> 2016/07/24 21:17:57 video/20150830 MLSS 2015 Video.zip: Not copying as --dry-run

I run the v1.32 windows amd64 program.

I will send the whole file via email, because it has all filenames in my personal back directory.

Thank you very much
 Thanks for that.  The return from B2 looks disappointingly normal.

In your debugging are you sure it was the B2 shasum that was wrong - could it have been the local one?

I will study your log some more.
 ```
func CheckHashes(src, dst Object) (equal bool, hash HashType, err error) {
    common := src.Fs().Hashes().Overlap(dst.Fs().Hashes())
    // Debug(nil, "Shared hashes: %v", common)
    if common.Count() == 0 {
        return true, HashNone, nil
    }
    hash = common.GetOne()
    srcHash, err := src.Hash(hash)
    if err != nil {
        Stats.Error()
        ErrorLog(src, "Failed to calculate src hash: %v", err)
        return false, hash, err
    }
    if srcHash == "" {
        return true, HashNone, nil
    }
    dstHash, err := dst.Hash(hash)
    if err != nil {
        Stats.Error()
        ErrorLog(dst, "Failed to calculate dst hash: %v", err)
        return false, hash, err
    }
    if dstHash == "" {
        return true, HashNone, nil
    }

    Debug(src, srcHash)
    Debug(dst, dstHash)

    return srcHash == dstHash, hash, nil
}
```

This is a code which prints the hash values at https://github.com/ncw/rclone/issues/578#issuecomment-234755514
Only the changes is calling two Debug function with srcHash and dstHash.
I wll also investigate this problem in more detail.
 Are you still having a problem with this?

Have you tried the latest v1.33?
 Yes, the problem is not solved with v.1.33 windows x64. So I solved this problem by divide and conquer - means run rclone sync with sub directories. :)
Now, I am out of the office and home, and come back to at 5th Sep. After that, I re-check this problem in more detail since the structure of the rclone is changed a lot.
Thank you
 Great thanks. The more help pinning it down the better. 
 > The confusion always seems to be between a "large_file_sha1" field of one file and a "contentSha1" of a small file...

Strange!

Did you find the corrupted hash anywhere else - maybe in the file headers?

Can you try again with the latest beta: http://beta.rclone.org/v1.35-19-g499766f/ - I did change the B2 directory iteration in 1.35 to simplify it a lot.

The rclone check is a good test to do.

If that doesn't help, then what we need to do is build rclone with the race detector to see if there is some concurrency problem in rclone.  Unfortunately I don't have a 64 bit windows machine to build it on (normally I cross compile, but you can't easily cross compile if you are using the race detector).

If you wanted to try this, then you would download go from golang.org, Set your PATH as described [here](https://golang.org/doc/install#testing), then issue the command `go get -u github.com/ncw/rclone`.  Then `go build -race github.com/ncw/rclone` which should build the rclone binary in the current directory with the race detector.  You use it as normal (run the check) and it will complain loudly to stderr if there are any races detected.
 I'm confident that this was fixed by 1d42a343d25b4ae6263fe2046c681f54ebee53dc

Please try a latest beta and re-open if you still see the problem!

https://beta.rclone.org/v1.36-41-gb651784/

Thanks

Nick  Ach, I'm sorry :-(

I think this is the same issue as #512. I need to disallow moves from source to source/destination as this is equivalent to a copy source to destination then a delete of the source :-(

You can use the –drive-use-trash flag if you want deletions to go into the trash. Not much use to you now though. 

I will have a think on how to prevent this in the future. 

Many Apologies 

Nick 
 Rclone will move it if the underlying remote is capable of it which drive is. 

I'm not at my computer at the moment - I'll take a look at the source tomorrow and come up with something definitive. 
 OK I've fixed this in this beta: http://pub.rclone.org/v1.30-30-g4f2daa6%CE%B2/

I've decided to re-work the way move works to make it more what people expect.

Here is the new help

### move source:path dest:path

Moves the contents of the source directory to the destination
directory. Rclone will error if the source and destination
overlap.

If no filters are in use and if possible this will server side move
`source:path` into `dest:path`. After this `source:path` will no
longer longer exist.

Otherwise for each file in `source:path` selected by the filters (if
any) this will move it into `dest:path`.  If possible a server side
move will be used, otherwise it will copy it (server side if possible)
into `dest:path` then delete the original (if no errors on copy) in
`source:path`.

**Important**: Since this can cause data loss, test first with the
--dry-run flag.
 I've pushed out the v1.31 release with this fix in.
  I've merged this in d645bf096602ee4b89468b2501117dac29dfcff6.  I made a few alterations to make it go better with the other docs.

Thank you very much for your contribution

Nick
  Implement cleanup for remotes
- [x] amazonclouddrive - not possible
- [x] b2 - done in #462
- [x] box - not possible easily (see below)
- [x] drive
- [x] dropbox - not possible
- [x] googlecloudstorage - not possible
- [x] hubic - not possible
- [x] local - not possible
- [x] onedrive - not possible
- [x] s3 - not possible
- [x] swift  - not possible
- [x] yandex
 @cederberg `rclone cleanup` should work properly with b2 now.  Can you open a new issue with a log file and some instructions on how to reproduce please?  I thought this might be related to #604 but I fixed that apparently! @cederberg might be caching I suppose... File a bug if you can reproduce it!  Thanks, Nick @eharris cleanup will do its stuff on all files, regardless of whether rclone deleted them.  It should only remove already deleted files.

> If the latter, I'm not sure I'd be comfortable with that without a pretty clear confirmation or --yes-really kind of option.

Lots of rclone commands can cause data loss if used incorrectly.  The `--dry-run` option is for those cases!  Yes you are right, "replaced existing" would be more accurate.  I've fixed this - it will be in the next beta.
  Note that if there were IO errors in the sync, then rclone won't delete files.  rclone will log `Not deleting files as there were IO errors`.

The API doesn't give control over which files end up in the trash or not, so if something has changed about which files end up in the trash then amazon have changed something most likely.

Did updated files used to end up in the trash - I don't remember testing that?
 Sure - check out the [filters page](http://rclone.org/filtering/).  `--exclude /path/to/lost+found/**` is probably what you want.  Check using `rclone ls` with the filter first.
 It doesn't matter where you put the `--exclude` flag.

Note that you don't repeat the path in the remote in the exclude, so you want something like this.  (I should have said it needed the relative path from the root of the sync.)  I'd put it in quotes too as it contains shell metacharacters `*`.

```
/usr/sbin/rclone sync --exclude "/JAZZ/lost+found/**" /home/isaiahsellassie/Music Adrive:Music
```

You can test whether it is working with the `ls` commands which obey excludes in the same way, eg

```
/usr/sbin/rclone ls --exclude "/JAZZ/lost+found/**" /home/isaiahsellassie/Music
```

Or if you decided you never wanted a directory called `lost+found` wherever it was found then you could use `--exclude "lost+found/**"`
 It should have left a log for that error that will give you a clue as to what happened. You can use the - - log-file option to save the log somewhere. 

You need to track down that last error or it won't do the remote deletions. 
 A full path is safest
 We need to stop it looking in the `lost+found` directory - any IO errors will stop remote deletions.

Which command line did you use with that log?  Did it have the `--exclude "/JAZZ/lost+found/**"` in it?  I'm pretty sure that should work.
 The first slash roots it to the start of filesystem hierarchy.  The `/**` means this directory and anything under it which is quite important.

I see what the problem is now - you need to upgrade rclone - the feature I'm trying to use was only put in 1.30.

I suggest you upgrade to the latest rclone 1.32 and try again.
 Excellent! Deleting empty folders is #100 so I'll close this one. 
  The reason why I haven't documented the config file is that the majority of the remotes can't be configured that way as they use oauth2 which requires rclone interacting with a web server.

`rclone config` for swift has the rackspace endpoints built in which should make configuring rackspace swift really easy.

The config file for rackspace should look like this

```
[rackspace]
type = swift
user = USER
key = KEY
auth = https://identity.api.rackspacecloud.com/v2.0
```
  As @lorenzoferrarajr suggests a truncated version of the submitting a pull request instructions will work.

You'll need a Go environment set up with GOPATH set.  See [the Go getting started docs](https://golang.org/doc/install) for more info.

Then install rclone and all of its dependencies

```
go get github.com/ncw/rclone
```

Look at the source code

```
cd $GOPATH/src/github.com/ncw/rclone
```

Rebuild the source code

```
go install
```

NB The binary is in `$GOPATH/bin` - most go developers put that on their PATH.

You don't need the makefile for development - it is only used for making releases.
  Just name the folder that you want stuff to go in on the end of the remote string. 

So for your example 

rclone copy /source/directory remote:folder/directory 
 If you prefer you could do this

```
rclone copy --include "/directory/**" /source remote:folder
```

Rclone works by copying from directory to directory (If you are familiar with rsync that is with putting a `/` on the end of the directories being synced).

I decided not to do what rsync does and do different things as to whether you put a `/` on the end of the path or not as it is a source of great confusion for people and any number of gotchas.
  I know it works for .co.uk so I would have thought it would just work for .es.

The setup in http://rclone.org/amazonclouddrive/ should work. If it doesn't then let me know what errors you get.
 Super :-)
  I cannot immediately see how there could be contention for [dstFiles](https://github.com/ncw/rclone/blob/v1.30/fs/operations.go#L639). All accesses appear safe, and unless it is leaked, all accesses should be reads when [readFilesMap](https://github.com/ncw/rclone/blob/v1.30/fs/operations.go#L565) has returned.

Is it possible for you to run this on another system, just to verify that this isn't a hardware issue?
 Yes I agree with you Klaus. There is possibly concurrent read access to the map at [in the background delete go routine](https://github.com/ncw/rclone/blob/v1.30/fs/operations.go#L596) and the [foreground checker](https://github.com/ncw/rclone/blob/v1.30/fs/operations.go#L639).  However Russ Cox states that [this access pattern is OK](https://groups.google.com/forum/#!msg/golang-nuts/HpLWnGTp-n8/hyUYmnWJqiQJ) ie no writers but multiple readers.

So it could be a bug in the runtime.  If it is, it isn't one that has been discovered already.

It could also be a bug in the hardware so it might be worth running memtest86 on it (or running a linux kernel compile - I've found that an excellent way of shaking out bad hardware).

I'll see if I can reproduce the problem to see if it is a runtime problem.

In the mean time you could  try the latest beta: http://pub.rclone.org/v1.30-28-g2a1d4b7%CE%B2/ in which I've re-architected the sync loop.
 I've built a version of rclone with the latest go which may or may not make any difference!

http://pub.rclone.org/rclone-v1.31-2-g41917eb-go1.7rc1.zip

When rclone fails does it always give the same or similar traceback?

Thanks

Nick
 Glad it is working!

The exit code was non zero because of the `Errors: 1576` indicating that there were some files which didn't get transferred.  You'd need to check the log to see exactly why.  It might be that they were caused by timeouts and retrying will fix them.  It might be that they are something specific (eg the file not being readable).

Errors are a fact of life with cloud storage systems, and rclone has low level and high level retries to help deal with that (see `--retries` flag)
 There shouldn't be issues with files with `?` in - that would be a bug. I'd like to see a log with `-v` of this happening if possible please.

The `InvalidObjectState: Operation is not valid for the source object's storage class` error seems to be because that object has been moved to Glacier.  Does that make sense?
 Ah, OK that makes sense.

What is happening is that you have some file names which aren't UTF-8 encoded on your filesystem.  By the look of it you have some latin-1 files.  This is quite common in linux file systems where only in recent years have they standardised on utf-8 encoding for file names.

If you look at that file in a file manager window, you'll probably find it won't be able to display the name properly either.

If you look in the [docs for local](http://rclone.org/local/) in the `Filenames` section you'll see that you can cure this with the `convmv` command - you run it once over your filesystem and it will convert all the file names into utf-8.
 I recommend you run convmv, but if you don't want to I fixed the bug which stops the fies with incorrect UTF-8 encoding being uploaded.  You can try that here: http://pub.rclone.org/v1.32-1-g9eeed25%CE%B2/ - it replaces all bad encoding with the unicode error symbol so the files get uploaded with mostly their correct file names.
 I think we have fixed everything here so I'm closing the ticket.  Please re-open if you disagree!
  I've fixed this - fancy giving the beta a go?

http://pub.rclone.org/v1.30-28-g2a1d4b7%CE%B2/
 @cg-cnu thanks for testing :-)
  I've merged this in 02a3bbaa3dc50f31d8fca089d2ee0eb6f190c1be

Note that I dropped the changes to MANUAL.txt/rclone.1 etc as these are auto generated.

Also there seemed to be a bit of one of my commits in there too.

Anyway it was easy enough to sort out.

Thanks for your contribution

Nick
  Not quite sure I understand what you mean - the config prompts for the `domain` if you put an auth URL in.

```
n/s/q> n
name> z
Type of storage to configure.
Choose a number from below, or type in your own value
 1 / Amazon Drive
   \ "amazon cloud drive"
 2 / Amazon S3 (also Dreamhost, Ceph, Minio)
   \ "s3"
 3 / Backblaze B2
   \ "b2"
 4 / Dropbox
   \ "dropbox"
 5 / Google Cloud Storage (this is not Google Drive)
   \ "google cloud storage"
 6 / Google Drive
   \ "drive"
 7 / Hubic
   \ "hubic"
 8 / Local Disk
   \ "local"
 9 / Microsoft OneDrive
   \ "onedrive"
10 / Openstack Swift (Rackspace Cloud Files, Memset Memstore, OVH)
   \ "swift"
11 / Yandex Disk
   \ "yandex"
Storage> 10
User name to log in.
user> z
API key or password.
key> z
Authentication URL for server.
Choose a number from below, or type in your own value
 1 / Rackspace US
   \ "https://auth.api.rackspacecloud.com/v1.0"
 2 / Rackspace UK
   \ "https://lon.auth.api.rackspacecloud.com/v1.0"
 3 / Rackspace v2
   \ "https://identity.api.rackspacecloud.com/v2.0"
 4 / Memset Memstore UK
   \ "https://auth.storage.memset.com/v1.0"
 5 / Memset Memstore UK v2
   \ "https://auth.storage.memset.com/v2.0"
 6 / OVH
   \ "https://auth.cloud.ovh.net/v2.0"
auth> http://z.z/
User domain - optional (v3 auth)
domain> somedomain
Tenant name - optional for v1 auth, required otherwise
tenant> ssss
Tenant domain - optional (v3 auth)
tenant_domain> sss
Region name - optional
region> sss
Storage URL - optional
storage_url> sss
AuthVersion - optional - set to (1,2,3) if your auth URL has no version
auth_version> sss
Remote config
--------------------
[z]
user = z
key = z
auth = http://z.z/
domain = somedomain
tenant = ssss
tenant_domain = sss
region = sss
storage_url = sss
auth_version = sss
--------------------
```
  Tenant is optional for v1 auth which rclone supports.  You are right it needs to be filled in otherwise.

I've fixed that in the help string.

Thanks for pointing that out.
  It is interesting, but probably should be in another application (rproxy, anyone?). It could of course piggyback on the cloud integrations of rclone.

The complexity of sync an app is not trivial. There are multiple things you need to take into consideration.
- Should it be completely async, or let the slowest backend set the speed?
- If it should buffer some operations - what happens if one backend returns an error?
- What if backends are desynced?
- Should multiple clients be able to write to the proxy at once?
- How should reads be handled? Desync issues galore.

So there are a lot of complex scenarios that have to be addressed. However, it is not an impossible task by any means.

I would personally go for option 2) in your case, and have some way of triggering upload on the remote server when your local upload has completed successfully. That way if your upload is incomplete, or your backup is broken you still have your previous backup on other providers.
 I agree with @klauspost this is interesting but the whole proxy thing would be difficult to implement.  I think your option 2) is probably the most sensible.  Note that rclone checks the consistency of uploads, and you can use the `rclone check` command too if you are worried about data consistency.

I've been playing with [minio](https://minio.io/) recently (an easy to install S3 server) which works with rclone (with the `--size-only` flag) - that could be part of a solution too.
 I think we are done with this question - closing now :-)
  Are you uploading lots of small files? That is the worst case for drive.

Google has a lot of rate limiting, per client, per application per cluster as far as I can see.  You can get caught up in one of those rate limits. rclone has 1 billion operations per day quota from google and it is using nothing like that.

The Google drive client uses a different API to the one rclone uses an unpublished one :-(

Some people have reported that adding their own credentials does help - I'd be interested to hear if it does.
 You could also try increasing `--transfers` to see if that helps.
 Very useful information thanks @balazer 

I can see myself that it is running extremely slowly now :-(

I'll try tweaking the parameters of the rate limiter and see if I can improve things.  I suspect I might need to talk to someone at google to resolve the problem though.

If someone wanted to contribute a tutorial or list of instructions on how to get your own developer ID then I can include them in the rclone docs.
  @balazer  @davekr8 thanks for the writeups - I'll put a combination of those into the google drive docs.

I've been in correspondence with google about rclone rate limits.  Apparently there **is** a global rate limit for rclone which **isn't** visible in the developer console!

Google have raised it to 300 Queries/second (from 100 Queries / second) and this has made rclone much more usable.  I think it could do with being higher still, but I don't think google want to raise it too quickly.
 The client errors are almost certainly 403 - rate limit exceeded.  That is drive's way of telling rclone to back off. There are lots of rate limits in drive - a per project one which isn't listed in the developers console and uploads per second and other invisible limits.

rclone tries to send stuff quickly, then backs off as it receives 403 errors.  This is the way it is supposed to work, but it is a bit annoying!
 The google developers told me that there are lots of other invisible (to me) rate limits.  I'm pretty sure one at least is uploads/second.  I would be suprised if another limited the total upload rate - I've never exceeded 200 Mbit/s on our 1 Gbit/s connection at work either.
 @zeshanb `--bw-limit` is rclone's throttle - I was talking about Google Drive's throttling which we can't see and have no control over :-(
 I'm going to write the conclusions of this up into the drive docs including instructions on how to make a drive developer's account.
  Stop removing the failed copy

For most remotes a failed copy means that the upload wasn't successful and the file won't be there anyway so we are wasting resources trying to delete it after failing to upload it.

In practice it is more likely that the file was signaled in error but is actually OK (eg ACD proxy problems)

Our aim is to never leave a file which is corrupt.

The scenarios where we should remove a failed copy
- The local fs - failed files will leave files that are half copied.
  - could fix this in the local fs though
- in the unlikely event that the MD5SUM/SHA1SUM didn't match after upload
- we should delete the corrupted file
- files which are uploaded in chunks (eg swift) might be readable before the last chunk has been uploaded - should probably fix this in the remote
 I have posted a potential fix for the Amazon problem to #606.  I forgot about this ticket when I was doing it though.

What I could do instead is implement this ticket and make sure 400 errors, 504 GATEWAY_TIMEOUT and 408 GATEWAY TIMEOUT return errors that shouldn't be retried from amazon cloud drive.

The one possible downside is the eventual consistency of amazon directory listings - when you upload something it doesn't always appear in the directory listing for a while (seconds usually), so if you uploaded a file, got this sort of error, then immediately restarted the sync then it would upload it again and get a different error at the end.

The fix I posted in #606 only deals with the TIMEOUT errors and not "400 Bad request", so probably what is required is a combination of these two fixes.
 That error is a really useful one to retry - gets used a lot in drive uploads. 

The proper solution would be for amazon to implement a chunked upload like all the other providers! 

Maybe the best solution in the meantime would be for rclone to detect if it had sent the last byte of the file or not. If it hadn't then the file definitely isn't complete. If it had then it could check for it (several times if necessary).  I  could probably implement this at a high level... Will think further. 
 @zstadler Did the file get to nearly 100% first?

At the moment if a 504 happens and the file isn't found after a minute it will be retried.  I should probably make that waiting time longer than 1 minute.

I think I'll have a go at implementing this issue this evening.
 @zstadler if not all the file got sent then the work-arounds won't be effective - a retry is the only option.

Do you have the whole log file I could look at - I't like to see whether the token refreshing worked or not?
 Unfortunately you can't resume an upload with ACD - the API isn't rich enough :-(

Thanks for the token refreshing log.  I'm hoping that will make a difference.
 @zstadler my request was denied too :-(
 I've done this in http://beta.rclone.org/v1.33-102-g441951a/ (will be uploaded in 15-30 mins)
 @zstadler thanks for letting me know.
  A new verb for rclone would be the way to go for this, eg `rclone list-remotes` maybe.

Note that rclone doesn't currently store the bit after the `:` in the config file which might make your plan harder.

This would be relatively easy to implement I think - nice idea!
 Even easier now we have cobra
 I've done this now - find it in this beta http://beta.rclone.org/v1.33-63-gace1e21/ in about 15 minutes!
 Thanks for testing. 

The man page will get updated at the next release. I normally do that as part of the release but maybe I should do that for the betas too. 

 I already did -l give it a go! 

```
rclone listremotes -h
```

Should show you the help
  Interesting!

If I try to download the link you sent me then Canel is the only option I get.

I can't do anything about onedrive thinking the file is a threat, so I guess the question is, is there anything in the API which could say "I'm OK with downloading a file which might contain viruses" and allow rclone to download the file.

Onedrive has a `virusSuspicious` error code which you can see here: https://dev.onedrive.com/misc/errors.htm but it doesn't look like it is returning that from the log.

That is the only mention of the word virus in the API docs.

When you try to download the file through the browser and you click OK, can you see what file your browser actually downloads? I'd do this with the [chrome devtools](https://developer.chrome.com/devtools) and the network tab.  Most browsers have something similar now-a-days.  I'm just wondering if there is a `?acknowledgeVirus=true` flag or something.
  I have been working on bringing rclone exactly into compliance for b2 as part of #420 . I'm almost there - I'll post a beta for you to try here to see if it makes any improvement.
 ...but yes you are right, the current 503 error handling is wrong and I've fixed that now.
 Here is a beta with the corrected error handling code: http://pub.rclone.org/v1.30-21-g018fe80%CE%B2/

It has quite a lot of other b2 features also (see #420 for the list) so testing apreciated!

Fix was c8e2531c8bbe6b73edc520b3221c82e7c9f8811b
 I've pushed out the v1.31 release with this fix in.  Let me know if you see any problems.

Thanks

Nick
  I'd like to be able to configure flags in the config file in general which I've put in #643
 As part of #616 you can now set this in the environment which is a step in the right direction, eg

    export RCLONE_DRIVE_USE_TRASH=true

for bash...  Can you send me a log with `-v` of it happening please?

What do you mean by "It will even crash eventually" - was there an error message?

Can you copy one of those files from acd to local disk? And then from local disk to b2? Just trying to work out where the problem is!

Thanks

Nick
 Thanks for that!

I can see something odd in the logs

```
2016/06/30 12:27:28 CentOS-7-x86_64-Everything-1503-01.iso: Received error: unexpected EOF - low level retry 1/10
2016/06/30 12:27:28 CentOS-7-x86_64-Everything-1503-01.iso: Removing failed copy
2016/06/30 12:27:29 CentOS-7-x86_64-Everything-1503-01.iso: Starting upload of large file in 76 chunks 
```

It looked like it retried the whole file, rather than retrying just the chunk which is odd.

I don't think that explains the getting stuck at 100% though - do you have a log of that?
 Figured out what that log means - it means that reading from ACD gave the unexpected EOF error.

Any chance you could try to replicate the issue and get me a log with `-v`?

Thanks
 I've realised this is the same issue as #587 - it is to do with the internal buffering on large file uploads for B2.  We read the file into memory in 96MB chunks very quickly - then while we upload the last of these chunks the transfer speed drops to 0.

You can see from the `Transferred:       3.100GBytes (34.553MByte/s)` lines that everything is proceeding properly.

@klauspost we discussed this before didn't we - did we ever make an issue about it?
 It is purely a display problem - the transfer is happening properly. Unfortunately rclone already uses the minimum chunk size so you can't make it smaller (which would help). 

One thing you could try is to set a --bwlimit of about the capacity of your Internet connection - that will probably work. 

I need to think of a better way of accounting the transfers so they display properly in this case. 
 The display problem was fixed as part of #602 and is now in the v1.33 release!
  Thanks for the report.

Resume is tricky..

However what I can and should do is put low level retries in for swift.  This would stop the upload failing in the first place by retrying the segments multiple times.
  Thanks for reporting that.

Currently rclone doesn't have a mechanism for an error so fatal it shouldn't retry so I'd need to build that first!
 Here is a beta with a fix for that: http://pub.rclone.org/v1.30-26-gcc62871%CE%B2/

Let me know if it works!
  I think a certain amount of this is to be expected from any cloud storage system.

From [the docs](https://cloud.google.com/storage/docs/json_api/v1/status-codes#500_Internal_Server_Error) rclone should be retrying 500 and 503 errors.

GCS doesn't have low level retries but the higher level retry (set by the `--retries`) flag should pick it up - hopefully you are seeing that.

Do these errors happen every now and again, or very often?

Do they always affect the same files? What size are they?

My feeling is that this is probably nothing to do with rclone, but I could make it better by implementing low level retries in GCS.
 You get 3 retries by default.

Can you answer these questions for me?
- Do these errors happen every now and again, or very often?
- Do they always affect the same files?
- What size are they?

Thanks
 503 Errors are most likely caused by the service being overloaded.  It is just possible that rclone is sending something which is upsetting GCS but it should really be returning a 4xx error code in that case.  It doesn't sound like there is a common thread between your files though.

I made myself an issue to address this in the future.  #556 

In the mean time using a higher value for `--retries` will work just fine, though a little less eficiently.
  I can't think of anything which would mean rclone running off cron was slower than not.

The only thing that comes to mind is what time are you running your syncs?  Maybe the cron jobs are run at a time when your internet connection is very busy or your ISP is implementing traffic shaping (very common in the evening when people are watching lots of internet TV).

Can you make a log with `-v` of it running off cron and either attach it here, or email it to me privately ( nick@craig-wood.com ) - thanks
 I see the problem...

You appear to be starting an rclone every minute of the 23rd hour.

This probably means your crontab looks like this

```
* 23 * * * rclone....
```

If you change it to this it should work better!

```
0 23 * * * rclone....
```

Here is the evidence in the log

```
60 matches for "starting with" in buffer: rclone(via cron).txt
      2:2016/07/01 23:00:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
     14:2016/07/01 23:01:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
     29:2016/07/01 23:02:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
     56:2016/07/01 23:03:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
     89:2016/07/01 23:04:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    126:2016/07/01 23:05:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    182:2016/07/01 23:06:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    238:2016/07/01 23:07:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    300:2016/07/01 23:08:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    370:2016/07/01 23:09:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    468:2016/07/01 23:10:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    561:2016/07/01 23:11:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    657:2016/07/01 23:12:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    751:2016/07/01 23:13:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    863:2016/07/01 23:14:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
    981:2016/07/01 23:15:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   1116:2016/07/01 23:16:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   1266:2016/07/01 23:17:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   1428:2016/07/01 23:18:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   1599:2016/07/01 23:19:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   1789:2016/07/01 23:20:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   1975:2016/07/01 23:21:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   2167:2016/07/01 23:22:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   2378:2016/07/01 23:23:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   2592:2016/07/01 23:24:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   2817:2016/07/01 23:25:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   3038:2016/07/01 23:26:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   3264:2016/07/01 23:27:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   3506:2016/07/01 23:28:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   3751:2016/07/01 23:29:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   4000:2016/07/01 23:30:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   4274:2016/07/01 23:31:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   4550:2016/07/01 23:32:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   4863:2016/07/01 23:33:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   5191:2016/07/01 23:34:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   5526:2016/07/01 23:35:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   5857:2016/07/01 23:36:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   6201:2016/07/01 23:37:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   6523:2016/07/01 23:38:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   6886:2016/07/01 23:39:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   7255:2016/07/01 23:40:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   7639:2016/07/01 23:41:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   8045:2016/07/01 23:42:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   8464:2016/07/01 23:43:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   8899:2016/07/01 23:44:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   9366:2016/07/01 23:45:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
   9827:2016/07/01 23:46:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  10321:2016/07/01 23:47:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  10824:2016/07/01 23:48:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  11338:2016/07/01 23:49:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  11858:2016/07/01 23:50:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  12396:2016/07/01 23:51:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  12959:2016/07/01 23:52:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  13575:2016/07/01 23:53:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  14192:2016/07/01 23:54:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  14849:2016/07/01 23:55:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  15520:2016/07/01 23:56:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  16224:2016/07/01 23:57:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  16940:2016/07/01 23:58:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
  17703:2016/07/01 23:59:01 rclone: Version "v1.30" starting with parameters ["/usr/sbin/rclone" "--bwlimit" "10M" "-v" "--log-file=/var/log/rclone/rclone3.log" "copy" "/media/iomega/OneDrive/Pictures/2016" "amazon:Pictures/2016"]
```
 No worries - it was interesting to debug!
  Nice idea.  Might be a bit of work.  I wonder if there are any Go frameworks about to help?
 A bit of research comes up with
- https://github.com/sanbornm/go-selfupdate
- https://github.com/inconshreveable/go-update
  Looking at the docs for `--files-from` I can see that they could do with improving.

I'll use this issue to do that :-)
  Yes you are right.  This is the same issue as #8 - I'm in the process of fixing this and will post a beta when it is ready.
 Here is a beta with a `--no-traverse` flag which should help.

Let me know how you get on.

http://pub.rclone.org/v1.30-11-g22ab4e4%CE%B2/
 @lorenzoferrarajr It will work for all the remotes, yes.
  That is a bit surprising!

I would I would have expected to have seen is that the file ended up at `remote:/test/file2.txt/file1.txt`

```
$ rclone-v1.30 -q ls drive:src
        0 file1.txt
$ rclone-v1.30 -q move drive:src/file1.txt drive:src/file2.txt
$ rclone-v1.30 -q ls drive:src
$ 
```

I'm working on fixing #518  and it looks like my fix for that will fix this too

```
$ rclone -q ls drive:src
        0 file1.txt
$ rclone -q move drive:src/file1.txt drive:src/file2.txt
$ rclone -q ls drive:src
        0 file2.txt/file1.txt
$ 
```

I'll post a beta here when it is ready.

There is no way to rename a file using rclone at the moment.  I should probably make a rename command...
 Here is a beta which should fix the problem

Let me know how you get on

http://pub.rclone.org/v1.30-11-g22ab4e4%CE%B2/
 Thanks for testing - Nick 
  It would be interesting to dig down into exactly why you are seeing what you are seeing and whether it is an rclone problem or an ACD problem.

It certainly takes longer to list big folders and rclone has to list the folder before transferring.

I could believe that the uploading a file into a directory with 50k files might take an extra few seconds as amazon has to check the directory for duplicates before uploading the file.  Is that what you are seeing?

It might be this change on rclone's side which is making the difference df1092ef33416e562ea22050640c94f38fa2df0f which checks for duplicates before uploading.

How long is the pause before each upload with a big directory do you think (assuming you are seeing that).

I would guess you are uploading lots of small files - is that the case?
 I did a test uploading 50,000 files (using the default transfers 4).  That took 3h47m31 so on average 3.6 files per second.

The first 1000 files were uploaded at 3.73 files / second, the last 1000 at 3.66 so no real difference there.

However I see what is the problem with your log...

rclone takes 22 seconds listing the remote directory before transferring a 100 MB chunk when there are 50k files in the remote directory.

This is as noted in #8 

Now that I've  put df1092e in, I could stop copy traversing the remote file system at all.  This would have several advantages in your case.  I'll put a `--no-traverse` flag in as an experiment and I'll post a beta here.
 Here is a beta with the `--no-traverse` flag - let me know what you thin!

http://pub.rclone.org/v1.30-11-g22ab4e4%CE%B2/
 Excellent - thanks
  Strange! Looks like some sort of B2 internal error. 

Did the files copy eventually? 

You could try setting `--b2-cutoff 1G` so it tries to upload them in chunks.
 I just read the [integration checklist again](https://www.backblaze.com/b2/docs/integration_checklist.html) backblaze recommend

> When uploading files larger than 200MB, use the large files support to break up the files into parts and upload the parts in parallel.

So I'll change the default for the `--b2-upload-cutoff` to 200MB in the next release.
 I've changed the default now.

You can try the latest version with lots of b2 changes here.

http://pub.rclone.org/v1.30-26-gcc62871%CE%B2/
  `-q` only shows errors - if there weren't any it will show nothing at all.

You probably don't want to use `--dry-run` and `-q` as it won't show you the files it was thinking of copying.

What were you expecting to see?
  It did change, yes, in order to fulfil #428 

Is that a problem for you?
 OK I'll see what I can do!
  Certainly something I'd consider adding should enough people want it or I receive a pull request for it :-)
 I think FTP sits well with rclone.  The trouble with it is that it is a rather loosely defined protocol and the whole control channel & data channel thing will make rclone's life more complicated.  The first thing to do would be to evaluate the various FTP client libraries available.
 In the this issue [as a table](https://help.github.com/articles/organizing-information-with-tables/) would be perfect.  Don't mind about column names.  Thanks in advance!
 Thanks @zeshanb :-)
 Please find a beta of the FTP backend here

https://beta.rclone.org/v1.36-114-gcdacf026/ (uploaded in 15-30 mins)

Please open new issues if you find any.  I just tried this with my computer and it worked fine.  I uploaded a file to drive then saw it appear on my phone and on the drive website.

Is is possible that you connected rclone to a different google drive by accident during the signup process?

You could try `rclone config` again to make sure.

It could be a google problem I suppose - some caching layer not working properly.

Nothing obvious springs to mind!
 Glad it is sorted! By all means suggest some words for the docs or the signup process if you think it could be improved.
  Actually it does already!

See the FAQ (could maybe be a better place for this info!)

## Can I use rclone with an HTTP proxy?

Yes. rclone will use the environment variables HTTP_PROXY, HTTPS_PROXY and NO_PROXY, similar to cURL and other programs.

HTTPS_PROXY takes precedence over HTTP_PROXY for https requests.

The environment values may be either a complete URL or a “host[:port]“, in which case the “http” scheme is assumed.

The NO_PROXY allows you to disable the proxy for specific hosts. Hosts must be comma separated, and can contain domains or parts. For instance “foo.com” also matches “bar.foo.com”.
 Hopefully the above helped.  I'm going to close this now.
  Didn't notice this before, but you need to update `docs/content/install.md` as  `MANUAL.md`  is auto generated from it.

Thanks

Nick
  You assume a two-way sync. rclone does not do that, it synchronizes from the source to the destination.

You are syncing FROM your local drive TO the remote gdrive. Therefore it will make your gdrive look like your local drive..
  This is because the [COPY](http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) command used to set the metadata can't set metadata on files bigger than 5 GB.

I think an acceptable work-around would be just to ignore requests to set the metadata for files > 5GB.

I've implemented this in the beta below:

http://pub.rclone.org/v1.30-3-ga67c746%CE%B2/

Please re-open the ticket if it doesn't work properly!

Thanks

Nick
  Excellent bug report - thanks :-)

Thanks for those logs - I can see that there is a problem with the retries...  The sha1 changes for the second retry which is very strange since assuming you are reading from the local filesystem then it should be cached in the object at that point.

I tried to simulate the problem by forcing a retry but I didn't manage to so there must be something else going on.

```
2016/06/18 23:41:23 POST /b2api/v1/b2_upload_file/c8995750732968595c560516/c001_v0001030_t0001 HTTP/1.1
Host: pod-000-1030-08.backblaze.com
User-Agent: rclone/v1.30
Content-Length: 3459189
Authorization: removed
Content-Type: image/jpeg
X-Bz-Content-Sha1: 0d6507aa7623230e21691a79c03a329975fab584
X-Bz-File-Name: 2009-07-27/IMG_9129.JPG
x-bz-info-src_last_modified_millis: 1286992637666
Accept-Encoding: gzip
```

And on the first retry it looks like this

```
2016/06/18 23:41:24 POST /b2api/v1/b2_upload_file/c8995750732968595c560516/c001_v0001030_t0001 HTTP/1.1
Host: pod-000-1030-08.backblaze.com
User-Agent: rclone/v1.30
Content-Length: 3459189
Authorization: removed
Content-Type: image/jpeg
X-Bz-Content-Sha1: 597831486f9228ad26ff522058d275512f6bb8f9
X-Bz-File-Name: 2009-07-27/IMG_9129.JPG
x-bz-info-src_last_modified_millis: 1286992637666
Accept-Encoding: gzip
```

Can you check the sha1 of that file for me both with sha1sum and rclone?

```
sha1sum /c/pictures/2009-07-27/IMG_9129.JPG
rclone sha1sum /c/pictures/2009-07-27/IMG_9129.JPG
```

Could you run those commands several times and make sure they give the same answer each time!

Some other questions for you
- Is it always that file which is a problem?
- What type of file system is `/c/pictures`
 Bam, found it!

If the file is closed without having the entire body consumed, the [hash is updated](https://github.com/ncw/rclone/blob/master/local/local.go#L570) to the partial value. We need to keep track of the consumed bytes in the reader above, and only update hashes if the consumed size matches the expected size.

If you want to have it generally available, it could be added to the [MultiHasher](https://github.com/ncw/rclone/blob/master/fs/hash.go#L119).
 Well spotted @klauspost - excellent sleuthing!

I'm just trying to think through the sequence of events
- local.Object.Open() is called in fs.Copy()
- b2.Fs.Update() is called
- This then calls local.Object.Hash(fs.HashSHA1)
- This calculates the sha1 and sets it in the object
- We try the PUT but it is rejected.
- b2 must reject it without reading the whole stream as we aren't using `Expect: 100-continue`
- local.Object.Close() is called by fs.Copy() and overwrites the correct hash it with a corrupt hash

So that indicates that your patch is doing the right thing :-)

It also indicates that we should probably not overwrite the hash in the local.Object.Close if it is already set.  Actually re-reading your patch I see you've done exactly that :-)
 @mejje here is a beta for you to try with @klauspost fix.

Please re-open the ticket if it doesn't work!

Thanks

http://pub.rclone.org/v1.30-2-ge0aa4bb%CE%B2/
 Thanks for testing! 

It is probably a caching thing on the website - that would be my guess. 
 @DavidCWGA @bittylicious I conjecture you are using crypt with b2 and this is 
#644 - here is a beta which should fix the problem:

http://pub.rclone.org/v1.33-6-g2eaac80%CE%B2/

If not can you open a new issue please with full info and a log with -v please?
  I've merged this - thank you very much :-)

6e35a3b3ce052284a50372f13b53b3706cf37569

I added a bit of docs.  I'd normally get you to do that but I wanted to get it into the 1.30 release which is imminent!

Thanks

Nick
  That is expected behaviour, for two reasons.

The first is that rclone doesn't manage directories at the moment #100 

The second is that hubic is based on swift, and swift only stores objects, not directories.
  It is probably a system stability issue unrelated to (but triggered by) rclone. In general no application should be able to crash your computer.

Found this somewhere on the net:

> These messages and reports are normal. They neither indicate a problem with the computer or the App. They only indicate that System Monitor is using services of the operating system core (kernel) more often as OS X is expecting it for average applications. Software developers can use such reports to reduce the energy consumption created by Apps.
 I'm going to close this as I don't think it is an rclone problem.

The logs you pasted are indicative of rclone doing lots of work - it might be that this is overheating your CPU and triggering instability.  You could check with a CPU stress testing tool.
  Are you running rclone off the crontab?

If so how do you check it has finished before starting the next one?

Are you using any flags?

Can you see why those rclones stopped from the log?

It would be easy to add a `--time-limit` flag which just killed rclone if that much time had elapsed - would that help?
 Are you still having problems with this?
 I'm going to close this as I haven't heard from you for a while, please re-open if you have more info.
 @magefesa I suggest you ask for help on the [forum](https://forum.rclone.org/).  I am not sure if that is a better solution. I can see good arguments for redirecting stderr and keep stdout on screen.

That said, the change is trivial by changing output [here](https://github.com/ncw/rclone/blob/master/fs/config.go#L468).
 I think stdout should be for stuff we intend the user to interact with so I'm not in favour of changing it to stderr unless we can find a strong precedent in some other unix tool.

That said, the standard behaviour of unix tools is to bypass stderr and stdout and output the password prompt directly to the terminal (and presumably read the prompt directly from the terminal).  I don't know how this is done though though it is probably possible with a bit of messing around with [golang.org/x/crypto/ssh/terminal](http://golang.org/x/crypto/ssh/terminal) which we use to read the password.

To see this try ssh to a host with an incorrect user that will ask for a password but with stdout and stderr redirected.
  I prefer option 1 as there is already a precedent for that - that is how b2 does it.  That would also make `rclone md5sum` work and enable checksum based syncing for large files.  Option 2 only lets you calcuate the etag when you download the file which is useful, but not as useful for rclone.

Is there a de-facto standard for the name of the metadata - I try to keep rclone compatible with other tools if possible?

This could also be implemented for swift (&hubic) - I wonder if there is any standard metadata keys for that also?
 I [asked on StackOverflow](http://stackoverflow.com/q/37844032/164234) a question about a standard metadata key for storing MD5SUMs.
 @jamshid wrote
> I don't understand this. Using the S3 ETag seems like the better option since it does not require adding a custom header.

Having an actual MD5 of big files would enable cloud to cloud copies from drive to s3 with checking for example, and it would enable `rclone md5sum` to work, neither of which would work with Option 1.  rclone does support arbitrary S3 providers with v2 and v4 auth - check the docs here: http://rclone.org/s3/

You just specify your endpoint in the config.

Many people use it with Ceph (eg Dreamhost).
  Thanks for the suggestion.

That library looks very good and has all the primitives rclone would need I think.
 Here is a beta of the SFTP support: http://pub.rclone.org/v1.35-60-g74801de-sftp%CE%B2/

Any testing much appreciated!
 Here is a slightly better beta which won't flake out after 1024 files!

http://pub.rclone.org/v1.35-60-gfdb7dfe-sftp%CE%B2/ I have merged this to master now - see here for a beta:

http://beta.rclone.org/v1.35-74-g48cdedc/

Any feedback much appreciated
 @kubark42 Hmm, I wonder if that is an OS X problem.  Can you make a new issue with the above in please?

Thanks  If the initial creation of the multipart upload fails, the body is then empty when it is retried.

This causes

```
2016/06/12 05:15:03 file: Failed to copy: Put https://www.googleapis.com/upload/drive/v2/files/XXX?alt=json&setModifiedDate=true&uploadType=resumable: http: ContentLength=129 with Body length 0
```

Audit for further occurrences of this.

Note also that the low level retries aren't being logged without `-v` or maybe at all, and the errors aren't being logged.
 Here is a beta which fixes the issue hopefully.

http://pub.rclone.org/v1.29-1-gbb75d80-59-ge4650ef%CE%B2/
 Thanks for the confirmation :-)
  I see what happened. 

When you configured rclone you put your email as the client id. You need to leave that blank and it should work. 

Hope that helps. 

Nick 
 Either should work.  Use `copy` if you don't want to delete stuff in amazon cloud.  Use `sync` if you want to delete stuff in the cloud that is no longer on your NAS.

Either way, experiment with the `--dry-run` flag first.
  Make `--dir-at-a-time` option. I don't like this name, but nothing else comes to mind at the moment!

This would make copy/sync/move do each directory at a time, not attempting to get an entire list of the remote in advance.

This would be useful to reduce memory usage and to reduce scanning. It would decrease efficiency in the general case.

This would help with these tickets
- #498
- #439 
- #236
- #8
 @diamondsw @DurvalMenezes I think rsync uses a similar idea

```
       -r, --recursive
              This  tells  rsync  to  copy  directories recursively.  See also
              --dirs (-d).

              Beginning with rsync 3.0.0, the recursive algorithm used is  now
              an  incremental  scan that uses much less memory than before and
              begins the transfer after the scanning of the first few directo‐
              ries  have  been  completed.  This incremental scan only affects
              our recursion algorithm, and does  not  change  a  non-recursive
              transfer.  It is also only possible when both ends of the trans‐
              fer are at least version 3.0.0.

              Some options require rsync to know the full file list, so  these
              options  disable the incremental recursion mode.  These include:
              --delete-before,   --delete-after,    --prune-empty-dirs,    and
              --delay-updates.   Because of this, the default delete mode when
              you specify --delete is now --delete-during when  both  ends  of
              the  connection are at least 3.0.0 (use --del or --delete-during
              to request this improved deletion mode  explicitly).   See  also
              the  --delete-delay  option  that  is a better choice than using
              --delete-after.

              Incremental recursion can be disabled using the  --no-inc-recur‐
              sive option or its shorter --no-i-r alias.
```

It would probably be worth studying exactly how it works.

On some remotes this will be a lot slower and on some it may make no difference...

I'm having second thoughts about the implementation though - doing one directory at a time doesn't fit in well with the rclone architecture.

Thinking about this some more, probably the best way if implementing this would be to make the directory listings optionally produce a sorted output.  This would mean that they couldn't list different directories in parallel, but it would make the syncing algorithm much easier as you would know exactly what you needed to copy or delete and you wouldn't need to store either the source of the destination listing at all.

Will ponder some more...

 I'm approaching the time when I'm going to need some beta testers for the new sync algorithm.

It passes all the unit tests and my own usage - now it needs some testing from other people...

Any volunteers? Here is a beta for this feature.  There is a temporary flag `--old-sync-method` to select the old sync method.

It should work identically to current rclone, except using a lot less memory.

http://pub.rclone.org/v1.35-50-g9d88ce3-low-mem-sync%CE%B2/

It is a major re-work of the syncing internals.  All the unit tests pass, and the code is well covered.  My own smoke tests pass too.

Test with care - use --dry-run first!

I'm particularly interested in 
  * any bugs
  * memory usage differences before vs after - especially on big syncs
  * speed differences before vs after

You can run this little ditty in another window to see rclone memory usage

    while [ true ]; do ps -eo rss,args --sort rss | grep [r]clone ; sleep 0.5 ; done I have merged this to master now - see here for a beta:

http://beta.rclone.org/v1.35-74-g48cdedc/

Any feedback much appreciated
 @DurvalMenezes Thanks for testing - very good data.

> OTOH, if directories are processed one by one, why is the RSS always increasing? Shouldn't it just "reset" to a lower-bound memory usage after each directory? Or is there possibly memory leak somewhere?

I'm not 100% sure... It might be the directory cache which caches directory names to IDs

It will also be keeping a note of files to delete as the default setting is --delete-after which requires them to be stored.  You can use --delete-during if you want to use less memory.

And there is memory fragmentation which is always a problem.  I would expect that to plateau though.

> Also, what should I watch for in the log file, before running it without the "--dry-run" switch?

Make sure it is copying the files you expect and deleting the files you expect.

> Please let me know if you need/want anything else.

What you are doing is brilliant - thanks. > OK, but it's literally millions of files, so I will do some sample checking for a few of the the "deleting" and "copying" messages in the log before running it for real (plus of course my de rigueur "find ... | md5sum" on top of an rclone mount, after all is said and done).

Sanity check is fine.  Your md5sum check will be the best check.

> Humrmrmrmr... something unexpected is happening here: the "rclone --dry-run sync" I started yesterday is still running, but the counters are now way over the number of files that should exist both in the remote and in the local file system:

Probably something went wrong and rclone is doing a retry.  Grep the logs for that. @DurvalMenezes 

> Just did:
> grep -i retry\|error ./20170205Z040712_rclone_sync_REDACTED_REDACTED
> But it showed absolutely nothing.
> 
> Any hints on what I should be grepping for?

You should get a message `Attempt %d/%d failed with %d errors`

> And, are retries really accounted for on "Checks" and "Transferred"? I was under the impression that these two numbers only reflected final files checked and transferred, independent of any retries.

I had a look in the source and the only thing that gets reset around retries are errors - the transfers and checks will continue to count up.
 > It was the same path/file being mentioned in the three lines.

Hmm, can you see that path in the web interface? What about your local disk?

> The "tofu" (?) character � was also shown twice, in exact the same position.

That is indicative of a non UTF-8 character, ie some latin1 encoding.

That might mean you need to run `convmv` tool ok your locak disk to make the file name encodings UTF-8 rather than latin1.

> A tentative conclusion: I don't think these 3 retries (referring all to the same 1 error on 1 file) explain the large amount of extra files being shown by sync (ie, more than 13M when it should have been less than 8M).

The retries will have caused a complete rescan of the files.  So at a guess it found this error after scanning 4ish M files, then repeated it twice more to give 13M in total. @DurvalMenezes thanks for the update - looking good for 1.36 :-) > shouldn't it try to replace them too, by the same rationale that justifies replacing invalid UTF-8 characters?

I am planning to do this as part of the next release.  It is quite a big change but my hope is to make it so that all file names can be copied anywere, with a bit of escaping - different escaping needed on windows or drive or acd for instance.  I think this is probably #28 - sometimes rclone manages to create duplicates :-(  This has got a lot better over the years but still isn't perfectly fixed.

You can use the `rclone dedupe` command to help manage the problem, or remove duplicates by hand - rclone doesn't mind.
 dedupe will help you remove the duplicate files.

The duplicate file names are rather annoying but don't happen too often.
 Can you post the comand line you used for the transfer too please?
 Have you tried the latest release? It will abort if the listing fails and it double check the file doesn't exist before uploading so I hope it might help your issues. 
 I'm going to close this as we have #28 open to track this issue
  I see, you are attempting a server side move of a large object.

I don't think that is supported directly by swift.

rclone should move the individual parts of the static large object.
  A good idea.  Not all swift servers support both SLO & DLO though so would have to be optional (flag controlled maybe or it could query the swift server to see what it can do).
  No I haven't seen this - thanks for bringing it to my attention.

I'll investigate further and if any fix is needed I'll get it into the next release.
 I have updated the google api package I build against and put the recommended fix in.

Here is a beta for you to try: http://pub.rclone.org/v1.29-1-gbb75d80-54-g6617157%CE%B2/

Please re-open if you find any problems
  I don't think there is a single option for that at the moment in rclone!

`move` is equivalent to `copy` followed by `purge`.

There is a good reason for using purge - deleting the empty directories as you've discovered, and also it is a lot more efficient on some remotes.

If you apply filters, then `move` becomes equivalent to `copy` followed by `delete`.  rclone currently doesn't delete the empty directories (but likely it will in the near future).

I'd probably suggest you use the `move` without the filters, then use a `mkdir` afterwards to re-create the directory - would that work for you?
  This is in the v1.31 release :-)
  Easy mistake to make.  +1 for use of potato as a nonce word ;-)
  If you do a `rclone --dry-run copy local remote:` then it will list any files that needs updating from the local to the remote.

Is that what you mean?
 I think `rclone --dry-run copy local remote:` will do exactly that.
 Sorry, not a shell expert :-(

I'm going to close this now as I think we've got to the bottom of the rclone stuff.

Thanks

Nick
  Do you mean copying stuff from `C:` to `D:`?

And by much slower, how much slower?

One of the reasons will be that rclone is doing MD5SUMs of the files to check the data integrity.

I'd expect the platform native copying tool to be faster than rclone though in general.

Here is `rclone` vs `cp` on my linux box on the rclone source tree - here rclone is about 50% the speed of cp.  I tried it on a directory of bigger files (ISOs) and rclone was about 75% the speed of cp.

```
$ time cp -a . /tmp/rclone-cp-1

real    0m1.459s
user    0m0.012s
sys 0m0.496s
```

```
$ time rclone copy . /tmp/rclone-rclone-1
2016/06/06 13:57:33 Local file system at /home/ncw/Code/Go/src/github.com/ncw/rclone: Building file list
2016/06/06 13:57:33 Local file system at /tmp/rclone-rclone-1: Building file list
2016/06/06 13:57:35 Local file system at /tmp/rclone-rclone-1: Waiting for checks to finish
2016/06/06 13:57:35 Local file system at /tmp/rclone-rclone-1: Waiting for transfers to finish
2016/06/06 13:57:36 
Transferred:     627.875MBytes (188.573kByte/s)
Errors:                 0
Checks:                 0
Transferred:         2620
Elapsed time:        3.3s

real    0m3.338s
user    0m6.912s
sys 0m0.952s
```
 No problems!
  rclone (like rsync) checks the modification date and the size first and if those differ does the upload.

If the size is the same, then rclone checks the sha1 and if that is identical then it updates the modification time.  For b2 you can't change the modification time of an object, so that means uploading it again.

If you'd rather use the size and sha1 checksum first then you want the `--checksum` option which causes rclone to upload if the size or sha1 checksum is different.

As for this part

> The SHA1 checksums of the files are checked on upload and download and will be used in the syncing process.

rclone checks the sha1 sums of objects it uploads to make sure they got uploaded properly, and downloads to make sure the data wansn't corrupted.

Hope that helps!

Nick
 No `--checksum` only checks the size and the checksum.
  Yes you are right...

I thought I wrote this in the docs but obviously not!

The command line parser I'm using for rclone can't do repeated options.

I made an issue about it not giving an error here spf13/pflag#72

In the mean time there are several workarounds as you've doubtless discovered
- use `--exclude-from`
- use a glob pattern, eg `--exclude "{AAA,BBB}"`
- use `--filter-from`

The last is the most general purpose.
  I agree that this is a good idea - such a good idea that there is already an issue about it at #371 !

Can I suggest you follow that issue and I'll close this one as a duplicate.

Thanks

Nick
  That is very strange - ACD definitely doesn't allow duplicate files.

Can you see the folder if you do `rclone lsd remote:`?

Can you check in the web interface that it hasn't got a space on the end or something like that?
 I'm going to close this as I think we've gone as far as we can with it.  Please reopen if you get more info!

Thanks

NIck
  I've merged that - thank you very much for your contribution.

-- Nick
  A very good question! 

Short answer - you can't at the moment. 

It is probably essential if you are working with case insensitive file systems though. 

Easy would be to add a flag say `--filters-case-insensitive` which makes all filters/globs case insensitive (rsync doesn't have such an option) or `--case-insensitive-globs`

Harder would be to make it settable for each glob. There are some ideas here: http://unix.stackexchange.com/questions/48770/how-to-match-case-insensitive-patterns-with-ls

What do you think? 
 :-)

I'm going to re-open this as I'd quite like to add the flag option at some point!
 The flag isn't very robust though.  Its all or nothing.  It would have been better to have the case sensitivity within each include/exclude. 

Perhaps within the 'filter' flags only something like this gets implemented:
--filter "+i file.JpG" 

where the i represents that line is case insensitive.  That would also allow building more flags unto each rule in the future if required.  Alas this seems to be a problem with amazon.

Here is a test for you to try: http://pub.rclone.org/v1.29-30-g53ead43%CE%B2/

It redoes the oauth on 401 errors. This has worked for some users...

There is a thread on the forum about the 401 errors:

 https://forums.developer.amazon.com/questions/27740/401-messagetoken-has-expired.html

Please add your complaints there too!
 Can you paste a bit of log with `-v` starting from the first 401 error?
 Here is a second attempt to fix the 401 errors - I think this one will work properly (cross fingers!)

http://pub.rclone.org/v1.29-30-g33611be%CE%B2/
 @bunset good news!  I have a further small patch to work around the occasional 403 errors during authentication which I'll post in a few minutes.

Yes rclone will exit with a non-zero exit code if there were any errors it couldn't retry.  If it doesn't then that is a bug and please report an issue!
 OK Here is beta 3 which
- does a re-authentication on 401 errors (working as per attempt 2)
- does a retry on that particular 403 error

http://pub.rclone.org/v1.29-33-g085677d%CE%B2/

In my testing this works perfectly now - hopefully it will for you too.
  This is a known problem - see #8 

I have recently done something which will help here - you can use an include filter to only include files on the top directory eg `--include "/*"` and that will now stop rclone recursing the destination.

If you know the list of files to transfer you can also use `--files-from`

You'll need this beta though

http://pub.rclone.org/v1.29-28-g36700d3%CE%B2/
  I've fixed this now - thank you very much for pointing it out to me.

I've now idea how this happened - very strange!

If you try rclone.org you might need to do CTRL refresh to see the new content.

Thanks again

Nick
  I'm in the process of fixing this...

If you try the latest beta you should find it works just fine.

http://pub.rclone.org/v1.29-33-g085677d%CE%B2/
 I've double checked this is fixed so I'm going to close this now.
 It will scan files in the directories that your files are in, but it shouldn't go into any other directories if that makes sense. That should make things much more efficient. 
 Here is what I tried in the rclone source directory

```
$ cat z
appveyor.yml
CONTRIBUTING.md
dircache
hubic/hubic.go
```

Then

```
clone -v --dry-run --files-from=z copy . /tmp/z
```

Which lists this

```
2016/06/04 15:10:23 rclone: Version "v1.29" starting with parameters ["rclone" "-v" "--dry-run" "--files-from=z" "copy" "." "/tmp/z"]
2016/06/04 15:10:23 Local file system at /tmp/z: Modify window is 1ns
2016/06/04 15:10:23 Local file system at /tmp/z: Not making directory as dry run is set
2016/06/04 15:10:23 Local file system at /home/ncw/Code/Go/src/github.com/ncw/rclone: Building file list
2016/06/04 15:10:23 cross-compile: Excluded from sync (and deletion)
2016/06/04 15:10:23 RELEASE.md: Excluded from sync (and deletion)
2016/06/04 15:10:23 make_manual.py: Excluded from sync (and deletion)
2016/06/04 15:10:23 Local file system at /tmp/z: Building file list
2016/06/04 15:10:23 redirect_stderr_unix.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 rclone_test.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 redirect_stderr.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 rclone-listdir-v1.29-23-gdbfa703.zip: Excluded from sync (and deletion)
2016/06/04 15:10:23 z: Excluded from sync (and deletion)
2016/06/04 15:10:23 versioncheck.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 README.md: Excluded from sync (and deletion)
2016/06/04 15:10:23 .travis.yml: Excluded from sync (and deletion)
2016/06/04 15:10:23 MANUAL.md: Excluded from sync (and deletion)
2016/06/04 15:10:23 Local file system at /tmp/z: Done building file list
2016/06/04 15:10:23 .gitignore: Excluded from sync (and deletion)
2016/06/04 15:10:23 Makefile: Excluded from sync (and deletion)
2016/06/04 15:10:23 rclone.1: Excluded from sync (and deletion)
2016/06/04 15:10:23 rclone.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 MANUAL.html: Excluded from sync (and deletion)
2016/06/04 15:10:23 ISSUE_TEMPLATE.md: Excluded from sync (and deletion)
2016/06/04 15:10:23 COPYING: Excluded from sync (and deletion)
2016/06/04 15:10:23 hubic/hubic_test.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 hubic/auth.go: Excluded from sync (and deletion)
2016/06/04 15:10:23 Local file system at /home/ncw/Code/Go/src/github.com/ncw/rclone: Done building file list
2016/06/04 15:10:23 Local file system at /tmp/z: Waiting for checks to finish
2016/06/04 15:10:23 appveyor.yml: Not copying as --dry-run
2016/06/04 15:10:23 CONTRIBUTING.md: Not copying as --dry-run
2016/06/04 15:10:23 hubic/hubic.go: Not copying as --dry-run
2016/06/04 15:10:23 Local file system at /tmp/z: Waiting for transfers to finish
```

This divides into 

```
     2016/06/04 15:10:23 appveyor.yml: Not copying as --dry-run
     2016/06/04 15:10:23 CONTRIBUTING.md: Not copying as --dry-run
     2016/06/04 15:10:23 hubic/hubic.go: Not copying as --dry-run
```

And

```
     2016/06/04 15:10:23 cross-compile: Excluded from sync (and deletion)
     2016/06/04 15:10:23 RELEASE.md: Excluded from sync (and deletion)
     2016/06/04 15:10:23 make_manual.py: Excluded from sync (and deletion)
     2016/06/04 15:10:23 redirect_stderr_unix.go: Excluded from sync (and deletion)
     2016/06/04 15:10:23 rclone_test.go: Excluded from sync (and deletion)
     2016/06/04 15:10:23 redirect_stderr.go: Excluded from sync (and deletion)
     2016/06/04 15:10:23 rclone-listdir-v1.29-23-gdbfa703.zip: Excluded from sync (and deletion)
     2016/06/04 15:10:23 z: Excluded from sync (and deletion)
     2016/06/04 15:10:23 versioncheck.go: Excluded from sync (and deletion)
     2016/06/04 15:10:23 README.md: Excluded from sync (and deletion)
     2016/06/04 15:10:23 .travis.yml: Excluded from sync (and deletion)
     2016/06/04 15:10:23 MANUAL.md: Excluded from sync (and deletion)
     2016/06/04 15:10:23 .gitignore: Excluded from sync (and deletion)
     2016/06/04 15:10:23 Makefile: Excluded from sync (and deletion)
     2016/06/04 15:10:23 rclone.1: Excluded from sync (and deletion)
     2016/06/04 15:10:23 rclone.go: Excluded from sync (and deletion)
     2016/06/04 15:10:23 MANUAL.html: Excluded from sync (and deletion)
     2016/06/04 15:10:23 ISSUE_TEMPLATE.md: Excluded from sync (and deletion)
     2016/06/04 15:10:23 COPYING: Excluded from sync (and deletion)
     2016/06/04 15:10:23 hubic/hubic_test.go: Excluded from sync (and deletion)
     2016/06/04 15:10:23 hubic/auth.go: Excluded from sync (and deletion)
```

So you can see it traversed the current directory and the `hubic` directory but none of the other directories.

Do you see a similar effect?

The fix should stop it scanning directories it doesn't need to but will still note the files in those directories.  

The latest beta is: http://pub.rclone.org/v1.29-48-ge2788aa%CE%B2/
 Ah, that makes sense.  It is very inefficient with hubic (also swift, dropbox, b2, s3) to list individual directories so rclone does a complete list for those remotes.  This should end up being quicker overall, but does mean that you will get each file in a `-v` listing.
 The overhead is the listing of the remote container.

How long does `rclone size remote:` take?

And how long does the copy take with the `--files-from` option?

When I try that on my swift server it runs at about 10,000 files per second.

One way of speeding things up would be to split the files list up by directory, so if you originally started with
- `rclone copy --files-list z / remote:`

with files list

```
/home/user/file1
/home/user2/dir2/file2
```

You could then do two rclones one for `/home/user` and another for `/home/user2/dir2`.

I've just made a ticket #517 which would be a better fix for this in the long run.
 I'm going to close this now as the initial problem is fixed.

The proper fix is probably #517 which I encourage you to subscribe to.

Please feel free to re-open if you have more info to add.

Thanks

Nick
  I see what you mean - nice idea.
 This could be implemented using an overlay file system in a similar way to crypt
 Yes the chunks would be resumable. The way it would work is as an overlay or modification of the local file system. Big files would appear as bigfile.1 bigfile.2 etc and these would sync separately. So if one part failed you could retry it. 
  Can you send me the full log (via email if you like) and the full command line too. 

Thanks Nick 
 Thanks for that.  I don't see anything out of order in there.

I think you are probably looking in the web interface to see the files.  Remember that b2 keeps the old versions of your files.  So if I do this

```
$ mkdir /tmp/b2-test
$ echo "hello world" > /tmp/b2-test/hello.txt
$ rclone sync /tmp/b2-test b2:bdelete-test
2016/05/24 13:52:00 Local file system at /tmp/b2-test: Building file list
2016/05/24 13:52:00 B2 bucket bdelete-test: Building file list
2016/05/24 13:52:01 Waiting for deletions to finish
2016/05/24 13:52:01 B2 bucket bdelete-test: Waiting for checks to finish
2016/05/24 13:52:01 B2 bucket bdelete-test: Waiting for transfers to finish
2016/05/24 13:52:02 B2 bucket bdelete-test: Waiting for deletes to finish (during+after)

Transferred:           12 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:        2.6s

$ rclone ls b2:bdelete-test
       12 hello.txt

$ rm /tmp/b2-test/hello.txt 

$ rclone sync /tmp/b2-test b2:bdelete-test
2016/05/24 13:52:30 Local file system at /tmp/b2-test: Building file list
2016/05/24 13:52:30 B2 bucket bdelete-test: Building file list
2016/05/24 13:52:31 B2 bucket bdelete-test: Waiting for checks to finish
2016/05/24 13:52:31 B2 bucket bdelete-test: Waiting for transfers to finish
2016/05/24 13:52:31 Waiting for deletions to finish
2016/05/24 13:52:31 B2 bucket bdelete-test: Waiting for deletes to finish (during+after)

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 1
Transferred:            0
Elapsed time:        2.7s

$ rclone ls b2:bdelete-test

```

All the files have gone according to rclone as expected.

However if I look in the web interface I see this

![b2](https://cloud.githubusercontent.com/assets/536803/15504575/8bde0994-21b7-11e6-81bd-b99beacd4739.png)

And if I click on it I can see that `hello.txt` is made up of two files, firstly its original contents, and on top of that a delete marker which means that the file appears as deleted to rclone so doesn't appear in the `rclone ls`.

![b2-1](https://cloud.githubusercontent.com/assets/536803/15504616/bdf2ff3e-21b7-11e6-8ab8-ed2d9a87b89a.png)

Now the obvious question is how to delete those old versions if you don't want them.  There is an issue about this #462

So in summary, I think b2 is working as intended, storing all the old versions of your files for you.
  What is the destination filesystem? (Google drive?) What OS are you running on? (Linux?) Which rclone version? 

If it stores anything it will be in `/tmp`, but IIRC we no longer create temporary files when uploading from local filesystem.
 rclone almost never needs a temporary file copy (only when copying from a remote which doesn't support SHA1 to blackblaze b2), and never when the local disk is one of the remotes.  

Temp files are stored in /tmp or TMPDIR.

On linux, `ncdu` is a great tool for finding out where all your disk space went.
 @wtluke no worries - we've all been there :-)
  No rclone doesn't have a FUSE interface sorry!  I did consider writing one - it would be a nice project for someone to use the rclone internals to implement it.
 I'm going to include this as an experimental feature in the next release.  It seems to work reasonably well...  I'll post a beta here for people to test before then.
 @isaiah36 file systems expect things to be 100% reliable, whereas cloud storage systems are a long way from 100% reliable.  The rclone command copes with this with lots of retries - we won't be able to use those in the fuse mount without taking local copies of all the uploads.  I might do that in the future, but for the moment `rclone mount` won't do that, so will be less reliable than the rclone command. It remains to be seen exactly how useful it is!

So to answer your question, yes you'll be able to view your cloud system in the file manager, but it won't be as reliable as using the rclone command.
 I have merged this to master. It needs lots of testing - I haven't tried it on OS X (and I don't know how to set up FUSE on OSX) - so I could do with some help with testing and docs for OS X.

Here is the the beta.  Note I've marked this as experimental.

http://pub.rclone.org/v1.32-25-gf22029b%CE%B2/

Do `rclone help mount` to see the docs.
 @Trentflix no the windows version doesn't support mount alas.  AFAIK windows doesn't have FUSE support.  It is only for Linux, FreeBSD and OS X at the moment.
 This is merged now and will be part of the v1.33 release.

Here is a beta: http://pub.rclone.org/v1.32-34-g037a000%CE%B2/

If you find any problems please open a new issue.

Thanks

Nick
  I'd like to fix this but I haven't figured out what is the problem :-(

Can you check the time and very importantly the timezone on your computer? Oauth2 relies on the time being correct. 

So on Linux check that

  TZ=UTC date

Agrees with searching for "UTC time"  on google - that will check you've got the timezone right. 
 Here is a test for you to try: http://pub.rclone.org/v1.29-30-g53ead43%CE%B2/

It redoes the oauth on 401 errors.  This may or may not be helpful!
 That is looking good. Have you tried some longer transfers? 
 The `504 GATEWAY_TIMEOUT` errors are a known problem with ACD itself - see this thread: https://forums.developer.amazon.com/questions/22563/408-request-timeout-errors.html
 Just noticed there is a thread on the forum about the 401 errors: https://forums.developer.amazon.com/questions/27740/401-messagetoken-has-expired.html
 The - 30 beta is supposed to reauth on a 401 error. It obviously isn't working properly yet though - can you post some logs of what happens around the 401 error please? 

As for choosing the endpoint that would be possible. I don't know whether would work though due to the way the underlying s3 storage is structured. 
 Here is a second attempt to fix the 401 errors - I think this one will work properly (cross fingers!)

http://pub.rclone.org/v1.29-30-g33611be%CE%B2/
 Oops sorry, I forgot to post the beta to this thread...

OK Here is beta 3 which
- does a re-authentication on 401 errors (working as per attempt 2)
- does a retry on that particular 403 error

http://pub.rclone.org/v1.29-33-g085677d%CE%B2/

In my testing this works perfectly now - hopefully it will for you too.
 @ppuskari A new issue would be a good idea. Can you attach a log with -v to the issue too please? Something showing 'the kill'  because I'm unsure what you mean by that. Thanks Nick 
  Not at the moment, sorry.

I'm not really familiar with how server side encryption works with S3 - would #59 solve the problem for you?

I'm planning on implementing client side encryption soon in #219 which might also be of interest.
 Server side encryption for S3 will be in the 1.30 release which is imminent
  I'm afraid timeouts are a fact of life with cloud storage providers.  That is why rclone retries things and you can see that since it said "Errors: 0" at the end, all was successful.

What appeared to happen is that it transferred the file, got a timeout message, then on the retry it tried again.

If this happens regularly then I can investigate further, but if it just a one off thing I'd say that is just life!
 I'm going to close this as I think we've trashed this one out now!
  The filtering rules can only be used in one of the filter command line flags, so

```
rclone -v ls gdrv:boards/sayas/\*.jpg
```

Won't do what you think unfortunately - it will be trying to list a directory called `boards/sayas/*.jpg`

What will work is something like this

```
rclone -v --include "*.jpg" ls gdrv:boards/sayas/
```

Apologies for the confusion!

rclone could conceivably convert what you wrote into the equivalent with the `--include` flag.
 I've added this to the filtering docs - thanks for the suggestions

Note also that rclone filter globs can only be used in one of the
filter command line flags, not in the specification of the remote, so
`rclone copy "remote:dir*.jpg" /path/to/dir` won't work - what is
required is `rclone --include "*.jpg" copy remote:dir /path/to/dir`
  Good idea.  I've done that - thanks for the heads up!
  Well spotted!  Fancy doing me a Pull request?
 This will be in the v1.30 release
  rclone has adaptive rate limiting built in to the remotes, eg for google drive it starts off with a rate limit of one transaction every 10mS and backs off when it gets rate limited.

However it might nice if rclone could limit the number of files per second it uploads or downloads to avoid triggering the rate limiting code in the first place.

Make this a flag - say `--transfers-rate-limit` or `--transfers-per-second` or `--trlimit` and you'd give it a number (floating point) you wanted per second, or maybe a minimum time between transfers `--transfers-pacing` which you'd give in seconds.
 @kflu It looks like you've set `--low-level-retries` to 1?  If you leave it at the default of 10 then it will retry the directory listings and should complete successfully even in the presence of 429 errors.  429 errors are normal from Amazon Drive and rclone copes with them fine usually.
 @kflu If you run with -v it prints the command line args. You can't set it in the config file. It would be worth trying the latest beta if you haven't already at beta.rclone.org 
 @pjpalomaki this isn't done yet, I'm afraid...  It sounds like google are being more aggressive about rate limiting certain things...  `rclone move` should work exactly like `rclone copy src dst` followed by `rclone purge src`, and yes `--checksum` should verify the checksums.

Note that you can verify all the checksums with `rclone check src dst`.

The "409 conflict" errors probably indicate that the directory listing went wrong.  Can you check the logs for errors there and paste them here please? This might be related to #475 also.

I'm currently working on the whole directory listing thing - at the moment it doesn't pass errors back properly.

Can you also tell me which platform you are on so I can send you a beta to try?
 Just looking at your other issue makes me realise the reason the files didn't get deleted is that there were IO errors in the copy phase.  You should see `Not deleting files as there were IO errors` in the log.
 Are the troublesome files in a directory with 100s or 1000s of other files? If so this is very likely #475
 I'm pretty sure that the fix for #475 will fix this.  I'll post a more tested beta for you to try over the weekend most likely.
 Here is a beta for you to try which has the fix in: http://pub.rclone.org/v1.29-28-g36700d3%CE%B2/
 Any chance you could send me a log with -v of this? 
 Sorry your log got lost in my INBOX!

This is puzzling about your log

First we get this

```
2016/05/16 22:59:35 GLkzH8Vi1v4CyzjUqcwXYYMB/23pSIXvypFQ6TyR2VawjeoWy/jO1nG9PfZwFosS1nbLysIeuv/DZ19IzpLWNFNlTJaXJEWE856O3uDa0SEtgKXSENGGnpQZnH,xgsVqymHAY,XEXY1qAIwbFA7qWRUInP-KOCusk5q: Excluded from sync (and deletion)
```

And then this

```
2016/05/16 23:00:56 GLkzH8Vi1v4CyzjUqcwXYYMB/23pSIXvypFQ6TyR2VawjeoWy/jO1nG9PfZwFosS1nbLysIeuv/DZ19IzpLWNFNlTJaXJEWE856O3uDa0SEtgKXSENGGnpQZnH,xgsVqymHAY,XEXY1qAIwbFA7qWRUInP-KOCusk5q: Failed to copy: HTTP code 409: "409 Conflict", reponse body: {"logref":"0c109899-1bba-11e6-ac16-03d37e08e7dc","message":"Node with the name dz19izplwnfnltjaxjewe856o3uda0setgkxsenggnpqznh,xgsvqymhay,xexy1qaiwbfa7qwruinp-kocusk5q already exists under parentId jpbRd_tKTs64C_QJJVBG_w conflicting NodeId: uLHNHnvOStSWKmtKvqaZZQ","code":"NAME_ALREADY_EXISTS","info":{"nodeId":"uLHNHnvOStSWKmtKvqaZZQ"}}
```

So what that seems to me is that your exclude rule matched for ACD, but somehow didn't match for the local file system.

So when rclone came to copy the local file it didn't know it was supposed to be over-writing an existing file and hence created a duplicate.

This explains the consistent 409 errors perfectly.

But why is that happening?

Is it possible that your excludefolder doesn't have the same case as the actual folder in your local fs?

Hmm, or I wonder if it is the `--min-age 30d` that is the problem.  ACD doesn't support date based syncing but it will still be applying `--min-age 30d` to the folder which it shouldn't be. So if your file on ACD is > 30d old, but the file on the local file system is < 30d old then this would explain the problem perfectly.  The dates on ACD are the time the file was uploaded.

Does that make sense to you?

Can you verify this please?  If you send me the results of these two commands I can check the dates and make sure that makes sense (or you can if you prefer - just find a file whch had a 409 conflict and compare the dates for it).

```
rclone lsl /home/user/localfiles/
rclone lsl remoteacd:backup/folder
```

So assuming this is the problem, then I need to make the destination fs filtering ignore times and sizes (basically anything which could be changed by a new upload).

I'm going to consider the whole copy logic too as the equivalent problem with google drive is a duplicated file.  Getting the Upload method to check for an existing file first would slow things down though.
 I realised the above approach won't work so here is a different tack - check in acd and drive to see if the file exists before uploading.  This will prevent 409 errors (and duplicate files in drive's case) but slow the upload down slightly.

Can you give this beta a test a go and see what happens?

http://pub.rclone.org/v1.29-1-gbb75d80-56-g9579d50%CE%B2/

Thanks

Nick
 I've committed this to master now - if it doesn't fix the issue - please re-open it!

Thanks

Nick
  Rclone backs off in the Amazon approved way on 429 errors then retries the file. 

500 errors are a bit more unusual - perhaps a problem at Amazon? 

It is quite normal to see errors transferring files. 

You should find the low level retries or the full retries clears them up though. Check the number of errors that rclone outputs in  it's final message.
  move always deletes files after the copy.

If there are any IO errors during the copy then move won't delete the files.

I'll make this a bit clearer in the docs.
 @makdisse Not at the moment, no. Do you want to make a new issue?
 I've re-worked the move docs and hopefully made them clearer - I'm going to close this now
  Hmm, yes, the wizard isn't that clever ;-)

Thanks for reporting - I'll have a think on what to do.
  There are some notes in [CONTRIBUTING.md ](https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md).  Not exactly a step by step guide, but it will give you a starting point!

Start by studying one of the other remotes (if it is like s3 then start from there).
  rclone doesn't care whether a file is a hardlink or not (and neither does linux in general) so will transfer it just find.

```
$ echo "hello" > a
$ ln a b
$ ls -l
total 8
-rw-rw-r-- 2 ncw ncw 6 May  9 11:38 a
-rw-rw-r-- 2 ncw ncw 6 May  9 11:38 b
```

If you try to sync that directory, rclone will sync files a and b, but it will treat them as completely separate so sync the data twice.

rclone doesn't support anything like the `-H` flag for rsync.
  What is your question?
 You only need the web browser for the initial configuration. Rclone will run fine after that without. 

You can also configure rclone remotely - see http://rclone.org/remote_setup/
  rclone doesn't support custom filenames for the copy at the moment.  All the machinery is there, rclone just doesn't have a way to express it.  A new command `copyfile` might be the way to go.
 @nodgin good!  There is also the related `moveto`
 I'm going to implement two new commands `cp` and `mv` which will work like their unix counterparts and will fix this problem.
 I changed my mind after some effort trying to implement the semantics of `cp` and `mv` which really don't fit a system that needs lots of retries (they aren't idempotent).

So I implemented `moveto` and `copyto` as originally planned.

Find a beta here http://beta.rclone.org/v1.34-43-gc265f45/ (will be uploaded in 15-30 mins).  I'm working on reporting errors in directory listings better at the moment.

Here is a test compiled from the new branch which should report errors in directory listings properly, exiting with an error if there were any.

http://pub.rclone.org/rclone-listdir-v1.29-23-gdbfa703.zip

That is what I suspect you are going to see.  The alternative is no errors, in which case I'd be pretty sure it is an Amazon problem.  Amazon do seem to be a bit flakey with their directory listings...
 Send the log and I'll see what I can make of it! nick@craig-wood.com or share it on acd. 
 Thank you for those logs.

So the 17:00 and 17:40 seem to be OK but there are files missing in the 17:20 log.

The only errors in all the logs are these

lsl-160508-1720.log:2016/05/08 17:20:08 HTTP/1.1 429 Too Many Requests
lsl-160508-1720.log:2016/05/08 17:20:13 HTTP/1.1 429 Too Many Requests
lsl-160508-1720.log:2016/05/08 17:20:23 HTTP/1.1 500 Internal Server Error

This is almost certainly the cause of the problem...

Drilling into the first error, this is a directory listing broken into 3 chunks.

However in the 17:20 log one of the chunks is missing.

What it looks like is that when rclone retried the directory listing using the token for chunk 2 amazon actually supplied chunk 3.

This indicates that it isn't safe to retry a directory listing with a token.  

I've reported a bug to amazon here: https://forums.developer.amazon.com/forums/thread.jspa?threadID=13107

I'm testing out a fix - I'll post a binary for you to try in a bit
 I managed to reproduce the problem and I managed to make a fix for it.

Here is a binary for you to try: http://pub.rclone.org/rclone-v1.29-24-g2db35f0-listdir.zip

I don't recommend you use that binary for anything other than listing directories at the moment, but I'm pretty sure it will fix your problem. I need to integrate quite a few changes and do a proper merge, but that binary should be good for a test!
 Here is a better tested beta which should perform all the normal functions of rclone and have the fix: http://pub.rclone.org/v1.29-28-g36700d3%CE%B2/
 Please re-open this if it doesn't work!
 @jkrinke if you can capture a log that would be super helpful - thanks.
 BTW I tested this by listing a directory with two subdirectories, each with 10,000 files,  I then ran 250 simultaneous lists to stress the system a bit.  In that test with the latest beta above, rclone either returned all 20,000 files or returned an error, it never returned less than 20,000 files without an error.

Can you check if rclone is returning an error in your tests too?
 I think that is a different problem - see #493 for background.

I made a beta with a potential fix for that here (it has the changes in the version above too)

http://pub.rclone.org/v1.29-30-g53ead43%CE%B2/

That should specifically fix the token expired 401 problems.
 Here is a second attempt to fix the 401 errors - I think this one will work properly (cross fingers!)

http://pub.rclone.org/v1.29-30-g33611be%CE%B2/
 Glad to see the 401 retry is working.

The message `HTTP code 403: "403 Forbidden", reponse body: {"message":"Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header. Authorization=Bearer"}` is straight from S3 (ie the underlying storage of ACD) by the look of it.  ACD doesn't uses `SignedHeaders`.  So that is probably an Amazon bug.

I've made a thread about this on the developer forum: https://forums.developer.amazon.com/questions/28183/403-forbidden-reponse-body-messageauthorization-he.html

If you could get me the error response with `--dump-headers` Amazon will need that to trace through their logs.
  `rclone copy` does't delete files - you need `rclone sync` for that.

I wonder if you are running the dropbox desktop client elsewhere and it is deleting the files you are uploading? Or are you using some other tool simultaneously with rclone?

I don't know what the 16 byte files might be about either, not seen anything like that :-(

Can you run rclone with the `-v` flag and see if there is anything interesting in the log?
 Can you send me the result of `rclone lsl s3:bucket` so I can see what the source looks like and `rclone lsl dropbox:destination` so I can see what the destination looks like.  Either post it here or send it privately to nick@craig-wood.com

Can you send me the log of the transfer too please (the one you made with `-v`).

Thanks

Nick
 I'm going to close this as we haven't made any progress on it recently.  If you have any more ideas please re-open.  Thanks, Nick
  Thanks for recounting your experience.

I have had some communication with Amazon about rclone rate limiting but that is a general rate limit applied to all rclone users.  I think they are happy that rclone is just quite popular (>16M API calls per hour) and not attacking their infrastructure and have relaxed API limits for rclone in general.

I'm not aware of Amazon having per customer rate limits, but it doesn't surprise me.  I know Google do the same with Google Drive so I expect it is a natural consequence of trying to run such a big service for so many users.
 Hmm possibly, or maybe a per user via the public API (as I think the website uses a different API)
 @a5m0 rclone is entirely client side, however it uses an application key from amazon.  You can get your own application key and use rclone with it of course.  The process isn't entirely straight forward though and it involves getting approval for the application.
 @left1000 do you fancy making a ticket with allowing the retry timer customization with a flag?  It would be quite easy to allow the user to set the base retry interval (1 second) and the normal pace of api queries (10 mS).
 Seeing the same issue here. When I request a new token it works for like 5 minutes. Using `--transfers=1` seems to reduce the time it takes.

This seems to be app-specific throttling at Amazon. What a piece of BS - so much for "unlimited" storage - guess I cannot my stuff even download without using their client when I cancel my subscription.
 https://forums.developer.amazon.com/questions/27740/401-messagetoken-has-expired.html
 Pressed return too soon!  On the amazon cloud drive forum there are lots of reports of 401 error problems.

https://forums.developer.amazon.com/questions/27740/401-messagetoken-has-expired.html

I made an attempted work-around here: http://pub.rclone.org/v1.29-30-g53ead43%CE%B2/

What seems to be happending is that the tokens are expiring before their valid to date.  rclone doesn't normally refresh tokens until they are passed that date.  The code above does do that so may help!
 https://forums.developer.amazon.com/questions/27740/401-messagetoken-has-expired.html
 Here is a current thread about the 401 errors

https://forums.developer.amazon.com/questions/27740/401-messagetoken-has-expired.html

And here is an attempted work-around: http://pub.rclone.org/v1.29-30-g53ead43%CE%B2/
 @dimatter can you send a log with -v please of what happens around the 401 error? 
 Here is a second attempt to fix the 401 errors - I think this one will work properly (cross fingers!)

http://pub.rclone.org/v1.29-30-g33611be%CE%B2/
 Glad to see the 401 retry is working.

The message `HTTP code 403: "403 Forbidden", reponse body: {"message":"Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header. Authorization=Bearer"}` is straight from S3 (ie the underlying storage of ACD) by the look of it.  ACD doesn't uses `SignedHeaders`.  So that is probably an Amazon bug.

I've made a thread about this on the developer forum: https://forums.developer.amazon.com/questions/28183/403-forbidden-reponse-body-messageauthorization-he.html

If you could get me the error response with `--dump-headers` Amazon will need that to trace through their logs.
 @rosahas the 401 errors in your log should be fixed by the beta above.

I've managed to capture a 403 error myself and I've added it to the [developer forum post](https://forums.developer.amazon.com/questions/28183/403-forbidden-reponse-body-messageauthorization-he.html)
 @rosahas yes that is the right code and from a second look at  your log it looks like it is successfully refreshing the token on 401 errors.  Are you still having problems?
 OK Here is beta 3 which
- does a re-authentication on 401 errors (working as per attempt 2)
- does a retry on that particular 403 error

http://pub.rclone.org/v1.29-33-g085677d%CE%B2/

In my testing this works perfectly now - hopefully it will for you too.
 @kpabba great glad it worked!

There is a ticket about Box support #97 which I haven't had time to work on. I suggest you follow that to show your support :-)
 This is all fixed now!
  I think this is the root cause of the error

```
2016/05/05 19:59:20 backup-20160504.tar.gz.gpg: Failed to stat /home/cameron/backup-20160504.tar.gz.gpg: lstat /home/cameron/backup-20160504.tar.gz.gpg: no such file or directory
```

It shouldn't cause a nil pointer deref - I'll investigate why it does that later.
 I've fixed this now.

Here is a beta for you to try - please re-open the ticket if you have issues with it.

Thanks

http://pub.rclone.org/v1.29-18-gd205dc2%CE%B2/

PS The strange upload bandwidths you see are caused by the large chunk size you are using.  You can see by the overall stats that the transfer is progressing in a regular fashion.
 Here is the result of a test I did so the bigger the better!

![drive-speed](https://cloud.githubusercontent.com/assets/536803/14474109/379b642e-00f3-11e6-80c1-7aacaea7f197.png)
  You can use the `rclone dedupe` command to de-duplicate your drive first.  See the relevant section in [the docs](http://rclone.org/docs/).  This can rename the duplicate file names to be different then then will copy to S3 just fine.

Is that helpful?

rclone doesn't support copying files from the trash at the moment, but if you restore it, then it can copy them.
 It would be relatively easy to make rclone look in the trash too, in drive/drive.go you'd just change this to true

```
    query := fmt.Sprintf("trashed=false")
```

On a flag would be best, eg `--drive-trash-only`

Then you would pass a flag `rclone --drive-trash-only copy drive: /blah` to copy stuff out the trash.

Fancy having a go at that?
  Check out this page which has some different ways of configuring rclone remotely: http://rclone.org/remote_setup/

I don't think binding rclone to 0.0.0.0 would work unfortunately, as as part of the setup of oauth apps you need to specify the actual address used for the return.  127.0.0.1 is what I've specified so that is what you have to use.
 @nodegin that is clever! I think `rclone auth` is easier but YMMV!
  This probably needs a flag in the config as to whether it is an Office365 account or not?
  I don't see anything obvious in the OpenELEC changelog.

rclone will find the cacerts if they are in any of these directories.

```
"/etc/ssl/certs/ca-certificates.crt", // Debian/Ubuntu/Gentoo etc.
"/etc/pki/tls/certs/ca-bundle.crt",   // Fedora/RHEL
"/etc/ssl/ca-bundle.pem",             // OpenSUSE
"/etc/pki/tls/cacert.pem",            // OpenELEC
```

But I suspect they aren't.

If you try this (from the FAQ) then it should make rclone work again

```
mkdir -p /etc/ssl/certs/
curl -o /etc/ssl/certs/ca-certificates.crt https://raw.githubusercontent.com/bagder/ca-bundle/master/ca-bundle.crt
```

I don't know what the exactly problem is though, whether it is an OpenELEC update or if the update went wrong.
 All the places are in `/etc` except for this one `/system/etc/security/cacerts` so you could try putting the `ca-certificates.crt` file in there.

Does OpenELEC have `cur`l or `wget`?  Try them on an https site and see if they work - I suspect they will give an error which will give you something to report to OpenELEC...
 I've reported an issue about this on the openelec project OpenELEC/OpenELEC.tv#4941 as it is a change from 6 -> 7 which has caused rclone to stop working.
 I forgot about `–no-check-certificate` - give it a go. Not sure if it will work for `rclone config` or not, if not you can copy the config from a different machine see the section here http://rclone.org/remote_setup/

Let me know if it does work for `rclone config` and if not I'll make it work.
 Looks like openelec have fixed the issue in OpenELEC/OpenELEC.tv#4941  so hopefully it will be working in the next Beta.

I've made a separate issue about the --no-check-certificate not working #468

I'm going to close this one now - thanks for reporting the problem.
  rclone should be doing this already - I've never had to make a new key.

What error message do you get?

Can you send me a log with `-v` please of the problem?

Thanks

Nick
 rclone calls the `b2_authorize_account` method every time it starts, and whenever it gets a 401 error so it really shouldn't require you to put a new one in every 24 hours.

rclone doesn't use python so that shouldn't affect things.

Can you send a log with `-v` of it going wrong please?

Thanks

Nick
 That bug I've fixed (I think!).

Are you definitely running rclone v1.29 - check the output of `rclone -V` ?

If you are, can you send me a log of what happens with the `--dump-bodies` flag ideally?  It should start with rclone calling `b2_authorize_account`.  (Delete lines with  authorization in before posting), eg try

```
rclone --dump-bodies lsd b2:
```
 Great, thanks
  After a quick look this does look like a bug. Rclone is treating "remote:Windows Fat" as a local directory rather than a remote.  Will investigate further! 
 I'm away from my computer at the moment - I'll check the source when I get back and all will become clear! 
 Use `F:\` (no quotes).

However, there is something weird in the args parsing. `"f:\"` is not the same as `f:\`. Something is doing some escape-stuff. Windows explicitly claims not to do escape processing on `\`, so it may be some quirk with the flag parser.
 I think you must have written this originally

```
rclone copy -v --log-file=logfile.txt "F:\" "remote:Windows Fat"
```

In the pastebin the first line is

```
2016/04/29 20:35:26 Local file system at : Replacing invalid characters in "\" remote:Windows" to "F:_ remote_Windows"
```

indicating that Windows thought rclone was being run with two arguments
- `F:\" remote:Windows`
- `Fat`

So I think this is caused by oddities in the windows command line parser as Klaus suggested, so

```
rclone copy -v --log-file=logfile.txt F:\ "remote:Windows Fat"
```

Should fix it

This is caused ultimately by [CommandLineToArgvW](https://msdn.microsoft.com/en-us/library/windows/desktop/bb776391%28v=vs.85%29.aspx) which states:

CommandLineToArgvW has a special interpretation of backslash characters when they are followed by a quotation mark character ("), as follows:
- 2n backslashes followed by a quotation mark produce n backslashes followed by a quotation mark.
- (2n) + 1 backslashes followed by a quotation mark again produce n backslashes followed by a quotation mark.
- n backslashes not followed by a quotation mark simply produce n backslashes.

The original command line had a mis-matched quote `"` so it would have been nice if it had given an error.
 @left1000 I don't think this is a silly question at all - it exposes a bit of windows oddness.  I should probably put this either in the docs somewhere!
 I've made another issue to write some more docs on the windows shell here #473

I'll close this ticket now as I think we've got to the bottom of it.
  I think this is a duplicate of #451

I've borrowed a mac from work for the weekend and I'll see if I can fix it!
 I have fixed this now.  Here is a beta for you to try.  Please re-open the ticket if you have any problems with it!

http://pub.rclone.org/v1.29-16-g8b2f6fa%CE%B2/
  Good idea.

I could see this working with a flag `--b2-no-versions` which if set would mean
- after uploading a file, delete any older versions of that file

Or alternatively I could add a flag (or command) which deleted all the older versions.  If it was a command it could be called `emptytrash` or `cleanup` or something like that.  This could then be adapted to the other Fs which have the concept of trash that you can empty (eg Drive).
 I'm not working on this at the moment, so any help much appreciated :-)  To implement the `cleanup` command I'd add an optional interface `CleanUp` to the Fs which would then remove all the trash for a given Fs.  This could then be implemented for b2 and later for drive, acd, onedrive all of which have trash which can be emptied.
 Here is a beta with the `cleanup` command: http://pub.rclone.org/v1.30-21-g018fe80%CE%B2/
 Hello @ncw 
When I move the files in local and run sync, the latest previous file version is changed to 'hidden' state.
The size of the file version is zero, but it still shows in the file lists of the bucket.

To delete hidden and alone files in cleanup operation, I wrote the code. It works well in my case.

Now, I am curious about the reason to remain the hidden files. Are there any speical reasons?
If not, could I push the branch and pull requests?

Thank you
Yours sincerely
  If you get this again then try with -v and -dump-headers and that will give us some idea what is going on. 
  Try without the storage_url you shouldn't need that. 

If that doesn't work then try the lsd with --dump-bodies and post the output (remove the Authorization lines). 
 Is it possible you got your username or password wrong? That is what is looks like. The second attempt tries to log in the racks pace way which fails too. 

Has your password got any funny characters in? (non alphabetic?) 

Can you log in with a different tool (eg the openstack swift client?) 
 Examining the service catalogue returned, there are no entries for swift in there.

In fact there is nothing in the `serviceCatalog` at all.

Has the user you are trying to use got permissions to use swift?

``` javascript
{
    "access": {
        "metadata": {
            "is_admin": 0,
            "roles": []
        },
        "serviceCatalog": [],
        "token": {
            "audit_ids": [
                "secret"
            ],
            "expires": "2016-04-28T21:30:27Z",
            "id": "secrettoken",
            "issued_at": "2016-04-28T20:30:27.191649"
        },
        "user": {
            "id": "870aab4e3af6441b8b5ebfb0dcaa8412",
            "name": "saverio.proto@switch.ch",
            "roles": [],
            "roles_links": [],
            "username": "saverio.proto@switch.ch"
        }
    }
}
```

I'd expect to see something more like this - note the `serviceCatalog.endpoints.publicURL` and `token.id` which is what the swift module is looking for.

``` javascript
{
    "access": {
        "serviceCatalog": [
            {
                "endpoints": [
                    {
                        "adminURL": "https://csproxy1.memset.com/...",
                        "internalURL": "https://csproxy1.memset.com/...",
                        "publicURL": "https://csproxy1.memset.com/...",
                        "region": "reading"
                    }
                ],
                "name": "memstore",
                "type": "object-store"
            }
        ],
        "token": {
            "expires": "2016-04-28T22:42:54.334167",
            "id": "...",
            "tenant": {
                "id": "...",
                "name": "..."
            }
        },
        "user": {
            "id": "admin",
            "name": "admin",
            "roles": [
                {
                    "id": "admin",
                    "name": "object-storage:admin",
                    "tenantId": "..."
                }
            ]
        }
    }
}
```
 I'm not sure what keystone version that is, but you should get the endpoints returned from a correctly configured keystone.  That is why I guess you've had to specify the storage_url to the openstack swift client - you shouldn't have to do that - it should read it from the service catalog.

So I would guess it is a problem with keystone config

One thing to try would be to put the region in the rclone config..
 Great - well done! 

Does that work without the storage url now? 
 I've written some more docs about these errors see commit below!
  A little bit of following the backtrace reveals the crash is here

github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:559

``` go
    return &UploadOutput{
        Location:  *complete.Location, // this is nil
        VersionID: complete.VersionId,
        UploadID:  u.uploadID,
    }, nil
```

Which means that ceph is returning the `CompleteMultipartUploadOutput` struct with a nil `Location`.

I don't know whether this is valid or not but this patch to the AWS library should stop the crash

``` patch
diff --git a/service/s3/s3manager/upload.go b/service/s3/s3manager/upload.go
index 4132bcb..b2d1afd 100644
--- a/service/s3/s3manager/upload.go
+++ b/service/s3/s3manager/upload.go
@@ -8,6 +8,7 @@ import (
        "sync"
        "time"

+       "github.com/aws/aws-sdk-go/aws"
        "github.com/aws/aws-sdk-go/aws/awserr"
        "github.com/aws/aws-sdk-go/aws/awsutil"
        "github.com/aws/aws-sdk-go/aws/client"
@@ -556,7 +557,7 @@ func (u *multiuploader) upload(firstBuf io.ReadSeeker) (*UploadOutput, error) {
                }
        }
        return &UploadOutput{
-               Location:  *complete.Location,
+               Location:  aws.StringValue(complete.Location),
                VersionID: complete.VersionId,
                UploadID:  u.uploadID,
        }, nil
```

Here is a build of rclone with that patch (for linux/amd64) - let me know what happens with that!  If it works I'll try to upstream the patch to the aws library.

http://pub.rclone.org/rclone-v1.29-aws-patch.zip
 Great!  I'll propose it upstream - thanks for testing.
 The patch has been accepted upstream which means it will be in the next release :-)
  I don't think the `whatis parse for rclone(1) failed` is a problem - it is only a warning.  It probably means rclone won't appear in the whatis database.

As for `Failed to load config file "/root/.rclone.conf" - using defaults: open /root/.rclone.conf: no such file or directory` - then next step is to run `rclone config` to make the configuration for the remote fs.

So yes rclone should work fine with ubuntu 14.04 (I develop it on ubuntu!).
  rclone doesn't run in the background (at the moment) it only syncs when you run it.

If you want to sync regularly then you can run it off cron.

There is a ticket about adding the ability to run as a daemon and listen to file system events #249 - any help appreciated!

Thanks

Nick
  I'll do this as part of making rclone an official B2 integration
 @i2plover not yet, sorry but it is on the list for the next release :-)
 I'm intending to do this and submit rclone for b2 integration soon, however events keep catching up with me!

Any help would be much appreciated, so if anyone wants to help I can give pointers.
 Here is the first attempt at large file support. Any files over 5GB it will upload in chunks.  You can set the threshold for chunk uploading with `--b2-upload-cutoff` and the size of chunks with `--b2-chunk-size`.  Note that the chunks are buffered in memory so don't make them too big!  96M is the default (the minimum is 100,000,000 bytes).

You can test with files < 5GB by using `--b2-upload-cutoff 100M` say.

http://pub.rclone.org/v1.29-1-gbb75d80-58-g8139881%CE%B2/

Interested in any feedback - send a log file with `-v` if you find a problem!

Thanks

Nick
 @mdeboer thanks for testing!  Yes the large file upload is single threaded at the moment.  I'll make it multi threaded in another ticket - #531.  I wanted to make it work first before making it super fast!

rclone is capable of uploading many large files at once though (see `--transfers` flag).

A 503 error should cause it to retry the chunk as you've discovered.

This code will be in the 1.30 release which will be shortly
 @ZacharyDuBois No problems!  B2 now enforce a different upload URL per thread and rclone does that already :-)
 @ZacharyDuBois rclone did that at one point but I fixed that a version or two ago.  Do you remember which version that was? 
 @ZacharyDuBois This was fixed in 6b6b43402b14a607695d9c05bff643c1837c81ea which was released in v1.28 so I'd be interested if you were using an earlier version to that or whether it is broken!
 @hajes sorry to hear that.  Can you make a new issue with a log with `-v` of it happening please?

Thanks

Nick
 @hajes can you stick that in an new issue please?
  I'm pretty sure this is the same problem as #399

This is caused by a known bug in onedrive which they haven't been fixing for the last 18 months!

I'm leaning towards the `--no-check-size` option as you'll see from that which is a work-around.
 I've implemented an `--ignore-size` flag which you can use to work-around this issue.  I've verified it does the right thing.

Here is a beta with that fix in for you to try.

http://pub.rclone.org/v1.29-1-gbb75d80-62-g46135d8%CE%B2/
  Rclone syncs one source to one destination at a time.

When you run rclone config it creates a different section in the config file under the name you give it. 

So of you create two remotes, one for Google drive called gd and the other for amazon cloud drive called acd then you can sync to them with two separate commands, eg

```
rclone sync /path/to/source gd:backup
rclone sync /path/to/source acd:backup
```

I hope that helps! 
 I'd suggest you write a little shell script with the syncs you want, and use one of the techniques from here

https://ma.ttias.be/prevent-cronjobs-from-overlapping-in-linux/

To make sure that only one copy of the shell script executes at once
 Not at the moment. I think the best sort of config file for that would be a shell/powershell script.
 I'm going to close this as I think the original question has been answered.
  Bucket to bucket sync uses a server side copy.

I wonder if there is an oddity with that and Ceph.

Did all the files that failed to copy have spaces in the names maybe?
 It would be interesting if you could run the copy with `--dump-bodies` and paste here failing transaction (remove any Authorization: lines first).
 I think this is very likely the same problem as #586 - ceph failing to decode the `X-Amz-Copy-Source` header properly.
 I'm going to close this in favour of #586 where we are waiting for dreamhost to update ceph to fix this problem.
  Can you send a log with `rclone -v` please?

I suspect this is related to unicode normalisation on OS X file systems - see #194

Unfortunately I don't have access to a mac to test :-(
 There are some notes (somewhat brief!) on how to setup go and install rclone for compiling  in [CONTRIBUTING.md](https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md)

Fixing #194 will fix this.

What is needed is a function which normalises OS X filenames into UTF-8, and a function which does the reverse.  These then need to be applied in the local file system (one in cleanUtf8Name). It isn't a completely straight forward change!
 I have borrowed a mac from work for the weekend to have a go at this.
 I have fixed this now.  Here is a beta for you to try.  Please re-open the ticket if you have any problems with it!

http://pub.rclone.org/v1.29-16-g8b2f6fa%CE%B2/
 @eduardbosch thanks for testing! If you get time to contribute one day then at least the test suite will pass on OS X now ;-)
  I've merged that thanks.

I dropped the changes to MANUAL.txt/md and rclone.1 as these will get auto updated at the next release when I build the docs.

Thanks

Nick
  Try the amd64 version would be my guess. 
 Are you using the 32 bit or 64 bit version of rclone?  What about your OS is it 32 or 64 bit?

Can you send a log of the error please?

I suspect this is the problem: http://william.shallum.net/random-notes/32-bit-golang-trace-breakpoint-trap-modify_ldt-enosys

So can you try `echo 1 > /proc/sys/kernel/modify_ldt` ?
 Looks like this is a known issue in the go runtime which will be fixed for the next go release (due on around 1st August) golang/go#14795 .

Here is a release of rclone compiled with go-tip for you to try

http://pub.rclone.org/rclone-v1.29-29-gcadf202-tip.zip
 This will be fixed when go 1.7 is released and I make new new rclone release with it - some time after 1st August most likely.
 Go 1.7 released now so this fix will be in v1.33.

Here is a beta to try: http://pub.rclone.org/v1.32-17-g3f71078%CE%B2/ - please re-open if it doesn't work!
  Good idea thanks!

I think just doing the minimum for the moment is fine.

Can you update the docs in docs/content/swift.md too please?  In particular the walkthrough.

Thanks

Nick
 I've merged that - thank you very much for your contribution.

Nick
  That looks really useful thanks.

I put a few suggestions for changes / fixes on the code as line notes.

Can you up date the docs in docs/content/googlecloudstorage.md too please?

The new config option needs showing in the walk through, and if you could write a section on how to get the JSON credentials file that would be very useful.

Thanks

Nick
 If you could rebase, squash and force push to update that would be appreciated!  Thanks
 I've merged that thank you!  Yes the Context stuff is rather cryptic!

I merged 3a1198cac5bf0b51e186acd7c280947744dd3062 afterwards which stops it trying to do the oauth flow if you specify the service account credentials - does that look OK to you?

Thanks you very much for your contribution

-- Nick
  Thanks for those - will merge shortly
 :+1: 
 I've merged that - thanks for your contribution :-)
  Firstly that is what Google drive does.  rclone does its best to retry stuff and it normally works pretty well.

It is worth trying v1.29 (just out) which has had the google drive retry schedule tweaked - that should improve things.

You shouldn't need to change checkers / transfers

You can keep retrying the sync until it runs with 0 errors (or use the `--retries` flag).
 EOF errors usually indicate something wrong with Google Drive or your internet connection.

The other error `An established connection
was aborted by the software in your host machine` means the same thing.

It could be your ISP closing connections at busy periods - I've seen that, or your router dropping connections - can you try using a different internet connection?

It is worth trying v1.29 as it has a number of google drive fixes.
 I made a patch for rclone which treats those sort of errors as worth retrying which should help when uploading big files.

Find the beta here

http://pub.rclone.org/v1.29-14-g1752ee3%CE%B2/

Let me know how you get on!

Thanks

Nick
 How big was the file it got stuck on?

Can you try again with the `-v` flag and see if it does some low level retries?
 If you could post a log with `-v` that would be very helpful. Of if you wanted to email me with it privately that would be fine too ( nick@craig-wood.com ).

I want to make sure the low-level retries are working with the beta for that `wsaend` error before speculating on how to improve the situation!

Thanks

Nick
 Assuming you ran it with `-v` I'd like to see the bit before that message - it should talk about retries hopefully.
 Thanks for the log.  I can see the retry didn't work :-(  You can see it trying the chunks, and when it gets the `wsaend` error, it just gives up.

I have an idea as to why - I'll post a beta for you to try in a little while!
 Here is a new beta for you to try - hopefully this will work!

http://pub.rclone.org/v1.29-1-gbb75d80-53-gf17cb1b%CE%B2/
 Third time lucky!

http://pub.rclone.org/v1.29-1-gbb75d80-55-g7fe653c%CE%B2/
 This appears to be fixed now :-)

Follow up bug in #520
  That command looks like rclone correctly synced the data - what makes you think it didn't?

What does

```
rclone check /home/norman/Desktop/html/ drive:/corsair/html
```

And

```
rclone size /home/norman/Desktop/html/
rclone size drive:/corsair/html
```

say?

(To get the data into the file use `2>&1` after the `> output`)
  How did you upload those files to dreamhost?

Is there a 0 sized file or something like that marking each directory?

If you do `rclone ls dreamhost:bucket` do you see files marking the directories?
 Here are some ideas for work-arounds

You could ignore all zero length files by adding `--min-size 0.001k` to the sync (syntax to be improved in #449)

You could delete all the zero length files (this actually would delete all 0 and 1 length files beware - see #450)

```
rclone -v --dry-run --max-size 0.001k delete remote:path
```

Remove `--dry-run` if happy.
 @jordigg I don't understand exactly what you mean?  Can you make a new issue, preferably with a log of the problem?  Thanks Nick
 I'm going to close this ticket now as I think we've finished the discussion.  Thanks, Nick
  Can you run `rclone size` on the source and the destination and post what it says? Eg something like
- `rclone size backups/daily.0/`
- `rclone size backb:$(date +%Y-%m)/websites/`

If you run top while the sync is running, then can you see the memory used by rclone increasing? 

Can you also try the latest beta from here: http://pub.rclone.org/v1.28-27-g0f8e7c3%CE%B2/ which has lots of B2 fixes.  (I'm just about to make a release!).

Thanks

Nick
 You have 2M files.  rclone stores a description of each one in memory while syncing.

Each file description might take might take ~200-600 bytes.  So I would expect this to take 0.5GB - 1.5GB of memory.  

SInce you are running it on a 2GB machine it is running out of memory, but probably only just.

Can you add some swap? Or run it from a machine with more memory?

I plan to fix this at some point, but in the meantime can you split the sync up into separate ones for each subdirectory?
 See #236
 I've halved the memory usage for v1.32 so you might want to have a go with that.

You can also use `rclone --no-traverse copy src dst` for minimal memory usage.
 I think using `rclone --no-traverse copy src dst` will to fix your problem at the cost of not deleting files in the remote that are deleted locally.

Can you try again with v1.33 that might use less memory too.
 I'm going to close this as I haven't heard from you for a while - please re-open if you have more info!

Thanks

Nick
 How much memory is rclone using?  And how much free memory do you have on your machine?

If you run this in another terminal while rclone is running it will produce stats on memory usage

    while [ true ]; do ps -C rclone -o pid=,%mem=,vsz= ; sleep 1 ; done

Note that for b2, rclone has to buffer the chunks for large files in memory.  It should only ever use `--transfers` * 96MB of ram though for that.

If you could also run `rclone memtest b2crypt:` that would be interesting as would a comparison with `rclone memtest b2:bucket`. Here is a beta for a low memory sync which I think should fix this problem.

tThere is a temporary flag `--old-sync-method` to select the old sync method.

It should work identically to current rclone, except using a lot less memory.

http://pub.rclone.org/v1.35-50-g9d88ce3-low-mem-sync%CE%B2/

It is a major re-work of the syncing internals.  All the unit tests pass, and the code is well covered.  My own smoke tests pass too.

Test with care - use --dry-run first!

I'm particularly interested in 
  * any bugs
  * memory usage differences before vs after - especially on big syncs
  * speed differences before vs after

You can run this little ditty in another window to see rclone memory usage

    while [ true ]; do ps -eo rss,args --sort rss | grep [r]clone ; sleep 0.5 ; done @sebastianmack Thanks for testing. I think that the beta I posted will work fine for the original posters problem, but it doesn't seem to have helped yours at all.

I suspect you are right, that it is to do with uploading large files.

I had a look at the code and came to the same conclusion that you did, that is it allows 2 * transfers chunks to be uploaded at once which agrees with your log - it shouldn't allow more than that.

So for transfers = 4, that should use about 4 * 2 * 96M =~ 800M which is more or less what your logs show for mem4 & betamem4.  The mem1 & betamem1 logs seem to show too much memory usage though.

I'll think about how to reign in the memory usage to 1 * transfers meanwhile I want to make sure that it isn't using more memory than that.

I've made another beta for you (doesn't have the low-mem-sync code in) to test which uses a buffer pool to keep the memory used by the transfers in.  This should be a bit more efficient hopefully.  Can you see what the memory usage is with `--transfers 1` and `--transfers 4` like before?

http://pub.rclone.org/v1.35-55-gba74fdf-b2-buffer-pool%CE%B2/

It will print some debug like this which will help working out which blocks are in use (with `-v` flag).

```
2017/01/29 22:18:26 B2 bucket test-isos: Getting upload block 0xc4274a6000
2017/01/29 22:18:27 B2 bucket test-isos: Getting upload block 0xc42d4a6000
2017/01/29 22:20:42 B2 bucket test-isos: Returning upload block 0xc4274a6000
```

Also, what happens to the memory use when you do a sync where there are no files that need uploading?  It should use minimal memory and you should see the difference between the low-mem-sync beta and the normal rclone.

Thanks for your help tracking this down!
 OK I've had a chance to look through the code and make sure that exactly the right number of transfers happen.

Here is the beta:

http://pub.rclone.org/v1.35-55-g3697a9f-b2-fix-mem-439%CE%B2/

Can you test this one out please? (ignore the `buffer-pool` one above - that approach is flawed!) Ok I've merged this to master for the 1.36 release - here is the beta

http://beta.rclone.org/v1.35-61-g9165691/ (uploaded in 15-30 mins) PS And thanks for helping me track this down :-) I'm not going to close this until I've merged the low memory sync code I have merged the low memory part of this master now - see here for a beta:

http://beta.rclone.org/v1.35-74-g48cdedc/

Any feedback much appreciated
 @brsf  thanks for testing :-)

 @kohts How much free RAM has the system you are running rclone on got?

It looks like it blew up when it got to about 400M....  Note that the default for `--transfers` is 4, and due to the way large file uploads work with b2 it will use 4 * 100M of memory for buffers.

So if you want it to use less memory, then you'll have to set `--transfers` lower.  Try 2 and see what happens. @kohts glad it works :-)  See also http://rclone.org/donate/ :-)  rclone will restart an interrupted sync just fine, however note that the sync works on a file by file basis, so if it was half way through transferring a big file then it will have to start again with that file.  That only really makes a difference if you are transferring HUGE files though.
 I'm going to close this issue as I haven't been able to reproduce the problem.
  Did it print anything else other than illegal instruction?  If so can you paste it here please.

What sort of hardware is it?

Can you paste the result of `cat /proc/cpuinfo`?

Can you confirm that the md5sum of the binary is

```
$ md5sum rclone
b4d6fd906383477fd68857469d0c1513  rclone
```

Thanks
 Pentium III doesn't have SSE2. I believe you have to set `GO386=387` environment variable when compiling to use legacy float point instructions.
 Exactly. Unfortunately you will have to compile your own to get a version without SSE2 instructions.
 @edd3a I'm not currently doing a 386 with 387 build.

Here is one for you to try which I built with the command (as suggested by @klauspost - thanks!)

```
GO386=387 GOARCH=386 GOOS=linux go build -v
```

http://pub.rclone.org/rclone-v1.28-30-gdd36264-387-387.zip

Let me know if that works or not!
 This will be fixed when I start compiling rclone with go 1.7 which will be 1st august I think
 Go 1.7 released now so this fix will be in v1.33.

Here is a beta to try: http://pub.rclone.org/v1.32-17-g3f71078%CE%B2/ - please re-open if it doesn't work!
 @wtangerine glad you found a work-around.

I could add a  GO386=387 official build, but I'm not sure it is worth it - what do you think?  I think just about all linux installations are 64 bit now-a-days so slowing down the 386 build a bit for increased compatibility sounds like a reasonable plan. Alternatively I could make a new build -386-no-sse.

Can you try this binary and see if it works for you?  I cross compiled this with `GOARCH=386 GO386=387 go build`.

[rclone-v1.36-22-gbc25190-no-sse.zip](https://github.com/ncw/rclone/files/910717/rclone-v1.36-22-gbc25190-no-sse.zip)
 @wtangerine no rush!  I expect that build to work (cross fingers). > not sure if that makes any difference but the "file" command reports my executable as dynamically linked, while your executable as statically linked.

I'm confused by "my executable" and "your executable".  How did you generate your executable?  Does it work?

The executable I sent you was statically linked.

Here is the latest rclone beta compiled with  `GOARCH=386 GO386=387 go build`
[rclone-v1.36-48-g58a82cd-no-sse.zip](https://github.com/ncw/rclone/files/981640/rclone-v1.36-48-g58a82cd-no-sse.zip)

I tried to see if I could see those instructions you mentioned. 

```
$ objdump -d rclone | grep ucomisd
$ objdump -d rclone | grep movq
 8049351:	0f 6f 00             	movq   (%eax),%mm0
 8049354:	0f 7f 44 24 08       	movq   %mm0,0x8(%esp)
 80496d5:	0f 6f 00             	movq   (%eax),%mm0
 80496d8:	0f 7f 03             	movq   %mm0,(%ebx)
 80496f1:	0f 6f 44 24 08       	movq   0x8(%esp),%mm0
 80496f6:	0f 7f 00             	movq   %mm0,(%eax)
 80946e4:	f3 0f 7e 00          	movq   (%eax),%xmm0
```

The "0f 6f 00" instruction is in your original list.  This is part of the implementation of sync/atomic.LoadUint64 which makes sense as these routines are written in assembler.

This looks like this (from src/sync/atomic/asm_386.s)

```
TEXT ·LoadUint64(SB),NOSPLIT,$0-12
	MOVL	addr+0(FP), AX
	TESTL	$7, AX
	JZ	2(PC)
	MOVL	0, AX // crash with nil ptr deref
	// MOVQ and EMMS were introduced on the Pentium MMX.
	// MOVQ (%EAX), %MM0
	BYTE $0x0f; BYTE $0x6f; BYTE $0x00
	// MOVQ %MM0, 0x8(%ESP)
	BYTE $0x0f; BYTE $0x7f; BYTE $0x44; BYTE $0x24; BYTE $0x08
	EMMS
	RET
```

Which seems to claim that those instructions should work.

Can you work out which instruction exactly you are getting the "illegal instruction" fault on?  You should be able to do this by running rclone with gdb, wait for it to fault, then run "layout asm".
 From rclone-v1.36-22-gbc25190-no-sse.zip which does fault

```
0807aa10 <runtime.check>:
 807aa10:       65 8b 0d 00 00 00 00    mov    %gs:0x0,%ecx
 807aa17:       8b 89 fc ff ff ff       mov    -0x4(%ecx),%ecx
 807aa1d:       3b 61 08                cmp    0x8(%ecx),%esp
 807aa20:       0f 86 85 04 00 00       jbe    807aeab <runtime.check+0x49b>
 807aa26:       83 ec 40                sub    $0x40,%esp
 807aa29:       c7 44 24 24 00 00 00    movl   $0x0,0x24(%esp)
 807aa30:       00 
 807aa31:       0f 57 c0                xorps  %xmm0,%xmm0
 807aa34:       f3 0f 11 44 24 20       movss  %xmm0,0x20(%esp)
 807aa3a:       f3 0f 11 44 24 1c       movss  %xmm0,0x1c(%esp)
 807aa40:       0f 57 c0                xorps  %xmm0,%xmm0
 807aa43:       f2 0f 11 44 24 30       movsd  %xmm0,0x30(%esp) # FAULT HERE
 807aa49:       f2 0f 11 44 24 28       movsd  %xmm0,0x28(%esp)
 807aa4f:       c7 44 24 3c 00 00 00    movl   $0x0,0x3c(%esp)
 807aa56:       00 
```

From rclone-v1.36-48-g58a82cd-no-sse.zip which doesn't fault

```
0807ae20 <runtime.check>:
 807ae20:       65 8b 0d 00 00 00 00    mov    %gs:0x0,%ecx
 807ae27:       8b 89 fc ff ff ff       mov    -0x4(%ecx),%ecx
 807ae2d:       3b 61 08                cmp    0x8(%ecx),%esp
 807ae30:       0f 86 20 05 00 00       jbe    807b356 <runtime.check+0x536>
 807ae36:       83 ec 48                sub    $0x48,%esp
 807ae39:       c7 44 24 24 00 00 00    movl   $0x0,0x24(%esp)
 807ae40:       00 
 807ae41:       d9 05 38 04 85 08       flds   0x8850438
 807ae47:       d9 c0                   fld    %st(0)
 807ae49:       d9 5c 24 20             fstps  0x20(%esp)
 807ae4d:       d9 c0                   fld    %st(0)
 807ae4f:       d9 5c 24 1c             fstps  0x1c(%esp)
 807ae53:       dd 05 a8 04 85 08       fldl   0x88504a8
 807ae59:       dd d9                   fstp   %st(1)
 807ae5b:       d9 c0                   fld    %st(0)
 807ae5d:       dd 5c 24 30             fstpl  0x30(%esp)
 807ae61:       d9 c0                   fld    %st(0)
 807ae63:       dd 5c 24 28             fstpl  0x28(%esp)
 807ae67:       c7 44 24 44 00 00 00    movl   $0x0,0x44(%esp)
 807ae6e:       00 
```

So it seems clear that the compiler has generated completely different instructions!


The first was build with go1.8 the second with go1.8.1

I had a look throught the release notes and I can't see anything which could cause that.

The only conclusion I can draw is that I somehow messed up the compile and didn't compile it with `GO386=387` :-(

Can you test rclone-v1.36-48-g58a82cd-no-sse.zip a bit more to see if it works for transfers etc? @wtangerine great.

I added `GO386=387` to the offical 386 build.  So you should find that the latest beta works for you

https://beta.rclone.org/v1.36-49-g5135ff7/ (uploaded in 15-30 mins)

Can you give it a quick test for me please?

Thanks Excellent :-)

Will close this (again) now.  That isn't an error message, it is just an info message and quite normal.

I would guess that it didn't find any duplicates.

Try it with `-v` so it prints more stuff.
  There is a drive API for this, but I'm afraid rclone doesn't implement it.

I didn't have any luck searching for a different solution either - sorry!
 I think this is out of scope for rclone, so I'm going to close this ticket.
  Can you post a log with -v and the command you are running please? Also which OS are you running? 

The latest beta is http://pub.rclone.org/v1.28-27-g0f8e7c3%CE%B2/

Can you try that (if that wasn't the one you tried) 

Thanks 

Nick
 Don't worry easily done! 
  Can you send me a log with `-v` please?  That will show the progress of the individual chunks.  rclone should be retrying the chunks if it gets an error.

Also you could try the latest Beta which has some drive related changes: http://pub.rclone.org/v1.28-27-g0f8e7c3%CE%B2/

Thanks

Nick
 Thanks for the log - I see what is happening.

rclone isn't retrying the chunk on EOF errors - it is treating those as fatal errors.

I'll post a beta with a fix for you to try
 I haven't made a beta yet, but I will soon! 
 This was implemented in 1752ee3c and you should find it works in the latest beta http://pub.rclone.org/v1.29-33-g085677d%CE%B2/

If not, please re-open the ticket!

Thanks

Nick
  What you are saying is that on copy, if no files were matched then return an error.

I'm not sure I want to make that the default, but it could be a flag which would be useful for `sync`, `size` etc.

How about `--error-if-no-files` or something like that?
  rclone doesn't retry 403 errors from amazon directly, but if `--retries` is in effect it will retry the whole sync.

Is that what you are seeing?
 rclone has the concept of errors it must retry now, so implementing this should be relatively easy.

Any takers?
 Any help much appreciated - rclone is getting quite popular and it is difficult keeping up!

I note that the new rclone v1.32 is much more efficient at copying single files which hopefully you'll find helpful.
  This is a peculiarity of `os.system` in python, eg

```
>>> import os
>>> os.system("/bin/false")
256
```

Docs for [os.system](https://docs.python.org/2/library/os.html#os.system) state

> On Unix, the return value is the exit status of the process encoded in the format specified for wait()

Docs for [wait](https://docs.python.org/2/library/os.html#os.wait) state

> Wait for completion of a child process, and return a tuple containing its pid and exit status indication: a 16-bit number, whose low byte is the signal number that killed the process, and whose high byte is the exit status (if the signal number is zero); the high bit of the low byte is set if a core file was produced.

I suggest you use [subprocess.call](https://docs.python.org/2/library/subprocess.html#replacing-os-system) instead which does the right thing.

```
>>> import subprocess
>>> subprocess.call("/bin/false")
1
```
  This should be easy to do!
 I've done this.  You can check it out in: http://pub.rclone.org/v1.29-48-ge2788aa%CE%B2/

The units are the binary units as used in the rest of rclone.
  Hmm,  try deleting the remote and reconfiguring. 

I have had another report of this - don't know what causes it but that should fix it hopefully. 
 I think this must be an Amazon problem...

This is the only reference I've manage to find mentioning the error:  yadayada/acd_cli#44

Which country are you in?

Could you send me a the result with `--dump-bodies` of the authentication going wrong? (Send me this by private email to nick@craig-wood.com as it will contain sensitive info).

If you can answer the above I can ask on the ACD developers forum and they may come up with an answer.

Thanks

Nick
 I've posted a question on the amazon cloud drive dev forum here: https://forums.developer.amazon.com/forums/thread.jspa?threadID=12536
 @schmorrison can you check the time and timezone on your computer? Oauth2 needs both to be accurate. 
 Will think about ls and timezones. 

Email the dump headers privately as it will contain sensitive info. nick@craig-wood.com 
 OK Here is beta 3 which
- does a re-authentication on 401 errors (working as per attempt 2)
- does a retry on that particular 403 error

http://pub.rclone.org/v1.29-33-g085677d%CE%B2/

In my testing this works perfectly now - hopefully it will for you too.
  Thanks for sending that.  I'm not having much luck working out what is going on though!

Can you try downloading the go compiler from https://storage.googleapis.com/golang/go1.6.linux-armv6l.tar.gz and see if that works?  Just run the go binary with no arguments and see if it prints something sensible.

If the compiler does work you could try building rclone locally.

Can you also paste the result of `cat /proc/cpuinfo`.

Thanks

Nick
 Here is a build of the tip of rclone development built with the tip of the go compiler - see if that helps!

http://pub.rclone.org/rclone-v1.28-28-g9539bbf-arm-go-tip.zip
 Thanks for trying that - I'm going to ask for some more help on the Go developer mailing list.

I think that this is an incompatibility between the go compiler or runtime and your ARM based system, rather than anything rclone is doing.
 It looks like this is caused by the kernel on your device being compiled with a 64k page size which isn't supported by go at the moment
- golang/go#10180
- golang/go#12480

There is a patch for the compiler to support this, but the compiler internals have changed a lot since then, so I couldn't apply it.

Hopefully those bugs will get fixed soon and then rclone will just start working.
 This is a go runtime problem unfortunately :-(

I'll find the go bug report later..
 Reading the go bug reports looks like this will be fixed for go 1.8. If I get time I'll back port the patch to go 1.7 and post a build of rclone. 
 I looked through the bugs above and it looks like the problem may be fixed in the development version of Go.

I've compiled up two versions of rclone for you all to test. There is an ARMv6 version and an ARMv7 version - it would be interesting if you could try both.

http://pub.rclone.org/rclone-v1.33-63-gace1e21-tip-arm.zip

The above was compiled with 

```
go version devel +56d35d4 Sun Oct 9 00:22:59 2016 +0000 linux/amd64
```
 Great thanks for testing.

Go 1.8 is due to be released in Feb 2017 so rclone will support these devices officially then.  In the mean time you can use that binary, or give me a nudge on this ticket and I'll compile you a new one.
 If you install the go 1.8 beta, set GOPATH, then

    go get -u github.com/ncw/rclone

Will build it for you.

I did a beta build using `go version devel +41908a5 Thu Dec 1 02:54:21 2016 +0000 linux/amd64` which is the go 1.8 beta1.  You can find that here: http://pub.rclone.org/v1.34-48-gc24da0b%CE%B2/ @DTrace001 I've uploaded the latest rclone compiled with the latest go for you to try.

http://pub.rclone.org/rclone-v1.34-75-gcbfec0d-arm-go-tip.zip

I've also included a `hello` binary which should just print hello world.

Can you try that too please?

Assuming one or both of those fails, then please post the output for both and also the output of `cat /proc/cpuinfo`

I'm not sure whether 64k pages are supposed to be working properly for go 1.8 - I'll ask on the developer mailing list once I've got your results. @DTrace001 thanks for that.  I'll ask on the go developers list and see if anyone has any ideas. I asked on the dev mailing list and they said to open an issue which I have done here: https://github.com/golang/go/issues/18408 - I'm hoping that @DTrace001 will respond to requests for more info about the hardware etc on that issue - I'm happy to continue compiling stuff if required. After golang/go#18408 has been declared fixed, I've rebuild the compiler with the fix and rebuild rclone for you to have a go with.

http://pub.rclone.org/rclone-v1.35-13-ge1a49ca-arm-go-tip.zip Thanks for testing. I'll start using go 1.8 to compile rclone when it is formally released. I think this will be in time for the next rclone release if things go to plan.   Assuming that this is either a ppc64 or  ppc64le (64-bit PowerPC big- and little-endian) chip, then you are in luck, provided it is running linux 2.6.23 or later.

As for how to cross compile, first set up a go enviroment (on windows/mac/linux it doesn't matter).  See: https://golang.org/doc/install

You will also need git installed.

Here are some instructions for linux/mac - not sure exactly how env vars work on windows.

Make sure GOPATH is set, eg

```
export GOPATH=~/bin/go

go get github.com/ncw/rclone
cd $GOPATH/src/github.com/ncw/rclone
```

Now for the cross compile bit

```
GOOS=linux GOARCH=ppc64 go build
```

This will make an rclone binary in the current directory.  Try ppc64le also if you don't know exactly which one to try.
  Can you download this little program: http://pub.rclone.org/mimetype.zip

and run it with a sketchup file as a parameter on the windows computer

It should print out the mime type that Windows thinks `.skp` files are.

I suspect it is going to print out `SKP` which isn't a valid mime type.  I think what the message from google means is that it doesn't have a `/` in.

I can put a work-around in to just revert to `application/octet-stream` if there isn't a `/` in the mime type.
 Here is a beta with the fix in: http://pub.rclone.org/v1.28-27-g0f8e7c3%CE%B2/

Have a go and re-open the ticket if you find any problems

Thanks

Nick
  Try `go get -u github.com/ncw/rclone` and see if that fixes it.
 What Go version are you using `go version`?
 rclone (or rather its dependencies) require Go 1.5 or later.
 Did you try a binary release from golang.org: https://storage.googleapis.com/golang/go1.6.linux-armv6l.tar.gz - that should fix your problems
  This will get fixed shortly as part of the directory listing refactor!
 This has been fixed as part of af4ef8ad8d1dcae105138d8046d6793c89de9a7b and f6a053df6e0ec83a8a4fce9db436285d99cafba5 .
 Or instead of doing `--include hb.db.2 copy acd:test` you could do `copy acd:test/hb.db.2` assuming I'm undersdanding what you are doing correctly.

If you want to avoid directory traversals using more than one file, then use the `--files-from` option.  The only thing that I can think of is that rclone set the modification time.

Conjecture
- drive client uploads the file but doesn't set the modification time
- rclone then sets the modification time (which you can do without uploading the file again)
- drive client then thinks the file has changed and downloads it again.

This is just a guess - I haven't used the drive client and I don't know how it works!
 Using `--checksum` or `--size-only` will stop this. Check the docs to see what those do: http://rclone.org/docs/
 I'm going to close this now - please re-open or open a new ticket if you need anything else!
 No there isn't.  I could make a flag - fancy making a new ticket with that suggestion?
  Yes this looks like a bug!  It is easy to fix - I'll post a beta for you to try a bit later.
 Here is a beta with the fix in: http://pub.rclone.org/v1.28-27-g0f8e7c3%CE%B2/

Have a go and re-open the ticket if you find any problems

Thanks

Nick
  rclone uses [the TypeByExtension function](https://golang.org/pkg/mime/#TypeByExtension) to turn an extension into a mime type.

If I try this on my linux machine it works perfectly, setting the correct mime type `"mimeType":"application/vnd.openxmlformats-officedocument.wordprocessingml.document"`

On windows this uses the registry to work out the mime type.

Does the server that you uploaded the docs from have Word installed on it? I suspect it doesn't know the mime type for `docx` documents.  Maybe if you installed the free Word viewer that would help?
 I have figured out what is going on I think - it is when a `.docx` file gets updated that the mime type gets corrupted.

To verify
1. upload `test.docx`
2. edit `test.docx`
3. upload it again - at this point it will change to a zip

Here is a beta with a fix.  It won't fix any files you've uploaded already unfortunately but it should fix the problem in the future.

http://pub.rclone.org/v1.28-23-g6cc9c09%CE%B2/
 Feel free to re-open if this doesn't fix your problem!

Thanks

Nick
 Did you try installing the word viewer to see if that gets the mime type correct?

https://support.microsoft.com/en-us/kb/891090

Also there is a small program you can try [here](http://pub.rclone.org/mimetype.zip) which will print the mimetype the same way rclone does it.  Pass it a docx file and see what it prints.

```
> mimetype.exe z.docx
".docx" is "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
```

That is what it should print!
 Thanks for trying that binary.  Odd result!

You need to install a viewer which can view `.docx` files I think.

According to https://support.microsoft.com/en-us/kb/891090 you need the word viewer and the Microsoft Office Compatibility Pack for Word, Excel, and PowerPoint 2007 and later versions' file formats which you can get from here: http://www.microsoft.com/downloads/details.aspx?familyid=941B3470-3AE9-4AEE-8F43-C6BB74CD1466&displaylang=en
 Great!  I think what I'll do is put this in the FAQ so if anyone has this problem in the future they will know what to do.
  You've arrived at the solution I would recommend :-)

You could use `rclone mkdir /home/user/files/` if you wanted instead of the `mkdir -p`
  The max parts issue I've fixed in  a1323eb20441946bbd575012f39b6ff8b43a5c8b which is in the latest beta.

So I'm interested in working out what has gone wrong there - in what way doesn't the upload finish? 

Can you post a log with -v with the latest beta from http://pub.rclone.org ? I didn't test the beta with ceph so it is possible that something is awry there. 

Thanks

Nick 
 Here is a link to the latest beta: http://pub.rclone.org/v1.28-23-g6cc9c09%CE%B2/

I tried uploading a 2.6GB file to dreamhost (which use Ceph) and that worked fine so I think this issue is already fixed - can you confirm?

Thanks

Nick
 Did you try the beta from the link I sent above `v1.28-23-g6cc9c09`, or a different one?

Does the same thing happen if you try copying from a local copy of the file (so local -> s3, not s3 -> s3)?

Which OS/bits are you using (eg linux 64 bit)?  I can send you a binary with some more debugging in which should help get to the bottom of this.

Thanks

Nick
 I put a test binary here

http://pub.rclone.org/rclone-s3-test/

It should create some logs like this!

```
2016/04/05 15:26:17 2.6gbfile: Multpart upload size 2831155200
2016/04/05 15:26:17 2.6gbfile: PartSize initial 5242880
2016/04/05 15:26:17 2.6gbfile: PartSize is now 5242880
```
 Remove the `--quiet` and you should see the logs
 Some other things to try - try with `--transfers 1` and see if that makes a difference.  You could also try with `--transfers 1 --dump-headers` which will make a lot of logs but you'll see exactly how big each chunk is and how many of them there are.
 That is interesting information thank you. 128299761. / 5242880 = 24.47123737335205 which is a lot less that 10,000, indicating that 5M for partsize should be just fine.  However somehow we read 1000 parts from that file.

Is `D210117.mp4`  128,299,761 bytes long = 122 MBytes? If somehow rclone has the size wrong at that point then that would explain a lot of things!  It fits in with all the `99%` you put in your log above too.

Does `rclone lsl store1_s3:Video/D210117.mp4` agree that the size is 128,299,761 bytes?

Can you try just transferring a single file like this

```
./rclone --dump-headers  -v --checkers 1 --no-gzip-encoding --retries 1 --transfers 1 copy store1_s3:Video/D210117.mp4 store2_s3:Video
```

and see if it still reproduces the problem.  If it does can you send me the output (direct to nick@craig-wood.com is probably best).  You might want to remove the `Authorization:` lines.

If that doesn't reproduce the problem then try your normal sync with `--dump-headers` and send me the log.

Thanks for your help - hopefully we are getting somewhere now

Nick
 > The size is exactly 122 MB (128299761 bytes), and lsl and ls both agree about the size as well as regular s3cmd ls).

That is relief!  There is something strange going on, but not that is good!

> I'll transfer just one file and send the output to your email ASAP. Thanks for your time!

Thanks and no problems
 What seems to have happened is that it reads 5 x 5 MB chunks, then one chunk of 315 bytes and then all the chunks it reads after that are 0 bytes long.

This is clearly a bug, but in exactly what I'm not sure!

Can you download that file correctly using rclone, eg

```
./rclone --dump-headers  -v --checkers 1 --no-gzip-encoding --retries 1 --transfers 1 copy store1_s3:Video/D210117.mp4 /tmp/Video
```

If that downloads correctly then it is pointing the finger at the S3manager code which is part of the amazon go library for accessing s3.  If it doesn't then it is pointing the finger at the ceph server itself.
 As well as the above, can you try this rclone: http://pub.rclone.org/rclone-s3-test2/

This has a fix to the S3manager code I mentioned.  I think what will happen is that it will return an error "Unexpected end of file", or rclone will complain about a corrupted upload.

I'm pretty sure that your download will be truncated when you try the download above which is a problem with the ceph server itself I think.

Thank you very much for your help

Nick
 I'm glad we've got to the bottom of that - thank you very much for your help.

I've submitted a fix for the s3manager upstream here: aws/aws-sdk-go#620

Which should make it into the next rclone release.
 This patch has been merged upstream and the fix will be in the next  binary release.

Thanks for your help

Nick
  As far as I know there isn't a global region in s3 eg - http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region

So I suspect  the bucket was created  in a different region to us-East-1 (which is the default). 

You could try some likely candidates to see what happens. (I can't find an easy way to discover which region a bucket is in!) 

I hope that helps 

Nick 
 No worries, glad I could help. 
  So you mean only you can write to the bucket but anyone can read? 

Rclone can't do that at the moment but you can do it externally,  eg

http://havecamerawilltravel.com/photographer/how-allow-public-access-amazon-bucket

Is this something you need to do once or many times? 
 I think doing this is probably out of scope for rclone, but if anyone sends a PR I'll look kindly upon it!
  Soon I'll add a max depth command line argument which will fix this :-) 

Also note the bugs section in the manual which mentions this specifically! 
  I've got no strong feeling about it, so I added a PR.
 Sorry, late to the party!  Yes a prompt ending in `:` seems to be the unix standard.

@leighklotz can you submit your pull request to this repo.  Note that you only need to change the markdown docs as the other docs are auto generated.
  Well spotted.  When I've fixed this I'll link a beta for you to try.
 I've fixed this now.

Here is a beta with the fix: http://pub.rclone.org/v1.29-41-g1fce83b%CE%B2/

Please re-open if it doesn't work!

Thanks

Nick
  Probably an off-by-one error in the acd pacer would be my guess..
 Unless I am misreading your post, it seems to do what you ask it. It does one "try", and one "re-try", so it will try two times in total.
 The retries option should really be called tries so set to 1 should mean 1 try.

That was my intention too with the low-level-retries flag, but something has gone wrong somewhere!
 The fix for this will be in the 1.30 release which will be shortly!
  B2 normally works quite well for me.

None of the cloud storage providers are perfect - that is why rclone has the `--retries` option.  Hopefully rclone did retry the errors.  You can keep retrying the sync until it runs clean.

17,000 failures in 130,000 is a 13% failure rate which is certainly bigger than

Most of the errors you posted in your first comment I don't recognise - they look like b2 being overloaded or possibly your internet connection being throttled - I've seen this before on home ISP connections.

This error `Failed to copy: sha1 did not match data received (400 bad_request)` is a little worrying - that indicates something corrupted the data somewhere.

The last error `more than one upload using auth token` I do recognise - that should have been fixed in 6b6b43402b14a607695d9c05bff643c1837c81ea which was part of the v1.28 build.

Can you double check your rclone version with `rclone -V` please?  Did you see any other occurrences of that error?

Are you doing the upload from the synology itself?  Can you try from a different machine? Can you try with a different ISP (difficult I know!).

Thanks

Nick
 How many transfers were you using?  I just uploaded 1000 files with `--transfers` set to 64 and 256 with no problems.  In fact b2 seems to work really well with `--transfers` set high.

I haven't been able to reproduce the `more than one upload using auth token` problem :-(

I've uploaded a beta of the latest code here - can you have a go and see if it makes a difference?

http://pub.rclone.org/v1.28-22-g93c60c3%CE%B2/

Note that you might want to use `--size-only` as the latest code supports modification time syncing (after a backblaze API change) so is likely to want to upload all your files again (unfortunately the backblaze API doesn't let you set the modification time without uploading the file again).
 How have you been getting on with the beta?
 @mejje if you can reproduce that, then can you open a new issue please and put a log with `-v` in please.  Or even better with `-v` and `--dump-headers`.

Note that I'm about to release v1.30 so you should probably test with that.
 We've fixed the sha1 problems in #533 so I'll close this one.  Please make a new ticket if you have any more specific issues.  Thanks, Nick
  Is there any chance you could send me one of those files that does get corrupted?

Can you also post the command line you are using just for completeness?

I've uploaded 1,000 random files to drive with mean size 64k, then copied them to onedrive but I didn't manage to reproduce the problem :-(  I'm using linux on ubuntu/amd64.
 I've managed to replicate the bug with your corrupted goofs - thank you very much for those.

Just downloading the files from onedrive with rclone is enough to cause the problem, so we can rule drive out of the equation.

I downloaded the files using the web interface and they were correct.

I haven't worked out what is going on yet but I will :-)
 To take the example of 1 file. it looks this big

```
-rw-rw-r-- 1 ncw ncw  97258 Feb 15  2014 HIGH-bridges.png
```

However according to rclone it is this big

```
$ rclone ls onedrive:goofs
   205029 HIGH-bridges.png
```

The web interface agrees that it is 200k.

![onedrive](https://cloud.githubusercontent.com/assets/536803/14025336/8852f0fc-f1e5-11e5-89ca-65f7943b2506.png)

So somehow onedrive has its metadata in a twist...

Which gives me an idea...

When I try uploading the files with `--no-gzip-compression` they appear properly, so somehow onedrive has decompressed and recompressed the file on the fly.

Can you see if re-uploading the bad files with `--no-gzip-compression` to onedrive and downloading them with the same fixes the problem for you?

I think this is probably a bug in onedrive, but possibly one that can be worked around!
 I uploaded the same file 100 times to onedrive and it is 200k half the time and 97k half the time!

https://onedrive.live.com/redir?resid=71A96798E7B1D253!10473&authkey=!ALWELG3BUcvK-gM&ithint=folder%2cpng
 After a lot of investigation, I've discovered that it is the updating the modification time which we do after the file is uploaded which triggers the problem.  If I stop doing that then the file is no longer has the wrong size when uploaded.  I'm reasonably convinced this is some sort of race condition in onedrive - I've been trying to reproduce it with the python SDK so I can report it as a bug.
 After a few more hours of experimentation, I've discovered that
- the bug doesn't depend on gzip compression
- the bug doesn't depend on the setting of modtime (PATCH)
- the bug can be reproduced with the official python SDK

It seems to be very sensitive to something that I haven't worked out yet.

Since I managed to reproduce the bug using the official SDK I reported it as a bug.

OneDrive/onedrive-sdk-python/issues/27

Hopefully someone from Microsoft will escalate the problem to the right person.
 Same bug: https://onedrive.uservoice.com/forums/262982-onedrive/suggestions/6711029-fix-size-metadata-bug. A dev has responded here: http://stackoverflow.com/a/27031491/681490 1½ year ago, so they don't seem to be in a hurry to fix it.
 Thanks for finding that Klaus. Not quite sure what we should do for rclone. Setting the size in the object returned by Put will allow the upload without the corrupted report but it will be uploaded every sync which maybe is OK since it doesn't happen to many files. 
 I have received an official response from Microsoft on OneDrive/onedrive-sdk-python#27

It is a known bug that Microsoft haven't fixed and if you would like it fixed then vote on [Uservoice](https://onedrive.uservoice.com/forums/262982-onedrive/suggestions/6711029-fix-size-metadata-bug) - I did!

I can't think of a sensible work-around for this - rclone needs to make sure the size of the file is correct and if it can't rely on the size of the file then it will assume that it has been corrupted.
 Checksum will still check file size. I'd need to add a --no-check-size option which you would use with --checksum that might be worth a try. 
 I've implemented an `--ignore-size` flag which you can use to work-around this issue.  I've verified it does the right thing.

Here is a beta with that fix in for you to try.

http://pub.rclone.org/v1.29-1-gbb75d80-62-g46135d8%CE%B2/

Please re-open the issue if you have any problems with it.
  At work where we have a 1 Gbit/s connection, I can get 2-300 Mbit/s transfers on big files.

I suspect you are probably transferring lots of relatively small files?  Google drive has some heavy duty rate limiting which limits uploads to about 2 per second.

You can try increasing `--transfers` to see if that helps.

Also you might get more speed if you try the latest beta which has some tweaks for google drive in

http://pub.rclone.org/v1.28-9-g9dccf91%CE%B2/
 Interesting...  Note that rclone will buffer that 128M in memory and by default will do 4 transfers at once, so that will take 0.5GB of memory.

Would you mind seeing how small a chunksize gives the most significant improvement? I might change the default, but 128M is too big for the default.

Thanks

Nick
 I did a few more tests and it looks like the bigger the buffer, the faster the transfer!  Google also recommends increasing the chunk size to as large as possible.

I don't want to increase the default memory usage of rclone too much, so I propose to make the default chunk size 8 MB (which is roughly 8 times quicker than the default) and write a note in the docs that if you are transferring large files then bigger is better!

![drive-speed](https://cloud.githubusercontent.com/assets/536803/14474109/379b642e-00f3-11e6-80c1-7aacaea7f197.png)
  Glad you sorted it. 

Nick 
  Yes you are right, this is the way rclone works right now.

I'm planning to fix this at some point though - after the directory listing refactor that is currently ongoing.

Thanks for the report

Nick
 Here is a beta with a fix for this: let me know how it goes!

http://pub.rclone.org/v1.29-28-g36700d3%CE%B2/

Thanks

Nick
  That looks like the right syntax.

When you say keeps the folders - what do you mean exactly?  Currently rclone isn't very good at managing folders - see #100 for more info!
 Ach, I should have noticed this earlier...

You need to use `/` when specifying filters not `\`, so the correct syntax for an exclude file is

```
/.svn/**
/.git/**
```

Or for a filter file

```
- /.svn/**
- /.git/**
```

I'll write that in the docs.
 The `/` at the start of the `/.svn/` anchors it to the root which is what I thought you wanted.

It looks like you actually want `.svn/**` which will match an `.svn` anywhere in a path component.

So I reckon this should be what you want!

```
- *.obj
- *.ncb
- .svn/**
- .git/**
- *.bak
- *.BAK
- *.suo
- *.idb
- *.ilk
- *.pdb
```
 I tried the above without the initial `*` and it worked fine for me.  I tested it by doing this

```
$ cat /tmp/exludes 
- *.obj
- *.ncb
- .svn/**
- .git/**
- *.bak
- *.BAK
- *.suo
- *.idb
- *.ilk
- *.pdb
```

```
$ rclone ls --filter-from /tmp/exludes .
     3956 README.md
```

```
$ ls -lart
total 28
drwxr-xr-x 193 ncw ncw 16384 Mar 11 14:18 ..
-rw-rw-r--   1 ncw ncw  3956 Mar 11 14:20 README.md
drwxrwxr-x   3 ncw ncw  4096 Mar 11 14:20 .
drwxrwxr-x   8 ncw ncw  4096 Mar 11 14:21 .git
```

There is a ticket about recursing into excluded directories here #395 :-)
 I think this is all cleared up now, and I've written a bit more in the docs.

If not please feel free to re-open!

Thanks

Nick
  This is because rclone is checking the md5sum, but ignoring the `X-Static-Large-Object: True` flag.  rclone (or actually the swift library) would normally expect to see an `X-Object-Manifest` header on a large object.

I made an issue here about it ncw/swift#67
 I've fixed this and put a beta for you to try here which should fix the problem above.

http://pub.rclone.org/v1.28-9-g9dccf91%CE%B2/

Could you also try `rclone md5sum` on a segmented file?  This should return a blank for any segmented objects.

Thanks

Nick
 I'll try again in a moment - it should be easy to fix. 

Thanks
 Here is another beta for you to try

http://pub.rclone.org/v1.28-10-gbd27473%CE%B2/

Hopefully that will fix it!

Thank for testing

Nick
 Thanks for testing the beta. 

Nick 
  How are the encrypted files stored? If it is one big file then you are right. However if it is lots of smaller files (say one per file on the disk) rclone will sync that fine in an incremental way. 
 I'm going to close this now - please feel free to re-open if necessary!
  Could easily be added as part of #389
  Proposal for issue #379

Currently the revised output looks like this:

```
2016/03/12 13:52:25
Checked:           192593 of 46673217 bytes (0.41%) in 5 of 1172 files
Transferred:       176799 of 186888 bytes (3.22 kB/s) in 41 of 50 queued files
Elapsed time:       53.6s
```
## Things to fix before merge
- [ ] `func Check(fdst, fsrc Fs)` should also update stats.
- [ ] If a file exist on src, that doesn't on dst, it is not added to "checked". That breaks this method of displaying stats.
- [ ] Add file deletion information.
- [ ] Tests.

Of course we should also agree that this is the right direction.
 Sorry @klauspost I have dropped the ball on this PR.  Are you still interested in doing this?  I think the proposed stats output looks great :-)
  Isn't that just `rclone copy`, or am I misreading your request?
 No problem - happens to all of us from time to time (and those who claim otherwise are lying)
  That is interesting!  The aws library is supposed to take care of that, but is obviously didn't...

What it looks like is that the file was uploaded with the default part size of 5MB.  According to my calculations an 80 GB file would reach 10,000 parts after 61% which looks plausible from the log above. I make the ETA 4993s but it failed after 3047s which is 61% so check!

I see the problem - the aws library only does its magic calculations if I'm passing an `io.Seeker` but I'm not.

I'll upload a fix for you to try in a few minutes.
 That should be fixed now.  Here is a beta for you to try.

http://pub.rclone.org/v1.28-8-ga1323eb%CE%B2/

Thanks for reporting the problem

Nick
 Thanks for reporting the bug and testing the fix

-- Nick 
 I've just tested this from S3 to S3 (I disabled server side copy) by copying an 8GB file from one bucket to another.  I didn't see any problems.

An 8 GB file gives: PartSize = 5242880, size = 8590983168, MaxUploadParts = 10000

So this should take 8590983168/5242880 parts which is 1639 parts, so well below 10,000

@zioproto are you trying from Ceph to Ceph too?  Is it possible this is a ceph bug?

If you do `-v --dump-headers` you can see all the parts as they go by and count them to see if they really do exceed 10,000

And are you both using a recent version of rclone? Versions before v1.29 don't contain the fix to the aws s3manager library which does cause exactly this problem (see #415) @mistur Try the transfer with `-v --dump-headers ` - that should tell you what is going on. @zioproto OK so definitely 10,000 parts which is interesting...  What does rclone say if you do the transfer with `-v --dump-headers`?  I think you should get a Content-Size header which will show you how big the parts are.

How long is I.tar.gz?  What do you get if you `rclone ls source_remote:million-song/I.tar.gz` is that the number you expect? This looks like a repeat of #415.  In fact it looks possible that the AWS library maintainer has broken my fix...

Can you try this binary where I've reverted the S3 library to a version which I know was good please?

[rclone-v1.35-76-gd091d4a-aws-sdk-revert.zip](https://github.com/ncw/rclone/files/761398/rclone-v1.35-76-gd091d4a-aws-sdk-revert.zip)
 > The binary is dynamically linked and not statically

It is a dev build - it shouldn't make any difference.

I'm expecting it to be producing errors - these were being masked before.

However I wasn't expecting `Failed to copy: SerializationError: failed to decode S3 XML error response` - does it do that consistently?

Did you see any other errors in the log?  Can you run with `-v` and check please?

Thanks

Nick Thanks for that... It looks like I reverted too much.

Here is another attempt with just the s3 uploader changes reverted

[rclone-v1.35-79-g50e190f-revert-s3-changes.zip](https://github.com/ncw/rclone/files/764225/rclone-v1.35-79-g50e190f-revert-s3-changes.zip)
 > the problem is still present but now I have an other error :
> 2017/02/09 17:18:47 D.tar.gz: corrupted on transfer: sizes differ 8223584977 vs 1090510786
> 2017/02/09 17:18:47 D.tar.gz: Removing failed copy

That is what I was expecting to see.

According to your first log, the read part of the transfer stopped before the end.  However due to a bug in the s3 library it didn't notice and kepts sending 0 sized parts until it got to the part number limit.

So that is a bug (in the s3 library) which needs fixing.

If you've managed to fix the unerlying problem with nginx settings then that is good too!

> the strange things is with the official 1.35, I didn't have the error message below, with the 1.35-DEV yes. and also with the -1.35-DEV rclone is able to use roundrobin DNS, with the official binary, It always goes to the same IP.

That is probably the difference between the dynamic linked version which will use your system resolver where the static one wont.  It might also be the difference between go 1.7 and 1.8 also.
 > is that possible to get an official dynamic linked binary of I must build it from sources ?

You'll need to build it from source.

The official build should do round robin DNS too - if it isn't then likely you have a local IP in the range of the destination I would guess...

...

Would you be able to try the transfer with this version of rclone, without your nginx fixes?

It should go wrong in the original way with the maxparts error, but it outputs lots of extra logging which will hopefully point at what exactly is going on.  Then I can make a test case for the aws lib.

[rclone-v1.35-80-g5b9f06d-386-s3-upload.zip](https://github.com/ncw/rclone/files/766924/rclone-v1.35-80-g5b9f06d-386-s3-upload.zip)

Thanks

Nick Thanks!  The [warned map](https://github.com/ncw/rclone/blob/master/local/local.go#L45) should have a mutex.
 This is fixed now - you can find a beta to test here

http://pub.rclone.org/v1.28-7-ge57c440%CE%B2/

Thanks for the report

Nick
 Thanks for reporting the bug and testing the fix :-)
  Just use `copy` as the verb instead of `sync` - that is the only difference between the two.
 @ashayh rclone works exactly like rsync - it only deletes files on the **destination** when using the `sync` verb.
 @ashayh Strange - that isn't how I think it works... Have you got the source and destintation the wrong way round?  It goes `rclone sync source destination`?  Can you post the command line you are using please? And can you see that file in either the source or the destination - maybe it got in the destination in the wrong place?
 I think maybe you want to use `rclone -n sync Nikon AMZN:Nikon`.  What I suspect is that you've uploaded some files already to acd but you are syncing them into a different place and it is showing you that you would delete some files if you were to sync like that.
  Sorry I missed this ticket!

Yes there has been some discussion on this.

I'm of the opinion that rsync's way of doing it, while clever, is extremely error prone and a mis-feature.

If you type

```
rsync -av --delete /path/to/my/files dest:/path/to/backup
```

Then one say with a bit of command line completion help, type

```
rsync -av --delete /path/to/my/files/ dest:/path/to/backup
```

Then you just deleted all the files in your backup and re-uploaded them :-(

I made rclone so that the trailing slash is irrelevant like it is in the rest of the unix world and it always refers to the directory contents.

I did write this in the docs to address this point for new users

> If you are familiar with `rsync`, rclone always works as if you had
> written a trailing / - meaning "copy the contents of this directory".
> This applies to all commands and whether you are talking about the
> source or destination.

I think, practically speaking, it is too late to change it, even if I wanted to :-(
 @jody-frankowski I'm going to fix this with two new commands `cp` and `mv` which will work exactly like their unix counterparts unlike `copy` and `move`
  If rclone supported webdav + the correct authentication scheme would that be enough to make it work with sharepoint online?
  It is how many files that takes the time rather than the total number.  On my connection, I get about 10,000 files listed per minute.

Can you time just the file listing element of it with `rclone size gdrive:backup`?  That will also count the files.  Can you paste the output of it?

Can you also try the latest beta which is here also which you should find is much quicker with drive.

http://pub.rclone.org/v1.28-6-gfdd4b4e%CE%B2/
 Great! The rate limit seems to be just the way Google drive works unfortunately. If you want never to be rate limited then you need to keep the request rate to below one per second and things take much much longer! 
 I'm going to close this issue - please re-open if you have more to add!

Thanks

Nick
  You have to list a bucket - all actions on hubic except for `lsd` work on buckets.

eg `./rclone -v ls hubic:default`

Hope that helps

Nick
 No problems!
  ## What happens now:

By default, rclone shows total bytes transferred and progress for each transfer thread (each file being transferred). For example `sync local remote:location` shows transferred bytes + 4 lines for 4 threads uploading 4 individual files to the remote backend.

![image](https://cloud.githubusercontent.com/assets/270528/13523491/0ec89132-e1f6-11e5-8471-1b4e157ec753.png)
## What should happen:

Rclone should display the total progress (and ETA) of the requested operation, i.e. a `sync` operation, which is effectively:

> progress = transfered_files / total_files \* 100%

The current progress display is not very useful, unless someone `syncs` a single file or manually checks the source size and compares against `Transferred`.

IMO The current per-thread progress status should live within `--verbose` progress display for diagnosing stuff like backend rate-limits or transfer limits. By default, rclone should display **total** progress of operation and **total throughput** of all threads - total download/upload speed maintained.

Maybe something inspired by curl progress? 
![](http://i.stack.imgur.com/m7O1i.png)

Or like wget ?
![](http://www.thelinuxterminal.com/wp-content/uploads/2014/10/wget-download-1140x344_c.png)
![](http://4.bp.blogspot.com/-DlhB25swqEs/UZe0CPLAv6I/AAAAAAAAAIc/Ig5nSOpv2fw/s1600/Progress+Label.png)
 Let me give this a shot.
 `rclone` starts transfer before it knows how all of which files it will transfer. This is so hash checking can be done meanwhile we are uploading.

We know how many files/bytes we will be checking in total when we have read the source file system. That means the metrics we can supply are:

```
Checked:         41253175 of 46673371 bytes (88.39%), 461 of 1172 files
Transferred:     17330604 of 17342459 queued bytes (415.53 kByte/s), 22 files
Errors:                 0
Elapsed time:       40.7s
```

Note the "queued" in the "transferred" stats. That number will keep growing, and cannot be used for an ETA, because we are only checking a few files ahead of the ones we are transferring.

As for showing individual transfers only in verbose mode, I do agree, but @ncw will have to approve changing the default behaviour.
 I'm not sure where I should comment now, as #389 overlaps.

I get the challenge. As the total bytes is a rolling metric, it might not be complete. However, that will depend on the dataset size, number of files and performance of "checkers". It's quite possible, that in many cases users will get the actual, final total number of bytes queued in a few seconds from starting. I do feel like it should be possible to represent that in an ETA and progress report.

So let's modify the example a bit:

```
Checked:         41253175 of 46673371 bytes (88.39%), 461 of 1172 files
Transferred:     17330604 of 17342459 queued bytes (40%, 415.53 kByte/s), 22 of 300 files
Errors:                 0
Elapsed time:       40.7s
```

Notice i've added percentage for transferred.
Now for the percentage, a stacked progress bar would do fine.

Here's an example for when there's a lot of files to check and the transfer takes more than the checking.

```
Synchronizing [>                                         ] 0%  1401 kByte/s
Synchronizing [==>                                       ] 0%  1401 kByte/s
Synchronizing [====>                                     ] 1%  1401 kByte/s
Synchronizing [#====>                                    ] 5%  1401 kByte/s
Synchronizing [#####=========>                           ] 10%  1401 kByte/s
Synchronizing [###########===========>                   ] 15%  1401 kByte/s
Synchronizing [#################========================>] 26%  1401 kByte/s
Synchronizing [#######################===================] 51%  1401 kByte/s
Synchronizing [##########################################] 100% 1401 kByte/s  
```

The `==>` bar is for checking and deletions, the `###` bar is transfer of new and modified files.

The total progress would be a product of both:
`total_progress = (checked_files / total_files) * (transferred_files / files_to_transfer)`

That means:
- the assumption is transfer is slower that checks,
- when checked all files (100%) but transferred 50%, total progress = 50%.
- when checked 50% files and transferred 0 files, total progress = 0%
- when checked 50% files and transferred 50% of queued files, total progress = 25%
- yes, when checking is slow and there's a lot of files, the total progress might go down, but that's why the visualization will give the user a better picture of what's happening.
- ETA should be a derivative of total progress.

```
Synchronizing [#######################===================] 31%  1401 kByte/s ETA 25m
```
 You assume we know "files_to_transfer" - we do not, because checks need to finish before we know if we should transfer, and there is a limited "queue" (default 4 IIRC) of files waiting to transfer, so checks are not done before there is space in the transfer queue. 

Transfer will never be "22 of 300 files", because there isn't space for 278 files in the transfer queue.
 I guess I didn't fully understand your current queue implementation.
Does that mean, that all checks will be suspended once the queue is full with 4 items ?
 Nah, I've done some experiments. Nothing meaningful can be produced without sacrificing the original design, which I'm quite fond of, really. I will however propose some better, non-verbose, status display in the other ticket.
 Agree. Just to elaborate - I see "late" checking as a positive feature. If you are doing a transfer that takes a lot of time, it will transfer files that are changed during the transfer and hasn't been checked yet. 
 That's what I meant :-) It's effectively a rolling operation. It's also lower resource and can be faster with larger sets. Maybe in the future we can have a mode flag which would allow for more rsync-esque operation with pre-scanning and queueing all changes, as this can have its advantages in some scenarios.
  > 2016/03/03 16:01:15 Failed to create file system for "hubic:backup": Error authenticating swift connection: Get https://api.hubic.com/1.0/account/credentials: oauth2: cannot fetch token: 301 Moved Permanently

Hubic access tokens expire after 6 hours.
In order for rclone to be usable for backup (or any form of automation) rclone must be able to refresh the token automatically.

---

The process is described here: https://api.hubic.com/sandbox/

Here's the key part:

> Refreshing an access token looks like getting a new one, but some parameter's values change.
> First, you need to authenticate your application, with Authorization: Basic header or passing your credentials (client_id, client_secret) in POST data. Remember, we only support application/x-www-form-urlencoded POST data.
> 
> POST https://api.hubic.com/oauth/token/ HTTP/1.1
> Authorization: Basic 
> [...]
> refresh_token=XYZ&grant_type=refresh_token
 The oauth2 framework that rclone uses will refresh the token automatically. I guess the problem is that after the token is received rclone switches to using the swift API and never touches the oauth server again. 

It looked like you got that message immediately you ran rclone which I don't understand. As part of the initial connection rclone will refresh the token. At least that is what it does here! 
 hmm... I've configured and run it 3 days ago (finishing successfully), then tried again today and it gave me that. I've run `config` again and refreshed the token this way (config wizard asked me about that, used the in-browser auth again). Let me try syncing again tomorrow morning and we'll see what happens.
 Working fine today. I'll report back when I see it again ... 

btw: what about the mid-transfer refresh? 
Why not add something trivial, like calling the (already implemented) refresh function every 7200 seconds when switching file context. It's not as smart as inspecting the token lifetime but it'd effectively allow long-lasting `sync` with hubic :-) (i'm about to start a 5-7 day transfer)
 > btw: what about the mid-transfer refresh? 

This should work already - when the swift token expires it will run through getting a new one from the hubic api.  If the oauth token has expired then it will refresh that first.

When I wrote

> I guess the problem is that after the token is received rclone switches to using the swift API and never touches the oauth server again

I was wrong and really should have looked at the code first ;-)
 :+1:  Thanks!
 Something is not right still ... 
Here's what I've encountered today:

``` shell
$ rclone sync /Volumes/Data hubic:container-name
2016/03/09 12:06:05 Failed to create file system for "container-name": Error authenticating swift connection: Get https://api.hubic.com/1.0/account/credentials: oauth2: cannot fetch token: 301 Moved Permanently
Response: <html>
<head><title>301 Moved Permanently</title></head>
<body bgcolor="white">
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx</center>
</body>
</html>
```

Then 3 seconds later, when I attempted the exact same command again (with `-v`), it started working:

``` shell
$ rclone -v sync /Volumes/Data hubic:container-name
2016/03/09 12:06:35 hubic: Saving new token in config file
2016/03/09 12:06:38 Hubic: Got swift credentials (expiry 2016-03-10 12:06:38 +0100 CET in 23h59m59.0401908s)
2016/03/09 12:06:38 Hubic Swift container container-name: Modify window is 1s
2016/03/09 12:06:39 Hubic Swift container container-name: Building file list
[...]
```

It's almost like it was invoking an invalid URL or something. I wish I'd run the first attempt with -v, maybe I'd get the url being called .... is there a `--debug` flag equivalent? I couldn't find it ...
 I have seen this too occasionally.  I've been assuming it is a temporary Hubic problem, but maybe I'm wrong about that.

If you can catch this using `--dump-bodies` that would be very useful. Be careful posting the output directly though as it will have authentication stuff in it.  Probably best to try a simpler command like `rclone --dump-bodies lsd hubic:` too otherwise you'll get too much log!

I will try to replicate too.

I made an issue here golang/oauth2#177 as I think it is probably a bug in that library, but I'm not certain.
 A thought: maybe some or most of the response errors during auth could also be retried? 
As per `--retries` it could try a few times (default 3) in case the backend is temporarily misbehaving.

Naturally not a solution, but would save those backup runs from temp. problems. We'd need to isolate hard auth errors though (i.e. invalid token).
 @Thinkscape good idea!
  There isn't at the moment. What would you like to see?

I could imagine passing in a `--description` flag which set the description for all files uploaded in one session?
 I was confusing drive with one of the other cloud storage systems.  In drive the file name is the description - and I don't think there isn't any other sort of description or comment you can set

Here is a list of things you can set on a file

https://developers.google.com/drive/v3/reference/files

So I don't think this is possible with drive.
 Yes you are right!  Looking at the code we set the Name (called Title in v2 API) to the name of the file.  This is the name you see when you look at the drive web interface.  We also set the Description to the name of the file.

So my original idea of a `--description` flag would work just fine!

Sorry for the confusion!
  This looks like it is a dropbox limitation - it can only operate on 10,000 files at once.  rclone sends a single command to dropbox to do a purge but dropbox immediately replies with that error.

http://stackoverflow.com/questions/24843780/dropbox-core-api-10000-file-move-strategy

You could try doing `rclone delete ...` instead.  This will be much slower, but it should work.

If that works I'll document the work-around in the dropbox docs.

Thanks

Nick
 Did you have a go with `rclone delete`?
  I've fixed this already in 6b6b43402b14a607695d9c05bff643c1837c81ea (there wasn't an issue about it so don't feel bad about not finding it!).

This didn't used to happen - I think Backblaze must be enforcing the one upload URL per thread rule which rclone wasn't obeying properly before (something I managed to miss from the docs).

So if you try the latest beta http://pub.rclone.org/v1.27-45-ge6f340d%CE%B2/ you should find that is fixed.
  The client secret is obfuscated so it isn't directly I'm the code - that was the best I could come up with. 

If you think through all the possible ways of having a client secret in an open source program you'll see that there aren't any better solutions, unless you want to run an auth server and handle all your users auth (with the attendant risk) which I wasn't willing to do. 

All the client secret allows is for someone to impersonate your app, there is no possibility of user data being disclosed so I deem it to be low risk. 

No I didn't ask Microsoft about it!
  As far as I know it isn't possible to set the modified date via the public API.  Some storage systems can and some can't - see the table here

http://rclone.org/overview/

Here is a thread on the developers forum about setting modified date

https://forums.developer.amazon.com/forums/thread.jspa?threadID=8762&tstart=0

As you can see there is a possibility of setting the date in additional metadata on the object.  This wouldn't show in the ACD web interface but would enable rclone to save and restore the modified date.

I'll mark this ticket as an enhancement to implement that feature.
 @felixbuenemann do you fancy working out how the official client manages to store the modification time?  There isn't a documented API for it...  To be most useful to rclone you'd need to be able to set the modification time (patch an existing object).

The next best idea I can think of is to use a label - say "Date: <date in a standard format>" which can be uploaded and patched.  Use the `rclone check` command that is what it is for. You might want the `-v` flag too. This will tell you about files which don't exist in one place or the other and tell you about checksum mismatches on files which exist in both places. 
 Glad you found that useful :-)
  Thanks for the suggestion.

There seems to be a comprehensive API here

https://developers.google.com/picasa-web/docs/2.0/developers_guide_protocol
 Foo! Is there a different API for google photos that you can find?
 @BenoitDuffez nice writeup thanks - a great way of downloading your pics from google photos.

It doesn't solve the upload part though which is what this issue is about.
  I recently did something very similar for swift - thanks! See c3a0c0c45198bad7e8fbfb191cf8c3527f41e7bb

I chose in the end to do the directory exists check first, and then if it fails attempt to create the bucket.

My reasoning for doing that was that creating the bucket isn't particularly common - you almost always copy to an existing bucket so the number of remote operations will be the same in the common case.

As for S3 that particular bit of code you've changed has been troublesome with the S3 look-alikes not producing exactly the correct error code (eg CEPH and Blackperl DS3) so changing it to check first would help there!

So what do you think about re-arranging slightly to
- check bucket exists
- create if didn't exist
- return error if create failed for any reason

Also is it possible to be a bit sharper on the error code that `dirExists` is checking for?  At the moment any error will cause the bucket to be shown to not exist.  It should really be checking for a specific error and returning a flag, and an error I think.

Thanks

Nick
 I think you want something like this for dirExists.  If it returns an error then you should report that.  If it doesn't then the bucket was either found or not found.

``` go
// Check if the bucket exists
func (f *Fs) dirExists() (bool, error) {
    req := s3.HeadBucketInput{
        Bucket: &f.bucket,
    }
    _, err := f.c.HeadBucket(&req)
    if err == nil {
        return true, nil
    }
    if err, ok := err.(awserr.RequestFailure); ok {
        if err.StatusCode() == http.StatusNotFound {
            return false, nil
        }
    }
    return false, err
}
```
 One test has started failing - I don't know why yet - it isn't your fault!  I'll fix it shortly.
 I've fixed the failing test - if you rebase the branch then force push it should pass!
 I've merged that in 694d390710a14304750897c5282d5b1f324a9559 (after rebasing and squashing the commits).

Thank you very much for your contribution
  Thanks for reporting this.  This is a duplicate of #310 which is scheduled for the next release so I'm going to close this one - if you'd like to follow the other ticket you'll get notifications about its progress.

You have provided the actual error message though which is super helpful - thanks.
  I just had a look at the API. It looks like rclone could do blob or file storage. Which of those is more interesting to you? 
  Can you run stat  on that file and post what is says please? 
 Maybe related to #261?

I made a [test program](https://gist.github.com/klauspost/5f87caf402a8abf369d5), that may be able to help here. It requires that you have Go installed.

If you put it into a file called `test.go` and runs `go run test.go "/encrypteddirectory"` it may be able to give us some additional information.
 Thanks - there is nothing jumps out right away.

What is `5Q6YSJOW3CUSPLSOBSB5KCN6USP4J`\- a file, or a directory, you seem to have both?

If it is possible for you to post the error and info on the same file, that would help.
 I attempted to replicate this but failed - the copy to acd worked perfectly!

Can you adjust the script below so I can replicate the problem?

(I tried on ubuntu 15.10 amd64.)

Thanks

Nick

```
encfs /tmp/encrypted /tmp/visible
cp -av testfiles/ visible/
rclone -v copy encrypted acd:encfs-test
fusermount -u visible/
```
 Ooops, forgot the `--reverse` in my first test.

Still no luck reproducing though.

I'm trying encfs 1.8.1-3 on Ubuntu 15.10 - how about you?
 Can you try the latest beta? http://pub.rclone.org/v1.27-23-ga3b4c8a%CE%B2/ - that was compiled with go 1.6 so has the latest runtime - maybe stuff got fixed there? I don't have a lot of hope, but it is worth a try!
 A symlink would count as a non/file directory yes - you can't transfer those with rclone.  There are already issues about that!
 Is there any chance you could tar up some files that fail for you and send them to me? I'd like to make sure we are testing the same thing!

If you could send two tars of both the encrypted files and the unencrypted files that would be really useful.

Obviously don't send me anything confidential!

Thanks

Nick
 @zachron Strange!  Let me know if you manage to get a reproducer!
 @codefolder How many files in that 40GB? It would be interesting if you tried `rclone size /home/unencrypted` and `rclone size /home/encrypted` and see if they both complete and how long they take.

In general I haven't found fuse based filesystems particularly reliable so I hope it isn't just encfs flaking out!
 Are you sure that the files it mentions really aren't symlinks?

Here is the function that produces that error.

That error is only produced if the file is a symlink, named pipe, socket or device.

``` go
// Storable returns a boolean showing if this object is storable
func (o *Object) Storable() bool {
    mode := o.info.Mode()
    if mode&(os.ModeSymlink|os.ModeNamedPipe|os.ModeSocket|os.ModeDevice) != 0 {
        fs.Debug(o, "Can't transfer non file/directory")
        return false
    } else if mode&os.ModeDir != 0 {
        // fs.Debug(o, "Skipping directory")
        return false
    }
    return true
}
```

I've been trying to replicate the problem but it all just seems to work for me :-(

```
$ rclone size /tmp/encrypted
Total objects: 225455
Total size: 7.039G (7558589897 bytes)

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            0
Elapsed time:         12s

$ rclone size ~/Code
Total objects: 225455
Total size: 7.039G (7558589897 bytes)

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            0
Elapsed time:        1.7s
```

What version of encfs & linux are you using @codefolder ?
 I'm going to close this as we haven't made any progress on it.  I'm pretty sure it is an encfs problem but I could be wrong.  Please re-open if you have any more thoughts.
  The `Can't redirect stderr to file` is the reason I think.  It shouldn't be saying that!

Which OS are you running on and how many bits? I'm guessing OSX...

I see the problem, I've made up a build tag `unix` in `redirect_stderr.go` - easy enough to fix!

In the mean time you can use command line redirection if you want

```
rclone commands  > rclone.log 2>&1
```
 This is fixed now and will be in the next release.

Here is a beta with the fix in if you want to try it.

http://pub.rclone.org/v1.27-30-gb5c5209%CE%B2/
 @anyheck - I made a new issue for this #698 - please subscribe to that issue for updates - thanks!
  It depends on exactly how dis-entangled you want it from rclone itself.

Provided you don't mind using the rclone config file, and its flags, then it is quite easy...

Create your fs using `f, err := fs.NewFs(remote)` where `remote` is the remote path you'd normally use in rclone.

See what an Fs object can do by looking in `fs/fs.go`.

See the main rclone.go file and you'll see that you don't need a lot of code to get going.

We don't guarantee any stable interfaces for rclone though.
  This is a known issue #28 which I haven't managed to reproduce.

If you could send me a log with `-v` of it happening that would be really useful!

Thanks

Nick
 rclone dedupe drive:Images

What I really need is the log from the run that uploaded the duplicates... 
 What does the dedupe look like - could you paste the output for one file please?
 > During a manual sync I am seeing a lot of these - 
> 
> > Failed to copy: Couldn't find or make directory "ASL Web/ROL board/477": Couldn't list directory: googleapi: Error 403: Rate Limit Exceeded, rateLimitExceeded

I'm afraid that is normal - Google Drive has lots of rate limiting.  Rclone will retry things and it shouldn't affect the outcome.

Can you paste a bit more of the log, before and after the "Couldn't list directory" error - that would be interesting.  Not being able to list directories could be the cause of the duplicate files...
 @isaiah36 I'd like to know are the files identical in the dedupe or not.  I'm thinking about options for #338 - maybe you could pitch in there with what you would like to see

I'm also really interested in the log before and after the 403 error, but **only** if it was "Couldn't list directory".  I know that the upload retries are working properly.

Thanks

Nick
 Were there any more logs relating to `Couldn't find or make directory "temp/mtch"`?
 That is useful thank you.  I have some ideas about what is going on and how to replicate this!

Thanks

Nick
 After a bit of investigation I think I've worked out what is happening.  If anyone would like to have a go here is a workaround - using rclone 1.28 add `--low-level-retries 20` to your command line.  This should stop the directory listings failing which is the root cause of the problem in my tests.  Let me know how it goes!

As for removing duplicates there is `rclone dedupe` - it isn't fully automatic yet.  I indent to make it fully automatic for the next release though!
 @isaiah36 yes that is correct.
 Here is a beta with a fix for the directory listings to make it much more reliable.  This should eliminate the most common cause of duplicates.

http://pub.rclone.org/v1.28-6-gfdd4b4e%CE%B2/
 @isaiah36 that is correct - the low level retries flag shouldn't need needed. 
  What size files are you copying and which remote storage system. Also which OS? 
 Currently rclone makes a temporary copy of the file it is copying before uploading to B2.  This is due to the way the b2 API works.  I'm hoping to turn that into reading the file twice, but that requires some re-architecting of rclone's internals.

There is a note about this on the b2 page: http://rclone.org/b2/

You can override which directory rclone uses to make temp file by setting the `TMPDIR` environment variable.

I'm going to turn this ticket into one to remind me to fix this properly!
 Would be possible if #282 is implemented. I might give it a try.
 @klauspost that would be fantastic! 
 The fix for this will be in the next release!
 Here is a beta for you to try: http://pub.rclone.org/v1.27-23-ga3b4c8a%CE%B2/
  Could you try this again with `echo %ERRORLEVEL%` after rclone has run?

As you can see when I try it in my Windows 7 VM, disabling networking with Virtualbox, I get the correct error level.

Thanks

Nick

![rclone-error-return](https://cloud.githubusercontent.com/assets/536803/13076801/c9d2046a-d4ac-11e5-8171-7085165e660f.png)
 @jackools - did Nick's proposal help you?
  This will be rclone doing metadata reads in `readMetaData` - it does these size 1 lists to find the ID of a file given a file name.

During a copy the files should be read with the list in big chunks (of 1000).  You can see this working if you do `rclone --dump-bodies ls b2:bucket`.  

If the object didn't come from a directory read then rclone has to do a metadata read to read the ID and hence the modtime / sha1.

I'm having trouble working out how in the normal course of operations `readMetaData` should ever get called!

If you do a `--size-only` sync then it shouldn't read the sha1 or the modtime so shouldn't need to do a metadata read, so I don't understand your result.

What was the exact command line that returned those size 1 directory reads?

(I tried the command line you posted but I didn't get any metadata reads.)

Thanks

Nick
 After some effort I've managed to reproduce this! Thanks for helping me pin it down.

It turns out that the problem is to do with the `/somedir/` - if you just use a bucket `b2pabilder:b2test20160218` then everything will work as expected.

When I've fixed the problem I'll post a beta here for you to try.

And for the answer to your questions

> Authorize
> List buckets (why??)

The reason why rclone does list buckets is to get the ID of the bucket given the name.  There isn't an API call to do that (if you can find one I'd be grateful!).

> List files one by one (<-- PROBLEM)
> Create bucket (what the heck??)

rclone creates the destination bucket when you are about to write to it. (It also does when doing `--dry-run` see #342 which will get fixed shortly!)

It is easier to try to create the bucket than to check it exists.

> List all files in one go (as expected!)
> Skipping everything as all is there.
 I've fixed this now. Turned out to be a stupid problem in the read metadata function!

Here is a beta for you to try - please re-open the ticket if it doesn't work as expected!

http://pub.rclone.org/v1.27-18-g85a0f25%CE%B2/

Thanks

Nick
 Thanks for testing it :-)
  This sounds like a duplicate of #336

If so then this beta should fix it

http://pub.rclone.org/v1.27-11-g1cd0d9a%CE%B2/
 Great glad that has fixed it. Duplicate file names are a known gotcha in drive - I agree it wasn't a sensible design choice! 

The fix is committed to master and will be in the next release. 
 I've noticed that drive exports seem to vary slightly. I don't know why - maybe a timestamp or something like that. It is a little frustrating but I don't think there is anything I can do about it. 
 I'm going to close this now - thanks for the report.
  Excellent idea!

I could really do with help with this though, so if you (or anyone you know) can package stuff then I'd love a contribution!
 @harupong very cool thank you! Would you like to maintain the chocolaty package for rclone on a permanent basis? Thanks Nick
  Can you tell me how to reproduce the problem?

I tried this to my rackspace cloudfiles account

```
$ rclone -v copy . rackspace:gzs
2016/02/14 16:52:32 Swift container gzs: Modify window is 1ns
2016/02/14 16:52:35 Swift container gzs: Building file list
2016/02/14 16:52:35 Swift container gzs: Waiting for checks to finish
2016/02/14 16:52:35 Swift container gzs: Waiting for transfers to finish
2016/02/14 16:52:37 file.gz: Copied (new)
```

But it didn't produce the error.

You said above

> On rackspace side, we added the header: Content-Encoding: gzip, but we leave the content-type to the original file (for example: text/plain).

How did you do that?

What I suspect is happending is that rclone is comparing the md5sum of the gzipped file locally, with the md5sum of the ungzipped data - since they differ rclone is deleting the file.
 How do you add the `Content-Encoding: gzip` header?  I can't replicate the problem until I can figure that out!

I see what is happening though...  rclone is obeying the `Content-Encoding: gzip` header and decompressing the file.  This file doesn't match the md5sum of what was uploaded - hence the corrupted error.

Removing the default `Accept-Encoding: gzip` that golang uses is easy enough, but that would likely affect performance for other uses (eg directory listings).

I put a binary [here](http://pub.rclone.org/rclone-no-gzip/rclone) for you to try.
 Thanks for testing that out and the explanation.

I think what I'll do is add an option something like `--no-gzip-encoding` to turn off the default `Accept-Encoding: gzip` which should fix your problem, and won't affect other users.
 This is done now - please find it in the v1.28 release.

-- Nick
  Good idea!

This would involve setting `x-amz-storage-class: REDUCED_REDUNDANCY` at the time the object was uploaded.

It looks like this would be supported by the aws sdk

``` go
api.go: // @enum ObjectStorageClass
api.go: ObjectStorageClassStandard = "STANDARD"
api.go: // @enum ObjectStorageClass
api.go: ObjectStorageClassReducedRedundancy = "REDUCED_REDUNDANCY"
api.go: // @enum ObjectStorageClass
api.go: ObjectStorageClassGlacier = "GLACIER"
api.go: // @enum ObjectVersionStorageClass
api.go: ObjectVersionStorageClassStandard = "STANDARD"
api.go: // @enum StorageClass
api.go: StorageClassStandard = "STANDARD"
api.go: // @enum StorageClass
api.go: StorageClassReducedRedundancy = "REDUCED_REDUNDANCY"
api.go: // @enum StorageClass
api.go: StorageClassStandardIa = "STANDARD_IA"
api.go: // @enum TransitionStorageClass
api.go: TransitionStorageClassGlacier = "GLACIER"
api.go: // @enum TransitionStorageClass
api.go: TransitionStorageClassStandardIa = "STANDARD_IA"
```

This could be made as either a command like flag `--s3-storage-class` or as a config file option. I'd probably favour the former.
 I'll try to get it done soon.  Or someone could send a pull request?
 I'm away at the moment. Will review next week :-)
 This has been done in #672 

Here is a beta to try: http://pub.rclone.org/v1.33-22-g7227a26%CE%B2/
  Note that go binaries as compiled in the default manner are nearly static.  They rely on the system resolver and not a lot else.

What you've written looks like a good way of building completely static binary.

Note also that the binaries I release on the downloads page are all static, except for the amd64 one for some reason (probably because that is my build platform).
  Thank you for excellent debugging.

I tried to replicate this problem with hubic directly.

That isn't quite what I see (using rclone with `--dump-headers`)

```
X-Object-Manifest: swift-test_segments/Vidéo/1455267454.171398132/10485760
```

Can you confirm which OS you are using? It could be an issue with the file system encoding if you are using linux.

Have you tried hubic directly with rclone (not via hubic2swiftgate) as rclone now supports hubic directly? http://rclone.org/hubic

> If a filename contains one or more accents (like the word Liberté.jpg), upload is OK, no error reported by rclone. But if I launch hubiC official sync software, when the soft compares uploaded file with rclone and local file, it says there is a conflict between both. No problem with filename without accents.

Can you try downloading the fie with rclone and see if it is correct?
 > X-Object-Manifest default_segments/Test/Vid??o de vacances.mkv/1455177144.1105052/9719029414

This looks like it is undecoded UTF-8 so might just be an artifact of the way Cyberduck displays the string.

> Also Content-Type is "application/octet-stream" instead of "video/x-matroska"

I think this is probbly normal.

As far as I can tell rclone is working properly with chunked files with accents

Upload

```
$ rclone -v --swift-chunk-size=1M sync swift-test/ hubic:swift-test
```

List

```
$ rclone --dump-bodies -v --swift-chunk-size=1M lsl hubic:swift-test
2016/02/12 10:25:04 HTTP RESPONSE
2016/02/12 10:25:04 HTTP/1.1 200 OK
Connection: close
Content-Length: 10485760
Accept-Ranges: bytes
Content-Type: video/x-matroska
Date: Fri, 12 Feb 2016 10:25:04 GMT
Etag: "8d431e7531abb83a6cf67e56d91c6f74"
Last-Modified: Fri, 12 Feb 2016 10:22:19 GMT
X-Object-Manifest: swift-test_segments/Vidéo.mkv/1455272496.908089688/10485760
X-Object-Meta-Mtime: 1455266867.393518171
X-Timestamp: 1455272538.21565
X-Trans-Id: 0599FFFA:C270_253BBF47:01BB_56BDB300_BF9CD77:5635

 10485760 2016-02-12 08:47:47.393518171 Vidéo.mkv
```

List segments

```
$ rclone --swift-chunk-size=1M ls hubic:swift-test_segments
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000000
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000001
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000002
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000003
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000005
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000004
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000006
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000007
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000008
  1048576 Vidéo.mkv/1455272496.908089688/10485760/00000009
```

Download

```
$ rclone -v --swift-chunk-size=1M sync hubic:swift-test  swift-test2

$ md5sum swift-test/* swift-test2/*
f1c9645dbc14efddc7d8a322685f26eb  swift-test/Vidéo.mkv
f1c9645dbc14efddc7d8a322685f26eb  swift-test2/Vidéo.mkv
```

Which makes me wonder if the official hubiC sync software is treating segmented files with accents differently to rclone?

Can you check?
 I see the difference

The python client library is url-encoding the name of the manifest

```
X-Object-Manifest: default_segments/Films%20SD/Les%20Coll%C3%A8gues.mkv/14199774
83.785818/2886609359/1073741824/
```

[The docs](http://docs.openstack.org/developer/swift/api/large_objects.html#dynamic-large-objects) indicate rclone is doing it wrong and the python client library is correct...

> You must UTF-8-encode and then URL-encode the container and common prefix in the X-Object-Manifest header.

I'll fix this.  I need to think about backwards compatibility a bit but I don't think it will be too much of a problem.
 I've fixed this now.  Here is a beta for you to try

http://pub.rclone.org/v1.27-45-ge6f340d%CE%B2/

Thanks for reporting the bug.

-- Nick
  The v2 api does allow setting modification times - the `client_modified` attribute: [See upload docs](https://www.dropbox.com/developers/documentation/http/documentation#files-upload)

It doesn't allow setting it on an existing object so it would require #348 fully useful.

On 28th June 2017 [API v1 will be turned off](https://blogs.dropbox.com/developers/2016/06/api-v1-deprecated/)

**Update** [deadline extended to 28 September 2017](https://blogs.dropbox.com/developers/2017/06/updated-api-v1-deprecation-timeline/)

NB: dropbox now have [an unofficial sdk for go](https://github.com/dropbox/dropbox-sdk-go-unofficial)

See also: #969 @paulraines68 `--update` is the option you want. http://rclone.org/docs/#u-update @paulraines68 
> So --update does work with dropbox and uses the service_modified time?

Should do! Dropbox v2 API has just hit master!

Please try this beta

https://beta.rclone.org/v1.36-144-g178ff62d/ (uploaded in 15-30 mins) Bit of trouble building: beta now here: https://beta.rclone.org/v1.36-146-g71028e0f/ @alpha-rudy which rclone are you comparing it with? Current beta

```
$ rclone -v sync /tmp/1000files dropbox:1000files
2017/06/27 10:04:57 INFO  : Dropbox root '1000files': Modify window is 1s
2017/06/27 10:05:05 INFO  : Dropbox root '1000files': Waiting for checks to finish
2017/06/27 10:05:05 INFO  : Dropbox root '1000files': Waiting for transfers to finish
2017/06/27 10:05:05 INFO  : Waiting for deletions to finish
2017/06/27 10:05:05 INFO  : 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:              1000
Transferred:            0
Elapsed time:        8.1s
```

v1.36

```
$ rclone-v1.36 -v sync /tmp/1000files TestDropboxOld:1000files
2017/06/27 10:05:52 INFO  : Dropbox root '1000files': Modify window not supported
2017/06/27 10:05:57 INFO  : Dropbox root '1000files': Waiting for checks to finish
2017/06/27 10:05:57 INFO  : Dropbox root '1000files': Waiting for transfers to finish
2017/06/27 10:05:57 INFO  : Waiting for deletions to finish
2017/06/27 10:05:57 INFO  : 
Transferred:      0 Bytes (0 Bytes/s)
Errors:                 0
Checks:              1000
Transferred:            0
Elapsed time:        5.1s
```

Now log the transactions

```
$ rclone -vv sync /tmp/1000files dropbox:1000files --dump-bodies --log-file dropbox.new
$ rclone-v1.36 -vv sync /tmp/1000files TestDropboxOld:1000files --dump-bodies --log-file dropbox.old
$ ls -l dropbox.new dropbox.old
-rw-r----- 1 ncw ncw 940172 Jun 27 10:06 dropbox.new
-rw-r----- 1 ncw ncw 943699 Jun 27 10:07 dropbox.old
$ wc -l dropbox.new 
5516 dropbox.new
$ wc -l dropbox.old
5206 dropbox.old
$ grep -c 'HTTP REQUEST' dropbox.new dropbox.old
dropbox.new:103
dropbox.old:103
$
```

So there does appear to be a bit of a slow down with the v2 API from 5s to 8s in the test above.  However it uses the same number of HTTP transactions of about the same size which means it is very likely the v2 API just is slower.

Does that agree with your tests @alpha-rudy 
 @mchudoba thanks for the heads-up!  I've merged that in 6a47d966a4109f6650f68297d1c2c41238e67c14

I rebased it and fixed a couple of typos.

Thank you very much for the contribution.
  I should put that in the docs - thanks for the reminder. 

I'll reopen this to remind me. 
  That statement should really say that Dropbox's public API doesn't allow the modification time to be set. They have a private API which does for their own syncer. 

Here us a thread on the Dropbox support forum

https://www.dropboxforum.com/hc/en-us/community/posts/202299459-API-v2-set-metadata-mtime
 Actually I think the v2 api does allow setting modification times - the `client_modified` attribute: [See upload docs](https://www.dropbox.com/developers/documentation/http/documentation#files-upload)

I haven't found way you can set them on an existing object though which rclone needs in an ideal world.

For normal operation (not with dropbox) rclone sets the mod time to be exactly the same as the local mod time which is quite a sensitive way of detecting whether files need syncing.  If rclone can't set the mod time it is still useful, but rclone can't currently use it as part of the sync.

I posted a [question on the dev forum](https://www.dropboxforum.com/hc/en-us/community/posts/204666633-Is-it-possible-to-set-client-modified-on-an-existing-file-in-v2-API-)...

The v2 api still doesn't return any form of checksum though.
 I made a ticket #348 to remind me to think about how I can do sync on a remote which can't set the mod time on an existing object.  This would be useful for b2 also.
  Delete works for me with v1.27.  Can you post the command line you are trying?
 Sounds like you do have 1.26.I wonder if you got a cached version of the download page - I update it on every release. Can you try downloading rclone again? 
  It is working for me.

I suspect a temporary glitch at Amazon.

Can you post the output of the command with the `-v` flag if you are
still having a problem?  Also tell me which platform and whether there
is anything special about your Internet connection (eg a proxy).

```
$ rclone -v copy 100MB acd:100-mb-test
2016/02/07 20:08:56 acd: Saving new token in config file
2016/02/07 20:08:57 100-mb-test: Failed to read info: Node not found
2016/02/07 20:08:57 Amazon cloud drive root '100-mb-test': Modify window not supported
2016/02/07 20:08:57 Amazon cloud drive root '100-mb-test': Building file list
2016/02/07 20:08:57 Amazon cloud drive root '100-mb-test': Reading ""
2016/02/07 20:08:58 Amazon cloud drive root '100-mb-test': Finished reading ""
2016/02/07 20:08:58 Amazon cloud drive root '100-mb-test': Waiting for checks to finish
2016/02/07 20:08:58 Amazon cloud drive root '100-mb-test': Waiting for transfers to finish
2016/02/07 20:09:47 100MB: Copied (new)

$ rclone lsl acd:100-mb-test
104857600 2016-02-07 20:09:46.982000000 100MB
```
 That is interesting....

I presume you are using CIFS on a linux machine?

[This thread](https://bbs.archlinux.org/viewtopic.php?id=126328)  looks relevant maybe.

Just to be sure you can `cp` the file onto the linux machine?
 Well done for finding the issue - a tricky one to track down.

I'll close this now we've come to the bottom of it.

Thanks

Nick
  Issue 1 is a bug - thanks for pointing that out. 

Issue 2 is a known problem with rclone. Rclone doesn't really understand directories - see #100 

I may do a stop gap measure to get rclone to delete empty directories which would fix most of the problems. 
 This also happens with swift so is likely a missing check in `operations.go`.
 The fix for creating directories on `--dry-run` will be in the next release - thanks for reporting it.

Nick
  The value is in Kbytes if you don't supply a unit. You shouldn't need to use --swift-chunk-size at all though - the default should work just fine. 
  In long directory listings we are seeing these errors

```
2016/01/31 19:14:17 Amazon cloud drive root 'XXX': Couldn't list files: Get https://cdws.us-east-1.amazonaws.com/drive/v1/nodes?filters=parents%3AXXX: EOF
```

The token expires every hour and sometimes we get errors immediately afterwards

```
2016/01/31 20:34:07 Amazon cloud drive root 'XXX: Couldn't list files: HTTP code 401: "401 Unauthorized", reponse body: {"message":"Token has expired"}
...
2016/01/31 20:34:20 XXX: Saving new token in config file
```

```
2016/01/31 23:22:31 Amazon cloud drive root 'XXX: Couldn't list files: HTTP code 400: "400 Bad Request", reponse body: {"logref":"XXX","message":"Next token is expired"}
```

Low level retry of these errors would be a good idea to make listings more reliable.  This should work even for the "Next token is expired" case.
 Here is a beta for you to try: http://pub.rclone.org/v1.27-23-ga3b4c8a%CE%B2/
  @jacobfarkas can you make sure you are using rclone 1.27 and using the flag `--drive-full-list=false` (until I fix #336).

Also can you check you can download the docs from the web interface?  Try right clicking on them in the drive web interface and selecting download - it might be that you don't have permissions to export the documents.

What happens when you run `rclone -v ls drive:path` - does rclone mention the docs at all in the debug output?
 Another thought - are these files shared with you? See #48
 Great.  I'll mark this as a duplicate of #336 then.
 Yes that is expected.  If `--drive-full-list=false` then it has the same cause as #336
  All good ideas - thanks!
 I've made a non interactive dedupe.  You can find a beta of it here.

I used it to dedupe my Google Photos directory which was really useful (there were loads of dupes in there put in by Google somehow).

http://pub.rclone.org/v1.28-6-gfdd4b4e%CE%B2/

And here are the instructions!

### rclone dedupe remote:path

By default `dedup` interactively finds duplicate files and offers to
delete all but one or rename them to be different. Only useful with
Google Drive which can have duplicate file names.

The `dedupe` command will delete all but one of any identical (same
md5sum) files it finds without confirmation.  This means that for most
duplicated files the `dedupe` command will not be interactive.  You
can use `--dry-run` to see what would happen without doing anything.

Here is an example run.

Before - with duplicates

```
$ rclone lsl drive:dupes
  6048320 2016-03-05 16:23:16.798000000 one.txt
  6048320 2016-03-05 16:23:11.775000000 one.txt
   564374 2016-03-05 16:23:06.731000000 one.txt
  6048320 2016-03-05 16:18:26.092000000 one.txt
  6048320 2016-03-05 16:22:46.185000000 two.txt
  1744073 2016-03-05 16:22:38.104000000 two.txt
   564374 2016-03-05 16:22:52.118000000 two.txt
```

Now the `dedupe` session

```
$ rclone dedupe drive:dupes
2016/03/05 16:24:37 Google drive root 'dupes': Looking for duplicates using interactive mode.
one.txt: Found 4 duplicates - deleting identical copies
one.txt: Deleting 2/3 identical duplicates (md5sum "1eedaa9fe86fd4b8632e2ac549403b36")
one.txt: 2 duplicates remain
  1:      6048320 bytes, 2016-03-05 16:23:16.798000000, md5sum 1eedaa9fe86fd4b8632e2ac549403b36
  2:       564374 bytes, 2016-03-05 16:23:06.731000000, md5sum 7594e7dc9fc28f727c42ee3e0749de81
s) Skip and do nothing
k) Keep just one (choose which in next step)
r) Rename all to be different (by changing file.jpg to file-1.jpg)
s/k/r> k
Enter the number of the file to keep> 1
one.txt: Deleted 1 extra copies
two.txt: Found 3 duplicates - deleting identical copies
two.txt: 3 duplicates remain
  1:       564374 bytes, 2016-03-05 16:22:52.118000000, md5sum 7594e7dc9fc28f727c42ee3e0749de81
  2:      6048320 bytes, 2016-03-05 16:22:46.185000000, md5sum 1eedaa9fe86fd4b8632e2ac549403b36
  3:      1744073 bytes, 2016-03-05 16:22:38.104000000, md5sum 851957f7fb6f0bc4ce76be966d336802
s) Skip and do nothing
k) Keep just one (choose which in next step)
r) Rename all to be different (by changing file.jpg to file-1.jpg)
s/k/r> r
two-1.txt: renamed from: two.txt
two-2.txt: renamed from: two.txt
two-3.txt: renamed from: two.txt
```

The result being

```
$ rclone lsl drive:dupes
  6048320 2016-03-05 16:23:16.798000000 one.txt
   564374 2016-03-05 16:22:52.118000000 two-1.txt
  6048320 2016-03-05 16:22:46.185000000 two-2.txt
  1744073 2016-03-05 16:22:38.104000000 two-3.txt
```

Dedupe can be run non interactively using the `--dedupe-mode` flag.
- `--dedupe-mode interactive` - interactive as above.
- `--dedupe-mode skip` - removes identical files then skips anything left.
- `--dedupe-mode first` - removes identical files then keeps the first one.
- `--dedupe-mode newest` - removes identical files then keeps the newest one.
- `--dedupe-mode oldest` - removes identical files then keeps the oldest one.
- `--dedupe-mode rename` - removes identical files then renames the rest to be different.

For example to rename all the identically named photos in your Google Photos directory, do

```
rclone dedupe --dedupe-mode rename "drive:Google Photos"
```
  This would be relatively simple to do at `rclone config` time - rclone could request a reduce set of permissions via oauth2 and then that remote would be read only.

I'l probably add another step in the config "Restrict rclone to read only? Y/N" or something like that.

Most of the other oauth based providers have this facility too I think
- [Drive](https://developers.google.com/drive/v2/reference/permissions/list)
- [Amazon Cloud Drive](https://developer.amazon.com/public/apis/experience/cloud-drive/content/getting-started)
- [One drive](https://dev.onedrive.com/auth/msa_oauth.htm#authentication-scopes)
- [Google cloud storage](https://cloud.google.com/storage/docs/authentication?hl=en#oauth-scopes)
- Dropbox could allow access to app folder only, not so much read only
 Found this snippet of code I was testing with - will be useful here!

``` patch
diff --git a/drive/drive.go b/drive/drive.go
index fce2b69..12f711d 100644
--- a/drive/drive.go
+++ b/drive/drive.go
@@ -54,7 +54,8 @@ var (
        driveUploadCutoff = chunkSize
        // Description of how to auth for this app
        driveConfig = &oauth2.Config{
-               Scopes:       []string{"https://www.googleapis.com/auth/drive"},
+               Scopes: []string{"https://www.googleapis.com/auth/drive"},
+               //Scopes:       []string{"https://www.googleapis.com/auth/drive.readonly", "https://www.googleapis.com/auth/drive.metadata.readonly"},
                Endpoint:     google.Endpoint,
                ClientID:     rcloneClientID,
                ClientSecret: fs.Reveal(rcloneClientSecret),
```
  ```
$ rclone ls drive:GDocs
     5570 Drawings.svg
    27580 Slides.pptx
     4297 Docs.docx
     3516 Sheets.xlsx
```

vs

```
$ rclone ls drive: | grep GDocs
$
```

Factored from #49

Note this is a temporary work around: use the `--drive-full-list=false` flag
 Here is a beta to try the fix for this: http://pub.rclone.org/v1.27-1-g5dd387f%CE%B2/
 I've merged this to master - you can find the fix in this beta

http://pub.rclone.org/v1.27-11-g1cd0d9a%CE%B2/

It will be in the next release
 @dolabriform the pacer messages are normal - Drive does that when it is getting congested I think.
  Can you write something in the docs too on how to configure it?  docs/content/swift.md

It should maybe have a section in the auto configurator (when you run `rclone config`) or do you think it is too obscure for that?
 Sorry this has taken me a while to get round to it!

I've tested the code, made a couple of small fixes and merged it - thank you very much for your contribution!
  I'm not sure I understand what you mean. The source files are never deleted?
 There is an `rclone move` command which might do what you want?

I've just noticed this is missing from the docs!
 I can confirm that! I'll make a fix and put a beta here for you to try
 This is fixed now and will be in the next release.

Here is a beta for you to try with the fix.

http://pub.rclone.org/v1.27-33-g3890105%CE%B2/

Thanks for reporting the bug

Nick
 rclone `move` works like it does because it can invoke a server side operation to move the whole directory tree if available.

I see where you are coming from though - I'll have a think!
 @reuu check that there weren't any errors copying.  If there were then rclone doesn't do deletions.
  That looks very useful thank you.

Would you mind adding a bit to the docs: docs/content/s3.md explaining the above and how you configure rclone to use the IAM roles credentials.

Thanks

Nick
 I'm not sure I really like the 1 second pause - what if the server is down for some reason then it will fallback to anonymous which probably isn't what you want.

I wonder whether we should change the anonymous access so it requires the `access_key_id` of `anonymous`. Your sequence of fallbacks if the credentials are empty would make more sense then
- AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY environment variables
- IAM role credentials
- An error if both of those fail

I note that the text in the configurator wording needs to change too

``` go
        Options: []fs.Option{{
            Name: "access_key_id",
            Help: "AWS Access Key ID - leave blank for anonymous access.",
        }, {
            Name: "secret_access_key",
            Help: "AWS Secret Access Key (password) - leave blank for anonymous access.",
        }, {
```
 On second thoughts I think you are right `access_key_id = "anonymous"` is probably not a good idea.

What do you think of having something like this in the configurator?

```
Select authentication type
1) access_key_id / secret_access_key - use this if not sure
2) anonymous access
3) use environment variables AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY 
4) use IAM role credentials (for within EC2 only)
```

Then change something like this

```
        Options: []fs.Option{{
            Name: "access_key_id",
            Help: "AWS Access Key ID - leave blank for anonymous access, env var or IAM",
        }, {
            Name: "secret_access_key",
            Help: "AWS Secret Access Key (password) - leave blank for anonymous access, env var or IAM.",
        }, {
```

So we could be explicit about exactly which options we are supporting.  Would that be useful to you?
 I made a few comments on the PR, but I think it is basically good - I like the concept of auth in the config file OR from the environment with the auto choices - nice one!
 Can you update, rebase, squash and update the PR and I'll merge.

Thanks

Nick
 That is looking really good.

I've merged and pushed to a branch `stengaard-iam-role-credentials`

While testing, I realised that auth from env vars doesn't seem to work.  I made a fix here ce4c1d4f356776e238b5ce367c42f0a40f680855

If you are happy that that fix does the right thing then I'll merge to master.

I also patched the docs here a0b9bd527ea0b6ea1e48392b87f56f26ab919e35
 I've merged this now.  Thank you very much for your contribution :-)
  Rclone already does that.  See the `--transfers` flag which is set to 4 by default.
  I see this strange message:

```
2016/01/29 06:15:11 backup-2016-01-28_21:20:00: Duplicate file detected
```

Obviously, the local file system doesn't allow duplicates. But the same file is shown twice in the Google Drive through the web interface. All files only get there through the API via rclone.
 I think this is #28 

I have been unable to reproduce this problem, but if you can get a log of it happening (with the `-v` flag) then post it to that ticket!

The best thing to do is delete the duplicates rclone detects in the web interface.

Thanks

Nick
 ...I just release rclone 1.27 from which you can use `rclone dedupe` to get rid of the duplicates.
 @J0s3f I've done this already in #338 - you can find instructions and a beta release to try it on that issue.

Cool script by the way!

Thanks

Nick
  I've had a look at the logs.

There are a lot of 403 rate limit exceeded errors which are normal with google drive. Google throttle the rate of upload to about 2 files per second.

The `Failed to copy: Post https://www.googleapis.com/upload/drive/v2/files?alt=json&uploadType=resumable: http: ContentLength=193 with Body length 0` are unusual.

Could your ISP be throttling those - killing the connections?  I know some ISPs do exactly that.

And the final thing I noticed is that there are lot of duplicates.  Sometimes people see one or two, but you have a lot more than that.  It makes me think maybe it is your ISP killing connections...
 If you want to fix the dupes, then you can try the dedupe command in this beta.

http://pub.rclone.org/v1.26-51-g3101a7f%CE%B2/

### rclone dedupe remote:path

Interactively find duplicate files and offer to delete all but one or
rename them to be different. Only useful with Google Drive which can
have duplicate file names.

```
$ rclone dedupe drive:dupes
2016/01/31 14:13:11 Google drive root 'dupes': Looking for duplicates
two.txt: Found 3 duplicates
  1:       564374 bytes, 2016-01-31 14:07:22.159000000, md5sum 7594e7dc9fc28f727c42ee3e0749de81
  2:      1744073 bytes, 2016-01-31 14:07:12.490000000, md5sum 851957f7fb6f0bc4ce76be966d336802
  3:      6048320 bytes, 2016-01-31 14:07:02.111000000, md5sum 1eedaa9fe86fd4b8632e2ac549403b36
s) Skip and do nothing
k) Keep just one (choose which in next step)
r) Rename all to be different (by changing file.jpg to file-1.jpg)
s/k/r> r
two-1.txt: renamed from: two.txt
two-2.txt: renamed from: two.txt
two-3.txt: renamed from: two.txt
one.txt: Found 2 duplicates
  1:         6579 bytes, 2016-01-31 14:05:01.235000000, md5sum 2b76c776249409d925ae7ccd49aea59b
  2:         6579 bytes, 2016-01-31 12:50:30.318000000, md5sum 2b76c776249409d925ae7ccd49aea59b
s) Skip and do nothing
k) Keep just one (choose which in next step)
r) Rename all to be different (by changing file.jpg to file-1.jpg)
s/k/r> k
Enter the number of the file to keep> 2
one.txt: Deleted 1 extra copies
```

The result being

```
$ rclone lsl drive:dupes
   564374 2016-01-31 14:07:22.159000000 two-1.txt
  1744073 2016-01-31 14:07:12.490000000 two-2.txt
  6048320 2016-01-31 14:07:02.111000000 two-3.txt
     6579 2016-01-31 12:50:30.318000000 one.txt
```
 I'm going to close this now as we haven't made any progress on the ticket recently.  Please re-open if more info turns up!  Thanks Nick
  The only time rclone deletes data is if you are using the sync command (not copy) and only from the destination. So what was the command running on the crontab? If it was syncing data to the cloud then it wasn't rclone.

Can you paste the command you were running on the crontab and the command you were using to copy data into the synced drive, that is the two commands that were running at once. 
 The Unix command

```
du -m | sort -rn | tail -n 100
```

WIll list the 100 directories with the least content starting from the current directory.
 `/usr/sbin/rclone sync /home/isaiah/Music Adrive:Music` only reads data from `/home/isaiah/Music` so I think it is unlikely rclone deleted any files in there.

Did you run out of disk space at one point?  That might explain it?
 I'm going to close this as I don't think we are going to get to the bottom of it! If you have any more info then please re-open.  Thanks, Nick.
  Have a look at [restic](https://github.com/restic/restic), it has a fully encrypted destination and deduplication. That may suit your immediate need better.
 Duplicate of #219 
  This should do the trick

```
rclone sync /mybackup/path remote:container > "/var/log/rclone.log" 2>&1
```
  Run through the config again, but answer `N` to this question. 

```
Use auto config?
 * Say Y if not sure
 * Say N if you are working on a remote or headless machine or Y didn't work
```

Go to the link in your desktop browser and paste the token. 
 No problem!
  v3 is now available in the go interface package - update rclone to use it.

* [x] do conversion - see #2007
* [x] fixup places in the VFS layer which might be confused by size < 0
* [x] fixup sync routines to treat a size < 0 as size unknown
* [x] investigate whether we can avoid reading the "root" id which is doing an extra transaction on startup
 @gustavorochakv thanks for the heads up.
 No it is still on v2.  Google have updated the v2 API to work with Team Drives.  The mtime is the key on S3 the modification time is stored.

This error actually means that the file "key" that was being updated doesn't exist.

http://www.benhallbenhall.com/2013/01/amazon-s3-error-key-exist/

It probably means that the transfer of that file failed in some way earlier.

Retrying the sync should fix it
 The 408 comes from the server so upping the rclone timeout probably won't help. I have found that cloud files gets  very busy at times. 
 That is exciting! Never seen that before. Can you post the whole log line with a few logs before and after? 
 So what that error means is that rclone went to set the modification time on the file, but s3 said it didn't exist which is strange indeed.

The `Unsolicited response received on idle HTTP channel starting with "H"` is strange also.

Both of those things make me wonder about your environment - I haven't seen either of those problems before.

Are you using the pre-compiled binary or did you compile it yourself?  If you compiled it yourself, can you try the pre-compiled binary?

Is it going through some sort of HTTP proxy?

Can you try the transfer on a machine which isn't within EC2?

It might be an eventual consistency thing.  If you retry the sync does it work afterwards?  Or is that file always a problem?

I tried a test with a file of that name on s3 and that didn't seem to be the problem. rclone could set the modification time just fine.

Does `rclone ls` list the file properly?  If so can you send me a snip from the listing?
 You can set the meta data after the object has been uploaded  using a COPY. 

It seems plausible that the + is causing the problem, but I can't replicate the problem. 

Did you try from your osx machine? 

Can you write a shell script that reproduces the problem? 

Eg

```
mkdir /tmp/test
echo 'hello' > /tmp/test/file-with+44.pdf
rclone sync /tmp/test s3:mytest49
touch /tmp/test/file*
rclone sync /tmp/test s3:mytest49 
```

Something like that should reproduce the problem - but I couldn't make it. 

It might be to do with the path of the file too. 

Does your bucket name really have < > in? That could be the problem. 
 I feel stupid now, because I just ran the script I sent you and it reproduces the problem for me now just fine!

```
2016/01/27 17:06:43 HTTP REQUEST
2016/01/27 17:06:43 PUT http://s3.amazonaws.com/mytest50/file-with%2B44.pdf HTTP/1.1
Host: s3.amazonaws.com
User-Agent: rclone/v1.26
Content-Length: 0
Authorization: AWS4-HMAC-SHA256 Credenti[snip]
Content-Type: application/pdf
X-Amz-Acl: 
X-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
X-Amz-Copy-Source: mytest50/file-with+44.pdf
X-Amz-Date: 20160127T170643Z
X-Amz-Meta-Mtime: 1453914402.34825253
X-Amz-Metadata-Directive: REPLACE
Accept-Encoding: gzip
```

```
2016/01/27 17:06:43 HTTP RESPONSE
2016/01/27 17:06:43 HTTP/1.1 404 Not Found
Transfer-Encoding: chunked
Content-Type: application/xml
Date: Wed, 27 Jan 2016 17:06:42 GMT
Server: AmazonS3
X-Amz-Id-2: 55baDyXyzc+SSEIWHVJ+yvSo+BW1Lr8yOdTg5nRuleC59dodpeWJAbSMFytup75eYps/6j3u4T8=
X-Amz-Request-Id: 56F1855EE2332F05

f8
<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Key>file-with 44.pdf</Key><RequestId>56F1855EE2332F05</RequestId><HostId>55baDyXyzc+SSEIWHVJ+yvSo+BW1Lr8yOdTg5nRuleC59dodpeWJAbSMFytup75eYps/6j3u4T8=</HostId></Error>
0
```

Which makes the problem blindingly obvious - `<Key>file-with 44.pdf</Key>` is a give away.

I've fixed this and uploaded a beta for you to try

http://pub.rclone.org/v1.26-48-gcae19df%CE%B2/

If it doesn't fix it for you then please re-open the ticket!
 Once transfers start memory should be stable. Can you see if it goes up while transfers are in progress? Maybe there is a memory leak. It is probably unrelated to this issue though. 
  An interesting idea - thanks.

I think that sort of output should probably be controlled by an option.

Exactly what do do with logs generated would be another thing to think about.

A full terminal windowed mode would be fun - I'd probably start from https://github.com/nsf/termbox-go which I've used before and is nicely cross platform, or maybe one of the higher level libraries on top of termbox, eg https://github.com/jroimartin/gocui
  Thanks for reporting that. I missed that the token expired in the docs. 
 Do you happen to know which error code backblaze returned? 401?  Or was it something else?

There isn't much of a clue in the docs

https://www.backblaze.com/b2/docs/b2_authorize_account.html

I might just fetch a new one every hour - that would probably do it.
 The actual error is

```
x: Failed to copy: Failed to upload: Authorization token has expired (401 expired_auth_token)
```
 I've fixed this.  It took a little while with a 24 hour test cycle! I've also fixed a number of other important B2 bugs!

Here is a beta for you to try.

http://pub.rclone.org/v1.27-41-g6b6b434%CE%B2/

Thanks for reporting the bug.

Nick
 See #373 for the multiple upload URLs - I've fixed that and there is a beta with that in on that issue!
  We considered all the alternatives, and all of them are worse. There are secrets for other platforms as well.

There is not good way of doing oauth from an open source app, unless it is binary only releases.

At least this way, we don't rely on some proxy setup, that is just as bad, and furthermore puts user login information at potential risk.

The only risk is that someone will claim to Google (and the user) that they are "rclone". It doesn't allow access to any information unless the user explicitly grants access. They could "piggyback" on the "rclone.conf" file, but any way of storing login information locally exposes that risk.

You could open an issue to add optional encryption of `rclone.conf`\- that is way more relevant.
 > Supposing a user allowed (on his machine) rclone to access its Google Drive. Does it mean that someone else using rclone on another machine has access?

Absolutely not.

The only thing you can do with the rclone client secret is pretend to be the rclone app which doesn't gain you anything really.

> Even having it into a public binary is just making it a bit harder but that's not changing really the security.

Note that the secret is obfuscated so you can't get it from looking at the source without a bit of work.  Again not really making it a lot harder to derive it.

I should probably rename the constant to `rcloneEncryptedClientSecret` or something like that which will give people who are viewing the source code a bit more confidence.

> I'd suggest to offer during the setup to either use the default key (with an notice it may reduce security), or let users provide their own (with a link to Google Developer Console to help set one up).

You can already provide your client details in the setup process.

Doing so doesn't change the security of anything though.
 I just read the linked article: http://security.stackexchange.com/questions/107370/should-i-trust-grive2/107372

I think the accepted answer is wrong

> If you allow this application to be trusted by your Google account, then any permissions you give the app will be inherently granted to anyone who decides to maliciously make use of the client secret together with the client ID.

You may give permissions to the app which is identified by the client secret, but you need to go through the oauth2 workflow to get a bearer token to actually use those permissions.  This requires interaction with the user and the user putting in their password.  The bearer token is the real security.
 > I think the accepted answer is wrong

It is. Otherwise there is no reason for the user to authenticate at all.

That said, we could rather easily allow users to encrypt `rclone.conf`. They would of course have to enter the key every time they start rclone, but that would make it impossible for programs to use the credentials in rclone.conf without the user knowing.

It looks like I could get some time next week, I can give it a shot.
 Not everybody are so fortunate to have such OS features :)

You could argue that is the users problem, if they run rogue software. But if I somehow got a copy of your rclone.conf, I would be able to access your files until you revoked rclone access on your cloud provider.
 @Chris2048 wrote

> There would be no token granted without a new user access approval (that right?)

Correct

> and afaik if the token was already obtained somehow, it can be used without the sk (is that right?).

Also correct. Though the token will expire eventually and the sk will be needed at renewal.

> I took the SE commentors advice, and setup my own app & key. When I inserted it into the rClone code

That was a rather long way round!  Next time just set `client_id` and `client_secret` in the config file, or during the setup process

```
Google Application Client Id - leave blank normally.
client_id> 
Google Application Client Secret - leave blank normally.
client_secret> 
```

> I think ultimately, any client-side code holding a SK needs either a per-client key, and if the protocol doesn't handle this case, obfuscation is maybe the best that can be done?

That was my view.  The Obfuscation of the secret key is enough to stop it appearing in the source directly, but won't survive an attacker.  Neither will any of the other methods though (witness the long line of cracked DRM technologies).

> Here is a suggestion that would impact user ease a bit, but maybe the obfuscated app/sk could be put on the github page, and any user wanting to use them have to enter them.

Not wildly keen on that idea on ease of use grounds!  But I see where you are coming from.

I'm yet to be convinced that there are any security implications for the users data by using rclone's credentials.

The implications are more on rclone's side - namely using up rclone's drive quota, doing bad things and getting rclone throttled / banned on drive etc.

@wernight wrote

> That's why I was suggesting a prompt in the shell that allows to input a client key while accepting empty input to use the default one (with a warning).

That is pretty much what it does aleady (see above).

As for a warning - what would it say?
 > NOTE: Using the default key is slightly less secure than getting your own key

Could you please elaborate how it is "slightly less secure", because AFAIK that isn't true.
 @wernight - They obviously don't know how oauth works. To access anything they need your access/refresh token, which is in your rclone configuration file. The client "secret" is used for nothing more than identifying the rclone client at Google. Using another client secret does not in any way provide additional security, it only means that Google sees your client as another piece of software.
 @yonjah 
> I created a folder at the root of a crypt remote called the_test. Inside this one I have another one called test_within. I want to move the_test (including its subdirectory) inside a folder called fixed.

I'm not sure how that would work.  rclone opens known URLs in the browser directly or uses https to known URLs so I'm not sure how you could insert a phishing site in the process. > There might be other providers who will not lock the redirect URL but than it is probably more of an issue with the service provider than with rclone

I'm pretty sure having a fixed redirect URL is part of the oauth spec - certainly all the providers I've used with rclone you have to specify the redirect URL as part of the signup for a new app.  Document that rclone sets non zero exit code on errors
 @kflu I've fixed the documentation here: http://rclone.org/docs/#exit-code
  I had a brief look at the python code and it looks like rclone could interface with it.  Having no official API documentation is a little unusual, but the python code is better than nothing!
  Do you fancy making a pull request with your suggestions?

Thanks

Nick
 Filtering applies to the source, or more accurately it is applied during the listing process.

All paths in rclone are relative to the root of the remote - if you want to see what rclone is matching run `rclone ls` with your filters.
  Thanks for that - will merge shortly!
  Interesting idea!  Probably beyond the scope of rclone, but maybe one day!
 There are a lot of union filesystems that can do this.  Perhaps use that in hte meantime.    I keep meaning to add a flag to allow people to add headers...

What would the header need to be?
 This looks relatively simple to add to the [Get](https://godoc.org/github.com/aws/aws-sdk-go/service/s3#GetObjectInput) and [Put](https://godoc.org/github.com/aws/aws-sdk-go/service/s3#PutObjectInput) code just setting the `RequestPayer` attribute to `requester`.

It could be controlled by a flag `--s3-requester-pay`

Any takers?
  Wow that is slow. I think the most generally usable solution would be to speed up the Swift directory lister, maybe by making it more concurrent. I cannot immediately wrap my head around the current code, but there may be a good place to insert it.

Local caches that cannot be easily validated is a can of worms. Any "use-at-your-own-risk" functionality is a huge support burden, since people rarely read the disclaimers or has problems understanding the implications. However, if Swift cannot be made working at reasonable speed, it may be an option.
 Reading the modification times from the remote is slow with swift/hubic - I think that is the problem.  When you do a listing you get size/name/md5sum but not modtime for all files very efficiently, but to get the modtime takes another http transaction.

You should find that `--size-only` is very much quicker.

I backup 25,000 pictures to swift regularly.  A no changes sync with `--size-only` takes 13s where as it takes 3 minutes 30 seconds without -  16x faster.

We've been discussing a local cache of hashes which would speed up the `--checksum` case enormously and that would become as fast as `--size-only` but much more accurate.
 Glad `--size-only` worked its magic!

As for local file list DB - sqlite is hard to beat for this.

rclone will at some point gain a local cache of files I'm sure!
 I think this ticket is done now.  We have the not perfect `--size-only` work-around and there are other tickets about metadata caches.
  > My understanding was that backblaze had no file size limits.

Unfortunately it does: ["Files can range in size from 0 bytes to 5 billion bytes."](https://www.backblaze.com/b2/docs/files.html).

> `Failed to copy: Failed to upload: Bad hex digit:   (400 bad_request)`

Does it keep giving the same error, if you retry the upload, or does it happen at random?
 ~~Regarding the `bad hex digit`, it could be because it uses [`fmt.Sprintf("%x", hash.Sum(nil))`](https://github.com/ncw/rclone/blob/master/b2/b2.go#L851). It could be that it skips printing if the first byte is a zero value. That would make it fail on 1 in 256 files.~~
 That is an interesting idea Klaus. I was thinking along the line of encoding of an unusual file name which I had problems with in development. 

@wkf Is it always the same files which fail with the percent message? If so could you post the names? 
 Nah, it prints all values: http://play.golang.org/p/KnMHMW71Zd

You are probably right, Nick. These are the rules they specify:

Names can be pretty much any UTF-8 string up to 1000 bytes long. There are a few picky rules:
- No character codes below 32 are allowed.
- Backslashes are not allowed.
- DEL characters (127) are not allowed.
- File names cannot start with "/", end with "/", or contain "//".
 @wkf can you share a failing file name?
 @wkf are you still seeing these problems?  Things have been moving fast at backblaze so I'm hoping the issue is now fixed :-)
 I'm going to assume this is fixed now - please re-open if not.

Thanks

Nick
 @hajes backblaze is a fast moving target - the large file support has only just been invented. Can you make a new issue please? 

Thanks 

Nick 
  Replace `remote` with the name you gave your drive.

Use `rclone config` to see the name of your remote.
  Have you got a file called `MG_2158.CR2` and another called `img_2158.cr2` would be the first thing to check?  Amazon cloud drive can't have files which differ only in case.

If it does that every time you sync then that is probably the explanation.  If not then retrying the sync will probably clear it up.
 I haven't seen `Next token is expired` errors before.  Does the listing take a really long time?

Worth trying at a different time of day.

Did you see about that duplicate file?

Thanks

Nick
 > Does the listing take a really long time?

Well, it is less than 13 minutes. I guess it is because of some server restarts/reconfiguration going on at Amazon.

The failure is in `func (s *NodesService) listNodes`, but it doesn't do anything else than reading nodes. Concurrent reads are limited by the pacer, so we don't do limit the client in any fashion.

Are you by any chance running multiple simultaneous uploads?

I don't know if writes from another client will invalidate the tokens. Depends on Amazon.
 >  same filename in different folders on ACD

This shouldn't be a problem.

I think the problem might just have been temporary - does the sync work properly now?
 @bunset thank you that log is very useful.

I can see the "Next token is expired" messages.  It is clear to me that these are the cause of the "409 Conflict" messages - rclone didn't read part of the directory listing, so missed out some files.

It would be fairly easy to change the recursion strategy for listing from depth first into width first - that should fix the token expired messages hopefully.

Also I'm concerned that the error in the listing didn't seem to get passed up to the master sync command - it really should have done.

Also rclone spends ages retrying those 409 Conflicts for no gain.
 @avgex Thanks for that - very helpful.

I can see the same "Next token is expired" followed later by the "409 Conflict" errors, so this looks like the same problem.

I note also that your file system appears to be in latin1 instead of UTF-8.  This may mean that some of the names with accents look a bit odd in Amazon's web interface ([see the local docs](http://rclone.org/local/)).  You might want to run `convmv` on the filesystem to make it utf-8 which would be the modern thing to do. (I made myself a ticket to write this up #300)
 I'm working on a fix - I'll put a link to a beta release in a day or two.
 I've made a beta for you to try: http://pub.rclone.org/v1.26-34-gde3cf5e%CE%B2/

Any feedback much appreciated.
 Thanks for your reports.  It looks like there is still a problem :-(  I'll see if I can replicate it.

If you could send me a log with `rclone -v --dump-headers` of it pausing that would be really helpful. It will be very big and contain sensitive info, so If you could email it to me privately that would be really helpful.

Here is a new beta.  I don't think it will fix the problem, but just so we are testing the same thing!

http://pub.rclone.org/v1.26-39-gc3a0c0c%CE%B2/
 Actually don't bother sending me a log - I've reproduced the problem!

Will make a new Beta when I've figured out what is going on...
 This should be fixed in version 1.27 which I've just uploaded :-)
 @avgex I see what happened there - that log was really helpful thank you.

The 409 errors are caused by the listing of the files going wrong.  Looks like I have a few more things to retry...

I made an issue here #340 - if you subscribe to that then you'll get updates on it.
 @bunset if you run rclone without the `-v` flag, it should only tell you about the errors rather than about everything it is copying.
 @fcarreiro are you using the latest version of rclone - 1.30 ?  If not can you try that.  If you get the problems with 1.30 then can you open a new issue with a log of all the errors - 409 errors are caused by the directory listing going wrong earlier which I believe is fixed in 1.30.
  Thanks you very much for the kind words and I'm glad you are enjoying rclone.

-- Nick
  This would be relatively easy to detect, but rclone doesn't have a fatal error notification in the sync.  It is highly concurrent so just exiting would break other in progress transfers which might suceed.  Needs a bit of thought.
 rclone does have an error which it knows it shouldn't retry now so this would be easy to implement.
  Do you mean directories, or files within those directories?  rclone doesn't manage empty directories (see #100) at the moment.

If it isn't just empty directories, then can you post a log with `-v` of rclone running please?

Thanks

Nick
 You can use the `--log-file` option to save the output to a file which you might find easier.

I can see it deleting old stuff at the end of the sync.

What might have happened is if there was an error in the sync, it doesn't delete stuff.

Looks like the sync completed correctly this time (according to that log), so it will have deleted the old stuff - can you check?
 Well if you had an errored sync that explains everything
 Try running

```
rclone size remote:dir
```

That will add up what rclone thinks you've got. 

Quite often file managers round up the sizes of things to the nearest disk block (4k say) so I wouldn't be surprised if they are different. Rclone size will tell you how many files there are - try comparing that too. 
 Good thinking to check the local directory with rclone! 
 I'll add a little more explanation to the docs
  CI failure is unrelated, because of #283 
 I've merged this in e7b7432079811c5d23bc18bce6a9093e026f9fb5 - thank you very much for the contribution.

If you think the [swift docs](http://rclone.org/swift/) need an OVH specific paragraph then do send in a pull request.

Thanks

Nick
  Try quotes round it, eg

```
rclone copy "e:\my files" "ACD:docs\my files"
```

That should fix it!
 Reopen, if quotes doesn't fix it.
  Noise about thumbs.db and .ds_store files not quitenened by `-q` is a bug which should be easy to fix.

Using `--include-from` and `--exclude-from` together is problematic, because the `--include-from` adds an implicit `--exclude *` at the end of the filter list.

It does say that in [the docs](http://rclone.org/filtering/), but perhaps it isn't clear enough that using `--include*` and `--exclude*` together won't work.

There is a solution though, use the `--filter-from` flag.

In an ideal world you'd be able to use `--include x --exclude y --include z` on the command line, but rclone's command line parser isn't clever enough to deal with duplicated entries, so I went for the implicit `--exclude *` entry.  I guess I could make that sink to the end of the filter list, so when you did `---include x --exclude y` the `--exclude *` would be moved to the end of the list.
 Yes one `--filter-from` file is your only option at the moment.
 I made a new issue #287 with the `-q` observation.

I'm going to leave this open to remind me to add a bit to the docs about not using include and exclude together.

Thanks

Nick
 I've made a small tweak to the include filters so it adds the exclude rule right at the end of the filter list (after everything else).  This means that your original usage would work now.  I've also added some more  stuff to the docs about it.
  This is a basic framework that will allow multiple hash types across filesystems.

The `fs.Fs` can support the `fs.HashedFs` interface to indicate that the files on it has support for one or more hash types.

This allows us to see if we have overlapping hash type(s) between filesystems.

In the `fs.Object`, the `Md5sum() (string, error)` function is replaced by a `Hash(r fs.HashType) (string, error)` function that returns a specific hash type. I considered a more complex request type, but I would expect this to fulfill our current needs.

Still missing:
- [x] Replace all instances of `Md5Sum()` in filesystems.
- [x] Find common hash types.
- [x] ~~Maybe disable automatic hash calculation on slow systems.~~
- [x] Tests

Future work:
- Add an optional local db with known hashes of local files with size/timestamp/possibly other information to ensure validity.

Related to issues #198, #157, #265 (Syncthing uses SHA256 and Murmur3).
 Ok, this should be enough for a base implementation.

@ncw, you are welcome to review the changes when you have the time. I do currently not have access to any FS with SHA-1 support, so if you are able to test that, would help greatly.
 I have commented on the commit.

An excellent piece of work :-)

I'll try the code next and see what happens...
 This seems to have broken Swift/s3/Google Cloud storage/Hubic/Drive all like this

```
rclone/fs$ go test -remote TestSwift:
--- FAIL: TestSyncAfterChangingContentsOnly (0.85s)
    fstest.go:66: potato: md5 hash incorrect - expecting "e4cb6955d9106df6263c45fcfc10f163" got "100defcf18c42a1e0dc42a789b107cd2"
--- FAIL: TestSyncAfterRemovingAFileAndAddingAFileDryRun (0.61s)
    fstest.go:66: potato: md5 hash incorrect - expecting "e4cb6955d9106df6263c45fcfc10f163" got "100defcf18c42a1e0dc42a789b107cd2"
```
 > As I understand, you want rclone to calculate hashes of every local files on each launch

No.

> Hash calculation on these files couldn't even be done within 24h.

Hash calculation is done while the objects are being _uploaded_ right now (even without this PR). If the destination supports hashing, the destination is checked. This functionality is kept as is. The only change is that SHA1 is also being calculated, so SHA-1 destination can check the hash.

The hash cache will not change any existing functionality. The only difference is that if the hash is calculated for a file, we store it, so if the hash is needed some time in the future we don't need to recalculate. In effect this would allow you to use hashes (`--checksum`) for your backup without recalculating hashes for all your unmodified files.
 @ncw - I have implemented all the things you mentioned.

Please review, but do not merge yet. GIT messed up when I rebased, so there is some junk in there. I will fix it up, or send a new PR.

(Use the Files Changed to see the changes)
 I've merged the rebased version #292 so I'm closing this.
  Could you elaborate on the use case for that feature?
 So in effect it is a "skip-existing-files" behaviour? 
 Back to the rsync man page I see these useful looking options...

```
            --existing              skip creating new files on receiver
            --ignore-existing       skip updating files that exist on receiver
```

I think the second one of these would be equivalent to `--name-only` but better named and with a precedent from rsync.

How does that sound?
 Added PR #276.
  This should be easy to fix with a bit of checking before mkdir.  Should probably do this in s3/gcs too as this operation in s3 has been troublesome in the past with third party implementations.
  That looks very nice - well done! One minor thing commented inline.

Can you add a bit for the docs too please - in the specific options section in `docs/content/drive.md`.

Thanks

Nick
 I have merged this - thank you very much for the contribution.
  Thanks for that - good catch.  Will merge shortly!
 Merged. Thank you very much.
  Thanks for analysing that - I agree with you.

I can think of two ways of fixing your problem.
- Get the remotes which understand directories to stop recursing on excluded directories.  This would be the most efficient fix.  At minimum I could to this for the local filesystem.
- Get the local filesystem driver to keep a note of the inode number of each directory it scans and skip directories it has already scanned.  This would speciffically solve the mount bind issue.

I should note that rsync also has issues with mount bind directories - It doesn't skip them with the `-x` flag like you might expect.
 This should work properly in the latest beta - fancy giving it a go?

http://pub.rclone.org/v1.29-28-g36700d3%CE%B2/
 Great - thanks for testing
 This will be in the next release
  Thanks for that - will merge shortly!
 Merged.  Thank you very much.
  That is a different sort of retry, one done on each HTTP transaction and there isn't a command line flag to adjust that :-(

The `--retries` flag makes rclone try the whole sync again rather than that one operation.

The error above might be caused by having two files with differing case `DSC04098.JPG` and `dsc04098.jpg` - amazon cloud drive doesn't allow that.
 It might not be a duplicate file name - the error is suggestive of that but it could be something else.

> Thanks for pointing that out ncw. In my case, I have thousands of pictures in my Apple photo Library (About 42 GB) which I'm trying to back up to Amazon Cloud Drive (ACD). I don't see a way to rename the all the duplicate file names unless there is an easy way that I don't know.

You can use a bit of shell magic like this to find them

```
cd directory/where/files/are
find . -type f -print | tr '[:upper:]' '[:lower:]' | sort | uniq -c | grep -v '^ *1'
```

You should check.  It might just be ACD being flakey...

> Background:
> Initial try- Initially when I tried to use rclone to backup the pics from my Photos Library (on NAS), it copied almost everything(40 GB) in about 24 hrs but I had to cancel it for other reasons. At this point I was hoping that I can re issue the rclone command and it would pick up the rest (About 2GB) of the stuff and finish in a couple of hours at the max.

That is how it is supposed to work, yes.

> Subsequent tries- I've tried the same command for about 4 or 5 times now and had to cancel every time after about 24 hours, except for the latest try which is still going on after 2 days. All this for the remaining 2GB left. In the standard output, I see a lot of retries which I wrote in the first post above. So far, it has transferred about 200 MB out of the 2 GB left.

This could be related to #263 - do you have lots of files in one directory?

> Questions:
> 1. Looks like ACD doesn't allow the same name files even in different directories, is that true?

No, it should be fine with same name in different directories.

> 1. Is there a way to exclude the same name files using RCLONE command

Not at the moment, but good idea!

> OR limit HTTP transaction retries too?

That is another good idea - I could make a `--low-level-retries` flag
or something like that which would default to 10.

> 1. Is my Final Goal described above possible with RCLONE at all?

You are using rclone exactly how it is supposed to be used.  It should carry on where it left off and your Final Goal should work fine.

The fact it isn't working probably means either a bug in rclone, or a bug in ACD itself.

> Suggestions:
> 1. Could the retries flag be renamed or the retries word in the standard out put generated be renamed to avoid the confusion.

I'd prefer to add a `--low-level-retries` flag - what do you think of that?
 Thanks for checking the duplicates.

I made a ticket about the retries documentation #278

The remote metadata caching in #180 is certainly interesting - if you want to discuss more then please do it on that ticket.

I think your original issue is caused by the general problems with amazon cloud drive which people are having at the moment which I'm hoping to find a work-around for.

However I'm going to use this ticket to implement a `--low-level-retries` flag and make your suggested change to the debug messages.
 You should find that 1.27 is much improved in this regards. 

I did about half of this already but there was too much in 1.27 to put it in too.  Hopefully I'll get it in 1.28
 That is taking much longer than it takes me..

For my Images collection (60k files, 550GB) it takes 1 minute and 20 seconds to do a "do nothing" sync.

Have you got a huge number of files in a single directory?

How long does `rclone size` take on the source and the destination of the sync?  That will give you and idea of where the bottleneck is.
  My guess I'd that you forgot the colon after the Gdrive,  eg

  rclone copy stuff Gdrive:directory 
 Glad it is solved now!
  Can you post a log with `-v` of this happening please? If it is too long for here then stick it on pastebin, or email it to me directly if it has sensitive stuff you don't want to share with the world.

Thanks

Nick
 Thanks for posting the errors.  Those look like winsock errors, ie your machine's TCP stack complaining that it couldn't connect.

https://www.nsoftware.com/kb/xml/07240801.rst

As for what causes it - it might be Amazon, your ISP, your router/firewall it is difficult to know.

rclone shouldn't go mad at this point, it should just carry on so there may be something which needs tweaking.  If you can get me a log of it going wrong that would be super helpful.

....

I realise I didn't do a good job of answering your original question, so I shall now!

> When uploading 100,000 or more files rclone will often get stuck trying to read the ls list of files from ACD.
> This will result in a large number of errors that say "file with this name already exists" because it will not find the file then it will add it to the list of files to upload then when it's time to upload it'll find it already existed.

Is that 100,000 files in one directory? Or 100,000 files spread out over many directories?

> Even worse this will often result in the checks portion of rclone getting stuck, literally forever, for some files. This means that when I want to sync 100,000-1,000,000 files or so to ACD I simply can't

I'm not sure why it gets stuck - is there anything in the log?

> I don't blame rclone, it's probably all ACD's fault, but I am wondering if anything could be done to help this out? perhaps rclone could save a ls from acd? allowing rclone some command to generate a full ls? allowing the user to run it a few times? to make sure they have a good ls? then start the sync using that locally stored ls? I don't know...

I've certainly considered this - see #180 

> I started just uploading large 7z files of my largest folders instead.
> 
> another sub issue here is thusly, if remote:E:\ contains let's say 10subfolders and those 10subfolders contain 1,000,000 files... and I want to upload a single file directly to "remote:E" instead of just confirming that file isn't one of the ten things in the root directory it will demand to check all 1,000,000 files (or well maybe not "check" but the ls command will be for all subdirectories)... this has resulted in me wanting to upload to "remote:incoming" then use ACD's shitty website to move the file back to "remote:E"...

That is a problem I've known about for a while #8.  I haven't thought of a satisfactory solution though - for syncing lots of files it is much faster to list the remote, but for syncing just a few it is faster not to - how can we tell which to use?  Maybe I could add in a command line flag `--no-traverse` or something like that.?

Note that sync always has to traverse the remote file system.

> ideally the ls rclone runs from ACD would only check subdirectories it needs to? this would speed things up a lot especially because ACD's response to ls is very buggy.
 I have seen the 409 issue in the integration tests too.  The integration tests used to pass perfectly every time with ACD, and now they are more like 50/50, with 409 error and complete stall, which sounds very much like the same symptoms.

I'm not 100% convinced this is an rclone issue rather than just ACD being overloaded, but we'll try to get to the bottom of the problem and fix it or work around.
 I agree 100% with you.  rclone isn't supposed to hang indefinitely - at most it is supposed to hack for `--timeout` so something is up there.

Hopefully we'll work something out for this soon!
 I'd like to fix the getting stuck issue.  Interestingly I still see it with the integration tests so I've got something to work on!

eg

```
cd fs
go test -v -remote TestAmazonCloudDrive:
```

shows the problem quite regularly.
 @isaiah36 that will certainly stop the sync completing - sounds like ACD is having a good day if that was the only problem!
 I've made a beta for you to try: http://pub.rclone.org/v1.26-34-gde3cf5e%CE%B2/ which should hopefully help.

Any feedback much appreciated.
 @left1000 it isn't quite released yet! Should be in a day or two.
 I made another beta here: http://pub.rclone.org/v1.26-38-g6cb0de4%CE%B2/
  There is a problem with downloading > 10GB files with ACD  #204 which I've worked out what is causing it, but I haven't had time to fix it yet.

This looks like it isn't related though.

What is the command line you are using?  Are you using `--bwlimit`?  If so #232 might be the cause.  It certainly looks similar though. 
 I have uploaded a new version of rclone a moment ago which definitely has the bandwidth limit fixes for amazon cloud drive.
 I made a thread on the amazon developer forum here: https://forums.developer.amazon.com/forums/thread.jspa?threadID=10979

A log of this happening with `--dump-headers` would be very useful.  Remove the `Authorization:` header before posting it publically.
 @santaclause88 if you can get me a log of it happening I can post it on the thread which might get some traction from amazon!

> A log of this happening with `--dump-headers` would be very useful. Remove the `Authorization:` header before posting it publically.
 @santaclause88 Thanks for that. I've given the forum thread some info from the log - lets hope Amazon take a look.
 Try Increasing --acd-upload-wait-time the default is 2m. It maybe needs to scale as the Suze of file. How big are the files you are uploading? 
 @felixbuenemann good idea.  I'll put that in then we can do a bit of testing. 
 @felixbuenemann I've fixed that already - try the latest beta: http://beta.rclone.org
 @felixbuenemann thanks - the fix was #712 BTW
 I've done a bit of analysis.  Firstly there is a 60 second timeout before we get the `408 GATEWAY_TIMEOUT` message.  You can see this because the stats show the files get to 100% 60 seconds before the error.

Taking that into account, the timing for various sized files look like this (I can only upload files up oto 20GB as that is the limit of my account - I put the 35 GB info from @arionl in too).

![acd-timeouts2](https://cloud.githubusercontent.com/assets/536803/19393938/d39c4538-922e-11e6-99b2-741b0957c6ca.png)

Which graph up like this

![acd-timeouts](https://cloud.githubusercontent.com/assets/536803/19393948/e0c81638-922e-11e6-96e2-309b21dd6ff4.png)

So what that means is that amazon take about 8.2 seconds per GB hashing the file == 125 MByte/s.  If that time gets to be greater than 60s then we get a `408 GATEWAY_TIMEOUT` error.

So a prediction of how long we need to wait after a `408 GATEWAY_TIMEOUT` error would be `FILE_SIZE_IN_GB * 8.2 - 60` seconds.

The outlier in the table would have needed 21.7 S/GB though, so to be sure of always waiting long enough we should wait (say) 30 seconds / GB - 60 seconds for the initial timeout.  So for a < 2GB file we wouldn't wait at all, for a 3GB file we would wait for 30s, for a 10GB file we would wait for 4mins30 and a 50 GB file we would wait for up to 24m30.
 Here is the data [in a google sheet](https://docs.google.com/spreadsheets/d/1Yd9mAdmxuOrWXeg91-porKYl0Cywy5-j_XNVN4jkVeQ/edit?usp=sharing) in case anyone wants to take a look
 @felixbuenemann yes the 1gbps figure occurred to me too.  I'll see if it is easy to calculate when we sent the last byte and use that if possible. 

@left1000 I have a total limit of 20gb non photo files on my account. Amazon have stopped doing that now - you can only get the unlimited version. 
 OK here is my latest attempt to fix this problem.  See below for changed docs.

http://pub.rclone.org/v1.33-66-g2e20a0f-acd-timeout%CE%B2/

Any feedback very welcome, with suggestions for the default parameter values.  In my tests I successfully uploaded a range of file sizes up to 15 GB.  The tests weren't successful before I introduced the extra 2m sleep.

#### --acd-upload-wait-time=TIME, --acd-upload-wait-per-gb=TIME, --acd-upload-wait-limit=TIME

Sometimes Amazon Drive gives an error when a file has been fully
uploaded but the file appears anyway after a little while.  This
happens sometimes for files over 1GB in size and nearly every time for
files bigger than 10GB. These parameters control the time rclone waits
for the file to appear.

If the upload took less than `--acd-upload-wait-limit` (default 60s),
then we go ahead an upload it again as that will be quicker.

We wait `--acd-upload-wait-time` (default 2m) for the file to appear,
with an additional `--acd-upload-wait-per-gb` (default 30s) per GB of
the uploaded file.

These values were determined empirically by observing lots of uploads
of big files for a range of file sizes.

Upload with the `-v` flag to see more info about what rclone is doing
in this situation.
 @arionl This issue caused the upload to fail most likely and rsync not to rename it to its final destination.  It didn't appear because of #680 most likely.  Note that acd doesn't support move yet so that is likely why it didn't get its final name from rsync #721.
 I've merged this to master here: http://beta.rclone.org/v1.33-76-g5b83270/ (will be uploaded in 15-30 mins)

Let me know if you find any problems and I'll re-open the ticket if you do!
 @felixbuenemann  I attempted to share [the doc](https://docs.google.com/spreadsheets/d/18wRo2JLojmXcuUUcbyco8RI5LEISNhNyOQI3QOPc6Ec/edit?usp=sharing) with you so you can edit it too. I've added your data to mine, and drawn a coulple of lines on the graphs with the current scheme and a proposed new scheme - feel free to adjust those settings.  It also shows the percentage hit rates of the data points we've got so far.

We could do with some more data with the bigger files.

The defaults currently stand at

```
    --acd-upload-wait-limit 60s
    --acd-upload-wait-time 120s
    --acd-upload-wait-per-gb 30s
```

Changing it to `--acd-upload-wait-per-gb 180s` would mean that a 50 GB upload (the max) might wait for 2.5 hours for it to appear.  Maybe that is OK though since a 50 GB upload likely took several hours to upload...

I'll all ears to your suggestions!
 @felixbuenemann interesting work!  I'd like to release the long delayed v1.34 pretty much as-is by the weekend so if you've got any thoughts on tweaking the existing timers for that, I'd be interested.  I suspect reverse engineering the resume might take a bit longer so let's do that in v1.35.
 Thanks for your analysis @felixbuenemann 

As a result I've drastically simplified the waiting into a single parameter 

```
--acd-upload-wait-per-gb duration   Additional time per GB to wait after a failed complete upload to see if it appears. (default 3m0s)
```

Which is much more satisfactory.

See commit e162377ca36b5da680876ccb361892c3586a56ab

Can you have a go with the beta and report back please?  I'd like to make the v1.34 release tomorrow if possible!

http://beta.rclone.org/v1.33-106-ge162377/ (uploaded in 15-30 mins)
 @felixbuenemann no worries.  I'm pretty sure the feature is working correctly so I've released it in v1.34 - if there is a problem then I'll fix it and make a new release.
  Hmm looks like it might be related to @klauspost windows file name unification? Klaus?
 @garypaduana - This is likely a Dokan bug. It is old and unmaintained, just as encfs4win, if you are using [this version](http://members.ferrara.linux.it/freddy77/encfs.html).

Have you tried some of the more recent EncFS software like this: http://encfsmp.sourceforge.net ?
 Does it work, if you clone from a subdirectory?

`\\?\R:\` does seem a bit strange. It could be a general problem with cloning from the drive root.
 It must be programmatically accessible using a [UNC path](https://msdn.microsoft.com/en-us/library/windows/desktop/aa365247%28v=vs.85%29.aspx#maxpath).

All paths are converted to UNC paths, otherwise we get into trouble with paths longer than 255 bytes.

I will look at the EncfsMP root problem tomorrow.
 Really the file system drivers should be fixed. 

However, since that probably isn't something you can do, I have added an option to disable UNC path conversion. See PR #271.
 @garypaduana - I have attached a build with the nounc option for you to test. I only have EncFSMP, 

[rclone-nounc-test.zip](https://github.com/ncw/rclone/files/78203/rclone-nounc-test.zip)

Documentation is here: https://github.com/klauspost/rclone/blob/add-nounc-option/docs/content/local.md#long-paths-on-windows
 The colon is added by the error printer, so it is not part of the requested path.

It seems like this check in Go [file_windows.go](https://github.com/golang/go/blob/master/src/os/file_windows.go#L204) is failing. That is the error returned. My guess is that Dokan/EncFS is returning the wrong information. I will create a test binary for you.
 Here is a small test to run, that might give us something more to work with: [test.zip](https://github.com/ncw/rclone/files/79771/test.zip)

I have put the source code [here](https://gist.github.com/klauspost/5f87caf402a8abf369d5) if you would rather compile it yourself.

Please run it, and paste the output here, then hopefully we have something to work with.
 ok, it is somewhere in the [directory reading code](https://github.com/golang/go/blob/master/src/os/file_windows.go#L92), most likely Dokan returning a wrong value somewhere. 

The only way I can see this triggering is if [opening the directory as a file](https://github.com/golang/go/blob/master/src/os/file_windows.go#L84) does NOT return an error. 

Go first tries to open a file as a plain file, if that fails, it tries as a directory before failing. It seems like Dokan/encfs reports "success" when opening `z:\` as a file, which it shouldn't, since it isn't a file.

There isn't much we can do from here. You can try filing a bug for Go, but I doubt they will include a workaround for (what I expect to be) a buggy filesystem driver. It may be fixed in DokanY, but I don't know if encfs4win has been compiled for DokanY.

Another workaround that could work, could be to share the drive, and access it as `\\localmachine\share`, though that is unlikely to make any difference.
  Which OS are you using?

It looks like that DNS resolution isn't working under cron.

Does it happen every time, or does it work sometimes?

Do you have any name resolver environment variables set normally, eg `RES_OPTIONS` - these won't be set under cron.
 Did you get anywhere looking at this?
 Did you manage to make this work?

You might get more help posting to a fedora forum - I don't think it is an rclone problem specifically.
 I think this isn't an rclone bug so I'm going to close this ticket.

I'm sorry I haven't been able to help you work out what is going on - you might get more help from a general Fedora / linux forum.

Please re-open if you do discover it is an rclone bug.

Thanks

Nick
 @mejje good suggestion.  The next release of rclone v1.30 will be compiled like that by default, so it might be worth retrying with that.
  Either download the binary at http://rclone.org/downloads/ or if you have a go install

```
go get -u -v -f github.com/ncw/rclone
```

Will produce an updated binary in `$GOPATH/bin`

I hope that helps!
 Glad that helped
  I uploaded a file called  `©.txt` and viewed it in the web interface just fine.  I also downloaded it with rclone.

Can you send me a log of it going wrong? Of make me an example that I can reproduce?

Thanks

Nick

```
$ mkdir /tmp/z
$ echo "Hello" >/tmp/z/©.txt
$ cat /tmp/z/©.txt 
Hello

$ rclone -v copy /tmp/z acd:test-copyright-symbol
2015/12/23 11:03:13 acd: Saving new token in config file
2015/12/23 11:03:14 test-copyright-symbol: Failed to read info: Node not found
2015/12/23 11:03:14 Amazon cloud drive root 'test-copyright-symbol': Modify window not supported
2015/12/23 11:03:15 Amazon cloud drive root 'test-copyright-symbol': Building file list
2015/12/23 11:03:15 Amazon cloud drive root 'test-copyright-symbol': Finished reading 
2015/12/23 11:03:15 Amazon cloud drive root 'test-copyright-symbol': Waiting for checks to finish
2015/12/23 11:03:15 Amazon cloud drive root 'test-copyright-symbol': Waiting for transfers to finish
2015/12/23 11:03:16 ©.txt: Copied (new)

Transferred:            6 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:        3.2s

2015/12/23 11:03:16 Go routines at exit 16


$ rclone -v copy /tmp/z acd:test-copyright-symbol
2015/12/23 11:03:32 Amazon cloud drive root 'test-copyright-symbol': Modify window not supported
2015/12/23 11:03:32 Amazon cloud drive root 'test-copyright-symbol': Building file list
2015/12/23 11:03:32 Amazon cloud drive root 'test-copyright-symbol': Finished reading 
2015/12/23 11:03:32 Amazon cloud drive root 'test-copyright-symbol': Waiting for checks to finish
2015/12/23 11:03:32 ©.txt: Sizes identical
2015/12/23 11:03:32 ©.txt: Unchanged skipping
2015/12/23 11:03:32 Amazon cloud drive root 'test-copyright-symbol': Waiting for transfers to finish

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 1
Transferred:            0
Elapsed time:       600ms

2015/12/23 11:03:32 Go routines at exit 13

$ rclone -v copy acd:test-copyright-symbol /tmp/zz
2015/12/23 11:05:11 Amazon cloud drive root 'test-copyright-symbol': Modify window not supported
2015/12/23 11:05:11 Local file system at /tmp/zz: Building file list
2015/12/23 11:05:11 Local file system at /tmp/zz: Waiting for checks to finish
2015/12/23 11:05:11 Amazon cloud drive root 'test-copyright-symbol': Finished reading 
2015/12/23 11:05:11 Local file system at /tmp/zz: Waiting for transfers to finish
2015/12/23 11:05:11 ©.txt: Copied (new)

Transferred:            6 Bytes (   0.01 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:          1s

2015/12/23 11:05:11 Go routines at exit 15


$ cat /tmp/zz/©.txt 
Hello
```
 Ah, I suspect your file system is storing file names in latin1, not UTF-8.  rclone expects the local filing system to be in UTF-8 which all modern filing systems are.

You can see that because your terminal is set to UTF-8 but when you run `ls` you get `?` in your file names instead of the accented characters.

I did a bit of searching - you might find this helpful: http://serverfault.com/questions/54911/debian-how-to-convert-filesystem-from-iso-8859-1-into-utf-8

If you do convert your filing system to UTF-8 then rclone should work fine!
 You should probably check out [the local file system docs](http://rclone.org/local/) too which has a section on encoding.
 Great - that is what I would have done :-) 
  Sorry missed this issue when it came in!

What do you think about adding a flag for this?  `--drive-authenticated-only` or something like that?

Fancy having a go at a patch?
 Sounds good!
 I'd say just make it for drive with a flag for the first attempt - I'd like to get the feature working first, before thinking about generalizing it.
 This fix for this is merged - thank you very much!  It will be in the next release.
  It only does the consumer one at the moment.

Unfortunately I don't have a onedrive for business account, but it should be relatively straight forward to make it work.

Do you have a onedrive for business account?  Would you be willing to do some testing?
 Thanks.  I'll let you know!
 @skelluk an account I could use would help enormously (I only have the free onedrive account!).  Email me offline at nick@craig-wood.com - Thanks
 @rafareino a onedrive for business account I could use would  be really helpful!  The source code for rclone is all on this site - it is a fully open source project.
 I really need someone who speaks Microsoft to own this integration.  By that I mean work out exactly how to get rclone registered as a Onedrive for business app.

This is the process: https://dev.onedrive.com/app-registration.htm#register-your-app-for-onedrive-for-business - note the first sentence `To register your app to work with OneDrive for Business, things are more complex` - they weren't kidding!

Which will involve having some relevant Microsoft subscriptions (I have none!) not a free trial which will expire.

I'm happy to provide advice about the oauth bits of it, but there are so many Microsoft bits of it that I really don't understand (not being a Microsoft developer).

Once someone has
- registered the app
- provided me with a Onedrive for Business test account - one I can use in perpetuity for running the rclone integration tests

I'm willing to do the actual integration which I think will probably only take a couple of hours!
 @philthynz 1 year would be fine - does that program give enough access rights to do what I need to do with rclone do you think?  Would it give me me access to a onedrive for business account + rights to make onedrive for business app?  If it does it seems ideal!
 @timoc very useful thank you Microsoft's offerings are very confusing.  I have a onedrive for education that I logon through Office 365.  Is that the same as a onedrive for business?  Or is it yet different again?     Please stop +1.  If you want to express a +1 on GitHub, mark the top post with a 👍 otherwise you are spamming everyone in the thread!  > Failed to copy: unauthenticated: Must be authenticated to use '/drive' syntax

I think this must be a Onedrive error.  I've noticed that the onedrive API is a bit flakey at times.

> Failed to copy: unexpected EOF

I tried copying some large files and it seemed to work ok.

Can you send me a log with `rclone -v` of this happening please?  I suspect it might be your service provider throttling the connections - I've seen very similar things on my home internet connection.

> specifically the "Above this size files will be chunked" statement.

I think that is a mistake!  I'll correct the docs to something like

```
–onedrive-chunk-size=SIZE

The chunk size for large uploads - must be multiple of 320k. The default is 10MB. Note that the chunks will be buffered into memory.
```
 Thanks for that.  If you copy one of the failing files to your hard disk first, then copy it to onedrive does it work?

I tried copying files form dropbox to onedrive like you are doing but I couldn't reproduce the problem.  You could try increasing the timeouts `--contimeout` and `--timeout` but I'm not hopeful that will help.

I suspect throttling by your ISP - have you tried to see if it works at a different time of day?
  rsync has these options

```
            --delete-before         receiver deletes before xfer, not during
            --delete-during         receiver deletes during the transfer
            --delete-delay          find deletions during, delete after
            --delete-after          receiver deletes after transfer, not during
```

rclone currently implements `--delete-after`.  I think you are asking for `--delete-before` - does that sound correct?
  If you look at the main rclone program you'll see it just uses the rest of the code as a library.

https://github.com/ncw/rclone/blob/master/rclone.go

You specify which remotes you want compiled in by the includes.

The interface for the local an s3 are identical and defined in https://github.com/ncw/rclone/blob/master/fs/fs.go

Those interfaces have been very stable, but I don't guarantee I won't change them!

If I was making some code to run with rclone internals then I'd probably copy `rclone.go` and start from there.  Using the rclone config file and concept of remotes will simplify your life (as in you configure stuff with rclone).
  ...Until #49 is fixed
 #49 is now fixed - you can copy google docs, so I'm closing this
  A nice idea.  Could use this library: https://github.com/fsnotify/fsnotify

Maybe a user interface I'd choose is something like this

    rclone fsnotify sync/copy/move /source/path remote:test

What this would do is run the sync/copy/move then wait for an fsnotify event and sync/copy/move the files that changed.

Maybe users would not want the initial sync/copy/move - not sure. +1
 +1
 @dibu28 are you interested in working on this?
 @timofonic: Please stop adding "bump" messages. This is a new feature, and there are a lot of those.

This is not as simple as adding two lines of code, this is complex and will require a lot of work to function properly. I have done file monitoring, and there are a _lot_ of cases you need to consider. This is also why a lot of the current applications offering this feature break all the time. 

This is a free time project, and @ncw (and the rest of us) are free to chose what we work on. In my personal opinion there are a lot of more important features to work on. Look at the amount of current "support" issues - if we add another "fragile" feature that will not be helped.
 @rhummelmose very nice.  It is about time I made a third party tools page which I could link that from... I made an issue to remind myself to do it #1019  Glad you got it sorted out.  Would a bit in the docs help here do you think?
 First time I've tried Hubic backend it was not obvious that I have to use "default" container to access files in hubic. And It is not mentioned in rclone docs. I've only found it in hubic api docs.
 @rlapray fancy suggesting an update to the hubic docs?
     rclone mkdir hubic:bucket

Should work when ovh fix hubic: http://status.ovh.co.uk/?do=details&id=12305  I suspect you have a duplicated file.  I've had a separate report of their being duplicates in "Google Photos".  Can you have a check in the web interface and see if there are two copies of that file?

Thanks

Nick
 There has been a persistent problem with rclone uploading duplicates which I haven't fixed yet #28 However it looks like Google photos has the same problem when it uploads photos from a phone. This is kind of reassuring that Google have the same problem with drive too! 

As for downloading duplicates with a different name - that is possible. What I'd have to do is detect them in the drive directory listing code and alter the external name there. One would want a consistent mapping so appending the internal file id might be necessary. That might not work out though if rclone uploads a duplicate - it will keep uploading  the same file. 

I'll have a think. It might be that a deduplicate option for rclone is easiest all round. What would you think of that? It would manually ask you about the duplicates and could offer to rename them or delete all but one. 
 Certainly rclone should warn the user about duplicates better that the above message though! 
 I've done the `rclone dedupe` command which should hep with this.

It isn't perfect but it will get better!

I'm going to close this ticket
 @Darkvater if you are still having the problem, can you open a new ticket with a log (with `-v`) - thanks Nick
  You are right - this has changed in recent-ish rclones to make the fully automatic process the default.

Which remote are you using?

With (for example) drive you can use the more manual process like this

```
Use auto config?
 * Say Y if not sure
 * Say N if you are working on a remote or headless machine or Y didn't work
y) Yes
n) No
y/n> N
If your browser doesn't open automatically go to the following link: https://accounts.google.com/o/oauth2/auth?client_id=20226481564...
```

That isn't possible with all the remotes though.

Failing that you can copy the config file - see [the second question in the FAQ](http://rclone.org/faq/) for details
 With Onedrive there isn't a manual fallback for the auth (Microsoft don't support it) do you'll have to use the copy the config from another machine technique in the FAQ.
 You will have to copy the config with OneDrive
 Hopefully that got it working for you - please re-open if not!

Thanks

Nick
  Thanks for the update; glad you got it working. You are right I don't keep those numbers up to date - perhaps I should. 
  Yes it is the expected behaviour by me, but one I'd like to fix. Subscribe to the issue if you want updates on it. 

Cheers 

Nick 
  Any chance you could send me a log of the problem with `rclone -v`?

Which rclone version are you using?

Which OS are you running rclone from?

The `500 The server as encountered an error please try again later` error unfortunately is not uncommon with drive.  It may be the cause of issue #28 which may be affecting you and is still outstanding.
 I should also have said, rclone is designed so that if you get an error in the sync, and you will undoubtledly get lots syncling 1,000,000 files, then just retry the sync and rclone will tidy up what next.  You can  use the `--retry` flag to help also.
 Did you have any luck getting a log of the problem?

Did retrying the sync work properly?

Thanks

Nick
  Can you send the output of rclone with `-v` please? Edit it if necessary to preserve privacy. Also rclone version, which OS you are using and which remote? Thanks
 I don't think there are any files under the dsink directory (that is what it looks like from the tree output) are there? 

Rclone only copies files - it will make the directories if there are files in them. That means it won't copy empty directories or delete directories when empty. 

If you look at issue #100 you'll see thoughts on improving thus aspect of rclone. 
 No problems!  Feel free to make more issues if you have more problems.

Nick
  rclone asks for Full dropbox permissions.

Note that it is only the rclone app you have on your computer that has these permissions - neither I nor rclone.org get to see or know anything about it.

You could create your own App on Dropbox and use its App key and secret and that should work for an app with only "App Folder" permissions.  I haven't tried it though so it might go wrong in unexpected ways!  If it does then file an issue :-)
 Great! That is good to know - thanks for reporting back. Nick
  It looks like you have duplicate files with the same name (see the output of the check command). If you delete the duplicates (with the drive Web interface) then it should work properly after that. 

As to what caused the duplicates? This is a known issue #28 which I haven't tracked down yet. 
 Thanks for doing the investigation. I didn't know you could get a "Google Photos" folder in your drive - thanks for the heads up, that is very useful!

Interesting to see that even google have problems with duplicate files in drive - glad I'm not the only one!

There is an issue about removing duplicates #41 which I'd like to do at some point too.
  By fragmented files, you mean files bigger than 5GB I take it? Or were these uploaded by a different tool?

I shouldn't do that!

Can you report
- rclone version
- OS version
- command line you are using
- output of that command with `-v` flag (edit the output to remove anything confidential)

Thanks

Nick
 Thanks for that.

Hmmm `A_FILE.ext: Sizes differ` is odd...

What does `rclone ls` say the size of  `A_FILE.ext` is and what is its size when you do `ls -l` on it?
 Yes that looks like a bug (or return of a fixed bug!)

I'll fix it in a bit and sent a beta for you to try
 I've fixed that and uploaded a beta for you to try - let me know how it goes!

http://pub.rclone.org/v1.25-10-g646143d%CE%B2/

Thanks

Nick
 Great!  It will be in the next release.
  I haven't spend much time optimizing memory usage in rclone recently. For example the amazon cloud drive `Object`s are fatter than they need to be - making them smaller would reduce memory usage a lot.

> Does rclone really needs to keep the entire directory tree in memory?

The way the internal interfaces are implemented at the moment it does.  Not all the cloud storage systems have the concept of directory and can they can return files in any order.

You'll see if you look at the code for sync (`fs/operations.go` `syncCopyMove`)  a `FIXME` which could help if implemented.

Probably what rclone needs is a bit of time with go's memory profiler!  I should add a `--memprofile` flag so anyone can do this.

You could work around this by using `lsd` to see the top level directories, then syncing each one of these individually - that would help probably.
 Memory usage in rclone could do with a round of optimization and checking each of the remotes doesn't use an excess of memory.
 @lcarstensen - that does indeed sound like a leak, or excessive buffering. If I copy 154k files local->ACD, memory never gets above 300MB.
 @lcarstensen I've added a `--memprofile` flag for you - you can find a binary with it here

http://pub.rclone.org/v1.26-15-g2646519%CE%B2/

To use it, first add `--memprofile mem.prof` to the command line of your command.

You'll need go installed to analyse it, which you do with

```
go tool pprof /path/to/rclone mem.prof
```

The two most useful commands are `top20` and `web` - they should give you a good idea what is going on.

If you'd like me to analyse it then you'll need to send me the `mem.prof` file and tell me exactly which rclone binary you used.
 How many files are you syncing? At the moment rclone loads each file name into memory first so more files means more memory. 

Which remote are you using? They use differing amounts of memory. 
 You can sync sub directories as a workaround.

I'll l fix the memory usage eventually though! 
 @nodughere 6k per object sounds way too big!

Below you'll find an rclone with a new command `memtest`

http://pub.rclone.org/v1.30-4-g12342eb%CE%B2/

This is what I get running it against a swift container with 60k files in.

```
rclone memtest swift:container
2016/06/21 16:35:05 60182 objects took 26790336 bytes, 445.2 bytes/object
2016/06/21 16:35:05 System memory changes from 7969016 to 45504760 bytes a change of 37535744 bytes
```

This loads all the info about the remote objects into memory in the same way as it does doing a sync.

Can you try that on your 2.5 million files container and report back what it says please?

If you could also try it on a local copy of all files (if available) that would be interesting too.  So

```
rclone memtest swift:container
rclone memtest /path/to/copy/of/files
```

Thanks
 I'm currently implementing #517 which will likely fix this issue too. Here is a beta for a low memory sync which I think should fix this problem.

tThere is a temporary flag `--old-sync-method` to select the old sync method.

It should work identically to current rclone, except using a lot less memory.

http://pub.rclone.org/v1.35-50-g9d88ce3-low-mem-sync%CE%B2/

It is a major re-work of the syncing internals.  All the unit tests pass, and the code is well covered.  My own smoke tests pass too.

Test with care - use --dry-run first!

I'm particularly interested in 
  * any bugs
  * memory usage differences before vs after - especially on big syncs
  * speed differences before vs after

You can run this little ditty in another window to see rclone memory usage

    while [ true ]; do ps -eo rss,args --sort rss | grep [r]clone ; sleep 0.5 ; done @fredericofs 
> In my case, it is only 1 folder (no sub-dirs) with 30gb of ~3mb files.
> Should I expect some improvement too?

A bit maybe!  I wouldn't have thought only 10,000 files would use much memory though... @kmanley 
Here http://pub.rclone.org/v1.35-68-g5eb5e5f-low-mem-sync%CE%B2/ is a beta compiled with go 1.8 rc3 which should work on the WD cloud. I have merged this to master now - see here for a beta:

http://beta.rclone.org/v1.35-74-g48cdedc/

Any feedback much appreciated
 @kmanley the fix for the page size isn't in the stable go compiler yet - I'm hoping it will be in time for the rclone 1.36 release.

In the mean time try this which I compiled with  `go1.8rc3`

http://pub.rclone.org/v1.35-75-g381b845%CE%B2/
  Looks like your second and subsequent attempts did load the filter file, but we're just  missing a command, eg copy or ls. 

`~` is interpreted by the shell not rclone and by the look of it osx's shell doesn't like the tilde being inside a string. 
 No worries - glad it is working now! Nick
  That is fantastic news - thanks!

I suggest you put a branch up somewhere and I'll comment on it.

Note that `empty folders not created/removed yet` is a general problem with rclone - see #100

> not everything is covered with unit tests

Hopefully you've worked out that running `go generate` in `fstest/fstests` will generate an integration test.  This needs a remote set up called something like "TestYandex" (depends on exactly what you called the Fs). If your remote can pass this, and the integrations tests in `fs` - run `go test -v -remote TestYandex:` you are doing very well!

I'll register an oauth for rclone and send you the details.

Look forwards to seeing the code :-)

Nick
 Thank you.
I will prepare the code and let you know.

Is it better to put code and api code into a separate project like "github.com/dibu28/rclone_yandex" and just import it in rclone.go or put it all into the folder called 'yandex' inside rclone folder and import ./yandex in rclone.go
 Please review the code at: https://github.com/dibu28/rclone/tree/rclone_yandex

Compiled and tested rclone commands (copy, sync, move, ls, lsl, md5) they are working.

Some functions still need to be implemented.
Please tell me if something should be fixed, implemented?
I also have some questions about rclone objects.
 Thanks for that! I'll send you some feedback tomorrow.

> Is it better to put code and api code into a separate project like "github.com/dibu28/rclone_yandex" and just import it in rclone.go or put it all into the folder called 'yandex' inside rclone folder and import ./yandex in rclone.go

If you think the API stuff is re-usable elsewhere (the `yandex_disk_api`) then you might want separate that on your own github.  The rest of the stuff (and the api if you decide) should go in a yandex folder and the import in rclone.go is the right way to go.

>  also have some questions about rclone objects.

You can ask here, or send me email to nick@craig-wood.com whichever you prefer!
 I've used credentials that you've sent to me. There is a little problem with call back url:
Then I try to get a token I get an error: "400 redirect_uri does not match the Callback URL defined for the client"
This is because you've set callback url to: http://127.0.0.1:53682/ in yandex control panel but rclone(oauthutil.go) sends http://localhost:53682/ url in request to oAuth.
You can change callback url in Yandex oAuth control panel to http://localhost:53682/
 Instead of using `RedirectLocalhostURL` use `RedirectURL` that has the http://127.0.0.1:53682/ address
 > If you think the API stuff is re-usable elsewhere (the yandex_disk_api) then you might want separate that on your own github. The rest of the stuff (and the api if you decide) should go in a yandex folder and the import in rclone.go is the right way to go.

I don't think it is re-usable now. And I've only implemented parts needed for rclone. I think would be better for now to leave it in the yandex_disk_api sub folder.

> You can ask here, or send me email to nick@craig-wood.com whichever you prefer!

I've sent you my questions
 > Instead of using `RedirectLocalhostURL` use `RedirectURL` that has the http://127.0.0.1:53682/ address

Done. Now using rclone credentials.
 Tested. Upload some files using new token.

oAuth page for rclone looks like this:
![ya_access](https://cloud.githubusercontent.com/assets/446262/11533654/0bb29678-9935-11e5-8c3e-beec3f0ad447.png)
 I've fixed all bugs in yandex rclone backend at
https://github.com/dibu28/yandex

It is now passes all unit tests and all tests with "go vet", "golint", "errcheck" and "go fmt".

Please, can you review the code. And if every thing is OK can we merge it to rclone?
 I will merge this shortly!
  I'm sorry you are finding the process painful!

The webserver only binds to 127.0.0.1 because that is what I told amazon when setting up oauth for rclone.  I can't supply different addresses for the redirect URL unfortunately on a case by case basis.

I could show the actual URL instead of the rclone internal one - that would work fine for google drive (for example) which doesn't require the last redirect, but wouldn't work for amazon cloud drive which does require that last redirect.

What I normally suggest people do with NAS is configure it on their desktop machines and copy the config file - see the second question in the [FAQ](http://rclone.org/faq/).

I did manage to make it work with `w3m` a slightly more modern text based browser than `lynx` though.

Unfortunately the oauth2 protocol was really designed to be used in a web browser :-(
 same, created .rclone.conf once on my desktop and copied to as many vps/desktops as needed. 
 I'm going to close this as I think that copying the config file should be the best way to fix this.

Thanks for the report

Nick
 Since this is likely to pop up frequently, even if it is added as FAQ, I may look into making it slightly more smooth.
  I recently fixed #170 which may be at the root of this.

I've uploaded a beta release - can you try with this and see if the problem persists?

http://pub.rclone.org/v1.25-8-g8cab32c%CE%B2/

Thanks

Nick
 Thanks for checking that out.

The fix will be in the next release.
 That is now fixed!
  @AlexisEvo That could well be it.

@dmitrym0 the other possibility is that it is some problem with eventual consistency that retrying the sync will fix - it certainly isn't expected behaviour.
 Glad you got to the bottom of it and thanks for the update. Nick
  I think this a nice idea.

Unfortunately not all the cloud storage systems can stream data - they need to know the size in advance.

I think it would need a different internal interface probably, as the current upload interface needs the size.
 @harrytuttle I just implemented an rclone cat command actually!  It will be in v1.33.

I think I need to solve the unknown size problem to make FUSE work properly so this ticket will probably be possible then (piping stuff to rclone for upload).
 @RaymondSchnyder You'll want #1001 which is the `rcat` command coming in 1.37 with any luck! Rcat isn't done yet - soon hopefully!  I just read the man page for rsync again and it has lots of control here - any bits you'd suggest that rclone should incorporate?
 This is a nice idea for an improvement.
 This is now done in the latest beta!  Is this normal practice for unix commands?  Don't debugs & errors normally go to the same place.

Can you think of an example program where they don't?

Potentially this would interact with the `--log-file` option so some care would be needed there.

I see how it would be useful to you though.

Potentially this could be a flag `--log-debug-to-stdout` or something like that?
 Does someone fancy having a go at this?  It should be relatively straight forward...  in `fs.go` adjusting the `Log`, `Debug` and `ErrorLog` functions.  Might need to make a new go logger which logs to stdout.

I'd probably prefer to have the output of `Log` and `ErrorLog` go to stderr, but the output of `Debug` (which is what you get when you pass the `-v` flag) go to stdout.

`-q` suppresses the `Log` function.

The final stats print needs a bit of working out what flags that should appear with (probably not `-q`!).

> We use it with out monitoring system to check whether a nightly backup had run ok.

rclone should reliably return an exit code for monitoring whether it succeeded or failed.
 I've now made debug output go to standard output and important stuff to stderr.

I also fixed all the other things which should have been obeying that (eg the transferred log).

See this beta for testing http://pub.rclone.org/v1.29-1-gbb75d80-53-gf17cb1b%CE%B2/

@kpabba you should find `rclone -v ... >debug-logfile 2>importatnt-errors` does what you want
 I view the status updates as important info that rclone hasn't died.  You can change their frequency or disable them with the `--stats` flag

```
  --stats duration                 Interval to print stats (0 to disable) (default 1m0s)
```
  This is expected behaviour - I agree it is counter-intuitive.

I really need to rethink the way rclone handles files - at the moment it handles files as a directory with everything except that one file excluded.

I shall ponder!
 Now that rclone is using cobra it is really easy to make another command copyfile

This could be used like

```
rclone copyfile /path/to/file remote:dir/newfilename
```

I'd make movefile too for renaming files. 

What would you think you of that? 
 PS note you could improve your script by doing

```
ln  real_file_name desired_file_name
```

Instead of cp
 I'm going to make two new commands `cp` and `mv` which work exactly like their unix counterparts.

I've made a prototype but I'm running out of time for this release so I'm going to bump it into the next.
 I changed my mind after some effort trying to implement the semantics of `cp` and `mv` which really don't fit a system that needs lots of retries (they aren't idempotent).

So I implemented `moveto` and `copyto` as originally planned.

Find a beta here http://beta.rclone.org/v1.34-43-gc265f45/ (will be uploaded in 15-30 mins).  Unfortunately the creation date can't be set, so it will be the time the file was uploaded.  rclone checks for timestamps being equal in the sync so I don't think this will work as it stands.  You can see the times rclone has if you do an `rclone lsl` - these will show the created date on acd.

I think you might be asking could rclone use the acd creation date and check if it is older than the current modified date on the file?  That isn't the way it works at the moment - it is a pure equality check.

There is a precedent for this in rsync which has this flag and it would probably be a reasonably easy to implement.

```
    -u, --update                skip files that are newer on the receiver
```

That could then ignore the Precision of the ModTime too.

I wouldn't want to do this by default though, but having it controlled by a flag seems like a reasonable idea.

1 week of md5s is a lot though!  We've had some ideas about caching the md5 calculation in #157 which would help.

However you might find the `--size-only` flag is good enough for what you need.

```
  --size-only[=false]: Skip based on size only, not mod-time or checksum
```
 I think `--newer` is probably the best we can do, but I think that would work for you just fine provided the clock of your source machine is correct.
 I've implemented `-u` / `--update` in 280ac26464417033224b97f74f6f92f932ddcb1d.  You can find a beta of the feature here

http://pub.rclone.org/v1.27-51-g280ac26%CE%B2/

Thanks for suggesting it

Nick
  Upload rclone releases to github too so there is redundancy.
 I've uploaded all the historical releases to github too.

Note that if you want to find the latest (for example) linux amd64 release on github, you can get the URL like this.

```
curl -s https://api.github.com/repos/ncw/rclone/releases | grep browser_download_url | grep linux-amd64 | head -n 1 | cut -d '"' -f 4
```

Which prints

```
https://github.com/ncw/rclone/releases/download/v1.25/rclone-v1.25-linux-amd64.zip
```
  Good idea! I've been  on the waitlist for some time now - just waiting for my invite.
 @durval-menezes I don't think there is a public API for that is there?
 I have made a prototype for a b2 backend.  There are several problems with it though, the major one being that to upload files we need to know the sha1 in advance.  I've worked around this by copying the data into a tempfile but it isn't optimal.

I'm having a problem uploading certain file names too for reasons I haven't figured out yet - I suspect a bug in B2...

When it working slightly better I'll put a beta out for you to try.
 The beta is ready! Should be fully functional.  Let me know!

http://pub.rclone.org/v1.25-20-g42c3005%CE%B2/
 I just remembered I forgot to put the directory listing paging in, so it will be limited to 1000 objects in a bucket - fix coming soon!
 @cbess you can download a binary from the link above http://pub.rclone.org/v1.25-20-g42c3005%CE%B2/ - I'll merge to master after a bit of feedback so please test!
 Here is a new beta with the 1000 file limit fixed: http://pub.rclone.org/v1.25-21-gfd120b3%CE%B2/
 @cbess Thanks for testing. I don't know if you saw the CONTRIBUTING.md file I put up today which has a section on how to write a new backend.  Not a lot of stuff on auth though!
 @Sunako I did yes via twitter and their support system, but I haven't heard anything back from them.

We have a plan to avoid the tempfile (see discussions in #282). This also requires the internal hashing changes in #292.

I still think Backblaze should change the interface though - it makes it a lot less useful as a cloud storage system than all the other ones I've implemented as you can't stream to it.
 @Sunako thanks!
  Thanks for that.  The stacktrace is nice and clear - thanks!

It looks like resp was nil as returned from `t.wrapped.RoundTrip` in [fs/loghttp.go](https://github.com/ncw/rclone/blob/master/fs/loghttp.go#L50)

It should probably be only dumping the response `err == nil`

``` go
    resp, err = t.wrapped.RoundTrip(req)
    buf, _ = httputil.DumpResponse(resp, t.logBody)
```
  Because the bwlimit limits the `io.Reader` when reading into a buffer it only limits the speed of reading into the buffer, not the actual upload speed.

This is a problem in particular with S3/Drive and multipart files. 

It is correct in the long term, but is burstier than it should be.

Could potentially fix by putting the limiter into the transport, or lower down the stack somehow.
 `B2` is also broken AFAICT, since it copies all files to temporary storage due to sha1 calculation.
 There is now a method for fixing this in in 037a000 using fs.AccountByPart.  The b2 case is now fixed but the drive and s3 need fixing
  Hi @Raynok  - you wrote me an email about this didnt' you which got lost in my inbox - sorry! Good choice making a ticket ;-)

What do you think about a [command line interface a bit like this](https://code.google.com/p/bwlimit/wiki/bwlimit)?
 That syntax `-bwlimit=100 7-21,50` won't work at the moment, I just wondered what you thought about that syntax?

Namely having a 1 hour granularity and setting it like that.

If we think it is a good idea then we can implement something
 @marcopaganini Does that mean you set `--bwlimit` as well as `--bwtimes`? I quite like there being only one command line, so as you suggested, but with the default bwlimit first, so something like `--bwlmit ="500k 23:00,off 05:00,3M 08:00,500k 20:00,3M"` (the `SizeSuffix` parser will parse the `3M` or `off` for you).
 @marcopaganini I guess the default limit is what you get until you get to one of the trigger times when the bandwidth limit changes. If you don't have that then you'll have to work out what the current limit should be at any given time.  You'll probably have to work that out anyway though - I imagine a go routine which runs every minute which resets the bwlimit to the limit appropriate to that time.

I think having two flags for setting the bwlimit may confuse people, but I'm happy to go with your suggestion for the exact syntax.
 @marcopaganini I mis-understood your original plan - I thought it was an ordered list of instructions of time:bwlimit, but I see now that it is a timetable and yes I agree with you now :-)
 @marcopaganini are you still working on this? Here is a beta with this new feature in: http://beta.rclone.org/v1.35-10-g3b0f944/ (uploaded in 15-30 mins)

Here are the new docs

### --bwlimit=BANDWIDTH_SPEC ###

This option controls the bandwidth limit. Limits can be specified
in two ways: As a single limit, or as a timetable.

Single limits last for the duration of the session. To use a single limit,
specify the desired bandwidth in kBytes/s, or use a suffix b|k|M|G.  The
default is `0` which means to not limit bandwidth.

For example to limit bandwidth usage to 10 MBytes/s use `--bwlimit 10M`

It is also possible to specify a "timetable" of limits, which will cause
certain limits to be applied at certain times. To specify a timetable, format your
entries as "HH:MM,BANDWIDTH HH:MM,BANDWITH...".

An example of a typical timetable to avoid link saturation during daytime
working hours could be:

`--bwlimit "08:00,512 12:00,10M 13:00,512 18:00,30M 23:00,off"`

In this example, the transfer bandwidth will be set to 512kBytes/sec at 8am.
At noon, it will raise to 10Mbytes/s, and drop back to 512kBytes/sec at 1pm.
At 6pm, the bandwidth limit will be set to 30MBytes/s, and at 11pm it will be
completely disabled (full speed). Anything between 11pm and 8am will remain
unlimited.

Bandwidth limits only apply to the data transfer. The don't apply to the
bandwith of the directory listings etc.

Note that the units are Bytes/s not Bits/s.  Typically connections are
measured in Bits/s - to convert divide by 8.  For example let's say
you have a 10 Mbit/s connection and you wish rclone to use half of it
- 5 Mbit/s.  This is 5/8 = 0.625MByte/s so you would use a `--bwlimit
0.625M` parameter for rclone.

 @friendlytherapist it will be in the 1.36 release, currently in the beta, yes.  This is a known bug I'm afraid - see http://rclone.org/bugs/

I'm planning on fixing it eventually though - see #100 for progress.

Thanks for the report

Nick
  I've had a few thoughts about this which I've jotted down in [rclone's notes](file.https://github.com/ncw/rclone/blob/master/notes.txt) (at the end). 

How would you see it working? Perhaps there would be an rclone config setup for setting the parameters of the encryption. 

Would you want to enter a password each time you ran rclone or would you be happy with the encryption key being in the config file? 
 https://github.com/ncw/rclone/blob/master/notes.txt
 You could also have a look at [restic](https://github.com/restic/restic), which has a lot of similar functionality to rclone, except that it only does encrypted content.

It is not my call if rclone should have encryption or not, but it could seem like something that maybe doesn't belong.
 My view is that an encryption layer for rclone would be relatively easy to add (if done right).

I would use a state of the art block cipher with a random IV for encryption / decryption.  The same for filename encryption if required.  I wouldn't put any integrity checking in - if you put the wrong password in you'll get random data out.

I quite like the idea, but then I'm a bit of a crypto geek anyway ;-)
 @InAnimaTe what I would imagine is that you'd encrypt file names and content.  If you wanted see what files you had you would use `rclone ls` which would decrypt the file names on the fly.  Is that what you had in mind?
 This is obviously a popular request, so I'm going to move it up priority.

I've been thinking about how to do it, and my current thinking is to encrypt both file names and contents using the well understood [secretbox](https://godoc.org/golang.org/x/crypto/nacl/secretbox) library (which we already use for config file encryption).  Secret box provides encryption and authentication using a set of extremely well designed ciphers.
 I'm intending to land this in the next release.  The code is done.  Once I've done a few more docs, I'll make a beta release and post it here - I'll be looking for review before I finalise everything for the release!
 I have pushed the code so far to the crypt branch.

This is feature complete and passes all the tests.

I would appreciate review of [the docs](https://github.com/ncw/rclone/blob/crypt/docs/content/crypt.md) - in particular a second pair of eyes on the the use of crypto primitives would be appreciated!

If anyone wants to look through [the code](https://github.com/ncw/rclone/tree/crypt/crypt) I'd be grateful too.

I'll merge this and add it to the v1.33 release in due course.
 I made a beta release here

http://pub.rclone.org/v1.32-19-g65e714f%CE%B2/

[Docs](https://github.com/ncw/rclone/blob/crypt/docs/content/crypt.md) 

It is possible that the encryption scheme for files or filenames changes before release so use this for testing only!
 @AshleyPinner all excellent questions

> Why are you using a static key with scrypt?

I wanted the whole encrypted remote to be keyed off a single user password.  If I used a salt with scrypt as part of the key derivation then I'd have to get the user to remember that too.

I could get the user to type in another password I suppose, but I wanted to keep it simple.

Let's think about the attack vector - an attacker gets a) a file from your cloud storage system and b) has access to the rclone source code.  They can try passwords and use rclone's key derivation to try to decrypt the file.  Having an additional password or salt would make that process harder for certain. However scrypt is designed to resist the case when the attacker knows the salt.  Making the salt the same for all rclone users would mean you could build a rainbow table if you were attacking many rclone users at once.  If just attacking just one it doesn't really help you.

Do you think asking the user for two passwords is the way to go?

> What design reasons want to use EME (which is based on ECB)?

EME is used for the file name encryption only, not for the contents which is done with NACL secretbox.

We need some specific properties for the filename encryption, namely that whenever we encrypt something we get the same result.  If we don't have that then we can't find files on the cloud storage system if we don't know what it encrypted to.  That 1:1 plaintext:ciphertext is the textbook definition of ECB.

I got the idea of using EME (and the library) from gocryptfs.  I originally used a design based on AES/CBC with the IV based off the hash of the name, but EME turned out to be perfectly designed for this and it makes the filenames a lot shorter.

> What design reasons didn't require a MAC for the crypto?

The file contents have a MAC - it is build into the secretbox format. Secretbox uses XSalsa20 to encrypt and Poly1305  to authenticate which is very strong.

The filenames don't have a MAC though.  They are somewhat protected in that they have to decrypt to strings with valid pkcs7 padding, without control characters which are valid UTF-8 strings.  This makes it unlikely that a decryption error would go unnoticed given that the minimum block size is 16 bytes.  In fact I make it about 1 in 13million chance that a decryption error would go unnoticed.

I'm of the opinion that the file names don't need a MAC. A 1 in 13 million chance for error detection is pretty good (about 23 bits).  A 1 in 13 million chance of a bad actor placing a file in your cloud storage whose name will decrypt is quite small, and even then the contents are protected with a much stronger  MAC.

Do you think that is a satisfactory answer?  I could add a MAC to the filename but I'm wary of bulking them up by another 25 characters for a 128 bit MAC.  I could add a crc32 (4 bytes) to the encrypted data which would make p(error) ~2^-83 bits at the cost of another 6 characters.
 Sorry haven't replied to this thread in a while, here are replies to all the questions together.

## Salt on scrypt

@AshleyPinner @ryanfb @tmikaeld @perception101

I think what I'm going to do is ask the user for two passwords.  The first will be used as it is at the moment.  The second will be optional (but recommended) and used for generating the salt for scrypt.  If you don't supply it it will default to the random built in one as it does now.

I can't generate a salt from the password - that leads to rainbow table style attacks too.

I'll make a mode where rclone can generate a long strong password you could never remember and show it to you so you can store it somewhere.

## MAC on filenames

@AshleyPinner @DanielDent

My thinking is that I'm not going to include a MAC on the filenames. To do it properly would bloat the filenames too much (and they are already too long - see later) and as it stands there is some resistance to corruption of filenames.

Having a MAC implies an attacker can generate filenames.  Assume that an attacker could put enough files in your cloud storage system to create a valid filename.  Now what?  They would have to break the main data encryption to do anything useful to them (like send you a virus).

I don't think any of [the comptetition](https://nuetzlich.net/gocryptfs/comparison/) use a MAC in the filename, except to make the file name encryption repeatable (HMAC used as IV) which doesn't really count.

## Encryption + FUSE

@RXWatcher1

Yes both should be in v1.33.  I'm not recommending FUSE for super reliable transfers though - file systems were never designed to work with unreliable cloud providers.

## Optional file name encryption

@Charnjit123

Currently file name encryption isn't optional.  It could be though.  However I need to be careful not to give extensions to the cloud provider that they understand because lots of them do things like make thumbnails and virus scanning.

So I'd probably want to add a `.bin` to each name if I did this.

## Flattened file system

@trisomeyr @TARSooper

> tried "sudo rclone copy --include "/subdir/test1.txt" crypt: ./" with flattened lvl 1. This doesnt copy only the single file but the whole directorystructure including the single file.

I'll check that out - thanks!

> rclone config

Yes that is expected but confusing - I'll see if I can do something better

> I seem to have a encountered a problem regarding lenght of the filenames (file works just fine without encryption):

Yes that is an unanticipated fly in the ointment. Each flattened encrypted filename on the remote has the whole path to that file in it, encrypted then base32 encoded so it will be at least 60% bigger.

That is in danger of killing the flattened filename scheme as it means that the path length of items \* 1.6 must be less than the limit of the cloud provider.  I don't actually know the cloud provider limits though.  Here are some I found out

| Cloud storage | Max path length |
| :-- | --: |
| Google Drive | 32767 |
| S3 | 1024 |
| B2 | 1000 but with 250 bytes per segment |
| Amazon Drive | 255 per file |

So that would mean on B2/Amazon Drive you can only have paths with a maximum of 156 bytes including all the directories, and the file name.

Is that an acceptable limit? Probably not.

Here is how [gocryptfs fixes the long names problem](https://nuetzlich.net/gocryptfs/security/#long-file-name-handling).  I might implement a similar scheme, but maybe not for this release as I want to get it out the door!

Or alternatively to steal another idea from gocryptfs - have a salt per directory which we store in an additional file.  We can use this to stop the filename prefix leak at the cost of reading an extra file per directory.  This may well be acceptable for those seeking higher security.  It also fixes most of the downsides of the current scheme so is probably a winner. The only thing it doesn't do is completely obscure the directory structure.

We probably still want the long file names fix as 156 bytes for path segments is still quite limiting but that can come later.

## Bug reports

@trisomeyr

Please put them here until release day, then make new issues for them.
 I'm going to have to re-think the more secure file name encryption. I've ripped out the flattened mode and added a no filename encryption mode - I'll make the v1.33 release with that and then explore how to make a more secure mode later.

Here is a beta with that in: http://pub.rclone.org/v1.32-39-g55100f3%CE%B2/

Docs [here](https://github.com/ncw/rclone/blob/crypt/docs/content/crypt.md)

This is approaching what I'll release with v1.33.

@spiffytech wrote

> If we're already paying the cost of reading an extra file per directory, would it make sense to solve the filename length issue by storing each file as sha(filename), storing the original filename inside a map in that directory's metadata file, and keeping the metadata file encrypted?

Good idea.  I think we'd need something a bit better than sha(filename) though.  It could just be a random uuid which would leak the least info. I'd probably put a `.bin` extension on so if the index got corrupted then you could read your data at least with the filename mode "off".

@taylorwmj wrote

> When are you thinking you'll release 1.33?

I'm winding up the release process now - should be a week - cross fingers!

@captainswain 

>  Do you think the fuse filesystem will be stable enough to stream media content? I would love to ditch acd_cli

That is probably an ideal use for it - it should cope fine with that (cross fingers!).
 @fantom-x good idea about storing the metadata elsewhere.  I could store it in the file too (encrypted of course) then that would give you a way of rebuilding the index.

@spiffytech lots of file reads per directory is undesirable as you pointed out.  A single index file is prone to corruption (think two rclones running at the same time), but storing the metadata in the files too would give you a way of rebuilding it which I like.

I've merged the current crypt implementation to master and will try to push out the 1.33 release tonight.

I've made another ticket about future directions in #637 please feel free to chip in there.

I'm going to close this ticket now - please make new issues if you find any problems.
 @jessepeterson an interesting idea. It would mean having a separate key (for the underlying symmetric cipher) for each message. You'd  store the public key encrypted key in the header of each file. 

Yes if rclone.conf was compromised then you'd have access to the public key encrypted data. 

My experience with this sort of thing is that the crypto for PKI is easy, it is all the certificate handling which is a pain. If there was some nice library I could use that would be idea! 

If you'd like me to explore further then please make another issue. 

Thanks Nick 
 @DurvalMenezes unfortunately ACD has quite a short  path length.  Looks like your K element which is 194 chars expands to 334 byte encrypted length which exceeds the 256 char limit (though amazon says it is 280 chars in the error message which is unexpected!).

The copy will just fail to copy that file - everything else should go through OK.

Can you rename it?  That would fix it!  Or don't use file name encryption, or wait for #637 to be done.

BTW did it actually print

```
&{AAAAAAAAAAAAAAAA/BBBBBBBB/CCCCCCCC/DDDDDDD/EEEEEEEEEEE/FFFF/GGGGG/HHHHH/IIIIIIIIII/JJJJJJJJJJ/KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK 0xc4202523c0}: Object not found - waiting (24/24)
```

With `&{...}`?  That is probably a bug! Though maybe I fixed it in 5b913884cf1d3b6442604abab38e9d7c2b8be0fe - was that from the latest beta?
  Can you do a like for like comparison - that would be useful.  What you've posted above the accounts look quite different.

Does the sync go wrong and need to get retried?

I recommend for drive and onedrive you don't set `--checkers=1` - that will really slow down the syncs as it will slow down directory traversal.  You can leave `--transfers=1`.

Here is a quick test I performed

Drive initial sync

```
Transferred:    179349706 Bytes ( 248.44 kByte/s)
Errors:                 0
Checks:              1416
Transferred:         1392
Elapsed time:    11m44.9s
```

Onedrive initial sync

```
Transferred:    182651899 Bytes ( 160.50 kByte/s)
Errors:                 0
Checks:              3954
Transferred:         1522
Elapsed time:    18m31.3s
```

Second sync drive

```
Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:              1391
Transferred:            0
Elapsed time:        7.1s
```

Second sync onedrive

```
Transferred:      4357352 Bytes (  92.85 kByte/s)
Errors:                 0
Checks:              2770
Transferred:           22
Elapsed time:       45.8s
```

I just discovered #218 which may be related - do any of your files have a `+` in the name?
 #218 will cause the sync to be retried - 3 times by default. Do you see that? 

Rclone uses its own application client ids if you left them blank. 
 Both! It will retry the file and if that fail retry the sync 3 times... 
 I did a comparison between drive and onedrive.  For me One drive is 2.3x slower than drive.  I don't know whether that is because one drive is slower than drive, or the rclone code for one drive isn't optimised as much as it should be.

I suspect your massive time difference is due to #218 though

## Drive

```
Transferred:    489935279 Bytes ( 896.30 kByte/s)
Errors:                 0
Checks:                 0
Transferred:         1141
Elapsed time:     8m53.8s
```

## One Drive

```
Transferred:    492778198 Bytes ( 386.08 kByte/s)
Errors:                 0
Checks:              1136
Transferred:         1146
Elapsed time:    20m46.4s
```

This had one retry to tidy up 5 failed files which took an extra 4 minutes
 I'm going to close this as I don't think I can do anything about it - I think it is Microsoft's infrastructure which is probably faster now anyway!
  I tried to reproduce this but failed.

The filtering code didn't change from v1.24 to v1.25.

That error comes from the bit of code where rclone is checking that the rules start with `+` or `-`.

So it looks like one of these
- maybe there is an invisible control char in your filters file
- maybe rclone's line parser isn't working on your platform

So to help resolve that, can you
- email me a zip (I don't want email to interfere with the line endings hence the zip) of the `rclone-filters.txt` file (to nick@craig-wood.com) or alternatively post a hex dump of the file (eg using `xxd -g1 rclone-filters,txt` if using linux)
- tell me which platform (OS, 32/64 bits, OS version) you are using
- double check you get different behaviours with the same file for rclone v1.24 and v1.25 - I find that hardest of all to understand so I'd really appreciate a double check.

Thanks

Nick

``` go
func (f *Filter) AddRule(rule string) error {
    switch {
    case rule == "!":
        f.Clear()
        return nil
    case strings.HasPrefix(rule, "- "):
        return f.Add(false, rule[2:])
    case strings.HasPrefix(rule, "+ "):
        return f.Add(true, rule[2:])
    }
    return fmt.Errorf("Malformed rule %q", rule)
}
```
 Glad you've managed to fix it.

If you do see the error again and can reproduce it then do reopen the issue.

Thanks

Nick
  That is a good idea.  Do you have a link to a doc where Amazon state that 50GB is the limit - I can't seem to find a definitive answer.

Thanks for the suggestion and glad you like rclone!
 Here is the best statement from Amazon that I've found [about the 50GB limit](https://forums.developer.amazon.com/forums/thread.jspa?threadID=4610)

You could use `--max-size=50GB` on the command line to get rclone to skip these files which would be a workaround. 

Rclone can upload files bigger than 10GB but it has a problem downloading them at the moment -  see #204
 What I can do is put a 50GB max size for transfer in the acd remote.  This will give an error when you try to upload/download a file of that size.  I'd probably control that with a flag `--acd-max-size` which defaults to 50GB.

To not see the error and have a clean sync you'd need to use the `--max-size=50GB` command line option (the error it returns could suggest that).

What do you think?

(See also my previous comment)
 @felixbuenemann nice one. Fancy sending a pull request to adjust the values in 4ce2a84? 
  Thanks or those!  In fact I noticed that I've got case sensitive and insensitive completely mixed up in the overview too!
 These are live and on the website now - thanks!
  A `--max-depth` is a nice idea.

It could work for copy/sync too probably.
 In the interest of trying to make consistent tools, here is how `du` defines `--max-depth`

```
 -d, --max-depth=N     print the total for a directory (or file, with --all)
                          only if it is N or fewer levels below the command
                          line argument;  --max-depth=0 is the same as
                          --summarize
```

`find` defines `-maxdepth`, but then `find` does a lot of mad things ;-)
  It's very useful to my usage. I'm use to move very old files to cheaper alternatives. I hope be useful to others.

Thank you
 Great idea - thanks.

Can you
- make sure the builds pass (hint - you need to `go fmt` your code)
- add tests (in `filter_test.go`) 

I'm not particulary happy with the unit of seconds for the parameters.  How about building an option parser based off `time.Duration` and [time.ParseDuration](https://golang.org/pkg/time/#ParseDuration)\- see [SizeSuffix](https://github.com/ncw/rclone/blob/master/fs/config.go#L67) for an example.  That would let the user specify `--max-age 24h` or `--min-age 32s`.  Have a go with this if you like, or leave it for the moment and I'll do it after this lands.

When you are done can you [squash your branch and force push](http://stackoverflow.com/questions/14534397/squash-all-my-commits-into-one-for-github-pull-request).

Thanks

Nick
 I'll do! Thanks for the suggestions. I never wrote a single line of Go before submit this patch.

Apparently the time.ParseDuration doesn't parse days or week. (Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".)
 You are doing well so far!  I suggest you ignore my suggestion for the Duration parser for the moment and we'll work on just getting this merged first.
 It's good?
 It would also help a lot, if you included [documentation update](https://github.com/ncw/rclone/blob/master/docs/content/filtering.md) for the new option.

Otherwise it looks very good to me.
 I couple of other things I thought of
- we should avoid calling o.ModTime() unless it is necessary as it can be an expensive operation
- there is a perfectly good time.Duration parser for the command line flags so we should use that!
 @klauspost my English is pretty bad. I'm not confident enough to write documentation. Somebody else should take the documentation update.

I utilized the DurationP at first version. But later I changed my mind as in practice I would use days, months or years to express age of files. Also I found a bit misleading be limited to hours. I did let this opened because the best solution would be write a custom parser which works with all that stuff IMHO, which defaults to seconds if no suffix is provided at all. Make sense?

I totally agree with o.ModTime(). It was my mistake assume it has been cached somewhere during the fetching of listing.
 > I'm not confident enough to write documentation. Somebody else should take the documentation update.

I'm happy to do that if you aren't confident with it.

> I utilized the DurationP at first version. But later I changed my mind as in practice I would use days, months or years to express age of files. Also I found a bit misleading be limited to hours. I did let this opened because the best solution would be write a custom parser which works with all that stuff IMHO, which defaults to seconds if no suffix is provided at all. Make sense?

I think I'd like to make sure we have a proper parser before I make a release with this in, but I'm happy to run with seconds or a DurationP for the moment.

I'd quite like to extend the Duration parser so it understands days `d`, weeks `w` and years `y` too.  There is a reason that the go team haven't done this already and that is that all of those units aren't necessarily a standard length.  However for rclone I'm not so worried about that!

> I totally agree with o.ModTime(). It was my mistake assume it has been cached somewhere during the fetching of listing.

It probably has been cached, but not definitely depending on exactly what the user is doing.
 I'm quite busy. If you give me few days/one week I'll make a custom parser to complete this feature in the right way, sounds good to you?
 @meirelles take your time - that is fine!

I'll review the commit in detail at the weekend.

I'm planning to make a release at the weekend, but I'll leave this one out of it for the moment as I've got some urgent fixes I need to push out.

You also asked (but I think you may have deleted it now?)

> why the o.Size() is cached and o.ModTime() isn't? They are both very important to check and sync. Would be a performance gain for operations.go keep it cached, normally both are returned together at the listing APIs. Perhaps only updating/invalidating after the Update/Remove call.

We cache it where we can, but not all remotes return it in the directory listing.  Drive/google compute storage/onedrive do but the others don't.  Otherwise it works as you suggest.
 Let's check it
 I've reviewed your commit - looking really good thanks.

I've added a few minor points to the code - can you fixup, rebase and force push please?

I've also fixed the build so can you make sure the build for this branch goes green too.

Thanks, and apologies for the delay

Nick

PS I'll write the docs after I merge it.
 Great. I'll do my best to comment after we decide how should behave the parseAge function. Thanks.
 Thanks for the input.  I think we should get this merged as soon as possible, before the master branch diverges too much!

Cheers

Nick
 Commented. Resync'ed with master. Let's merge it. Thanks.
 I've merged this.  I've also committed a fix! e69e1810901727f0078adedcef815aa44948103f

Thank you very much for your contribution :-)
 Looks great! Thank you.
  @nodughere is `--swift-chunk-size` sufficient?

If so please close the ticket, if not then explain further

Thanks

Nick
 Yes you are right that these aren't documented!

I'm going to reopen and re-purpose this ticket to that end.
 The new docs are live and on the website now - thanks!
  I think this is the same problem as #205 - what do you think?
 I'll fix #205 soon - I suggest you don't use `sync` with `--exclude` until then.  You could use `copy` though.
 I've fixed this in v1.25.

I've spent some time thinking about the algorithm and writing unit tests, but I'd appreciate a second opinion!  If you find a problem then please open another issue.

Thanks

Nick
  All the remotes give size on directory listing anyway so speed-wise this would be the same as `--size-only`.
 If you would like me to reproduce your results then please post the command lines you used - thanks!
 Here is an example showing that a `--size-only` sync is quick with a swift backend.

```
$ rclone --dry-run --size-only sync local swift:Images
2015/11/09 20:44:52 Swift container Images: Building file list
2015/11/09 20:45:05 Swift container Images: Waiting for checks to finish
2015/11/09 20:45:06 Swift container Images: Waiting for transfers to finish
2015/11/09 20:45:06 Waiting for deletions to finish

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:             58159
Transferred:          404
Elapsed time:       14.2s
```

And without `--size-only`

```
$ rclone --dry-run sync local swift:Images
2015/11/09 20:47:19 Swift container Images: Building file list
[snip]
2015/11/09 20:53:59 Swift container Images: Waiting for transfers to finish
2015/11/09 20:53:59 Waiting for deletions to finish

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:             58159
Transferred:          404
Elapsed time:     6m40.1s
```
 Can you post the actual commands with output  like I did above.  You can use `--dry-run` like I did so no transfers or deletions actually happen. Doing it with `copy` is fine.

Could you also post the result of running `rclone size` on the container too please (along with times).
 Can you try that with `--dump-headers` and see if you can work out what is happening? 
 Can you also try `rclone size "hubic:default/newworks"` please? And post the results. Thanks!
 OK I think we are getting there!  What you are saying is that 0 byte files that hubic uses to mark directores are causing an HTTP request.

I can see why that is, and yes, I'm ignoring the content type in the listing.

I should be able to fix that relatively easily.
 This fix has gone live in v1.25

Please test it and if you find any problems then open a new issue.

Thanks

Nick
  `rclone` uses the checkers to read the modification time only.

So if you do `rclone ls swift:container` which doesn't need the mod time, it is much quicker than `rclone lsl swift:container` which does.

Likewise doing a sync with the `--size-only` flag will be much quicker.  You can use the `--checksum` flag to avoid the checkers too, but then you will incur the cost of an MD5SUM on the local filesystem.
 > Done with size-only, the time is the same. Tested onna folfer with 40k files.
> Swift commandline return list in 3/4 seconds.
> Rclone 45 minutes.

What exactly did you do? Please post the command lines.

Did you try `rclone ls container:bucket` ? That should be very quick.

> Can you please add or fix the option for readind metadata only for manifests?

I don't understand what you mean here?

> The swift list -l return also the mod time, you don't have to check the metadata when using checkers.

The time returned here is the time that the object was uploaded.  This is useless for syncing purposes as you can't set it.  rclone stores the mod time of the uploaded file in `X-Object-Meta-Mtime` the same way the swift program does.

So when you do `rclone lsl` it is showing you the value of `X-Object-Meta-Mtime` as the time, whereas `swift list -l` is showing you the `Last-Modified`.
 One thing that occurs to me - is it possible that all, or a lot of your files have been segmented?  That would explain things...
 I think we've identified the problem in #208 so I'm going to close this ticket
  I seem what you mean.  Here is my reproduction

```
$ mkdir a
$ mkdir b
$ touch a/one.jpg
$ rclone -v sync --exclude "*.jpg" b a
2015/11/09 07:40:50 Local file system at /tmp/a: Modify window is 1ns
2015/11/09 07:40:50 Local file system at /tmp/a: Building file list
2015/11/09 07:40:50 Local file system at /tmp/a: Waiting for checks to finish
2015/11/09 07:40:50 Local file system at /tmp/a: Waiting for transfers to finish
2015/11/09 07:40:50 Waiting for deletions to finish
2015/11/09 07:40:50 one.jpg: Deleted
```

`one.jpg` shouldn't have been deleted unless `--delete-excluded` was passed in.

Can you tell me make me a reproduction like the above which shows rclone trying to delete .git directories and spamming the terminal?  I haven't been able to do that.

Thanks
 Thanks.  Can you tell me what you've got in your  `rclone-filters.txt` file and I'll try to reproduce that problem too.
 Thanks for doing that.  I think the `.git` spam is just because you don't have permission to delete those files rather than a problem with rclone.

It will go away when I fix the underlying issue.
 I've fixed this in v1.25.

I've spent some time thinking about the algorithm and writing unit tests, but I'd appreciate a second opinion!  If you find a problem then please open another issue.

Thanks

Nick
  The error message is an S3 error message.  Presumably ACD is based on S3, but I don't think it should be exposing S3 error messages, which makes me wonder whether this is an ACD problem.

I'm pretty sure the web gui uses a different API to the one used by rclone since it can, for example, delete files permanently which the published API can't.

Tomorrow I'll try a 10 GB file myself and see what happens.

Any thoughts @klauspost  ?
 Unfortunately I can't upload a >10GB file as I only have 10GB of quota :-(
 Excellent detective work - the error message now makes sense.

The redirected request is going to S3 - `cd-na-prod-content.s3.amazonaws.com`

S3 is getting auth info from the URL but rclone will be adding an `Authorization:` line with the oauth credentials for ACD also which is confusing it.

The best way to fix this is for rclone not to put in the `Authorization:` header if it detects that it has been redirected to S3, or maybe if there is `Signature` in the URL string.

I don't think stripping the query strings will work as then it will be sending ACD auth to S3.

I'm not quite sure how to code this though! Perhaps putting in a redirect policy which stops redirecting when it gets a 301 with a `Signature` in the URL.  The 301 would be returned to the client and it could then do another fetch using a non oauth client...  It might be that this can be fixed in the oauth transport. Needs a bit of thought.
 That link is very interesting.  It makes me wonder if the current behaviour is a bug in the [oauth library](https://github.com/golang/oauth2).  Will investigate further.
 Don't worry we haven't forgotten about it! We've been thinking about it ☺
 Hmm, I just had another look at this.  The error message from amazon has changed giving a 400 error message immediately.

This means that unfortunately I can't fix this in rclone.  It does mean that amazon is working on large file handling though which is good, but at the moment It looks like that files bigger than 10GB can't be downloaded by any means (including by the web site - which was working).

There is a [forum thread](https://www.amazon.com/gp/help/customer/forums/ref=cs_hc_g_pg_next?ie=UTF8&forumID=Fx1SKFFP8U1B6N5&cdThread=Tx3HYABOMJUH2PE&cdPage=2&cdSort=oldest) about this too.

```
2016/01/22 13:21:46 HTTP REQUEST
2016/01/22 13:21:46 GET /cdproxy/nodes/3e4ts-snip-Aw/content HTTP/1.1
Host: content-eu.drive.amazonaws.com
User-Agent: rclone/v1.26
Accept-Encoding: gzip
```

```
2016/01/22 13:21:46 HTTP RESPONSE
2016/01/22 13:21:46 HTTP/1.1 400 Bad Request
Content-Length: 46
Connection: keep-alive
Content-Type: application/vnd.error+json
Date: Fri, 22 Jan 2016 13:20:39 GMT
Server: Amazon-Cloud-Drive
X-Amzn-Requestid: 7cd83496-snip

{"message":"Currently unsupported file size."}
```
 I found a moderately definitive statement from amazon on the [developers forum](https://forums.developer.amazon.com/forums/thread.jspa?threadID=8815&tstart=0) from Sep 11, 2015 

> Currently, there's a maximum download size of 10GB, so we don't recommend allowing uploads larger than this. However, the API has no upper restriction on upload file size. The download limitation should be removed in the future. 

I should probably put this in the docs...
 I tried downloading a 12GB file from the web interface and it failed right at the start (using chrome/linux).  It didn't give any sort of error message though I can see it made a "400 Bad request" in the javascript console, which agrees with my findings with rclone.

If you can send me a file link to test that would be very useful ( private email nick@craig-wood.com ).

I wonder if jdownloader is using a different mechanism for downloading - I'll try with it later.
 @victor21813 thanks for the links - I've done some investigation and I think the thing to do for large files on amazon cloud drive would be to request a `tempLink` and download them without auth from that link.

[Getting file metadata section](https://developer.amazon.com/public/apis/experience/cloud-drive/content/nodes)

That should be a direct download from S3 which doesn't have any limits.

I haven't tried it yet though - I'll let you know how it goes!
 I have fixed the download of files > 10GB using the tempLink approach.

Here is a beta for you all to try!

http://pub.rclone.org/v1.26-50-gd4df3f2%CE%B2/

Thanks for any feedback

Nick
  For large objects in Swift that were uploaded with multipart, then the md5sum is useless and rclone will ignore it.  Rclone only segments objects bigger than 5 GB BTW.

If the MD5SUM is missing then rclone assumes that the md5sums match.

A normal sync uses (size, mod time) to see if files are identical.  If size is the same, but mod time is different then it uses md5sum to check if the files are really the same.  If they are then it resets the mod time.  If the md5sum isn't present then it is equivalent to it matching.

If you do a `--checksum` sync, then rclone will compare (size, md5sum) for files, but if the md5sum is missing it will assume it matches, so it will just be comparing (size).

So, I think you'll find rclone works exactly as you suggest already :-)

Checkout the [Equal function](https://github.com/ncw/rclone/blob/master/fs/operations.go#L93) if you are interested in the details.
 > On swift side, i have the same files but with different mtimes and sizes = 0 byte.

`sizes == 0` imples that they are manifest files doesn't it? Which explains the above.

> The files on swift have the mdsum in the etag, they are not large. Why it shouldn't be avaible?

rclone thinks they are manifest files, the Etag is the Etag of the manifest file, and therefore not useful.

It is unfortunate rclone does the md5sum of the local file first before finding the remote has no md5sum, but that is the way it goes sometimes!

> Point1. It should check the size and not the mtimes at the beginning. It should compare local filesystem file size with swift content-length and for redundancy the size reported from listing folder.(here rclone should say that they are already different and not equal)

That is exactly what rclone does.  It checks the size first. rclone thinks the sizes are the same, hence the md5sum check.

> Second run: I've tried to copy the files from local to hubic with -n -v -c
> Now rclone says in no time the sizes differ and mark it for copy. Good, it doesn't run the md5 check.

That is inconsistent with the first run where rclone thought the files were the same length...

> 1) How does rclone check the size : content-legth or directory listing

It can be either.

> 2) How does rclone identify large objects ? The only method is checking the presence of manifest metadata. It shouldn't check the file size because an user can upload a file with chunks of 10mb for example.

Only by looking for `X-Object-Manifest`

---

After a bit of testing, I see the problem.

When rclone updates the modification time, it is removing the `X-Object-Manifest` header which explains the inconsistent runs above.

Here is my reproduction

```
mkdir test2a
dd if=/dev/zero of=test2a/100k bs=1k count=100
rclone -v --swift-chunk-size=16k copy test2a swift:test2a
touch test2a/100k
rclone -v --swift-chunk-size=16k copy test2a swift:test2a
# This last one uploads the file again saying sizes differ
rclone -v --swift-chunk-size=16k copy test2a swift:test2a
```

It should be easy to fix
 > when rclone finds a 0 byte file should first check if it is a large object.

That is exactly what it does!

If you tell me what OS and bits (32/64) I'll send you a binary to test the fix.
 Try [this](http://pub.rclone.org/windows-amd64/rclone.exe).

I also put in the not yet released hubic remote (do `rclone config` and you'll see)

Let me know how you get on
 The Swift auth token normally expires and there is a mechanism in swift for fetching a new one when it does.  The hubic remote implements that, though I haven't actually tested it yet!
 This fix has gone live in v1.25

Please test it and if you find any problems then open a new issue.

Thanks

Nick
  This shouldn't be allowed, instead it gives a rather cryptic error

```
$ rclone -v copy /tmp/many_files/file1.tst s3:
2015/11/08 09:57:40 S3 bucket : Modify window is 1ns
2015/11/08 09:57:40 Attempt 1/3 failed with 1 errors and: MethodNotAllowed: The specified method is not allowed against this resource.
    status code: 405, request id:
```

Probably should check all methods in s3, swift and googlecloudstorage that they have a bucket/container where necessary.
  Yes, I'll fix that. Rclone is supporting go 1.3 to 1.5 at the moment. I didn't notice that the travis build had broken because of the OS X build failure :-( 

I wonder if travis  supports gccgo  ... 
  I've been thinking about this already, so yes :-) 
 This is now live in v1.25

Please test and if you find any problems then open a new issue!

Thanks

Nick
 @roms2000 Great news! Thanks for the update. 
  Onedrive is capable of Move and MoveDir with the [update method](https://dev.onedrive.com/items/move.htm)
 To make --backup-dir work really well on onedrive this is needed.

(Fix the overview table when implemented!)  Go tries to load the root certificates from these places on linux.

```
    "/etc/ssl/certs/ca-certificates.crt", // Debian/Ubuntu/Gentoo etc.
    "/etc/pki/tls/certs/ca-bundle.crt",   // Fedora/RHEL
    "/etc/ssl/ca-bundle.pem",             // OpenSUSE
    "/etc/pki/tls/cacert.pem",            // OpenELEC
```

You could try the `--no-check-certificate` flag but you'd have to compile your own as it hasn't made it into a release.

I should probably put this info in the docs as it isn't the first time someone has asked!

Let me know if that helps
 @doph thanks for that.  I'm going to re-open this ticket to remind me to put something in the docs!
 I put your suggestion for fixing in the FAQ entry - thank you very much for that.
 @bengki I guess using `-k` aka `--insecure` makes sense if the platform doesn't have a ca-bundle.  I'll put that in the FAQ - thanks.
  This isn't a huge issue, but we should probably normalize the files names when comparing them, otherwise encoding issues could lead to duplicate files/directories.

It seems like NFC (Canonical Decomposition, followed by Canonical Composition) is what we want.

NFKC is more intrusive, but can be useful in cases where the underlying filesystem does transforms like `[]rune{'ﬂ'}` → `[]rune{'f', 'l'}`. However, it also transforms `¼` → `1/4`. Here is [a list of more substitution examples](http://www.unicode.org/reports/tr15/).

It doesn't look like it is a huge task, but will need a few tests.

Here are the important changes I would by looking for usage of the `Fs.Remote()` interface value, which to me indicates operations across filesystems.

If you think this is the right way to go, I will see if I can create some sensible tests.

``` diff
diff --git a/fs/operations.go b/fs/operations.go
index 7b8fc86..e537415 100644
--- a/fs/operations.go
+++ b/fs/operations.go
@@ -10,6 +10,8 @@ import (
    "sync"
    "sync/atomic"
    "time"
+
+   "code.google.com/p/go.text/unicode/norm"
 )

 // CalculateModifyWindow works out modify window for Fses passed in -
@@ -182,7 +184,7 @@ tryAgain:
    actionTaken := "Copied (server side copy)"
    if fCopy, ok := f.(Copier); ok && src.Fs().Name() == f.Name() {
        var newDst Object
-       newDst, err = fCopy.Copy(src, src.Remote())
+       newDst, err = fCopy.Copy(src, norm.NFC.String(src.Remote()))
        if err == nil {
            dst = newDst
        }
@@ -211,7 +213,7 @@ tryAgain:
            err = dst.Update(in, src.ModTime(), src.Size())
        } else {
            actionTaken = "Copied (new)"
-           dst, err = f.Put(in, src.Remote(), src.ModTime(), src.Size())
+           dst, err = f.Put(in, norm.NFC.String(src.Remote()), src.ModTime(), src.Size())
        }
        inErr = in.Close()
    }
@@ -338,7 +340,7 @@ func PairMover(in ObjectPairChan, fdst Fs, wg *sync.WaitGroup) {
                    ErrorLog(dst, "Couldn't delete: %v", err)
                }
            }
-           _, err := fdstMover.Move(src, src.Remote())
+           _, err := fdstMover.Move(src, norm.NFC.String(src.Remote()))
            if err != nil {
                Stats.Error()
                ErrorLog(dst, "Couldn't move: %v", err)
@@ -384,7 +386,7 @@ func DeleteFiles(toBeDeleted ObjectsChan) {
 func readFilesMap(fs Fs) map[string]Object {
    files := make(map[string]Object)
    for o := range fs.List() {
-       remote := o.Remote()
+       remote := norm.NFC.String(o.Remote())
        if _, ok := files[remote]; !ok {
            files[remote] = o
        } else {
@@ -444,7 +446,7 @@ func syncCopyMove(fdst, fsrc Fs, Delete bool, DoMove bool) error {

    go func() {
        for src := range fsrc.List() {
-           remote := src.Remote()
+           remote := norm.NFC.String(src.Remote())
            dst, dstFound := delFiles[remote]
            if !Config.Filter.Include(remote, src.Size()) {
                Debug(src, "Excluding from sync")
```

``` diff
diff --git a/fs/limited.go b/fs/limited.go
index 3c15ba2..b94c677 100644
--- a/fs/limited.go
+++ b/fs/limited.go
@@ -4,6 +4,8 @@ import (
    "fmt"
    "io"
    "time"
+
+   "golang.org/x/text/unicode/norm"
 )

 // Limited defines a Fs which can only return the Objects passed in
@@ -58,8 +60,9 @@ func (f *Limited) ListDir() DirChan {

 // NewFsObject finds the Object at remote.  Returns nil if can't be found
 func (f *Limited) NewFsObject(remote string) Object {
+   remote = norm.NFC.String(remote)
    for _, obj := range f.objects {
-       if obj.Remote() == remote {
+       if norm.NFC.String(obj.Remote()) == remote {
            return obj
        }
    }
@@ -72,6 +75,7 @@ func (f *Limited) NewFsObject(remote string) Object {
 // will return the object and the error, otherwise will return
 // nil and the error
 func (f *Limited) Put(in io.Reader, remote string, modTime time.Time, size int64) (Object, error) {
+   remote = norm.NFC.String(remote)
    obj := f.NewFsObject(remote)
    if obj == nil {
        return nil, fmt.Errorf("Can't create %q in limited fs", remote)
@@ -106,6 +110,7 @@ func (f *Limited) Precision() time.Duration {
 //
 // If it isn't possible then return fs.ErrorCantCopy
 func (f *Limited) Copy(src Object, remote string) (Object, error) {
+   remote = norm.NFC.String(remote)
    fCopy, ok := f.fs.(Copier)
    if !ok {
        return nil, ErrorCantCopy

```
 I'm assuming that this is a fix for #194?

If so then it would be a lot less ugly if we could confine it to the local file system, and assume that the remotes will be sensible and just return whatever utf-8 we give them, rather than mangling it like OS X and HFS+ do?
 No. I actually didn't know that the root cause for #194 was normalization.

Assume we sync a file from FS A -> B. When we read back the directory of B, we get it back in a different form. The next time we sync A -> B, the file will not be found since the representation differ, and therefore re-sync'ed every time.

This compares the normalized form of the files.

That said, we should probably give each FS a change to do individual normalization (like the Windows local file system), but this should still be here.
 @klauspost Been thinking about this...

I think you are right - the only way to do this is in the copy operations, at the interface between one file system and the other.

The alternative would be to say that all remotes must output normalised file names.  We could do this, but for file systems that have unnormalised file names (like HFS+ on OS X) we'd need to keep a copy of the internal unnormalised name and the `Remote` name and be able to translate between the two which gets very complicated very quickly. This doesn't cover the case where someone might have uploaded an unnormalised file name to S3 say and there is no easy way to get the unnormalised name from the normalised name.

So yes, it would be great if you have a go with this.

I'd probably make some utility function just to try to reduce the visual complexity of the change.

``` go
func NormString(s string) string {
  return norm.NFC.String(s)
}

func NormRemote(o Object) string {
  return NormString(o.Remote())
}
```

I'm pretty sure that this will fix #194
  Interesting. I have not experienced that behavior.

What speed is the built-in stats showing? 
Are there any errors?
What does you commandline look like?
 Here is a quick test I did

```
$ dd if=/dev/zero of=/tmp/1gb bs=1M count=1k
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB) copied, 2.71206 s, 396 MB/s
$ ifconfig eth0
eth0      [snip]
          RX bytes:560186803 (560.1 MB)  TX bytes:226423925 (226.4 MB)

$ rclone --stats=10s -v copy 1gb acd:test10
2015/11/02 13:55:08 acd: Saving new token in config file
[snip]
2015/11/02 13:55:47 1gb: Copied (new)

Transferred:   1073741824 Bytes (25917.80 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            1
Elapsed time:       40.4s

$ ifconfig eth0
eth0      [snip]
          RX bytes:590700646 (590.7 MB)  TX bytes:1360582953 (1.3 GB)
```

So a 1 GB file transferred using 1.056 GB of data for me so only about 5% overhead. (I tried it with a random file with identical results).

Binaries are uploaded with encoding to ACD which has an overhead but it shouldn't be 10x!

Did the transfer have lots of retries is the only thing I can think of - a log with the `-v` flag will show that.
 @TiGWolf just the `-v`, unless you want to `--dump-headers` which is really only useful for developers!
 It _seems_ like it has transferred 39.55GB when the log ends. 39545913184 Bytes = 39.55 GB  (gigabytes)
 I can see that there were 7 retries in that transfer

```
F/F6/F65/F657/F6575E34680B7694C5075981679F2716
F/F6/F65/F65D/F65D0FBC282BE1E0E5A92868F70A99AD
F/F6/F6B/F6B6/F6B66F27FE4A3237FCFA9C651B5028EE
F/F7/F74/F74A/F74A4C1E7F2E64529384510861CC86AE
F/F7/F79/F79B/F79B80D1DE8A0AB296ACE33A7E86D26E
F/F8/F8C/F8C0/F8C0664C035B79300FBA6D0E5FD6361F
F/F8/F8C/F8C2/F8C2AD921C6B750E5CAFC22600BB6C5A
```

What is the total size of those files? If they total to > 30Gb then that is the problem.
 I think I see the problem. This file starts transferring near the beginning but doesn't finish`.../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9` by the end.  It is > 50GB I think.  So 30 Gb of it get transferred but then rclone is stopped and Amazon discards the partial upload.

So I think if you can transfer this file to the end, then your bandwidth excesses will disappear, or you could just do a run transferring very large files with `--min-size=10G` (or try a run skipping it using `--max-size=10G`)

```
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9:  0% done. avg: 6956.9, cur: 3297.4 kByte/s. ETA: 5h38m3s
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9:  0% done. avg: 6317.1, cur: 4470.8 kByte/s. ETA: 4h9m7s
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9:  0% done. avg: 6387.5, cur: 5513.3 kByte/s. ETA: 3h21m48s
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9:  0% done. avg: 6088.8, cur: 5292.9 kByte/s. ETA: 3h30m3s
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9:  0% done. avg: 6160.9, cur: 5685.7 kByte/s. ETA: 3h15m21s
[snip]
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9: 44% done. avg: 6842.9, cur: 10657.6 kByte/s. ETA: 58m28s
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9: 44% done. avg: 6851.5, cur: 10620.3 kByte/s. ETA: 58m30s
 * .../A90/A903/A9031270513D28BB0AFB9D71DA9E0AE9: 44% done. avg: 6860.1, cur: 10589.9 kByte/s. ETA: 58m30s
```
 @TiGWolf suggest you delete that zip as Klaus and I have a copy now
 @TiGWolf are you happy that we've explained the problem - can I close the issue now?

Thanks

Nick
  This has the same cause as #172 and #190 so I'm going to close as it is a duplicate. 
  This has the same cause as #172 I'm pretty sure.  It is because the hubic client creates 0 sized files for each directory that it makes. Rclone isn't ignoring  these as it should (hence the message about limited fs) so the fix in #172 (which I haven't done yet)  should fix this. 

You should find rclone works fine to a destination you didn't create with the hubic client though. 
 This should be all fixed with v1.24 - let me know if it isn't

Thanks for the report

Nick
  There is a backwards compatibility problem if we switch over to this - rclone assumes segments are stored in a `_segments` container when managing the container so a bit more code than this would be needed I think - I'll have to check.

@Tolsi do you have a view on whether naming the containers like this is a good idea or not?
 Sorry for the delay - fell off my radar!

I think I'm not going to merge this as-is. If you read through the code, you'll see that rclone assumes that segments are in `_segments` containers - it doesn't read the manifest like it should, so changing the name of the segments containers will cause problems for users. You could consider this a bug, and I'd be very interested to see a PR to fix it!

If you would like to resend with a feature flag, eg `--swift-segment-container-prefix` or something like that then I would accept it.

Thanks

Nick
 I'm going to close this for the moment - please send a new PR if you want to carry on with the feature.

Thanks

Nick
  I will take a look at it:

https://github.com/aws/aws-sdk-go#aws-sdk-for-go
 Thanks Klaus
 Fixed with PR #187 - Thanks Klaus
  Files bigger than 2gb is suspicious...

Which OS are you using and is  it 32 or 64 bit?

Do any files bigger than 2gb transfer Ok? 

"no route to host" is a very odd error and normally only caused by networking problems. I wonder if your Nat gateway or ISP gave up at 2gb for some reason.

What is the make/model of your router/gateway and who is your isp? 

I'll try to reproduce but if you could answer the above that would be very helpful. 

Thanks

Nick
 Are you on wireless network? "no route to host" indicates that the connection was dropped somewhere.

I have uploaded many 10+GB files, so it isn't a general problem.

Unfortunately ACD has no way of resuming uploads, so I  don't expect there is much we can do.
 @klauspost I tried uploading a 5GB file with both a 32 bit and 64 bit version of rclone and both worked fine.

@Wowfunhappy My top suspect would be your ISP killing the connection for some shaping reasons or the NAT box blowing up (just because!).

Unfortunately ACD doesn't have chunked upload either so we can't avoid the very large single TCP connection.
 Wireless disconnects can happen for very short intervals, even if coverage is generally good.

If you don't want it to happen, wired network is the way to go. 

As a side-note, rclone will automatically retry the interrupted files up to 3 times, after it has tried transferring all files, so if you just leave it running, they should be uploaded at some point.
 Glad we got to the bottom of it and thanks for your help working it out. In future Amazon have said they will do a better API for uploading files, but until then you'll have to stick to the Ethernet! 
  I see what you mean... Did you have any specific examples that you think should just be retried without increasing the delay?
  I've thought about metadata caches, but I've been reluctant to add them because of the problems of keeping them up to date. I could see it maybe as an optional flag.  However my preference is to try to make the sync as fast as possible without.

You should find a no changes sync doesn't take very long.  ACD returns all the metadata as part of the directory listings so no extra transactions are required, just one per directory.

Did you measure how long a no changes sync takes?  It would be interesting to compare that to the time that `rclone size` takes - I think they should be about the same.

FYI here is my acd

```
$ rclone size acd:
Total objects: 5582
Total size: 9.908G (10639075938 bytes)

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            0
Elapsed time:       13.5s
```

So it is scanning about 400 files / second for me.  That will vary of course but it varies as number of directories mostly, so directories with more files in will scan quicker.

One avenue of making ACD faster would be to try to get it to return all the files in one transaction, rather than having to read each directory one at a time.  This might be possible with the changes API, or by better use of the list
  I wonder if this is a duplicate file problem? Can you check in the google drive web interface that there aren't duplicate copies of the avi files? If there are, delete all but one and the sync should work properly after that.

Sometimes rclone causes duplicate files on drive and I haven't figured out why yet!
 Can you send me the output when you do the copy without `--dry-run` but with the other flags you used?  That would be useful information.  Rclone logs whether it is updating an existing file or not.

Can you try doing

```
rclone ls "gdrive:rPhotos/Wedding stuff/Wedding DAY/Wedding" > files.txt
```

Then search in `files.txt` for `MVI_2628.AVI` and see whether it is in there more than once? And the size is as you expect.  Replacing `ls` with `md5sum` and repeating would be useful too. That should check the directory traversal code is working as expected.

The results of

```
rclone -v check "V:\Pictures\Wedding stuff\Wedding DAY\Wedding" "gdrive:rPhotos/Wedding stuff/Wedding DAY/Wedding"
```

Would be interesting too (check doesn't support excludes yet (see #169) so there will be a bit of noise in there).

What happens if you try the sync without `--checksum`?

Have you tried the sync with `--size-only`?

Thanks for helping me work out what is going on!

Nick
 Thanks for doing some good investigation.  It seems we are narrowing the problem down. I tried to reproduce the problem with some spaces in the directory names (good idea) but that didn't work, alas.

So it looks like
- the files MVI_5670.AVI etc are on your drive
- they don't appear in the rclone listing

I can't figure out why at the moment, but if you could do

```
 rclone -v --dump-bodies ls "gdrive:rPhotos/Wedding stuff/Wedding DAY/Wedding" >ls.log 2>&1
```

and email `ls.log` to me privately ( nick@craig-wood.com ) that might give me a clue!  The dump will have some `Authorization:` lines in so don't post it here .  I'll delete the log as soon as I'm finished analysing it. You can delete the `Authorization:` lines before sending it to me if you like.

Thanks

Nick
 Hmm, so the AVI files aren't in the listing at all which explains rclone's behaviour perfectly.

Did you upload those files with rclone?  Or were they shared with you?

Can you try this?
- goto the [google API explorer](https://developers.google.com/drive/v2/reference/files/list#try-it)
- And put this as the q parameter `"0B6eUhdSrAP1zYkZrcWhSSmRIWk0" in parents`
- That should list everything in that directory.
- You'll have to authorize the API explorer first (flip the switch)

If that shows the AVI files then can you email me the output please!

Thanks

Nick
 You need to put exactly this in the q field

```
"0B6eUhdSrAP1zYkZrcWhSSmRIWk0" in parents
```

And hopefully that will do something.
 Aha! Thanks for clearing that up - I hate mysteries where code is concerned. Yes drive is case sensitive (just realised that the storage overview page has it exactly backwards!) 

You posted the command lines you used and if you look back in this thread you can see the difference there which I really should have spotted - so don't feel guilty! 

Glad we sorted out! 

Nick 
 I think we've sorted this one out now, so I'm going to close the issue.
  The easiest option is to say N to auto config. That works on a remote server.
 @diego-vieira The original question was about drive. I'm afraid that ACD doesn't have a non-auto config mode. If you are trying to set rclone up on a headless box then set it up locally and copy the config file.  See the first item in the FAQ for how to do that. http://rclone.org/faq/

Hope that helps. 

Nick 
  What CPU is in that machine?

My guess is that it is SSL eating up the CPU. 

@ncw - is pprof included in rclone? Can we do a cpu profile?
 It is very likely the SSL.  Go implements its own SSL which isn't quite as CPU efficient as openssl which might explain the 1.5 MB/s vs 2.85 MB/s difference.

You can run the profiler with rclone using the `--cpuprofile=rclone.prof` flag.  This will make a file called `rclone.prof` which you can then examine with the go tools `go tool pprof rclone rclone.prof`, then the command `top20` usually will tell you what is going on.  If you'd prefer, email me the `rclone.prof` file (along with the exact version of rclone you are using or the binary if you compiled it yourself) and I'll analyse it and post the results here!
 Close? I don't think there is anything we can do. SSL will be faster on x64 in 1.6 now that it looks as if  [CL 8968](https://go-review.googlesource.com/#/c/8968/) will land. But unless someone converts it to ARM it will remain as it is.
 I'm going to close this as it seems the CPU load is due to
- SSL performance on a slow processor
- Go SSL routines not being as fast as they could be

We can't do anything about the first, and the second will improve over time I'm sure, but neither can be fixed by your humble rclone developers!

Thanks for the report

Nick
 @rindeal I'd consider an alternate build for ARM using openssl.  Fancy sending a Pull request?
 @rindeal - I'm ever hopeful :-)  Actually I think the code changes would be quite easy as the library is a drop in replacement for the go http library, but making the build will be painful as it will need a full cross compile environment for ARM and all the libraries.
 @rindeal I do have plan B which is to rewrite the ARM AES implementation in assembler and submit it for go 1.7.  (I already did MD5 and SHA1).  Depends on whether I have enough time really!
  @filmil - could you explain your use case a bit more in depth?

What are you copying to/from? 
How much time is spent before actual copying starts?

I have uploaded several TB to Amazon Cloud Drive, and resume time has never been an issue. I think the optimal way is to see if there is a way to speed up the current method, before throwing in "magic".
 Thanks for reporting this.  I'm going to close this it seems rclone is working as intended - maybe not quite as fast as possible though. Rest assured we will continue to try to make the resumes as fast as possible!

Thanks for the report

Nick
 Great! You should find the next version of rclone is much faster with drive. You can try a beta at pub.rclone.org if you want (choose the latest). 
  > Everytime I make a change in the local folder I shared my google drive with, it get's undone
> after a few seconds.

Are you using something else to sync this directory as well as `rclone`?  If you are that will cause problems.  You could run rclone on a separate directory (locally and in drive) to any other syncing systems.

Can you show the commands you use to run rclone, what happens and what you expected to happen?

>  Feels like rclone doesn't upload anything but mirrors the shared gdrive.

rclone is a one-way syncing program.  So if you run it like this it will sync from drive to local, mirroring the drive to local disk.

```
rclone sync drive:Folder LocalFolder
```

And like this to sync the other way from local to drive, mirroring your local folder to the remote drive.

```
rclone sync LocalFolder drive:Folder 
```

Drive doesn't do bi-directional sync yet like the google drive client does (it might in the future).
 Just wondering if you'd had a chance to look at my reply? 

Thanks

Nick 
 @dodekaeder sorry we couldn't make it work for you.  There are lots of happy users of rclone with drive so it is possible to make it work - let me know if you want to have another go some time.
  > purge doesn't work as is for folders created within swift. ls command doesn't work either. I have to run the command mkdir on the subdirectory before it can be purged.

How did you originally create the folders.  Can you give an example so I can reproduce the problem?

> 2015/10/13 03:49:54 Attempt 1/3 failed with 2 errors and: Container Not Empty
> 2015/10/13 03:49:54 Waiting for deletions to finish

Yes, I think that is a bug.  It shouldn't be attempting to delete the container if you are using a sub-directory.  The same bug is probably in `s3` and `google cloud storage`!

> EDIT: Those issues seem to be related to the way my Swift provider is managing directories. I get a third issue when using ./rclone sync swift:container/sub local. Directories in sub are created as 0-byte-files (and I get errors for all files in those directories).

That is the old style way of creating pseudo directories in swift.  I thought rclone ignored them, but checking the code apparently it doesn't, so that is probably a bug.

How did you create those directories? Using the swift tool?

Who is your provider? And which version of Swift or Openstack are you using?

Thanks

Nick
 Directory Markers are zero-length objects which have the metadata content-type set to "application/directory".

It sounds like that is what the client is creating and rclone isn't ignoring them like it should.

Most clients don't bother creating them though as swift now deals quite well with so called pseudo directories.
 @axtux thanks for confirming that - I'll think about how to fix properly.  It will require an implementation of `Purger` for swift if we filter the 0 length files out...
 @xgiovio I'll try to do it this week - cross fingers!
 @axtux @xgiovio All these issues should be fixed in v1.24 - let me know if they aren't

Thanks

Nick
  I would expect them to work with spaces. I'll check out why not shortly! 
 Ah yes I see why!  What I shall do is check new remote names against `fs.matcher` then tell the user the remote name is invalid.  I'll allow spaces too!
 I've released this in v1.24

Thanks for the report

Nick
  Does it also happen if you use more than 1 transfers?
 Can you try varying the `--bwlimit` upwards? Try doubling it until it works... I suspect that the pauses introduced by the limiting are causing the server to misbehave.

I don't currently have access to a s3/ceph server. Looks like I could sign up for a free trial to see for myself though!
 At least I cannot reproduce with ACD on Windows, so it could be an S3/ceph-specific issue.

I looked for some possible races/deadlocks in "Account", but I couldn't immediately find one. The [only place the bandwidth limiter waits](https://github.com/ncw/rclone/blob/master/fs/accounting.go#L322) shouldn't have any side-effects, even though we hold the "file.mu" lock while it is running.
 I managed to replicate this both with S3 and with dreamhost (s3/ceph).  It doesn't seem to be a problem with any of the other providers.

It might be a problem in the https://github.com/aws/aws-sdk-go library but I haven't had a chance to look yet.
 @ncw - I have done some digging in the S3 library.

It seems like the library [buffers up to "Partsize"](https://github.com/aws/aws-sdk-go/blob/master/service/s3/s3manager/upload.go#L329) before sending any data. That is obviously bad for the bandwidth limiter, since it will only limit the read into the buffer, not the actual upload.

Also, there is fairly silent automatic retries on the [request handler](https://github.com/aws/aws-sdk-go/blob/master/aws/request/request.go#L200). I think it has at least 3 retries before failing. This can be adjusted in the [*s3.S3](https://github.com/aws/aws-sdk-go/blob/master/service/s3/service.go#L17) that contains a [*service.Service object](https://github.com/aws/aws-sdk-go/blob/master/aws/service/service.go#L22).
 I've eventually tracked the problem down to the token bucket library rclone uses.

I've sent in a fix for it tsenart/tb#7

This fixes the stalling problem, but doesn't address the problems @klauspost states above (which need a re-design of the limiter - putting it into the transport probably).
 This has now been fixed upstream and the fix will be in the next release.

There are still issues with the smoothness of the transfer, but I think that is a job for another ticket #222

@Ferni7 thanks for reporting the bug and sorry it has taken a while to track it down!
  Go tries to load the root certificates from these places on linux.

```
    "/etc/ssl/certs/ca-certificates.crt", // Debian/Ubuntu/Gentoo etc.
    "/etc/pki/tls/certs/ca-bundle.crt",   // Fedora/RHEL
    "/etc/ssl/ca-bundle.pem",             // OpenSUSE
    "/etc/pki/tls/cacert.pem",            // OpenELEC
```

Do you have something in one of those places? Or maybe somewhere else?

I wonder where wget is looking for the certs?  Maybe you could strace it and find out?

I have thought about putting an `--insecure` flag in which would ignore the certificate check, but I'm not sure it would help in the case of this error.
 PS it might be possible to support hubic directly - rclone knows how to do oauth
 You can find the `--no-check-certificate` in v1.24 if you need it
  That would be an unusual setup!  Can you tell me a bit more about it?  Why doesn't keystone return the correct storage URL?
 I see what you are getting at.  One thing you should bear in mind is that rclone doesn't copy all the metadata at the moment - see #111

I think this feature is probably too specialised for rclone.  You could have a go with a quick and dirty patch like this which would set the storage url from something called `storage_url` in the config file.  If the token expires it will attempt to get another one from keystone though which will overwrite the storage url.

If this works I might consider putting it in as an undocumented feature (with a bit more error checking!).

``` diff
--- a/swift/swift.go
+++ b/swift/swift.go
@@ -151,6 +151,7 @@ func swiftConnection(name string) (*swift.Connection, error) {
    if err != nil {
        return nil, err
    }
+   c.StorageUrl = fs.ConfigFile.MustValue(name, "storage_url")
    return c, nil
 }
```
 Just wondering if you had any more thoughts on the above?
 @xlucas can you explain how adding a `storage_url` helps you use ACLs with rclone?  Maybe this should be better supported with rclone?
 OK I'll put the above patch in with a bit of docs - unless you want to do a PR?

I note that the patch needs to not override the StorageURL if the `storage_url` config parameter is empty, so it isn't quite right as-is!
 Actually come to think of it, the StorageUrl will get overwritten on reauthentication.  So it really needs a fix like the one used in the Hubic remote: https://github.com/ncw/rclone/blob/master/hubic/auth.go which makes it a bit more complicated.

What you would do is do the auth then wrapped the returned Auth in one which just override the `StorageUrl` method.
  Good idea!  Easy to do. How about something like the output of `rclone size` ?

```
Total size: 563.957G (605543949787 bytes)
```
 @AntoineGR excellent news!
  There looks to be quite a complete [go library](https://github.com/t3rm1n4l/go-mega) which is promising!
  @dav1303 is right. Setting the environment variables "HTTP_PROXY" and "HTTPS_PROXY" (or their lower case equivalents) should be picked up. It must be a parseable URL and http type, so I guess "socks" doesn't work.
 The standard go proxy environment vars should work. I should put what the are into the docs! 

 [Go stdlib docs on proxy](https://golang.org/pkg/net/http/#ProxyFromEnvironment) 
 There is now an item in the FAQ about this - see "Can I use rclone with an HTTP proxy?" at http://rclone.org/faq/

Thanks for the report

Nick
  How big are the objects you are uploading?

Can you paste a log with the `-v` flag showing me what is going on?

Thanks

Nick
 Drive is really fussy about uploads - it can manage no more than 2 per second in my tests, and loves throwing a whole heap of different errors at you.

Only files bigger than 256k (by default) will be uploaded in chunks.  If there is an error in a smaller file then it will be just be retried.  For the larger files the individual chunks are retried, and if that fails (which it does sometimes) then the whole file is retried.  Failing all of that, rclone will retry the whole sync.

It looks like rclone is doing the right things with the retries from the logs you pasted.

The main thing is that does rclone complete properly? In the final summary does it say `Errors: 0`.  In my testing `rclone` pretty much always manages that no matter how badly drive is behaving!
  Yes you are right this is a bug.

I'll fix it for the next release, but in the mean time you can work around it by changing the URL - the redirect_url bit should look like this `&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob` (replace it up to but not including the next `&`).
 This should be all fixed in 1.23 - thanks for the report
 Ah, didn't think of that. Glad 1.23 working though! 
  Would it be an improvement if local filesystem would read ".md5" files, so it could get checksums without reading the file?

If you aren't against it, I wouldn't mind implementing it. Of course it should check the file modified times.

I would try to make it generic, so it could also be used by other filesystems without native checksum support.
 Is there a standard for `.md5` files?

rclone could potentially create it and or update it too so it would act like a cache.

I'd probably say this should be accessed with a command line flag `--md5-file` or something like that.  Though you might want a file both for the receiving and sending remotes!

Probably if you passed an `--md5-file` then it should default to a `--checksum` sync.

I like the idea.

We should probably explore what the user interface would look like first before diving into the code - that would help clarify our thinking on exactly what the feature does.
 It is pretty stable format.

Sample:

`5e2eb149b6d704dcb46d685d60dbe0a9 *IMAG0013.jpg`

There can be different delimiters, `" |"`, `" *"`, `"  "`. The path can also be included.

An md5 sum file can contain md5 sums for one or several files, so a file can contain md5 sums for all files on a folder, so we cannot assume any names for the files.

My implementation thoughts are like this. First time the `fs.Md5sum()` is called it, it reads the same directory as the file, and filters out the md5 sums. If the filename matches, the precalculated md5 will be used if modtime is before modtime of the md5 file.

As for commandline options, I though that only creation would require a special flag, since`--checksum` is off my default anyway.
 I don't really like the magic behaviour of adding stuff to `fs.Md5sum`.  I'd much prefer the user to have to explicitly state they want this behaviour.

I was thinking that there would be only one md5sums file rather than lots though, but maybe lots is OK too.

I can see something like that working well with the local Fs
- have an md5sum file with the md5sums for the whole tree in
- rclone uses it as a cache for reading/caching md5sums

I've shied away from storing metadata on the remote fs that don't support things in the past as it is a hard problem keeping it up to date especially if it is being updated from two places at once. So I don't think I'm in favour of doing this except on the local Fs.

I think we should think clearly about what problem we are trying to solve?  Is it
1. saving the user time by caching md5s on the local Fs
2. using pre-caclulated md5s whenever possible
3. To work around limitations of remote FS which can't store MD5SUMs
1. is useful, though an MD5SUMs file isn't ideal as it doesn't have the mod time of each file in it - a json file with modtime and md5sum would tell you on a per file basis whether you needed to update the md5sum or not.  Using a .md5 file per file might work for some things (like an archive of big files).
2. I'm not so sure about - how common are .md5 files?
3. I'm probably not in favour of since the synchronisation problems where people use it from two places are quite hard. Though they become much easier if we are thinging of an .md5 file per file.

What do you think?
 @ncw - it is meant as a _local_ option to make it possible to get md5 matching without needing to do a complete file scan on every sync. Syncing that would make it feasible to sync many gigabytes of data quickly with md5 matching.

For the first implementation the idea was only to enable _reading_ md5sum files. That way people can use their favorite tool to create them. Maybe rclone at some point should offer updating them, or have its own format.

My initial through was that it should be opt-in to use md5-files.

@michaelthwaite My proprosal was to use the [existing md5sum format](https://en.wikipedia.org/wiki/Md5sum). This format unfortunately doesn't include timestamps. However, checking modification time on both files should give a fairly reliable idea if the md5sum is current.
  It says unknown client_id and it appears to be blank which is odd.

I did change something in the reading of client ids in 0872ec3204a605b4aa6f14cecb0392c80068d8ae which went into v1.21.

If you try 1.20 does it work?

I wonder whether you've got a space or some invisible character at in your config file, or maybe the config file has windows line endings for some reason?

Can you check your config file?  It should look like this where there is nothing after the `=` sign in `client_id` or `client_secret`

```
[acd]
type = amazon cloud drive
client_id = 
client_secret = 
token = {"access_token":...secrets...
```
 It seems likely that the change  0872ec3 is the cause of the problem but I can't figure out why!

What platform are you on (Windows/OS X/Linux, 32/64 bit)?  I'll send you a test binary to print out the `client_id` as I think that is the key to the puzzle.
 @shawnbissell [Here](http://www.craig-wood.com/nick/pub/rclone-osx-amd64) you'll find an OSX 64 bit binary which should produce two extra logs like this

```
2015/09/30 09:29:37 Read from config file client_id=""
2015/09/30 09:29:37 Using client_id="amzn1.application-oa2-client.6bf18d2d1f5b485c94c8988bb03ad0e7"
```

When using amazon cloud drive.

The above is what it should say, I wonder what yours will say!

Thanks

Nick
 When you add the remote just leave the client id and client secret blank -  that should work (ie use rclone's) 
 What happened in 1.21 is that I corrected a bug which was ignoring those settings completely, so it didn't matter what you put in them.  So it should have never worked except for the bug!

Thanks for spending the time to work this out with me - I'll add something to the docs to try to make it clearer what those fields are for.
 I've put some wording changes in to hopefully make this clearer in future

Thanks

Nick
  You want to refer to the remote as `s3:1000genomes` - no `//` required.  Did you see that form in the docs somewhere (that was the old way)?

I'm not sure how you specify anonymous access with the S3 API (or even if it is possible), but if you have some AWS credentials you can read this repo just fine with rclone.

Here is what I have in my config file

```
[s3]
type = s3
access_key_id = secret
secret_access_key = secret
```

And here is what I typed to get a listing

```
rclone lsl s3:1000genomes
```

And like this to sync to local disk

```
rclone -v sync s3:1000genomes /tmp/1000genomes
```
 I figured out how to fix the anonymous access - I'll put this in the next release
 Anon access should work fine now.  I put a section in the s3 help showing how to set it up.

http://rclone.org/s3/

Thanks for the report

Nick
  Nice idea!  And reasonably easy to implement too.
 This is all done in 1.23 - let me know what you think!

Thanks for the suggestion

Nick
 @austinginder Thanks for doing some testing.

The above error looks like a transient problem...  Error 502 is bad gateway which means some bit of dropbox's architecture wasn't working properly - maybe the front end proxies couldn't speak to the backend servers or something like that.  Not anything rclone could cause anyway!
  Glad you are enjoying rclone!

The unix tradition is to make command line tools which don't prompt by default, but to add a flag (eg `-i` for interactive) which then prompts before critical operations.  What do you think of that idea?

In the specific case of ACD rclone can't actually delete anything, only put it in the trash (as there is no API for permanent delete available) so you can always rescue it out of the trash.
  Look great! I don't know swift personally, so my comments are of a general nature.
 @Tolsi Firstly - excellent work - thank you! I've put some inline comments too

> @klauspost @ncw what you can say about sync mode and deleting necessary segments files? If I sync folder with 12Gb file 'test', then rsync create in swift files 'test/000000000' (5Gb), 'test/000000001' (5Gb), 'test/000000002' (2Gb) and 'test' (manifest file). After that rsync removes 'test/000000000', 'test/000000001', 'test/000000002' as files which are not present in the source folder. How can we avoid that? This logic presented at operations.go:460.

This is the reason that this feature becomes complicated!

I think the correct thing to do is to store the parts in a separate container.

The convention used by the swift tool is to upload into a separate container called `..._segments`.  See [the openstack large objects doc](http://docs.openstack.org/developer/swift/overview_large_objects.html) for more info

Ideally rclone would work exactly like that using a compatible set of conventions.

rclone should then delete the parts when it deletes a file with a manifest too.

> because md5 hash of manifest file isn't equals md5 of original file.

What we should do here is return an empty Md5sum when we detect a file is a manifest.  The rest of rclone understands an empty md5sum to mean that it is unknown.
 @Tolsi If you want I'll do the `meta` to `headers` change then merge your stuff on top of it in a new branch?
 @Tolsi I pushed the merged code into the swift-large-files branch.  You probably want to checkout that branch fresh and add any changes on top of it.  You can then use --force when you push to your branch (the one that this pull request is attached to).
 @Tolsi push to your branch Tolsi:stolmachev-swift-large-files and it will update this pull request

> We don't create segments containers manually, it creates automatically on first write.

Perfect

> For now remove method don't removes segments files for now on call Remove() on manifest file, I should fix it tomorrow.

OK

> Should we handle Ctrl+C cancellation of upload process and remove already uploaded segments of big file?

There isn't a framework for doing that within rclone at the moment - just leave it as a FIXME in the code for the moment.
 @Tolsi great work thanks!

I've squashed your commits and added some fixes of my own after a bit of testing.  I've pushed these to the swift-large-files branch for you to look at.

I think it is looking quite robust now. Syncs work without the `--size-only` flag. It does the right thing when files go above and below the chunk size limit.

There is one failure in the unit tests which I'll look at at some point!

Anything you want to comment on, or add?
 @Tolsi thank you very much for your review, and thank you for driving this feature forward.

I'll merge to master a bit later!
 I've merged this to master now - thank you very much for your contribution.  I'll release it in v1.22
  @ceptonit Thanks for the update.  Other people have also told me they have been having trouble with ACD being down too.
  Adding '-v' information similar to ACD should be fairly easy.
 Putting the same parallel directory listing as in ACD would speed up this phase greatly. 
 We've speeded up the directory listings greatly, and if you use -v you'll see progress of them.

Hopefully together both of these will be useful for you.

Thanks for the report

Nick
  Yes you are correct - that is an oversight in the recent oauth changes. It's easy to fix though!

Thanks for the report and glad you like the software

Nick
  Great idea!
  Shorten the URL to be used by the user and automatically use the returned code by the server. The browser is opened on `http://(bindaddress)/auth`, and redirected to the actual URL. When the code is returned it is automatically inserted, instead of requiring a copy+paste.

This is also a workaround for the "open" package, which escapes `&` wrongly on Windows, so the opened URL's are invalid.
 Nice - thanks!

I didn't do that originally because I was thinking about users which might be running the config on a server somewhere and authing on their laptop at home.  What I was imagining was that it would wait for the user to paste the code, or the reply from the server and whichever it got first it would use.

I couldn't figure out how to make an interrupt-able readline though so I left it how it was.

That use case is unlikely though, so maybe a flag would cover it.

It occurs to me now that setting the return URL for the other users of the oauthutils (drive, dropbox, google cloud storage) would make it more convenient for them too if we could solve that problem.
 > which might be running the config on a server somewhere and authing on their laptop at home. 

I considered that, but would it work, because they are redirected back to 127.0.0.1 from Amazon (or others) anyway. Unless they are into rolling their own client, it can only redirect back to that. Either way, if we get that use-case, the code is easy to modify for that case.
 @darren12345 we are just talking about amazon cloud drive at the moment which requires that you be running a local webserver to collect the oauth replies.  You can always configure rclone on a remote machine and copy the config file.

@klauspost I think the fact that you have to run a local webserver for amazon cloud drive means that my concerns are invalid so I'll merge this code.  Can you update the docs in `docs/content/amazonclouddrive.md` with the new workflow please?

Thanks

Nick
 Ah nice - hadn't seem it had already been written. Documentation updated.
 Thanks for a super feature.  I've squashed the commits, gofmted it and merged it!
  [ipfs](http://ipfs.io/) is an interesting concept, and is would seem that it could be feasible to have support for it.

Even though the reference implementation is written in Go, it should probably be implemented via the REST interface.
  A great idea, however I think this is mostly a duplicate of #59 - can you add your comments there please and close this ticket!

Thanks

Nick
  That is a good idea - thanks.  When doing an upload with rclone we always know the size of the object - telling it to the remote in a `Content-Length` header seems very sensible.  It might be a little tricky to implement though due to the layers involved.

rclone does that additional request to read the MD5SUM as well as the length so it can't stop doing it unfortunately.  Swift has very good integrity checking on upload with MD5SUMs, but not all the other cloud storage systems do.
 @nodughere Thanks for the offer of a test swift.  I wrote and maintain the [swift library](https://github.com/ncw/swift) for Go.  I'd be interested if you have access to a swift installation with v3 auth?

I note your concern about the eventually consistent system causing a problem.  In practice (and through 100s of runs of the unit tests) it doesn't except very occasionally.  If it does detect a problem like that then rclone will just retry the transfer.
  Thanks for the report.

This is a duplicate of #49 

Can you add your comments to that ticket then close this one please?

Thanks

Nick

PS very useful links - thanks!
 @mullenkamp No worries! Unfortunately there are a lot of open tickets - so many features to add and not enough time!
 @mullenkamp thanks and you are welcome!
  Implement these for Amazon Cloud Drive
- [ ] Copier - Maybe patch the parents to include multiple parents? See below
  - there isnt an API for a straight forward copy though.
  - the file has to have the same name so impractical to implement (see @breunigs comment below)
- [ ] Mover - PATCH the node to change the parent directory
- [ ] DirMover - PATCH the directory to change the parent directory
 There isn't any server-side copy, but the other two have server support.

Also `SetModTime` should be implemented, if that is possible. The file PATCH function documentation states _"The allowed fields to updates are name, labels, description"_.
 @klauspost my reading of that was that `SetModTime` wasn't possible, but if it is that would elevate ACD to first class supported file system.
 @E6L thanks for the note about the duplicate issue. 

Interesting idea about making multiple parents - yes that would make a copy sort of like taking a hard link in Unix. I wonder what delete does whether in would just remove one of the links - you'd probably have to unmatched it to delete a multiple linked file. 
 @breunigs Well spotted :-(  I think all those nasty corner cases make Copy impractical :-(  However Move is a very useful primitive.
  Thanks for that!  Will merge later today.
 I merged that in d2b537d9a1ce337b7cae9637d80c78c92030a968 - thank you very much for your contribution
 @colinn small but perfectly formed ;-)
  Thanks for the patch - that is all live now!
  That's odd! I wonder if you've got two files with the same name on Drive? Can you post a log with -v? There is a duplicate file warning you might want to watch out for. 

I don't think : in the files should cause any particular problems.
 According to some quick analysis (which could be wrong) you have these files as duplicates on drive.  Duplicate files (files with the same file name) can't exist on the local filesystem but they can on Drive.  These confuse rclone no end as it syncs one, then the other each time.

Can you check in the drive web interface to see if they really are duplicated?

Posting the results of `rclone ls "drive:My Tracks Backup"` would be very interesting too!

Thanks

```
      2 2014-02-13 Cycling.kmz
      2 2014-02-13 MediaCityUK.kmz
      2 2014-02-16 10:55.kmz
      2 2014-02-26 08:13 Walk From Briercliffe.kmz
      2 2014-02-26 14:49 Cycling With Gail.kmz
      2 2014-03-08 09:02 Walk Back From Garage.kmz
      2 2014-03-13 Drive Home.kmz
      2 2014-03-13 Drive To Work.kmz
      2 2014-03-14 Drive Home From Work.kmz
      2 2014-03-14 Drive To Work.kmz
      2 2014-03-17 Drive Home From Work.kmz
      2 2014-03-17 Drive To Work From School.kmz
      2 2014-03-18 Drive Home To School.kmz
      2 2014-03-18 Drive To Work.kmz
      2 2014-03-19 Drive Home From Work Via Tesco's.kmz
      2 2014-03-19 Drive To Work From School.kmz
      2 2014-03-20 Drive Home Via Tesco.kmz
      2 2014-03-20 Drive To Work.kmz
      2 2014-03-21 Drive Home To School Via Fuel.kmz
      2 2014-03-21 Drive To Work.kmz
      2 2014-03-24 Drive Home From Work.kmz
      2 2014-03-24 Drive To Work From Jo's.kmz
      2 2014-03-25 22:46.kmz
      2 2014-03-25 Drive To Work.kmz
      2 2014-04-10 12:22 Walk To Meet Gail.kmz
      2 2014-04-13 07:53 Biking With Gail.kmz
      2 2014-05-07 08:10 Walk From Ford Garage.kmz
      2 2014-05-17 12:17 Coldwell Reservoir.kmz
      2 2014-05-25 16:18 Walk In Lowther Estate.kmz
      2 2014-06-26 19:57 Bike Ride.kmz
      2 2014-07-03 19:04 Ride To Barrowford Locks.kmz
      2 2014-07-20 10:55 Walking.kmz
```
 Duplicated files on drive is a common theme - one which I really need to sort out properly.

If you had used Drive as the destination you would have got a warning about duplicate files- I think I need to put the same warning in for Drive as the source.

I'll re-purpose this ticket to make that happen.
 @russwilde try [rclone dedupe](http://rclone.org/commands/rclone_dedupe/) that will help!
  No rclone can't do that yet.

However it could be done I think.  Rclone would need to store a little bit of metadata - time of last sync. It could then use that for working out what to do in the delete files case.
 I've made a note about this for the FAQ - I'll leave this ticket open for future developments!
 None at the moment.  All the primitives needed are there in rclone, it just needs a 2-way sync expert to fill in the gaps!
  That is a puzzle!

I've done quite a bit of searching about this error from Ceph, and it seems that Ceph requires either `Transfer-Encoding: chunked`, or a `Content-Length`.

I've double checked using the debug facilities in the AWS SDK and I believe rclone is sending a `Content-Length`  for all PUT and POST uploads.

rclone is working with my test ceph (unfortunately I don't know which version as I'm using a donated account) and also with S3.

Some questions for you:
- is there a proxy between you and the ceph node
- how have you got RadosGW set up - are you using the recommended way with the patched apache?
- is there anything else unusual about your setup?

Hopefully we can get to the bottom of this!

Thanks

Nick
 @stuartbfox did you have any ideas on the above?  Thanks -- Nick
 When I try this with rclone 1.27 I can see a `Content-Length` header in the PUT so I conjecture this is fixed now.

If not then please re-open or make another issue.

Thanks

Nick
  I don't think this is possible with any of the cloud storage providers, unless you've got a cunning idea I haven't thought of? 

Unless perhaps you lost the idea that your are just uploading objects and you upload and store diffs. Rclone would then put the diffs back together for you on download. 

That would be a big change in philosophy though as eg you wouldn't be able to see your docs on google drive. 
 I'm going to write this up in a new FAQ entry, something like this

### Why doesn't rclone support partial transfers / binary diffs like rsync?

Rclone stores each file you transfer as a native object on the remote
cloud storage system.  This means that you can see the files you
upload as expected using alternative access methods (eg using the
Google Drive web interface).  There is a 1:1 mapping between files on
your hard disk and objects created in the cloud storage system.

Cloud storage systems (at least none I've come across yet) don't
support partially uploading an object. You can't take an existing
object, and change some bytes in the middle of it.

It would be possible to make a sync system which stored binary diffs
instead of whole objects like rclone does, but that would break the
1:1 mapping of files on your hard disk to objects in the remote cloud
storage system.

All the cloud storage systems support partial downloads of content, so
it would be possible to make partial downloads work.  However to make
this work efficiently this would require storing a significant amount
of metadata, which breaks the desired 1:1 mapping of files to objects.
 @mrschyte Thanks for the report - I've put the FAQ item live now
  @mlanner nice idea - I've wanted exactly this before when moving stuff about in Swift.  Not sure how I'd implement it - I'll have a think.
 @jamshid not a bad idea.. I wonder if it can be done in a cross platform way from go...
 An implementation idea - we pass the source object to the Put methods.  Do a type assertion at that point to break out the metadata - not too tricky.
  According to [stackoverflow](http://stackoverflow.com/questions/18672860/can-not-create-bucket-the-account-for-the-specified-project-has-been-disabled)

This can happen when the Cloud Storage service isn't turned on for your project.  Do the following:
1. Visit http://cloud.google.com/console
2. Select your project
3. Visit the APIs & Auth tab
4. Find Google Cloud Storage in the list of services
5. Turn on Google Cloud Storage

Does that help?
 If if I look in my developer console I seem to have `Google Cloud Storage` and `Google Cloud Storage JSON API` enabled. Have you got both of those?  I think that the one with JSON in is the important one!

Using the project number is correct.

If that doesn't work: Did you create the config using `rclone config`? There should be a token in `.rclone.conf` if so - can you check?  `rclone --help` will show the location of the config file under the help for `--config`.  If there isn't a token line, then use `rclone config` and run through the configuration again.
 @chuan137 did you try the above?  Let me know how it went - thanks
 I'm going to close this for the moment.  Please feel free to re-open with more comments.

Thanks

Nick
 @jaslinfernando are you having the same problem? Did you try the above steps? 
  If I try uploading a file called `.DS_Store` through the website I get "Sorry! This file name is not allowed.".  

A bit of searching and this is [the best list I can find of file names which aren't allowed](https://www.dropbox.com/en/help/145) - see Ignored files.  I've also seen people saying to avoid windows tempfile names.

Factored from #104
 @philiptzou I have verified this - thanks for the report!

I'll fix shortly

```
/tmp$ mkdir /tmp/dbtest
/tmp$ cd /tmp/dbtest/
/tmp/dbtest$ echo hello > floop.txt
/tmp/dbtest$ echo hello > .DS_Store
/tmp/dbtest$ rclone -v sync /tmp/dbtest dropbox:dbtest
[snip]
2015/09/02 08:35:42 .DS_Store: File name disallowed - not uploading
2015/09/02 08:35:42 .DS_Store: Corrupted on transfer: sizes differ 6 vs 0
2015/09/02 08:35:42 .DS_Store: Removing failed copy
2015/09/02 08:35:42 .DS_Store: Failed to remove failed copy: Path '/dbtest/.ds_store' not found
```

I think that Update probably needs to return a sentinel error which can be detected in Copy/Sync.
  Dropbox is case sensitive.

A sync which has both `file.txt` and `File.txt` will behave erratically.

Warn the user in this case.  Ideally just sync the alphabetically first one, so the sync is consistent.

Factored from #104
 @austinginder rclone relies on this property already when doing the syncs.

What I could do is set a `CaseInsensitive` flag on the dropbox backend, then use this in the `Sync` and `Copy` routines to do the ignoring there.  I think that would be the correct approach...

Potentially this could be useful for OSX which by default has case insensitive file systems too. (But the file system can be case insensitive too!) 
 @kel30a glad you got to the bottom of it.  That is probably a bug - the dropbox code is supposed to cope with directories in different cases as dropbox doesn't reliably preserve the case of directories as you've seen.  Can you report a separate issue please?

Thanks

Nick
  I think that is a very good idea.  Unfortunately the downloads server (hosted on Swift) doesn't support symlinks so I'll have to upload everything twice, but that isn't a big deal I think.

Eg, I'll upload a `rclone-v1.19-linux-amd64.zip` and a `rclone-current-linux-amd64.zip`.

This would probably be useful to the Arch AUR rclone install: https://aur.archlinux.org/packages/rclone/
 I've just released this - you'll see the new links at http://rclone.org/downloads/ in the downloads for scripting section.

Thanks for the suggestion
 @mlanner great!  I'd be interested to see your script - can you email it?  Thanks Nick
 @koen-serry can you use the `-j` flag of unzip which ignores the internal directory structure?
  Thanks for the report.

If you retry the sync it should tidy up the 56 files.

Currently rclone doesn't retry, but I'm considering a --retries flag to automatically retry for every backend.

Also I could make dropbox retry for this specific case - I'll take a look at the API docs.
 If I try uploading a file called `.DS_Store` through the website I get "Sorry! This file name is not allowed.".  

A bit of searching and this is [the best list I can find of file names which aren't allowed](https://www.dropbox.com/en/help/145) - see Ignored files.  I've also seen people saying to avoid windows tempfile names.

Of 18 errors above, there are 15 to do with the ignored file list.  That leaves 3 lock errors.  However 2 of those are to do with directories called `.DS_Store` so there is actually only 1 file missing from the sync.

Perhaps if rclone ignored the files which dropbox doesn't like that would go through too.

You could always try a more narrow sync to get it to go through, eg

```
rclone -v copy wp-content/blogs.dir/106/files/2014/01/ dropbox:wp-content/blogs.dir/106/files/2014/01/
```

Anyway I think that is two bugs now
- retry the lock fails
- add an exclude list for files dropbox can't upload
 @austinginder well done - good investigation - that sounds spot on.

rclone could give a warning quite easily.  It would be fairly tricky to pick the first alphabetically as the internals of rclone are concurrent and it doesn't actually get the listing then transfer files, it does everything all at once.

I'll make a separate ticket for that
 @darren12345 I made a ticket for `--retries` in #109 so I don't forget!
 Since the lock problem happens very rarely and is difficult to detect, I'm going to implement `--retries` #109 in favour of this ticket.

Since the other issues have been split out into #107 and #108 that leaves nothing left in this issue so I'm going to close it.

Thank you very much for taking the time to report the issues, and do follow the issues above if you are interested.
  I can see the slow uploads here too,

I'm not entirely sure this is an rclone problem though, there are lots of complaints about slow upload speeds in the [dropbox forum](https://www.dropboxforum.com/hc/en-us/community/posts/201863435-Slow-Upload-Speeds-Why-)

I'll ask dropbox developer support for their opinion on whether I can do anything about this.

Thanks for the report
 I received this reply from Dropbox support

> Thanks for writing in. Unfortunately the official Dropbox desktop client
> doesn't use the Dropbox API, and the client is substantially more
> complex and works somewhat differently, so it's difficult to draw a
> direct comparison here.
> 
> Also, note that all of the Dropbox servers are located in the US. Your
> connection speed to Dropbox depends on the routing you get between your
> ISP and our servers, and may be slower than your ISP's rated speeds.
> 
> Sometimes resetting or retrying your connection gets you a different
> route and better speeds, but that is outside of our control. Some ISPs
> also throttle sustained connections so if you see an initial high
> connection speed followed by lower speeds, that could be the reason.
> 
> We're always working on improving our infrastructure, but otherwise, as
> user or developer, there's not much else you can do to change the
> overall speeds you get, which depends on many different factors.
> 
> Sorry I don't have a more straightforward answer here!

So I think this probably isn't an rclone problem.

I tried again at work (we are an ISP) where we have a 10 Gbit/s direct to the London Internet Exchange and I got the same result a measly 64kB/s upload speed.

I'm sure that dropbox has worked faster in the past.

It seems to be limited to 64kB/s per file, so if I transfer 4 files at once I can get 256 kB/s transfer speed which is better but still not brilliant.

I'm following up with Dropbox support for more info!
 @mrschyte Dropbox suport haven't come up with anything from their end.

I'm convinced this is some kind of network problem though, rather than an rclone problem.

I tried rclone again this morning and got a slightly improved 88 kB/s.

Can you paste a [traceroute](http://blog.iweb.com/en/2009/09/how-to-do-a-traceroute-on-windows-linux-and-osx/3111.html) to `content.dropboxapi.com` so I can compare it with mine?
 [This thread](http://www.scootersoftware.com/vbulletin/showthread.php?13640-Dropbox-poor-upload-speed-and-upload-problems-with-large-files) suggests that increasing the chunk size could improve performance. I'll try that in a bit and report back! 
 Yes, that is it!  I set the chunk size to 64 MB (from 64kB) and my upload went fro 64kB/s to 1.5 MB/s.

I'll experiment with the buffer size and find the smallest sensible value.

I'll also put a flag in so users can change it.
 Testing the new flag

On a super fast Internet connection, uploading a 222MByte file

```
rclone -v --dropbox-chunk-size=2M --stats=10s copy bigfile dropbox:test
```

Record second reading and final reading

| Chunk size | Speed after 20s | At Completion |
| --- | --- | --- |
| 64k | 78.85 kByte/s |  |
| 128k | 154.41 kByte/s |  |
| 256k | 263.91 kByte/s |  |
| 512k | 419.83 kByte/s |  |
| 1M | 737.77 kByte/s |  |
| 2M | 1382.89 kByte/s | 1013.74 kByte/s |
| 4M | 1414.69 kByte/s | 2173.71 kByte/s |
| 8M | 2765.42 kByte/s | 2626.53 kByte/s |
| 16M | 3152.70 kByte/s | 3049.36 kByte/s |
| 32M | 3161.28 kByte/s | 3608.87 kByte/s |
| 64M | 5051.45 kByte/s | 3502.92 kByte/s |
| 128M | 6319.20 kByte/s | 4017.70 kByte/s |

Chunks aren't buffered into memory, so no real disadvantage making
them very big.

Choose 128M as the default.
 I've released a binary with this fix in at http://rclone.org - a massive improvement - thanks for raising the issue.
 Alas no. You can change the default chunksize with drive with a flag, but I think you'll find it's about right already. 
  Some of rclones remote fs do understand the concept of folders, eg
- drive
- local
- dropbox

Make an optional interfaces (eg `Mkdir`, `Rmdir`) for these FS to manage the creation and deletion of folders.  This would enable empty folders, and deletion of empty folders on sync.
 @mullenkamp thanks for the vote of confidence! I can see a plan coming together for this.
 I think what I might do for this is just delete empty folders on the destination at the end of the sync.

This is relatively straight forward and will satisfy nearly all cases, except for that of syncing an empty directory.
 I wrote a little script to delete all empty dirs under a remote: http://pub.rclone.org/rclone-delete-empty-dirs.py3

Run it with python3.

Use the --dry-run flag with it first to see what it does.

It isn't very efficient but should get the job done. You may need to run it more than once if you have empty directories within other directories that only have empty directories in.

I'll probably integrate the logic into rclone into a new command, maybe `rclone rmdirs` or something like that - see #831
 @lanrat that download works for me - try again!

@cemsbr 

> The lsd command in the script of @ncw fails for me. Executing it outside the script gives the message level value not supported.

That indicates you are using an old version of rclone - you'll need to upgrade.
 I've made a new `rclone rmdirs` command which will delete any empty directories (in a nested way) from a remote.

You can try it in this beta: http://beta.rclone.org/v1.34-25-gf3365dd/ (uploaded in 15-30 mins)

Use it like this

    rclone rmdirs --dry-run remote:

When happy then do

    rclone rmdirs remote:
 Oops messed up the build: try http://beta.rclone.org/v1.34-30-g943a093/ (uploaded in 15-30 mins) Third time lucky: http://beta.rclone.org/v1.34-36-g539853d/ @calisro I can confirm that.  Should be easy to fix - will post a beta here! @calisro I've fixed that in http://beta.rclone.org/v1.34-51-gcb9f1ee/ (uploaded in 15-30 mins) I've put this issue on the 1.37 milestone since all the pieces are in place now :-) @RolluS note that you can delete empty directories with `rclone rmdirs` now (note extra s).

https://rclone.org/commands/rclone_rmdirs/  A nice idea and I didn't realise rsync had that feature
 This has some similar ideas to #18
 Now most remotes can do Copy or Move/Delete this is now a practical feature to implement.

It would also require #197 and #721 in an ideal world.
 I've implemented this now - please find it in this beta.  Any feedback much appreciated!

http://beta.rclone.org/v1.35-33-g47ebd07/ (uploaded in 15-30 mins) Here are the docs for `--backup-dir`

### --backup-dir=DIR ###

When using `sync`, `copy` or `move` any files which would have been
overwritten or deleted are moved in their original hierarchy into this
directory.

The remote in use must support server side move or copy and you must
use the same remote as the destination of the sync.  The backup
directory must not overlap the destination directory.

For example

    rclone sync /path/to/local remote:current --backup-dir remote:old

will sync `/path/to/local` to `remote:current`, but for any files
which would have been updated or deleted will be stored in
`remote:old`.

 @simnether wrote:
> I am guessing files previously present in "backup-dir" will be overwritten?

Yes you are correct.  I'll add that to the documentation.

The intention is that you'd make a new backup dir for each day, or each backup, so it is up to you how granular you want the old backups to be.  I don't really want to rename the files - that would complicate the implementation. @dsrbecky wrote
>  My intention is to always set a different backup-dir based on date.

My plan is that rclone will grow a `backup` command eventually which will automate the use of `--backup-dir` which will do exactly that. @balazer wrote
> One note for your documentation, on Google Drive at least, I found that if the file in the backup-dir already exists, it will remain there alongside the moved file. This is totally fine, and I actually prefer having duplicates instead of overwriting in this case.

Hmm, that is unexpected!  That probably means I haven't thought through enough what happens if there is an existing file in the backup-dir.  @unnfav - you are right ACD complains about naming conflicts here in my tests.

I don't really like rclone creating duplicate file names.  Even though drive allows it, it causes trouble with practically everything else!  I could allow this behavior for fses which allow duplicate files I suppose, but I forsee it causing problems!

So my preferred course of action would be to overwrite the files in the backup-dir.

As for the technical mechanism... The solution is quite simple - to pass in a dst Object to `Move` if one exists and it will delete it first. 

I'll re-open the ticket to remind me to fix this. @unnfav wrote
> As for allowing duplicate naming, wouldn't it be a nightmare to do restores if various versions with the same filename exist in a directory?

Yes it would.

So what does everyone think about this plan?
  * --backup-dir will overwrite existing files when storing new files in the DIR
  * you can set --suffix to give those files a new name - by default it will be empty so files will be stored with their original name

I don't intend to implement `--suffix` without `--backup-dir` for rclone - rsync has to jump through hoops of fire to make that work with automatic filters etc.

To fix the original overwrite issue, the most efficient thing to be will be to load the metadata for the objects in backup-dir into memory.  This should be a small fraction of the objects in the destination dir which also are loaded into memory. OK Here is the next revision.  It supports --suffix and won't duplicate files in drive or cause 409 errors with ACD

http://beta.rclone.org/v1.35-40-gb6848a3/ (uploaded in 15-30 mins)

Please test and let me know how you get on - thanks!

New docs

### --backup-dir=DIR ###

When using `sync`, `copy` or `move` any files which would have been
overwritten or deleted are moved in their original hierarchy into this
directory.

If `--suffix` is set, then the moved files will have the suffix added
to them.  If there is a file with the same path (after the suffix has
been added) in DIR, then it will be overwritten.

The remote in use must support server side move or copy and you must
use the same remote as the destination of the sync.  The backup
directory must not overlap the destination directory.

For example

    rclone sync /path/to/local remote:current --backup-dir remote:old

will sync `/path/to/local` to `remote:current`, but for any files
which would have been updated or deleted will be stored in
`remote:old`.

If running rclone from a script you might want to use today's date as
the directory name passed to `--backup-dir` to store the old files, or
you might want to pass `--suffix` with today's date.

### --suffix=SUFFIX ###

This is for use with `--backup-dir` only.  If this isn't set then
`--backup-dir` will move files with their original name.  If it is set
then the files will have SUFFIX added on to them.

See `--backup-dir` for more info.
 @balazer thanks for testing :-) Whoohoo!

Thanks for testing @robjlg and @simnether .

I think I'll close this ticket now which is another one ticked off for the 1.36 release :-) @robjlg wrote:
> When do you intend to release the 1.36 version?

The plan is by 19th Feb @fboyd78 thanks for testing.  Thanks for the request!
 @theonewolf wrote
> Anything we can do to help out here? Is the interface for adding a new cloud provider well defined in your Go codebase

The process is documented here: https://github.com/ncw/rclone/blob/master/CONTRIBUTING.md#writing-a-new-backend

I've had 3 backends contributed by third parties so it is possible!

If I was doing it I would first look for a well supported go module for Box.  If there wasn't one, then I would use rclone's rest library (see the onedrive remote for an example) which is straight forward.  The most time consuming bit is turning the JSON interface into Go struct definitions.  Google rate limits uploads quite strictly. You can see rclone  retrying with the 1/10, 2/10 etc. However at the end you can see `Errors: 0` which means it was successful in the end. 

You can check this by running the copy again and it should say `Transferred : 0`
  Encoding for google drive files with Russian naming looks fine while doing ls command but after copying files to local drive I can not see russian letters anymore. Looks like some re-encoding happens during download. 
 Which OS are you using? 

Can you send me the output of `rclone -v copy` (or some of it)?

Thanks

Nick
 Drive supports full utf-8 file names which is good for all languages.

The windows API supports utf-16 which rclone uses which should also be good for all languages.

So this should work properly provided the windows support is installed.

One of the unit tests which I have run on Windows involves uploading and downloading Chinese character set files.

All that said, there could be a problem, so any help getting to the bottom of it most appreciated.

Thanks

Nick
 sorted this out, it was unix console locale settings issue. rclone works just fine
 @okhrustov excellent - thanks!
 I tried to replicate your problem by creating directory trees with Cyrillic characters on gdrive and downloading them to my linux machine, but I haven't been able to.

The error you reported

> 2015/08/12 13:37:40 Operations/Clients/Отчеты/_Архив/XXXXX/Отчет_XXXX 1-31.05.2014.xlsx: Failed to open: Get : unsupported protocol scheme ""

Comes from the [go http transport](http://golang.org/src/net/http/transport.go#L209) and indicates it tried to process a URL without `http` or `https` which is really strange as the URLs used come straight from Google drive, from the directory listings.  It probably means the url is empty for some reason.

Can you do this for me which will tell me what google drive is returning...
- Go to: https://developers.google.com/drive/v2/reference/files/list#try-it
- Click the Authorize requests using OAuth 2.0
- Agree to the access request in the popup
- Enter this into the q box: `title contains '1-31.05.2014'`
- Then click the EXECUTE button

This should return information about the troublesome file. I'm particularly interested in what the `downloadURL` is and what the `title` is. Can you email me the whole output (nick@craig-wood.com)?

Thanks

Nick
 @okhrustov thanks for sending me that.  I can see that the xlsx file doesn't have a `downloadUrl` which is strange, but consistent with the problem.

How did that file get on your drive? Did you upload it, or create it with google docs? Did someone share it with you?

This file has an `md5Checksum` but no `downloadUrl` which isn't consistent with [the docs](https://developers.google.com/drive/v2/reference/files#resource).  It should have both or neither as far as I understand.
- `downloadUrl`   string  Short lived download URL for the file. This is only populated for files with content stored in Drive.
- `md5Checksum`   string  An MD5 checksum for the content of this file. This is populated only for files with content stored in Drive.

How many other files do you see this for?
 Usually  files are uploaded to the cloud via Google Drive app for Windows /
Mac. Some files can be uploaded manually in the browser.

There are about ~50 files with such problem.

Oleg.

On Thu, Aug 13, 2015 at 2:49 PM, Nick Craig-Wood notifications@github.com
wrote:

> @okhrustov https://github.com/okhrustov thanks for sending me that. I
> can see that the xlsx file doesn't have a downloadUrl which is strange,
> but consistent with the problem.
> 
> How did that file get on your drive? Did you upload it, or create it with
> google docs? Did someone share it with you?
> 
> This file has an md5Checksum but no downloadUrl which isn't consistent
> with the docs
> https://developers.google.com/drive/v2/reference/files#resource. It
> should have both or neither as far as I understand.
> - downloadUrl string Short lived download URL for the file. This is
>   only populated for files with content stored in Drive.
> - md5Checksum string An MD5 checksum for the content of this file.
>   This is populated only for files with content stored in Drive.
> 
> How many other files do you see this for?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/ncw/rclone/issues/95#issuecomment-130636148.
 OK, with a bit of help from the Google Drive Developers forum I've got to the bottom of this.

That file has `"copyable": false`.  Consequently it has no `downloadURL`.  I suspect that file was shared with you originally and whoever shared it with you didn't allow you to copy it.

I need rclone to deal with those files a bit more sensibly as it just can't download them.
 thank you for shedding light on this
 I've reopened this as I want rclone to give a sensible error message 
 I've made the v1.18 release report this properly.

Thanks for your help
  I'll merge this shortly - thanks!
  Thanks for doing that - will merge shortly!
 I've merged this now - thank you very much!
  Thanks.  I'll merge this shortly. I'll combine this with #91
 I've merged this now - thank you very much!
  Great, thanks, will merge shortly!
 I've merged this now - thank you very much!
  This is annoyingly difficult to fix!  Will think about it.
 This could maybe be fixed by converting all paths in local filesystem to UNC paths, which could also be the fix for #124, #129 and #130.
 @dav1303 This should be all fixed in the 1.20 release.  Thank you for the report and let us know (open a new issue) if you have any more problems.

Thanks

Nick
  It might be possible to manage folders better for any cloud storage systems which understand the concept of folders (Drive is the only one at the moment) through use of some optional interfaces.
 @dav1303 managing folders would require changing the internals of rclone.  It is possible to make it work with Google Drive, but not with Amazon S3 for example, so it would require a bit of thought.
 I've made another issue to track the feature necessary here #100
  This is a known problem - see the bugs page: http://rclone.org/bugs/

Unfortunately object storage systems don't really understand directories, so it is difficult to manage them properly.  It is obvious when to create them, but not so obvious when to remove them!

Google drive is the exception here though - it does understand directories.
 I've made another issue to track the feature necessary here #100
  rclone can't do an incremental sync like rsync - that just isn't possible with any cloud storage system I know.

However some cloud storage systems (drive and s3 for definite) can resume uploads which I think would satisfy your needs.
 @felixbuenemann that does look quite straight forward - worth exploring!
 I've managed to make it work partially with rclone.  Doing some testing overnight....  I wish there were some more docs!  @felixbuenemann what you made is good though!  I note that LocalID, md5 and size are documented as part of the [java API](https://developer.amazon.com/public/binaries/content/assets/javadoc/amazon-cloud-drive-sdk-for-android/com/amazon/clouddrive/model/uploadfilerequest.html)

It might be that there are more goodies about the resume endpoint in there but I haven't found it yet!  Perhaps [this](https://developer.amazon.com/public/binaries/content/assets/javadoc/amazon-cloud-drive-sdk-for-android/com/amazon/clouddrive/model/nodestatus.html) - I'm not sure!

It would be worth downloading the java sdk source if it is available and having a look at it I think.  Post a link if you find it!
 I think there must be a bit more to the API.
- list outstanding requests - old partial uploads hang around for ages and cause 408 errors if you try to upload something witht the same name as a pending file even though it isn't visible in the directory listing
- cancel an outstanding request

I haven't manage to get the `/resume` endpoint to give me anything but a `204` error, even when uploading 5 GB files.  I think I'm supplying everything - check this out and see if I've missed something

```
2016/11/03 09:45:23 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2016/11/03 09:45:23 HTTP REQUEST (req 0xc420510690)
2016/11/03 09:45:23 POST /cdproxy/nodes?suppress=deduplication&localId=298a60807e4f1433233c2dc093f01258 HTTP/1.1
Host: content-eu.drive.amazonaws.com
User-Agent: rclone/v1.33-DEV
Transfer-Encoding: chunked
Authorization: XXXX
Content-Type: multipart/form-data; boundary=b9c371dbfd359cf845278e03d5b914745e7fd1dc5f3786679964dc9ac7b3
Accept-Encoding: gzip

1
-
1da
-b9c371dbfd359cf845278e03d5b914745e7fd1dc5f3786679964dc9ac7b3
Content-Disposition: form-data; name="metadata"

{"name":"z8","kind":"FILE","parents":["e7NQ_giKQFmKg_wWvvhDvQ"],"size":7,"md5":"e561f9248d7563d15dd93457b02ebbb6"}
--b9c371dbfd359cf845278e03d5b914745e7fd1dc5f3786679964dc9ac7b3
Content-Disposition: form-data; name="content"; filename="z8"
Content-Type: application/octet-stream

potato

--b9c371dbfd359cf845278e03d5b914745e7fd1dc5f3786679964dc9ac7b3--

0

2016/11/03 09:45:23 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
```
 @felixbuenemann great work.  I finally understand how you are supposed to use the API.  You are supposed to chunk the uploads into multiples of 5243915 and do the uploads in lots of parts.  After each chunk you'd do a query to /resume to make sure it got there OK and send the next part.  When you send the past part you get the 200 OK message and at that point the /resume endpoint no longer works (that is why my testing earlier failed).

This means that you can send the file in chunks and retry only one chunk when things go wrong.

I'm not sure what the best strategy for rclone to use might be.  Given that the internals of rclone stream files, whatever we do we will end up buffering things in memory.

So we could upload large files in 50MB chunks buffered in memory with a nice interface for retrying the chunks.  This is probably how the interface is intended to be used.

Or rclone could do what it does now, but not upload the last 10MB (say) of the file.  It could then poll the /resume node until things looked sensible, then upload the last 10MB which hopefully wouldn't give the dreaded 408 timeout message.
 @felixbuenemann 

> No, you can still query the resume endpoint after a download is completed
> This doesn't seem to work for me.  If I do the upload in one chunk then `/resume` only ever returns 204. 

What I was hoping is that I could use the `/resume` endpoint to work out what is happening when we get a `408 GATEWAY TIMEOUT` response - but in this case all I'm getting is the 204 response.  I've tried waiting for 100 seconds at a variety of file sizes.

However if I leave off the last 1k of the data, then I immediately get a sensible result from the `/resume` endpoint so I know my code is working.

Here is a log I made - take a look and see if you think I'm doing something wrong... [acd-resume-test-nonull.txt](https://github.com/ncw/rclone/files/576563/acd-resume-test-nonull.txt)
 @felixbuenemann - abort the upload at the start, maybe after sending 0 bytes - excellent idea thanks - I'll give that a go.
 @felixbuenemann @olihey I've spend some time experimenting with the resume feature.  I have got it to work using @olihey 's instructions - thanks.

I've realised that choosing the localId as being an md5 of the file name is what is causing the problems I have re-uploading files - when I re-upload the same file.

Also I've also realised that rclone filtering PENDING nodes out of the listings is causing the 409 Conflict erorrs.

The immediate problem I'm trying to solve is not having to wait for big files to appear after uploads.  I have a proof of concept which uploads 5MB of data with resume info, queries the resume endpoint then uploads the rest of the data.  When it gets a 408 timeout then it can check the resume endpoint and wait for the file to complete.

This is how http://pub.rclone.org/rclone-v1.34-05-gd2e33fd-acd-resume-test.gz works.  It probably has loads of bugs though!  Try with `-v --dump-headers` to see what is going on - use `--dump-bodies` if you are feeling brave!  It only does the above for new files not for overwrites.

However it occurs to me that I can solve this problem in a simpler way potentially. Letting rclone 'see' nodes in PENDING state should allow rclone to verify that they've been uploaded much quicker.  I'm going to experiment with this next!
 @olihey I haven't actually implemented the resume bit yet!  However I have made a plan to implement resume under ACD and to fix the waiting for big uploads now that the individual bits are tested.

For files bigger than a certain size (TBD - say 1GB)
- When starting upload check resume status of LocalID
- If found then
  - discard that many bytes from the stream
  - possibly might be able to seek
- resume the file

LocalID should be made of
- a hash (either MD5 or SHA1)
- the file size
- the file name
- If can't get hash then should refuse to resume
  - it needs to guarantee the file is the same

For errors at the end of a big upload (eg 408 timeout)
- Allow PENDING files to show in the listing
- Can then see immediately whether the file is worth waiting for
- Can wait for it to be the right size or not
 @felixbuenemann  - good idea looking for the PENDING node...

Note that rclone checks the md5 sums after copy anyway which will be more efficient than trying to calculate it in advance (where we need to read the file twice - rclone calculates the md5sum as it goes along) so we don't need to put the md5 metadata in at all if we don't want to.
 @felixbuenemann I've pushed the rclone changes to an `acd-resume` branch and the corresponding changes to go-acd to a branch of the same name.  None of my experiments were entirely satisfactory so I'd really appreciate some help :-) @felixbuenemann 

I forgot to post my outline of a plan...

For files bigger than a certain size (TBD - say 1GB)

  * When starting look for an existing node (we do this anyway)
  * If found
    * work out whether to resume
      * if it is in state PENDING
      * get the resume info
      * if the size from the resume block matches
      * check md5 if it is in the resume block
  * If no node found then use a random LocalID
    * if --acd-checksum (or different flag?) used add an md5 to the upload parameters
  * If resuming
    * discard that many bytes from the stream
    * possibly might be able to seek
    * resume the file

LocalID should be made of

  * a hash (either MD5 or SHA1)
  * the file size
  * the file name
  * If can't get hash then should refuse to resume
    * it needs to guarantee the file is the same

For errors at the end of a big upload (eg 408 timeout)

  * Allow PENDING files to show in the listing
  * Can then see immediately whether the file is worth waiting for
  * Can wait for it to be the right size or not
 @felixbuenemann one multipart part sounds reasonable 

As for local id - on a restart of rclone it would be nice to do a retry on a partially upload file.  @lastb0isct I would like to do resume for ACD in particular since it doesn't have a chunked upload...

@felixbuenemann - you were having a look at this weren't you?  (Sorry for the delay in replying - I've been on vacation)

If you run the big sync again does it still copy all the files or does it work properly?

I wonder if it could be something like your computer changing timezone, or going to DST...

Are you 100% sure you used the same destination for the `sync` and the `copy`?

If you use the -v flag to rclone it will print out a great deal of info, including why it is copying a file

Eg

```
 "Sizes differ"
 "Modification times differ by %s: %v, %v"
 "Size and modification time the same (differ by %s, within tolerance %s)"
```

If you can still reproduce the problem, examining those log lines would be very helpful and if you can paste a representative sample that would be great!

Thanks
 @AntoineGR Thanks for trying!
 I'll close this now - please re-open if you can reproduce.

Thanks

Nick
  I'm afraid I don't know very much about IAM roles.  If @zeshanb idea works I'd be happy to write it up in the docs.

This would probably be made easier by the branch I have for using the official AWS API...
 Thanks for having a go

-- Nick
  What happens if you run it again - does it complete properly?  The second time you run it it should only do an incremental.

Another thing to try might be to set `--checkers=1` and `--transfers=1` to see if that makes a difference.
 In https://www.dropbox.com/developers/datastore/docs/http it states that the maximum size of a datastore is 10MiB which corresponds exactly to that `10485823L` number above.  I hadn't noticed that limitation before :-(

How many files are you trying to copy? That would be a useful number to know.

I think the solution for you is for me to implement a flag to not use the datastore which will have various minor consequences for rclone metadata but it should fix actually transferring the files.

At the very least I need to document this limitation.

See #55 for some background.

Thanks for doing the tests

-- Nick
 OK that explains it - thanks.  You'll need the fix to sync this folder...
 @darren12345 No, --size-only doesn't ignore the datastore :-(  I'm back from vacation now and I'll fix this soon!
 This should be all fixed in v1.18!
 @austinginder excellent - thanks for letting me know!
 @allanlaal I think that is a different issue. Perhaps related to https://www.dropboxforum.com/t5/API-support/The-remote-server-returned-an-error-414-Request-URI-Too-Large/td-p/192840

Can you make a new issue please with a log with '-v --dump-headers --retries 1'

Thanks 

Nick   Great stuff thank you!  Will merge shortly...

Thanks

-- Nick
 I've merged this - thank you for your contribution!  It will appear on the website when I make the next release which should hopefully be this week.
  Can you re-run with `-v` and send me the log please?  You can use the `--log-file` option to save the log to a file, eg `rclone -v --log-file=C:\rclone.log copy/sync ...`

Thanks

Nick
 @deajan Thanks very much for that link. According to later on in that thread it looks like the specific bug referred to was fixed by google in December 2014 though...

This might also be to do with the library not renewing the token quick enough which should be fixed in #102 when it is released (shortly with v1.20)
 I'm going to assume that the fixes in #102 fixed this since no-one has reported this recently.
  I'd say you have a file bigger than 2 GB and that you are running on a 32 bit machine.

It will work if you try it on a 64 bit machine most likely.

However that does look like a bug

In `github.com/stacktic/dropbox/dropbox.go` it looks like the below should be an `int64` instead of an `int`.

``` go
type Entry struct {
    Bytes                int       `json:"bytes,omitempty"`        // Size of the file in bytes.
```

I'll investigate further - thanks.
 You are right, it is a bug in the dropbox library I'm using.  I'll submit a fix to the author, but it will require a breaking interface change so I don't know how well it will go down.
 Upstream issue stacktic/dropbox#19 created
 Proposed fix for the above stacktic/dropbox#20
 This has now made it to the v1.18 release

Thanks for the report
  As long as you are using `rclone copy` not `rclone sync` things will go fine.

I assume you aren't copying the same files, but if you are the latest uploaded should win.

I wouldn't expect rclone or google drive to get confused, but if it does make an issue!
 > By the way, I was not sure if the sync option was valid for use with Drive because I did not see it documented and in others storage systems like S3 I saw it documented.

`rclone sync` does work on drive!  Those are examples in the docs, rather than what each system can and can't do - maybe I should make that clearer.

> To set up the same Google Drive account on other servers is necessary to obtain a different token from Google or would it be sufficient copying the [remote] block to each configuration file?

You should be able to just copy the config - that is what I do, though I haven't tried it from two different locations simultaneously so it is conceivable drive might complain about that.
 Thanks for doing that testing.  I'm not suprised that running two rclones at exactly the same time causes duplicates - rclone traverses the drive at the start of the run so won't actually notice new files added during the run.

Note that you can examine the duplicates with `rclone lsl` and `rclone md5sum` that will print the duplicates.

I should probably revise my original advice into the FAQ, something like this.  What do you think?

---

### Using rclone from multiple locations at the same time

You can use rclone from multiple places at the same time if you choose different subdirectory for the output, eg

```
Server A> rclone sync /tmp/whatever remote:ServerA
Server B> rclone sync /tmp/whatever remote:ServerB
```

If you sync to the same directory then you should use `rclone copy` otherwise the two rclones may delete each others files, eg

```
Server A> rclone copy /tmp/whatever remote:Backup
Server B> rclone copy /tmp/whatever remote:Backup
```

The file names you upload from Server A and Server B should be different in this case, otherwise some file systems (eg Drive) may make duplicates.
 The FAQ is in preparation - I haven't put it up yet! 
 See the FAQ here!

http://rclone.org/faq/

Thanks for the report
  This is the same issue as #53 I think.  We are currently working  on a fix #65 , but it is proving trickier than we first thought!

I put a note about this in the [dropbox docs](http://rclone.org/dropbox/) recently.
 This should be fixed by #65 - let me know if it isn't.

Thanks for the report

Nick
  I've recently added some docs on this issue. Check the limitations section in http://rclone.org/drive/

You should find that rclone retries the copy and it works fine. If you get some errors at the end of the sync then just retry the rclone command. 
  I will look into this at the weekend - thanks for the report!
 Ok I've had a chance to think about this now.

Unfortunately dropbox doesn't store MD5SUMs of objects - rclone has to upload them into an ancillary database which dropbox call a datastore.  This means that this is happening
- you upload file with rclone - db is correct
- desktop client uploads a change to the file - db is now out of date
- you download the file with rclone - db is out of date - rclone complains about corrupt MD5SUMs

So I think this is expected behaviour (if not very helpful).

Possible solutions
- A work-around would be to make sure rclone syncs to a different directory to the desktop client.
- Alternatively I could add a flag to rclone to tell it to ignore the MD5SUMs and download stuff anyway.
- Or radically stop storing MD5SUMs and mtimes in the datastore.  I might have to do this anyway as dropbox are dropping support for datastores.

At minimum I need to add stuff to the dropbox docs explaining the situation!

Any further ideas?

Thanks

Nick
 > I don't suppose there's a way of forcing rclone to generate a hash on a file that is stored in Dropbox without downloading it first?

Alas, no.

> I suppose ideally you'd want a switch that will tell rclone to either ignore hashes, mtimes or both when syncing.

Rclone recently gained a `--checksum` flag which only uses the checksums not the md5sums,  this speeds up transfers between two remotes (eg s3 and swift).

I'll implement a `--size-only` flag which just checks the size when syncing.
 I've updated the dropbox docs and implemented the `--size-only` flag in v1.15 which should work around your problems.

Thanks for the report and please re-open the ticket if you find a problem with the fix.
 `rclone --size-only` is what I would write, but `--size-only=true` is a valid alternative.

I can confirm that `--size-only` doesn't fix your problem - I made an exact reproducer in my dropbox account - something I should have done first :-(

re-opening the ticket for a better fix!
 I'm just about to upload a new version to fix that - please re-open the ticket if there are still problems!  

Thanks

Nick
 Try go get -u -v -f ./... in the rclone directory to update rclone and its dependencies
  Thanks for the pull request!

Can you patch the docs please too? You'll need to update the help output in  `README.md` and `docs/content/docs.md`

If you can tidy up those small things then I'll merge it :-)

Can you update this commit please rather than sending a new pull request.  Suggest you
- do the fixes and make a commit in your branch
- rebase to master
- squash all your commits down to one
- push with --force to overwrite the commit here

Thanks

Nick
 PS nice test!  Well done for getting your head round the testing framework :-)
 > However because of #73 I'm unable to build this locally so I confess that I haven't been able to run tests on this since rebasing.
> 
> I suspect that there is something I can do locally to get round #73 like delete a load of locally fetched packages and run go get or something?

`go get -u -v -f` should fix the not up-to-date packages

Sorry I didn't realise that was your problem, I thought you were just noting that you couldn't go back in time to old versions
 Your patch looks great :-)

I'll give it a bit of testing then merge it over the weekend.
 I've merged your change as you'll see in the master.  It will be in the next rclone release (this weekend) - thank you very much!

Nick
  rclone had quite a few dependencies and I don't (yet) version these alongside rclone, so if the dependencies change then you'll see errors like the above.

It isn't entirely satisfactory I know, but I haven't felt like doing the whole vendoring thing until the community works out which direction it is going it!

[Here is part of the discussion](https://groups.google.com/forum/#!msg/golang-dev/nMWoEAG55v8/iJGgur7W_SEJ)

So in summary, the above is unfortunate but expected.
 @zeshanb it is to do with the fact that google changed the API of their go cloudstorage module recently rather than any config change
 `go get -u -v -f` in the rclone source directory to update the stale dependencies should fix the build for the latest checkout `git checkout master`.

The older versions don't build because the libraries they depend on have changed.
 I should read my own docs - see the RELEASE.md file!  It should be `go get -u -f -v ./...`

If you haven't got `-f` then you are using go 1.3, so leave it out `go get -u -v ./...` (or upgrade to go 1.4)

If you really can't get the upgrading to work, then delete `$GOPATH/src/google.golang.org/api` and then `go get google.golang.org/api/...`

Hope that helps!
  Thanks for the pull request - definitely on the right track :-)

As for tests... The unit tests are a bit tiresome as they depend on each other...

I would edit `fs/operations_test.go` `TestSyncAfterChangingModtimeOnly` function to set the flag do a sync and check that nothing copied over, then unset the flag and go into the sync code that is there already.

Hopefully that is enough of a hint - let me know if you need more!

Thanks

Nick
 Obsoleted by #74
  Thanks for the pull request / discussion point!

If your sole aim is to fix #61 then implementing that would probably be much simpler.

The simplest possible thing would be to implement (from the rsync man page)

```
-c, --checksum              skip based on checksum, not mod-time & size
```

That should fix #61 - I'll write that in that ticket

---

I think you are aiming at this with your `--bloblist` flag?

```
--files-from=FILE       read list of source-file names from FILE
```

(I've been trying to keep the concepts and command line flags as similar to rsync as possible)

Though I'm not sure you'll need it after #61 is fixed?

That should be a separate PR fixing this ticket.

---

What do you think of the above?

Thanks for spending the time to make a pull request.

Nick
  Sounds very sensible, and if you want to have a go I'd be happy to review the pull request.

Thanks

Nick
  Thanks for your report.

I just tried this on linux and it does preserve the timestamp when I tried `rclone sync local1 local2` and used `ls -l` to check the timestamps.

Can you tell me
- which OS you are using?
- which filing system?
- how you tell the date and time isn't preserved?

Thanks

Nick
 Ah ha!  As far as rclone is concerned directories don't actually exist, which means
- timestamps aren't preserved
- it doesn't remove empty directories when it should

I'd like to fix the second of those, but the first I don't have a plan to fix.

I'll add something to the docs to make it clear the limitations.

Thanks for the report

Nick
 I've updated the rclone docs trying to explain these issues a bit better.  See http://rclone.org/bugs/

Thank you very much for the report

Nick
  You probably have an old version of the `github.com/ncw/rclone/googlecloudstorage` library - this changed recently.

Try 

```
cd $GOPATH/src/github.com/ncw/rclone
go get -u -f -v ./...
```

and that should update all the dependencies.
 Ah, you are using go 1.3 - in that case leave the `-f` flag off `go get -u -v ./...` should do it.

Alternatively install the latest go which is really easy using [godeb](https://github.com/niemeyer/godeb) which will download the latest source release for you and build it into a `.deb`.
 What I do is make sure that $GOPATH/bin is on the path, eg

```
$ echo $GOPATH
/home/ncw/Code/Go

$ echo $PATH
/home/ncw/Code/Go/bin/:/home/ncw/bin:etc...
```

I don't think there is a way of placing the binary directly in `/usr/local/bin` though.

You can make the man page with `make rclone.1` which uses `pandoc`. You'll have to copy it to a standard place.

It would be quite easy to package up rclone into a .deb - I've never quite got round to it though!
 In 5835f15f21178a2c8f9a6f8e7e4aaff372764410 some linux install instructions were added.

I've also improved the man page somewhat, but that IMAGE thing is still there!

I would like to package this up as a deb some time, but I don't have time at the moment.

I'm going to close this ticket as it has now gone off topic.

If you'd like to open specific focused tickets about changes you'd like I'd be grateful, or even better send some pull requests.

Thanks for your contributions

Nick
  Thanks for the report.

This is a known problem see #28 which I haven't managed to get to the bottom of yet!

Can I check which version of rclone you are using?

It is possibly the retries which cause the problem, and it is possibly Drive being inconsistent at the time that the retry is done and not showing the file that has just been uploaded, causing rclone to make a new one.

To fix the problem I really need a reproduction of the problem on my laptop, but I haven't managed to make that yet.

If you could help with that that would be really useful.

Thanks

Nick
 Very interesting - thank you.

Drive seems to have some quite serious rate limiting.  It will take large files very quickly but it doesn't like lots of small files.  In my testing the limit is about 2 per second which agrees quite well with your initial transfer.

The second transfer cleaned up the errors as expected, then in that and subsequent transfers it is uploading the same file again and again, duplicating it each time.

The problem almost certainly ocurred during the initial transfer.

So
- rclone doesn't think that file has been uploaded for some reason
- so it transfers it again
- however it was there so becomes duplicated

Once you have a duplicate then I can see a mechanism for you getting another on each transfer.

Questions
- That file has a non ASCII character in it.  Is it the only one or are there others with that character in?
- Can you do an `rclone lsl remote:Backup/ps382038.dreamhostps.com` and grep out the lines which mention that file and post them here?  I'd expect there to be 5 or so.
- Can you use the drive web interface to verify that the there are the same number and sizes of that file? I'm hoping there will be one more...

Thanks

Nick
 Thanks for the update - look forward to seeing the rest of the answers :-)

In the mean time

> 2 Kbytes per second which agrees quite well with me initial transfer?

That is 2 **files** per second max, not 2 kBytes/s.  Your initial transfer was

```
Transferred:  23384
Elapsed time: 3h12m23.442490003s
```

Which is 2.03 files / second
 @dbareiro Thanks for you replies and testing!

> > Drive seems to have some quite serious rate limiting.
> 
> Let me see if I understand. This limitation has nothing to do with the implementation of rclone itself, but will be a limitation that would affect any client attempting to connect to Google Drive, right?

That is correct.

> In the latter case, I think it would be impractical try to use Google Drive as a repository for transfer of high volume of data. That is, now I'm trying to transfer the backup of only two sites (2.5 GB). But if I wanted to transfer the backup of all sites (30 GB or so), transfer times would be extremely high and would make this impractical.

Note that it is only the upload of new files that appears to be rate limited.  So if you are repeatedly copying the same filesystem structure then rclone will only copy the differences and this will be quick provided not too many files have changed - that is what I'd recommend.

Note that I'm planning to make a `--retry` flag which will automatically retry if there were errors in the transfer which would automate this process.

I'm halfway through making a server side copy command which would mean you could do bandwidth efficient daily backups like this (without downloading everything and uploading it to copy it which is what will happen if you try these commands with the current rclone!)

```
# Do the sync - this will be an incremental
rclone sync /path/to/directory drive:backup/current
# Make a copy with todays date
rclone copy drive:backup/current drive:backup/201505021
```

> Although perhaps there is some other storage system (compatible with rclone) which does not have this strong limitation on the transfer rate that has Google Drive.

None of the other rclone backends have this limitation. There are loads of swift providers (Memset Memstore, Rackspace Cloud Files, Runabove, etc) which are reasonably priced.  There is also Google 
 Cloud Storage and Amazon S3.

---

Thanks for trying out with `--transfers=1`.

---

The mostly likely cause of duplicate files does seem to be the non ASCII characters...  rclone can handle UTF-8 characters just fine, but it looks like you have a character in the latin1 encoding since it isn't showing properly in my browser...

**I've managed to replicate the problem**  - attempting to transfer an invalid utf-8 filename seems to cause the problem.  So what is happening is your filesystem encoding for file names is set to latin1 - however rclone assumes it is utf-8 so is reading what is invalid utf-8 strings and sending them to drive.  Drive doesn't like them so it converts them into the invalid unicode character '\ufffd'.  So when rclone reads the files back the name it is expecting isn't there and it uploads it again.

Solutions for rclone...
- rclone can do this substitution itself - that will fix the problem however it is losing the meaning of the latin1 encoded files
  - [encoding of file names](http://unix.stackexchange.com/questions/2089/what-charset-encoding-is-used-for-filenames-and-paths-on-linux) is a bit of a mess on linux
  - [mac encoding](http://stackoverflow.com/questions/9757843/unicode-encoding-for-filesystem-in-mac-os-x-not-correct-in-python)
  - [windows](http://stackoverflow.com/questions/2050973/what-encoding-are-filenames-in-ntfs-stored-as)

I need to check whether Go translates windows/mac filenames into UTF-8 (I think it does).

Which platform are you using? Win/Mac/Linux?  And where was that file created?
 I have fixed this issue in v1.14 (hopefully!)

Can you give it a test and re-open the ticket if necessary

Thanks for helping me narrow it down

-- Nick
  Fantastic contribution - thank you!

Here are a few things I've noticed.  Can you fix and update the pull request?

Thank you very much

Nick

## dropbox tests

I couldn't get the tests to pass.

```
--- FAIL: TestFsListRoot (13.65s)
    fstests.go:225: Didn't find "sw8qmkqzec0dgwtgehdsmh4mev4z38pc/file name.txt" (false) and "sw8qmkqzec0dgwtgehdsmh4mev4z38pc/hello? sausage/êé/Hello, 世界/ \" ' @ < > & ?/z.txt" (false) or no files (count 1289)
=== RUN TestFsListFile1
2015/05/18 20:16:26 Not found "file name.txt"
2015/05/18 20:16:26 Not found "hello? sausage/êé/Hello, 世界/ \" ' @ < > & ?/z.txt"
--- FAIL: TestFsListFile1 (1.40s)
    fstest.go:96: 2 objects not found
=== RUN TestFsNewFsObject
--- PASS: TestFsNewFsObject (43.32s)
=== RUN TestFsListFile1and2
2015/05/18 20:17:17 Not found "file name.txt"
2015/05/18 20:17:17 Not found "hello? sausage/êé/Hello, 世界/ \" ' @ < > & ?/z.txt"
--- FAIL: TestFsListFile1and2 (7.49s)
    fstest.go:96: 2 objects not found
```

To run the tests against Dropbox you need to make a remote called `TestDropbox`, then run `go test -v` in the dropbox directory.

You should also be able to run `make test` from the top level directory

```
[TestDropbox]
type = dropbox
app_key = 
app_secret = 
token = secretstuff
```

## travis tests

The travis tests failed because of your use of "github.com/stretchr/testify/assert".  This can be fixed by adding `-t` to the `go get` line in the `.travis.conf`

## misc

There is a typo a few times in `nametree.go` - `CaseInsensetive`
 I've fixed "googlecloudstorage/googlecloudstorage.go:45: undefined: storage.DevstorageFull_controlScope" - it was a breaking API change in the google cloud storage library which I hadn't updated locally yet.

You should be able to fix your problem by splitting the loop into two and scanning for directories first?
 Sorry I've been away!  I'll try to review this at the weekend.

Thanks

Nick
 The unit tests for the dropbox module work fine

```
cd $GOPATH/src/github.com/ncw/rclone/dropbox
go test -v
```

But unfortunately the integration tests fail :-(  I run these as part of doing a release (via the `test_all.sh` script).

```
cd $GOPATH/src/github.com/ncw/rclone/fs
go test -v --remote TestDropbox: --subdir
```

Any ideas?

Thanks

Nick
 Note it is the integration tests which fail

```
cd $GOPATH/src/github.com/ncw/rclone/fs
go test -v --remote TestDropbox: --subdir

=== RUN TestSizeSuffixString
--- PASS: TestSizeSuffixString (0.00s)
=== RUN TestSizeSuffixSet
--- PASS: TestSizeSuffixSet (0.00s)
=== RUN TestInit
--- PASS: TestInit (1.65s)
    operations_test.go:73: Testing with remote Dropbox root '7jbygeeds6cn1zrc0sm7h64q2mkchrew/s7rlu27g'
    operations_test.go:80: Testing with local "/tmp/rclone654934112"
=== RUN TestCalculateModifyWindow
--- PASS: TestCalculateModifyWindow (0.00s)
    operations_test.go:89: ModifyWindow is "1ns"
=== RUN TestMkdir
--- PASS: TestMkdir (1.08s)
=== RUN TestCopyWithDryRun
--- PASS: TestCopyWithDryRun (0.67s)
=== RUN TestCopy
2015/06/10 17:16:27 Not found "sub dir/hello world"
--- FAIL: TestCopy (3.15s)
    fstest.go:96: 1 objects not found
=== RUN TestLsd
--- PASS: TestLsd (0.22s)
=== RUN TestCopyAfterDelete
2015/06/10 17:16:27 Not found "sub dir/hello world"
--- FAIL: TestCopyAfterDelete (0.23s)
    fstest.go:96: 1 objects not found
=== RUN TestCopyRedownload
2015/06/10 17:16:27 Not found "sub dir/hello world"
2015/06/10 17:16:28 Not found "sub dir/hello world"
--- FAIL: TestCopyRedownload (0.49s)
    fstest.go:96: 1 objects not found
    fstest.go:96: 1 objects not found
    operations_test.go:375: Cleaning temporary directory: "/tmp/rclone654934112"
=== RUN TestSyncBasedOnCheckSum
--- FAIL: TestSyncBasedOnCheckSum (4.32s)
    operations_test.go:375: Cleaning temporary directory: "/tmp/rclone654934112"
    operations_test.go:207: We synced, though we shouldn't have.
=== RUN TestSyncAfterChangingModtimeOnly
2015/06/10 17:16:34 Not found "empty space"
--- FAIL: TestSyncAfterChangingModtimeOnly (2.07s)
    fstest.go:83: Unexpected file "check sum"
    fstest.go:96: 1 objects not found
=== RUN TestSyncAfterAddingAFile
2015/06/10 17:16:37 Not found "empty space"
2015/06/10 17:16:37 Not found "potato"
--- FAIL: TestSyncAfterAddingAFile (2.51s)
    fstest.go:83: Unexpected file "check sum"
    fstest.go:96: 2 objects not found
=== RUN TestSyncAfterChangingFilesSizeOnly
2015/06/10 17:16:40 Not found "empty space"
2015/06/10 17:16:40 Not found "potato"
--- FAIL: TestSyncAfterChangingFilesSizeOnly (3.01s)
    fstest.go:83: Unexpected file "check sum"
    fstest.go:96: 2 objects not found
=== RUN TestSyncAfterChangingContentsOnly
2015/06/10 17:16:42 Not found "empty space"
2015/06/10 17:16:42 Not found "potato"
--- FAIL: TestSyncAfterChangingContentsOnly (2.66s)
    fstest.go:83: Unexpected file "check sum"
    fstest.go:96: 2 objects not found
=== RUN TestSyncAfterRemovingAFileAndAddingAFileDryRun
2015/06/10 17:16:43 Not found "empty space"
2015/06/10 17:16:43 Not found "potato"
--- FAIL: TestSyncAfterRemovingAFileAndAddingAFileDryRun (0.66s)
    fstest.go:83: Unexpected file "check sum"
    fstest.go:96: 2 objects not found
=== RUN TestSyncAfterRemovingAFileAndAddingAFile
2015/06/10 17:16:46 Not found "empty space"
2015/06/10 17:16:46 Not found "potato2"
--- FAIL: TestSyncAfterRemovingAFileAndAddingAFile (2.93s)
    fstest.go:83: Unexpected file "check sum"
    fstest.go:96: 2 objects not found
=== RUN TestLs
--- FAIL: TestLs (0.23s)
    operations_test.go:331: empty space missing: ""
    operations_test.go:334: potato2 missing: ""
=== RUN TestLsLong
--- FAIL: TestLsLong (0.26s)
    operations_test.go:347: empty space missing: ""
    operations_test.go:351: potato2 missing: ""
=== RUN TestMd5sum
--- FAIL: TestMd5sum (0.23s)
    operations_test.go:363: empty space missing: ""
    operations_test.go:366: potato2 missing: ""
=== RUN TestCheck
--- PASS: TestCheck (0.00s)
=== RUN TestFinalise
--- PASS: TestFinalise (1.74s)
    operations_test.go:375: Cleaning temporary directory: "/tmp/rclone654934112"
FAIL
exit status 1
FAIL    github.com/ncw/rclone/fs    28.126s
```
 No worries!  The above is what I noticed when merging your code and running the integration tests.
 Looks good to me!

I've merged it - thank you very much, and it wll be in the next release.

Note that I rebased, squashed and gofmted it.

Thank you very much for your contribution

-- Nick
  This is already an issue in #45 - can you add yourself to that issue to be notified of progress!

Thanks

Nick
  You can have any characters at all in a drive file name.

There are quite a few characters you can't have in a windows file name ([see ms docs for more info](https://msdn.microsoft.com/en-us/library/aa365247))

Linux/OS X are much more forgiving - it is only `/` and NUL that you can't have in a file name.

One thing I could do is map the forbidden characters to unicode equivalents, eg I could map `/` to the unicode division slash which looks pretty similar and is allowed in filesystems.

```
/ => ∕
```

Or I could just replace all characters which can't be represented on the current filing system with `_` which would give the same result as the google drive sync.

This would mean making each fs declare which characters it couldn't deal with (or possibly whole names as (for example) `con` is an invalid file name in windows)
 This was partially fixed in f50f353b5d7e76d8f666ac1383f5c23021b6ea34 the `:` gets escaped as `_` for windows file systems.

The path separators `/` are still an issue though - this needs to be fixed in the drive code.
 I'm planning to fix all this properly for 1.37 @aldones I have a plan to fix all of this which will probably take some time to implement!

In the mean time I'd replace `/` with `／` which is Unicode Character 'FULLWIDTH SOLIDUS' (U+FF0F) then I can make these transforms reversible.

Go is really easy to pick up for C programmers - the major thing that takes getting used to is the type declarations are backwards.  [The go tour](https://tour.golang.org/) is an excellent place to start for experienced programmers.  I think of Go as being mostly a super C with no memory management, closures and great concurrency.

To change this in `drive/drive.go` would do  `/` to   `／` in the `ListDir` function where it says `remote := job.Path + item.Title` (just change `item.Title`) and  `／` to `/` in the two places where `&Object{}`s are created.  So adding 3 [strings.Replace](https://golang.org/pkg/strings/#Replace) would do it probably!
 Looking at this again I think there is only one place that needs patching as all the `item.Title`s ultimately come from the `list()` function.  This should fix the can't find the directory problems too.

Untested:

```patch
diff --git a/drive/drive.go b/drive/drive.go
index 3a8aa0f1..32f92a7e 100644
--- a/drive/drive.go
+++ b/drive/drive.go
@@ -273,6 +273,7 @@ OUTER:
 			return false, errors.Wrap(err, "couldn't list directory")
 		}
 		for _, item := range files.Items {
+			item.Title = strings.Replace(item.Title, '/', '／', -1)
 			if fn(item) {
 				found = true
 				break OUTER
``` I have done a revised patch which I have actually bothered to test this time!

Please try the binary here

https://pub.rclone.org/v1.36-199-g89d5491a-drive-slash%CE%B2/

(or on drive-slash branch)

It seems to work for me!

```
$ rclone ls drive:slash_test/
       92 z／slash
       92 has／slash/z

$ rclone ls drive:slash_test/has／slash
       92 z

$ rclone sync drive:slash_test /tmp/slash_test

$ tree /tmp/slash_test/
/tmp/slash_test/
├── has／slash
│   └── z
└── z／slash
```

What do you think?  I've merged this to master now - you can find it here

https://beta.rclone.org/v1.36-209-gb44d0ea0/ (uploaded in 15-30 mins)

In an idea world this would have tests, but I only have integration tests for each remote and this doesn't fit into the framework.  That would be quite a good performance optimisation...

For some remotes md5sum is easy to calculate (eg swift) and some it isn't (eg local filesystem), same with reading the modification time.

In the case of s3 and swift the md5sum is easy to fetch, whereas the modification time is harder as it requires an extra transaction - hence the difference in speed.

To implement the above I'd need to make each remote express a preference for md5sum/modification time or no preference, then `copy` and `sync` could take note of these.

I'll have a think - thanks for the idea.
 The simplest possible thing would be to implement (from the rsync man page)

```
-c, --checksum              skip based on checksum, not mod-time & size
```

Then you could use this flag for copying between swift and s3 directly
  rclone only cares about objects, directories don't actually exist on most cloud storage systems (eg s3, swift), so you are correct empy directories will be left behind.

I haven't thought of a satisfactory way round this yet though!
 I've updated the docs about this issue here

http://rclone.org/bugs/

I'm going to close this issue as I don't have a plan for changing this at the moment.
  I see.  Wouldn't be too hard!  I'd probably make an `--add-header` flag which all the remote storage systems could use not just s3.

Thanks for the suggestion.
 **NB** this additional header only wants to be set on the operation that actually uploaded the file - not on all the other operations (eg listing directories etc)
 I'm all in favour of a general purpose option to add headers...  Anyone fancy sending in a PR?
 rclone now has all the machinery to make this relatively straight forward.

I would implement two flags which could be repeated

  --upload-header "Header: String"
  --download-header "Header: String"

Which would be applied to Uploads (specifically Object.Update or Fs.Put) or Downloads (specifically Object.Open).

These would need to be applied as options (there is a general purpose HTTPOption already). A bit of work would need to be done in each remote to make this work.  The options would be applied in the Copy primitive (and possibly elsewhere - this might need factoring).  Thanks for the pull request!

This has been under discussion in #39 this was the wording that we came up with there.

```
Sync the source to the destination, changing the destination only.  Doesn't transfer
unchanged files, testing first by modification time then by
size.  Destination is updated to match source, including deleting files if necessary.
Since this can cause data loss, test first with the --dry-run flag.
```

Can you update the pull request with that wording (and patch README.md too) and I'd be delighted to merge it!

Thanks

Nick
 I'm doing a release today so I'm going to add those changes in. So no need to update the pull request!

Thanks for reminding me about the wording though.

Nick
  Thanks for the pull request :-)

I don't really like the returning `nil` for any `S3Error` which is effectively what your patch does.  That would mask auth errors, server errors etc.

The docs say

> Your previous request to create the named bucket succeeded and you already own it. You get this error in all AWS regions except US Standard, us-east-1. In us-east-1 region, you will get 200 OK, but it is no-op (if bucket exists it Amazon S3 will not do anything).

So in that case no error would be reported anyway.

What error is being returned? I'd consider a patch which just added another error special case.

Thanks

Nick
 If you change your mind and want to do a special case for `BucketAlreadyExists` then I'd certainly consider that.

On the other hand waiting for GreenQloud to fix it sounds like to good option too :-)

Thanks

Nick
  Something strange is going on...  `86b051dddf5ce98625874650d15546d2-56` isn't even a valid MD5SUM.  That was reported by the source which was AWS.

This looks like it describes the problem
- http://stackoverflow.com/questions/6591047/etag-definition-changed-in-amazon-s3

So I would guess that you uploaded the file using a multipart uploader into S3.  rclone is expecting that it was uploaded in a single part and so has an MD5SUM for an Etag.

I could blank the md5sum if I detect one with a `-` in, which would cause that check to be skipped which is probably what you want.

What do you think?
 I have replicated this by uploading a file with `s3cmd`

```
s3cmd --multipart-chunk-size-mb=5 put bigfile  s3://testbucket

rclone md5sum s3:testbucket
492cb23b77fdf5abbd91e67239457f26-13  bigfile

rclone -v copy s3:testbucket /tmp/testbucket
...
2015/05/09 10:30:37 bigfileb: Corrupted on transfer: md5sums differ "492cb23b77fdf5abbd91e67239457f26-13" vs "fdfbade8da873cc0ca8c0e3047b2ebc9"
2015/05/09 10:30:37 bigfileb: Removing failed copy

```
 And after the fix, md5sums are reported as blank

```
rclone md5sum s3:testbucket
                                  bigfile
```

Copying now proceeds just fine

```
rclone -v copy s3:testbucket /tmp/testbucket
...
2015/05/09 10:33:45 bigfile: Invalid md5sum (probably multipart uploaded) - ignoring: "492cb23b77fdf5abbd91e67239457f26-13"
2015/05/09 10:33:45 bigfile: Copied (new)
```
 I've fixed this in v1.13 - please re-open the ticket if the fix isn't correct.

Thanks

Nick
  [Datastore API is being deprecated](https://blogs.dropbox.com/developers/2015/04/deprecating-the-sync-and-datastore-apis/).

Support ends October 2015, endopoints will be removed in April 2016.

rclone uses the datastore API for storing metadata about the files (md5sum and modification date).

Need an alternative...

Ideally store the metadata on the objects, but this doesn't appear to be possible with the new API.
 @austinginder I think I might take a different approach and stop storing metadata at all for dropbox.  This would downgrade the syncs somewhat so the MD5SUMs wouldn't be checked and every sync would be a `--size-only` sync.  However it would have the advantage of making dropbox a lot more reliable and faster.

I'll try contacting them anyway and see what happens!
 I had a chat with Dropbox support and they come up with a couple of new ideas - [Parse](https://www.parse.com/) or [Firebase](https://www.firebase.com/)

I think the options are
1. Store the data in a file manually (eg in one big json file, or lots of little ones)
2. Downgrade dropbox to a size only backend
3. Use a 3rd party database to store the metadata eg [Parse](https://www.parse.com/) or [Firebase](https://www.firebase.com/)

All of these options could potentially work with other cloud providers (eg Microsoft) which don't provide MD5SUMs or Dates.

I think Option 1) would work, but I know from bitter experience that keeping that file up to date will be very complicated and it will be prone to corruption. Using one metadata file per file would work better, but that has the disadvantage that they would all appear in the dropbox interface.

Option 2) fits in well with rclone's philosophy of small and lightweight.  It would be fairly straight forward to implement and lower the barrier to entry for new cloud storage providers.

Option 3) would work too, but I'm not a big fan of using two different providers to solve the metadata problem.

My preferred choice would be option 2)
 @darren12345 yes that would work but it wouldn't be ideal as there is stuff that can get out of sync (the local .json file and the remote cloud storage)
 Yes it only compares sizes (not mtime) which is less than perfect, but for nearly all use cases it is good enough.  I hope that doesn't inconvenience you too much.

I may at some point add a DB or json file or something like that, but I decided not to for the moment.
  This isn't the only complaint I've had about rclone and cloudfiles.  I think the default timeouts may be too short (10 seconds connect, 60 seconds data) and they aren't configurable unfortunately.

I'll fix that in a new release.

My plan would be to add these flags (following rsync as usual)

It might be nice to allow these in the config file too

```
       --timeout=TIMEOUT
              This option allows you to set a maximum I/O timeout in  seconds.
              If no data is transferred for the specified time then rclone will
              exit. If unset it will use the default timeouts for the remote.

       --contimeout
              This option allows you to set the amount of time that rclone will
              wait  for  its connection to a remote to succeed.  If the
              timeout is reached, rclone exits with an error.  If not set it will
              use the default timeouts for the remote.
```
 I've released v1.13 which introduces the `--timeout` and `--conntimeout` flags which have larger defaults than the swift module used to use.

If you still see problems, then please experiment with the values using these flags and I can change the defaults.

Thanks for the bug report

-- Nick
  I forgot to write this in the docs, but dropbox is case insensitive which explains some of the above.

I tried to replicate this but I couldn't yet...

Is it just the the tools directory which appears duplicated?  Where did it come from?  Did you upload it?

Can you make a reproducer for me so I can see the problem on my dropbox account?

Thanks

Nick
 @shalupov thanks for the investigation - that link is useful.

I'll have a think on how to make rclone ignore the case in paths.

The easiest way would be just to lower case everything, but that isn't ideal as people really like the case in their file names to be correct!

The other way will involve building an internal map of paths in lower case.  This should be possible since the last component is always case-reliable.
 I've made a note about this in the rclone docs which will appear shortly at http://rclone.org/drive/
  I don't know what exactly you can store in flickr's free space, but it might be possible!

Any pointers to APIs?
 From a quick look
- could store md5sums in a tag
- has ability to set dates
- Might have to map directories to tags, or maybe photo sets

So probably could make an rclone interface...
  Did `rclone copy` produce any errors?  Drive is a little unreliable sometimes, so you might need to run the copy again.  rclone won't attempt to copy any files it has copied already, so this should be nice and quick.

rclone only copies files at the moment, it doesn't copy google documents as these aren't stored as files in drive (even though they look like they are) - that probably explains the missing things.  rclone won't create local directories it isn't creating any files, but they will show up on `rclone lsd`.
 There are a couple of problems with rclone downloading google documents.  These are
1. These files don't have md5sums so the integrity checking rclone does won't work
   - possibly could be worked around though by doing a HEAD request on the `exportLink`
2. These files can be downloaded using the `exportLinks` which are exported in the [files resource](https://developers.google.com/drive/v2/reference/files), however they are exported in multiple formats
   - should rclone just pick a format to download?
     - if so how?  User preference?
   - The file name is missing its extension - should rclone add a file version with the extension?
   - Should rclone download all the formats?
   - A quick test indicates that for a document it offers `docx, odt, rtf, html, txt, pdf` formats for export
 Server side copy / move needs to work with docs also
 @mikecentiq both good ideas - I made a ticket about it #250. Sorry for the frustrating time! -- Nick
 Thanks @TimJWatts for getting me out of analysis paralysis!  I have made an implementation which I'll put into the next release unless anyone finds any serious problems with it!

Here is a link to a beta to try

http://pub.rclone.org/v1.26-50-geb8c1b1%CE%B2/

Note that this doesn't address server side copy or move...

And here are the docs

#### --drive-formats

Google documents can only be exported from Google drive.  When rclone
downloads a Google doc it chooses a format to download depending upon
this setting.

By default the formats are `docx,xlsx,pptx,svg` which are a sensible
default for an editable document.

When choosing a format, rclone runs down the list provided in order
and chooses the first file format the doc can be exported as from the
list. If the file can't be exported to a format on the formats list,
then rclone will choose a format from the default list.

If you prefer an archive copy then you might use `--drive-formats
pdf`, or if you prefer openoffice/libreoffice formats you might use
`--drive-formats ods,odt`.

Note that rclone adds the extension to the google doc, so if it is
calles `My Spreadsheet` on google docs, it will be exported as `My
Spreadsheet.xlsx` or `My Spreadsheet.pdf` etc.

Here are the possible extensions with their corresponding mime types.

| Extension | Mime Type | Description |
| --- | --- | --- |
| csv | text/csv | Standard CSV format for Spreadsheets |
| doc | application/msword | Micosoft Office Document |
| docx | application/vnd.openxmlformats-officedocument.wordprocessingml.document | Microsoft Office Document |
| html | text/html | An HTML Document |
| jpg | image/jpeg | A JPEG Image File |
| ods | application/vnd.oasis.opendocument.spreadsheet | Openoffice Spreadsheet |
| ods | application/x-vnd.oasis.opendocument.spreadsheet | Openoffice Spreadsheet |
| odt | application/vnd.oasis.opendocument.text | Openoffice Document |
| pdf | application/pdf | Adobe PDF Format |
| png | image/png | PNG Image Format |
| pptx | application/vnd.openxmlformats-officedocument.presentationml.presentation | Microsoft Office Powerpoint |
| rtf | application/rtf | Rich Text Format |
| svg | image/svg+xml | Scalable Vector Graphics Format |
| txt | text/plain | Plain Text |
| xls | application/vnd.ms-excel | Microsoft Office Spreadsheet |
| xlsx | application/vnd.openxmlformats-officedocument.spreadsheetml.sheet | Microsoft Office Spreadsheet |
| zip | application/zip | A ZIP file of HTML, Images CSS |
 @dolabriform @michaelthwaite Curious - it works for me.

![gdocs](https://cloud.githubusercontent.com/assets/536803/12751690/8ba730c6-c9b6-11e5-9bd0-463fd3e5068f.png)

```
$ rclone ls drive:GDocs
     5570 Drawings.svg
     4297 Docs.docx
     3516 Sheets.xlsx
    27579 Slides.pptx

$ rclone copy drive:GDocs /tmp/adfsf
2016/02/02 14:06:16 Local file system at /tmp/adfsf: Building file list
2016/02/02 14:06:17 Local file system at /tmp/adfsf: Waiting for checks to finish
2016/02/02 14:06:17 Local file system at /tmp/adfsf: Waiting for transfers to finish

Transferred:        40962 Bytes (  14.49 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            4
Elapsed time:        2.7s

$ ls /tmp/adfsf/
Docs.docx  Drawings.svg  Sheets.xlsx  Slides.pptx

$ rclone copy drive:GDocs /tmp/adfsf
2016/02/02 14:06:33 Local file system at /tmp/adfsf: Building file list
2016/02/02 14:06:35 Local file system at /tmp/adfsf: Waiting for checks to finish
2016/02/02 14:06:35 Local file system at /tmp/adfsf: Waiting for transfers to finish

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 4
Transferred:            0
Elapsed time:          2s
```

Can you post a log with `rclone -v` of what happens?
 @dolabriform I can confirm that google docs aren't showing up properly when you list from the root.  I've made a ticket for it #336.  I suspect the fix for that will fix @philiprhoades problems too.

Any more comments about this to issue #336 please and I'll post a beta there when I've got to the bottom of it!

@philiprhoades wrote:

> > ```
> >   5570 Drawings.svg
> >   4297 Docs.docx
> >   3516 Sheets.xlsx
> >  27579 Slides.pptx
> > ```
> 
> None of those are native GDs are they? (GDs native files don't show
> extensions in the GDs environment).  I can only clone non-GD..

They are exported versions of the native google docs - that is why
they have extensions.
 I found a work-around `--drive-full-list=false` should fix it....
 @philiprhoades wrote

> None of these work to display two Writer files:
>  rclone -v --drive-full-list=false ls gd:CNASALERC    

Can you make sure you have permission to export these files? Right click on them in the drive web interface and select download and see if that works.

Also you say "Writer file" are these "Google Docs" files or something different?

If you can tell me how to reproduce the problem that would be really helpful!

Thanks

Nick
 Here is a beta which should work without the `--drive-full-list=true` work-around: http://pub.rclone.org/v1.27-1-g5dd387f%CE%B2/
 @robchallen wrote

> There seem to be some inconsistencies on the --drive-formats selection of export format for google docs

I've fixed this in #336 - there is a beta on that ticket for you to try - comments there please :-)
 @philiprhoades Try the binary from here: http://pub.rclone.org/v1.27-1-g5dd387f%CE%B2/ - that should be the right one. - Can you put coments #336 about that please? Thanks Nick
 @philiprhoades sorry for the confusion.  That link should link to the right code to try.

A beta's link on the website is a good idea - I'd need to make the beta's a bit differently though as at the moment I make them with individual branches of code I want people to check.

> and then compared the .odt and .ods files that DID work with the previous version and although they both open OK, the files have different cksums - not sure why they converted differently but still open OK . 

Google seems to convert the files on the fly each time, and they are sometimes different - I don't know why!  Maybe they have a timestamp in or something like that.
  It probably needs a/another directory traversal with the `sharedWithMe` flag
set in the directory listing...

Perhaps it is possible to add `(sharedWithMe or !sharedWithMe)` [docs](https://developers.google.com/drive/web/search-parameters)
 @glahue alas no! Pull requests always appreciated ;-)
 @mcoffin thanks for that
 @jacobperron I think this is a separate issue as this is about Google Drive in particular.

So can you open a new issue with the info above in please?

Thanks I think making a Pseudo directory is probably the wrong direction to go in though as it doesn't really fit the way drive works and if you work against the grain of the underlying tech then it causes you problems further down the way!

I'd make a command line flag to start with as that is easy

```go
	driveSharedWithMe      = fs.BoolP("drive-shared-with-me", "", false, "Only show files that are shared with me")
```

then in listAll do something like

```go
	if *driveSharedWithMe {
		query = append(query, "sharedWithMe")
        }
```

And see if that works.

If it does, then adding a config option as well would be the thing to do I expect... @danny8376 good point.  If it was a config option then you could have a shared with me remote and a non shared with me remote which would allow copying like that.  Perhaps a bit inconvenient. Please find the first iteration of --drive-shared-with-me in

    https://beta.rclone.org/v1.36-11-g9ab4c19/ (uploaded in 15-30 mins)

Thanks @danny8376  @lcasey001 you are correct - this won't allow you to do a server side copy...  When you say your key do you mean the API key?

What auth URL are you using?
 I tried to reproduce this with a swift cluster.  I tried passwords `test/test` and `test\test` but both of those worked fine with rclone and v1 auth.

Looking at the v1 auth code, all it does is put the API key into an http header which are allowed to have `/` and `\` in.

I see the note you are referring to in the [ceph docs](http://ceph.com/docs/v0.67.9/radosgw/config/)

> Important Check the key output. Sometimes radosgw-admin generates a key with an escape () character, and some clients do not know how to handle escape characters. Remedies include removing the escape character (), encapsulating the string in quotes, or simply regenerating the key and ensuring that it does not have an escape character.

That seems to suggest that for ceph, to use a password with `\` in you should enter it in quotes, eg `"test\test"`.  That seems like a ceph specific work-around as with swift it works fine with `\` in passwords.

So my feeling is this is a bug/incompatibility in Ceph rather than a problem with the swift client.

PS I also tested passwords with `\` in using v2 auth against swift which does use json.  That worked fine too.
 @zioproto 

I tried `test\/test` and `test/\test` and they both worked fine on a swift cluster.

Can you try my suggestion above?

> That seems to suggest that for ceph, to use a password with \ in you should enter it in quotes, eg "test\test". That seems like a ceph specific work-around as with swift it works fine with \ in passwords.

Thanks

Nick
 @lvmm Yes you are right... Would you mind making this into a separate issue please?  It isn't related to the swift keys discussed in this one.

Thanks

Nick
 @zioproto I have finally managed to replicate this.

When you got your credentials out of ceph, you probably got a json dump which looks something like this

``` json
{
    "user_id": "xxx",
    "display_name": "xxxx",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "auid": 0,
    "subusers": [],
    "keys": [
        {
            "user": "xxx",
            "access_key": "xxxxxx",
            "secret_key": "xxxxxx\/xxxx"
        }
    ],
    "swift_keys": [],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    },
    "temp_url_keys": []
}
```

Because this is a json dump, it is encoding the `/` as `\/`, so if you use the secret key as "xxxxxx/xxxx" in the above example it will work fine.

I'll add this to the docs for s3

Thanks

Nick
 There is now a section about this in the docs: http://rclone.org/s3/

Thanks for the report
  Yes you are correct.  rclone doesn't fragment files for swift so is limited to the 5 GB file size.

Some thought would need to be given to where to store the chunks and how to avoid seeing them in listings etc (in another container would probably be best).  There are some problems with the MD5SUMs of segmented files too, but it is all possible.
 @swill The limitation is in the swift object store.  It has a really clunky way of having files bigger than 5 GB - you have to upload the chunks as separate files then upload an index of them.

The problem is where to store the separate chunks - in a separate container would be ideal so the chunks don't appear in a normal directory listing.

Perhaps the thing to do would be to use the convention used by the swift tool and upload into a separate container called `..._segments`.  See [the openstack large objects doc](http://docs.openstack.org/developer/swift/overview_large_objects.html) for more info.

The [swift library you linked above](https://github.com/ncw/swift) supports all that just fine but rclone would have to do the heavy listing of making the `_segments` container, uploading the chunks etc.

I wasn't aware of the swift tool convention before - that makes the prospect of doing this a lot more appetizing!
 @ctennis I'm now using the official Amazon S3 library and also using the chunking support so it should be good for files up to the s3 maximum file size of 5TB.  
 @ctennis no patch to the swift module needed, just some code in rclone. It is a non trivial amount of code though :-(
 This feature will be released with rclone 1.22
  A good idea - thanks. It certainly looks possible...

As for encryption - my plan would be to put that into rclone as a encrypting local filesystem which would make it available for all remote file system types.
 @jbuchbinder cool - thanks! 
 Support for Amazon Cloud Drive is now live in v1.20.  It should be pretty stable, but if you find any issues with it then you know what to do!

Thanks

Nick
 @OldGlory747 Find it on the downloads page http://rclone.org/downloads/ :-)
 @OldGlory747 Gosh, those are ancient builds - I didn't even know they were there!  Did you see a link to them somewhere I need to change?
 @OldGlory747 Ah OK!  I've deleted those, so hopefully they will disappear from the google search soon. Thanks for letting me know.
  The documentation is a bit confusing - sorry! Your interpretation isn't correct you'll be glad to know. There is another ticket about this #39 and here is the new suggested wording for the docs

```
Sync the source to the destination, changing the destination only.  Doesn't transfer
unchanged files, testing first by modification time then by
size.  Destination is updated to match source, including deleting files if necessary.
Since this can cause data loss, test first with the --dry-run flag.
```

So your example would become
- Create a bunch of files
- Copy a bunch of files to Drive
- Make a new file
- Run `sync <local> <remote>`
- New file is copied to Drive
- Delete the new file locally
- Run `sync <local> <remote>`
- New file is deleted from Drive
 The file is permanently deleted using the [delete method](https://developers.google.com/drive/v2/reference/#Files)

If you wanted the file to go to the trash then I could use the trash method instead.  Maybe this should be an option?
 In the context of syncing, when you delete a file locally

```
rm local/file
```

Then sync

```
rclone sync local drive:local
```

The copy on drive will be deleted permanently, not put into the trash / bin.

I could make it put it into the trash / bin with an option - either a command line flag, or an option when the drive remote is set up.
 I'm fixing the docs in #39

I'll turn this ticket into a feature request ticket for trashing files instead of deleting them.
 @zeshanb Wow that is amazing - thanks! I'd like to put it up somewhere so I can embed it in the rclone website - I'll send you an email to discuss
 Trashing files instead of deleting them was done in #82 so I think this is all done now
  @mtello you said "google drive" in your subject but you have "google cloud storage" in your config file. If you want "google cloud storage" then follow @zeshanb advice, otherwise remake the remote using [these instructions](http://rclone.org/drive/) choosing "drive" as a source type
 I'll improve this in the next release - the chooser will look something like this

```
Type of storage to configure.
Choose a number from below, or type in your own value
 * Amazon Cloud Drive
 1) amazon cloud drive
 * Backblaze B2
 2) b2
 * Google Drive
 3) drive
 * Dropbox
 4) dropbox
 * Google Cloud Storage (this is not Google Drive)
 5) google cloud storage
 * Hubic
 6) hubic
 * Local Disk
 7) local
 * Microsoft OneDrive
 8) onedrive
 * Amazon S3 (also Dreamhost, Ceph)
 9) s3
 * Openstack Swift (Rackspace Cloud Files, Memset Memstore, OVH)
10) swift
 * Yandex Disk
11) yandex
```
  Interesting idea!  I don't know anything about HDFS so I've done a small amount of reading...

rclone would work fine with HDFS if it was mounted in the file system (eg using NFS Proxy or Fuse).  rclone is very undemanding on the local filesystem it scans directories, opens and reads or writes files in sequence which I think should all be supported by HDFS.

Alternatively it could use the API.  There has been come work using go with hadoop
- https://github.com/hortonworks/gohadoop/ (HDFS not supported yet though)

Or use [LibHDFS](http://wiki.apache.org/hadoop/LibHDFS)

I don't have access to a hadoop cluster so I can't work on this, but would be grateful for assistance or patches!
  Good idea!  I'll see if I can add a flag to do this.

Though there is the question of which one to delete...
 @juniormonkey good point!
 I have made the dedupe command which you can try in this beta

http://pub.rclone.org/v1.26-51-g3101a7f%CE%B2/

### rclone dedupe remote:path

Interactively find duplicate files and offer to delete all but one or
rename them to be different. Only useful with Google Drive which can
have duplicate file names.

```
$ rclone dedupe drive:dupes
2016/01/31 14:13:11 Google drive root 'dupes': Looking for duplicates
two.txt: Found 3 duplicates
  1:       564374 bytes, 2016-01-31 14:07:22.159000000, md5sum 7594e7dc9fc28f727c42ee3e0749de81
  2:      1744073 bytes, 2016-01-31 14:07:12.490000000, md5sum 851957f7fb6f0bc4ce76be966d336802
  3:      6048320 bytes, 2016-01-31 14:07:02.111000000, md5sum 1eedaa9fe86fd4b8632e2ac549403b36
s) Skip and do nothing
k) Keep just one (choose which in next step)
r) Rename all to be different (by changing file.jpg to file-1.jpg)
s/k/r> r
two-1.txt: renamed from: two.txt
two-2.txt: renamed from: two.txt
two-3.txt: renamed from: two.txt
one.txt: Found 2 duplicates
  1:         6579 bytes, 2016-01-31 14:05:01.235000000, md5sum 2b76c776249409d925ae7ccd49aea59b
  2:         6579 bytes, 2016-01-31 12:50:30.318000000, md5sum 2b76c776249409d925ae7ccd49aea59b
s) Skip and do nothing
k) Keep just one (choose which in next step)
r) Rename all to be different (by changing file.jpg to file-1.jpg)
s/k/r> k
Enter the number of the file to keep> 2
one.txt: Deleted 1 extra copies
```

The result being

```
$ rclone lsl drive:dupes
   564374 2016-01-31 14:07:22.159000000 two-1.txt
  1744073 2016-01-31 14:07:12.490000000 two-2.txt
  6048320 2016-01-31 14:07:02.111000000 two-3.txt
     6579 2016-01-31 12:50:30.318000000 one.txt
```
  @khurshid-alam rclone (taking the lead from rsync) deliberately doesn't follow symlinks as doing so then opens it up to recursive directory loops and other nasty things like that

I could implement this flag (from the rsync man page)

```
   -L, --copy-links
          When symlinks are encountered, the item that they point to  (the
          referent) is copied, rather than ignoring the symlink
```

However rclone will never be able to copy symlinks themselves (you can't store these on object storage systems), so it will work like rsync with the `-L` flag and without the `-K` flag.

The `-L` flag would only work on local filesystems.

Eg if your source directory looks like this

```
   root
      link -> /path/to/dir

   /path/to/dir
      file one
      file two
```

The destination would end up looking like this

```
  root
    link
        file one
        file two
```

Does that match your expectations?

Possibly this should be the default behaviour of rclone - I'm not sure.

The reverse sync needs some more thought - what if there is a symlink in the local fs when we are copying from remote to local:.  Do we

```
* Follow it
* Delete it and create the file/directory directly
* Other
```
 I'll try to get this done for v1.35
 @darylstyrk not yet - I haven't released v1.34 yet, this should be in v1.35.  I'll post a beta here in due course for people to try
 I have implemented `-L`, `--copy-links` as per rsync.  This effectively follows symlinks.

Please find a beta here

http://beta.rclone.org/v1.35-48-g94947f2/ (uploaded in 15-30 mins)

If anyone is still interested in `-l`, `--links` then can you make a new ticket please. @theluke You can download it to a seperate place and use the full path to invoke rclone, or overwrite your current installation whichever you choose. @maakuth you are right - there is a small mistake in a recent commit - thanks for reporting.

Try this - this should fix it

http://beta.rclone.org/v1.35-149-g5fba913/ (uploaded in 15-30 mins) @tjcinnamon you'll need the latest beta for this @tjcinnamon yes, which should (cross fingers) be out this weekend.  @gustavorochakv Hmm, yes that is a little confusing!

You are correct in what it actualy does.  How about re-wording like this?

```
    Sync the source to the destination, changing the destination only.  Doesn't transfer
    unchanged files, testing first by modification time then by
    size.  Destination is updated to match source, including deleting files if necessary.
    Since this can cause data loss, test first with the --dry-run flag.
```

As for versioning - I wasn't actually aware what rclone does created a version, but I've verified it does just now.  What it does (on google drive) is if the destination doesn't exist it creates it, but if it does then it updates it.  I guess this is causing the versions.  The [api docs for update](https://developers.google.com/drive/v2/reference/files/update) don't state it makes a new version but it makes sense that it does.

I can document that in the google drive docs, and that will tell everyone that this behaviour is deliberate.

I take it you think this is a good thing?
 @zeshanb If you can reproduce rclone deleting the source data (that would be a bad bug) can you add an issue with instructions on how to reproduce? 

Thanks

Nick
 This is all fixed in v1.13 now!
  Nice idea - thanks.

I'm working on server side copies at the moment which this would fit in nicely with.
 Tech design:
- make new optional interfaces `Mover` and `DirMover`
  - `Mover` should have the same interface as `Copy` - not be on an object
- make a new rclone command "move"
  - This will attempt to use `DirMover` if available and the destination doesn't exist
  - if not it will attempt to use `Mover` on each file
  - if that doesn't work it will copy then delete the source
    - copy will use `Copier` interface if available

FS
- Drive: can implement `Mover` and `DirMover` this using [this api](https://developers.google.com/drive/v2/reference/files/patch) see also [go library](http://godoc.org/google.golang.org/api/drive/v2#FilesPatchCall) will need to set Title and possible add and remove parents
- S3 doesn't support moving/renaming objects
- Swift doesn't support moving/renaming objects
- GCS doesn't support moving/renaming objects - can't change object name with [patch](https://cloud.google.com/storage/docs/json_api/v1/objects/patch)
- Dropbox can implement `Mover` and `DirMover` [api](http://godoc.org/github.com/stacktic/dropbox#Dropbox.Move)
- Local - can implement both with rename

See also #99
 @ChrisSLT thanks for the feature request!  Let me know how you get on and open a new ticket if you find issues.

I've factored the server side move stuff out of this to #115
  How do you make a non-owned file?  I'll have a go at reproducing your problem it if I can do that!

Thanks

Nick
 I could [trash](https://developers.google.com/drive/v2/reference/files/trash) these files instead of deleting them - you can trash a file you don't own. What do you think of that idea?
 The API docs don't say if you can trash folders, but I expect that will work as it works with delete.
 Maybe I'm not understanding your request?

> Can you make it possible to trash individual files and a folder?
 I see what you mean.

Just talking about delete for the moment.

For 1) this should work but doesn't :-(

```
rclone -v purge drive:testdel/a.txt
```

For 2) purge should do the trick

```
rclone -v purge drive:testdel
```

As for choosing whether to `trash` or `delete` I could
- try deleting first, then if that fails, try trashing - that would fix your initial problem
- make a config file option which affects the behaviour of purge / and deleting files (eg when running `sync`)
- make a command line option to do the same
- make a new command `trash` which works like `purge` otherwise
  - this doesn't help with the `sync` command though, unless I make a `sync_and_trash` option which is starting to sound messy

Which would you prefer?
 I was away - but I'm back now!

The move command is in progress in #35 so I don't want to discuss that on this ticket!

I'd like to figure out `delete` vs `trash` as I think it would be helpful for drive users.
 OK will experiment - watch this space!
 1) I made an issue for that #82 

2) I have a prototype in a branch which I need to test and merge

Both waiting on a stretch of free time!
 @ChrisSLT Sorry for the lack of progress on this!  I've been away on vacation and I have rather a lot of other commitments.  I'd particularly like to finish 1) as it has been in a branch for ages, and 2) is pretty easy, so don't despair!
 @ChrisSLT thanks for that.  I've put that in a new issue #112

Have you tried the `--drive-use-trash` new in rclone 1.18 which should fix the original issue in this ticket.  This works for files and directories.
 I think all of these issues are fixed now.
- `--drive-use-trash` should fix the can't delete non owned files #82
- `rclone move` has been implemented #35
- `rclone purge` is now fixed for files in #112

If I've missed anything then please open a new issue as this one is too long now!

Thanks

Nick
 @memeplex you are correct there isn't a way of putting this in the config file at the moment.  Can you make a new issue with your request please?

Thanks

Nick
  Factored from #28 

Eg from @benfry

```
(50GB file)
2015/02/27 17:17:22 
Transferred: 51805750272 Bytes (12582.34 kByte/s)
Errors: 0
Checks: 0
Transferred: 0
Elapsed time: 1h7m0.837699975s
Transferring: Blade2_C_Drive006.v2i

2015/02/27 17:17:54 Blade2_C_Drive006.v2i: Failed to copy: Upload failed: googleapi: Error 401: Invalid Credentials, authError

Transferred: 52428762515 Bytes (12631.21 kByte/s)
Errors: 1
Checks: 0
Transferred: 1
Elapsed time: 1h7m33.449969925s
```

This can be reproduced very easily using the `--bwlimit` flag to make transfer take an arbitrarily long file.

The auth errors look very much like these problems
- http://stackoverflow.com/questions/23789284/resumable-upload-error-401
- https://code.google.com/p/google-api-python-client/issues/detail?id=231

I have tried
- refreshing the auth every 5 minutes during long transfers
  - doesn't work - as it doesn't refresh the auth that is in progress
- [using the Resumable Media API](http://godoc.org/github.com/google/google-api-go-client/drive/v2#FilesInsertCall.ResumableMedia) in the google drive API
  - doesn't work as it isn't finished according to the author

That leaves using the chunked API
- https://developers.google.com/drive/web/manage-uploads#resumable

NB might need to do this for google cloud storage too which probably has the same underlying problem
 @madralphw I'm currently testing a fix for this which is looking encouraging.  If you want to try a test binary, then drop me an email to nick@craig-wood.com  with your platform (windows/linux 32/64 bit) and I'll send you one.
 I've fixed this in rcone v1.12 which you can download from rclone.org

It switches to chunked mode for uploading big files which should make big file uploading much more reliable, and mean you won't get the auth problems any more.

Please re-open this ticket if you see the auth problem again.

Thanks for the report

Nick
  It might be that the docs are wrong!

rclone (like rsync) checks the size and modification time, and if these are identical assumes that the file is identical. That is a pretty good assumption normally, and it saves lots of disk and network IO.

rsync has a `-c` flag

```
    -c, --checksum              skip based on checksum, not mod-time & size
```

which checks checksums.  I should probably implement that for rclone at some point.

When you run `rclone check` it does show the difference, but `rclone sync` refuses to update it because the mod-time and size are identical - is that what you are seeing?
 Yes time and size.  The bug is in the docs which I'll fix shortly!

I'd like to implement the `-c` flag at some point too.
  Drive is working much better than it used to.

It isn't perfect though - I have some more improvements I'd like to make.

However if it fails to upload a file, re-running rclone will fix it.

So if you are worried about that, you could run rclone more than once in your script - the first time it will almost always transfer everything, but if it didn't you could run it again.

I could implement a `--retries N` flag which would make rclone retry a sync until successful or it tried N times.

So I think you should give it a go, and if you find bugs, then please report them!
 Let me know if you have any problems.

I've made a note of the `--retries` idea in the notes.txt file so I'll close this now.
  rclone got a major fix to make drive transfers a lot more reliable in v1.09.  It is possible that the fix might be causing duplicated files though, and equally possible that the duplicated files were caused by an earlier version of rclone.

Can you see if you  can replicate it with v1.10 please?  As in you see v1.10 creating a duplicate file.

If you could send me the log rclone produces using the `-v` flag around the time you see a duplicated file that would be really helpful.

In the mean time I'll see if I can get rclone 1.10 to make duplicated files too.

Thanks

Nick
 I've tested this with rclone v1.10 and I haven't seen it making duplicate files. I used my lots of small files test set which causes lots of retries which I thought might be the problem.

To answer your questions
- interrupted transfers: if you restart the transfer then rclone should figure out what has and hasn't been uploaded and carry on
- deletions do only occur after a sucessful sync, but rclone isn't expecting duplicate files so won't attempt to delete any of them

Let me know how your testing goes

Thanks

Nick
 Thanks that is really helpful.

I can see that the root cause of the duplicated files is the `googleapi: Error 401: Invalid Credentials, authError` error.

When that happens it fails to upload the file, and also fails delete the failed upload, leading to a duplicate.

I need to find out why those errors are happening which will require a bit of research - watch this space!
 I think you may be right about these being two separate issues - the failure to auth and the duplicate files.  Could the duplicates have been made by an earlier version of rclone?

The auth errors look very much like these problems
- http://stackoverflow.com/questions/23789284/resumable-upload-error-401
- https://code.google.com/p/google-api-python-client/issues/detail?id=231

The google drive bug is fixed so it might be that I need to do a bit more work resuming the upload on auth failure.
 I have managed to reproduce the auth error problem and I'm working on a fix for that.  I'll probably break that into a separate issue.

I tried your suggestions with + or @ symbols for making duplicates but I haven't had any luck.  I'd hope that it isn't these funny characters causing a problem since the unit tests cover these tricky cases (I had to fix the google drive api to fix precisely this!) but you never know!

Do you think you could make a reproducer for me? Find a selection of files which you can sync to a fresh directory on drive and it will cause duplicates (every now and again is fine).  Perhaps it will need multiple syncs, or the timestamps changing on files or the length changing. You don't need to send the the files, all I need to know are their names and lengths (output of `rclone lsl /path/to/local/directory` would be perfect)

Thanks

Nick
 > However I do get "exit 9" or "exit 11" at the end of a sync.

If you are referring to the number of go routines at exit, then that is only for debugging, and you can ignore it.  It should never be huge, but the exact value is unimportant.  I put it in when I found a bug in the drive api [which I fixed](https://github.com/google/google-api-go-client/commit/967f22b3d5e3956a46b9ccfad98025dc3e3e707e)

> As an aside, a successful sync (errors 0, checks 11240, but "Go routines at exit 9") to a folder that already has duplicates doesn't remove the duplicates

It doesn't remove the duplicates, you are correct.  It assumes that duplicates aren't possible and the internal datastructures are arranged with that assumption!  It could be changed with a bit of work, but I'd rather not have the duplicates at all...
 @madralphw I'm working on that as we speak.  It affects all transfers which take longer than 1 hour.  Unfortunately neither of my first two attempts worked to fix the problem so cross fingers for attempt number 3!
 I have factored the authError parts of this ticket into #33 to leave this one free for the duplicate file problem
 I still haven't managed to replicate the duplicate file problem

However I've added checking in rclone for duplicate files so in sync/copy/check it will log a message about duplicate files so at least I can see when it is happening more easily.
 Thanks for that - I haven't done a lot of testing with deep folder paths, I'll try some more and see if I can make some duplicates.
 @benfry I've fixed an issue which creates duplicate files on drive in #66 and released v1.14

I don't think that it is your issue but it might be worth a test!
 @Kazu1226 thanks for the report.  If you have a log of duplicated files being created that would be really helpful.
 @jacobmcnamee thanks - very interesting. I haven't had a good reason for switching to oauth2 but that sound like a good one. 
 @mbevin I don't think `--checksum` or `--size-only` will help, but I'd be prepared to be proved wrong if you try it and find they make a difference! Thanks for the info -- Nick
 What I could really do with is some kind volunteer to script rclone to do something like this
- make a directory
- upload some files
- check for duplicates - exit if any found
- remove the directory
- start again

That can just be left running until it finds some duplicates so I can reproduce the problem.
 @mbevin yes you are probably right
 After a bit of investigation I think I've worked out what is happening. If anyone would like to have a go here is a workaround - using rclone 1.28 add `--low-level-retries 20` to your command line. This should stop the directory listings failing which is the root cause of the problem in my tests. Let me know how it goes!
 Here is a beta with a fix for the directory listings to make it much more reliable.  This should eliminate the most common cause of duplicates.

http://pub.rclone.org/v1.28-6-gfdd4b4e%CE%B2/
 It would be interesting if you could estimate whether it is getting better or not.

The stats say rclone uploaded all but 3 files which is good.

I have another idea about the duplicate files - that the underlying cause may be duplicate directories.

I've put the drive fixes into v1.29 which I've just released and `dedupe` has a non-interactive mode too which should clear up the duplicates for you.
 @vishalmote I apologise, I didn't mean to suggest your data wasn't valuable, merely that 3 errors in 1420 is a better score than the number of duplicates suggest.

You can increase `--retries` to make it try harder to sync.  Also I note that you've set `--checkers` and `--transfers` very low - I suggest you set these higher again which will speed stuff up.
  For min/max we are talking about

```
        --max-size=SIZE         don't transfer any file larger than SIZE
        --min-size=SIZE         don't transfer any file smaller than SIZE
```

For include/exclude we are talking about these flags

```
    -f, --filter=RULE           add a file-filtering RULE
        --exclude=PATTERN       exclude files matching PATTERN
        --exclude-from=FILE     read exclude patterns from FILE
        --include=PATTERN       don't exclude files matching PATTERN
        --include-from=FILE     read include patterns from FILE
```

You'd probably want the related

```
        --delete-excluded       also delete excluded files from dest dirs
    -C, --cvs-exclude           auto-ignore files in the same way CVS does
```

rsync include exclude patterns are quite complicated!  I'd probably implement just include (+) and exclude (-) rules to start with.
 OK that is quite a lot of votes for this issue - I'll try to get it in the next release!
 I've released this feature in rclone 1.22.

See the [filtering docs](http://rclone.org/filtering/) for more info!

If you have any suggestions for improvements (eg to the docs!) or find any problems, please open a new issue!
 Wow - nice!
 @shawnbissell well spotted - I'll fix that now!
  I have fixed, this already but it hasn't made it to a release yet - watch out for 1.10!
  Hmm, interesting!  I don't have a google apps drive account to test it with unfortunately, so it isn't tested with them at all.

Can you do some other operations like `ls`, `ls`l, `lsd` with the `-v` flag to see if it prints any interesting debug?

Is there something special about authenticating with a google apps account?

Thanks

Nick
 @Edke I think I may have figured out the problem.  Do you by any chance have a great deal of files or directories in your google drive account?

If so this may fix it

```
rclone --drive-full-list=false -v copy  rclone-v1.07-linux-amd64.zip  drive:RCLONE
```

I should probably make that the default - I can't remember the rationale now so I'll have to re-investigate
 @Edke thanks for testing that - I'll try to make that flag not necessary for you in the next release.
 Excellent!  There is a problem with drive and 403 errors (see #20) which I'm working on at the moment too.  I can see you got 58 errors above which were probably caused by it - if you try the sync/copy again it should transfer the missing files very quickly.
 I've fixed this and made a new release v1.08.

If this doesn't work, please re-open the ticket.

Thank you very much for your bug report.

-- Nick
 Great, thanks for testing it for me :-)
  Hmm, looking at the code I think you are right.  I'm only listing the first 10,000 files by the look of it.

Do you have 17,000 files by any chance?

I'm currently rewriting the S3 code to use a different library, and I'll try to fix this as part of that re-write.
 I fixed this with the old library.  Please re-open if it doesn't work!

Thanks

Nick
 Rclone now lists in chunks of 1024 whereas it used to use 10,000 so you'll definitely get more list bucket requests.  Using a smaller number than 10,000 means rclone can get on with transferring stuff once the first 1024 have come back, while it is fetching the next chunk.

If this is a problem I could add a flag to control the chunk size.
 I've just done a bit of debugging with rclone and it definitely only does 1 request per 1024 directory entries.

I can only see itemised `$0.005 per 1,000 PUT, COPY, POST, or LIST requests` in my console, so could these be `PUT` requests? `rclone` will do `PUT` requests both when uploading a file, and when updating its metadata (eg timestamp).
 If the timestamp of the file changed - that is the only reason with s3 to update the metadata.
  You can do this already

Copy `file1.txt` into the bucket

```
rclone copy file1.txt memstore:test-bucket
```

Retreive `file1.txt` back again into the current directory

```
rclone copy memstore:test-bucket/file1.txt .
```

It isn't entirely intuitive, I'd agree!
 I'll try to make the docs clearer about copying single files.

Yes I'd like to add include, exclude and max-size flags at some point!
  Thanks for the feedback. You can certainly help with testing things out!

I'd like to fix the Google Drive problems as apart from those errors it works really well. The errors are caused by rclone doing too many requests per second as far as I can see.   Here is what my quota for rclone says, so that should allow 250 requests/second/user.

```
FREE QUOTA  10,000,000 requests/day
REMAINING  9,933,268 requests/day 99.33% of total
PER USER LIMIT  250 requests/second/user
```

I can try raising the number (I've already upped it from 100/s) or I could put some deliberate delays into rclone.  However I don't really think rclone is doing 250 requests per second.  It might be doing 25 in 100mS.

I've just raised the quota to 1000 requests/s/user.  I don't know how long it takes to take effect, but it would be interesting if you have another go in a few hours.

Next thing to try would be to pace the rclone requests so they always come more than 10 mS apart (say) which would be quite easy but require a bit of extra code.

As a workaround you can just run the rclone command again (and again) until it exits with no error - it will get there in the end - that is what I do! (I use Google Drive quite a lot.)

Another thing you could try is setting `--checkers 1` and `--transfers 1` - that might help.  Would be an interesting data point.
 Hmm, a quick experiment shows that even waiting 250ms between queries isn't enough to avoid the 403 rate limit exceeded error.

I think I'll have to implement the exponential backoff and retry algorigthm here

https://developers.google.com/drive/web/handle-errors

As the quota setting in the drive control panel obviously isn't doing anything!
 I spent some time making an algorithm to fix this and I hope I've got it right in v1.09.  Please can you have a go and let me know if it isn't working for you.

Thanks for the bug report

Nick
 - Is it your sense that with Google Drive now there can be 10K+ files and it
  will sync w/o errors?

Yes that should work fine now (cross fingers!)
- What about with Amazon S3, have you got it returning more than 10K files
  now?

Not yet.  That is a separate issue #22 . I'm finding rather a lot of bugs in the S3 library at the moment...
- Lastly, have you implemented the max size or include/exclude stuff like rsync that you want me to test?

No, not yet. If you want you can make an issue with exactly which features you would like - that will put it firmly on my radar!

It is the google drive performance that should be really different in v1.09.
 https://github.com/awslabs/aws-sdk-go is what I'm using.  It was working quite well until recently but alas now seems very broken!
 I have decided to fix this with my old s3 library which does work.  Expect an update shortly!
 Super, thanks!
 Yes that is what it does - hopefully it didn't report any errors in the stats printed at the end.

When I try it I see it upload 20-30 files very quickly then rate limit, so get quite a few retries then start uploading quickly again.  That seems to be google's rate limiting in action.  It doesn't seem possible to upload more than about 2 files a second.
 @klokan Interesting idea, I suspect that it won't work though - even if you get a new access token, google still knows who you are because you have to use the refresh token they gave you to get a new access token. So I think there is some underlying identity that google will keep track of instead.

As for lots of small file uploading - yes this is annoying.  Unfortunately I think it is an intrinsic part of drive.

I just checked my quotas for rclone - google must have put them up automatically!  I can't believe that you are doing more than 1000 requests/second so I think the rate limiting must be part of drive.

```
Free quota  
1,000,000,000 requests/day
Per-user limit  
1,000 requests/second/user
```
 @timwhite The idea in this issue might fix your problem #485 - comments on that issue please!
  I think this probably isn't a bug.  Google cloud storage doesn't actually store directories at all, so your first command

```
rclone --verbose=true mkdir google:iceburg-vault/test
```

Will ensure that the container `iceburg-vault` is created, but it won't create a directory in GCS because there are no such things!

That explains why

```
rclone --verbose=true ls google:iceburg-vault
```

doesn't show anything.

If you copy a file into test you will see something, eg

```
$ rclone copy z gcs:floop/test
$ rclone ls gcs:floop/test
   754576 z
$ rclone ls gcs:floop
   754576 test/z
$ rclone lsd gcs:floop
           0 0001-01-01 00:00:00         0 test
```

Hopefully that makes sense.

You say also "The inability to manipulate files under buckets also breaks sync commands -- they appear to do nothing" - can you provide an example?

As far as I can see syncing to google cloud storage is working fine

```
$ mkdir testdir
$ cd testdir
$ echo 1 > one
$ echo 2 > two
$ ls -l
total 8
-rw-rw-r-- 1 ncw ncw 2 Jan 19 16:44 one
-rw-rw-r-- 1 ncw ncw 2 Jan 19 16:44 two
$ cd ..
$ rclone sync testdir gcs:floop/test
2015/01/19 16:45:32 Storage bucket floop path test/: Building file list
2015/01/19 16:45:32 Storage bucket floop path test/: Waiting for checks to finish
2015/01/19 16:45:32 Storage bucket floop path test/: Waiting for transfers to finish
2015/01/19 16:45:33 Waiting for deletions to finish

Transferred:            4 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            2
Elapsed time:  2.444223806s

$ rclone sync gcs:floop/test testdir-recovered
2015/01/19 16:45:48 Local file system at testdir-recovered: Building file list
2015/01/19 16:45:48 Local file system at testdir-recovered: Waiting for checks to finish
2015/01/19 16:45:49 Local file system at testdir-recovered: Waiting for transfers to finish
2015/01/19 16:45:49 Waiting for deletions to finish

Transferred:            4 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 0
Transferred:            2
Elapsed time:  1.172942466s

$ ls -l testdir testdir-recovered/
testdir:
total 8
-rw-rw-r-- 1 ncw ncw 2 Jan 19 16:44 one
-rw-rw-r-- 1 ncw ncw 2 Jan 19 16:44 two

testdir-recovered/:
total 8
-rw-rw-r-- 1 ncw ncw 2 Jan 19 16:44 one
-rw-rw-r-- 1 ncw ncw 2 Jan 19 16:44 two

$ rclone lsl gcs:floop/test
        2 2015-01-19 16:44:55.596390773 one
        2 2015-01-19 16:44:58.724390892 two

Transferred:            0 Bytes (   0.00 kByte/s)
Errors:                 0
Checks:                 2
Transferred:            0
Elapsed time:  738.599123ms
```
 No problem!

I've considered making the command line semantics like rsync which does one thing or another depending on a trailing slash, but I decided to keep it simple.  Thanks for the heads up - I'll try to make the docs clearer.
  Your idea for the remote to remote copy is how I would approach it.

This has one disdvantage with rclone as it stands today in that it will effectively download the data and re-upload it.  However I have been thinking about an allowing bucket to bucket copies which would be exactly what you want.  S3, Swift and GCS all allow this.  Here is the docs for GCS
- https://cloud.google.com/storage/docs/json_api/v1/objects/copy

So if I were to implement that then the copy to backup first would work really quite well I think.

As for 

```
[sync] Deletes any files that exist in source that don't exist in destination.
```

I think it is badly worded, it "deletes files in the destination that don't exist in the source" as you would expect.  I'll fix the wording
 Interesting idea!

I think I'd simplify the logic slightly and make it a new rclone command

```
rclone sync3 /path/to/backup remote:/backups/base remote:/backups/changes.2015-01-19
```
- for every file in/path/to/backup
  - if it is in base unchanged - skip
  - if it is modified in base
    - copy the file from base to changes if it exists in base
    - upload the file to base
- for every file in base but not in backup
  - move it from base to changes

This would mean that base would end up with a proper sync of backup, but changes would have any old files which changed or were deleted.  It would then effectively be a delta, and you would have all the files at both points in time.

You could re-create the old filesystem easily, except for if you uploaded new files into base - there would be no way of telling just by looking at base and changes that those new files where new or just unchanged old files.  This may or may not be a problem!
 Sorry missed your last comment..

Yes, Versions sounds like it would be simpler for people to understand.

The renaming scheme needs a bit of thought - windows doesn't deal well with files with funny extensions.

Implementation wise, it is quite similar to the schemes above.
 I'll just note that rclone now has bucket to bucket copy and sync which may be helpful!
  Thank you for the bug report.

Unfortunately what actually happened is missing from the top of the error you pasted.  There should be some lines before the backtrace with the actual error.

Can you send everything rclone prints?  If you think it has confidential info in it then you can email it to me privately.

rclone should pick up from where it left off if you retry the transfer.

To answer your other question, you shouldn't need to set `--modify-window` normally.  Because the various object storage systems and OSes store time to differing accuracies, rclone calculates the modify window which is the time period it expects the date stamps files to agree in.  If you use the `-v` flag you can see rclone calculating the modify window.
 From private emails, it is now clear there is a memory leak in rclone, most likely in the google cloud storage backend.  I'll work on tracking that down.
 Please re-open if this fix doesn't work for you - thanks

-- Nick
  Looks like this is because I cross compile the binaries from linux

http://stackoverflow.com/questions/20609415/cross-compiling-user-current-not-implemented-on-linux-amd64

If you have a go installation should should be able to build it yourself fairly easily with `go install github.com/ncw/rclone` and that should work according to the above.

Unfortunately I don't have an OSX machine to test.

I'll see if I can put in a workaround on OSX

Thanks for the bug report

-- Nick
 I found a library which I could use to fix this problem

https://github.com/mitchellh/go-homedir

Which solves exactly this problem.  I'll have a go at using that.

```
This is a Go library for detecting the user's home directory without the use of cgo, so the library can be used in cross-compilation environments.

Usage is incredibly simple, just call homedir.Dir() to get the home directory for a user, and homedir.Expand() to expand the ~ in a path to the home directory.

Why not just use os/user? The built-in os/user package requires cgo on Darwin systems. This means that any Go code that uses that package cannot cross compile. But 99% of the time the use for os/user is just to retrieve the home directory, which we can do for the current user without cgo. This library does that, enabling cross-compilation.
```
 You could as a workaround use `rclone --config ~/.rclone.conf` and that should give the same effect.
 I've just uploaded v1.06 which should fix this problem.  Please re-open this ticket if it doesn't.

Thanks for the bug report

Nick
 No I do't think that would work.  The Google cloud storage is a very simple objects in a bucket system.  Whereas Google Drive is much more like a file system with directories and files - conceptually quite different.
  We could add a path into the config relatively easily so then you'd have to type

```
rclone sync source: dest:
```

What do you think of that?
  rclone doesn't do changed blocks backup, it only works on a whole file basis.

So each time a VM image changed you'd be backing it all up which is probably not what you want.

You could get rclone to backup a gold image then backup the rdiff deltas.  However you'd probably be better off using a tool which is specialised for that, eg [duplicity](http://duplicity.nongnu.org/)
  It looks plausible if not completely straight forward.

Here is the [rest reference](http://msdn.microsoft.com/en-us/library/windows/apps/hh243648.aspx)

From this I can see that
- OneDrive doesn't support setting the modification date of files
- OneDrive doesn't return the MD5SUM of the files

So it will be necessary to store these in some kind of metadata.  OneDrive's support for metadata seems limited to
- description
- comments

Using description would be the highest performance as it is returned with every object.  This would mean adding a specially formatted description such as `Uploaded by rclone (md5sum: 293847abde7892456748391dcde909384, Last modified:  2001-02-03T04:05:06.499999999Z)`.  This could be added to any descriptions already in place.  Otherwise this could be put in a comment.
 Now that rclone supported remotes which don't have md5sum or modtime properly this should be straightforward.
 @ismail @bobobo1618 @phudson I've release the first version of this in rclone v1.24.  Give it a go and make issues if you find problems!

See the page in the docs about in, in particular the limitations: http://rclone.org/onedrive/

Thanks

Nick
 @ismail  thanks for pointing that out.  Can you make a new issue please?

Thanks

Nick
  Thanks for the report.  I see the problem - I'll fix it shortly!  Then you'll get to see an error message from Google Cloud Storage most likely.
 I've fixed this (hopefully!) in version 1.04 which you can find on http://rclone.org/

Thanks for the bug report
  I think adding Google Cloud Storage would be quite easy to add looking at the docs.

I presume using the API KEY method of access rather than the OAUTH method?

As for scale - I use rclone for transferring about 50,000 files and it works fine for that.  Metadata is stored in memory so that will be the ultimate limit, but I expect metadata for 1 million files wouldn't take more than 500 Mb of memory, most likely much less than that.
 I've committed a beta version of google cloud storage support to master - it will be in the next release!

I ended up using the OAUTH authentication as I don't think the API key method can do everything, but I may have been mis-understanding the docs!
 Fixed in version 1.02

Open a new issue if you find any problems!

Thanks

Nick
  I can confirm that rclone seems to be buffering the entire file in memory for Google Drive which it definitely shouldn't be!

I'll try to fix shortly.
 I've uploaded new binaries to http://rclone.org for this release
 Thanks very much for reporting this bug

-- Nick
  Which OS are you using?  It would be useful to see the full output of stat.

What happens if you do `cp -av file1 file2`

Do you think you could attach a zip of files which reproduce the problem?

Your log looks really strange as though the HTTP transaction has got corrupted.  Is there something funny between you and the Internet?  A proxy maybe?
 I've managed to replicate that - thanks for narrowing it down.

It should be quite easy to fix - I'll do so shortly.

Thanks

Nick
 I'll leave this open until I've put the fix in
 I've release a new version which fixes this (see http://rclone.org/ for downloads)
  I can confirm this problem.  There is logic to skip deletes on dry run, but the `--dry-run` flag setting isn't being passed down to it for some reason that I'll work out shortly!

Thanks for the bug report.
 I've made a new release with this fix v0.99 - thanks for reporting the problem!
  That one is relatively easy to fix like this (at the cost of a small amount of inefficiency).  If you can get rclone to work with gccgo I'd consider a pull request.

``` diff
--- a/fs/config.go
+++ b/fs/config.go
@@ -147,7 +147,7 @@ func Command(commands []string) byte {
        if len(result) != 1 {
            continue
        }
-       i := strings.IndexByte(optString, result[0])
+       i := strings.Index(optString, string(result[0]))
        if i >= 0 {
            return result[0]
        }
```
