  Fixed in https://github.com/google/ExoPlayer/commit/19a3d94022d4450eaccaaf48bcbf71f27aef4418  I don't think we support either at the moment. Marking as an enhancement. We're unlikely to prioritize this, however, so the most likely path to this being supported would be an external contribution.  Sounds like a bug with the decoder (presumably the OMX.qcom.video.decoder.avc implementation for this SoC). Perhaps the decoder outputs some video buffers before the format, which probably isn't a valid thing to do.

We'll get hold of a device and try and reproduce. It should be pretty easy to provide a workaround (e.g. by just not reporting the size if it hasn't been set yet) if necessary.  There's not enough information here for us to help. What are you defining as connection life, and how are you measuring it? I don't understand what beatrate means, either. What type of media is being played?  Could you confirm it is not C.TIME_UNSET? I'm pretty sure it will be. `C.TIME_UNSET` is an allowed and documented return value. The type of file you're trying to play makes determining the duration very difficult. The player would need to download and parse the entire file to work the duration out. When I try and play the file in VLC I see that it does show a duration, but that the duration shown constantly changes over time (i.e. it's estimating, and the estimate is being updated during playback).

If you control the source content then you should use a more appropriate container format (e.g. mp4).  This sounds like an issue with your project rather than anything wrong with ExoPlayer. Perhaps you're missing a dependency somehow. I'd suggest taking a look at our demo app, checking that works, and assuming that it does work trying to figure our what your project is doing differently.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!  From #1988: "An important caveat is that when playback transitions into a ClippingMediaSource the player needs to do a frame-exact seek operation. To avoid buffering you therefore need to ensure that the clipping start point corresponds to a synchronization sample/key-frame. (Playback should work even if you don't do this, but transitions from one source to the next will not be seamless.)" Is the clipping start point a keyframe? Have a look at #2923 again. We added a new constructor to [ClippingMediaSource](https://github.com/google/ExoPlayer/blob/dev-v2/library/core/src/main/java/com/google/android/exoplayer2/source/ClippingMediaSource.java#L72) which lets you disable the discontinuity. Note, that this is only advisable if you can ensure your clipping start point is a key-frame as @andrewlewis already pointed out.  I think the problem here is that the stream indicates it includes an audio track, but then doesn't actually contain any audio data. The player is waiting for audio belonging to this track before starting playback, which I think is the right thing to do.

Commenting out [this line](https://github.com/google/ExoPlayer/blob/r2.4.2/library/core/src/main/java/com/google/android/exoplayer2/extractor/flv/FlvExtractor.java#L187) forces the extractor to ignore the audio track, allowing playback to proceed.

Closing on the assumption this is bad media, since the stream should not indicate presence of audio that it does not actually contain. If you know you're playing only this type of stream then you could make the above change in a local copy of ExoPlayer as a workaround for your app.  We do not provide a direct API for this, but there are multiple approaches you could look into if you're trying to grab video frames in a particular format. For example you could use TextureView.getBitmap to get the current frame in a bitmap, at which point you could do your own conversion into a preferred format.

Without a proper description of the use case it's impossible to provide any solid guidance regarding whether what you're trying to do is feasible, and if so what the best approach would be. If you want to provide more information, please do so here and we'll re-open the issue. Thanks!  This is not something we intend to support. The DASH IF recommendations mandates that audio/video are demuxed ("only non-multiplexed Representations shall be used, i.e. each Representation only contains
41 a single media component"). If I remember correctly, most DASH profiles mandate this too.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
  I don't think it's valid to have line breaks, or indeed any whitespace, inside of a BaseURL element. None of the examples in either the DASH spec (ISO 23009-1) or the DASH IF recommendations (http://dashif.org/guidelines/) do so.

Please let us know if you kind find some documentation indicating otherwise. Thanks! In that case, to be more precise :)... If you do add line breaks or whitespace inside of the BaseURL element, then I don't think you should expect it to be trimmed.  I don't know which version you are using but try `TsExtractor.MODE_MULTI_PMT`. Multiprogram TSs are supported. However, preparation time might be longer and some users might prefer to use single PMT mode. If you are going to do that, might make sense to filter out all PMTs that don't correspond to the service you want to play. If you run into any issues, please include all the info requested in the issue template. In the not so far future, removing extra PMTs might not be necessary. The idea behind MODE_SINGLE_PMT was that users that obtain their TSs from tuners were able to filter the streams of a single program. So, that mode expects a single PMT to exist.

> Shouldn't it create only one PmtReader?

It would be arbitrary to pick the first found PMT and discard the rest. It would cause undesirable non-deterministic behavior. As I mentioned before, this is undergoing some changes, which might apply to this conversation. But they are not yet decided. 

So, the best way to go right now is to either use multi PMT mode or filter out the PMTs of services which you don't want the extractor to render. If using multi PMT, you can use the ID of the tracks' formats to differentiate services.  - There is no requirement that you use `SimpleExoPlayerView`. You can use a `TextureView` directly if you prefer. You can use `SimpleExoPlayer.setVideoTextureView` to set a `SimpleExoPlayer` instance to output to a `TextureView`.
- If you do want to use `SimpleExoPlayerView`, you can specify `surface_type` as @b0g8 notes above. You can retrieve the `TextureView` from the `SimpleExoPlayerView` using `SimpleExoPlayerView. getVideoSurfaceView` and then casting to `TextureView`. You seem to have somehow used a combination of the two suggestions. That's not a sensible thing to do. Either use the first suggestion, or use the second one (probably the second one).  The answer is the same as in #2200. If you need tight synchronization, you'll probably need to be mixing the audio prior to feeding it into a single AudioTrack instance. This is not a use case we currently support I'm afraid. If you don't need tight synchronization then you can just use two `ExoPlayer` instances (i.e. an extra one for the background music).  Dupe of https://github.com/google/ExoPlayer/issues/2974. Because you already have an open issue that's exactly the same, which is https://github.com/google/ExoPlayer/issues/2974.  The ConcatenatingMediaSource waits till all child media sources are successfully prepared before starting playback. If one of them fails for whatever reason, the whole playlist fails.

When you say the playlist "throws" a 404, I assume you mean that either [ExtractorMediaSource.Listener.onLoadError](https://github.com/google/ExoPlayer/blob/d979469659861f7fe1d39d153b90bdff1ab479cc/library/core/src/main/java/com/google/android/exoplayer2/source/ExtractorMediaSource.java#L54) or [AdaptiveMediaSourceEventListener.onLoadError](https://github.com/google/ExoPlayer/blob/d979469659861f7fe1d39d153b90bdff1ab479cc/library/core/src/main/java/com/google/android/exoplayer2/source/AdaptiveMediaSourceEventListener.java#L137) gets called (depending on the type of media you use). When this happens you can set up a new ConcatenatingMediaSource without the failing source and retry. This should solve your problem.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.
- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.

<!-- need_sender_cla --> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.
In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.

<!-- need_author_cla -->  Can you just look at where that constructor is called by the ExoPlayer library, and trace back to where the data is coming from (right click -> Find Usages in Android Studio makes this really easy)? You should be able to answer your own question this way. You should receive a "Do you want to find usages of the base method" dialog when you try and find usage of that method. If you click Yes then you should find a usage (in SimpleDecoderAudioRenderer's maybeInitDecoder method). I'm having a hard time working out what you're doing, which is making it difficult to provide any int here. Let's say you start from the working ExoPlayer demo app. What is it exactly that you're trying to do, and at a high level what changes have you tried to make in order to achieve your goal? You appear to be hooking into the Flac extension at a very low level; certainly lower than we would envisage ever being necessary. Ok. It sounds like there are two different things you need to do:
1. Use the Flac extension in the normal way. I don't think there's any need for you to customize it, or understand what it's doing internally, or anything like that.
1. Add processing to the decoded audio data. This is a completely separate topic, and should probably be done using an `AudioProcessor`. You can inject a custom processor by creating an extension of `DefaultRenderersFactory` and overriding `buildAudioProcessors` to instantiate your custom processor. I don't really understand what your question is. I think the only thing you need to change is the `DefaultRenderersFactory`, which you need to extend so that you can override `buildAudioProcessors` as explained above.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
  No? It seems unlikely an app will disable multiple renderers and need to un-disable them all simultaneously (and you've provided no actual use case to suggest otherwise). It also seems trivial just to iterate over the number of renderers if you want to do this. If your use case is as you describe, you can simply use `setSelectionOverride` for everything, and do a full reset with `clearSelectionOverrides`. You can disable a renderer for a given set of tracks using `setSelectionOverride(index, tracks, null)`.

The reason renderers can be disabled with both `setSelectionOverride(index, tracks, null)` and `setRendererDisabled(index, true)` is that the first applies only when a specific set of tracks is available, where-as the second persists across playback of different pieces of media that have different sets of tracks. If you want to reset going from one piece of media to another anyway, there's no reason you can't just use the first option. Yeah, looks broken. Thanks for reporting! I've filed https://github.com/google/ExoPlayer/issues/2988 to track a fix (and test).  I am using Exoplayer 2 in my video player for playback.
When playing HSL streams is it possible to switch rendition if one of the renditions mentioned in m3u8 file is not available. 
We receive a master m3u8 file from the server. This has all the available renditions. Sometimes it can happen that one of the renditions is not available and can respond with status code of 500, 501, 502, 503, 400 or 403 and if Exoplayer chooses that rendition to play it throws an error. 

What I am trying to achieve here is if one of the rendition is bad rendition and exoplayer is not able to play it, then it should choose the next/other rendition to play which are available but should not throw onPlayerError callback. 

I have also looked at https://github.com/google/ExoPlayer/issues/676 where it talks about changing video track selection using TrackSelector and TrackSelection. 

I have implemented Exoplayer.EventListener where I listen for  
```@Override public void onTimelineChanged(Timeline timeline, Object manifest) { }
@Override public void onTracksChanged(TrackGroupArray trackGroups, TrackSelectionArray trackSelections) {
    // Here I get all video formats and text tracks using 
}
@Override public void onPlayerError(ExoPlaybackException e) { handleError(e); }
```

So when Exoplayer loads and if it picks the rendition which is bad at first place, I get a callback in onPlayerError. 
Is it possible for Exoplayer to choose the other rendition if the one it has picked is bad. > Is it possible for Exoplayer to choose the other rendition if the one it has picked is bad.

This is already done by ExoPlayer. However some discussion is in order here. The spec says:

> `If the server wishes to remove an entire presentation, it SHOULD`
   `provide a clear indication to clients that the Playlist file is no`
  `longer available (e.g. with an HTTP 404 or 410 response).`

5xx HTTP error codes are reserved for server errors, whereas 404 and 410 explicitly speak about resource changes. If you can use 410 to indicate unavailability, that will make ExoPlayer do exactly what you aim for. If not, we should discuss whether 500 error codes are appropriate. This is because some other users have complained we should not retry if the player gets a 5xx error. E.g: #2844. But I think there are others. Hope this answers your question. Please close if so.
 Thanks for your answer and information. I agree that for 5xxx HTTP error codes Exoplayer should not retry but in case there are 5 renditions available and out of these 5 renditions, 3 comes from the same server and 2 comes from some other server. Consider the 3 coming from the same server, that server encountered a problem and now if exoplayer picks any one of them we will receive 5xxx error code and no point in retrying. However the remaining 2 which comes from another server, Exoplayer should try to play them instead of throwing an error. So my point is if we are sure that all the available rendition will throw 5xxx error then only Exoplayer should throw an error otherwise it should try to play the one which is good. Sounds like a good argument. I'll give it a thought and post any updates here. Thanks for giving it a thought. Just to add here AvPlayer in iOS already does this, so this functionality becomes important for us as we are trying to keep the behaviour of both platforms i.e. Android and iOS to be in sync.   If this is for playing TS streams (not encapsulated within HLS), then it looks like we do set the language from the descriptor tag [here](https://github.com/google/ExoPlayer/blob/51de6e53ebf7188bd6e469165986283f6ef75b3d/library/core/src/main/java/com/google/android/exoplayer2/extractor/ts/DefaultTsPayloadReaderFactory.java#L151).

- Are you using a recent enough version of ExoPlayer to be getting that code (2.3.0 and later, I think).
- Are you overriding the caption formats using `FLAG_OVERRIDE_CAPTION_DESCRIPTORS`, in which case it's probably up to you to be specifying the language when you do the override? Got it, thanks. How does the subtitle get enabled in your code today? I suspect your only option currently is to enable it using `setSelectionOverride`. I'd also expect that override to continue applying when moving from one TV channel to another, because the tracks available to the text renderer probably look identical for all channels? An override for a renderer is applied if the tracks available to that renderer match those for which the override set. In your case the text renderer probably sees a single closed caption track, with a format that's pretty much empty, and importantly, identical for all TV channels.

If you feel somewhat uneasy relying on this, then yes, I would too. If one of the channels happens to include a descriptor then the format for that channel will be different, so the tracks available to the text renderer will be different, so the override wont apply to that channel.

Extending DefaultTrackSelector and overriding selectTextTrack to do something like always select the first (and probably only) track if some preferred text language has been set is probably your best bet for current ExoPlayer releases.

I'm not sure what the best long term solution is. We could provide an option on DefaultTrackSelector to specify that a text track with no language should be enabled if available, and if no text track is available in the preferred language. Or something like that?  This is a duplicate of #2833.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
 You've completely omitted the last section of the template, for example (it's probably not hard for you to read through it and see what's missing, rather than us having to do it for you ;)). Thanks.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.
- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> The change needs to go into the `dev-v2` branch, so we'll close this pull request and submit a change directly to that branch today. Thanks for bringing this issue to our attention!  Not in HLS, no. Track #2014 for multiple muxed tracks support in HLS. My suggestion is that you demux the audio streams and expose them as EXT-X-MEDIA elements. Keep in mind that these should be [packed audio segments](https://tools.ietf.org/html/draft-pantos-http-live-streaming-23#section-3.4). Unfortunately, I haven't done this before, but I don't think it would be too complicated with ffmpeg and maybe some extra scripting.

> Is there a way to make ExoPlayer to see all tracks from the TS segments?

You could try modifying the HlsMediaChunk code to replace TsExtractor.MODE_HLS with TsExtractor.MODE_SINGLE_PMT, but this is not officially supported and might caused undesired behavior depending on the encoding of the media. Note that demuxing the alternative audios is also the smart thing to do, as it will save some wasted bandwidth. > This is particularly a pain when your live TV broadcast contains multiple subtitle tracks, let's say DVB-S.

DVB subtitles are not supported by the HLS spec. ExoPlayer does support multiple 608 streams.

The change you propose above is similar to my suggestion of using a different mode. However, this is not supported and there might be future changes that break this patch. I think DVB subs will work anyway. I haven't tested this, though. Of course, the single stream per type limitation will apply unless you override it yourself, as discussed above.  Fixed in `dev-v2` in https://github.com/google/ExoPlayer/commit/a7ed199622500584d5070335f2381288198a53e7  We'll be making this much easier from the next release (and easier for the `dev-v2` branch shortly, in advance of the release). Please try the instructions under the "Locally" heading in the README in the dev-v2 branch ([here](https://github.com/google/ExoPlayer/blob/dev-v2/README.md)).

Note the instructions will only work with the dev-v2 branch at this point, so you'll need to checkout that branch for them to work. They will not work on release-v2 until the changes are merged into that branch.  Dupe of https://github.com/google/ExoPlayer/issues/2882.  The "Default" mode automatically adapts to streams based on the current bandwidth estimate. When you look at the debug line just above the video, you can see the current resolution (for example, r:640x320). Depending on your network this may change while playing the video. > In my HSL stream, there're 4 tracks: 240p, 360p,720p and a source track. 

Could you clarify what source track is? 

> When the player change track automatically, how can I know which track (track index) is currently playing?

`SimpleExoPlayer's` `getVideoFormat` and `getAudioFormat` will provide quite a bit of information about the video and audio currently playing. I see. This is not a term I had heard before.

You could take advantage of the default track selector. Configure the DefaultTrackSelector you are using to use the max video size or bitrate you'd like. Like so:

```
DefaultTrackSelector trackSelector = new DefaultTrackSelector();
// Using the video size.
trackSelector.setParameters(trackSelector.getParameters().withMaxVideoSize(...));
// Using the video bitrate.
trackSelector.setParameters(trackSelector.getParameters().withMaxVideoBitrate(...));
// Assuming you are using SimpleExoPlayer.
SimpleExoPlayer player = ExoPlayerFactory.newSimpleInstance(/* renderersFactory */, trackSelector);
```

I guess in your case the bitrate alternative is better. If none of the previous work, you could also subclass DefaultTrackSelector and write your own selectVideoTrack, overriding the existing one. For example, if you know your _source track_ is listed first, you could filter out the video track with id 0. I suggest you play around with the default track selector.

A few notes: 

- The resolution looks strange, do all videos have the same width but change height?
- Do the streams not contain audio? If they do, you should probably add the corresponding codecs entry.
- The NAME attribute is not defined in the spec, so you count on a different property in the format to identify the source track.

Hope this helps. As an additional point: The best way to exclude the source track is to not include it in the master playlist to start with. You shouldn't push this kind of functionality to the client, since in doing so you're relying on *all* client players supporting this functionality, where-as you could just solve the problem in a single place on the serving side by not including it to start with.  Why don't you just fork in these cases? It's less than 100 lines of code (excluding comments). If you want custom behaviors, it's kinda unlikely in my opinion that you want to inherit some of the base behaviors (and be at the mercy of those changing from release to release). Doesn't seem like a big deal to fork for the use case you describe. We like to keep things final where possible, since it helps to limit the (already very large) library API surface. Thanks.  You're trying to play a DASH stream using `ExtractorMediaSource`, where-as you should be using `DashMediaSource`.  Also asked in #2972.  Duplicate of #2872. It is undergoing internal review. Should be soon.  For 1. and 3. you can use `ExoPlayer#onTimelineChanged`. The first argument will be the [`Timeline`](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/Timeline.html) from which you can obtain the window. The second argument will either be or include the manifest (depending whether you are using DASH/SS or HLS).

For 2. I am not sure what checking the bandwidth means. To select the bandwidth, you can use your own `AdaptiveSelectionFactory` when building the media source. Or `AdaptiveMediaSourceEventListener`, if you want to see loading events. I guess you could call `getBitrateEstimate` from the BandwidthMeter you are using.  > There's no comments besides auto-generated boilerplate comments,

Which specific parts would you like to see comments attached to? Just commenting everything doesn't always make things clearer in practice.

> It's overkill and overly complicated

I think what you really mean is that it's over-complicated for your specific use case. There are other issues on this issue tracker asking for more features in the demo app, heh. It should be pretty easy for you to delete from the demo app until you get down to your use case (and conversely, going in the opposite direction isn't as easy).

> Having a simple project that plays a local, bundled, hardcoded MP3 would be fantastic.

We're thinking about providing simple, use case specific demo apps. It's not going to happen soon though. Rephrased the title of this issue to track what I think we really need to do here.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.
- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> >  It's a legal m3u8 file.

A very quick look at the HLS RFC suggests the opposite. VLC being able to play something isn't the same thing as something being legal according to the spec. On what are you basing your statement? Even though I am not 100% sure about comments being forbidden between STREAM-INF and URI, I am sure the included snippet is not [spec](https://tools.ietf.org/html/draft-pantos-http-live-streaming-23#section-4.3.4.2) compliant: 

` The URI line that follows the EXT-X-STREAM-INF tag specifies a Media
   Playlist that carries a Rendition of the Variant Stream.  The URI
   line is REQUIRED.  Clients that do not support multiple video
   renditions SHOULD play this Rendition
`

Considering the first and third lines don't have valid URIs, even if we change the parser to ignore comments between STREAM-INF and the URI, this playlist would still not be supported by exoplayer.  Do you understand under what conditions null can actually be returned, and have you managed to reproduce? I see it's marked as `@Nullable`, but I'm finding it hard to work out when null can actually be returned (I followed the source code as far as `IContentProvider`). It seems plausible null isn't ever returned, and it's just a result of code going through the `IContentProvider` interface.

If you do understand the conditions, and/or if handling is trivial anyway (which I guess it is), then yes feel free to fix in this PR. What will you do in that case though? Throw something? What? Maybe `FileNotFoundException`? `IOException` seems fine as well. Thanks!  Please can you add a link to where this is reported to Widevine? This isn't an ExoPlayer issue, and the Widevine team are best placed to confirm the issue. We'll likely close this one (up to @wvpaf). It's good to have the cross-reference though. adding @pgrov   This is already tracked by #855.  The code is correct how it is currently. drm_license_url is the URL to use if there isn't one specified in the `KeyRequest`.  There's a small issue in the DVB parser. I'll push a fix shortly. Brilliantly, there's also a small bug in the DVB spec itself, which corresponds to half of the issue :)!  This is because changing tracks currently triggers a seek, and MPEG-TS is not seekable. We are considering avoiding seeking on track selection, at the cost of using more memory, but right now this is working as intended. This is fixed in `dev-v2`. Thanks for testing. Please sync and try again; I submitted a one line indexing fix that I think resolves this problem. Fix is here: https://github.com/google/ExoPlayer/commit/076a77e598dc9f32110c4fcbe9fd67cc4a5e023b Yes, the same benefits will be coming to HLS soon. We're using https://github.com/google/ExoPlayer/issues/2718 for tracking.  Could you please follow the issue template? If you could replace "that link" and "another link" with actual links, that would help me understand what the problem is.

> Why it send GET request

The GET requests are necessary to fetch playlists and chunks. Bue maybe I am not understanding correctly. I am inclined to think that the server is redirecting ExoPlayer but not Vitamio, or the other way around. Try to emulate Vitamio's http request's header with ExoPlayer. It could be the user agent, or other header. Unfortunately I cannot help you anymore, as this is most likely not an ExoPlayer bug.  ExoPlayer will generally request whatever URI you provide, and my understanding is that the system will use a proxy server if one is configured in Settings for the relevant network connection. If you're running a local proxy server you'd just pass a URI referring to localhost. If that doesn't answer your question please clarify what you're trying to do. This sounds outside the scope of this issue tracker.  Are you sure they're not TTML captions? I don't think we made any changes to WebVTT captioning between 2.4.0 and 2.4.1. We fixed TTML positioning in 2.4.1 (at least in theory). It's quite possible that your TTML caption files are actually specifying that the captions should be rendered at the top of the screen (or that the implicitly specify this by not specifying something else), and that what the previous behavior was incorrect.

There's nothing we can do to look at this without test content (or at least the TTML part of it). We need to see sample TTML files to investigate this properly. @zharf - Yes, that looks like the problem for your case. Thanks for the sample. We should fix that, after which you'll get proper positioning as is defined in your TTML file.
@evandipietro - There's no way we can verify whether your case is the same or different without a sample TTML file, so we'll use this issue to track supporting referential styling as in @zharf 's case unless we learn more.

Thanks! The best short term fix for this is probably to ignore regions that don't directly define origin and extent, which will cause the the captions to go back to being displayed at the bottom again. We currently set default origin and extent in this case, but that results in unexpected behavior when the origin and extent are actually defined in the chained styles that we're not currently resolving. Does that sound sensible? Please could you verify that the above change resolves the issue, up until the point at which we can properly support resolving of chained styles. You can either try on the `dev-v2` branch, or cherry-pick the change into `release-v2` if you want everything else to stay the same. We may cut a small release picking up a few bug fixes, including this one, if verification is received. Thanks! The fix will be in 2.4.3, which will be released today or early next week. Renaming this issue to track the enhancement of properly supporting referential styling.  The ID3 header indicates that the ID3 data ends at 317827 bytes but it actually ends at 325982 bytes. So I think this file is technically not valid.

You can get it to play by increasing `Mp3Extractor.MAX_SNIFF_BYTES`. The current value is set such that it searches up to the maximum size of one MPEG audio frame to resynchronize (4 KB). A larger search distance increases the time and space cost, which is bad if you are trying to play a format that is accepted by an extractor later in the list. It looks like the platform searches up to 128 KB.

Assuming this is a widespread problem, I will increase the search distance a bit so that this file plays. If you have any information about how the file was produced it would be useful to know so we can determine the search distance needed to work around whatever bug that set the wrong ID3 size. Andrew increased the sniffing distance a little. The chosen value is arbitrary though, and obviously playback will still fail if the same problem occurs but where the ID3 header is out by a larger value (unless we're missing something, the "proper" fix is to fix the media).  This is already tracked by https://github.com/google/ExoPlayer/issues/2909, you don't need to file a separate issue.  > I'm wondering if there's a way to filter out tracks based on their mimeType so that they don't show up when I call MappedTrackInfo.getTrackGroups(..).

If this is just to prevent the selector from selecting the VTT streams, I don't think this is the best way. I can think of two ways of easily achieving this:
- (Best way I can think of) Subclass DefaultTrackSelector and override selectTextTrack so that it prefers TTML over VTT.
- Pass a SubtitleDecoderFactory that does not report support for WebVTT. This would be the answer to what you ask about specifically, I think.

> is it ok to use the DefaultTrackSelector.setParameters(..) approach

Yes. It's the way to parametrize the default track selector. The other way is setting overrides (you can look into the demo app for more info on this).

Out of curiosity, could you elaborate on why you prefer TTML subtitles? Please close the issue if this answers your question.  This is quite far outside the scope of this issue tracker. We're unable to offer advice on this kind of large customization effort; we simply don't have enough time. Sorry.  Dupe of https://github.com/google/ExoPlayer/issues/2843.  It should be very easy for you to test your stream in the latest versions of v1 and v2, adding your sample to the demo app. Many improvements and bugfixes have landed for webvtt in HLS, particularly for v2. Please file a new issue without ignoring the issue template if you see this reproducing in newer versions.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.
- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.

<!-- need_sender_cla --> I'm unclear what "check CLA update" means. Currently you're not listed as being associated with any CLA, as far as I can tell. Thanks! Merged a different fix.  Proposed fix here: https://github.com/google/ExoPlayer/pull/2941 I think getMaxInputChannelCount actually returns 1 for all non-standard decoders, on all devices, prior to the O release of Android. This is due to [applyLevelLimits](https://android.googlesource.com/platform/frameworks/base/+/android-7.1.1_r12/media/java/android/media/MediaCodecInfo.java#1061) being implemented incorrectly. This is definitely the case on N. We'd have to check back through the source code to see if earlier releases were also affected. There's a fix in O, although it didn't make it into the preview.

So I wonder whether we need to come up with a more general solution to this? It's pretty easy to work out if a decoder is non-standard; the mimeType doesn't have a corresponding block in applyLevelLimits. A more open question is what max channel counts to assume in each case. It doesn't seem ideal just to assume support for unlimited channels? We should probably just work out what the correct max channel counts are for `OMX.dolby.ac3.decoder` and `OMX.dolby.ec3.decoder`, and override to those values prior to O for those decoders (regardless of what device they're on)? If we set to the max according to the spec then that's probably the same as assuming support for unlimited channels, because it's likely impossible to make streams with more channels than the spec allows. We're more interested in what the actual decoder implementations support, which may well be less than the maximums according to the spec (e.g. I'm not convinced `OMX.dolby.ec3.decoder` can actually support 16 channel input). We can try and do some digging also, to figure out what the correct values should be. Ask Dolby, probably. We will do some digging. We've merged a slightly more general fix for this issue. Please give it a try and verify. Thanks!  I'll take a look into this as soon as I can. I haven't updated in some time, feel free to ping me. Have you by chance tried the platform player to see whether the observed behavior is similar?   We do not have an ETA at this moment, and still need to fix some behavior (e.g. how seeking interacts with ads in the timeline) before we can release to stable. Please follow #2617. It's not done yet, which is why #2617 is still open. Updates will appear there when available.  The demo app includes Widevine protected samples, and can be used as a starting point to see what you need to do.  I don't think this is a good idea. ExoPlayer's API is better thought out and can be improved over time. It's preferable for developers to incur a one time cost switching over to it. For developers who need to be able to switch between ExoPlayer and MediaPlayer, it's preferable to write a wrapper around MediaPlayer that implements ExoPlayer's API, not the other way around.  Yes, this is a known issue unfortunately. My understanding is that it'll be fixed in O. The fix hasn't landed yet, so there's no point in you trying to flash O at this point in time. [Internal bug ref: 36511863] Please follow #2587 if you're interested in receiving an update when the issue is fixed (they will be fixed at the same time). Re-opening this because they turned out to be separate issues. I believe so.  Can you just shuffle the array of MediaSources before passing it to ConcatenatingMediaSource's constructor? Or do you need something different?  Please try using `setVideoDebugListener`. If that's not sufficient for your needs, please explain exactly what your use case is for needing this. Please also see https://github.com/google/ExoPlayer/issues/2800, which is similar to this one.  I think the reason is that from 2.4.0 all the actual code was moved into other libraries, which this AAR depends on. See also this [blog post](https://medium.com/google-exoplayer/exoplayers-new-modular-structure-a916c0874907). Is there an actual problem using them, or was the concern just that they were much smaller?  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
  Could you provide a bit more context to understand what the purpose is?

Note that the samples will be in the timebase of the PROGRAM-DATE-TIME tags. Which means that if you get the playback position, it will map with the program-date-time tags in the media playlist.

Also, from the last release onwards it is possible to get all tags from the manifest, but I don't think it will provide useful information. > also I want to pull the current absolute timestamp at any time.

I think `getCurrentPostiion` gives you exactly this. Regarding getting a callback for a specific playback position, see #2090. Hope this answers your question. Yes, there was some discussion about this and it was eventually decided it should be done this way. It shouldn't affect your visibility of timestamps, however. `getCurrentPosition` is derived from sample timestamps, which are the ground truth of the stream. Could you provide a stream that reproduces this? If you don't want to make it public you can send it to dev.exoplayer@gmail.com. Following the issue template makes this process much more agile. Yes, sorry I misguided you. As documented in ExoPlayer `getCurrentPosition()`:
`Returns the playback position in the current window, in milliseconds.`
The start of the window is always 0, AFAIK.
To get what you need, you'll have to add to that value, the current window's `positionInFirstPeriodUs`.

I am going to mark this as an enhancement to populate other relevant information of the window in HLS.
 Keep in mind the units in which each method returns the values. They are either included in the method's name or documented in the javadoc.  Duplicate of https://github.com/google/ExoPlayer/issues/2871. This will be fixed in the next release.  There is a trade-off between memory consumption and switching speed. In V1 we buffered all tracks in memory and so could switch to them quickly (although the implementation is technically not quite right). In V2 we buffer only the tracks that are enabled, which saves memory but results in slower switching.

This was done intentionally, although we are revisiting this decision currently. It could be that we revert back to how we did things in V1, although in a technically correct way. > Memory of android devices were rised up day by day.

Citation needed (particularly for low end Android devices) ;). We're going to revert back to allowing seamless switching between multiple audio tracks. It's still incorrect to state that memory of Android devices is increasing much, however, particularly at the low end. This is fixed in `dev-v2`.  This isn't something we support directly, and it's difficult to come up with a general purpose solution. It is possible to manually set the preferred refresh rate, if you want to set it to match the content being played. You can query the available rates using:
```
Display display = activity.getWindowManager().getDefaultDisplay();
float[] refreshRates = display.getSupportedRefreshRates();
```
And set your preferred rate using:
```
Window window = activity.getWindow();
WindowManager.LayoutParams layoutParams = window.getAttributes();
layoutParams.preferredRefreshRate = refreshRates[preferredRateIndex];
window.setAttributes(layoutParams);
```

Note that in practice, most people opt not to do this. TVs often handle mode changes in a very ugly way (e.g. screen flashing black for a while), and of course if you set the refresh rate to something like 24Hz, that means any playback control animations (or whatever) are all going to run at 24Hz too, which is going to look pretty janky. You can mitigate this to some extent by selecting the highest refresh rate that's a clean multiple of the content FPS (so if the content is 24fps you should probably prefer 48Hz to 24Hz, if both are available).

Note that we do some frame release timestamp adjustments inside of the player to do a pretty decent job of mapping 24fps (and other fps values) onto the default 60Hz display refresh. In practice these adjustments do a pretty good job of minimizing the issue. You can look in the `Format`s of the selected tracks to see what their frame-rates are. Alternatively, if you control the serving side of whatever you're developing, you may well be able to deliver the frame-rate in some out-of-band metadata. You can then decide whether you want to switch, and switch as described above.

This is not something we support directly in the library, and so we're not in a position to provide detailed advice. In `VideoFrameReleaseTimeHelper`.  Forcing usage of the secure decoder for playback of both the clear and protected content probably also works around the issue (and allows you to use ConcatenatingMediaSource). I'd suggest giving that a try. You could inject a custom `MediaCodecSelector` implementation that wraps the default one and forces selection of secure decoders when available. The `getDecoderInfo` method would look something like:

```
if (MimeTypes.isVideo(mimeType)) {
  // Always prefer secure video decoders, if available.
  MediaCodecInfo secureCodecInfo = wrappedSelector.getDecoderInfo(mimeType, true);
  if (secureCodecInfo != null) {
    return secureCodecInfo;
  }
}
return wrappedSelector.getDecoderInfo(mimeType, requiresSecureDecoder);
``` Ah yes, my suggestion above probably doesn't work for the reason you describe. For a clean workaround we'd need to make more invasive changes. For example we'd need to allow acquisition of a `DrmSession` without any `DrmInitData`, so as to obtain the `MediaCrypto` up front. We'd then need to put the workaround into `MediaCodecRenderer`, I think. It wouldn't be necessary to have a custom codec selector.  I tried to reproduce your problem with the mp3 file in the demo app (Misc -> Google Play (MP3 Audio)) and the described MediaSource combination. I wasn't able to hear any noticeable amount of silence between loop iterations. 

Could you send me a link to the mp3 file you used? 
And how long is the gap we are talking about approximately?  The above mentioned [commit](https://github.com/google/ExoPlayer/commit/56205893e5f392531baa9bc1d33a56858193a36e) solves your problem as long as you have media sources with audio-only tracks. 

You can also enforce the gapless playback for other media sources by passing `false` as a last parameter to [ClippingMediaSource](https://github.com/google/ExoPlayer/blob/dev-v2/library/core/src/main/java/com/google/android/exoplayer2/source/ClippingMediaSource.java#L72). Note, however, that this is only advisable if you can ensure that all tracks start with a key-frame at the specified startPosition of the clipping.   See #2122.  Please can you revert the indentation changes before we look at this? Specifically (but not limited to) C.java and TsExtractor.java. If you look at the diff (by clicking "Files changed"), it appears as though you've deleted and re-added huge parts of these files because you've changed them to have 4 character indentation rather than 2 character indentation. This makes it very hard to review what you've actually changed. ExoPlayer source code uses 2 character indentation (and 4 character indentation for line continuations only).

Thanks!  > Is this correct?

Yes :).

> If I wanted to toggle the text track on and off, should I use setRendererDisabled or setSelectionOverrides?

If you're just playing a single video then I don't think it makes any difference.

It does make a difference if you're playing media that contains multiple periods. For example if you've concatenated multiple media items together using `ConcatenatingMediaSource`, or if you're playing a multi-period DASH stream. In these cases the available tracks can be different moving from period to period. Calling `setSelectionOverride` sets an override that's specific for a set of available tracks. So if you move to another period that has a different set of available tracks, the override wont apply. Disabling the renderer will apply across all periods. I do not see that issue in the demo app when disabling via the track selector. Are you sure? Please file an issue with specific reproduction steps in the demo app if so.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.
- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok -->  Listen for `ExoPlayer#EventListener#onTimelineChanged`. The manifest argument should carry an `HlsManifest` object, which contains a reference to the master playlist and a snapshot of the media playlist. Playlists (master and media) carry all their tags in the field `tags`. You can map each `EXT-X-STREAM-INF` to a format using the rest of the attributes. Unfortunately there is no better way, as we don't directly support this attribute. As far as I know, it is not defined in the spec.  See [ExoPlayer supported formats -- sample formats](https://google.github.io/ExoPlayer/supported-formats.html#sample-formats) and the [Android CDD](https://source.android.com/compatibility/cdd).  Please include all the information required in the issue template. The bugreport is missing.  You're looking in the right place (taking the demo app and updating `EventLogger` to output logging in the various `onLoad...` methods is a good place to get started and see what data is available). We do not report segment number, and do not plan on doing so. Media start and end time is preferred because it more gracefully handles cases like two segments being downloaded in a single request.

I'm not sure what you mean when you say:

> i'm getting the arrival time with the variable mediaStartTimeMs but sometimes a n segment will have a smaller arrival time than the n-1 segment ( not logical !)

What is an "arrival time"? To make it clearer what you're seeing, perhaps you can add logging to `onLoadStarted`, and then provide us with the sequence of logging that you see and that you believe to be unexpected. `mediaEndTimeMs - mediaStartTimeMs` is correct. I don't understand what you mean when you say it "should be a static one". As already explained above, we do not report segment number and do not plan on doing so. 

`AdaptiveMediaSourceEventListener` is only intended for reporting information (e.g. for logging / debugging purposes). It's not intended that such a listener be implemented to actually modify the way in which the player behaves. So it's pretty unclear what you mean when you say the information could be useful for chunk replacement.  I'm not sure what you're asking, but if you want to find out when the player starts playing the next playlist item you can use `onPositionDiscontinuity`. See #2053.  The original cause of failure (clipped off the top of what you've provided), is:
```
EventLogger: com.google.android.exoplayer2.ParserException: The edited sample sequence does not contain a sync sample.
EventLogger: 	at com.google.android.exoplayer2.extractor.mp4.AtomParsers.parseStbl(AtomParsers.java:398)
EventLogger: 	at com.google.android.exoplayer2.extractor.mp4.Mp4Extractor.processMoovAtom(Mp4Extractor.java:343)
```
It appears the problematic media has a strange edit list, that's discarding the first 4 video frames for no apparent reason. The first frame is the only key-frame, so after applying the edit list we end up with no key-frames from which to start playback.

Playing this type of media correctly is tracked by https://github.com/google/ExoPlayer/issues/1659, although there doesn't seem to be a valid reason for the edit list to exist in the piece of media. If you have control over the media itself then you should just fix it to not include an edit list. Advising on correct media preparation is outside the scope of this issue tracker, sorry. I'd suggest asking on StackOverflow or similar.  I don't think this is normal behavior. I test live streams daily but do not observe the described issue. You can get the amount of skipped and dropped frames in the Demo App's debug overlay on top of the screen under the label sb and db respectively. That's a good starting point. The logcat should also include relevant logging (in the demo app, that is) when a certain amount of dropped frames is reached. This information can be obtained in your app through the different available listeners. Please provide more context for us to evaluate what the issue here is.  Is there a way to do this within the DASH spec (or DASH IF recommendations)? It doesn't look like `@label` is actually defined anywhere in either.  `getBufferedPercentage` is the correct thing to be using. We should make sure the value doesn't go negative. Beyond that you'll need to provide proper reproduction steps if you want us to investigate, and fill out the issue template correctly. The returned value is now constrained to be in the range [0,100]. Closing due to lack of information beyond that.  I can observe in the provided log that the highest resolution is being selected. Could you please try adding video size parameters to the DefaultTrackSelector so that the lowest resolution is picked? Like so: 

`selector.setParameters(selector.getParameters().withMaxVideoSize(256, 144));`

The logs should show something like:

```
D/EventLogger(14336):     Group:0, adaptive_supported=YES [
D/EventLogger(14336):       [ ] Track:0, id=133, mimeType=video/avc, bitrate=246236, res=426x240, fps=30.0, supported=NO_EXCEEDS_CAPABILITIES
D/EventLogger(14336):       [ ] Track:1, id=134, mimeType=video/avc, bitrate=617173, res=640x360, fps=30.0, supported=NO_EXCEEDS_CAPABILITIES
D/EventLogger(14336):       [ ] Track:2, id=135, mimeType=video/avc, bitrate=1118095, res=854x480, fps=30.0, supported=NO_EXCEEDS_CAPABILITIES
D/EventLogger(14336):       [X] Track:3, id=160, mimeType=video/avc, bitrate=110578, res=256x144, fps=15.0, supported=NO_EXCEEDS_CAPABILITIES
D/EventLogger(14336):       [ ] Track:4, id=136, mimeType=video/avc, bitrate=2270192, res=1280x720, fps=30.0, supported=NO_EXCEEDS_CAPABILITIES
D/EventLogger(14336):     ]
D/EventLogger(14336):   ]
```

We are also pushing a fix to prevent this kind of selections when the tracks exceed codec capabilities. This is probably fixed by the change ref'd above.  Apparently this device can be upgraded to Android 4.2.2. Is the upgrade available for your test device, and if so does taking it resolve the issue?   In the `dev-v2` branch there's an `ExoPlayer.setRepeatMode` method that allows looping behavior to be toggled on and off. It's intended that you should use this rather than `LoopingMediaSource` except for certain niche use cases.

Will that let you do what you want?  This sounds niche enough for us to not support it directly. I'd suggest you implement your own UI components for this.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks! The attached logcat has a lot of audio underruns reported (like `internalError [18.06, audioTrackUnderrun [53664, 279, 724]]`).

I couldn't reproduce this on a Nexus 6P using the demo app playing the provided URL. Are you able to reproduce the issue on any other devices? Do you see the same issue if you play the stream using the platform player via `adb shell am start -a android.intent.action.VIEW -d "http://stream-two.doc123.nl:8000/movie/bh9kpOhVO0/MkFohi23nG/21149.mp4" -t "video/*"`? Closing due to lack of requested information.  Until this is fixed, I think you can call `defaultExtractorsFactory.setMp3ExtractorFlags(Mp3Extractor.FLAG_ENABLE_CONSTANT_BITRATE_SEEKING)` to work around the issue. Thanks for the excellent issue report by the way; very helpful!  We removed this permission approximately 2 years ago. I don't think it was ever required, so likely it was added unintentionally and later removed. Obviously we can't fix versions going backward in time, so you should upgrade to either the latest 1.x or latest 2.x version if you a version with the permission removed.

You can probably remove it safely from the version you're using, yes.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
  We make our best to provide robust support, even sometimes when the stream is not technically valid. The amount of things that can go wrong is innumerable, and this issue tracker is a nice museum of malformed media. If we try to study and workaround every possible media malformation (we actually do sometimes), we won't have time to improve ExoPlayer for correct media. Besides, client-side-wise, the HLS spec is quite tricky as it is. 

We try to work around malformed media issues, but fixing the media should always be the first approach. > 1 - Which version of the spec is ExoPlayer following? Is there any place we can keep track on that?

We aim for the newer ones but we try to support anything allowed by any of the versions of the spec, unless we have good reasons not to. The newer the spec version, the higher the chances of us supporting it eventually.   

> 2 - Is there any tool you can recommend to validate manifest/playlist files?

I don't know any. We'll probably write a [medium](https://medium.com/google-exoplayer) blogpost including the best practices for generating HLS content, but no automatic tool. Let me know if you run into any.  Sounds like you're manually enabling a decoder yourself in the platform, and observing that it doesn't work very well. Isn't that most likely to be an issue with the decoder? What decoder is it that's actually being used?

There's nothing we can actually do to look at this issue given the lack of a bug report, device details and so on (edit: ignore previous comment about test content). If it's a Google codec then you should probably file an issue on Android directly, as described [here](https://source.android.com/source/report-bugs), under GfxMedia presumably. This is beyond what we realistically have time for, particularly given the decoder isn't enabled by default. Sorry. Please do file an issue on Android directly if you want someone to take a look. Thanks. Does this imply you know how to fix it too ;)? Feel free to send a pull request if so. It's quite low priority for us, so it'll almost certainly be much faster for you to fix it.  We do not support synchronized playback within a single player of multiple audio tracks. You'd have to do some quite in-depth customization to support that kind of use case, which is beyond the scope of this issue tracker.  The live playlist type [is not supported by the spec](https://tools.ietf.org/html/draft-pantos-http-live-streaming-23#section-4.3.3.5). I will remove this exception so as to ignore the value if invalid, like in this case.  If you know the URLs for the ad media at the time of instantiating the media source, you can use ConcatenatingMediaSource to concatenate ClippingMediaSources that load portions of content with MediaSources that load ads. For example, to play one minute of content with a midroll ad at 30 seconds and preroll/postroll ads, you could structure the media source like: `ConcatenatingMediaSource(prerollAdMediaSource, ClippingMediaSource(contentMediaSource, 0 s, 30 s), midrollAdMediaSource, ClippingMediaSource(contentMediaSource, 30 s, 60 s), postrollAdMediaSource)`.

If you don't know the URLs of the ads in advance you'll need to make a custom media source, like ImaAdsMediaSource. In that case, you can't construct a MediaSource in advance with all the ad URLs. We don't currently provide a direct way to make the player insert ads based on ID3 metadata, but you may be able to handle the metadata yourself using MetadataRenderer, and re-`prepare` the player as required. Instantiate an `ImaAdsMediaSource` passing in your existing `contentMediaSource`, as described in the [README](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ima).  This is tracked by https://github.com/google/ExoPlayer/issues/2565.  `DashMediaSource` takes a `AdaptiveMediaSourceEventListener` argument through its constructor, which is invoked every time a load is started/stopped/canceled/completed. You should be able to do some counting in your listener implementation to figure this out.  Provided the change in DRM scheme corresponds to the start of a new Period, then yes, this is something we intend to add support for. We don't support it currently. Marking this as an enhancement for tracking.  Device restarts for O need to be reported on the Android issue tracker, as described [here](https://source.android.com/source/report-bugs). You should be reporting this issue on the Android issue tracker, as explained above.  As long as you're handling the buffer synchronously in your processBuffer call, I'm pretty sure you should always have access in `processOutputBuffer` (prior to `super.processOutputBuffer` being called). You could also take a look at how/when buffers are being fed into `AudioTrack` from the regular `processOutputBuffer` implementation. The buffers must be valid at those points.

Unfortunately, since this is a customization rather than an issue with the library itself, we're unable to help further on this issue tracker.  The reason behavior is different is that ExoPlayer does frame-accurate seeking (in all cases). I think VideoView does key-frame accurate seeking, which is faster but less accurate. We can use this issue to track supporting key-frame accurate seeking as an option, but it's low priority. You shouldn't rely on the player reporting exactly your seek position back through getCurrentTime. The reported position may be adjusted slightly. If you want to seek in tiny increments like this, you should find a way to keep track of the last position that you performed a seek in your own code, and subtract 45 from that value each time. With that approach each position you pass to seekTo really will be 45 less than the previous value passed.

If we could keep this issue on topic, which is to support seeking to the nearest key-frame, that would be good. Thanks.  E-AC-3 can be played via HDMI passthrough where the connected device advertises support. It is possible for a devices to advertise support AC-3 but not E-AC-3.

See #2148 for more information on passthrough and on-device decoding of these formats.  If you've already found a method that does what you're trying to do (`maybeDiscardUpstream`), can't you inspect that method and figure out what it's doing differently?  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!  The error you're seeing suggests that the custom `DataSource` component you're using is not correctly handling requests for data at non-zero offsets. We're not able to assist in debugging custom components. Note that we do provide official components for encrypting/decrypting to/from disk (`AesCipherDataSource` and `AesCipherDataSink`).

If you're able to reproduce without use of the custom component, please file a new issue.  Because that's part of an extension you have to build manually if you want to use it:

https://github.com/google/ExoPlayer/tree/release-v2/extensions/flac Let's use https://github.com/google/ExoPlayer/issues/2851 to track this.  It should hopefully be obvious to most readers that you're supposed to pass the looping source to get looping. I think the guide is sufficiently clear. If we were to spell everything out explicitly then the guide would be unnecessarily long and verbose for the majority of readers.  We do not provide any built in support for playback of images. You could potentially write your own custom renderer (e.g. `ImageRenderer extends BaseRenderer`) to provide direct support within the player, or just manage displaying of images yourself outside of the player.  I'd suggest you report this to the device manufacturer. Sounds like we should probably blacklist OMX.MTK.AUDIO.DECODER.RAW in `MediaCodecUtil.isCodecUsableDecoder` for some combination of Util.DEVICE and/or Util.SDK_INT values.

Do you have a good sense of what the blacklisting condition should be? I can try and check with MediaTek also, to see if this is a known issue. Internal bug ref: 62337687 Apparently the MTK raw decoder is using the wrong endianness. They've fixed it going forwards, and OEMs can pick up the fix for their devices, but it's unlikely the fix will reach all of the affected devices in the wild. We'll most likely push a change that prefers OMX.google.raw.decoder on affected devices, if both are available.  @parkgrrr, the cause of the issue described here is clear, I think: The server is resetting the media sequence:
> mediaSequence suddenly change to start from zero

A patch is on the way to throw an error when this condition is met, so the application can detect this condition and reset the player. If you don't think your issue is related to this, please file a new issue inlcuding all the information requested in the issue template.  If everything builds fine when you clone the `release-v1` branch, including the demo app, then this sounds a lot like a setup issue with the way you're configuring your project. This would be outside the scope of this issue tracker.  You need to implement your own `TrackSelection.Factory` to use instead of `AdaptiveTrackSelection.Factory` which is what you're currently using. Use `AdaptiveTrackSelection` as a starting point. If you look at its implementation, one of its methods is `updateSelectedTrack`. It's this method that selects the track for the next chunk load.  If you want to make player controls that are substantially different to the default ones, then you should implement your own. Wiring a "next" button up to do something completely different than "next" doesn't sound like a great idea.  Pretty sure we just mirror whatever's in the source media. Assuming the source media uses some vaguely sane standard, then you should be able to normalize it into RFC 5646 using `Util.normalizeLanguageCode`. Hm, actually, I'm not about the second part of my answer.  If I remove the `keepScreenOn` attribute from the demo application's [player layout](https://github.com/google/ExoPlayer/blob/195a93e/demo/src/main/res/layout/player_activity.xml#L21), I find that the screen can turn off while the player is paused. See also #930.

`adb shell dumpsys power` shows what WakeLocks are held. A bug report includes the same dumpsys output. You're probably either playing audio continuously or your app is holding a WakeLock, which we can't really help with. I suggest looking at the output of `adb shell dumpsys power` to debug this yourself.  This is discussed/tracked in https://github.com/google/ExoPlayer/issues/677  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks! The URI included in the code snippet plays well on the demo app. This suggests there is an issue with your code.

We might be able to help you if you fill the issue template as requested above.  It is live content, I am not sure how looping should work.  The change above updates ExoPlayer to use `AVERAGE-BITRATE` to populate `Format.bitrate` when it's available, since that's what `Format.bitrate` is documented to contain.

It's somewhat unclear to me that there's a need to also include the peak bitrate in the `Format` object. Unless people are going to write ABR algorithms that make use of it, the need to include it is pretty hypothetical. If you do have a concrete near term plan to make use of it, please could you clarify how you intend to use it (particularly given you don't know where the peaks are).  Please stop filing this type of question here. The purpose of this issue tracker is to report actual issues with the ExoPlayer library, not as a place where you can get general programming help and your own code debugged. You need to ask this kind of question on StackOverflow.  Does the sample play correctly with the built in players on other devices (e.g. the Samsung S7)?  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!

Note: if I had to guess based on the limited information provided, I'd say the device is running out of memory for AudioTrack buffers perhaps due to not releasing the players when you have finished with them. Please provide the output of `adb bugreport`.  Documenting common errors is discussed and tracked in https://github.com/google/ExoPlayer/issues/1611. I've updated the link (it's pretty trivial to find the Javadoc for ExoPlaybackException using Google, or just looking at the source code...). The fact it's one year old just indicates it's not very high priority for us. Posting the same thing again isn't going to speed anything up. Thanks.  This is expected because the underlying platform destroys the surface, and because we cannot draw into the new surface until after we've re-buffered and decoded the first frame at the current playback position. Sorry.  @sureshcnvidia - Any idea about this one? I'm guessing it's not Shield specific, however, as it's also reported as occurring on the emulator. [Possibly related to internal bug ref: 62512584]  This is a pretty advanced use case :). We don't provide a way to synchronize SimpleExoPlayer (or ExoPlayer) instances. With a bit of work, however, you might manage to get all of the streams playing at once inside a single instance. You'll need to touch a few components, but at a high level you'll need to do something like:

1. Inject a custom `RenderersFactory` instance that builds as many video renderers as you need. You can probably extend `DefaultRenderersFactory` to do this, then override `buildVideoRenderers` to build N instances of `MediaCodecVideoRenderer`. You can see the implementation in `DefaultRenderersFactory` to see how it builds a `MediaCodecVideoRenderer`, and copy that.
1. If you have control over the source content, make it so that you have a single big manifest containing all of the adaptation sets that you need. You can then use `DashMediaSource` as normal. If this isn't possible, create all of the `DashMediaSource` instances and then merge them using `MergingMediaSource`.
1. At this point the remaining work is to ensure the `TrackSelector` selects the correct track(s) for each of the video renderers. `DefaultTrackSelector` wont do anything like this for you, so you'll probably need to implement your own `TrackSelector` from scratch.

Once that's all wired up, it *might* "just work", although note that many devices only have a limited number of video decoders available (and for low end devices, the limit might be 1).  Please, do send the media to dev.exoplayer@gmail.com. Is this specific to the two devices mentioned, or do you see it on other devices as well? I gave the assets a try on a Galaxy S8 as an initial check, and cannot reproduce there. How frequently do you see the issue, as a % of total attempts? As per above we're not able to reproduce this, and it hasn't been reported by anyone else which suggests that it's not a widespread issue. If this affected all apps using ExoPlayer on the S7 we'd expect to have received other reports by now.

A good way to figure out whether it's your content or not would be to see if you can reproduce with any other content obtained from other sources. If it is specific to your content then you'll probably need to work out how to fix that. Alternatively, a pragmatic fix would be to append a little bit of silence onto the end of your files. Closing because I don't think there's anything actionable we can do at this point in time.  Playing out the remaining buffer rather than stopping immediately for this use case is tracked by https://github.com/google/ExoPlayer/issues/1606. For the "stopping" case, you haven't provided sufficient information for us to tell what happened. Please supply a full bugreport as requested in the issue template, captured shortly after encountering the failure.  I can't follow those instructions either.

@vigneshvg - It looks like they were originally added by you in the readme for VP9, before being copied elsewhere. Could you clarify how they're supposed to work? In particular, the checked out ExoPlayer is probably in a different directory to the user's own application project. It doesn't appear these instructions include enough information for the ExoPlayer directory to be found (and I suspect that what's happening is that it isn't found). I managed to compile flac here using gradle without needing to pull the whole exo player folder in my sources.

https://github.com/PaulWoitaschek/MaterialAudiobookPlayer/blob/master/exoplayer-flac/build.gradle#L17

The task that prepares the dependency is here:

https://github.com/PaulWoitaschek/MaterialAudiobookPlayer/blob/master/buildSrc/src/main/java/de/ph1b/audiobook/ndkGen/PrepareFlac.kt#L15

But building that stuff was really really painful. I also got it working for opus, but it fails with very cryptic errors often times.

It would be so great if you could just ship these through maven. Ping @vigneshvg - Could you explain the instructions? If they don't work, we should remove/update them. We do not provide pre-compiled versions of the demo app. Providing these is tracked by https://github.com/google/ExoPlayer/issues/1920, but for non-technical reasons this is currently non-trivial for us to do. It's not my extensions, it's just the exoplayer extensions drawn together so they build with all these manual steps involved. 

Rather that me building it I would rather push for the exoplayer people just release it to maven. 
There is an issue for that but it has both status unclear. Maybe that can be escalated? 

It took me quite some time to get it to build and I think there would be a lot of hours saved if not everyone has to handle the complicated build process itself.  Distributing extensions via jCenter is tracked by https://github.com/google/ExoPlayer/issues/1434. Let's just use this issue to track making sure the manual instructions make sense. Well I just modified my build.gradle to let the Android plugin handle the build. 

Then it's really straight forward as you only need to put the flac sources in the jni folder. 

For opus it's more complicated as I needed to modify the build files to get it working. 
 Instructions have been clarified on the dev-v2 branch.  The server returns an HTML page when I request that using `wget`. You need to give the player the URI of the actual video not an HTML page embedding it.

Do you need to support DASH/HLS/SS? If not, you can probably just use ExtractorMediaSource directly and don't need to call `inferContentType`. `inferContentType` only tries to detect DASH vs HLS vs Smoothstreaming vs Other. It is not intended to detect the container format (mp4 vs mp3 vs ogg, etc). You could write your own method for this. Container detection by suffix is not something that ExoPlayer does at the moment. If you are going to use `ExtractorMediaSource` with all the default extractors (the included log suggests that), you could let the sniffing mechanism do the work for you, which should be the ground truth. Also, as @andrewlewis says
>Do you need to support DASH/HLS/SS? If not, you can probably just use ExtractorMediaSource directly and don't need to call inferContentType.
  Please provide a full bug report as requested in the issue template. A snippet of logcat is not a full bugreport, as is also explained in the issue template ;). Note also that we support FireOS on a best effort basis only, since it is not Android. Note also that this is almost certainly an issue with FireOS rather than ExoPlayer. It shouldn't be possible for anything at the application layer, which ExoPlayer is, to cause a device reboot. From the issue template:
> Capture a full bug report using "adb bugreport". Output from "adb logcat" or a
log snippet is NOT sufficient  @AquilesCanta - This might be fixed by https://github.com/google/ExoPlayer/commit/002dd72e702929a33d8940cfbaa758459e0067c6? Or alternatively, it might be a stuck playlist case? This is most likely the second. Otherwise, the errors would be propagated via `AdaptiveMediaSourceEventListener#onLoadError`.

@m35w can you please try to reproduce by

>It only happens (sometimes) when I leave my application with HLS playback running for many hours (it's a stream which has a m3u8 updated periodically).

Once this happens, you could confirm the "playlist is stuck" by loading the playlist repeatedly. The expected behavior is that the media sequence increases with an interval at most as long as the target duration. If it doesn't, it means that the server got stuck, and ExoPlayer is waiting for new segments to be added to the playlist, which is not happening.

Once you confirm, we'll use this issue to track this enhancement. Ok, this is something I have not seen before, as far as I recall. Please provide a stream that reproduces this to dev.exoplayer@gmail.com. I will have a look as soon as I have time. I suspect this might be a server side issue. Although I would expect a timeout to happen.   This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!  A similar question has been asked here: #2740, please have a look. The next version (2.4.1) will contain separate attributes for played_color and unplayed_color to allow further customization.

In the current version (2.4.0), the name played_color attribute is slightly misleading because it applies to the entire timeline. The left part is rendered opaque while the right part is rendered in the original colour.
For example, if your played_color is set to #33FFA500 (translucent orange), the played part will be painted in #FFFFA500 (opaque orange) while the unplayed part will be painted in the translucent orange. Judging from your screenshot, you are setting played_color to opaque orange, such that the played and unplayed part are both coloured the same.  I don't think this is HLS specific. We should get smarter about not retrying non-recoverable error types/codes in general, and apply consistent rules for all loads. As well as non-recoverable HTTP error codes, there are also cases like `FileNotFoundException` for local playback, and so on, where we should fail immediately.  The surface should still be holding the last frame just fine, so it's likely the black you see is actually another view being made temporarily visible on top. My guess is you're using `SimpleExoPlayerView`. It has a "shutter view" that does exactly this. Marking this as an enhancement, in which we'll provide an option on `SimpleExoPlayerView` to disable the shutter becoming visible if something has been rendered into the surface.

Shouldn't take long to add support for this, but in the meantime you could edit `SimpleExoPlayerView` yourself locally, or override the player view's layout xml to be the same as the default one but without the shutter.
  > It is unclear whether this is a bug in ExoPlayer (should ExoPlayer wait for AudioTrack release before creating a new one?) or in AudioFlinger.

ExoPlayer does wait for `release` to return in [`initialize`](https://github.com/google/ExoPlayer/blob/e87e231/library/core/src/main/java/com/google/android/exoplayer2/audio/AudioTrack.java#L635), though I'm not certain whether you're referring to the Java AudioTrack API or something lower level.

I think it's likely this is a platform issue. Do you have a bug filed on the AOSP issue tracker? If so I can follow up internally. Thanks. We release the platform AudioTrack on a background thread, but `releasingConditionVariable` ensures that a subsequent initialization of a platform AudioTrack will be blocked until that background thread has actually finished releasing the previous one.

Closing this issue, since this appears to be a platform problem and there's an AOSP issue tracking it, as ref'd above.  The end time looks too small -- the unit for the start/end position parameters is microseconds (as documented). For example, to play the first three seconds you'd pass 0 and 3000000 as the start and end positions To investigate further we'd need the information requested in the issue template, especially a link to the media you're trying to play. [Here's the issue template](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE). It's useful to be able to reproduce the error, so we need a link to the actual file you're trying to play. Please either attach it here, or email it to dev.exoplayer@gmail.com. We also need the MP4 you're trying to play (attached here or emailed). If the issue isn't reproducible in the demo app, there isn't much we can do to help. It sounds like there might be an issue with how you're passing the surface to the player but it's difficult to say.

Closing for now as this doesn't seem to be a problem with ClippingMediaSource. I haven't, but I'm having trouble coming up with any reason this would be affected by using a Fragment vs an Activity, unless your app is doing something different with how it handles Surfaces in the two cases.

Are you using SimpleExoPlayerView? If so, maybe set a breakpoint in `onRenderedFirstFrame` to verify that it's getting called to hide the `shutterView`. Could you make a small patch to the demo app that puts the SimpleExoPlayerView in a Fragment to demonstrate the issue?

Do you see video if you don't use ClippingMediaSource but just play the unclipped MediaSource? @AmrMohammed89 Glad to hear it's fixed.

@AliAbozaid Please file a new issue if you still have the same problem.  I can't look into this in detail right now, but from the logcat (please follow the issue template to get a full bug report next time), I can see a date parsing error. You could try making the parser ignore that error and see if it plays after that to make sure that's the issue. If the content is actually malformed, you might consider fixing it. I'll come back to this issue as soon as I have free time. Agree the media is malformed. 2017-05-17T03:00:52.296-6:00 isn't a valid datetime. It should be 2017-05-17T03:00:52.296-**0**6:00. That said, we could trivially handle the case by making one of the digits before the colon optional? We've made our date parsing more lenient in https://github.com/google/ExoPlayer/commit/f16967cdfeda442128edc754cba0c40949a2d429, so this stream now plays successfully. To reiterate, however, the media is not spec compliant.  A GL issue sounds like overkill. It should be perfectly possible to render UI components over the top of the surface; playback controls being the most obvious example. So an approach that should work would be to add a view on top of the surface that's transparent in the middle and black around the edges, in whatever pattern you're trying to achieve. I don't think you can do what you're attempting using `SurfaceView`. Have you tried configuring `SimpleExoPlayerView` to use a `TextureView` instead? You should be able to set `app:surface_type="texture_view"` in your layout xml to do this. Then try your original approach with `RoundedCornerLayout` again.  Thanks! It will probably have to wait until next week, but I will get to it as soon as I have some free time. Anything not supported should be ignored by default. I'll look into this as soon as possible. Sorry for the inconvenience. The issue here is that the second chunk is not a continuation of the first one. It declares an audio while the first one does not. This is not allowed by the spec unless a discontinuity is signaled.

> Each Media Segment MUST carry the continuation of the encoded
   bitstream from the end of the segment with the previous Media
   Sequence Number, where values in a series such as timestamps and
   Continuity Counters MUST continue uninterrupted.

However, even with discontinuities, ExoPlayer does not support changes in the track configuration, like adding a track type that was not there or removing one. The best way to go here is fix the media, particularly because even though the pmt does not declare audio, the audio PES packets are there.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!

Worth mentioning: The file plays well in the demo app. Why are you overriding the renderers to provide only an Opus renderer? If you're providing only an Opus renderer then you're not going to able to play WAV content.

Please also properly fill out the issue template, as has already been requested above, else we do not have sufficient information to help. If you override the factory to provide only an Opus renderer, the player is only going to be able to play Opus.

It's not feasible for us to provide sample code for every possible customization. In general, if you want to replace a default component with a custom one, you should first take a look at the default implementation to see exactly what it does. If you look at `DefaultRenderersFactory` you'll see that it includes the Opus renderer automatically if `extensionRendererMode` is set to `EXTENSION_RENDERER_MODE_ON`. So for your use case you can just use the default factory and set that parameter through its constructor.  > Is it recommended to create a custom LoadControl to address the higher memory usage?

If you want non-default buffering policy, then yes this is the recommended way to do that. Note you can also configure DefaultLoadControl to some extent, by passing different values through one of its constructors.

> If creating a custom LoadControl is recommended, then what could be a minimum buffer size that can be used that doesn't affect video start time (time required to see the first frame after clicking load button)?

The size of the buffer doesn't have any effect on start time, unless you make it so small that playback is essentially broken. The bare minimum size depends somewhat on the details of the media you're trying to play; I wouldn't recommend trying and push it too low.

> Does reducing buffer size affects/increases video start time? If so, what part of the exo player r2.2.0 should I address to decrease video start time.

As above, the buffer size shouldn't have any effect on the video start time. LoadControl's `shouldStartPlaying` method can be tweaked to have playback start more aggressively. You can also tweak the parameters passed into `DefaultLoadControl`'s constructor to do this (e.g. making `bufferForPlaybackMs` and `bufferForPlaybackAfterRebufferMs` smaller than the default values allows playback to start earlier).

> Does changing DefaultAllocator default buffer size from 64 KB to 256 KB has any affect on above scenario?

No. For what it's worth, I can't think of any technical reason why that would be the case (and I'm doubtful it is). What you're suggesting doing is basically what `SimpleExoPlayerView` does internally by itself anyway, so it's hard to see how one would be significantly slower or faster.

Closing this issue, since the original questions have been answered. Thanks!  Not right now, I'll use this issue to track Extractor factory injection.  Is the goal to use HDMI passthrough if it's available, and use FfmpegAudioRenderer otherwise? I think this should just work if you add a MediaCodecAudioRenderer to the renderers list before the FfmpegAudioRenderer, as in [DefaultRenderersFactory](https://github.com/google/ExoPlayer/blob/release-v2/library/core/src/main/java/com/google/android/exoplayer2/DefaultRenderersFactory.java#L213). The DefaultTrackSelector should use the first renderer that supports the format, which will be the MediaCodecAudioRenderer if passthrough is supported (or the device has a built-in decoder for the format), and will be the FfmpegAudioRenderer otherwise.

If that's not what you're trying to do, please clarify. Thanks. HDMI passthrough does not rely the device having any particular audio decoders. We provide the functionality via MediaCodecAudioRenderer because it's convenient. MediaCodecAudioRenderer will use HDMI passthrough if the AudioCapabilities passed to its constructor declare that passthrough is possible. FfmpegAudioRenderer on its own does not support passthrough.

To reiterate my previous comment, if HDMI passthrough is possible, MediaCodecAudioRenderer will use it. Otherwise, if there is an FfmpegAudioRenderer listed and compiled with the right decoders, it should be used and decode the audio to PCM. So you just need to make sure those two renderers are passed to the player, with the MediaCodecAudioRenderer first. Please set a breakpoint in `MediaCodecAudioRenderer.supportsFormat` and confirm that [this branch](https://github.com/google/ExoPlayer/blob/0e6ef0e/library/core/src/main/java/com/google/android/exoplayer2/audio/MediaCodecAudioRenderer.java#L148) is taken. If not, passthrough mode is not available and you can step through the code to figure out why. Some possible explanations: the HDMI connection is not advertising support for those formats, or you're passing `null`/default AudioCapabilities to the renderer's constructor. @darwinvtomy Could you provide more details about what you mean by "Also OMX.google.raw compontent is not properly configured for Passthrough ."? ExoPlayer relies on this component for passthrough mode in MediaCodecAudioRenderer. @darwinvtomy If the renderer got as far as initializing the AudioTrack then the codec may be fine. It sounds like this device has an issue with its passthrough implementation. Does passthrough mode work from any other applications on this device? How did you verify that audio is being output via passthrough, rather than being decoded on the device and output in PCM format? If the `AudioTrack` throws when trying to initialize encoded output, there's not much we can do unfortunately. Also, not having the correct HDMI audio capabilities is also problematic as there's no way for the app to detect that it should be able to output using passthrough. The provided `media_codecs.xml` files do not declare any on-device AC-3 decoders, so I don't think you're missing anything.

I suggest following up with the OEM regarding how the player in the firmware is able to use this feature. Please reopen if you get more information about how this can work, but I will close this for now as there's nothing actionable, and I think the questions from the original posts have been answered.
  You need to provide more information about exactly what you're trying to do if you want us to help.  They will work if you enable the `DefaultTsPayloadReaderFactory.FLAG_ALLOW_NON_IDR_KEYFRAMES` flag. On the release branch this is easy to do through the `DefaultExtractorsFactory#setTsExtractorFlags` method. @andrewlewis is the correct person for answering that. But, basically, the flags makes the reader assume that I-frames are also IDR frames. It could be the case that the stream contains an I-frame which is succeeded by P or B frames that references frames before the I frame. This would mean that the decoder does not have the required information to decode said P or B frame. There is some useful information in the [MediaCodec docs](https://developer.android.com/reference/android/media/MediaCodec.html), look for "Stream Boundary and Key Frames". We are evaluating turning the flag on by default. We need to make sure it is safe to do. Hope this is helpful. Yes. To emphasize: if you set the flag then play a stream where a frame after a (non-IDR) I-frame refers to a frame before that I-frame, I think you could see error concealment artifacts appearing in the decoder output, as the decoder is not receiving a valid bitstream. The artifacts could appear on frames between the frame you seek to and the next IDR frame. I don't have any data on how common it is to have streams where I-frames can't be treated as IDR frames, but anecdotally it does seem to be rare. You could try playing it with a player like ffplay/mplayer/vlc/... in verbose logging mode to see if the video decoder outputs warning messages related to bitstream errors or error concealment. On cursory inspection, it certainly looks like the bitstream is incomplete or invalid.  This happens because most segments have their first keyframe at the end. So the decoder has to discard every sample until the one that allows decoding the posterior ones. This is counter-advised by the spec:

> The server SHOULD attempt to divide the source media at points that
   support effective decode of individual Media Segments, e.g. on packet
   and key frame boundaries.

Basically, the solution (the only one I can think of) is to remux the content so that the segment division helps individual segment decoding. I don't know how to reflect this using dvbsnoop unfortunately. `mediainfo` might be the better way in this case. `ExoPlayer` is also a good choice and will be much easier/faster for you (if you need to learn how to use `mediainfo`). You should just add the proper debug logging.   Assuming that what you want is the playback position, you can use `ExoPlayer.getCurrentPosition()`. It should be pretty easy for you to implement this yourself using `Handler.postDelayed`. The application knows the rate at which it's interested in receiving updates, and so is better placed to implement polling at the desired rate. This issue is also discussed in https://github.com/google/ExoPlayer/issues/2090.  This issue tracker is not designed for you to copy/paste large amounts of your own code into to have other people debug it for you. Please can you fill out the issue template properly when filing future issues. In particular, please provide a link to the test content that wont play, along with any necessary details (e.g. license server) that would be necessary to play it.

ClearKey DRM is not supported in ExoPlayer V1. You need to be using V2 (the latest release is 2.4.0), as already suggested in https://github.com/google/ExoPlayer/issues/2798. Try starting with the V2 demo app. Add your ClearKey sample into `media.exolist.json`, which is a file in the demo app, like:

```
  {
    "name": "ClearKey DASH",
    "samples": [
      {
        "name": "Test Stream",
        "uri": "http://your-test-content-url",
        "extension": "mpd",
        "drm_scheme": "cenc",
        "drm_license_url": "your-license-url"
      }
    ]
  },
``` As already explained above, if you want a sample:
1. Stop using V1 ExoPlayer.
1. Check out the V2 demo app.
1. Add your content to `media.exolist.json` as described above.  ExoPlayer V2 re-uses decoder instances where possible as it plays through multiple periods of content (e.g. that have been concatenated using `ConcatenatingMediaSource`).

However the decoders are [always released](https://github.com/google/ExoPlayer/blob/ecb62cccd50d891338253faf768d400df0ecfa66/library/core/src/main/java/com/google/android/exoplayer2/ExoPlayerImplInternal.java#L635) and re-instantiated if the user explicitly seeks to a period other than the one currently being played, even in cases where it's possible that the instances could be retained and re-used. We should look at optimizing this case. We should also look at re-using decoder instances when a new MediaSource is passed via prepare(), where it's possible to do so.  The hprof file shows there are 30 instances of `SimpleExoPlayer`, so yes it looks like application code is just not properly releasing player. Note that this is overkill for releasing a player:

```
player.prepare(null);
player.removeListener(playerEventListener);
player.stop();
player.release();
player = null;
```

Just calling `player.release` and then null-ing the reference is sufficient. Passing `null` to `player.prepare` is incorrect and will actually cause an internal playback failure, so you should definitely remove that line.

I'm going to close this, since it's not an ExoPlayer issue. If you need help debugging your own code, I'd suggest asking on StackOverflow. It wont do any harm. I don't see why it would be necessary in general though (and our demo app doesn't unregister). The player holding a reference to a listener isn't going to stop the player from being GC'd. It's references in the opposite direction that you'd need to be wary of.  Hi Erez!

Can you please provide us with a test stream which exhibits the problem?

From the issue template:

### Link to test content
Provide a link to media that reproduces the issue. If you don't wish to post it
publicly, please submit the issue, then email the link to
dev.exoplayer@gmail.com including the issue number in the subject line. Positioning:
This isn't a regression. The reason the older release "works" is that it completely ignored the positioning information. It just so happens that the default we chose is similar to what you're specifying in the TTML file. In more recent releases we take into account positioning information but we're not handling `tts:displayAlign` properly, which is why the positioning is incorrect.

Color:
More investigation needed. Positioning is now fixed for this sample (as of the change ref'd above). Updated issue comment to refer to the remaining problem with the background color. I think the background color issue is working as intended. The way captions are styled is quite complicated, because (a) various caption formats define embedded styling, and (b) for accessibility reasons devices are required to allow global configuration of subtitle styles. Unfortunately the two weren't really considered together.

In ExoPlayer, by default, we merge the embedded style and the device specified style when deciding how captions should be rendered. Background color is defined at multiple levels:

* Window background (background of the rectangular region within which the caption is placed)
* Text background (background that bounds the text more closely on a per-line basis)
* Embedded background (background that spans only certain parts of the text, for example a single word)

In this case window background is explicitly transparent in the embedded style. Text background is undefined, and so is obtained from the device specified style. By default it's black, which is why you see a black background. You can fix this by overriding the device specified style. For example the following will result in the captions being rendered with transparent text background:

```
simpleExoPlayerView.getSubtitleView().setStyle(new CaptionStyleCompat(Color.WHITE,
    Color.TRANSPARENT, Color.TRANSPARENT, CaptionStyleCompat.EDGE_TYPE_NONE, Color.WHITE,
    null));
```  These aren't intended for general purpose use outside of the player. They're intended for the `LoadControl` specifically. Note also that they're invoked directly on the player's internal playback thread, which is normally not what you'd want if you're trying to implement some external logic.

It's likely that whatever you're trying to do would be better off done in some other way, for example using an `ExoPlayer.EventListener`. What is it that you're trying to do, and why are the existing event listener callbacks not sufficient? I don't think `LoadControl.onPrepared` is called when you think it is. It's called as soon as the playback thread receives the `MediaSource` but before it's actually done anything with it. There will typically be zero buffer at the point when it's fired. So it doesn't seem at all correct for the use case you describe.

If you want to do something when the player is ready to be played then you should use `ExoPlayer.EventListener.onPlayerStateChanged`. The first call with `playbackState == ExoPLayer.STATE_READY` is the point at which the player is ready.  Hi

I think I recall having seen this as well but it seems to me it was device specific. 

Can you please provide more info? Here are the parts of the issue template which are important for this kind of issue:

Does this also happen when you create a completely new ExoPlayer instance?

### Link to test content
Provide a link to media that reproduces the issue. If you don't wish to post it
publicly, please submit the issue, then email the link to
dev.exoplayer@gmail.com including the issue number in the subject line.

### Version of ExoPlayer being used
Specify the absolute version number. Avoid using terms such as "latest".

### Device(s) and version(s) of Android being used
Specify the devices and versions of Android on which the issue can be
reproduced, and how easily it reproduces. If possible, please test on multiple
devices and Android versions.

### A full bug report captured from the device
Capture a full bug report using "adb bugreport". Output from "adb logcat" or a
log snippet is NOT sufficient. Please attach the captured bug report as a file.
If you don't wish to post it publicly, please submit the issue, then email the
bug report to dev.exoplayer@gmail.com including the issue number in the subject
line.  This is not really an ExoPlayer question. Please ask general Android questions on StackOverflow.  On a Galaxy S7 at least, I don't think this method is ever called. The same is probably true for the Sony XZ as well. Do you actually see the behavior you describe on real Galaxy S7 and Sony XZ devices? If so, what does the call stack look like and what version of Android are they using?

These questions aside, yes, we should merge this. I doubt it'll make a difference for the devices mentioned though...? Are you using a really old version of ExoPlayer or something? You'd be better off using MediaCodecUtil.isSizeSupportedV21 (or the one that also takes rate) on API level 21 and above, which is what logic internal to ExoPlayer does.  This looks like a networking error rather than anything ExoPlayer related. I'd suggest you try serving the content from a different server. If the issue doesn't reproduce there then it's likely a server side issue.  It appears the URL you provided sporadically redirects to (at least two) different destinations. The actual content after redirection is different depending on which destination you're redirected to. For example, I saw these redirections and corresponding content lengths:

http://podcastcdn-11.ivoox.com/audio/1/1/6/3/273611.mp3?secure=tfnju6iCTdvdWZU7NTZnmg==,1494632328
Content-Length: 22589648

http://podcastcdn-7.ivoox.com/audio/1/1/6/3/desmontando-a-darwin-maximo-sandin-bioantropologo273611.mp3?secure=Z-_Y4FTl2kj_j1-BJBA_hw==,1494632302
Content-Length: 22663670

The longer of the two appears to have an extra ~9 seconds of audio prefixed on the front of the stream. What I suspect is happening is that the server, for whatever reason, is somehow prefixing the response with another ~9 second long mp3 file. I don't think the type of concatenation that the server is doing is valid. In particular, I think the XING header from the ~9 second long file is being left intact in the concatenated response. When ExoPlayer sees this XING header it uses it to calculate the duration of the media, and (correctly!) determines it to be ~9 seconds long. If the server wants to be concatenating media this way then it needs to ensure that the end result is actually a well-formed media file in its own right.  Please attach a full bug report (`adb bugreport`) as requested. This is probably a device specific issue that needs to be followed up with the device OEM.

You mention that the default player on the device can decode AC-3. If you pass `null` AudioCapabilities when constructing the audio renderer, you will likely get the same behavior. Yes: if the device has HDMI passthrough mode implemented correctly and you attach it to a receiver that supports AC-3, it should work. However, it's clear from the bug report that the AudioTrack is failing to initialize. You could try to debug this a bit yourself by verifying that the AudioTrack is being configured for [ENCODING_AC3](https://developer.android.com/reference/android/media/AudioFormat.html#ENCODING_AC3) and seeing if any other AC-3 streams play on the same device. Beyond that, I suggest asking the device manufacturer to investigate. HDMI passthrough from ExoPlayer works on other devices.

Did you try passing `null` as the AudioCapabilities when constructing the audio renderer, as I suggested above? This will cause the device's AC-3 decoder (if present) to be used even if the HDMI connection is advertising support for passthrough.

I'll close this for now as there's nothing actionable for us to do. Please reopen if you can determine that this is a bug in ExoPlayer. Thanks! `supportedEncodings=[2, 5, 6]` indicates that the audio track can output 16-bit PCM and AC-3/E-AC-3 via passthrough.

`MediaCodecInfo: NoSupport [channelCount.support, 6] [AML.google.ac3.decoder, audio/ac3] [p201, IV3012A7, amlogic, 22]` indicates that the device has an AC-3 decoder but it doesn't declare support for decoding 6 channel input. This could either be an actual limitation of the decoder, or the decoder could be under-reporting its capabilities. You could try modifying MediaCodecInfo to return `true` from `isAudioChannelCountSupportedV21` to try using the decoder anyway, and see if it works. Based on the logging you pasted, the stereo 44.1 kHz format exceeds the advertised decoder capabilities Can you follow up with the OEM regarding whether the decoder actually supports this format? If so, it is underreporting the capabilities and they should fix it.

If you change the code to return `true` from `isAudioChannelCountSupportedV21` and `isAudioSampleRateSupportedV21`, but playback still didn't work, I think you would have seen a different error. What is the error in that case? From the log: `E OMXNodeInstance: setParameter(3e:AML.google.ac3.decoder, ParamAudioPcm(0x4000002)) ERROR: UnsupportedIndex(0x8000101a)` `E ACodec  : [AML.google.ac3.decoder] configureCodec returning error -1010`.

Please follow up with the OEM to find out why `MediaCodec.configure` is throwing when provided with a stereo 44.1 kHz format. It is not surprising that an error is thrown, given that this exceeds the advertised codec capabilities, but if they support AC-3 it seems like they should support this input format. To use passthrough mode, you just an AudioCapabilities instance when constructing MediaCodecAudioRenderer, as we do in [DefaultRenderersBuilder](https://github.com/google/ExoPlayer/blob/1e12c07/library/core/src/main/java/com/google/android/exoplayer2/DefaultRenderersFactory.java#L219). Then passthrough will be used automatically if it's supported by the connected device. As I noted above, based on the AudioCapabilities logging you pasted AC-3 passthrough should work on this device, and you'll need to follow up with the OEM if it doesn't. Passthrough will be used automatically if it's available. #2148 has some more information on this.

To disable using the built-in decoder you could provide your own MediaCodecSelector implementation that returns `null` when asked for an AC-3 decoder on this device. `supportedEncodings=[2]` indicates that only 16-bit PCM can be output over the HDMI connection, not AC-3. It doesn't relate to on-device decoding.

Based on your comments above, the AudioTrack can't be initialized for passthrough output when connected to an A/V receiver, and the on-board AC-3 decoder doesn't support the format. You'd need to follow up with the OEM to address these issues. I can't really give any more assistance here. As mentioned repeatedly above, you need to get the OEM to assist with this issue. Your questions are basically answered already above: `supportedEncodings=[2, 5, 6]` indicates that the audio track can output 16-bit PCM and AC-3/E-AC-3 via passthrough, and `supportedEncodings=[2]` indicates that only 16-bit PCM can be output (which is expected if no A/V receiver is attached).

Note that AudioCapabilities tell the player what formats can be output via passthrough, and are unrelated to whether the device can decode AC-3 (for example).  @Avetri is correct. ".m2ts" support is tracked  by #1488.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent --> This is modifying 784 files in a dead release candidate branch. I'm pretty sure that's not what you intended to do? Pull requests should go to the `dev-v2` branch.  The intent structure for the demo app is described [here](https://google.github.io/ExoPlayer/demo-application.html). How to fire intents programmatically is a general Android question; you'd be best asking on StackOverflow.  No. In my experience service providers normally implement this out-of-band. For example by hosting small thumbnail images, and requesting/displaying those during a seek. Note: The response above is about ExoPlayer's support, rather than about any functionality defined by the standards themselves. I know HLS and DASH have some trick-play functionality defined, which could help with this kind of feature. We don't support it, however, and I'm not hugely familiar with exactly what they define. If you're interested then you'll need to go and read the relevant specs.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!  I'm not sure I understand the question. What's the difference between the tracks changing and the tracks playing? If the only difference is whether the player is playing or paused then you should be able to check that state and work out what you want in application code. If you're looking for points where the window and/or period indices change, you should use `ExoPlayer.EventListener.onPositionDiscontinuity`. This callback is invoked at the point in time when the indices change. Right, but you can check whether the index has changed in onPositionDiscontinuity. For example, store the index for which you're displaying metadata somewhere, then check whether the index is different in onPositionDiscontinuity. If it is then update the metadata. I think you need to provide more detail about your use case for us to assist further, because it's not really very clear exactly what you're trying to do. Specifically:

- What MediaSource are you passing in to the player (presumably some kind of concatenating source), and what's the use case for wanting to disable automatic playback across its entries?
- At the point where you'd want playback to halt, would you expect the last frame of the previous video or the first frame of the next video to be displayed? Would you expect the next video to be buffered ready for playback, or not? You should just not use ConcatenatingMediaSource for this use case. The whole point of that class is to allow you to seamlessly play through multiple pieces of media in order, but this is not what you actually want to do.

For your use case, you can prepare the player with a regular (i.e. non-concatenated) MediaSource representing whatever individual piece of media you want to play. You'll receive STATE_ENDED when the piece of media has finished playing. You can then prepare the player with a regular MediaSource representing whatever you want to play next, and so on.

To answer your final question: STATE_ENDED occurs when the entire MediaSource has finished playing. For a ConcatenatingMediaSource it occurs when the entire concatenation has finished playing (i.e. you've played up to the end of the final item in the concatenation). That's really not a good solution to this problem. You should use `ExoPlayer.EventListener.onPositionDiscontinuity` to figure out when transitions occur . `onPositionDiscontinuity` is invoked for some other cases too, but you can call `ExoPlayer.getCurrentWindowIndex` and compare it to what window you think you're in to determine whether a transition has occurred. Sorry, but it's just not a correct thing to do. `ExoPlayer.EventListener.onPositionDiscontinuity` and `ExoPlayer.getCurrentWindowIndex` are APIs intended for this purpose.

Conversely, `createPeriod` and `releasePeriod` are internal methods that are not intended for this purpose at all. They're intended for internal use by the player when it needs to instantiate and release `MediaPeriod` objects. We do not make any guarantees about when the player implementation will do this. We may choose to make changes in the future that do things like cache `MediaPeriod` instances, avoiding the need to create and release them as often, for example, which will break your usage of these methods. I'm still struggling to see why this isn't an easy thing to do. The typical use case I'm imagining is that there's a `ConcatenatingMediaSource` and the goal is to update some UI when transitioning from one source to the next. If the UI is showing state for the current source then your app is already tracking lots of state about the current source, one way or another. It's hard to imagine that tracking an extra integer makes any practical difference. It's just a matter of doing this:

```
public void onPositionDiscontinuity() {
  int newIndex = player.getCurrentWindowIndex();
  if (newIndex != currentIndex) {
    // The index has changed; update the UI to show info for source at newIndex
    currentIndex = newIndex;
  }
}
```

Why is this difficult?  We will provide a blog post and/or documentation soon. Marking this as an enhancement for tracking purposes.  ExtractorMediaSource doesn't include an extractor for WMA containers, so you'd need to implement one.

You'd also need to modify the FfmpegLibrary class so that it picks the `asf` FFmpeg decoder when it sees the relevant MIME type. Supporting WMA is tracked by https://github.com/google/ExoPlayer/issues/2155, but is low priority (i.e. is unlikely to get done unless someone outside of the core development team implements it and sends it as a pull request). #2155 tracks supporting WMA containers. Doing that involves implementing an `Extractor` subclass that parses the structure of WMA files. I'm afraid we don't have the resources to give guidance on how to implement specific extractors, but your best option is probably to start by reading/understanding one of the existing Extractor implementations. Some customizations are more complicated than others -- you're adding support for a new file format and the amount of work required will probably depend on how difficult it is to parse its structure. Hopefully the documentation on the Extractor interface and the provided implementations provide enough information to get started.

Regarding MP3 playback sounding wrong, if you file a new issue with the requested details we'll try to investigate. Note that you should be able to [play MP3s](https://developer.android.com/guide/topics/media/media-formats.html) on Android devices without resorting to using the ffmpeg extension, however.  I'm able to reproduce this. Whilst it's almost certainly a device issue, we'll take a look and see if there's anything we can do to work around it. I think the workaround ref'd above fixes this issue.  I'm not sure what "more flexible" means in concrete terms. Please could you specify exactly what you'd like to see changed?

Note that you can implement and inject your own `LoadControl` implementation if you want a custom buffering policy, so unless the interface itself isn't flexible enough then I think it's already possible for you to do what you're trying to do. I don't see any reason why you shouldn't be able to make your own implementation that allows changing of the values (as long as you take care to do it in a thread safe way). I wouldn't expect a custom `LoadControl` to cause the issues you describe. If you're able to provide a simple patch to the demo app that lets us reproduce what you've done and the issue you're seeing, then we'd take a look at that for you. I'm not able to reproduce the issue with the patch you've provided. I see playback just get stuck buffering (both audio and video stop), but that's working as intended because the patch sets the buffer to an unusably low duration when the button is pressed. The patch doesn't appear to be an accurate reflection of what you're actually trying to do, since it makes the buffer smaller* rather than bigger.

To investigate issues like this we need clear reproduction steps. For example a patch to the demo app that actually does what you're trying to do, and can be used to reproduce the issue you're seeing following clearly defined steps. Closing this issue for now, since I don't think we have that. If you're able to provide this then please let us know.

\* Note that LoadControl only determines whether more media is buffered, and so when specifying a smaller buffer the way this works is that the buffer naturally drains down to the new reduced limits as playback continues. It is not the case that LoadControl can proactively cause already buffered media to be removed from the buffer for this case.  Please provide proper reproduction steps; there's nothing we can do with questions this vague. How do you know that a BLW error is actually occurring? In absence of other information, the `onPlayerError` callback not being invoked indicates that there hasn't been an error. . !

Do you see other callbacks being invoked (e.g. if you add logging to `onLoadingChanged`)? Do you see other callbacks being invoked (and does your approach to logging work in those other callbacks)? What do you see happening to the state of the player (e.g. in `onPlayerStateChanged`) when the error occurs? Do you see any of the callbacks being invoked at any time (unrelated to the behind live window issue)? At least `onPlayerStateChanged` should be called at some point. If not, that suggests the event listener is not being registered. As Andrew says, it sounds like the listener is just not registered or there's something wrong with the way you're logging the callbacks being invoked. If `onPlayerStateChanged` were not being invoked then I think pretty much everything would be broken!

I'd suggest using the ExoPlayer [demo app](https://github.com/google/ExoPlayer/tree/release-v2/demo) as a starting point. If you see the callbacks not being invoked in the demo app then please provide clear reproduction steps. If the callbacks are being invoked there then the issue lies somewhere in your own code. It's unclear from the above whether you've resolved this, but I'm pretty sure there's no ExoPlayer issue here. As per above:

> If you see the callbacks not being invoked in the demo app then please provide clear reproduction steps.

along with a proper bug report, as is requested in our issue template. There is still no firm evidence in this bug that the `BehindLiveWindowException` you're expecting to be notified of is even occurring inside of the player, except "but BLWE has been written in logcat" with no bug report provided so we can actually see the logging you're referring to.

We're not going to look at this without complete information (including a full bug report - a small snippet of logcat is not sufficient) and proper reproduction steps.  It's been renamed to AdaptiveTrackSelection (because it's not used for audio as well). Note that it's pretty easy to answer this kind of question yourself just by looking through the project's change history (https://github.com/google/ExoPlayer/commit/ce5c0c18f952caa19b863345252fd267804d7e83).  It's `null` for non-adaptive playbacks. Adaptive (i.e. DASH, SmoothStreaming and HLS) playbacks have the concept of a manifest that provides the player with information about the available media required for playback (e.g. the available qualities, the URL of each media chunk etc). The object in these cases is a representation of the manifest. The manifest structure varies by standard, so it'll be an instance of either `DashManifest`, `SsManifest` or `HlsManifest`depending on the type of media being played.

In most cases an application can ignore the manifest. It's only really useful if a content provider is doing something like inserting custom data into their manifests on the server side that they want to get at during playback.  Is there a shutter involved? If so, are you making it invisible? If there is a SimpleExoPlayerView involved, maybe you should also add `mExoPlayerView.onRenderedFirstFrame();`. Like in:

```
@Override
public void onRenderedFirstFrame() {
    mExoPlayerView.onRenderedFirstFrame();
    LogUtil.i(this,"MainActivity.onRenderedFirstFrame.");
}
``` The problem is that `SimpleExoPlayerView` sets itself as the video listener (in its `setPlayer` method), and relies on being the registered listener. By calling `setVideoListener` yourself you're replacing the listener with your own. `SimpleExoPlayerView` then doesn't get the events that it needs.

We could change `setVideoListener` to `addVideoListener`/`removeViewListener` if you have a legitimate use case for needing to listen to the events. Normally, though, I'd only expect the player view to need these events. Marking as needs-more-info to clarify what the use case is for needing these events. Yes, looking into it into detail I noticed the class that actually implements `VideoListener` is an internal class, not actually `SimpleExoPlayerView` so the approach I presented won't work. Please reply @ojw28's comment above, so we might think about multiple listeners support. Using `setVideoListener` is appropriate for that use case. But in that use case it doesn't make sense for you to also be using `SimpleExoPlayerView`. Either the output is `SimpleExoPlayerView` (in which case it's the registered video listener) or you're outputting to some other Surface (in which case you're the registered video listener). So I don't think your use case requires the ability to have multiple video listeners simultaneously on the player... > We could change setVideoListener to addVideoListener/removeViewListener if you have a legitimate use case for needing to listen to the events.

@ojw28 I think a legitimate use case could be performance monitoring. While the SimpleExoPlayerView needs the onRenderedFirstFrame to remove the shutter, a performance monitoring listener wants to know the time it takes until the first frame is rendered and report that time.

On the other hand there is also simpleExoPlayer.setVideoDebugListener which allows to set a VideoRendererEventListener which has a onRenderedFirstFrame(Surface surface) callback which is quite the same thing.




 Right, the debug listener is suitable for that case. Closing this for now.  You can get the current window index in the [onPositionDiscontinuity](https://github.com/google/ExoPlayer/blob/release-v2/library/core/src/main/java/com/google/android/exoplayer2/ExoPlayer.java#L170) method. This method is called every time the window or period index changed, so it is guaranteed to contain the new index.

On the other hand, [onTracksChanged](https://github.com/google/ExoPlayer/blob/release-v2/library/core/src/main/java/com/google/android/exoplayer2/ExoPlayer.java#L132) gets called because the available track selection changed. This more or less happens at the same time as the window index change, but the calls to these two methods are not guaranteed to be in any particular order. In your example, it was before `onPositionDiscontinuity`. Thus, `player.getCurrentWindowIndex() ` still returned the old window index.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
 Are you using the demo app? Are you adding your sample to "media.exolist.json" or similar and including `"drm_scheme": "playready"`? Have a look at the demo app. The method `initializePlayer()` in `PlayerActivity` is a good place to start. In addition to looking at the demo app as a starting point, if you're new to ExoPlayer you should definitely be using the latest version of ExoPlayer (i.e. 2.4.0). You're using a V1 release, which is quite old now. See https://github.com/google/ExoPlayer/issues/2827.  Could you clarify why it being a single file is important? Thanks. I'm still somewhat confused. Which jars are colliding? If it's an ExoPlayer jar colliding with Unity then how does having a single ExoPlayer jar make things easier vs having multiple jars? Presumably they'll collide either way. Ok, and where are you getting the jars from and how do you include them in your project? Do you mean aars? Hm. I'm not sure it would make a difference. Can you provide details of exactly what failure you're hitting? Is there a simple way we can reproduce the issue? That sounds like an inefficiency with your development setup as much as anything. Why do you need to be copying aar files around in the first place? Can't you just set things up so that your project has the ExoPlayer modules you need included directly? As an example, we don't need to copy/paste aar files around to build our demo app whenever we make a change to one of the modules. Even if you do need to copy aar files, it feels like something you could write a pretty trivial script for.  Yup, understood. It's pretty difficult (and a lot of extra code) to validate all parts of all media for correctness, so we have a pretty broad `try { ... } catch ( ... ) { propagate as playback failure }` to handle unexpected cases without killing the process. There are however a few specific callbacks that aren't covered by this logic, one of which is the `ManifestCallback.onLoad*` callbacks.

We'll use this issue to track getting them covered correctly, after which the `IndexOutOfBoundsException` will be propagated out the front of the player as a playback failure, but will not cause process death. We do not have an ETA for this. It's preferable to fix this on the client side as a defensive measure. However if you're aware of your manifest server generating invalid manifests, you should really fix the problem on the server side at which point this becomes a non-issue. We have multiple large third party providers using ExoPlayer, who simply don't encounter this issue because their servers do the right thing.  Most likely the server doesn't support range requests, in which case the player has no alternative but to do this. If that's the case, you should use a more appropriate server configuration.  The demo app is a good place to start. If you play any content with subtitles there, you'll see a "Text" button on the top of the screen. You can modify that code a little bit to achieve what you are looking for.  Apparently this device can be upgraded to Android Nougat. Is the issue still reproducible after that update, or only on Android Marshmallow? If you have this device, please could you provide a bug report? Realistically I don't think we're going to be able to do anything here. Closing for now.

Aside: I think we decided the source claiming a Nougat update is available for this device was suffering an off-by-one error :).  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/SimpleExoPlayer.html#setVolume-float- Great; glad you figured it out!  The default time bar now draws in the vertical center of the view, and should also respond correctly to `match_parent` and `wrap_content`.  Please could you make a pull request to the `dev-v2` branch instead?  TextureView can't be used to show Widevine L1 DRM content, which requires a secure output path. See also our [FAQs](https://google.github.io/ExoPlayer/faqs.html#should-i-use-surfaceview-or-textureview). It is possible on devices that support VR mode running Android N or later. Please look at [Secure texture video playback](https://source.android.com/devices/graphics/arch-st#st_vid_play) as a starting point. If you're using the Google VR SDK you might also find the sample [sdk-videoplayer](https://github.com/googlevr/gvr-android-sdk/tree/master/samples/sdk-videoplayer) useful. It shows how to play a video using the secure output path.  Are you using [DefaultBandwidthMeter](https://github.com/google/ExoPlayer/blob/release-v2/library/core/src/main/java/com/google/android/exoplayer2/upstream/DefaultBandwidthMeter.java)? You can pass in an event listener when instantiating it if you want to handle that event. Beyond that, I'm not sure what information you're looking for, so please clarify. Thanks! Pass something that implements `BandwidthMeter.EventListener`. You don't have to pass any particular object. Please read the code for `DefaultBandwidthMeter` if you want to see when the event listener is invoked.  I tried just adding that sample URL the demo app and seeking worked fine. The only case I can think of where that wouldn't be the case is if you're serving the file from a server that doesn't indicate the content length in its response headers. In that case it's working as intended that the duration is not known, and you should configure your server more appropriately for media playback. Note: This is probably the same as another similar issue you filed, here: https://github.com/google/ExoPlayer/issues/2306.  @marcbaechinger - Could you take a look at this? It looks like `OggPacket.packetArray` is too small, and simply resizing it to fit what's being read fixes the problem. I'm not sure what the implications are though.

Are some of the assumptions in the parsing logic incorrect? `MAX_PAGE_PAYLOAD` is used as the size of `packetArray`, but also elsewhere, for example in `VorbisReader` when the format is instantiated. Yes, exactly. There is one huge packet at the start of the ogg stream (383238 bytes). Ogg packets have no size limit per spec. So this is valid. I look into this. Thanks.  I think this is the same request as #1434. Please reopen if not. I don't think we're likely to do this, for a couple of reasons:
- Different apps are going to want to compile FFMPEG to include different decoders, depending on their needs. See the FFMPEG extension section in our [supported formats page](https://google.github.io/ExoPlayer/supported-formats.html) for some of the options. Different apps may also want to provide support for different architectures. We could provide a default extension via jCenter, but it would likely be suboptimal for the majority of applications, either because it's unnecessarily large or because it lacks functionality.
- FFMPEG has some [licensing considerations](https://www.ffmpeg.org/legal.html). The easier we make it to readily bundle FFMPEG into applications, the less likely developers are to find, read and consider these. So it's in part by design that we don't make this really easy.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> Closing since I think this is already fixed in dev-v2. Please let us know if not.  Pretty sure this is fixed already (in a better way). See https://github.com/google/ExoPlayer/issues/2607.  Yes, this is on our list of follow-up work for variable speed playback.  I suspect this is an issue with the license configuration for our test content (and that it happens on all devices as a result). I see the following error when I try, which suggests the license response doesn't include the correct keys:

```
MediaCodec$CryptoException: Crypto key not available
```

@wvpaf - This is Widevine test content. Has something changed about the way the content or the licenses are configured? It appears the license response doesn't contain the correct keys to play the content. Example:

https://storage.googleapis.com/wvmedia/cenc/h264/tears/tears.mpd
https://proxy.uat.widevine.com/proxy?video_id=HDCP_V1&provider=widevine_test

@MizzleDK - It's unlikely the two issues are related. Right, on further investigation this isn't a configuration issue. Widevine have been taking a look at the reports, and it appears there are multiple things going on:

1. The following devices are thought to have L3 Widevine only (i.e. no L1). Failure is expected when using L3 Widevine if the license policy requires an HDCP level:

- Samsung Galaxy A3 SM-A300FU
- Samsung SM-T315 (Unconfirmed L3)
- Samsung GT-I9195
- KindleFireHdx8 KFAPWI (Unconfirmed L3)

2. Nexus 5 is a known device specific issue. The HTC M8 report looks the same, also.

3. The others will need further investigation.

Do you need to be requiring an HDCP level in your license configuration? It's somewhat unclear to me why HDCP is relevant for devices where the display is integrated (i.e. all phones and tablets), and it seems this requirement would prevent you from ever providing content to devices that only have L3 Widevine. I'm also surprised there's no provision for such devices; for example the possibility of streaming up to 480p without L1/HDCP. That's correct, but I *think* (please don't take this as a guarantee) that just requiring a secure output path is sufficient to guarantee HDCP in that case. My understanding was that you only have to start specifying HDCP requirements in the license policy if you care about the actual version. Hi @ojw28 is there anything actionable at this point? 
 @wvpaf - I think WV need to do some investigation into the other devices (e.g. the Exynos devices) as per our internal thread. Hi,
Digging through the provided logs looks like ;
Samsung Galaxy S6 SM-G920F & Samsung Galaxy S6 edge SM-G925F - are indeed failing HDCP. the policy on the asset is HDCP v2 and the device supports HDCPv1 : see in the log, 

<6>[782705.805478]  [2:mc_log: 3066] MobiCore mcd: d01|[Error]:tlwvdrm:: Fail to decrypt for insufficient HDCP version
<6>[782705.805502]  [2:         mc_log: 3066] MobiCore mcd: d01|[Error]:tlwvdrm:: current HDCP version is 1
<6>[782705.805517]  [2:         mc_log: 3066] MobiCore mcd: d01|[Error]:tlwvdrm:: key control block HDCP version is 2

This would be valid if there is an external device connected to the reported devices, which in this case there isnt. 
This is a bug in the OEM's implementation that was addressed in more recent versions of the devices. 
We will attempt to engage the OEM and ask if they have any plans to address.
  It would be useful to have sample media and specific steps to reproduce this, if possible. Thanks!  A fix will be available in the next push.  You are ignoring part of the template. Read the template and include all the information required there. Of particular interest here is the media file. > why use so much memory?when I use videoview,it will use little memory

VideoView and MediaPlayer both use similarly large amounts of memory. The difference is that the memory is allocated inside the media server process, where-as ExoPlayer allocates the memory inside your application process.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

This is possibly caused because the file's tracks are poorly interleaved.  @MizzleDK - Thanks for the high quality report! Unfortunately this feels like something that will be quite hard to figure out, unless you're able to determine what's special about the samples that don't work vs the ones that do. Are you able to play the content successfully on (a) other AndroidTV devices, and (b) non-Android devices? If the content works on other AndroidTV devices that might imply a device specific issue.

@sureshcnvidia (from nVidia): Are you aware of any related issues, and/or would it be worthwhile someone at nVidia taking a look and seeing what's going on internally? Thanks! Hi,

I have been off work and would be till this week.

Request to wait till Monday for me check the issue on SHIELD TV.
BR,
Suresh

Sent from my iPhone, Please excuse any typos.

On 16-May-2017, at 4:57 PM, Michell Bak <notifications@github.com<mailto:notifications@github.com>> wrote:


I still haven't made any progress on finding out if the samples differ. I'm really baffled by this.

Does it make any sense to the ExoPlayer developers that I can make it work by disabling the audio and video tracks and then re-enable them one at a time?

-
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/google/ExoPlayer/issues/2770#issuecomment-301753248>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AO4yBktGWLihJWOEWIz-o9njUH5qNJv9ks5r6YgagaJpZM4NQQsP>.

-----------------------------------------------------------------------------------
This email message is for the sole use of the intended recipient(s) and may contain
confidential information.  Any unauthorized review, use, disclosure or distribution
is prohibited.  If you are not the intended recipient, please contact the sender by
reply email and destroy all copies of the original message.
-----------------------------------------------------------------------------------
 Hi @MizzleDK would it be possible to share the link to test contents on my email id? Sounds like it indeed is a Nvidia Shield specific issue @MizzleDK Let me also validate on Nexus Player and get back on this. Great, will debug and post more info on this shortly. Definitely Nvidia Shield specific then. @MizzleDK @ojw28 

Looking into the uniqueness of the samples, the reason why it was not working on Nvidia Shield is because the encrypted samples had all IV's zeroed out. Our PlayReady reference implementation had this restricted to avoid "replay" attacks (https://crypto.stackexchange.com/questions/8600/why-should-i-use-an-initialization-vector-iv-when-i-have-unique-keys ), and always liked salted IV's. 

However, it was later realized that, if the keys are always different and do not repeat in CTR mode, having all zero IV is still secure enough. (https://crypto.stackexchange.com/questions/8151/counter-mode-static-iv-but-different-keys ). This issue has been fixed and the next OTA of Nvidia Shield would have this fixed. Thanks for the analysis @sureshcnvidia! I'll go ahead and close this, since it's not an ExoPlayer issue.  I just went through the instructions in the [README](https://github.com/google/ExoPlayer/tree/release-v2/extensions/ffmpeg) on a Mac and it worked fine. I suggest starting from the instructions in there and making the modifications you need.

This might be unrelated, but I did notice that your pasted configuration includes what look like en-dashes on the lines for enabling the MP3 and AAC decoders. You probably want to replace those with hyphens. Please run `head -n 1 ${FFMPEG_EXT_PATH}/jni/ffmpeg/ffbuild/config.log` and paste the output here. This should show the arguments passed to FFmpeg's configure script.  I'm not an expert in this area, but I'm not convinced this is a good idea. Some specific concerns that spring to mind:

- Aren't there cases where it *would* be preferable for local requests to still go through a proxy? For example if the proxy was itself local (I'm unsure if what I've written makes sense, just a hunch!).
- It's unclear whether isLocalAddress may involve a non-trivial overhead. I suspect it may do, which would adversely affect the common case.
- In the enterprise environment you speak of, I think you can probably just avoid the problem in the first place by configuring the proxy setting to exclude local addresses. Proxy settings on a phone I'm using include a "Bypass proxy for" setting. Isn't that the correct solution for this problem? Else you're relying on *every* individual application that might make local requests to explicitly ignore the proxy setting, which is non-standard behavior.  #420 has some information about caching, which should help with this.  Video codecs are heavily optimized for forward playback. It's extremely difficult to smoothly play video backward without using huge amounts of memory. It is possible to play key-frames only, but this is not something we're likely to support any time soon. Marking as an enhancement nevertheless, for if/when we do get around to it.  We will try and document customization points such as this a little better. You can do what you want as follows:

```
DefaultHttpDataSourceFactory dataSourceFactory = new DefaultHttpDataSourceFactory(userAgent);
dataSourceFactory.getDefaultRequestProperties().set("headerName", "headerValue");
return new ExtractorMediaSource(uri, dataSourceFactory, ...);
```  It sounds like you're trying to implement offlining of DRM protected DASH content, by downloading one of the video variants, and presumably one of the audio variants too. We'll be supporting this directly in the future, and this is tracked by https://github.com/google/ExoPlayer/issues/2643. Please follow that issue for updates.

We do not plan to provide a sample or guidance before we add support (since doing so is more or less equivalent, and so is similarly something that's pretty complicated and takes some time to get just right!).  I think our code expects the tunneled decoder to be listed first (or for a single decoder to support both non-tunneled and tunneled mode).  If you manually change getDecoderInfo to ignore the first (i.e. non-tunneled) decoder, do you see tunneled playback working correctly in that case? It sounds like an issue that you need to report to the device manufacturer. Tunneled mode effectively hands the A/V output responsibilities to the device manufacturer's implementation, so there's not much we can do in ExoPlayer if it's not working properly. It's hard even to query whether it *is* working properly, since we do send the data "down the tunnel" for output, and that's time we see it.  Have you read the Javadoc for Timeline, Window and Period, and getCurrentPosition? They describe the model used in some detail:

http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/Timeline.html
http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ExoPlayer.html#getCurrentPosition--

Please let us know whether you're able to answer your question, or whether you're still stuck. If still stuck, perhaps you could provide a concrete example of specific calls that are not working as you expect. If you just want elapsed time, couldn't you just time this yourself at the application level? I don't think you need any player support to do this. Beyond that, it's hard to advise what to do without access to a test stream that's setup in the way you describe. It's unclear whether this:

> Get the playback position to display it, for example, if Exoplayer has been playing the content for 1 minute and 25 seconds, I would like to display 01:25

and this:

> What I need is to get the currentPosition in the interval (0, duration).

are the same thing. They sound like different things to me, so it's still quite unclear what you're asking for. In the `Timeline` Javadoc I think you probably have a case that corresponds to the third diagram (live stream with limited availability). Is that correct? If so, are you trying to get:

1. The position relative to the start of the window. This value will normally not get larger over time, since the window is moving at the same rate as playback. This is what I'd expect to be delivered by `getCurrentPosition`.
1. The position relative to the start of the broadcast. Value will get larger over time, and will start at a value equal to approximately the time that the live stream has been running for.
1. Something else.

Are you trying to get (1), but seeing that `getCurrentPosition` isn't giving you that? I think we need working test content to assist. A small snippet of a manifest is not sufficient for us to help in an efficient way. Please provide working test content. Ah, so the problem is that you have a VOD manifest that's actually just a static snapshot of a previously broadcast (and possibly still ongoing) live stream. The timestamps in the VOD manifest don't start at t=0 for this reason, but you want positions reported as though they do. Is that correct?

Please provide working test content, and we'll take a look at figuring out how to do this. We're not going to investigate without. Thanks. It's somewhat ambiguous from the SmoothStreaming specification exactly how this use case is supposed to work. That said, we were definitely doing some things wrong with the timeline construction. I've made some changes and now you should see the window position advancing from 0 as you expect. Please give it a try by using the latest from the `dev-v2` branch! Nope, sorry. We're not going to cut a release just for this, so it'll be ready when it's ready. In the meantime you'll need to patch the change in locally if you want it.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  This is supported already (by the fact CeaDecoder uses a TreeSet). Trying to play some content of this form would presumably have allowed you to answer this question for yourself. If you're actually seeing captions being presented in the wrong order, please let us know.  I have just checked out r2.4.0 from github and successfully played the TS fragment from dropbox. Are you sure you are using that same file? Surely irrelevant, but FLAG_ALLOW_NON_IDR_FRAMES for the video to show. Can you provide a TS fragment that reproduces the issue? The live stream might make it much harder for me to reproduce. Can you send it to dev.exoplayer@gmail.com now? I have a few minutes to check it out. Thanks for the stream. You should use `MODE_SINGLE_PMT` (`mode` argument in the TsExtractor constructor). What's going on is that the PAT of your stream is declaring more PMTs than actually exist, so ExoPlayer gets stuck waiting for them, not allowing preparation to finish. I'll make it the default mode for the next push. We'll be adding a convenience method to DefaultExtractorsFactory. However, we cannot change the fact that it is a extractor mode.  This isn't useful information for the many apps that will only ever be playing content with par=1. Can we show it only if it's not equal to 1 or Format.NO_VALUE (both of which are the boring case)? Are you planning to update this? Else we will close it. Thanks.  I tried this on a Nexus 7 running Android 4.3 and the crash does not reproduce. I suspect you have more information about the affected devices, for example device model numbers, exact builds of Android 4.3, and possibly device fingerprints (which contain both of these together with additional useful information). Please can you provide this information, if you have it?

Also, what % of user devices are you seeing this issue on? Are you sure it's not just happening on devices that have been unlocked and flashed with some random unofficial/broken Android build? I'm not convinced you can't drill down further into the devices. "samsung" isn't enough information to do anything useful with. Can you not get actual device models, build numbers etc? Heh, thanks for the info. Looking at the SCH-I535 and SM-N900S specifically, can you see what the exact Android OS build is on those devices, or the full device fingerprint (this is much more specific than just "4.3"). Thanks. @erdemguven - Unsure if there's anything actionable here at this point, but if there's anything obvious we can do, it might be worthwhile :).  See [Supported formats (sample formats)](https://google.github.io/ExoPlayer/supported-formats.html#sample-formats).  As general guidance, to get this to work well I think you'd need to have some code that decides whether to apply the effect based on the frame timestamp before the frame is rendered, rather than polling the player position. The player position is determined by the audio track, but video renderers may show frames (imperceptibly) late or early relative to this position.

To give more specific help it would be useful to know what renderer you're using (e.g., `MediaCodecVideoRenderer`) and how are the effects applied (e.g., using an pixel shader). Closing due to lack of information.  To better understand this, you need to have an understanding of ExoPlayer's threading model, as described [here](http://google.github.io/ExoPlayer/doc/reference/). Note that operations performed on the player, and events delivered from the player, are both asynchronous.

This means that a STATE_READY event will be delivered to you asynchronously, and the setVolume operation will be applied asynchronously. Hence this code style of code will not synchronously set the volume to 0 before playback starts:

```
public void onPlayerStateChanged(boolean playWhenReady, int playbackState) {
        switch (playbackState){
            case ExoPlayer.STATE_READY:{
                setVolume(0f);
                ...
```

To set the volume to 0 before playback starts, you'd need to call `setVolume` before calling `setPlayWhenReady(true)`.

Note that the asynchronous application of `setVolume` means the way you're calling it repeatedly to do a crossfade is also quite tenuous. At best you have very little control to ensure that the fade will be smooth. You might be better off looking at extending `MediaCodecAudioRenderer` and implementing the fade directly in that class; although this is fairly complicated and is something you'd need to figure out for yourself. We cannot reproduce the issue when setVolume(0) is called before setPlayWhenRead(true). We tried it and it worked as expected for us. You need to follow the issue template and provide proper steps to reproduce in our demo app if you want us to investigate further, since as far as we can determine everything works correctly (which rather suggests a bug in your application code).  Are you building a release version of your APK? If not, the audio playback may not be smooth (see https://github.com/google/ExoPlayer/issues/26#issuecomment-296133260).

If you see this problem with a release version of your APK, please provide all the information requested in the issue template so we can investigate further. @clauskrogholm Testing on a Samsung Galaxy S4 running Android 4.2.2, a 44.1 kHz stereo MP3 played back fine at 1.25 speed. This was on a debuggable (non-release) build in the demo app. Please could you provide a link to the stream you're trying to play so I can try to reproduce the issue you're seeing? Thanks. I can reproduce the issue with that file. @clauskrogholm I'm afraid I spoke too soon -- although playback is not smooth with that file on a debuggable APK on my test device, it plays fine if I use a signed, release APK.

- Please could you double check that the APK you're testing with is a signed, release APK not a debuggable one? `aapt dump xmltree demo.apk AndroidManifest.xml | grep debuggable` should have no output if it's not debuggable.
- Note that some distortion is expected due to the way the speed adjustment implementation works, but audio should not cut out or stutter. (If it's not clear whether the behavior is a bug or expected, please attached a video recording.)
- It would be useful to have the output of `adb bugreport` to check if there's anything else going on and find out exactly what build the device is running. Thanks. @clauskrogholm Could you also describe what the distortion sounds like or attach a video of the problem?

We've found an issue with using extensions and variable playback speed, where the byte buffer endianness is set incorrectly. This can result in very noisy sound, rather than the stuttering playback experienced when using a debuggable APK. It's probably unrelated to your issue unless you're using the FFmpeg extension to play your MP3. @clauskrogholm I think you might be right that the distortion is expected with Sonic. One way to verify that this isn't the same performance issue would be to run on the most powerful device you can find running Android Nougat and check whether you hear the same thing.

There is a quality/performance trade-off, and if you can find another suitable speed/pitch adjustment library that gives output that sounds better when processing music we could investigate integrating it as an alternative. Thanks! One other thing that's worth trying is to use Android's regular MediaPlayer API to play the sample at 1.25x, which is supported on more recent API levels. IIRC MediaPlayer also uses Sonic internally. If you experience the same distortion using that approach then it's probably expected with Sonic. If MediaPlayer is distortion free then that would point more toward an ExoPlayer issue. In that case it sounds like things are working as intended.

@andrewlewis did find some incorrect rate/pitch adjustment logic in the Sonic library, which we've fixed in https://github.com/google/ExoPlayer/commit/631cce9171f37f98a71caa6921f5d589af4bbf4f. Apparently it does sound a little better with the fix, so give that a try and see if it helps! The fix has also been merged into Sonic, and should be merged into Android O as well.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
  This crash is entirely located in own code (HpLib_DashHpLibRendererBuilder), and is not an ExoPlayer issue. But for what it's worth, the error description `java.lang.ClassCastException: VideoPlayer cannot be cast to com.google.android.exoplayer.upstream.BandwidthMeter$EventListener` suggests that you need to make VideoPlayer implement BandwidthMeter.EventListener. If this isn't clear, please read the code for the demo app.  They are compatible, it's just that gradle isn't clever enough to figure it out. You can resolve the problem by amending your example to explicitly also depend on the most recent version of com.android.support:support-v4, like:

compile 'com.google.android.gms:play-services-ads:10.2.0'
compile 'com.google.android.exoplayer:exoplayer:r2.4.0'
compile 'com.android.support:support-v4:25.3.1'

The need to do this is a side effect of the modularization of the Android support library, and is necessary if you end up depending (directly or transitively) on both a pre-modularization version of the support library and one or more of the individual modules from a post-modularization version. In this scenario gradle doesn't have enough information to correctly resolve the dependencies.

The first modularized version was 24.2.0, so the workaround wont be necessary once play-services-ads updates to depend on a version greater than that.  I gave this a try and it seems the captions display correctly in the demo app. They're a little bit out of sync, but they're out of sync in the source content, so that's not a player issue. Which specific captions that are in the [actual caption file](https://cdn.curiositystream.com/system/MediaClosedCaptions/ccs/000/003/185/original/Flight33_Miniverse_Rev4.vtt) do you see not being displayed, or not being displayed at the time specified in the caption file?

I'm going to close this because I'm pretty sure everything is working fine, but if you believe otherwise and can provide specific information as requested above, please let us know! @ofuwape, please clarify. What seems to occur with the default option? Out-of-syncness or subtitles not displaying? In order for us to look into this, please provide specific reproduction steps in the demo app. Try not to leave anything to interpretation, this is most likely not going to be looked into unless it is clear that there is an exoplayer issue. If you are using 2.4, try the dev branch. There has been a bugfix on the DefaultTrackSelector that will make the vtt track the default selection for the provided stream.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> If you're testing a URL/URI then you should be using inferContentType(Uri uri) instead of inferContentType(String fileName). The method that takes the Uri already does this for you.

  The name `played_color` is misleading. The value is used as the bar color both to the left and to the right of the scrubbing handle, except that the opacity is overridden to be opaque for the part of the bar on the left of the handle.

You should be able to see this if you set the color like `app:played_color="#880000FF"`. In this case the section of the bar to the left of the handle should be opaque blue while the section to the right should be translucent. (The other relevant color, which can also be overridden, is the `buffered_color`; if you saw a different color to the right of the scrubbing handle, that might be due to the player having buffered past the playback position, and possibly to the end of the content.)

I'll fix the naming of the attribute, but I suspect you might also want the ability to set the colors to the left and the right of the scrubbing handle separately? Please clarify if this is the case and if so I'll mark this as an enhancement. Thanks! I suspect `#EA4239` is implicitly opaque. The color is used unmodified on the unplayed portion, and is made opaque for the played portion, and in this case there's no difference because the color is already opaque.

I'll mark this as an enhancement to track being able to specify colors for the played/buffered/unplayed parts of the bar and the scrubber handle separately.

If you want to customize how the buffered section is shown, you can make your own implementation of `TimeBar` and use it with `PlaybackControlView`.  Does this reproduce in the ExoPlayer demo app? And are you using custom renderers of some kind?
 Also, please properly fill out the issue template. If you don't provide the information we request, we're less likely to be able to help. We don't provide anything called demo_speed, as far as I know. It looks like you're using https://github.com/yangwuan55/ExoPlayer. This is an unofficial fork and is not controlled by us. It looks like the bug is specific to modifications that have been made on that fork. I'd suggest using official versions only.

Note that ExoPlayer supports changes to playback speed directly as of `2.4.0`, as described [here](https://medium.com/google-exoplayer/variable-speed-playback-with-exoplayer-e6e6a71e0343).  Please see our [FAQs](https://google.github.io/ExoPlayer/faqs.html#how-do-i-keep-audio-playing-when-my-app-is-backgrounded).  I don't have much time to look into this right now so it will have to wait until I have some time or a teammate picks it up.

In the latter case, this seem only to reproduce when a text track is enabled in the currently played source. So, demo app reproduction steps:

1. Create a playlist, like so:

```
{
        "name": "HLS PLAYLIST",
        "playlist": [
          {
            "uri": "https://devimages.apple.com.edgekey.net/streaming/examples/bipbop_4x3/bipbop_4x3_variant.m3u8"
          },
          {
            "uri": "https://devimages.apple.com.edgekey.net/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8"
          }
        ]
      },
```

2. Play using the demo app. Make sure the text track is enabled. Might not be by default.
3. Seek to the very end of the seek bar.

Expected behavior: Playback should transition to the second media source.
Actual behavior: The player goes into an endless buffering state. It's possible that this is the same as #1874, although let's leave them as separate issues until we have a chance to confirm. It's relatively likely we'll get around to fixing #1874 first, at which point we should probably just check this issue again and see if it still reproduces.  Please help us to help you by providing sufficient information when asking questions. "The call back" is not descriptive enough for us to know which callback you're referring to. It's most likely working as intended, however, since bitrate is generally only useful in the context of adaptive (e.g. DASH / SmoothStreaming / HLS) playbacks. A plain WebM file is not adaptive. Adaptive streaming formats are: DASH, HLS, SmoothStreaming. Estimates of actual link bitrate are available from `DefaultBandwidthMeter`, but that's different from media bitrate. I don't think there's a way of fetching the media bitrate for non-adaptive media.

It's pretty unclear what you're actually trying to do. Saying you want to get the current bitrate doesn't really answer the question, since that's not by itself useful, and it's unclear what you can actually usefully do with the information you're looking for. What is the actual useful user/developer feature that you're trying to implement? What bitrate do you actually want? For WebM I think bitrate would just be considered the file size divided by the duration. Exposing the file size is tracked by https://github.com/google/ExoPlayer/issues/2638. The duration is already exposed, so you'd have enough information to calculate the bitrate at that point.

Also, if you're in control of the content on the server-side then you should already know the bitrate of everything you play, without having the client report it back to you.  Windows in this context are described [here](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/Timeline.html). If you're asking about playing multiple videos at once in different windows on screen, or something like that, then the code you've highlighted is unrelated. You haven't provided a clear description of what you're actually trying to do, so it's not possible for us to assist further.  Could you send a link to the content via e-mail as well? Sorry, I haven't had time to look into this. Will update this once I do. Hello, @sheaam30. Sorry for the delay. Can you provide a link to the live stream to dev.exoplayer@gmail.com? Being able to reproduce the issue myself will make debugging much easier. I see the chunks are available, so it should not be much of an inconvenience. @sheaam30, this is a friendly ping. Please provide the actual streams to reproduce the issue (unfortunately, a copy of the manifest is not enough). 

@yueyueniao2012 Please file a new issue including all the information required in the issue template. I am not able to reproduce the described behavior in the Demo app (dev branch).  @yueyueniao2012, please file a new issue. The original reported issue and your issue might have nothing to do with each other. Closing this issue due to lack of required information. If you have encountered a similar behavior, please file a new issue, including **all** the information requested in the issue template.  The stack trace seems to indicate you are using an ExtractorMediaSource, while HlsMediaSource is the correct one for m3u8 playlists. Other than that, it seems to play well on the demo app.  We're just waiting for Bintray to link the new modules to jCenter, which appears to be at least a semi-manual process. The issue should be resolved once they do this. They're normally pretty quick, so hopefully this will be fixed pretty soon! Leaving open until linked so others can find the answer more easily.  This is not currently supported. Marking as an enhancement, but it's unlikely to be treated with any high level of priority. Yes, that should work. I think I'll close this in that case; it seems easy enough to support on the application side rather than as a player feature.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 Please can you provide a full bug report? The issue template explains how to capture one. Assuming an 8 line snippet of log output is the exact bit we need to debug this issue is probably not correct. Thanks! Your server really isn't setup well for delivering media. It's serving content using chunked transfer encoding and does not indicate the content length in the response headers. You can see this using curl:

```
curl -v http://popo.festiware.com/files/episode/content/episode_86954697df8ee7def709e39a01567a0511f72795_1.mp3 > test.mp3
* Hostname was NOT found in DNS cache
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 188.166.188.221...
* Connected to popo.festiware.com (188.166.188.221) port 80 (#0)
> GET /files/episode/content/episode_86954697df8ee7def709e39a01567a0511f72795_1.mp3 HTTP/1.1
> User-Agent: curl/7.35.0
> Host: popo.festiware.com
> Accept: */*
> 
< HTTP/1.1 200 OK
* Server nginx is not blacklisted
< Server: nginx
< Content-Type: audio/mpeg
< Transfer-Encoding: chunked
< Connection: keep-alive
< Cache-Control: no-cache
< Date: Tue, 25 Apr 2017 16:22:02 GMT
```

For this particular piece of media, the result is that the player cannot determine the media duration, cannot seek and cannot resume. It actually looks to the player as though it's a live stream. At some point the connection is being disconnected (most likely from the server side), and the best the player can do is start again.

In contrast, if you try the other link:

```
curl -v http://k003.kiwi6.com/hotlink/fhdbn390il/episode_1.mp3 > test2.mp3* Hostname was NOT found in DNS cache
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 104.28.28.76...
* Connected to k003.kiwi6.com (104.28.28.76) port 80 (#0)
> GET /hotlink/fhdbn390il/episode_1.mp3 HTTP/1.1
> User-Agent: curl/7.35.0
> Host: k003.kiwi6.com
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Tue, 25 Apr 2017 16:25:02 GMT
< Content-Type: audio/mpeg; charset=utf-8
< Content-Length: 11707455
< Connection: keep-alive
< Set-Cookie: __cfduid=d11e029c5853ae9ca4e19508439e524d21493137502; expires=Wed, 25-Apr-18 16:25:02 GMT; path=/; domain=.kiwi6.com; HttpOnly
< Last-Modified: Tue, 25 Apr 2017 13:36:10 GMT
< Cache-Control: public, max-age=1800
< ETag: "58ff50ca-b2a43f"
< CF-Cache-Status: HIT
< Expires: Tue, 25 Apr 2017 16:55:02 GMT
< Accept-Ranges: bytes
* Server cloudflare-nginx is not blacklisted
< Server: cloudflare-nginx
```

Note the presence of Content-Length in the response headers (allowing the player to determine how long the media is), that chunked transfer encoding is not used, and that the server supports partial range requests as indicated by the Accept-Ranges header (allowing the player to resume where it left off should the connection be broken).

You'll need to configure your server to better support serving of media.  Not yet supported, unfortunately. There was no plan either, but seeing increasing interest on this we might get to it. I cannot make any promises regarding the when. I will post any updates in #474.  Can't you just use `view.findViewById`? For example `view.findViewById(R.id.exo_play)`? You should be able to call it on the `SimpleExoPlayerView` instance? I'm pretty sure findViewById() searches the whole view hierarchy. You should even be able to do it on the root view of your whole activity once the `SimpleExoPlayerView` is inflated and attached (although if you have multiple such views in the hierarchy then you'd want to be more specific so you know which view you're going to find, and also starting further down the hierarchy is obviously more efficient). Yes, the views are inflated into the SimpleExoPlayerView.  Can you provide reproduction steps in the demo app? If not, can you provide a minimal code setup that reproduces the issue?  It is not clear what the issue is:

> Playing HLS REPLY,sometimes it will stuck.

What is HLS REPLY, and what does it mean it gets stuck? Does it transition into buffering state and does not go to ready state or does it block the application from responding?

 Closing due to lack of clear reproduction steps.  Did you try updating the Nexus 10 to Android 5.1? Whilst it would be good to avoid this issue on Nexus 10s that haven't taken the Lollipop update, it's obviously much lower priority if the issue has been fixed in the platform. The vast majority of Nexus 10s in the wild have taken the Android 5.1 update, and of course users who haven't taken it can opt to do so. I'm able to reproduce this, but it doesn't seem particularly easy to debug what's going wrong. It seems specific to the content, or at least the way that the content has been encoded. I'm not saying it's not encoded correctly, I'm saying that many other streams do play correctly. It's unlikely we'll be able to allocate resources to look at this in the near term.  Because it's really hard to implement it properly, in a way that neither adds a whole bunch of complexity or makes things less efficient elsewhere.

Note that this code isn't actually active in V2 at the moment (we actually never discard). There's a TODO in ChunkSampleStream to start applying the discard logic again, although in practice it's unclear whether it's that worthwhile. There's an argument to saying that if network conditions were poor and yet you've got a big enough buffer to be discarding from it, you should have probably been requesting a higher quality.
 That's one reason. We could store each chunk in its own buffer, but there are negative implications to doing so (it's not possible to do this without incurring one of the following: less efficient packing of buffered media in memory, significant additional complexity, additional memory allocations).

There are a few other reasons as well. It's hard to provide an exhaustive list, but as an example (that I haven't thought through particularly carefully, so sorry if this makes no sense :)) suppose your buffering policy wants to place a limit on the size in bytes of buffered media. How would you enforce this limit if it were possible to have to replace chunks from the middle of the buffer? You'd need to (potentially) discard some chunks from the end of the buffer regardless of their quality in order to avoid violating the policy, which would require the discard logic to know about the details of the buffering policy, which would require dependencies between components that are currently unrelated, as well as additional complexity in components like `LoadControl`.

So yeah, there are lots of fairly nuanced trade-offs involved and it's likely not worth the additional complexity/cost.
  As per Android's [Javadoc](https://developer.android.com/reference/org/xmlpull/v1/XmlPullParser.html), the `XmlPullParser.setInput` implementation is supposed to detect the input encoding.

Whatever platform you're running on, it appears to use `org.xmlpull.mxp1.MXParser` as the `XmlPullParser` implementation, and my guess would be that it's not correctly detecting the input encoding. Android devices I have to hand all seem to use `org.kxml2.io.KXmlParser`, which is detecting the input encoding correctly.

So as far as I can tell parsing is working correctly on Android devices. Non-Android platforms are outside the scope of this issue tracker.  The classes you're trying to import don't exist in release 1.5.2, so that's expected. You can only use classes that actually exist in the version of ExoPlayer you're using. Why are you using such an old version? If you want help please provide a clear description of the problem. Just saying that it gives an error is not enough information for us to help. There is insufficient information here for us to help. When using 1.5.2 the errors you see are working as intended, because the classes you're trying to import don't exist. When using 1.5.15 you haven't provided a clear description of the errors you see. It sounds like your code is just broken, and debugging your code is beyond the scope of this issue tracker.

I'd suggest you take a look at the ExoPlayer demo app, which does compile and which you can find for r1.5.15 [here](https://github.com/google/ExoPlayer/tree/r1.5.15/demo). Perhaps using that as the starting point for your project would be a good idea.

As an aside, we'd strongly recommend using the latest ExoPlayer V2 release (r2.3.1) over ExoPlayer V1.  Yes. This was a compromise we made deliberately in V2. We're aware it's annoying, but the way we used to achieve no disruption was technically incorrect for several reasons. We do plan to optimize it again in the future.  Supporting this use case is tracked by #2501.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
  @ChernyshovYuriy - Can't you just use `HttpDataSource.Factory.setDefaultRequestProperty` on the standard factory? Why do you need a custom implementation? Ah, yes. The up to date way of doing this is to use `HttpDataSource.Factory.getDefaultRequestProperties` and then edit the properties object (which is what the deprecated method does internally). Seems like the simplest approach.  There are a few existing issues about playing multiple videos at once (e.g. #1286). Please search existing issues before filing a new one.  ExoPlayer's extractors don't have the same API as MediaExtractor, but it's probably quite straightforward to implement a wrapper that gives a similar interface. (I think the main difference to be aware of is that MediaExtractor has a blocking method to read a sample, whereas ExoPlayer's extractors read data incrementally.)

I'd start by reading the [Extractor](https://github.com/google/ExoPlayer/blob/ba9a82e/library/src/main/java/com/google/android/exoplayer2/extractor/Extractor.java) interface and its dependencies, and looking at how it's used from [ExtractorMediaPeriod](https://github.com/google/ExoPlayer/blob/ba9a82e9823ce55bd0f1e5492556a06862657d16/library/src/main/java/com/google/android/exoplayer2/source/ExtractorMediaPeriod.java). As this isn't really a playback issue, providing more detailed guidance is probably outside the scope of this issue tracker.  It's not correct to assume that we cache the media as a simple file. We may do things like fragment the media into multiple cache files, for example, so that the cache eviction policy can work with finer granularity than a whole piece of media. We may also arbitrarily change the on-disk cache format at any time.

So there is no direct files access. If you want to read the data from the cache you should do so by instantiating a CacheDataSource instance and then reading from it in the normal way (i.e. calling `open` with a `DataSpec` corresponding to the piece of media, calling `read` to read the data, then calling `close` when you're done). You could write the data into a file as you're reading it, if that's what you really want.  I'm following up to find out whether GVR could handle mono audio. I'm not sure that just passing through the the input is the best option, as the audio wouldn't respond to head rotation.

If you only need to handle mono input, as a temporary workaround you could try duplicating the channel to make it stereo. One way to do that is to make `ChannelMappingAudioProcessor` publicly visible and add one before the `GvrAudioProcessor` in your `buildAudioProcessors` method. Then call `setChannelMap(new int[] {0, 0});` to make it output a stereo stream with the first channel of input duplicated. I'm not sure about the correctness of this option from a spatial audio standpoint, though. There's work underway to add support for handling mono input in the GVR audio SDK. I don't have an ETA yet but I'll update this issue when the functionality is available via `GvrAudioProcessor`.  I don't think `-PbuildDir=../test` is a valid thing to do. It looks like this argument specifies the build directory of each module relative to its root.

Using `../test` means that `library/core`, `library/dash`, `library/hls` etc all end up trying to use the same build directory `library/test`, which is probably why bad things start to happen.

For comparison, using `-PbuildDir=test` works fine, with the modules having separate build directories `library/core/test`, `library/dash/test`, `library/hls/test` etc. I'm not adverse to supporting out of tree builds; I was just saying using `../someDir` doesn't appear to be the correct way to do it.

Is `externalBuildDir` something new that you're defining, or does it already have a use already that you're changing the behavior of? If it's something new that you're defining then your proposal looks good to me. Feel free to send a pull request to the `dev-v2` branch; thanks!  I'm not sure I follow the first part of this question. What do you think is happening when the CryptoException occurs, and why do you think it's expected? If only audio is being played, that suggests to me that the CryptoException is being thrown from the audio renderer, and I don't think it is expected. Please clarify; thanks!

The second part of the question looks unrelated, and is on an unsupported device. So let's focus on the first part. If no video tracks are selected (as is the case if the tracks are all unsupported), then playback should work in the same way as if there were no video tracks in the media to start with. We wont be obtaining a video decoder or feeding it buffers, which is why I think the failure is in the audio renderer. Please could you take a look and verify this is indeed the case?

We would not expect failure in this case. Is the issue device specific, or is it occurring on all devices that don't support UHD? Does playback of the SD/HD samples succeed? Are you sure the issue is even specific to the UHD case? Thanks. @JamieAndresakis - If you're seeing video at all then that's not the same issue. This issue is specifically for the case where video tracks aren't selected.
@ChernyshovYuriy - Unless you're able to answer the questions above I'll go ahead and close this, since there's nothing obviously actionable. It's definitely a different issue. The error isn't actually the same. You're seeing "Error decrypting data: insufficient output protection" and in the original post the error was "Unknown Error".

The "insufficient output detection" error is indicative of the device refusing to decrypt the content because the protection level of the video output path isn't sufficient to protect the content. Is it possible you're plugging the Shield into an unusual display? I'd class anything that's not HDMI into a normal TV as unusual. It's possible that the Shield and the display haven't been able to negotiate HDCP if so. Alternatively, have you made any modifications to the demo app? @JamieAndresakis - No worries! I'd assume it's related to HDCP in that case. Plugging into a computer monitor isn't really a common use case. We've found that support can be quite variable (and all bets are off if you're using any kind of non-standard cable). At the platform level it's likely that the right thing is happening. I'm unsure why the license policy of the UHD sample content is different (if that is the case). In any case, let's assume everything is working as intended for now. If you encounter further issues please feel free to open a new issue.

@ChernyshovYuriy - If this means the end of your work with ExoPlayer, then best of luck in your future endeavors!

I'll close this for now, since it doesn't look like we have anything actionable at this point in time. Cool :).  ExoPlayer doesn't include any RTMP functionality at the moment, so I don't think we can answer this. Please ask your question on one of the existing [RTMP-related threads](https://github.com/google/ExoPlayer/search?q=rtmp&type=Issues).  Adding support for extraction of AMR containers is tracked by #2527. (If you've built FFmpeg with AMR decoding you should be able to play AMR tracks embedded MP4s already, though.)  ExoPlayer gets stuck in buffering after re-adding the surface view a few time.

I created a minimal example that demonstrates that problem. I used ExoPlayer 2.3.1 and tested it on a Nexus 5X with Android 7.1.2 and on a Moto G4 Play with Android 6.

I found this issue because I am removing the surface from ExoPlayer in a recyclerview when the view gets recycled but while trying to find the cause of this I created this demo without all the complexity.

It just prepares an ExoPlayer and when the user clicks on the screen it attaches / detaches the `SurfaceView`.

I logged all events and it gets stuck in buffering state:

```
04-19 08:11:47.199 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=idle
04-19 08:11:47.513 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=buffering
04-19 08:11:47.888 D/player: onTimelineChanged
04-19 08:11:47.941 D/player: onLoadingChanged
04-19 08:11:49.023 D/player: onTracksChanged
04-19 08:11:50.001 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=ready
04-19 08:11:55.792 D/player: onLoadingChanged
04-19 08:12:05.599 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=buffering
04-19 08:12:05.664 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=ready
04-19 08:12:09.579 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=buffering
04-19 08:12:09.623 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=ready
04-19 08:12:12.021 D/player: onPlayerStateChanged with playWhenRead=true and playbackState=buffering
```

```kotlin
class MainActivity : AppCompatActivity() {

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)

    val container = findViewById(android.R.id.content) as ViewGroup

    val surface = SurfaceView(this)
    container.addView(surface)

    val videoTrackSelectionFactory = AdaptiveTrackSelection.Factory(DefaultBandwidthMeter())
    val trackSelector = DefaultTrackSelector(videoTrackSelectionFactory)
    val player = ExoPlayerFactory.newSimpleInstance(this, trackSelector, DefaultLoadControl())
    player.addListener(object : ExoPlayer.EventListener {
      override fun onTracksChanged(trackGroups: TrackGroupArray?, trackSelections: TrackSelectionArray?) {
        Log.d("player", "onTracksChanged")
      }

      override fun onPlayerError(error: ExoPlaybackException?) {
        Log.d("player", "onPlayerError")
      }

      override fun onPlayerStateChanged(playWhenReady: Boolean, playbackState: Int) {
        val state = when (playbackState) {
          ExoPlayer.STATE_BUFFERING -> "buffering"
          ExoPlayer.STATE_ENDED -> "ended"
          ExoPlayer.STATE_READY -> "ready"
          ExoPlayer.STATE_IDLE -> "idle"
          else -> "unknownState$playbackState"
        }
        Log.d("player", "onPlayerStateChanged with playWhenRead=$playWhenReady and playbackState=$state")
      }

      override fun onLoadingChanged(isLoading: Boolean) {
        Log.d("player", "onLoadingChanged")
      }

      override fun onPositionDiscontinuity() {
        Log.d("player", "onPositionDiscontinuity")
      }

      override fun onTimelineChanged(timeline: Timeline?, manifest: Any?) {
        Log.d("player", "onTimelineChanged")
      }
    })


    val dataSourceFactory = DefaultDataSourceFactory(this, packageName)
    val dashUri = Uri.parse("http://www-itec.uni-klu.ac.at/ftp/datasets/DASHDataset2014/BigBuckBunny/15sec/BigBuckBunny_15s_simple_2014_05_09.mpd")
    val mediaSource = DashMediaSource(dashUri, dataSourceFactory, DefaultDashChunkSource.Factory(dataSourceFactory), null, null)
    player.prepare(mediaSource)
    player.setVideoSurfaceView(surface)
    player.playWhenReady = true

    container.setOnClickListener {
      if (container.childCount == 0) container.addView(surface)
      else container.removeView(surface)
    }
  }
}
``` Any chance of a simple example in regular Java? Working out an unfamiliar (albeit fairly obvious, by the looks of things) language and having to mess around installing Android Studio extensions isn't that efficient for us :). It's also worth trying your sample against the latest `dev-v2`, as we've made some changes around surface handling there (with a few more yet to land). I could not find any snapshot repo. Here the top version in Java. Its really trivial and just adds and removes views like I described.

```java
public class A2 extends AppCompatActivity {

  protected void onCreate(@Nullable Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    
    final ViewGroup container = (ViewGroup) this.findViewById(android.R.id.content);
    final SurfaceView surface = new SurfaceView(this);
    container.addView(surface);
    
    Factory videoTrackSelectionFactory = new Factory(new DefaultBandwidthMeter());
    final DefaultTrackSelector trackSelector = new DefaultTrackSelector(videoTrackSelectionFactory);
    DefaultLoadControl loadControl = new DefaultLoadControl();
    SimpleExoPlayer player = ExoPlayerFactory.newSimpleInstance(this, trackSelector, loadControl);
    
    player.addListener(new EventListener() {
      public void onTracksChanged(@Nullable TrackGroupArray trackGroups, @Nullable TrackSelectionArray trackSelections) {
        Log.d("player", "onTracksChanged");
      }

      public void onPlayerError(@Nullable ExoPlaybackException error) {
        Log.d("player", "onPlayerError");
      }

      public void onPlayerStateChanged(boolean playWhenReady, int playbackState) {
        String state;
        switch (playbackState) {
          case ExoPlayer.STATE_IDLE:
            state = "idle";
            break;
          case ExoPlayer.STATE_BUFFERING:
            state = "buffering";
            break;
          case ExoPlayer.STATE_READY:
            state = "ready";
            break;
          case ExoPlayer.STATE_ENDED:
            state = "ended";
            break;
          default:
            state = "unknownState" + playbackState;
        }
        Log.d("player", "onPlayerStateChanged with playWhenRead=" + playWhenReady + " and playbackState=" + state);
      }

      public void onLoadingChanged(boolean isLoading) {
        Log.d("player", "onLoadingChanged");
      }

      public void onPositionDiscontinuity() {
        Log.d("player", "onPositionDiscontinuity");
      }

      public void onTimelineChanged(@Nullable Timeline timeline, @Nullable Object manifest) {
        Log.d("player", "onTimelineChanged");
      }
    });
    
    DefaultDataSourceFactory dataSourceFactory = new DefaultDataSourceFactory(this, this.getPackageName());
    Uri dashUri = Uri.parse("http://www-itec.uni-klu.ac.at/ftp/datasets/DASHDataset2014/BigBuckBunny/15sec/BigBuckBunny_15s_simple_2014_05_09.mpd");
    DashMediaSource mediaSource = new DashMediaSource(dashUri, dataSourceFactory, new com.google.android.exoplayer2.source.dash.DefaultDashChunkSource.Factory(dataSourceFactory), null, null);
    player.prepare(mediaSource);
    player.setVideoSurfaceView(surface);
    player.setPlayWhenReady(true);
    
    container.setOnClickListener(new OnClickListener() {
      public final void onClick(View it) {
        if (container.getChildCount() == 0) {
          container.addView(surface);
        } else {
          container.removeView(surface);
        }
      }
    });
  }
}
``` Thanks. I cannot reproduce the issue with the latest `dev-v2`. Could you give that a try to see if you see the same behavior there? I just pushed some recent changes to `dev-v2` to make sure you have all of the same changes I tested with, so be sure to pull them if you give it a try. Any news on this?

I made a sample project here:

https://github.com/PaulWoitaschek/ExoPlayerAttachingBug/blob/master/app/src/main/java/de/paul_woitaschek/exoplayerbug/MainActivity.java

It immediately starts a video. When you click on the screen it removes the view. When you click it again it re-adds the view.

After a few times the video stops playing.
Tested with ExoPlayer 2.4.0 and Nexus 5x, Android 7.1.2, + Android Emulator, latest android It's a real blocker for me as I can't use ExoPlayer inside a RecyclerView as it does exactly that. Any more info I can provide? I took a look at this and was able to reproduce, thanks. It's a fundamental limitation of MediaCodec (in the Android platform) that it needs a Surface attached to it at all times. When the SurfaceView is removed from the view the underlying Surface is destroyed. This in turn forces ExoPlayer to destroy its MediaCodec instance, because there's no longer a Surface for it to be attached to.

When the SurfaceView is added back to the view hierarchy its underlying Surface is created again. ExoPlayer creates a new MediaCodec instance, however since it's a new instance it can only start decoding from the next key-frame. It does this and displays the next key-frame immediately, then waits for the playback position to reach the position of that key-frame before rendering subsequent frames. This is why even after a single remove/add cycle, you observe that the first frame that's displayed is from the future and frozen for a while.

If you remove/add multiple times the player skips ahead 1 key-frame each time. If you do this enough, the player gets stuck in a weird state where it's buffered a long way into the future compared to the correct playback position, but by successively rendering all of the key-frames it actually doesn't have any frame to render. The player doesn't think it needs to buffer, but it's also in a state where it cannot transition to the playing state because it hasn't rendered a frame.

The fundamental limitation of MediaCodec is a real pain, unfortunately. There are however things we can do in the library, and things you can do in your application:

On the library side:

- We should delay rendering the next key-frame until the playback position actually reaches the corresponding time. This will change the behavior from seeing that frame frozen for a while to seeing a blank surface for a while. Neither is a good user experience, but the latter has the benefit that the video renderer is prevented from skipping arbitrarily far ahead as a result of multiple remove/add cycles. This will prevent the stuck buffering state and allow playback to always continue properly from the next key-frame regardless of how many remove/add cycles occur, so will in general produce more predictable behavior.

On the application side, there are a bunch of things you can do to try and provide a better user experience.

- On API level 23 and above you can solve this problem using `DummySurface`, which we recently added to the library. This gives proper seamless re-join. It wont work prior to API level 23, because the approach relies on `MediaCodec.setOutputSurface`. The basic idea is to use the `DummySurface` as a means of ensuring there's always a surface attached to the `MediaCodec`. Rather than calling `player.setVideoSurfaceView`, manage the Surface lifecycle yourself. Something like this should work well from your activity:
```
    SurfaceManager manager = new SurfaceManager(player);
    surface.getHolder().addCallback(manager);
```
With:
```
 private static final class SurfaceManager implements SurfaceHolder.Callback {

    private final SimpleExoPlayer player;
    private DummySurface dummySurface;

    public SurfaceManager(SimpleExoPlayer player) {
      this.player = player;
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
      player.setVideoSurface(holder.getSurface());
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
      // Do nothing.
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
      if (dummySurface == null) {
        dummySurface = DummySurface.newInstanceV17(false);
      }
      player.setVideoSurface(dummySurface);
    }

    public void release() {
      if (dummySurface != null) {
        dummySurface.release();
        dummySurface = null;
      }
    }

  }
```
In the future, we hope to be able to hook `DummySurface` up automatically inside of the player for cases where it helps.

- For pre-API-23 there are complicated solutions involving off-screen rendering into GL textures. It would take quite a bit of time and effort to put together sample code, however.

- One easier option is to disable and re-enable the video renderer, but this causes re-buffering and so isn't ideal. Nevertheless, you can do it something like this:

```
  private static final class SurfaceManager implements SurfaceHolder.Callback {

    private final SimpleExoPlayer player;
    private final DefaultTrackSelector trackSelector;

    public SurfaceManager(SimpleExoPlayer player, DefaultTrackSelector trackSelector) {
      this.player = player;
      this.trackSelector = trackSelector;
    }

    @Override
    public void surfaceCreated(SurfaceHolder holder) {
      player.setVideoSurface(holder.getSurface());
      trackSelector.setRendererDisabled(VIDEO_RENDERER_INDEX, false);
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
      // Do nothing.
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
      player.setVideoSurface(null);
      trackSelector.setRendererDisabled(VIDEO_RENDERER_INDEX, true);
    }

  }
``` Thanks for your response! What is `VIDEO_RENDERER_INDEX`? I'll try the combination of these 2 SurfaceManagers tomorrow. 

The optimal behaviour would be for the playback just to immediately continue and not stuck. Also after each attach / detach - scroll up / down there is a different frame when paused which looks quite awkward. It's the index of the video renderer that you're disabling. You can query the type of each renderer using ExoPlayer.getRendererType(), which should allow you to figure out which index to use. We're going to use https://github.com/google/ExoPlayer/issues/677 to track the library side change mentioned above. Closing because #677 is tracking the library side change.  Something *really* weird would have to happen on the server for this to end up happening on the client. Nevertheless, it should be easy just to check the type and call into `onLoadError` instead if it's not what we expect.

There's no way playback will be able to continue in this case (unless the server corrects what it's doing before the player gives up), but it will fail more gracefully. This is fixed in `dev-v2`. The fix will be in `r2.4.0` and above, so keep a look out for that. We will likely release later this week.  If the Android TV box is connected via HDMI, please read #2148. Otherwise, this is probably covered by the enhancement in #1741. There's not nearly enough information here to investigate this. As requested in the issue template, we need a full bug report. Also, I don't know anything about RockFFPlayer, but if it's an ExoPlayer renderer then the issue probably lies there.

If you want to use ExoPlayer's HDMI passthrough support, please refer to #2148 as mentioned in my previous comment.  It's quite hard to cater for exactly what everyone wants with the UI components we're providing, at least without ending up with a huge number of parameters. For the specific behavior you want, what would you expect the control timeout logic to be? We currently do not time out the controls if the player is not initialized or in the ended state (but we do time out the controls if the player is playing or buffering). Would you expect the controls to be made visible if there was a playback error?

I think if you're going to use standard UI components, you have to accept how they behave to some extent. If you want very specific behavior it's appropriate that you should consider implementing your own UI components to do exactly what you want. That aside, we may consider supporting this depending on your answers to the questions above. Basically what you're after is just a way to disable auto-showing of the controls under all circumstances, correct? We could probably add that.

Regarding the timeout, you don't really have full control. In particular the timeout is only applied in playing and buffering states. There's no way to set a timeout that applies in the ended or not-yet-started states (you could of course do the timing yourself and manually hide, if you wanted).

Would a way of disabling auto-showing of the controls, with the timeout behavior left as it is now, be sufficient for your needs? @tonihei - Fancy adding support for this (disabling auto-showing of controls), since you've been adding a few options in the UI classes? @or-dvir - To clarify, with this setting enabled would you expect the controls to still be made visible on user input (e.g. user taps, trackball events and key presses)? Or to put it another way, which of the following do you want to suppress:

1. Just the automatic showing of controls when the playback state changes (e.g. to the ended state)
1. All showing of controls (automatic showing + showing based on user input), except when you programmatically ask for the controls to be displayed

Thanks. Thanks for the clarification. I *think* you already have the second one (setUseController(false)).  Where have you added these lines? I'm guessing you've added them in the wrong place, but you'd need to provide your build.gradle files (or at least sample build.gradle files that reproduce the issue) if you want someone to help. I tried copying those Gradle files into Android Studio and they appear to work fine for me. It's all very messy though, and there are a few unrelated warnings about how you're mixing and matching different versions of support library modules, which is a bad idea.

Are you sure there's not a button to install the missing modules immediately underneath the error? I'd suggest you try cleaning your Gradle files up a bit, making sure you have everything (e.g. Android Studio) up to date. If all else fails, I'd suggest asking on StackOverflow, since I think this is an issue with your local setup and/or Gradle files rather than with ExoPlayer.  Needs a proper description + sample media.  I don't understand this. We're not going to merge something we don't understand and wait for the description of what it does to be provided later. That's not the correct order to be doing things in. Thanks for the explanation. I see the problem, but isn't this change only a partial solution? In particular, the way `WebvttSubtitle.getCues` works looks like it would cause the position (19% in your example) to be discarded. I think a more complete fix would be to:

1. Change `isNormalCue` to `isAutoLine` and implement it as in this change, only updating the Javadoc.
1. Fix `WebvttSubtitle.getCues` to do something sensible that correctly retains the position of each cue. Note that the positions may be different (e.g. 19%, then 25% for the second line) and I think the line still needs to be incremented in this case.

Thoughts?  It wouldn't be much work to change ExoPlayer to make a range request for the `moov` atom in this case. Specifically, if we encounter an `mdat` atom having not yet encountered the `moov` atom, we'd make a range request starting at the first byte after the end of the `mdat` atom and hope to find the `moov` atom there. Marking as an enhancement. If you provide us with some sample media then we'll take a look.

As an aside, if you're generating this content yourself, you really should be putting the `moov` atom at the beginning. Even with the approach described above you need to make three requests before playback starts with the `moov` atom is at the end, compared to just one if it's at the beginning. Hence startup latency / efficiency is significantly worse if the `moov` atom is at the end. The logic for parsing atoms in `Mp4Extractor` should [initiate a seek](https://github.com/google/ExoPlayer/blob/17762eb/library/src/main/java/com/google/android/exoplayer2/extractor/mp4/Mp4Extractor.java#L263) if it needs to skip over a large amount of unneeded data (>= 256 KB). Please provide sample media so we can check whether this is working as intended.  I don't understand this change. I'm dubious it's correct given the method description includes "Returns whether or not this cue should be placed in the default position" and you're changing it to return true for something that specifically requests a non-default position... Please provide a proper description.  It's just getting the data to be organized into buffers in the way that MediaCodec expects it. It's neither a workaround nor is it related to any known bugs.  There is no known issue with `getBufferedPercentage`. If you're playing a live stream or a stream with unknown duration then 0 would be expected because it's not possible to provide a percentage when the duration of the stream is unknown. Note that you can use `getBufferedPosition` instead in such cases.

There's insufficient information provided in this issue for us to assist further, and it does not follow the issue template. So I'll go ahead and close this. If you're still having problems, please file an issue that properly adheres to the issue template. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).  Your URL appears to be redirecting to `https://accounts.google.com` rather than serving the MPD. It does this even if you simply try and load it in an incognito window in Chrome. So it looks like a problem with the way you're hosting the content.

As an aside, it's recommended that you avoid using underscores in your URLs. Use hyphens instead.  So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent -->  So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent --> Confirmed @rkrishnan2012 has signed the CLA.  So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent --> Confirmed @rkrishnan2012 has signed the CLA.  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla --> So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent -->  This is tracked by https://github.com/google/ExoPlayer/issues/1606.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent --> CLAs look good, thanks!

<!-- ok --> So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent -->  The media is broken. If you look at the media playlist for that subtitle track, it declares that each segment has a 60 second duration:

```
......
#EXTINF:60,
https://www.dr.dk/mu-online/api/1.3/subtitles/segment/urn:dr:mu:manifest:58e23e346187a4025c2641a0?segmentsizeinms=60000&index=21&subtitleType=Foreign
#EXTINF:60,
https://www.dr.dk/mu-online/api/1.3/subtitles/segment/urn:dr:mu:manifest:58e23e346187a4025c2641a0?segmentsizeinms=60000&index=22&subtitleType=Foreign
#EXTINF:60,
https://www.dr.dk/mu-online/api/1.3/subtitles/segment/urn:dr:mu:manifest:58e23e346187a4025c2641a0?segmentsizeinms=60000&index=23&subtitleType=Foreign
#EXTINF:60,
https://www.dr.dk/mu-online/api/1.3/subtitles/segment/urn:dr:mu:manifest:58e23e346187a4025c2641a0?segmentsizeinms=60000&index=24&subtitleType=Foreign
#EXTINF:60,
https://www.dr.dk/mu-online/api/1.3/subtitles/segment/urn:dr:mu:manifest:58e23e346187a4025c2641a0?segmentsizeinms=60000&index=25&subtitleType=Foreign
#EXTINF:60,
https://www.dr.dk/mu-online/api/1.3/subtitles/segment/urn:dr:mu:manifest:58e23e346187a4025c2641a0?segmentsizeinms=60000&index=26&subtitleType=Foreign
......
```

So subtitles between 26:32 and 27:05 should all be in the one (or at most two) segments that span this duration of time. If you look at the actual segment content you'll see that this is not the case; subtitles have been placed into the wrong segments.

When ExoPlayer performs a seek it will load the segments starting from the one that corresponds to the seek position. In this case the corresponding segment contains only the single subtitle that's displayed. The other subtitles are incorrectly declared in the segments that correspond to the previous ~8 minutes of the content.

The only way a player would successfully display all subtitles for this media is if it were to blindly download all of the subtitle segments regardless of what the seek position is, which is inefficient and not something we'll be doing. The correct fix is to fix the media.  The bug reports you've provided both seem to be missing their top halves, which contain information we'd like to take a look at. How are you capturing the bug reports exactly? Is it possible for you to provide a complete one? Also, what type of DRM are you using to reproduce this (Widevine, or PlayReady, or does it reproduce with either)? Thanks! Thanks. Are you able to also grab `/data/anr/traces.txt` from the device, or get the stack traces in your app when the ANR occurs from somewhere?

It's also possible that this will be fixed by Sony's M update, which coincidentally looks like it's set to resume rolling out tomorrow: http://sony-eur-eu-en-web--eur.custhelp.com/app/answers/detail/a_id/138486  I suspect the problem here is that if you don't specify the repetition count for `LoopingMediaSource` we default to `MAX_VALUE`, which in practice will loop forever. If you then concatenate something on the front there will be more than `MAX_VALUE` periods in the timeline, so we probably end up with an overflow.

You can pretty easily validate this theory by explicitly passing the number of repetitions to the `LoopingMediaSource` as a second argument. You should find `Integer.MAX_VALUE - 1` works, but `Integer.MAX_VALUE` doesn't. If so, you can pass `Integer.MAX_VALUE - 1` here as a workaround, or any value that's larger than the number of loops anyone's ever going to watch in practice (e.g. 100000).

Could you test this and let us know if that is indeed the issue? If so, we'll come up with a proper fix also.  > Before [...] attaching exoplayer to MediaSource

Before calling `prepare(MediaSource)`, ExoPlayer does not know what the stream looks like. In that case, you are on your own, unfortunately.

> Before starting playback

You can prepare the player with playWhenReady == false, see what the window looks like (`ExoPlayer.EventListener#onTimelineChanged` will help), seek to the position you are interested in, and then set playWhenReady to true. This should work, from what I understood.

> end location of HLS

I assume you mean the live edge or the start of the window.
  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
  If you're using `DefaultHttpDataSource` I guess it should just work: it uses `java.net.HttpURLConnection` and, according to this [best practices](https://developer.android.com/training/articles/security-ssl.html) page, the `HttpsURLConnection` subclass supports SNI since Android 2.3 (API level 9, which is lower than ExoPlayer's minimum API version). Have you tried it, and if so was there some issue?  Are you instantiating a new renderer or SimpleExoPlayer when you play the second video, or reusing the ones used to play the first video? Please provide full steps to reproduce the issue and I will take a look. Thanks. Unfortunately the underlying exception is not being logged properly because (based on the stack trace you posted above) it's being incorrectly accessed as a source exception.

I suspect the bug might be that `GvrAudioProcessor.release()` is not fully resetting the state, so `GvrAudioProcessor.flush()` will throw a NullPointerException when it's reused. If you need a workaround for now, please try instantiating a new GvrAudioProcessor and player each time.  What @ChernyshovYuriy said. Although guessing: There's probably a cross-protocol redirect (e.g. https -> http). You can find related issues [here](https://github.com/google/ExoPlayer/issues?utf8=%E2%9C%93&q=is%3Aissue%20cross%20protocol).  There's nothing stopping you from using ExoPlayer within a Fragment. We do not provide a demo that does this, but it shouldn't be any different than using any other component and/or library within a Fragment.  We already avoid decoder release and re-initialization if the codec advertises that it's adaptive. Furthermore, Android 6.0.1 requires decoders are adaptive according to the [Android CDD](https://source.android.com/compatibility/6.0/android-6.0-cdd.pdf). See section 5.3:

> Device implementations MUST support dynamic video resolution and frame rate switching through the
standard Android APIs within the same stream for all VP8, VP9, H.264, and H.265 codecs in real time
and up to the maximum resolution supported on the device.

So the Custom ROM is not compliant with the Android CDD, either because its codecs aren't adaptive or because they're not advertising themselves as being adaptive.

Alternatively, you're using a codec other than one of the four mentioned above. In this case there's nothing we can do at the ExoPlayer level. Don't think so. Looks like it's in the right place already. It does looks strange that a single codec component apparently supports *all* of those types. Closing since this seems like a custom ROM issue. You should contact whoever provides the ROM should you require further help working out what the codecs xml file should contain.  Using `FLAG_DETECT_ACCESS_UNITS` worked for me. Note: Providing an easy way of specifying the flags is tracked by https://github.com/google/ExoPlayer/issues/2657.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
  As described in the issue you reference, you can use [isCurrentWindowDynamic](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/SimpleExoPlayer.html#isCurrentWindowDynamic--) for this. Yes, that's a good point. The method described will only work for DASH, HLS and SmoothStreaming.

You can get on-demand (i.e. non-live) content that's not seekable. You're probably better off looking at the duration (ExoPlayer.getDuration). If it's unknown, that's a pretty good sign that you're dealing with a live stream. So you could try OR'ing this approach with the one based on the window being dynamic.

By the way, did you mean || instead of && in your comment? I don't think && is what you'd want? That approach seems like it would get it right in the vast majority of cases, yes. Give it a try and let us know.

It's actually quite difficult to determine whether a stream is live in the completely general case. If you compared an on-demand stream that doesn't define its duration or seek information and is served from a server that doesn't tell you the content length, that would be impossible to distinguish from a genuine live stream. Admittedly that combination is quite far fetched.

On-demand media isn't seekable in the case that it doesn't define any seek information (e.g. in the media container) to enable the player to perform an efficient seek. Raw MPEG-TS streams are a good example of this (HLS is seekable because the media playlist effectively provides seek information at a higher level). Given there are lots of properties you could look at to try and determine whether a stream is live, we should probably add an isLive method and implement our best-attempt logic there (which can be updated over time, if needed). Marking as an enhancement.  Prepare the player with a [ClippingMediaSource](https://github.com/google/ExoPlayer/blob/25a966dc2300161448fa74dee5ee98322ff65604/library/src/main/java/com/google/android/exoplayer2/source/ClippingMediaSource.java), passing the source you want to clip and the start/end times to its constructor: `player.prepare(new ClippingMediaSource(hlsMediaSource, 0, 120 * C.MICROS_PER_SECOND));`. Those errors are output by the media framework and I think they're harmless/expected.

Please could you provide steps to reproduce this in the demo app, including a link to the sample media (send it to dev.exoplayer@gmail.com if you don't want to post it here)?  I'm not aware of any examples. The level of abstraction you're looking at is probably higher than anything we do directly on the ExoPlayer team. I suspect you'd find people with the ability to help on StackOverflow (or similar).  The underlying platform does not provide support for MP2. It doesn't look like MP2 is allowed in the HLS spec, either. It might be possible to use the FFmpeg extension to play MP2 inside raw TS (not sure about the HLS case).

@andrewlewis - Is there a reason MP2 isn't listed as an option for FFmpeg [here](https://google.github.io/ExoPlayer/supported-formats.html)? Looks like the `mp3` decoder supports audio/mpeg-L2, so it should be added. As above, the FFmpeg extension might allow you to play MP2 successfully. It's not supported by the underlying platform. Closing.  We're using https://github.com/google/ExoPlayer/issues/2808 to track providing sample code for this use case.  Fixed in `dev-v2`.  Thank you for pointing these out.

> The error is on configuring vp8, possible I can ignore it, but want to mention it.
make[1]: *** No rule to make target 'vp8_rtcd.h'.  Stop.
Makefile:17: recipe for target 'vp8_rtcd.h' failed
make: *** [vp8_rtcd.h] Error 2

This error can be safely ignored. See comment in [1].

I am able to reproduce the armeabi and mips failures with r14b.

Regarding armeabi failure:
This is a really weird failure coming from [2]. It most certainly is a clang or android-ndk bug as it is not supposed to generate 'SSAT' instruction when -march=armv5te. Per [3] 'SSAT' is supported only on armv6 or higher. This is out of my scope, i have filed a bug internally with the android NDK team. But as far as the vp9 extension build goes, the only workaround i can provide as of now is to not use r14b (or r15 beta).

Also, FWIW armeabi is a really silly architecture that does not have many devices in the wild. So you may be ok just building for armeabi-v7a (which works with r14b).

Regarding mips failure:
This failure is coming from libyuv and again something that works with r10. I have contacted the libyuv authors about why this fails in r14b. I will update this bug once i hear back from them.

[1] https://github.com/google/ExoPlayer/blob/release-v2/extensions/vp9/src/main/jni/generate_libvpx_android_configs.sh#L109
[2] https://github.com/webmproject/libvpx/blob/master/vpx_dsp/loopfilter.c#L18
[3] http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0489c/Cihgfjab.html Unfortunately, both the armeabi and the mips issues seem to be a bug in the compiler itself. I have filed a bug with libyuv [1]. There is a fix that disables the violating code [2]. You will be able to build it once that lands.

In the meantime, the best workaround i can provide is to suggest use of android NDK r13 instead of r14+. Neither of those issues happen in r13.

Closing this bug as it is pretty much outside ExoPlayer project's scope.

[1] https://bugs.chromium.org/p/libyuv/issues/detail?id=700
[2] https://chromium-review.googlesource.com/c/486127/  I think your approach is wrong. Why are you setting the override every time the player reports that it's ready, as opposed to just setting it once?

The ExoPlayer demo app does quite a good job of demonstrating the ability to select a subset of tracks for adaptation. You can select the subset you want under the "Video" button in the demo. I'd suggest you take a look at the calls being made there. I'm quite confused about what you're trying to do. What is the difference between what you're trying to do and what the demo app successfully does? Or are you saying that the feature is broken even in the demo app? I think the problem is that the "GPRS moderate" profile appears to be way too bad for pretty much anything. With that profile enabled, it took >4 minutes to load a single fairly simply web page in Chrome.

For ExoPlayer, it appears that the requests time out due to the poor quality of the connection. Under these conditions our bandwidth estimate doesn't get updated because we're not successfully loading any data from which an estimate can be derived. I don't think it's really important what happens, however. Playback of any variant is going to fail under conditions this poor.  The stereo variant for setting volume was deprecated in the platform, at least on AudioTrack. See [here](https://developer.android.com/reference/android/media/AudioTrack.html). I do wonder what the recommended approach is though. @andrewlewis - Any ideas?  Thanks for the patch!  Are you certain `player.getCurrentPosition()` is returning 0? If you've replicated the logic in the demo app, is it possible that the real issue is `player.isCurrentWindowSeekable()` returning false?

I was able to reproduce a couple of times in the demo app, but the problem in that case was `player.isCurrentWindowSeekable()` returning false. This occurs if the steps are performed so quickly that the player doesn't manage to load a timeline between the activity being resumed and paused.  It should be pretty easy to write your own piece of code that looks at the playback position and fires events at specified times. It's not something you *need* the player to do for you. Having the player do it for you anyway for convenience is tracked by #2189.  This is working as intended. Some renderer implementations are pipelined (i.e. have a certain amount of internal buffer). If we were to wait for the first piece of the media to fully play out before moving to the next, that would require completely emptying the renderer pipelines. This in turn would result in stuttering at the point of transition. It would also break gapless audio playback.

> Handling all of them (the streams) in the renderer is difficult, since each data is quite large. Workarounds would include: do not set hasReadStreamToEnd, until the end of the current media being rendered, or introduce initial discontinuity for each (don't know how to do that)

I'm not really sure what you mean by this. It's up to your renderer implementation to choose how far it reads ahead, since it's the thing that's reading from the sample stream. If you feel your renderer is reading too far ahead, just don't read anything else from the sample stream for a while.  It has been this way since the beginning of time (or at least since Exo V1). What did change recently is that we assign an id to the track. We proactively declare a 608 track (service 1) because, as you can observe in the Apple sample HLS stream, the manifest does not always declare the CC service. TSs could also declare the track through a caption service descriptor, but often they don't.

The id is not constant however, it is the PID of the containing H264 elementary stream + 8192 (maximum valid PID + 1). I hope this answers your question. If you know that no CC service is provided in the stream, just ignore it.  I guess you are using an `ExtractorMediaSource`. In that case, one of the constructors has an `ExtractorsFactory` argument. You can implement your own factory that constructs an `Mp3Extractor` with the flag on. > Is that a good way to do that?

Not the best, in my opinion. You probably don't need all the extractors. If you are only playing mp3 files, you just need the mp3 extractor. You don't need reflection either. Something like this should be enough:

```
@Override
public Extractor[] createExtractors() {
  return new Extractor[] {new Mp3Extractor(Mp3Extractor.FLAG_ENABLE_CONSTANT_BITRATE_SEEKING)};
}
```

But it depends on your application. This issue tracker is intended for bugs and feature requests. Please direct any further questions to a programming Q&A, like stack overflow.

>How can make it without ExtractorMediaSource?

The media source exclusively depends on the kind of media you are using. You don't have much choice there. If you are playing mp3 files, you'll need an ExtractorMediaSource.
  I am a bit confused about what the issue is. 
> Link to vtt file is included in m3u8 link

Assuming the m3u8 file you mention is a master playlist, there should be no actual vtt files referenced there, but rather a media playlist, which in turn points to the vtt files. You can have a look at HLS media samples with VTT subtitles in the demo app.
>In the demo app the subtitle can be turned on in the TrackSelectionHelper class by using the selector.setSelectionOverride method

Selection overrides are an alternative `DefaultTrackSelector` parameters. Subtitles should be enabled by default in the demo app, generally.
> I'm not sure this method still utilizes adaptive streaming for the video

As long as you don't provide overrides for video selections, video should remain adaptive.

You also ignored some of the elements from the issue template, the media would have been particularly helpful to understand your issue.  Just a clarification, which @ojw28 pointed out: The DefaultTrackSelector only enables text tracks by default when they declare the preferred language (as defined by the selection parameters), when they are marked as DEFAULT, or as FORCED. This is the case of Apple 16x9 basic stream, so you can look into this in the demo app. But keep it in mind in case your media does not meet these criteria. In that case, probably something like:
```
// To keep any existing parameters we modify the current ones.
DefaultTrackSelector.Parameters selectionParameters = defaultTrackSelector.getParameters();
// The language string format is defined in RFC 5646.
defaultTrackSelector.setParameters(selectionParameters.withPreferredTextLanguage("spa"));
```
This should be made effective immediately. I hope this helps.
 To answer the original question: Using selection overrides is absolutely fine, and is the recommended approach if you want to select a specific track.

The parameters based approach @AquilesCanta outlines is good/better if you want to select a track based on rules (e.g. "I want to enable the Spanish track" in the example above). It's simpler because you don't have to look at the details of each track and its format. You can also configure the rule before you know what the tracks are.  @stellabei can you please provide a full adb bugreport? @stellabei can you please provide a **full** adb bugreport? The snippets shows the error but does not include some entries that we would expect to see in a full unfiltered bug report.  
thanks,
 @stellabei  the 'afterupgrade.txt' log does not provide any clues on the initial issue.

Are you only getting reports of issues on this one device after the upgrade? Do you know if any of these 
Motorola Droid Turbo 2 devices that succeed playback after upgrading?

Having a full adb log from a failing device would be best.  Does the same issue occur if you add and remove a regular `Surface` instead of `SimpleExoPlayerView`? If so this is not an ExoPlayer issue, and so I'd recommend filing a bug on the Android issue tracker and/or asking on StackOverflow.

Aside: Please fill in the issue template properly. It's unclear whether this is device specific, for example. Thanks! I made a minimal change to the demo app to test this. The change I made was to `PlayerActivity`. I stashed `rootView` as a member variable and modified `onClick` to remove the view using `rootView.removeViewAt(rootView.indexOfChild(simpleExoPlayerView));` if one of the buttons is pressed. I also gave `rootView` a bright background color so it's obvious when views above it have been removed properly.

Everything works as expected when I do this, so closing the issue as not reproducible. It doesn't really sound like an ExoPlayer issue anyway. If you want us to investigate further you'll need to file a new issue that properly fills out the issue template, including clear reproduction steps as to how this can be reproduced in the demo app. If you cannot reproduce the issue by making minimal modifications to the demo app, it's likely a bug in your application code.  @yanivlankry this should be fixed when the latest changes published to GitHub. 
If you wish, until then you can load local and online manifests with:
```
defaultDataSource = new DefaultDataSource(<parameters>);
DashUtil.loadManifest(defaultDataSource, manifestUri)
``` This is fixed already in `dev-v2`. `OfflineLicenseHelper` no longer has a dependency on `DashUtil`.  What do you intend to do with the data once you've retrieved it? It's very unclear exactly what you're asking for. What do you actually intend to do with the data. What does "personal use" mean? If you just want to obtain the media file, why not download it directly using a normal http request?  This is not an ExoPlayer issue. Try stack overflow. I'd try %23 as in 

`/storage/sdcard1/Music/SARIIN/ICP Tapes %23 1.mp3`  Hi @kidmiracleman, yes we're working on DASH downloading currently. It should be published in a few weeks. HLS and SmoothStreaming support will follow. Let's use this as an overall tracking issue for offline support. @ram992 yes there will be a download manager.

@KiminRyu There will be a listener interface and broadcast events depending on the download method you use. int totalSegments, int downloadedSegments, long downloadedBytes are provided but this might change. @ram992 Sorry I can't as it's hard to estimate at the moment. No. We will update this issue when significant progress is made. You may see some commits that look odd from time to time, where the commit message and actual content of the commit don't align well. This includes the one you reference. This is a side effect of the way we use internal tools for development work + mirror to GitHub.  If you're trying to implement a preview, wouldn't it be a lot more efficient to extract the thumbnail on the server-side and just deliver it as an image to the client? It feels pretty heavy to be instantiating a video decoder and buffering the head of a video stream if that's the use case.  - Given you have to tell ExoPlayer the URL (or file) that you want us to play, surely you know what the file extension is already (because you've given it to us)?
- We only report bitrate if it's defined by the media. This is typically true in DASH, HLS and SmoothStreaming where bitrates are specified at the manifest level. It's typically not true for other media, in which case we set the value to -1 as indicated by the documentation.  Please could you provide steps to reproduce in the demo app? Specifically, where do you call `getCurrentPeriodIndex`? Thanks. If you prepare the player once with a `ConcatenatingMediaSource`, you can then seek to different sources using `player.seekTo(windowIndex, positionMs)`, where `windowIndex` is the index of the source in the concatenation to seek to. For an example, please refer to [PlaybackControlView.previous/next](https://github.com/google/ExoPlayer/blob/361241f/library/src/main/java/com/google/android/exoplayer2/ui/PlaybackControlView.java#L587).

@yookore Do you have steps to reproduce the original issue reported here in the demo app?  I'm not sire I agree that it's the player's responsibility to tell you the size of a file. Can't you just query it directly (if it's a local file) or make a HEAD request (if streaming over the network)? What is it that you actually want it for? > Because file size and duration is from the same source, such as metadata of videos, so I think they may be on the same level.

One thing to consider is that in many cases the file size of a track is undefined (or not possible to determine in a reasonable way). For example in HLS and SmoothStreaming each track is fragmented into many separate files. The same applies to DASH except in certain on-demand profiles. For live playbacks the file size is always undefined. The only case where file size does make sense is playback of regular on-demand media (e.g. an mp3 or an mp4).

> I think there are some situations that this method may be useful, for example, I want to exchange different track not only based on the bandwidth but also concerning in the file size I will download.

I'm not sure I follow this. If you're doing adaptive playback (e.g. DASH, HLS and SmoothStreaming), the `Format` for each track will have the bitrate field set, which you can use as a relative measure of file size. If you're considering multiple tracks not in the context of an adaptive playback, I'm not sure exactly what your use case is.

We could consider add file size just in the case of regular on-demand media playbacks, if the server provides it to us. But I'm still confused about your use case. What is it exactly that you're trying to do? How do you end up with a single media source where track bitrates are unavailable, and where knowing the file size of each track can be used to implement a feature that it's otherwise not possible to build? Thanks! Have you considered injecting a DataSource that provides what you need? You could intercept the header information yourself in that case. I haven't tried but subclassing DefaultHttpDataSource might work for you. It's still somewhat unclear exactly what your use case is. Clearly describing a valid use case might motivate someone to take a look. Without a clearly described or possibly invalid use case this is less likely. Marking this as an enhancement for tracking, in any case.  If you have a proguard map, please could you provide the deobfuscated field name? Thanks. This stack trace looks wrong to me: though AudioTrack:779 looks correct for the call to processBuffers, it doesn't make sense for AudioTrack:816 to be do a field access which could cause this. I don't have that specific device available but I wasn't able to reproduce the error on another API 17 device. Is it possible that the map used to get this stack trace in Crashlytics is outdated?

(The ExoPlayer core library doesn't depend on GMS or Firebase so I think the GMS internal package name is spurious/unrelated.) We don't use the support library and some line numbers in the stack trace don't make sense (e.g. ExoPlayerImplInternal:2466 doesn't exist) so I think there's something going wrong with how this is being proguarded/deobfuscated, or a problem with the JVM on that device.

I'll close this for now as I don't think there's anything actionable here, but please reopen if you can find a way to reproduce this or get a different stack trace in ExoPlayer that makes sense. Thanks! As per above, this does not appear to be an ExoPlayer issue. The stack traces do not make sense. They reference line numbers and imply the existence of dependencies like ExoPlayer -> Base_Widget_AppCompat_ActionBar_TabBar that don't exist. If these devices are running a custom Android build, then given what you're seeing it's likely the Android build is broken.  That's a complicated question to answer in practice. You're probably interested in performance relative to the VP9 decoder that the platform provides, but the platform decoder varies by device and there's nothing like a (working) platform API for querying how performant a platform decoder is. Performance of both the platform decoder and the one in the extension may be affected differently by things like thermal CPU throttling, what else your app is doing and other processes that are consuming CPU, so relative performance may vary at runtime. Furthermore, what "performance" means really depends on what you want to optimize for. If you're playing 10 second clips there's probably no point optimizing for power consumption. If you're playing full length movies, it's far more important. So it's a really messy topic in practice. We provide the extension as an option, and leave it up to you to evaluate whether it works well for you (e.g. by looking at analytics data gathered from your app, evaluating your priorities and so on).

As some general guidance:

* One advantage of the extension is that you'll be running the exact same decoder software on all devices. This will better isolate you from variation (and possible bugs) in the different platform decoder implementations. If your requirements aren't particularly strenuous (e.g. short, low resolution videos) then performance probably isn't that important, in which case it may well make sense to use the extension just to eliminate this kind of potential variation between devices.
* If a device provides a hardware VP9 decoder then it will probably consume less power than the extension decoder. This is particularly relevant for long duration content. You'd hope it would be more performant in terms of max resolution / frame-rate capability as well, but I'm not sure that's a given. There's no platform API to query whether a decoder is hardware based or not, since in practice the line is quite fuzzy, but if it's called "OMX.google.vp9.decoder" you can pretty safely assume it's a Google provided software decoder. If it's something else then it's probably provided by the system-on-chip vendor and is likely hardware accelerated in some way.
* If the platform decoder is "OMX.google.vp9.decoder" then using the extension is likely to give better performance, particularly when running on older Android releases, simply because the extension will be using a more recent version of libvpx.
* There is of course an overhead in terms of application size when bundling the extension, which is not incurred when relying on the platform decoder.

@vigneshvg - Anything else to add (or anything I've said that's inaccurate :))?  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
 Thanks. We'll push a fix for this shortly. The fix is just to swap:
```
      // Should never happen.
      throw new ParserException();
```
for:
```
      // We're not interested in this metadata.
      return;
``` I think we'd need a specific reason to parse additional properties. I think we already derive width and height from the actual video stream, for example.  Seems to me the device does not provide decoding support for mpeg-2 audio. You can try the ffmpeg extension to decode it through software. Have a look at the [supported formats page](https://google.github.io/ExoPlayer/supported-formats.html) for more information.  It looks like you're using ExoPlayer V1. You should move to ExoPlayer V2 for more robust ID3 handling. Cool. Note there are a couple of remaining fixes in flight, which are tracked by https://github.com/google/ExoPlayer/issues/2604. I think your case is different though, and hopefully already fixed. If not, please respond to that issue. Note we wont be able to assist without sample content (provided privately if necessary via dev.exoplayer@gmail.com). I might be wrong, but I think the chunks are hosted in a different server than the playlist, and it's giving me a 403. Can you fix it? I gave the stream a try. It seemed to be doing something really weird where the video tracks were periodically being identified as audio, which was breaking playback. I couldn't reproduce any ID3 related failures, however. Are you sure you're still seeing ID3 failures, and not other failures related to incorrect track identification (which I suspect is an issue with the streams, although I didn't have time to confirm). I haven't gotten any ID3 errors, either. Please try the dev branch and let me know if you are still getting the same ID3 decoding error. There has been some work on that area. 

The provided chunk does not declare any video (even though the stream in which it is presented does contain video). ExoPlayer does not currently support structural changes in the TS, except for specific cases under which the removal of video does not fall. As a workaround, you could add a black screen video to the chunks that have no video. Adding support for dynamic changes in the stream tracks is not on the roadmap, unfortunately. Everything was working smoothly until I started getting a 403, around after 2 minutes of playback. Does the issue reproduce immediately for you?  Please see https://github.com/google/ExoPlayer/issues/2627. This sounds like an error in your application code. There's no enough information for us to help.  The MediaFormat class was renamed to [Format](https://github.com/google/ExoPlayer/blob/r2.3.1/library/src/main/java/com/google/android/exoplayer2/Format.java) in version 2. If that's not what you're asking about please clarify the question.  As long as you are providing the alternative audio tracks demuxed (see #2014), you can either use a track selection override (have a look at the demo app for this) or set the according selection parameters to the default track selector:

`selector.setParameters(selector.getParameters().withPreferredAudioLanguage("eng");`

> So perhaps track selection can only be done right after a prepare? 

It can be done at any point. The demo app is a good example of this.

> And is the best solution to create our own TrackSelector? Or is there an existing one that could easily fit the bill?

Unless you are doing seriously complicating the DefaultTrackSelector should provide what you need. You can even subclass it to override a specific track type selection.

Hope this helps.
  It's unclear what this question means. Also, there's a limit to the amount of per-app customization guidance we can provide. Please ask such questions on StackOverflow. This tracker is more for actual bugs/issues.  See #2014. This is not supported (except for CC). If you want to provide multiple audio track choices, the smartest choice is to demux them and declare them through EXT-X-MEDIA:TYPE="AUDIO". 

For closed captions you do not require any changes in the TS chunks, but you should add the corresponding EXT-X-MEDIA:TYPE="CLOSED-CAPTIONS" in the master playlist. Equivalent to that would be
```
extractor = new TsExtractor(TsExtractor.MODE_NORMAL, timestampAdjuster,
    new DefaultTsPayloadReaderFactory(esReaderFactoryFlags));
```
But keep in mind this is not its intended use, and we will most likely not provide support for any consequential issues.  I don't really understand the question. The ExoPlayer demo app should work fine on Android TV as well, so I'm not sure what the difference is vs mobile and tablet. FireTV is a bit different, depending on the version of FireOS. See [here](https://google.github.io/ExoPlayer/supported-devices.html). I don't know about the leanback app. I suggest you ignore it and use the most recent ExoPlayer demo app. We'd need a new issue that fills out the whole issue template to look at that.  We do filter out variants that devices claim they're unable to support. If a device is getting stuck it's likely reporting its capabilities to us incorrectly.

To the more general point, I don't think it's incorrect for us to adapt to a resolution higher than that of the display. ExoPlayer doesn't know how you're rendering the video. Some examples where it's more complicated than that:

- If you're doing something like 360 video where only part of the video is actually shown at any given time, then you probably want the video resolution to be significantly greater than that of the display.
- In the opposite direction, if you're only showing the video in a small window on the screen, you could argue we shouldn't adapt to resolutions beyond just that window.
- Even for the standard fullscreen case, if we have one variant significantly below the display resolution and another variant significantly above it, you would probably want to choose the higher one given the choice, if the device supports it.

The above aside, you can tell `DefaultTrackSelector` to enforce limits of the video size if that's the behaviour you want. Try:

```
Parameters params = defaultTrackSelector.getParameters();
Parameters newParams = params.withXXX().withXXX()...;
defaultTrackSelector.setParameters(newParams);
```

The relevant parameters methods are `setMaxVideoSize` (sets a maximum resolution), `withViewportSize` (takes a more nuanced approach, allowing one variant above the specified size to be selected, plus considers things like potential screen orientation changes) and `withViewportSizeFromContext` (ditto assuming the viewport is fullscreen on the primary display).  Have you tried `ExoPlayer.EventListener.onPlayerStateChanged()`. You can look at the demo app for a usage example. Perhaps the server is not correctly adding the #EXT-X-ENDLIST tag to the media playlists when the stream ends. In this case the server is technically indicating that it could still add segments, and the player will therefore wait in the buffering state. This should be easy to check. `onTimelineChanged` passes a copy of `HlsManifest`, from which you can access the media playlist, which has the property `hasEndTag`. Please update this issue if ExoPlayer is not behaving as expected. No. You should ask Wowza to implement the HLS spec properly. I am looking into exoplayer not transitioning into IDLE state after not being able to refresh the live window.  Hi @brosvaby can you please provide a ADB bugreport. We need to understand what the device is running and what the system thinks is happening. Hi @brosvaby any luck on the ADB bugreport?  Please, see #2619.  You probably just need to ensure the track is selected. A caption track will only be selected by default if the media indicates that this should be the case. Else you have to select it yourself. Note that in the Apple stream you reference one of the caption tracks is marked `DEFAULT=YES`, which is why that one is enabled by default.

If you take a look at the ExoPlayer demo app, you should see that you're able to select from the available text tracks by clicking the "Text" button. For the Apple sample stream you should see both VTT and 608 tracks, and be able to select between them. Once a 608 track is selected you'll receive the output. So I suggest you take a look at how the demo app does that, and also look at `DefaultTrackSelector`. The `NoSuchMethodException` issue occurs when proguard removes the constructor being invoked. It should have been fixed by https://github.com/google/ExoPlayer/commit/1eede5d5bb9c0271592b05dbd0db320214d09b18, but that fix was present in 2.2.0. Is that definitely the version you're using? If so, does your application's proguard configuration somehow override the default one that we're specifying for the library? If so, that's likely the issue.  Please fill the issue template. You can find several sample urls with parameters in the demo app.  > Firstly, I am getting the 'Multiple renderer media clocks enabled' exception fire from ExoPlayerImplInternal.jave : function 'enableRenderers'. This is because we inherit a class from SimpleDecoderAudioRenderer which, naturally, returns itself in the function getMediaClock. What is the best practice here?

This is a side effect of having both audio renderers enabled at once (i.e. the problem described below). You can only have one renderer that implements MediaClock enabled at a time, since these renderers drive timing for the playback (and two things trying to do this simultaneously doesn't make sense :)). The fix for this is just to fix the issue below.

> Secondly, (diverting around the first issue) when I playback the DASH stream...I am getting the two audio renderers both play their first streams...at the same time...together.

This is a track selection problem. It sounds like your DASH manifest contains a mixture of tracks playable with the extension and tracks playable with ExoPlayer's built in audio renderer. What do you actually want to happen? We should probably make `DefaultTrackSelector` so that it doesn't enable two renderers implementing `MediaClock` by default, since that's not a sensible thing to do. In which case behaviour would be that the first one for which a selection can be made will be enabled. Perhaps we only want to enable one renderer of each type by default. We'll use this issue to track coming up with a sensible default behaviour, in any case.

To work around the problem in the meantime, you could extend `DefaultTrackSelector` in your own codebase and override `selectTracks` to do something like (untested code):

```
selectTrack(...args...) {
  // Get the defaults.
  TrackSelection[] selections = super.selectTracks(...args...);
  // Iterate over the defaults, clearing any secondary audio selections.
  int rendererCount = rendererCapabilities.length;
  boolean seenAudioSelection = false;
  for (int i = 0; i < rendererCount; i++) {
    if (C.TRACK_TYPE_AUDIO == rendererCapabilities[i].getTrackType()
        && selections[i] != null) {
      if (seenAudioSelection) {
        // We already have audio. Clear this one.
        selections[i] = null;
      }
      seenAudioSelection = true;
    }
  }
  return selections;
}  We plan to launch an extension wrapping the [Interactive Media Ads SDK](https://developers.google.com/interactive-media-ads/docs/sdks/android/) soon, which will make it possible to play VAST and VMAP ad tags. It's still work in progress but I will make sure this issue is updated once it's submitted to `dev-v2`. Please take a look at the [IMA extension](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ima) on the development branch. The [README](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ima) has information on using the extension. Note: this is a preview, and we are still making improvements and fixes.  You can probably use `ExoPlayer.isCurrentWindowDynamic` for this. Please give it a try! Sorry, ignore the response above. I thought you were asking how to determine if the stream is live or not.

The ExoPlayer demo app lists the audio and video tracks (they're also logged by EventLogger). Take a look at what the demo app code does to achieve this, and it should be apparent how you can query the tracks and discover what's available.  Please stop repeatedly filing new issues. If you provide a clear description of the issue in #2609 then we'll reopen it.  Please stop repeatedly filing new issues. Please respond to #2609 only. You have not provided anything approaching a clear description of the issue, so we are unable to help. Also, it sounds like your issue is in your application code, as opposed to being an issue with ExoPlayer.  Same as #2609.  Same as #2609.  Not supported. But it should be fairly easy to implement. An approach I can imagine is, on each `onTimelineChanged`, compare the current playback position with `liveEdge - liveMaxLatencyDurationCount`. If less, seek to, say, `liveEdge - liveMaxLatencyDurationCount / 2`. This is just an idea, though. Having small latency might incur into many seeks, so you should evaluate what is most appropriate for your app. Hope this helps!  It's extremely unclear what the issue is that you're describing, and we will be unable to assist without a clearer description, sorry. This is really not a clear description of the issue. ExoPlayer has nothing to do with the webview. Neither does ExoPlayer do anything special with the back key; handling is typically up to the application, which points toward this being an issue in your application code.  If you're seeing the position increase a few seconds past the duration, it's probably due to the fact that headers in variable bitrate MP3s only provide enough information to see approximately in general: when seeking to a given position we have to interpolate between approximate byte offsets based on the table of contents, which can introduce some error in the timestamps of samples read after seeking. The longer the stream, the more sparse the table of contents, making the potential error larger. To seek exactly it would be necessary to load and parse the entire stream, which is wasteful.

If you're seeing the position increase more than a few seconds past the duration this is likely a different issue, so please provide detailed steps to reproduce. Thanks!  We should fix the issue for Galaxy Nexus specifically, as reported here. I'm not aware of other devices that see this kind of native crash. If you're aware of specific devices, please let us know. I think the issue is less that we're setting KEY_MAX_INPUT_SIZE at all, and more that we're just setting the value to be unnecessarily high. Codecs on these API levels are not adaptive (i.e. cannot handle seamless resolution switching). Hence ExoPlayer releases the codec and instantiates a new instance when a resolution switch occurs, which is why resolution switching isn't completely seamless on older devices.

However in V2 it appears we're setting KEY_MAX_INPUT_SIZE on the basis that the decoder might be adaptive. Importantly, this means we'll use the max input size for the highest resolution stream when configuring the codec for one of the lower resolutions. The codec may choose to instantiate more input buffers at lower resolutions than at higher ones, and hence when configuring for a lower resolution the issue may cause more memory to be allocated than would actually be needed to configure at the higher resolution.

I've pushed a change that tweaks our logic to correctly take into account whether the decoder is adaptive. Please give it a try on the `dev-v2` branch. It fixed the issue for me on Galaxy Nexus. Great. We'll try and do a bugfix release sometime in the next couple of weeks (picking up this change and a couple of others).  Unconditionally swallowing exceptions isn't really an acceptable fix for this issue. At the very least the workaround needs to be targeted to the specific devices, codecs and api levels that the issue is known to affect (as appropriate). See for example how `codecNeedsFlushWorkaround` is targeted to SDK levels, specific decoder names and devices.

As a second point, this is a fairly extreme workaround. It's likely there exists a less aggressive way of achieving the same thing. For example, is forcing `codecIsAdaptive` to `false` sufficient? If so, I'd suggest creating a targeted workaround in `MediaCodecRenderer` that does something more like that.

As a final point, pull requests should be made to `dev-v2`, not `release-v2`. Thanks! One more thing: If you're going to provide a fix for something, please describe the fix in sufficient detail for us to understand it (e.g. at the very least a bug report that includes a full stack trace demonstrating what goes wrong). Thanks!  It's extremely unclear what the issue is that you're describing, and we will be unable to assist without a clearer description, sorry. You may find this issue relevant: https://github.com/google/ExoPlayer/issues/26  I think we're already at the point where most ID3 parse errors don't stop playback. I'm hesitant to put a big try/catch block around the whole thing; we're pretty close. Seems there are still a few edge cases we're failing on, however.

We'll get the two raised here fixed. After those fixes go in, please let us know if you're still encountering failures. We can then decide whether the continue playing whack-a-mole for a little bit longer, or to give up and go for the brute force try/catch. Pretty sure the issues here are fixed in `dev-v2`. Please give it a try. Yet more robustness fixes will arrive as a result of https://github.com/google/ExoPlayer/issues/2663. Sometime this week, with any luck. Fixed in 2.4.0. By the way, you can find this out by clicking on the referenced commit(s) in the issue thread and then looking at the tags listed at the bottom of the commit message :) (it's not obvious, but they're there!).  Thanks for the report! Unfortunately the bug report didn't capture the whole playback session. Could you do an additional test run where you start `adb logcat > log.txt` with the device attached, then reproduce the issue, then kill logcat. This will generate a logcat file that's guaranteed to capture the whole playback session. Thanks!

[Aside: Logcat output isn't a substitute for a full bugreport. A full bugreport is always important. In addition, logcat output captured as described above can be helpful if the logcat captured in the bugreport didn't cover a long enough period of time]. I can't see anything obvious in the logs. It sounds like the device is broken, but it's only using L3 Widevine according to:

> 03-27 11:25:32.063 E/DRMLEVEZ( 5338): requiresSecureDecoder=false, securityLevel=L3, mimeType=video/avc

@wvpaf - Can you see anything? Could you ask the device manufacturer to take a look if not?  Please provide clear reproduction steps in the demo app if you want us to look at this, including what test media we should use. Closing due to lack of information. Also, if the issue doesn't reproduce in the demo app, that suggests the problem is in your own code rather than ExoPlayer.  Same as https://github.com/google/ExoPlayer/issues/2579.  This sounds like an issue with your code rather than with ExoPlayer. There's insufficient context for anyone to be able to help (e.g. what is "exists()").

If you believe ExoPlayer doesn't correctly apply a volume set on it prior to playback, please fill out the issue template properly and supply all of the required information, including a bug report and clear reproduction steps. If you file a new issue please provide steps to reproduce in the demo app. I tried adding `player.setVolume(0f)` immediately after creating the player in [PlayerActivity.java](https://github.com/google/ExoPlayer/blob/b98de97/demo/src/main/java/com/google/android/exoplayer2/demo/PlayerActivity.java#L262) and audio is silent from the start as expected.  Yes, there are a few cases where we don't correctly handle and propagate internal errors (which we don't expect to occur, but we should make sure to never cause process death). I'm actually quite confused by your post of stack overflow. There's no evidence there that the stack trace you've posted there actually caused your app to crash. It looks like it would have been caught and propagated to onPlayerError as expected. Please can you provide a proper bug report, or at least complete logcat showing the app crashing. If your app is actually crashing then I think you'd expect a corresponding logs to begin with FATAL EXCEPTION. Something like:
```
AndroidRuntime: FATAL EXCEPTION: threadname
AndroidRuntime: Process: your.package.name.here, PID: 1234
``` Your app is crashing due to your own code:

```
E/AndroidRuntime: FATAL EXCEPTION: main   
Process: com.mypackage.name, PID: 17960   
java.lang.IllegalStateException   
    at com.google.android.exoplayer2.util.Assertions.checkState(Assertions.java:79)   
    at com.google.android.exoplayer2.ExoPlaybackException.getSourceException(ExoPlaybackException.java:111)   
    at com.mypackage.name.ui.activities.PlayerActivity$1.onPlayerError(PlayerActivity.java:260)
```

Our Javadoc indicates that `getSourceException` throws `IllegalStateException` if `type != TYPE_SOURCE`, so you need to check the type before you make that call. Yes. Although if you don't care what type the cause is, you can just do this:
```
@Override
public void onPlayerError(ExoPlaybackException error) {
    Log.e(TAG, error.getCause().getMessage());
}
```  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> This change touches 45 files. It's going to take quite a long time for us to look at it. In the meantime, you could helpfully re-send it to the correct branch (`dev-v2`, not `release-v2`). Thanks! Ah yes, ignore comment about branch, sorry! Regarding the best direction for adding RTMP support:
- I don't think it matters too much whether it's a pure Java solution or depends on a native library, so long as any native library part doesn't contain things that people are likely to need to customize or extend.
- It should probably be a separate module if part of the core library (e.g. `library-rtmp`), or an extension (e.g. `extension-rtmp`). The difference between a library module and an extension module is that extension modules depend on external libraries to provide their functionality, where-as library modules implement it themselves.

Regarding this change specifically:
- It appears to include code taken from an LGPL licensed project. We cannot accept contributions that do this. Nor can we accept contributions that include derivative works of such projects. The CLA requires that all contributions are your original creations. So options are (a) implement RTMP from scratch yourself, or (b) follow the extension model, with the extension module having a dependency on an external RTMP library. I would have thought (b) is probably the better approach, since (a) is going to involve re-inventing the wheel, and moves the maintenance burden of the RTMP implementation to the ExoPlayer project maintainers. Pull request https://github.com/google/ExoPlayer/pull/2503 used `net.butterflytv.utils:rtmp-client` and looked promising, but the contributor hasn't yet refactored it to be a proper extension. If someone were to do that then we'd definitely take a serious look at merging such an extension. I'm going to close this due to the licensing issues mentioned above, and because https://github.com/google/ExoPlayer/pull/2503 is looking like a more promising approach. Let's try and get that one merged instead.  Looks like an authentication problem unrelated to ExoPlayer. Please see "401 Unauthorized" [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).  Closing because this is a question about your own code rather than about any code in ExoPlayer. This is not a use case we support directly.  This is a device specific issue. My understanding is that Qualcomm are investigating.
[Internal bug ref: 36511618]. No; we'll update this issue when possible.  It is virtually impossible for us to diagnose the issue with this amount of information. We will need to reproduce ourselves. From the stack trace, I assume you are feeding a transport stream. Try figuring out what are the characteristics of the stream that produce the leak, that differ from a regular TS which, according to what you said, does not reproduce the issue. Note also that the memory leak could be anywhere in your application. In the OOM shown ExoPlayer is asking for an allocation of less than 1MB, which isn't by itself unreasonable. Something in your app is consuming a lot of memory, which is why the allocation fails, but there's no evidence it's ExoPlayer that's doing this. It could equally be anything else in your application code. Closing for now, since there's nothing actionable here and no evidence ExoPlayer is the root cause. If it were ExoPlayer, I'd expect other users of the library to be reporting the same thing. I'm not sure we've received any other reports, which kind of points toward something specific to your application code (or streams). As @ojw28 said above, the stack trace only shows what triggers the exception.

I'd suggest emulating the circumstances under which the error was reported. After a few hours, a memory dump should show a suspiciously high amount of a specific type of object allocated. That will give you a significant hint pointing towards the root cause.  I think the limitation here is that you can only create a limited number of audio tracks at once (hence the code in our audio track wrapper that makes sure we've released one underlying `android.media.AudioTrack` instance before creating another). Each `SimpleExoPlayer` will use its own audio track. The limited resource isn't in your app (or process). Hopefully it's fairly intuitive that audio playback requires allocation of some resources in the platform itself, and that such resources are finite. This is why there's a limit on the number of AudioTrack instances that an application can create.

If you need to play a large number of audio streams at once you probably need to be mixing them together in application space and sending the mixed audio to a single AudioTrack instance. This is not something ExoPlayer supports.  It means what it says (the keys have expired). What license and renewal durations are your licenses configured to use?  We've not heard any reports of this. It's pretty unlikely this is happening with Google/YouTube services, else I suspect we'd have heard about it.  It's unlikely to be anything at the level of the DASH manifest or the container either, since that's all handled by ExoPlayer code. That leaves the device decoders. Is there something special/unusual about the codecs / resolutions / content of your streams? Do you know if YouTube is working correctly on these devices? I've not heard anything to suggest otherwise. Wikipedia suggests that China and US variants use a Qualcomm SoC vs an Exynos SoC for other markets, which could be relevant. It could also be that the roll-out of Nougat hasn't reached any non-US devices yet. They definitely have the ability to deliver regular MP4 streams, but as far as I know they don't have a dynamic fallback that will transparently fall back from DASH to regular MP4 on a given device. I think it's more of a back-end configuration thing where certain devices can be blacklisted for DASH streams. So if YouTube DASH playbacks are failing on S7, I'd expect users to notice. I don't think that's related. 10 bit color scheme definitely falls into the "special case" category. 10 bit color depth is unsupported on Android except for some HDR Android TV devices with tunneled mode enabled.  If you have demuxed audio and video then disabling the renderer is actually the right thing to do. We'll stop requesting the video stream in that case, so there will be a big bandwidth saving. We'll seamlessly rejoin the stream when the renderer is enabled again.

If your audio and video are demuxed then you can do the above, but the rejoin wont be seamless. The right thing for seamless is to detach the surface as you are doing, and for us to fix this bug. There are actually a bunch of things slightly wrong with surface attach/detach whilst the renderer is enabled. We'll use this issue to track these problems in general. We still have some further changes to come around making surface switches more seamless, but the initial issue as reported here should now be fixed.  See #1894 and similar issues (there are plenty). It works if you enable FLAG_ALLOW_NON_IDR_KEYFRAMES.  There are a few reasons:

- MediaExtractor uses a threading model that's not well suited to ExoPlayer.
- It also doesn't provide as much control over buffering as we'd like.
- Implementation of the extractors under MediaExtractor varies between versions of Android.
- Extractors under MediaExtractor lack some functionality that we're able to support in ExoPlayer extractors.

Implementing our own extractors avoids all of the above. It also means that we can add new features to the extractors and have them work on all versions of Android, rather than adding them into the platform and have them work on new versions of Android only.  How to prepare/encrypt content (on the server-side) is out of scope of this issue tracker. ExoPlayer is only responsible for client side playback. The ExoPlayer demo app demonstrates playback of Widevine protected content, so that and @richardissuperman 's response above should serve as a good starting point. You should also test your content using [Shaka Player](https://shaka-player-demo.appspot.com/demo/). Select "custom asset" from the bottom of the asset drop down. If you content doesn't play there either, then something is wrong with your content or license server. This is beyond the scope of this issue tracker. If you have content that plays successfully in Shaka Player but not in ExoPlayer, please let us know. Please don't file identical issues. As per my response above:

> You should also test your content using Shaka Player. Select "custom asset" from the bottom of the asset drop down. If you content doesn't play there either, then something is wrong with your content or license server. This is beyond the scope of this issue tracker. If you have content that plays successfully in Shaka Player but not in ExoPlayer, please let us know.

  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!
  You can try using a `LoopingMediaSource` on top of the `ConcatenatingMediaSource` to achieve repetition. Depending on whether it's repeat all or one, you use or don't use the `ConcatenatingMediaSource`.  I'm sure there are ways to implement this along the lines of what you suggest, but to do it really seamlessly (and simply) we'd probably need to add some direct support. Marking as an enhancement. Setting the repeat mode is now supported in the dev-v2 branch (`ExoPlayer.setRepeatMode`). You can also enable a toggle on `SimpleExoPlayerView` (`SimpleExoPlayerView.setRepeatToggleModes`). Please give it a try!  Thanks for reporting this. It looks like we should subtract `streamOffsetUs` in `BaseRenderer.skipToKeyframeBefore`, so that it becomes:
```
protected void skipToKeyframeBefore(long timeUs) {
  stream.skipToKeyframeBefore(timeUs - streamOffsetUs);
}
```
If you could confirm whether that fixes the issue, that would be great. Thanks! Yes, we're aware that's a problem. Feel free to file a separate issue for tracking purposes.

We're currently trying to figure out exactly how this kind of case should behave. Note that you can disable the renderer (rather than just not having a Surface attached to it) as one way to avoid this problem, although you may find that enabling it again is not seamless. We may change it to be seamless in the future and at that point require that the renderer be disabled, although this is still under discussion.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!  The issue here might be that there is a change in the number of tracks included in the transport streams. To analyze ts files `mediainfo` and `dvbsnoop` are useful. With these you can check the number and type of tracks included in the underlying ts files. Using a debugger and adding breakpoints to ExoPlayer is also a good way, but might take longer to do efficiently.

If you need to concatenate different HLS streams, instead of putting them in the same playlist, it might be an option to use a `ConcatenatingMediaSource`. The ts themselves might be well formed, but a change in the number of tracks (for example, audio or video appearing/disappearing) can cause this, if they belong in the same playlist. We won't be able to investigate the issue without the media, unfortunately. The provided .ts contains two PMTs, and the first one only declares audio. This would make ExoPlayer fail only when there is a discontinuity tag for this segment or when adaptation happens when playing a chunk with similar characteristics, I think. Removing the first PMT from the stream should make it work. As mentioned before, please try reproducing the issue after removing the PMT that only declares audio. If you manage to reproduce the issue in that case, comment again below, including all the information requested in the issue template.  Please see #1263.

With the FLAG_DETECT_ACCESS_UNITS, It will play well on Galaxy S6, at least. I didn't manage to play the sample on Sony Bravia 4k. But that is a different issue.  You can easily make the display go black when the player transitions to the ended state by having a black View above the surface that you make visible when you see the transition.

Showing the last frame in this case is surprisingly non-trivial to do without incurring any additional cost (e.g. latency through the rendering pipeline), so we've kind of ignored the problem up until now. It doesn't seem like a big issue. We'll use this issue for tracking, but it's unlikely we'll be fixing it any time soon. I don't think that's what happens. The last frame of the video should remain visible. Are you sure the last frame in the video isn't black (or whatever "empty" means in this case)? Hm. Not sure about that. I don't think we have much control over this kind of thing at the ExoPlayer level; we simply release buffers to the surface (and that's about it). I don't know what EMVideoView does internally. You should probably report the issue to the author of that class. In any case, I don't think this discussion is relevant to this issue.  I haven't managed to reproduce in SONY BRAVIA 2015 or SONY BRAVIA 4K. Adaptation works well there.

But please have a look at #1411 and #953.  Duplicate of https://github.com/google/ExoPlayer/issues/2588.  Please try the latest dev version. Since 2.1 we have added support for multiple CC channels in a Transport Stream. Note that Exoplayer automatically detects CC services only if they are signaled by the caption_service_descriptor. The provided TS files didn't include it. In this case, you need to specify yourself the channels you expect: I think VLC proactively offers all of them, even if they don't actually include anything (this is the case of channel 2 in CNN.ts).

You can define the channels through the `closedCaptionFormats` constructor argument of `DefaultTrackSelector`. To use this constructor you will also have to provide your own Extractor factory to Extractor Media Source. Update this issue if this was not helpful. I will use this issue to track CEA support in H262. A commit reference will appear here once it's done. 608 and DVB don't have much in common, and cannot be mistaken for one another by our extractor. For DVB support track #1781. Regarding 608 detection, it's because we expose 608 channel one by default. Considering it has DVB subtitles, the stream probably does not include 608 CC, as they are used in the ATSC standard.  Is it possible for you to send your change as a proper pull request to `dev-v2`, for consideration? We're not able to accept changes provided in other ways (because we need to maintain a proper history of where the change originated from, that the author signed the CLA, etc). Thanks.  Can you provide more context on what you are trying to achieve here?

Any onLoadCompleted() that does not have a corresponding processLoadedPlaylist() is an unnecessary http request (and parsing), so you should have made the network request in the first place. Can you explain and provide concrete examples? Can you please clarify:
> but since calling loadPlaylist is performed even other than executing with processLoadedPlaylist as the starting point, it depends on the state of processLoadedPlaylist It is that it should not be.

Again, if possible, a concrete example would be helpful. In all three cases it makes sense to make the reloading of the playlist take place later: If there is a pending refresh, it means that we have loaded the playlist recently, and that a reloading will most likely not see anything new.

Regarding this particular pull request: Do you agree that we should not discard a playlist that has already been loaded? If so, please close the issue.

Regarding 
>refreshPlaylist is specifically invoked by the onPlaylistRefreshRequired event.

My original aim with the extra condition was to avoid any kind of reloading when a playlist reload is pending. If there is a pending load, it means that the Playlist tracker has decided it needs to try at that point. Any other component has no better knowledge of playlist lifecycle than the tracker, so it does not matter where the loadPlaylist() comes from. Basically, as you said:
>the HLS reload routine takes precedence over the above event

I don't think the method's name (I guess you mean `loadPlaylist()`) is misleading. The playlist will be loaded. It's up to the playlist tracker to decide when. I hope I am not misunderstanding anything. Again, thanks for the pull request! All improvement suggestions are more than welcome. > should be loaded accordingly when necessary

I agree with that. But, if loaded, it should be processed. This is not the case in the PR.

> this implementation can not cope when it is necessary to reacquire the playlist immediately 

When would this be the case? Bear in mind that the fact that someone other than the tracker requests a playlist loading does not mean that it is the correct moment to load the playlist. Please clarify why you think it is necessary to reacquire the playlist immediately. 

> processLoadedPlaylist should exist as an independent process until run.

Why? Do you think there is a heavy computational load in processLoadedPlaylist? Otherwise, it will add a lot of synchronization complexity to the tracker, which now runs completely on the playback thread. This is highly desirable. > Do you consider Tracker's behavior to be the highest priority in any case?

Yes. Please let me know if you run into any new issues with the new version. Thanks again for the PR!  See the javadoc for frameRate:

`The frame rate in frames per second, or {@link #NO_VALUE} if unknown or not applicable.`

And `NO_VALUE == -1`. This means the frameRate was not provided by the media. Hope this helps. I don't know whether there is a way to figure out the frameRate. You could have a look at how the DemoApp builds it's debug view, where it shows the amount of renderedBuffers, see `SimpleExoplayer.getVideoDecoderCounters()`. With this, you might come up with an approximation to the frame rate. Keep in mind that the frame rate could be variable.

No need to say that if you provide the frame rate through the container, this won't be an issue. I'd suggest following up on this in Stack Overflow.  This is a duplicate of #2233. Have a look there. The short answer is that every chunk declares a higher duration than it actually contains (actual duration ~10.6, declared duration 11). This gets accumulated after some time, which means that ExoPlayer think it has buffered more than the loaded chunks actually contain. 

 We might do something about this in the near future, but the best solution is to update the playlists to a more recent HLS version.  - The ffmpeg extension is not part of the core library. Unless you're explicitly building and using the extension as per the instructions [here](https://github.com/google/ExoPlayer/tree/release-v2/extensions/ffmpeg), its existence isn't relevant to you.
- The ExoPlayer ffmpeg extension can be thought of as a bridge onto ffmpeg, and is licensed under the Apache License (Version 2.0). However for it to be useful you'd also need to be building and including ffmpeg in your application as per the build instructions. Ffmpeg has its own license(s) that you should be aware of if you do this. Refer to the ffmpeg [licensing page](https://www.ffmpeg.org/legal.html) to learn more. @vigneshvg - Do you have any x86 instructions? @needz, please try the following:

```
./configure \
    --libdir=android-libs/x86 \
    --arch=x86 \
    --cpu=i686 \
    --cross-prefix="${NDK_PATH}/toolchains/x86-4.9/prebuilt/linux-x86_64/bin/i686-linux-android-" \
    --sysroot="${NDK_PATH}/platforms/android-9/arch-x86/" \
    --extra-ldexeflags=-pie \
    --disable-asm \
    --target-os=android \
    --disable-static \
    --enable-shared \
    --disable-doc \
    --disable-programs \
    --disable-everything \
    --disable-avdevice \
    --disable-avformat \
    --disable-swscale \
    --disable-postproc \
    --disable-avfilter \
    --disable-symver \
    --disable-swresample \
    --enable-avresample \
    --enable-decoder=vorbis \
    --enable-decoder=opus \
    --enable-decoder=flac \
    && \
make -j4 && make install-libs
```
Note: It seems to be necessary to disable using assembly (`--disable-asm`) for 32-bit x86, due to an issue with text relocations.

Also, include x86 in the `APP_ABI` when building the JNI code:

```
cd "${FFMPEG_EXT_PATH}"/jni && \
${NDK_PATH}/ndk-build APP_ABI="armeabi-v7a x86" -j4
```

I plan to update the extension's README to include these instructions soon. @needz, please try removing libswresample from `LOCAL_SHARED_LIBRARIES` in `Android.mk` and rerunning the command.

Sorry for the confusion -- while testing the instructions for x86 I removed libswresample as it's not needed when using libavresample. I intended these instructions to be standalone but missed out that part of the change. Thank @andrewlewis (if you look at the change you'll see I'm not the author, just the one who pushed it :)).  It's unclear what the problem being described is. Please provide clear reproduction steps (ideally using the demo app or a minimal modification of it). These MP4s have video streams but there is no surface to render to, as unlike the demo app you're not using a `SimpleExoPlayerView`. If you have control over what files you're playing, I think the best fix is to remove the video streams so that the source MP4s only have audio. Alternative options include disabling the video renderer (e.g. `trackSelector.setRendererDisabled(0, true)`), or subclassing `SimpleExoPlayer` and overriding `buildVideoRenderers` to do nothing.  I'm quite confused by this issue. I don't think the player does crash, since the exception is immediately caught and handled? What is the exact behavior you're seeing, and what is the behavior you expect? Closing as working as intended. There's no evidence here that there's actually a crash (only that an exception was logged prior to being handled). It's not valid to insert arbitrary discontinuities into an mp3 stream, and this is not something we support or intend to support. Correct behavior is that the player will fail (without crashing the process), which I think is what already happens. If poor network conditions cause a break in connectivity and a discontinuity is introduced as a result, then that's a valid case that we already handle correctly. It's different to the stream just containing arbitrary discontinuities, because we're aware that it's happened.

I'm not sure the negative length comment is particularly relevant. Sure, we could add a large number of checks everywhere through all of our extractors for invalid data, all of which would just throw some other exception instead. The end result is the same, the code is messier and the checks consume additional CPU time, which is why we just let it fail and catch the exception at the top level.  There is insufficient information here for us to help. Please provide all of the information requested in the issue template, including clear reproduction steps and a full bug report.

The issue you describe probably falls under being able to smoothly switch/detach/re-attach a surface, which is discussed in and tracked by https://github.com/google/ExoPlayer/issues/677.  I am not familiar with mingW64. I have no knowledge as to how to debug this, but my first guess is, it fails at "D:/Program". Is there any chance that you have a space in your directory name e.g. "Program Files" or some such? If so, can you please try it in a directory without spaces? Closing on the assumption this is an issue with the development environment. I think we'd recommend using Linux of macOS, if possible.  It's quite unclear what this issue is describing, but this sounds wrong:

> I released player, and changed the url as "http://hls.ciguang.tv/hdtv/video.m3u8", which is a nice hls video source.

As per the documentation for [release](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ExoPlayer.html#release--), the player must not be used after calling it. If you want to re-use the player instance you should just call `prepare` again with the new source. Call `release` only when you're completely done with the instance.

> When I replace "http://hls.ciguang.tv/hdtv/video.m3u8" as "http://cdn-media.jingzong.org/mp4/02/02-041/02-041-0001.mp4 ", it plays normal. But Still will send this Error Msg:

404 errors indicate some variants/segments declared in the HLS manifest/playlist are missing on the server. This is a content issue.  Even DefaultHttpDataSource uses OkHttp under the hood on most recent versions of Android (because OkHttp is used in the platform as the default network stack). If I remember correctly the underlying network stack will try and hold connections open so they can be re-used for some period of time. Most likely the server you're requesting chunks from is closing the connections, in which case I think a change in server configuration would be required to allow them to be re-used. I'm not an expert on TCP, but I think both sides need to send FIN when the connection terminates. Are you sure the client isn't sending FIN+ACK because it received a FIN from the server? We did look at this (quite a long time prior to this issue being filed) and convinced ourselves that connections were being re-used correctly. I don't think we have solid enough evidence to warrant us spending time checking again at this point, although if you get to the point where you're convinced it's no longer working, please let us know.

It would also be a good idea to try with some of the test content in the demo app (i.e. not hosted on your servers) to help rule out the possibility this is caused by server-side behavior, which still feels like the most likely possibility to me at this point.

Closing for now. Thanks!  It's quite difficult in general to detect stale manifest cases. It's also quite unclear to me how this particularly stack trace results from there being one.

The above aside, the player does not crash in this case. It fails gracefully, which is working as intended given the manifest was incorrect. I don't think we're inclined to spend any effort on throwing a more accurate exception for this case.  I think one way of turning this off in the current release is to extend `SimpleExoPlayer` and override `buildMetadataRenderers` to do nothing. We should add an option to `DefaultTrackSelector` to turn off enabling of the metadata track, as a simpler way of doing this. Note: Putting a 13MB image at the start of an MP3 file is a pretty bad thing to do (a player will need to download and throw away that 13MB even if it doesn't want it, which will significantly increase startup latency). If you know the content creator, it would be a good idea to suggest they fix this issue. Oh right, yes, the ID3 data isn't delivered in a metadata track in this case. It's in audio track's `Format` object. We could probably add a `FLAG_DISABLE_ID3_PARSING` to `Mp3Extractor` that you can set if you don't want it.

I think we'd still want to parse at least the gapless metadata, since that's used by the player internally. So in the case of the flag being set we'd probably need to propagate a setting to the `Id3Decoder.decode`, call telling it to only bother parsing frames that are useful (COM/COMM).

I also think the way the metadata is read and parsed is very inefficient. I have a feeling there may be three copies of the data held in memory simultaneously, so if the image is 13MB it might be that we need 39MB of allocation simultaneously to handle it. I'll take a look at reducing that. The memory allocations are as follows:

1. `DefaultExtractorInput` has an internal peekBuffer. The way this resizes means it currently allocates ~26MB for the problematic file. It's pretty trivial to get this down to ~13MB by making the logic that resizes the peekBuffer more sane. We should also consider having `DefaultExtractorInput` trim its peek buffer after advancing past a large amount of peeked data, so that the remaining 13MB can be reused.
1. `Mp3Extractor.peekId3Data` takes a copy `id3Data` (~13MB). This is short lived but overlaps with the allocations described above and below, so contributes to the peak memory requirement. We can probably eliminate this by providing direct access to `DefaultExtractorInput`'s peekBuffer.
1. `Id3Decoder.decodeApicFrame` takes a copy (~13MB). This is long lived and placed into the `Format`. We can't do anything about this one unless a flag is set on the extractor.

So it should be possible to reduce peak memory requirement from ~52MB to ~26MB, and further down to ~13MB if the flag is set. The first of the changes above adjusts the `DefaultExtractorInput` peekBuffer resize logic to reduce the peak memory requirement by ~13MB and in-playback requirement by ~26MB for the problematic sample. The second change adds a flag to `Mp3Extractor` that you can set to disable metadata parsing if you don't need it. That'll reduce both the peak and in-playback requirement by a further ~13MB.

We can still improve on (2) mentioned above, but that change will require some more in-depth work. Hopefully the changes we've made will be sufficient to allow you to avoid running out of memory. Leaving this issue open to track (2) as a lower priority item. When creating an `ExtractorMediaSource`, pass something other than `DefaultExtractorsFactory`. For example if you know your app only has to play mp3 streams, you can instead pass:

```
new ExtractorsFactory() {
  @Override
  public Extractor[] createExtractors() {
    return new Extractor[] {new Mp3Extractor(Mp3Extractor.FLAG_DISABLE_ID3_METADATA)};
  }
}
``` You can now disable ID3 metadata whilst still using `DefaultExtractorsFactory`, simply by calling `setMp3ExtractorFlags(Mp3Extractor.FLAG_DISABLE_ID3_METADATA)` on your `DefaultExtractorsFactory` instance.

I think we've improved things here sufficiently, and further optimization would be pretty in depth and is not something we're planning to look at any time soon. So I'll go ahead and close this issue.  Sorry for the delay on this. I think I know what the issue might be. I'll run some tests and update this issue. A fix will be available in the next push. Fixed in `dev-v2`. Please give it a try!  @vigneshvg - Could you take a look? Thanks. It looks like libyuv changed recently and is now dependent on libjpeg. For what we need we should be able to revert to an older version of libyuv.

I'll update the instructions accordingly. Yep, working on a fix. Will push it shortly. Thanks!  Please attach a bug report as specified in the issue template. There's more than one variant of Mi Box. We need to know exactly which one and exactly what build of Android it's running (which is the kind of information that a bug report includes). @mcd2000 - We ask for the information in the issue template for a reason. There are, for example four different builds of Android 6.0.1 for the device you're using. The information you've provided is not sufficient for us to determine which build you're using. Please provide a full bug report as requested. I'm following up internally and will update this issue if/when I hear anything back.  Does [`SimpleExoPlayerView.setDefaultArtwork`](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ui/SimpleExoPlayerView.html#setDefaultArtwork-android.graphics.Bitmap-) do what you want? Your question is quite vague. Great; glad that helped!  Thanks for the pull request! This is something I had noticed some time ago but hadn't gotten to measure. We already have an internal fix for this which prevents the initiating the premature load, instead of discarding the already loaded playlists. Please let me know if you still notice too many requests. There is one minor optimization that can still be done, but I don't know whether it is worth it, though. No, we have not yet pushed the fix to GitHub, it will happen soon. A reference to the commit should be automatically posted here. Fix is ref'd above. Closing this.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok -->  On version 2.2, https://bitdash-a.akamaihd.net/content/sintel/hls/playlist.m3u8 plays well, including subtitles. For the DemoApp, english subtitles should be enabled by default (you can make sure through the Text button at the top of the screen). Note that the first subtitles in the stream appear at 2:15, so you should seek if you don't want to wait 2 minutes.  Appears to work correctly.  Duplicate of #2509.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  Can you provide some more information about the two devices individually? The Redmi Note 3 didn't ever ship with Android 4.4.2, as far as I can tell. I'm not sure how you got hold of the stack trace, but normally there would be a build fingerprint or some more detailed device information if you're getting it back via an analytics service / feedback mechanism.

Also, is the stack trace exactly the same for the two devices, or slightly different? In particular is `OMX.MTK.AUDIO.DECODER.MP3` identical in both, or does it differ?

It's likely the problem can be solved by blacklisting the problematic decoders, but we may need the information above if we're to do that. Thanks! Thanks for the information! We should be able to get hold of a Redmi Note 3 to see if blacklisting the problematic decoder fixes the problem.

p.s. I saw which app this is for in the stack trace. It's great; happy to see you adopting ExoPlayer! Apparently there are two Redmi Note 3 variants:

1. Mediatek based: http://www.gsmarena.com/xiaomi_redmi_note_3_(mediatek)-7769.php
1. Qualcomm based: http://www.gsmarena.com/xiaomi_redmi_note_3-7863.php

I've got hold of a Qualcomm one and playback is fine there, so the issue must be specific to the Mediatek variant (which makes sense given it's a Mediatek decoder that's failing). It's likely that the issue can be fixed by adding the following to `MediaCodecUtil.isCodecUsableDecoder`:
```
    // Work around https://github.com/google/ExoPlayer/issues/2542
    if (Util.SDK_INT < 23 && "OMX.MTK.AUDIO.DECODER.MP3".equals(name)
        && "hennessy".equals(Util.DEVICE)) {
      return false;
    }
```
How hard would it be for you to get a modified version of your app to one of the affected users, so they can confirm that it does fix the issue? We'd be happy to merge the change if it's confirmed successful. Great. Let us know the outcome and we'll go from there. The reason blacklisting a decoder in the suggested way often works is that most devices also ship Google's software decoders as well. In this case blacklisting the problematic decoder results in the Google one being used instead. Unfortunately those symptoms would tend to indicate that this device only has the single MP3 decoder, in which case blacklisting isn't an option.

I doubt we'll be able to do anything to address this issue without getting hold of the physical device and trying to see if there's some way of feeding data into the MP3 decoder that it's happy with. It's possible that the MP3 decoder is just completely broken, in which case it wont be possible to fix the issue.

As an aside, you could blacklist if you were to include the FFMPEG extension in your project with `–enable-decoder=mp3`. I doubt you'd want to do that though, particularly if this issue only affects a single device.  You'll need to report this issue to the device manufacturer. I don't think it's an ExoPlayer issue. It's almost certainly an issue with the AVC decoder on this device, or something in the graphics pipeline.  > The size in tfhb.default_sample_size is in bytes?

I think so, yes. Note that it's possible for data in the `trun` box to override the size depending on the atom flags (see `FragmentedMp4Extractor.parseTrun`), but this feature is not typically used for TTML segments in SmoothStreaming.

> I have seen other "alternative" field, just before mdat (4 bytes). The value of this field is the correct length +8, but I have not been able to find documentation about this field. It seems that this field is used by software like "Mp4 Explorer", instead of tfhb.default_sample_size to extract the sample content. Which one should be used?

I'm unsure what that is. We have SmoothStreaming content including TTML subtitles that has been provided to us by streaming providers previously, all of which appears to specify the sample size correctly in the field we currently use. Which suggests that usage is correct and that the media attached to this issue has an off-by-one error.

> It's possible to make more robust the parsing process to use the already parsed information if there is an error in the xml and it throws and XmlPullParserException, and continue with the playback?

It's not ideal to arbitrarily suppress errors as it makes it harder to discover when something is broken. If this were a widespread issue we'd consider it, but as of now we don't have evidence suggesting this is the case. Streaming providers have been using ExoPlayer for SmoothStreaming for 2-3 years now without this issue having been reported previously. If the issue is limited to a single content provider or piece of content, whoever is responsible for that content should correct their media. Oh right, I've figured out what the 4 bytes you refer to are. That's just the size of the `mdat` box. Every box starts with its length (4 bytes) followed by its type (4 bytes), as defined by the `Box` class in 14496-12.

It's not correct to use the size of the `mdat` box as the size of the sample, as you suggest other players may be doing. An `mdat` box normally contains multiple samples, so if you were to take the whole of content of the `mdat` box you'd need to manually scan it to work out where the individual sample boundaries are.

Even the scanning approach as described above is not valid, because an `mdat` box is permitted to also include arbitrary non-sample binary data in addition to sample data (normally at the end of the box, but it's also possible for it to be at the start). Hence it's required for a player to use the sample sizes provided by the `trun` (or default size), together with the offset to the first sample in order to guarantee correct operation for valid media.

Closing, since it seems we're doing the right thing and using the `mdat` box size is definitely not a correct approach. You should ask the content provider to fix their media.  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).

If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks! Does this always happen at the same time? I wonder if the device is trying to adapt to a variant that exceeds its decoding capabilities. If this was the case, the  failure should happen at a somewhat random position. You can test this by playing the media in the demo app and selecting the higher resolution variants. If doing this triggers the exception throw, then that's the issue.

To fix it, you should either use a track selection that excludes the unsupported variants or include the CODECS attribute. In the last case, the first suggestion should come for free. I will wait for updates before doing any further testing. I am going to test this with an S4 GT-I9505 today.  I am not able to reproduce the issue on S4 GT-I9505 running 4.2.2. I have tried several times, following your instructions, with and without random adaptation.

It does not make much sense to me that it reproduces when adaptation is on only, and at a specific moment, considering adaptation should happen pseudo-randomly.

Regarding the error code, I have seen mentions of Qualcomm bugs in stack overflow, it would be a good idea to do some more research on that end. See if you can narrow down the reproduction steps: selecting those two specific ids only means that some adaptation between the two might happen at some point, but that does not fit well with the freezing precise positioning and the correct functioning of non-adaptive playback. We do not provide support for Amazon's port. We only support FireOS on a best effort basis, because it's not officially Android and there are known incompatibilities in FireOS vs how Android's APIs are supposed to behave. Please report FireOS incompatibilities to Amazon. Please provide test media that reproduces this issue. It's not really clear to me that it's the same issue (and even if it is test media is helpful, since we can't reproduce with the media currently provided in this issue). I cannot reproduce the error mentioned at the top of this issue with Nexus 5/7 using the stream ref'd above. @mitchellwit - Are you sure you're actually seeing the same codec error, as opposed to some other failure?  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok -->  We should probably expose this via `ExoPlayer.EventListener.onTimelineChanged`. Currently we're exposing the media playlist only. Although what information do you actually need to pull out of the `HlsMasterPlaylist`? It's fairly likely the information you need is exposed somewhere else, just not in `HlsMasterPlaylist` form. You could try using `EventListener.onTracksChanged`. The `trackGroups` variable will hold information about the tracks we've determined are present in the media. It becomes a little complicated because the representation of tracks here splits out muxed streams, but if you're dealing with reasonably simple cases it should be possible to derive the numbers you're looking for from the groups.

Else you'll probably have to wait for us to expose the master playlist through `onTimelineChanged`. Marking this as an enhancement to track doing this. For the record. We are changing the API in the next push:

`onTimelineChanged` and `getCurrentManifest` will now return an `HlsManifest`, which holds a reference to the master playlist and the current window snapshot.  This issue does not include sufficient information for us to help. Please include all of the information clearly requested in the issue template when filing future issues. Thanks. The media is a fragmented mp4 file without a sidx box. For fragmented mp4 to be seekable in ExoPlayer, it needs to include a sidx box. We do not support seeking in media that does not include an appropriate index to allow seeks to be performed in an efficient way.  Please, before going on, read the issue template. 


I will need clarification about what the issue is:

1. Which Exoplayer version are you using?
2. Try to provide the exact steps to reproduce the issue. **We depend on this to help you**. Assuming you are using version 2, are you creating a new media source per TS file? Are you using dinamically altering the files without changing the MediaSource?
3. What does "track order" mean? Are the first packets of each elementary streams arriving in different order? Is the PID order changing? Is the PMT declaration order changing?

Remember that if you don't want to post something publicly, you can send it to dev.exoplayer@gmail.com.  > If the ad's video track is in track 2 and audio track is in track 1, **then the transition is messed up**.

What does this mean?

> ExoPlayer doesn't see the switch of video and audio track

What does this mean?

> many different problems such as stalling or skipping content

Stalling as in buffering? preparing? ready but frozen video? Skipping content is skipping frames or the whole file? This could happen if the first frame is not a key frame in the second file, for example. If the second file plays, at all, then the issue is not related to PIDs. 

Please, try updating to the player to the latest v1 version. Also, there has been a lot of work done on concatenating, ad insertion and playlists in V2. Consider migrating, and you will find proper explicit support for what you are doing.

What I suggest is that you provide enough information to reproduce ourselves. A description of the media is not enough. The code you use for transitioning between one file and the other is also important. You have not provided a bug report either. Before I start looking into this, please provide the media you are playing to dev.exoplayer@gmail.com.  You have quoted a snippet from the HLS code. Are these files part of an HLS stream? If yes, I will need a link to the manifest. If not, please provide the code you use to transition from one file to the other. Closing due to lack of information.  onRenderedFirstFrame is what you need. What exactly is a "hidden surface", and how are you hiding it and making it visible? To help with debugging, I'd suggest you try posting a delayed message/runnable in `onRenderedFirstFrame`, that makes the `TextureView` visible 500ms later than if you were to make it visible immediately at that point.

Is there still an issue when the `TextureView` eventually is made visible? ExoPlayer will definitely have released multiple frames to the `Surface` by that point, so if you're still seeing the same problem then it seems likely the effect you're seeing is some lower level graphical thing around the visibility of the `TextureView`, that we probably don't have control over, or an issue in your application code.

If you're not seeing the problem when you add the delay, how far can you reduce the delay until the problem re-appears? If delaying 16ms or 32ms (i.e. one or two VSYNCs) reliably avoids the problem, then pragmatically I'd suggest simply doing that. If it can be proven that there's potentially a 1 or 2 VSYNC delay for the frame to be made visible, we could potentially consider adding such a delay prior to firing `onRenderedFirstFrame`. I think we'd need to understand what the delay is first, however. `onRenderedFirstFrame` is triggered at the point where a buffer has been output from the decoder and released to the surface. So yes, it should be taken into account. Not really. Sorry! Does this happen on all devices? Is it possible for you to provide a minimal change to the ExoPlayer demo app that allows us to easily reproduce the issue? We always render the first frame as soon as possible. For HLS it's possible that a segment doesn't have a key-frame near the start. It the chunk spans time [5-10], the user seeks to t=6 and the first key-frame in the chunk is at t=9, then we'll request the [5-10] chunk and find that the first frame we can actually display is the key-frame at t=9. Hence we'll display this frame, then not render any more frames until audio actually reaches t=9, then play audio and video together from that point.

This is not a problem if key-frames are positioned near the start of each segment (which I think is a recommendation somewhere in one of Apple's docs). In both DASH and SmoothStreaming it's required (in practice at least) that every segment begin with a key-frame. Not really, sorry. You'll probably have to do some more digging yourself. Maybe if we were to report the presentation timestamp of the rendered frame via onRenderedFirstFrame, you'd be able to compare that against the current playback position. When the playback position starts to exceed the presentation timestamp you'll know that frames are being rendered regularly. You could try making that change; I don't see any reason not to accept a pull request doing that if it helps.

It's also possible that when the first frame gets rendered may change slightly as fallout from some changes being made related to https://github.com/google/ExoPlayer/issues/2582. It's somewhat unclear to me how they'll affect the issue described here. Those changes will likely land in the next few weeks, so might be worth re-evaluating this with the latest `dev-v2` after that issue has been closed.  You should definitely try updating to 2.2.0 as a first step. The 2.0 release was a pretty huge revision to the library, and was inevitably a little immature as a result. Many issues have been ironed out in the subsequent smaller releases. If you're doing background playbakc then you should also ensure you're holding a wakelock and wifilock as detailed in https://github.com/google/ExoPlayer/issues/930.

The above aside, the snippet of log you attach suggests the decoder is unhappy with the media being fed to it. Unfortunately there is insufficient information for us to debug the issue, which is why we ask for a full bug report in the issue template. Please provide one if you wish for us to investigate further. Please just update this one. Thanks!  You can create your own playback control UI that just doesn't have a forward button, or has a dummy one that's disabled. Some information about customizing the playback controls can be found [here](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a).  @wvpaf - I'm not sure I understand this question. Do you?  Agreed this is a general Android UI question and hence is unrelated to ExoPlayer.  Closing because issue ignores the issue template and does not provide the information required for us to investigate the issue.  Please can you attach a sample stream to this issue, or email one to `dev.exoplayer@gmail.com` (and update this issue to indicate that you've done so)? Thanks!  YouTube issues should be reported to YouTube (via the built in send feedback feature).  Please provide sample content so we can reproduce the issue, together with a full bug report captured with `adb bugreport`, as is clearly requested in the issue template. Closing due to lack of required information.  Just to confirm, the claim here is that the same assets play correctly on the affected devices using ExoPlayer V1 (and that ExoPlayer V1 does switch up into the affected resolutions)? That sounds strange. I think our usage of the decoders and MediaDrm are essentially the same going from V1 to V2.

Are your assets H264 or HEVC? You mention HEVC assets in the demo app, but you also mention the H264 decoder on these devices. Please clarify. Thanks!  The file is identified as a fragmented MP4 file due to the presence of the `mvex` box in the `moov` box, but strangely it contains no `moof` fragments. At first glance it looks like it may technically be allowed to include an `mvex` box in unfragmented files but it seems unnecessary (and we don't support it at the moment). The file plays fine if I force using the non-fragmented `Mp4Extractor`. How did you produce this file?  Both videos work well on both V1 and V2 (latest versions). Please make sure you are using an updated version. If that solves your problem, please close the issue. When you say:

> Reproducible for almost every device

you mean the issue or the media? I didn't have any inconveniences with nexus 5x. I have just tried and not been able to reproduce the issue on release-v2. Please be concrete about:

> Reproducible for almost every device

It does not reproduce on 5X, Galaxy S6 or Android One. @yahim91 The file plays well for me in the DemoApp using my Nexus5x running 7.1.1. @yahim91  and @slezadav , please file a new issue including all the information requested in the issue template if your issues persist.

I am closing this issue due to inactivity in the scope of the original report.  Closing (insufficient information provided).  Please, see #1782.  I don't think this information is exposed currently, but it should be fairly easy to expose it. It would necessarily be asynchronous though (e.g. a callback method invoked each time tunneling is enabled or disabled). Marking as an enhancement to consider exposing this information.

In the meantime, if you're just curious as to whether it's being enabled, you can add a log line [here](https://github.com/google/ExoPlayer/blob/r2.2.0/library/src/main/java/com/google/android/exoplayer2/trackselection/MappingTrackSelector.java#L490) that prints whether `enableTunneling` is true or false.  You just need to provide a hint to the player to tell it that it's an HLS stream. The sample should work fine in the demo app if you add it to `media.exolist.json` as:
```
      {
        "name": "My Sample",
        "uri": "http://x.y.z.w/_SR_VFY4Lm0zdTgjMjAxNzAyMjhUMTI0NTAwIzIwMTcwMjI4VDEzMjkwMCNNalkzTXpCa1pXWmhkV3gwTVRRNE9ESTRNekU0Tnk0d01q",
        "extension": "m3u8"
      },
```
This ensures that a `HlsMediaSource` is instantiated, rather than a `ExtractorMediaSource`. Note that in ExoPlayer V1 you were always required to explicitly indicate the type (HLS/DASH/SS/OTHER).  @otefhg Thanks for reporting this. I'll submit a fix for parsing the audio specific config soon. It appears that we don't currently have any code that would pass an escaped AOT to `buildAacAudioSpecificConfig`, as we only use it for creating CSD when it's absent (in ADTS and SmoothStreaming manifests). Could you clarify how/where the fix for that method will be used? Could you clarify whether the fix in 422c2d0 is sufficient? If you also need a fix for `buildAacAudioSpecificConfig`, please clarify where it will be used, as I don't think we currently pass an extended AOT anywhere in the library. Cool. Thanks for reporting this!  If the problem is that `inferContentType` cannot infer the type correctly, but you already know the type, then the simple solution is just to not call `inferContentType` in the first place? The ExoPlayer demo app also supports hinting of the type to help with cases where it cannot be inferred. For example you can add a sample with hint to the samples json file like:

```
      {
        "name": "Name of sample",
        "uri": "http://myserver.com/video/example1.isml/manifest?t=2017-02-22T17:30:00-2017-02-22T17:55:00",
        "extension": "isml"
      }
```

where `extension` is the hint. You could also just implement your own `inferContentType` method that's smart enough to work for this use case. It should be pretty trivial to do so? There's no reason why you have to use the implementation provided by the library.

Re-opening this to track making the library version a bit smarter for this case, but we'll handle it as low priority.  There's not much information to go on in your report (e.g., what Android build the device is running and what URL you are trying to access), but it looks unrelated to ExoPlayer.

This is likely a problem with how your HTTP server or client is configured. A search for that error message turned up some results that may be relevant so I suggest you look for help on Stack Overflow or similar.  The file starts with 426 bytes of XML. The format is detected and the stream plays fine if you trim that from the start of the file, e.g. with `tail -c +426 file.mp3`.

The XML looks like some kind of metadata, but I don't think the stream is valid (you can't prepend arbitrary stuff to an MP3 stream). If this is an official part of MP3 please provide a link to the specification and we can look into supporting it. Otherwise, you'll either need to change the media you're serving so it's valid, or somehow strip the data client-side as a workaround.  @vigneshvg - Apparently you know about this ;) ([source](https://developers.google.com/web/updates/2013/07/Alpha-transparency-in-Chrome-video)). My guess would be there's no support for this on Android as I doubt the platform decoder will even output an alpha channel? Just a guess though - please clarify what your expectations would be! The platform decoder does not support alpha channel.

It is theoretically possible to build an app that uses a bundled VPx decoder to decode the YUV and Alpha channels separately and render them in a surface that supports alpha blending. Although, as of now, it is beyond the scope of ExoPlayer (and the VP9 extension).

If there's enough interest (or use-cases), it could be added into the VP9 extension where we have two instances of the decoder (similar to how chrome supports it) and change the OpenGL shader to include the appropriate alpha channel values. But it may not be a simple change.  This is a duplicate of #2495. A fix will be available soon.  Closing due to lack of clarity. Tracked in https://github.com/google/ExoPlayer/issues/2509  The stream you link to doesn't contain any audio, so this is working as intended. That's a completely separate issue than the one originally reported here. The most recent link you've provided works fine for me. Note that you need a `http://` prefix (i.e. you should be using `http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4`, not just `clips.vorwaerts-gmbh.de/big_buck_bunny.mp4`).

If you're still having issues, please file an issue that includes all of the information requested in the issue template (in particular, a full bug report captured with `adb bugreport`). Else we do not have sufficient information to investigate the problem.  If you're using the ExoPlayer V2 demo app then you should see metadata output to logcat with the log tag "EventLogger". It should be pretty easy for you to find where that happens in the demo app code, and then go from there.

We currently parse metadata for MP3 and MP4. I don't think we parse metadata for OGG yet. Please file a separate feature request if you're able to confirm that this is the case, linking or attaching media that you know to contain metadata that's not being output to "EventLogger". Thanks! I'm not sure developers do need a simpler way. It's pretty simple already, once you remove the parts that you don't need:

```
MappedTrackInfo mappedTrackInfo = trackSelector.getCurrentMappedTrackInfo();
if (mappedTrackInfo != null) {
  for (int rendererIndex = 0; rendererIndex < mappedTrackInfo.length; rendererIndex++) {
    TrackSelection trackSelection = trackSelections.get(rendererIndex);
    if (trackSelection != null) {
      for (int selectionIndex = 0; selectionIndex < trackSelection.length(); selectionIndex++) {
        Metadata metadata = trackSelection.getFormat(selectionIndex).metadata;
        if (metadata != null) {
          // Metadata!
        }
      }
    }
  }
}
```

I'm not convinced that it's possible to make it simpler without introducing assumptions that might be true in your simple use case, but that may not be true in the general case (e.g. the assumption that there's only one set of metadata in the media, rather than potentially multiple pieces of metadata attached to different tracks, which may or may not be selected). `SimpleExoPlayerView` pulls the artwork out of the `Metadata` object to display it, so take a look at what that class does (specifically, look at `SimpleExoPlayerView.setArtworkFromMetadata`).  So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent --> @b95505017 - Do you have any plans to split this request, address the comments and sign the CLA? Else I'll go ahead and close this. There's definitely nothing we can do with it without the CLA being signed, and I'd rather not leave it open indefinitely. CLAs look good, thanks!

<!-- ok --> This is looking much better now; thanks! I have some concerns about the underlying library being used. I've filed a couple of issues on that library directly. If you're able to help with, or comment on, those issues directly, that might help speed things up! The issues are:

https://github.com/ant-media/LibRtmp-Client-for-Android/issues/32
https://github.com/ant-media/LibRtmp-Client-for-Android/issues/31
https://github.com/ant-media/LibRtmp-Client-for-Android/issues/30 If there's a pure Java RTMP library available somewhere under a sensible license, that's distributed via jcenter, then it's fine to depend on that instead. I'd rather not implement the whole thing inside the extension if possible; having a dependency is preferable. If the whole thing is implemented inside the extension then it needs to be original work, not copy/paste or derivative of some other project, as per the CLA. I got this working reasonably well, but I found another issue (https://github.com/ant-media/LibRtmp-Client-for-Android/issues/33). I think we'll probably need the issues we've raised to be fixed in LibRtmp before we're able to merge this. Alternatively, we could depend on a different project if there's a better alternative. @mekya - Thanks for the improvements so far! There are a couple of remaining issues (mostly minor things at this point, but now might be a good time to make those tweaks if you agree they're good ideas). Once we have those + a release with the 64bit variants, I think we should be good to go ahead and merge an updated version of this change. I don't think I have any other suggestions, no. Thanks! @mekya - Do you have any kind of time estimate for the remaining changes + an updated release? No worries if not; just curious :). Ping @mekya :).  This is not a supported feature of our demo app. It would be possible for you to do this kind of thing with your own application, but doing so fall outside the scope of this issue tracker.  Call [setControllerShowTimeoutMs(0)](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ui/SimpleExoPlayerView.html#setControllerShowTimeoutMs-int-) Ah, understood. There's no built in way to do this currently. Marking this issue as an enhancement. In the meantime, it shouldn't be too hard for you to build your own UI component, or modify `SimpleExoPlayerView` to support this yourself. This is supported in `dev-v2` and will be supported as of `r2.4.0`, which we hope to release later this week. You can:

1. Call setControllerShowTimeoutMs(0) to disable timeout.
1. Call setControllerHideOnTouch(false) to disable hide on touch.

The combination of the two should result in controls being always visible.  Given we're able to play DASH content provided by many providers successfully, I think it's likely this is a content issue. Please validate the correctness of the video segments. If you can point to a specific thing in the media that's valid but we're not handling correctly, please let us know. At a high level it looks like the NAL unit lengths aren't specified correctly. The parsing logic ends up out of sync with the NAL unit headers and starts reading clearly incorrect lengths, leading to the failure. I think you'll need to spend some time debugging this yourself.

You may have to resort to manually inspecting and understanding the bitstream to see what's wrong, unfortunately. This is quite time consuming, and given this appears to be an isolated case we don't have cycles to help. If you discover the bitstream is valid and we're failing to handle some obscure case, please let us know specifically what that case is and we'll take another look.

You could also try reporting/discussing the issue with whoever provides the encoder and/or packager that's generating these streams.  We only support aac audio and h264 video in the flv container. We have no plan on expanding this support, since there's very little demand for it. We would accept high quality pull requests adding support for additional formats within flv, but will not be able to spend time providing guidance on how to implement it; you'd have to be sufficiently motivated to figure it out for yourself ;). Sorry!  We don't enable text tracks by default unless the media indicates that they should be enabled by default. If you try playing that stream in the demo app, you'll see that it's possible to manually enable the tracks through the UI by clicking the text button. So you should look at what the demo app does and do something similar in your app. Some of the responses in this issue may be helpful for you when figuring out which pieces of code you need to use: https://github.com/google/ExoPlayer/issues/2489  Hm. We're attempting to render a caption with a negative width. It's unclear whether the subtitle itself contained invalid positioning data to cause this. Is it possible for you to provide the media that reproduces the crash so that we can determine the root cause, or is the media gone now, given it was a live stream?

Either way, we should just not render anything rather than crash if we do end up in this state. Hence marking as a bug. I do think the subtitle data must have been invalid, since the only case I can see this happening is where the subtitle specifies it should be positioned entirely to the right of the display area. As above, we'll provide a fix that discards the subtitle and logs a warning for this case, rather than crashing. Fixed in the dev-v2 branch.  Oops, thanks!  A fix will be available in the next push. Next time, please provide a link to the problematic media. Fixed in the dev-v2 branch.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> There is already an open pull request for this (https://github.com/google/ExoPlayer/pull/1781). Also, requests need to go to the `dev-v2` branch, not the release branch.  Are there devices where this issue does not reproduce? This would be strange for a NPE.

Please provide a stream to reproduce the issue. Closing due to inactivity.  See #2491. Closing due to lack of required information. Please provide proper reproduction steps and test media if you want us to take a look.  Please, read the issue template and provide all the required information. I am going to assume this is a duplicate of #2493. If not, please file a new issue including all the information requested in the issue template.  Agreed this question is outside the scope of this issue tracker; it seems like a general question about Android view/ui customization.  If you modify the demo app to build your source, do you see both options listed under the "Text" button? If so, I suggest you try answering your own question by taking a look at how the demo app is doing it. Yes. Pretty much! Tracks are arranged into groups. The `trackGroups` variable is an array of `TrackGroup` instances. Each `TrackGroup` may contain multiple individual tracks, each with its own format. A `TrackGroup` contains multiple individual tracks when it's possible to adapt between different quality streams of the same content. So for an adaptive video playback you'll typically see a `TrackGroup` containing multiple individual video tracks whose formats have different resolutions or bitrates. You don't generally expect to adapt between different text tracks, so a text `TrackGroup` will typically contain only a single track.

So to add comments to your block of code:

```
trackGroups = trackSelector.getCurrentMappedTrackInfo().getTrackGroups(rendererIndex);
// Look in trackGroups to locate the track you want to enable. You should end up with the
// index of the group and the index of the individual track within the group.
int trackGroupIndex = .....;
int trackIndexWithinGroup = .....; // Probably 0.
// Create the selection override
override = new MappingTrackSelector.SelectionOverride(
    new FixedTrackSelection.Factory(), trackGroupIndex, trackIndexWithinGroup);
trackSelector.setSelectionOverride(rendererIndex, trackGroups, override);
```  It gives me a 403. Can you fix it? The link doesn't work for me either. It's either geo-restricted or no longer available. If it's geo-restricted then ideally provide a non-restricted test stream. If that's not possible, at least let us know to which country it's geo-restricted. I tried proxying via France, US and GB, and see a 403 response code in all cases. Closing because a working test stream has not been provided.  _No description provided._  The mp3 file contains malformed ID3 data. Uploading it to http://tagmp3.net, making no changes and downloading the result removes the malformed ID3 data and results in a playable file.

That said, we'll make a change to handle (by ignoring) this type of malformed data, since it's trivial for us to do so.  > One requirement we have though is that the user should not be able to play DRM protected content on rooted devices.

I don't think this is a sensible requirement to have. It's your responsibility to specify the level of output protection you require when issuing keys to play the content. It's the responsibility of the underlying platform to only decrypt and play the content if the specified level of output protection is available, which may or may not depend on whether the device is rooted. If the device is rooted but still meets the level of output protection specified, then I can't think of a valid reason why you shouldn't allow playback to proceed. Preventing playback in this case will needlessly annoy many users.

> What protection does exoplayer have when it comes to DRM content on rooted devices? If this is possible is there anything we need to do to enable this?

ExoPlayer is pretty much just an intermediary when it comes to DRM. We parse the necessary information from the media container and plumb it to the underlying platform (e.g. via MediaDrm). So your question is more about the underlying platform. You should be able to trust that the underlying platform will enforce the specified security level whether a device is rooted or not. Many devices are able to support L1 Widevine with secure video output even when rooted. I suspect the vast majority of high end phones fall into this category. Some devices only support L3 Widevine whether they're rooted or not. A small subset of devices may support L1 Widevine when not rooted and only L3 Widevine when rooted. @nitro52 - I suggest you look at clarifying the requirement with the studios. For non-Android devices it may be that it makes sense. For Android devices I think playback should be allowed based on whether the device satisfies a certain Widevine security level (e.g. L1 or L3), not on some other arbitrary piece of information. It's possibly just an oversight.
@wvpaf - Do you know if this kind of requirement is normal in studio agreements? @nitro52 & @ojw28, studio requirements differ too much to generalize.

However, the app should be able to tell if the device is rooted and invalidate the keys that have been delivered. Root detection does not automatically stop playback as mentioned earlier, typically playback continues but with a reduced video quality. 
to determine if rooted see : http://stackoverflow.com/questions/3424195/determining-if-an-android-device-is-rooted-programatically Closing as answered.  `ExtractorMediaPeriod` normally maps a timestamp onto a byte offset in `ExtractorMediaPeriod.seekToUs` using its `seekMap`. I guess your Matroska stream will be missing a cues element, so this seek map will be unseekable. You can verify this by breakpointing in that method and seeing what happens when you try to seek.

As a next step I'd probably try making a wrapper `Extractor` around `MatroskaExtractor` which passes in your own `ExtractorOutput` wrapper. Pass a factory that instantiates this wrapper to `ExtractorMediaSource`. You can intercept the call to `seekMap` and pass a replacement seekable seek map to the extractor output. Then the other piece you would need is a `DataSource` that adds the right query string parameter based on the last seeking operation. (You might need to lazily open the underlying source when reading starts, as `Extractor.seek` is called after `DataSource.open`.) The wrapped `Extractor` sees the same data as if it were really seeking in a complete stream. I haven't tried any of this so it might not work. There may also be simpler ways to achieve the same thing. Hope this helps! Regarding recreating the `MatroskaExtractor` on every seek: is your wrapper correctly passing through the call to `seek(long, long)` before the new `DataSource` is read? That method resets some state in preparation for reading data from a different position, so it's important to call it. If you are calling that method but it's still failing I'd probably spend a while trying to debug what's going on, as it should work, assuming the server is providing valid data. From the extractor's perspective this looks like a 'normal' seek. If an error occurs while loading data, `ExtractorMediaSource` will try to resume from the last read byte offset. Can you make your server support range requests and just reopen using whatever query parameter you used the last time?

If that's not feasible then you could just stop and restart the player at the current playback position (just seeking won't work as seeking to the current position is a no-op). This is a pretty bad solution, though, as it will interrupt playback, and it's inefficient.  Please see #930 and #2276.  Most likely you're playing a multi-period DASH stream where the formats change going from one period to the next. You need to use V2 or above to play this type of stream.

It's not possible for us to diagnose further, since you've not provided most of the information we request in the issue template.  The ability to switch surface is difficult due to underlying platform constraints, as discussed in https://github.com/google/ExoPlayer/issues/677. Making a parent view invisible/gone or detaching it from the view hierarchy will normally cause the surface to be destroyed. A new surface will be created when the view is made visible or reattached to the hierarchy. Hence the use cases described here do result in surface switches and so run into the same underlying constraints.

There's a class called `DummySurface` that we recently added to mitigate this behavior. You currently need to wire it up manually, however. Follow https://github.com/google/ExoPlayer/issues/677 for more details.  Please feel free to send a pull request to `dev-v2` to fix this.  > Did ExoPlayer 1 switch up mid-chunk when possible?

Yes, but that aspect has not changed. That is, neither version can cancel a chunk download to start downloading a different variant, though we might do something about this in the future.  

> takes noticably longer that before

Something that might be worth checking is whether your streams contain multiple audio renditions. Ideally, please send a stream that reproduces the longer preparation time to dev.exoplayer@gmail.com, so I can run a few experiments. If you want to run some yourself, metrics will be welcome. 

> Are there any ways to influence the adaptive switching behavior for HLS streams in ExoPlayer 2?

To customize this, you can inject your own TrackSelection.Factory in the DefaultTrackSelector. There is useful information about this in #676 and related issues. Finally, the `trackselection` package is the best place to go. I will track hls adaptation related enhancements with this issue.  Have you managed to reproduce with a physical device? Please provide all the information in the issue template, as well. No, assuming you can reproduce this using the DemoApp, please send a link to the stream, as requested in the issue template. There is not enough information for me to know what the issue is. For starters, I will need the media sample you are using. Try reproducing with a few concatenated chunks instead of an HLS manifest. If it reproduces, send me the chunks. Also send the bugreport from the physical device, please. I am closing this issue due to lack of required info.

 @mcart666, please open a new issue including all information required in the issue template.  Could you clarify what is not "proper" about dpad seeking if you just make the SeekBar focusable? Are you saying just making it focusable is sufficient, or are you saying that this pull request is required? If so, what does this pull request achieve that just making the SeekBar focusable (and nothing else) does not? @andrewlewis Is working on making some other changes to the seek bar, which likely mean this change wont be necessary. The dev branch has a new `TimeBar` class now. Is this change still needed, or does that behave in the desired way? If it is still needed, the implementation should probably be moved from `PlaybackControlView` into `DefaultTimeBar`? The `DefaultTimeBar` we'll push shortly (probably today) implements similar timeout behavior to this PR: after pressing the left/right directional buttons the seek will occur after a timeout, unless the user presses the center/enter button which causes the seek to happen immediately.  I think for getting real bit-rate, you may need to know two parameters.
1. Real size of a particular media chunk.
2. Media chunk's duration.

For 1, The actual size of a particular media chunk could be known by **sampleBytesTransferred** within **DefaultBandwidthMeter**.
In order to get it, I tried to implement the EventListener of BandwidthMeter and create a new API = onTransferEnd() to inform the downloaded media chunk size, as the patch below.
[RealChunkSize.txt](https://github.com/google/ExoPlayer/files/786522/RealChunkSize.txt)
It prints out each media chunk's size when having downloaded it.

It is not good in practice since the function is run on main thread (thread id = 1).
So a rework is in need to make the thread run on internal thread instead.

2 seems to be easier; so I believe a try will simply accomplish it. 
 We don't expose this information. It doesn't sound particularly important, so I suspect you'll just have to manage without ;).  Whilst it might be convenient for you in the short term if we were we to do this, it would effectively add *every* field and method to the public API, meaning that changing literally anything could potentially break developers when they go to upgrade from one version of ExoPlayer to the next. This is not a good idea, and is not something we'll be doing.

It sounds like you're fixing something that doesn't work properly currently. Why not just contribute a fix directly to the library if so?  ExoPlayer is a library for media playback, not media generation, and hence this is not a supported feature.  Does #2017 help? It looks like the `DataSourceFactory` you're passing to `SingleSampleMediaSource` is null. Please share the code you're using to construct the `MediaSource`s.  Prepare the player with an `HlsMediaSource` to play an HLS stream. It looks like you're using `ExtractorMediaSource` at the moment.  Are you using `FLAG_ALLOW_NON_IDR_KEYFRAMES` when building the `TsExtractor`? It plays fine for me with the flag set. Without it I just see a black screen. Please clarify. Thanks! Note: I was testing on a model running L (API level 22) rather than M (API level 23). Note: I have also made sure that FLAG_DETECT_ACCESS_UNITS was enabled as well. Still not reproducible.

Regarding the Android version being 22 instead of 23, I am inclined to discard the possibility of a regression issue if it plays well using other apps:

>It could be device's performance, but same video using same decoder (OMX.MTK.VIDEO.DECODER.AVC) in 2 other apps is played well.

I also doubt it is the lack of flags, as this does not cause dropped frames. I can only think of a device model variation from what we have (could you please send the output of `adb shell getprop ro.build.fingerprint` for that device?) or an APK difference. Please provide any information that might help us reproduce or understand the issue.  I tried the debug APK without extensions. Didn't reproduce. This seems to be device-specific. We're looking into getting the problematic device but it may take a while. @stari4ek If possible, please could you provide a photo of the label on the back panel of the TV with readable text? Apparently the build fingerprint does not provide enough information to know exactly what the device is. Detecting access units in H.264/AVC is relatively expensive because we have to unescape and parse slice headers to determine whether the NAL unit is the first VLC NAL unit of the picture.

Do you have the option of only using streams that include access unit delimiter NAL units and not using the flag?  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla --> CLAs look good, thanks!

<!-- ok -->  I think this is relatively straightforward to achieve in V2. First, create a `SimpleCache` into which you'll offline media. In this example we'll use `NoOpCacheEvictor` to ensure nothing's evicted from the cache. Note that in a full solution you should pass an evictor implementation that's smart enough to evict media that the user no longer wants to be offlined.
```
cache = new SimpleCache(cacheDir, new NoOpCacheEvictor());
```
You'll already have a `DataSource.Factory` somewhere that you're injecting into your media source. Let's call this `defaultFactory`. Make a new `DataSource.Factory` that generates `DataSource` instances that can read and write from the cache using the AES encryption components:
```
new DataSource.Factory() {
      @Override
      public DataSource createDataSource() {
        DataSource dataSource = defaultFactory.createDataSource();
        // Wrap the DataSource from the regular factory. This will read media from the cache when
        // available, with the AesCipherDataSource element in the chain performing decryption. When
        // not available media will be read from the wrapped DataSource and written into the cache,
        // with AesCipherDataSink performing encryption.
        return new CacheDataSource(cache, dataSource,
            new AesCipherDataSource(SECRET_KEY, new FileDataSource()),
            new AesCipherDataSink(SECRET_KEY, new CacheDataSink(cache, Long.MAX_VALUE)), 0, null);
      }
    };
```
If you use this `DataSource.Factory` for playback you'll get caching with encryption and without eviction. To offline media without playback, you can just read a file through a `DataSource` chain like:
```
public void offlineMediaUri(Uri mediaUri) {
  DataSource dataSource = cachingDataSourceFactory.createDataSource();
  Log.i("Offline", "Starting");
  DataSpec dataSpec = new DataSpec(mediaUri);
  try {
    dataSource.open(dataSpec);
    // Pull the media through the DataSource chain.
    byte[] scratchSpace = new byte[32 * 1024];
    int bytesRead = 0;
    while (bytesRead != C.RESULT_END_OF_INPUT) {
      bytesRead = dataSource.read(scratchSpace, 0, scratchSpace.length);
    }
    Log.i("Offline", "Finished");
  } catch (IOException e) {
    Log.e("Offline", "Failed", e);
  } finally {
    Util.closeQuietly(dataSource);
  }
}
```
Note that if you don't want caching during playback of non-offlined content, you'll need to use a different `DataSource.Factory` for playback than for offlining. You can create a `CacheDataSource` that reads from the cache but doesn't write to it by passing a null sink. You can disable caching by passing null as the sink for the `DataSource.Factory` used during playback. The code above already directly encrypts the stream prior to it being written to disk, so I think it does what you want. The code sample provided above (`offlineMediaUri`) is exactly code for a download operation that is not part of playback. Probably, but it's unclear why you're trying to avoid it. It's not a significant amount of extra code for you. Note also that the approach described does nice things like:

- Support resumption of a partially completed download. If `offlineMediaUri` fails part way through, calling it again will do this. The already downloaded portion will just be read from the offline cache (which is a no-op), and then the part that's not yet downloaded will be read from the upstream source and written to the offline cache. The `DataSource` chain handles this automatically. You can just call `offlineMediaUri` repeatedly until it succeeds.
- Playback of a partial download where the media is read from the offline cache or network as required. > our download mechanic has a great deal of existing infrastructure, saving offline content using a chunked, resumable download api in it's own service, we supply the file name and location based on user prefs, downloaded content is tracked in a database and might be moved en masse when settings change, but really i'm just trying to get a minimum working example so i can really understand what's happening.

Most of this feels pretty unrelated to the actual mechanism for downloading the media. There's no reason why you can't take the `offlineMediaUri` snippet and incorporate it into your service instead of your current code for offlining the media. I doubt there's anything stopping you from moving the directory, either. The problem with going for a simple approach is it'll either lack basic functionality or you'll have to re-implement things like download resumption, which then ends up not being simpler at all.

> i appreciate you taking the time to respond and don't want to take advantage of it; i wonder if you see anything in the file write snippet earlier that would explain why the simple approach i described didn't work - is updateInPlace inappropriate here? would an AesCipherDataSource be able to read a file encrypted without an AesFlushingCipher (e.g., encrypted by some other mechanism outside of or previous to the library), or does it need to be written using an AesFlusingCipher as well?

I can't see anything obviously wrong with your code. But I maintain the suggested approach is going to be much easier. If you want to go down some alternate path and it's not working, then you'll need to debug what's wrong yourself. @rafaeladel - You'd be much better off just following the official advice in my first response to this issue, including use of the library provided `AesCipherDataSource` and `AesCipherDataSink` components. There's really no need to reinvent the wheel. - If you're serving content using https then it seems unnecessary for it to be further encrypted for transmission. If you're not serving content using https then I would strongly urge you to do so.
- If you're serving content encrypted despite the above using AES/CTR/NoPadding then I think the advice in my first response still more or less applies. You just need to tweak the `DataSource` chains a little (i.e. remove `AesCipherDataSink` from the chain used for downloading since the media is already encrypted) to make sure that the right steps occur in the chain.
- If you're serving the content encrypted with something other than AES/CTR/NoPadding, then I guess at that point you need to go down a custom implementation route.
- It's unclear what "otherwise unable to encrypt in-app" refers to here, since I can't think of any valid cases where this statement would hold? I'm unsure what the problem is that you're seeing, but I don't think it's decrypting the entire file up to the seek position. That's just not what the code does. If you look at the implementation you should be able to verify that it starts decryption from the specified position offset. @RajatSingh That link points to an HLS stream, not a media container file. See [Why do some streams fail with UnrecognizedInputFormatException?](https://google.github.io/ExoPlayer/faqs.html#why-do-some-streams-fail-with-unrecognizedinputformatexception).  All representations in the stream linked in the first bullet item are in H.264/AVC High profile (the codecs attribute starts `avc1.64`), which exceeds the minimum requirements for video decoder support on Android API 16 builds. So it is not safe to assume that all devices can decode it (though many will be able to in practice). Please see [Supported formats](https://google.github.io/ExoPlayer/supported-formats.html#sample-formats) for more information.

If Baseline profile streams also can't be played and the device has another (software) decoder please let us know and we can blacklist the problematic decoder. Thanks.  On the rendering side, `MediaCodecAudioRenderer` and `MediaCodecVideoRenderer` support whatever codecs the device has, but on the extraction side we have to add support for each format individually (which can be non-trivial). See also [Supported formats](https://google.github.io/ExoPlayer/supported-formats.html) for more information.

Note that for surround playback it is common to output encoded audio directly to an audio receiver (passthrough). The Android platform supports passthrough of arbitrary formats from Nougat onwards, and #2147 tracks exposing this functionality. We're unlikely to prioritize it in the near future, however.

Not sure about TrueHD vs Atmos but [this page](https://blog.dolby.com/2014/06/dolby-atmos-home-theaters-questions-answered/) suggests that TrueHD is an extension.  Heh, this is an oversight. Thanks! We'll merge shortly.  Sure; can do. Thanks!  Please, see #2432. Thank you for the thorough report!  1. #2122 tracks showing all concatenated periods in the seek bar.
2. Please could you provide URLs to the streams you're concatenating so we can reproduce the issue? Thanks. I don't have a diagnosis about why the transitions between the clips are not seamless yet but I wanted to note a few things:

- Call `ExoPlayer.getCurrentWindowIndex()` to get the index of the currently-playing stream. `ExoPlayer.EventListener.onPositionDiscontinuity()` is called when the value changes.
- To show a complete timeline for a sequence of files like this, in general we'd need to prepare every source up-front to find out its duration. The feature requested in #2122 likely won't address this use case, at least initially, though if your app knows the durations of the streams already you might be able to provide this information to the UI (TBD if/how this would work). ClippingMediaSource may be a better way to split up a file if that's your use case.
- Maybe you know this already but these MP4s have audio tracks in AC-3 format. That format is not supported by built-in decoders on the majority of non-TV devices. I had a look at the first two files. Each file has an edit list that offsets its sample timestamps -- 1.6 seconds for the first file and 3.9 seconds for the second. This explains the pause at the start of each stream's playback. Do you know why these edit lists are present, and/or how the files were produced? Could you try passing `-use_editlist false` to see if that fixes it? I suspect this either an issue with FFmpeg or with the original content. You'll probably get better advice on an FFmpeg-specific mailing list or a general site like Stack Overflow.

Please reopen this issue if you discover that we are interpreting the edit lists incorrectly. Also, playing a sequence of MP4 clips in this way may be suboptimal depending on what you're trying to do, so I repeat my suggestion of using ClippingMediaSource. I'm not sure what you mean by 'duration of the edit lists', but to see the contents of the edit lists you could (for example) add some logging to `AtomParsers.parseEdts`, or set a breakpoint there. In case it helps, the sample tables get updated based on the edit list in `AtomParers.parseStbl`. Thanks!  This is tracked by #2427, thanks!  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> No need to send a pull request for this. We have a change internally that does the same thing, that will be automatically mirrored into dev-v2 this week.  Please see #2053.  Duplicate of #867.  You can do this by adding a listener to the ExoPlayer instance (`ExoPlayer.addListener`) and implementing the listener's `onPlayerStateChanged` to obtain or release audio focus as necessary. Note that `onPlayerStateChanged` will be called whenever `playWhenReady` is changed. Ah, I see the issue. We should take a look at supporting this case. Thanks! You can intercept play/pause calls after the patch ref'd above.  I'm not convinced this is the correct fix for the ref'd issue. Whatever is calling getSegmentDurationUs is doing something wrong, since it's passing an index that's outside the bounds of the index. This is analogous to code trying to index into an array with an index >= its length. I also don't think it makes sense to fall through to the `else` block the problematic case.

The correct fix is to repair the calling code to not try and obtain the segment duration of a segment that's outside the bounds of the index. Would it be possible for you to take a look at applying that kind of a fix? The desired behavior is that the internal code that makes the call that fails shouldn't be making the call. There wont be any error to report to the app once this is the case. Not really. If it's a live stream correct behavior is to wait for a segment to be added to the timeline. If it's an on-demand stream correct behavior is to treat the content as having 0 duration (and so transition to the next piece of content or the ended state). It's actually not that trivial to fix this. DashSegmentIndex doesn't have a way to indicate that it's empty. I think getFirstSegmentNum probably ends up returning 0 and getLastSegmentNum probably ends up returning -1. But -1 is defined to be INDEX_UNBOUNDED, which isn't right. getLastSegmentNum probably needs changing to be exclusive rather than inclusive, so both returning 0 (or the same value) indicates an empty index. I can push a fix for the ref'd issue sometime this week; I think it touches quite a lot of files (about 7)? Closing this PR for now. Thanks!  Please could you expand on what "some reasons" are? The "main" role is represented in the track's `Format.selectionFlags` as `C.SELECTION_FLAG_DEFAULT`.  Closing due to inactivity.  The question seems quite confused (what does VideoView have to do with anything; isn't it unrelated to ExoPlayer?), so I'm not sure what you're asking. If you're trying to use TextureView shouldn't you have: `app:surface_type="texture_view"`? You can then get the TextureView from the SimpleExoPlayerView and retrieve the Bitmap using:
```
TextureView textureView = (TextureView) simpleExoPlayerView.getVideoSurfaceView();
Bitmap bitmap = textureView.getBitmap();
```  This is not something we support directly. If you render to a `SurfaceView` then I'm not sure whether it's possible. If you render to a `TextureView` or via GL then it should be possible, but the implementation will not be ExoPlayer (or video) specific. You should probably seek help around applying effects to TextureView on StackOverflow or similar.  608 is text. ID3 is metadata. So this looks like it's working as intended to me? > I was of the opinion that Closed Caption belongs to metadata renderer, am I wrong?

Yes ;). This has never been the case. Closed captions have always been considered text.

> for "mimeType=application/cea-608" returns True, which makes me feel confused now.

This is just returning whether the mime type starts with "application". Which it does.  I'm unsure about sizing a `SurfaceView` larger than the display, but it isn't something I'd necessarily expect to work. Have you tried using `SimpleExoPlayer.setVideoScalingMode(C.VIDEO_SCALING_MODE_SCALE_TO_FIT_WITH_CROPPING)` together with `RESIZE_MODE_FILL`? I think that probably achieves what you're trying to do whilst sizing the `SurfaceView` to have the dimensions of the display rather than any larger.

Note that `RESIZE_MODE_FILL` is also equivalent to just using a `SurfaceView` directly (i.e. not in an `AspectRatioFrameLayout`), if that's any easier.  We should also match both {bitrate} and {Bitrate} according to the spec, rather than just {bitrate} as we do currently. We'll push a fix shortly.  Please could you provide detailed steps to reproduce the issue in the demo app, including links to the streams you are playing? Thanks. Closing due to lack of information.  Have you seen #1299? Are you in control of the media being played? If not then where is it typically coming from?

We're unlikely to add support for this case. As per #1299 it's unlikely MediaPlayer is doing a particularly good job of allowing seeking (there's likely a significant trade-off of one kind or another involved). I think it's quite problematic to turn it on blindly, particularly with respect to playlist support. We could probably have it as an option that you can enable on the extractor. You'd need to pass your own ExtractorsFactory to ExtractorMediaSource that sets the option, if we were to go down that route. If you look at `FragmentedMp4Extractor` you'll see there are some flags that can be passed to the constructor to enable different non-standard behaviors. The no-arg constructor passes 0 (i.e. no flags). You could do something similar in `Mp3Extractor` to allow using `ConstantBitrateSeeker` if no other seeker can be configured (`FLAG_ENABLE_CONSTANT_BITRATE_SEEKING` or something). Initially there will only be one flag defined, but let's use the same style for consistency and so that we can add other flags in the future.

I think we'd be happy to accept a pull request that does this, if you feel like putting one together. Once that's submitted all you'll have to do in your own code is to create your own `ExtractorFactory`, but you can do that with very little code so this should be acceptable I think.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  I think this is fixed in r2.2.0. If you believe otherwise, please file a new issue including *all* of the information clearly requested in the issue template, since there is not enough information provided here for us to assist. You can read a bit about why it's (unfortunately) difficult to do what you're trying to do in https://github.com/google/ExoPlayer/issues/2093, https://github.com/google/ExoPlayer/issues/1084 and https://github.com/google/ExoPlayer/issues/318. If possible you should handle the configuration change yourself as the demo app does, to avoid the problem. It's more work and you might have to do a bit of UI magic in code that would otherwise happen automatically, but it's possible. I'm fairly sure YouTube handles the orientation change itself, for example.  I don't understand the question. Could you please clarify exactly what you are trying to do?

HLS is an adaptive streaming protocol, not a compression format (like H264, for example). In other words, HLS is not something you decode. In exoplayer, FFmpeg can be used as an extension to decode, for example, a video stream in a certain compression format. This is particularly useful when the device does not include hardware support for decoding such format. Can you please provide more context?  Closing due to lack of clarity. There doesn't appear to be a description of a problem or a question in this issue.  Dear @petitTrung:

I think the problem is similar to #2083; please refer to the logics below. 

```
  private int getBufferTimeState(long bufferedDurationUs) {
    return bufferedDurationUs > maxBufferUs ? ABOVE_HIGH_WATERMARK
        : (bufferedDurationUs < minBufferUs ? BELOW_LOW_WATERMARK : BETWEEN_WATERMARKS);
  }
```

```
  @Override
  public boolean shouldContinueLoading(long bufferedDurationUs) {
    int bufferTimeState = getBufferTimeState(bufferedDurationUs);
    boolean targetBufferSizeReached = allocator.getTotalBytesAllocated() >= targetBufferSize;
    isBuffering = bufferTimeState == BELOW_LOW_WATERMARK
        || (bufferTimeState == BETWEEN_WATERMARKS && isBuffering && !targetBufferSizeReached);
    return isBuffering;
  }
```



```
  /**
   * A default size in bytes for an audio buffer.
   */
  public static final int DEFAULT_AUDIO_BUFFER_SIZE = 54 * DEFAULT_BUFFER_SEGMENT_SIZE;
```

There are four parameters within DefaultLoadControl; two of them control the load timing; the other twos control when the playback starts by having sufficient buffering. Please refer to the figure below.
The maxBufferUs & minBufferUs are about the load timing but bufferForPlaybackAfterRebufferUs & bufferForPlaybackUs are about when the playback starts by having sufficient buffering.

![1](https://cloud.githubusercontent.com/assets/14846473/22848468/b8cfffe2-f02f-11e6-8781-3e3ba43aa81d.png)

Refer to shouldContinueLoading(), a typical loading pattern includes 3 stages. 
At the first stage we continuing loading until maxBufferUs is reached as below.
![2](https://cloud.githubusercontent.com/assets/14846473/22848573/812da840-f030-11e6-9aee-f0f224144a37.png)

Since then isBuffering = false and bufferTimeState == BETWEEN_WATERMARKS; so shouldContinueLoading() returns false as below.
![3](https://cloud.githubusercontent.com/assets/14846473/22848632/d758d3a2-f030-11e6-9b14-ff4f6c7857df.png)

Finally, when bufferTimeState == BELOW_LOW_WATERMARK again we recover the download, as the figure below.
![4](https://cloud.githubusercontent.com/assets/14846473/22848672/ffc588ee-f030-11e6-9bd9-fffb11aa52b3.png)

Per the flow described, you could see within the period of [t1, t3] actually we **DO NOT** load anymore. Therefore you may meet the condition, for example to have merely ~ 15 seconds buffering.

Beside, there is a limit upon buffering size.
From your description, your application is about radio. Hence it is limited by DEFAULT_AUDIO_BUFFER_SIZE = 54 * DEFAULT_BUFFER_SEGMENT_SIZE; (assume there is NO video).
So if the radio's bit-rate exceeds (54 * 64 / 30) = **115.2 KBps**, you still will not be able to have 30 seconds maximum buffering. 
In short, typically the buffering data will be from 15 to 30 second if your connection speed is good enough.

You have two choices to enlarge it.
1.  Enlarge both  maxBufferUs & minBufferUs (instead of maxBufferUs only).
2.  Remove the limit by applying "drip-feeding" method.
    isBuffering = bufferTimeState == BELOW_LOW_WATERMARK
        || (bufferTimeState == BETWEEN_WATERMARKS /*&& isBuffering*/ && !targetBufferSizeReached);
It is, once you are below maxBufferUs, do fetch immediately. 

Finally, if it is a LIVE streaming. you are still limited by current available data at source.
For example, the source may own at most 20 seconds data so you could not buffer more than it.
 









 For radio or any other live stream, depending on the exact type of the stream, it might not be possible to build up a large buffer in a sensible way. For example if the stream/server only allows you to request the past 5s of the stream, then the only way a player could build up a 30s buffer would be for it to sit around buffering for 25s after opening the connection, prior to starting playback. That wouldn't be a good user experience. Note also that the larger the buffer, the further behind "live" the user will be.

I suspect in your case the stream/server is effectively controlling the buffer size to something around 5s, by only making that duration of past media available. In which case changes you make in the player probably wont have any effect.  Duplicate of #2176.  1. I doubt this is related to the output Surface, unless you are doing something non-standard. You could check that the error is reproducible in the ExoPlayer demo app to eliminate this possibility.
2. `E VideoDecoder: Invalid parameter: nalu_data = NULL for naluType 0x7` looks suspicious, as if the decoder is not getting SPS. Can you reproduce the error on other devices playing the same content?
3. It's quite likely so I'll mark this as device-specific for now. Testing on other devices is a good way to check.

There are some DRM-related warnings (`W WVCdm   : CdmEngine::ExtractWidevinePssh: Unable to read PSSH atom size`) but I'm not sure if they're significant. Please provide the output of `adb bugreport` after reproducing the error. The logcat output on its own is not sufficient to investigate this. Can you reproduce this error when playing the Widevine samples in the demo app? If so, please take the bug report after playing one of those. Thanks! @wvpaf - Could you take a look at the second bug report / device logs provided above. Specifically, this:

```
02-10 16:29:55.130 12839 12868 I WVCdm   : CdmEngine::OpenSession
02-10 16:29:55.130 12839 12868 I WVCdm   : Level3 Library May 16 2014 23:14:03
02-10 16:29:55.130 12839 12868 W WVCdm   : Could not read /data/mediadrm/IDM1013/ay64.dat2: No such file or directory
02-10 16:29:55.170 12839 12868 E DRM-LibSEP: resultlen = 112.
02-10 16:29:55.170 12839 12868 E DRM-LibSEP: wKeyDataSize = 72.
02-10 16:29:55.170 12839 12868 E DRM-LibSEP: wDeviceIDSize = 32.
02-10 16:29:55.170 12839 12868 E DRM-LibSEP: drm_wv_mod_load_usage_table():45: DEBUG - usage_table_data is NULL, retrieving expected size from FW.
02-10 16:29:55.170 12839 12868 D WVCdm   : OEMCrypto_Initialize Level 1 success. I will use level 1.
02-10 16:29:55.190 12839 12868 I WVCdm   : CdmEngine::QueryKeyControlInfo
02-10 16:29:55.200 12839 12868 W WVCdm   : BufferReader::Read<T> : Failure during parse: Not enough bytes (4)
02-10 16:29:55.200 12839 12868 W WVCdm   : CdmEngine::ExtractWidevinePssh: Unable to read PSSH atom size
02-10 16:29:55.200 12839 12868 W WVCdm   : BufferReader::Read<T> : Failure during parse: Not enough bytes (4)
02-10 16:29:55.200 12839 12868 W WVCdm   : CdmEngine::ExtractWidevinePssh: Unable to read PSSH atom size
02-10 16:29:55.200 12839 12868 I WVCdm   : CdmEngine::GenerateKeyRequest
02-10 16:29:55.210 12839 12868 D WVCdm   : PrepareKeyRequest: nonce=450873380
02-10 16:29:55.490 13362 13532 I OMXClient: Using client-side OMX mux.
02-10 16:29:55.500 12839 12869 E wrs-omxil-core: OMX_GetHandle(): exit failure, OMX.Intel.aac.decoder not found
02-10 16:29:55.500 12839 12869 W linker  : libstagefright_soft_aacdec_mdp.so has text relocations. This is wasting memory and prevents security hardening. Please fix.
02-10 16:29:55.500 12839 12869 I MDP_OMX_CORE: Created OMXPlugin : OMX.Intel.aac.decoder
02-10 16:29:55.510 12839 12867 E OMXNodeInstance: setParameter(1866465283) ERROR: 0x8000101a
02-10 16:29:55.530 13362 13362 D EventLogger: audioDecoderInitialized [2.61, OMX.Intel.aac.decoder]
02-10 16:29:55.530 12839 12869 E DRM-LibSEP: drm_wv_mod_start_playback():6444: ERROR - Chaabi returned error value of 0x35.
02-10 16:29:55.530 12839 12869 E MDRMSession: DecryptCTR failed: DRM_WV_MOD_ERROR_MEMORY_NOT_ALLOCATED (0x35)
02-10 16:29:55.530 12839 12869 E WVCdm   : Decrypt error result in session sid2 during unencrypted block: 1
02-10 16:29:55.530 13362 13532 E MediaCodec: queueCSDInputBuffer failed w/ error -2998
02-10 16:29:55.530 13362 13362 D EventLogger: audioFormatChanged [2.61, id=0, mimeType=audio/mp4a-latm, bitrate=134359, channels=2, sample_rate=44100, language=eng]
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: Internal runtime error.
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: android.media.MediaCodec$CryptoException: Unknown Error
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at android.media.MediaCodec.native_dequeueOutputBuffer(Native Method)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at android.media.MediaCodec.dequeueOutputBuffer(MediaCodec.java:1033)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at com.google.android.exoplayer2.mediacodec.MediaCodecRenderer.drainOutputBuffer(MediaCodecRenderer.java:868)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at com.google.android.exoplayer2.mediacodec.MediaCodecRenderer.render(MediaCodecRenderer.java:489)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at com.google.android.exoplayer2.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:464)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at com.google.android.exoplayer2.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:300)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at android.os.Handler.dispatchMessage(Handler.java:98)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at android.os.Looper.loop(Looper.java:135)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at android.os.HandlerThread.run(HandlerThread.java:61)
02-10 16:29:55.530 13362 13453 E ExoPlayerImplInternal: 	at com.google.android.exoplayer2.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
``` venkataramanab@ I think your issue is content related so not the same root cause.

ckarthickit@ the issue you are reported is with the SoC we will need to forward this data to Intel. Hi,
The OEMs, Asus and Intel investigated and agree this is an issue, however the chip is aged enough they are not planning any updates to this device platform.  I am using r2.2.0 and getting exception while playing video using DASH url .Scenario is while I pressed home button between playing content some time its resume video but some time I am getting java.io.EOFException following is exception log please help .

```logcat
EventLogger: internalError [3.97, loadError] java.io.EOFException at com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFromDataSource(DefaultExtractorInput.java:252) at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFully(DefaultExtractorInput.java:67) at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFully(DefaultExtractorInput.java:76) at
com.google.android.exoplayer2.extractor.mp4.FragmentedMp4Extractor.readAtomPayload(FragmentedMp4Extractor.java:346) at
com.google.android.exoplayer2.extractor.mp4.FragmentedMp4Extractor.read(FragmentedMp4Extractor.java:248) at
com.google.android.exoplayer2.source.chunk.ContainerMediaChunk.load(ContainerMediaChunk.java:127) at 
com.google.android.exoplayer2.upstream.Loader$LoadTask.run(Loader.java:295) at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:422) at
java.util.concurrent.FutureTask.run(FutureTask.java:237) at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112) at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587)

ExoPlayerImplInternal: Source error. java.io.EOFException at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFromDataSource(DefaultExtractorInput.java:252) at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFully(DefaultExtractorInput.java:67) at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFully(DefaultExtractorInput.java:76) at
com.google.android.exoplayer2.extractor.mp4.FragmentedMp4Extractor.readAtomPayload(FragmentedMp4Extractor.java:346) at
com.google.android.exoplayer2.extractor.mp4.FragmentedMp4Extractor.read(FragmentedMp4Extractor.java:248) at
com.google.android.exoplayer2.source.chunk.ContainerMediaChunk.load(ContainerMediaChunk.java:127) at
com.google.android.exoplayer2.upstream.Loader$LoadTask.run(Loader.java:295) at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:422) at
java.util.concurrent.FutureTask.run(FutureTask.java:237) at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112) at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587) at
java.lang.Thread.run(Thread.java:841) E/EventLogger: playerFailed [3.98] 

com.google.android.exoplayer2.ExoPlaybackException at
com.google.android.exoplayer2.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:345) at
android.os.Handler.dispatchMessage(Handler.java:98) at
android.os.Looper.loop(Looper.java:136) at
android.os.HandlerThread.run(HandlerThread.java:61) at
com.google.android.exoplayer2.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40) Caused by: 

java.io.EOFException at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFromDataSource(DefaultExtractorInput.java:252) at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFully(DefaultExtractorInput.java:67) at
com.google.android.exoplayer2.extractor.DefaultExtractorInput.readFully(DefaultExtractorInput.java:76) at
com.google.android.exoplayer2.extractor.mp4.FragmentedMp4Extractor.readAtomPayload(FragmentedMp4Extractor.java:346) at
com.google.android.exoplayer2.extractor.mp4.FragmentedMp4Extractor.read(FragmentedMp4Extractor.java:248) at
com.google.android.exoplayer2.source.chunk.ContainerMediaChunk.load(ContainerMediaChunk.java:127) at
com.google.android.exoplayer2.upstream.Loader$LoadTask.run(Loader.java:295) at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:422) at
java.util.concurrent.FutureTask.run(FutureTask.java:237) at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112) at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587) at
java.lang.Thread.run(Thread.java:841)
```
 Please provide all the information required in the issue template. Thanks for response can you explain which type of information you required from my side 
Steps to reproduce issue
gradel dependecy
`compile 'com.google.android.exoplayer:exoplayer:r2.2.0'`
` compileSdkVersion 24
        buildToolsVersion '24.0.2'
        minSdkVersion 16
        targetSdkVersion 24`
Android version 4.4.4 api 19
and lollipop 

1. Playing DASH url from exoplayer 
2. In between video playing player release
3. Again initialize player it seek to current resume postion
 ` resumeWindow = player.getCurrentWindowIndex();
        resumePosition = player.isCurrentWindowSeekable() ? Math.max(0, player.getCurrentPosition()): C.TIME_UNSET;`
.Some time it resume and play video very fine but some time its call Player error state 
4 .On playerError state I am showing retry button and from retry button 
`initializePlayer()` but it never resume again 
5. when I seek it to start postion retry start work 

I have observed that when I get error some times `player.isCurrentWindowSeekable()` giving me false 
on override method 

` @Override
    public void onPositionDiscontinuity() {
        if (playerNeedsSource) {
            resumeWindow = player.getCurrentWindowIndex();
        resumePosition = player.isCurrentWindowSeekable() ? Math.max(0, player.getCurrentPosition()): C.TIME_UNSET;
        }
    }` 


IF you got my problem please suggest me  Can you explain how to reproduce this using the ExoPlayer demo app, or some minimal variant of it? Does it only reproduce for certain streams? If so, please provide one that reproduces the issue. This issue occurs when I press home and then resume again and again 
I have observed a case when I hit retry its not resume but when I drag seek bar it to start position  Player resume from start correctly .

This scenario occurs in Demo also but player set to start position automatically 
 Please can you provide precise reproduction steps for the demo app (i.e. exactly which stream you're playing, exactly what steps you're following, and how often the issue reproduces)? There's really not enough information here for us to reproduce the issue. @ojw28 
Just talking about Exoplayer demo application I am sharing a video with reproduction steps on youtube 
`Device version -kitkat 4.4.4
Model number -HM NOTE 1LTE`

**Video url -https://www.youtube.com/watch?v=pRi7bJJcACE**
In video I am playing from demo video list  **Widevine DASH :MP4,H264** ->first video 

1. Open video and select above video for playing
2. seek video to a particular position 
3.Now press home button and open app from recent again and again after 4-5 time 
4. Video starts from starting position 
I have attached full log 
[exolog.txt](https://github.com/google/ExoPlayer/files/779927/exolog.txt)

 There's no EOFException in the latest attached log. Neither is there any visible failure in the attached video. It's very unclear what the issue is you're trying to report. I see the playback starting from the beginning rather than the previous position, but this seems to be completely unrelated to the rest of this issue, which is about EOFException? Yes its completely unrelated but with my streaming url for same scenario I
am getting EOFException .I just wanted to know particular problem arise
with me only .I am using amazon DASH url .Should I handle EOFException
onPlayerError()? Despite multiple requests, you've yet to provide proper reproduction steps for the issue being reported. I'm going to close this. If you want to file a new issue that includes all of the information that's clearly requested in the issue template, please feel free to do so!  Please see #2139.  What, specifically, is the failure? Works well for me. I think whether this fails depends on which version of javadoc is being used. If I remember correctly java8 versions are stricter, and treat this kind of HTML issue as an error where previous versions didn't. You can probably use an earlier version to generate javadoc correctly. That said, we should obviously correct the errors. Thanks for reporting!  Disabling the unused renderers through the TrackSelector should fix your issue.  I have a feeling this is fixed in the `dev-v2` branch, and that it's no longer necessary to disable the renderer . I could be wrong though. Please give it a try if you have a chance. Pretty sure this is fixed in recent V2 releases. Closing.  Sanity check: Am I right in thinking the DASH IOP example is actually in violation of the DASH spec, because it uses string AdaptationSet@id values where-as the DASH spec says the type should be xs:unsignedInt? The example should use integers, I think? Heh, I filed one through the form on the DASH-IF website. I had no idea it got mirrored to GitHub. I'll file any future issues on that tracker directly! Thanks for confirming. This is now supported in `dev-v2`.  > Making the view non-final will help a lot in term of View inflation performance.

Will it really? Can you provide some data that backs up this claim? Allowing extension looks like it would let you to remove one FrameLayout from the view hierarchy, but I doubt this has much of a measurable performance impact. It might also be possible for your users to eliminate the additional view by including yours using the <merge> tag.

> Is there any special reason to make this View final?

We make things final where we don't think there's a good argument for extension as a way of limiting the (already very large) API surface, and guiding developers toward other (preferred) way of achieving their goals. @botondbutuza - What's your use case for requiring extension? Adding an extra button could be pretty easily done without extension. Replacing the layout using one of the approaches described [here](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a#.elkgs273q) would let you add an additional button, and you could find that button in a wrapping class using `findViewById` as normal.

I'm less sure about logging. It looks like almost everything in SimpleExoPlayerView is private, so I doubt you're able to access anything extra via extension. Or have you also modified SimpleExoPlayerView to make a whole bunch of private stuff protected instead? I don't think we can/will commit to not making breaking changes to what are effectively internal implementation details. So whilst this approach might work, you'll end up with your extension class repeatedly breaking when you update to a newer version of the library. In which case aren't you better off just taking a copy of the class and adapting it cleanly to suit your needs? Yeah. I'm not really convinced extension is really the right way to do it though. If the base class doesn't know about the views you're adding, it's not necessarily going to just "do the right thing" (e.g. show and hide the button at the correct times). It might work right now for your use case, but I suspect there are lots of other use cases where it wont work quite right. The logging part is also not really addressed. Isn't it better just to fork the class? If you want to customize controls to this extent, you're really best off just forking the class and adapting it to suit your needs.  It's giving me 403. Can you send another one?  Hello, I was not able to reproduce. I have tried version 2.2.0, 1.5.14 and 1.5.12, so I assume this particular stream does not cause the failure. Could you provide a link that reproduces the issue in the DemoApp in either version 2.2.0 or 1.5.14? I don't remember whether there's a bugfix for this particular issue in between, but there might be, in which case the only solution would be to update or manually backport the fix.

Once you have a link, please provide the reproduction steps via e-mail. Thanks!  Heck, it has just expired. I think I know what the issue is. You can check this yourself. It might be the case that some of the variants include audio only? If this is the case, there is no more need for debugging, I think.

Adapting to audio only variants is not supported currently. I need to discuss with my team whether it's going to be added to the roadmap. In any case, it's most likely going to be added to V2 only, so keep that in mind. But I highly doubt it.

Regarding the fix: Either add a CODECS attribute to the EXT-X-STREAM-INFs stating the actual codecs being used by the variants or directly remove those tags from the playlist. The CODECS attribute is the way to go, it has been around this HLS version 1, and marked as SHOULD later on. I don't really see a good reason not to add it. So variants with video will have something like CODECS="avc...,mp4a..." and audio only something like CODECS="mp4a..." without the avc part. Please confirm here if my supposition is correct (was about to do it just when the url expired), and if so, feel free to close the issue. If not, we can try again tomorrow. After looking into the transport streams I can confirm that there are at least two audio-only variants. So [my comment](https://github.com/google/ExoPlayer/issues/2428#issuecomment-278458411) still applies. Adding the codecs information is the best you can do right now. You can track #545 if you are interested in this feature. Both links give me 403.  Can you fix this? Every second chunk of the variants does not declare video (but does contain the actual payload, which explains why it works when the player can reuse the extractor). For what it's worth, it is somewhat harder to reproduce in V2, and if we were to introduce any modification, it would be in that version.

I am sorry to hear that some streams used to work and no longer do. We are usually flexible about this, if the fix is simple and relatively sensible. But in this case, I can see no simple way to patch Exoplayer. Even less using assumptions that make sense in a majority of cases. It is impracticable to maintain a correct implementation if we patch it to keep support of all non-spec media. In the same way, we cannot provide a well defined behavior for all incorrect media. if it works, it does not guarantee it will keep working. The ground truth is always the spec. I'll do my best:

> What exactly is the spec that our playlist/video is not complying to? 

The violated spec is not HLS itself, but the Transport Stream spec (ISO/IEC 13818-1). 

```
The Program Map Table [...] specifies, among other information, which PIDs, and therefore
which elementary streams are associated to form each program.
```

> What do you mean by "does not declare video"?

So this is what the first chunks look like:

```
        Stream_type loop: 

            Stream_type: 27 (0x1b)  [= AVC video stream as defined in ITU-T Rec. H.264 | ISO/IEC 14496-10 Video]
            reserved_1: 7 (0x07)
            Elementary_PID: 257 (0x0101)
            reserved_2: 15 (0x0f)
            ES_info_length: 0 (0x0000)


            Stream_type: 15 (0x0f)  [= ISO/IEC 13818-7 Audio with ADTS transport sytax]
            reserved_1: 7 (0x07)
            Elementary_PID: 258 (0x0102)
            reserved_2: 15 (0x0f)
            ES_info_length: 0 (0x0000)
```

This is okay, there is one PAT, one PMT and the PMT declares, as you can see, an audio and a video elementary stream. This was the output of `dvbsnoop -tssubdecode -s ts -if 456826_00_03_MM30_Exercise_1650.mp4Frag1Num0.ts 256`.

Now if we look at `dvbsnoop -tssubdecode -s ts -if 456826_00_03_MM30_Exercise_1650.mp4Frag1Num1.ts 256`. (note the file is Frag1Num1 now, the second chunk).

```
 Stream_type loop: 

            Stream_type: 15 (0x0f)  [= ISO/IEC 13818-7 Audio with ADTS transport sytax]
            reserved_1: 7 (0x07)
            Elementary_PID: 258 (0x0102)
            reserved_2: 15 (0x0f)
            ES_info_length: 0 (0x0000)
```

There is only audio. As a side note, the third chunks are okay again. However, the PID 257 (which corresponds to video in the first chunk) is populated in the second chunk, so I suspect only the PMT is wrong. The media content is actually there.

> Are you referring to the playlist containing the chunks?

No, I was referring to the chunks themselves. Playlist looked fine to me, though I was not looking for anything there, so don't take my word.

> Or specifically to chunk #2 of that playlist, the .ts file?

Specifically chunk number 2 of every media playlist (I didn't actually check them all, I think). The snippets above were obtained from the 1650000 variant.

> The video production team mentioned one possibility could be chunks being split in non-keyframe frames, as the cause of the issue. Is this what you're referring to?

No, that is not enforced by the spec and we do not count on it. Though it's always better, as stated by the spec:

```
The server SHOULD attempt to divide the source media at points that support 
effective decode of individual Media Segments, e.g. on packet and key frame boundaries.
```

I hope this answers your questions.  @Fed93 please add this to the flac extension proguard file (extensions/flac/src/main/proguard-rules.txt):
```
-keep class com.google.android.exoplayer2.util.FlacStreamInfo {
    *;
}
```
and try again. Good, soon a fix for it will be committed.

Unfortunately we don't support seeking if the flac file doesn't contain an seek table. See #1808  Reopening until fix is merged.  There are a few reasons:

* There used to be only 3 sample stream wrappers. Each of them(if present) would download one chunk to prepare, and assume the others tracks had the same shape. Sometimes this is ok, if the only changing aspect is the language, for audio and text. This leads me to:
* The codecs attribute is optional, and other information is not available in EXT-X-STREAM-INF or EXT-X-MEDIA. Particularly in older spec versions, like number of audio channels. This affects the decision of whether the format is supported by the device. After preparation, figuring these things out is too late.
* Lastly, maybe less important, this change allowed us removing non-adaptive track groups and selecting two audio tracks at the same time.

I hope this answers your question.  We might do something about this in the not-so-remote future.  That definitely shouldn't happen, but I cannot reproduce the issue either. Please provide proper reproduction steps, preferably using the ExoPlayer demo app or some minor modification of it. No worries; thanks for the update :).  A fix for this will be available in the next push.  Would you kindly provide a stream that reproduces the described issue to dev.exoplayer@gmail.com? 

Regarding device compatibility in Exoplayer: it only applies to codecs. e.g: A device might not have decoders for VP9 or support a particular H265 profile, but container formats or streaming protocols are not affected. You can find [here](https://source.android.com/compatibility/cdd.html) useful information about codec compatibility.

In other words, if both your DASH and HLS streams use the same compression formats, all streams _should_ be supported by the same set of devices. Right now, it is advisable to strive for the _technically better_ streaming format. In this context, DASH. Yes, the streams don't need to be yours. I just need to reproduce the issue myself.  Thank you for the e-mail. I will have a look as soon as I have some free time. I'll let you know when I have anything interesting to share. Just as a heads up: I have been giving it a look, but the streams I've generated so far don't play in other players (vlc, for instance. Oddly, with similar behaviors) and don't pass validation. I'll try to figure out the cause, though. Honestly, I haven't checked the HLS streams. Will do later. The HLS version seems fine, from what I have played around. @ojw28 will probably be able to tell you more about the DASH one. But its certainly broken. The YouTube DASH manifests that you're attempting to play are invalid, and are not what YouTube serve to ExoPlayer themselves.

The first version of DASH live that YouTube developed for web (i.e. streams + support in their html5 player) was not spec compliant. In particular, there were some issues with the way the manifests were being generated and updated. This is understandable, since the spec is quite hard to follow and probably wasn't even finalized at the time. YouTube later fixed their manifests, but to avoid breaking existing players that were consuming the invalid manifests generated by their first attempt, they decided to serve both the invalid and fixed versions side by side.  I suspect the html5 player has not been updated to move onto the fixed versions, which is why the approach you're using to obtain the streams gets you the invalid ones. The YouTube Android app will be requesting the fixed versions for use with ExoPlayer.

So in conclusion, if you're reverse engineering an API don't expect what you find to be nice and shiny and spec compliant ;). We're not able to provide any details with how you might find the compliant streams, for obvious reasons. Yeah. More fundamentally, you should consider whether you really want/need to be generating manifests that look anything like the one you were looking at, which appears to be ~2MB. That's going to be fairly horrendous if you're having to re-load it every few seconds. It'll probably gzip very well for transfer over the wire, but that's still a lot of data the player will need to repeatedly process.

It's possible to generate far more concise manifests for VOD, Live and Live-Event use cases if you use SegmentTemplate rather than explicitly listing all of the segments in the manifest.  These are two completely different issues, aren't they? Please, create one new issue per topic and most important of all:

Read the issue template and provide **all the required information**. Logcats alone are almost always insufficient.  In the snippet of code in the StackOverflow post, you instantiate a `DefaultDataSourceFactory` that gets passed in to the `ExtractorMediaSource`. If you want to use a custom `DataSource`, you need to instead instantiate a `DataSource.Factory` that creates instances of the custom `DataSource` that you want.  Currently, the only encryption method supported is AES-128. I doubt we will be adding any non-spec defined methods in the near future. Am I correct to say this encryption method escapes the spec? You can see what formats (and content protection) we support [here](https://google.github.io/ExoPlayer/supported-formats.html).  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> We'll be merging official support for variable rate playback fairly shortly. Hi @ojw28 :

> We'll be merging official support for variable rate playback fairly shortly.

Do you mean to support "setPlaybackSpeed" or a more complete solution (rewind & high speed too)?

Thank you! It's unclear to me how "playback speed" and "high speed" are different things. We're not doing anything related to rewind at this point. Closing this PR due to incoming official support. It's also proposed to merge into the wrong branch (release rather than dev).  I think I have rootcaused the issue. The solution might have to wait a bit, though. 

Note to self: Codec reconfiguration without reinstantiation is attemped, but it is not working well. When not, playback works well.  What I could observe is that the PSP and PPS differ between content and ads, I imagine that using the same parameters would fix the issue. I am not 100% sure, though. I need to research a bit more. I think this should definitely be fixed in exoplayer.

An ignorable note is that you change any value(like width, for instance) that would make MediaCodecVideoRenderer#canReconfigureCodec() return false, that would make things work, but this is just anecdotal. It does not reproduce on Sony Bravia 4k, I wonder if we could apply a codecNeedsAdaptationWorkaround here, @andrewlewis? Either that or codec reconfiguration is not working properly and the TV's decoder is somehow managing to work around it. That is indeed strange! We have already contacted the media codec team to see what's their view on this. We will post updates here. There are still no news about this, guys, but it's still being tracked. @Ambroos I'm afraid we haven't heard anything back from Qualcomm yet but will ask them for an update.

Knowing that this issue occurs after some adaptations is not enough to implement a targeted workaround -- we really need to find out what characteristics of this adaptation are causing the problem. One option is to try making incremental modifications to the parameter sets in the content/ads and seeing whether the issue is still reproducible with each modification. On a different chipset we saw issues with adaptations where the maximum number of reference frames changed but not the resolution, so you could try changing the number of reference frames used in the content/ads and see if this affects whether the issue is reproducible or not. Would you mind providing a logcat output using while using nexplayer? That will allow us to determine whether a decoder is being instantiated. Sorry for the delay on updating this, but we haven't had any response either. I'll see what I can do to provide a solution from our side. I'll have a reply next week, probably on Tuesday/Wednesday.  Hello, @Ambroos. We still haven't decided what to do about this as we don't know the root cause of the reconfiguration error (making it hard to target a workaround). I don't have much time right know to allocate to the required research, so it will have to wait unfortunately.

Being urgent for you, I can suggest disabling codec reconfiguration on devices that reproduce the problem. To do this, you can return false on MediaCodecVideoRenderer#canReconfigureCodec()[1] for them. This is not ideal, since adaptation works well on most other cases, but I think this is a sufficient patch until a proper solution is built in.

I know it's inconvenient to add changes to the library itself, but being a small change, I don't think there are great upgrading costs to it. Let me know if this solves your issue for now. I'll try to come back to this as soon as I have some more time.

[1] MediaCodecVideoTrackRenderer#canReconfigureCodec() on v1. Other than a short pause (say, 100 ms), there shouldn't be any problems. Of course, in comparison, there is simply not much choice here. Do keep me updated if you run into problems. Something that @ojw28 brought up: It would probably be a good idea only to disable reconfiguration if the resolution is unchanged.  It looks to me like this is working as intended. The video track in this mp4 file includes an edit list (edts box) that appears to specify this behaviour during playback. If we simply ignore the edit list then playback works fine, and it's likely that other players appear to be playing the content correctly because they don't suppoert edit lists (where-as in fact ExoPlayer is doing what the media really does specify by handling them - and if the media isn't supposed to be paused in this way the edit list should not be present in the media).

I'll leave this open for @andrewlewis (resident edit list expert) to confirm, since I'm not 100% sure on the structure of edit list boxes (the specification is quite vague). Not without modifying the ExoPlayer library and then building from source. It's unclear that's really what you'd want to do anyway, given it would result in incorrect playback of all media that *does* have an (intentional) edit list?  I don't think it's possible to use MediaCrypto/MediaDrm in conjunction with software decoder extensions (i.e. decoders that aren't in the platform). @wvpaf can confirm. That parameter name isn't accurate, so don't read anything into it. If you look at what happens on the JNI side of the opus extension, you'll see that `opusSecureDecode` is hardcoded to fail (see [here](https://github.com/google/ExoPlayer/blob/r2.2.0/extensions/opus/src/main/jni/opus_jni.cc#L128)).

It is theoretically possible to:
- Implement your own native library that implements DRM and an opus decoder.
- Build `ExoMediaCrypto` and `ExoMediaDrm` implementations on top of this native library.
- Specify that this native library should be used by the opus extension, by calling [`OpusLibrary.setLibraries`](https://github.com/google/ExoPlayer/blob/r2.2.0/extensions/opus/src/main/java/com/google/android/exoplayer2/ext/opus/OpusLibrary.java#L34).
If you were to do all this you could have a software based DRM solution that works with the opus extension, and the reason `OpusLibrary.setLibraries` and `opusSecureDecode` exist is to make this possible.

The above is not something I'd recommend attempting unless you're a DRM expert. It's quite involved on several fronts.  The download link on that page doesn't seem to work. Please could you share some instructions describing how to download the file? If the player is stuck buffering, I'd guess this might be the same as #1386 but we'd need to look at the media to know for sure. I can't reproduce this issue (the stream plays fine in the ExoPlayer demo app on a Nexus 6P).

Please provide the output of `adb bugreport` so we can find out what device/build is being used. @begetan The emulator doesn't support this video size (`D/MediaCodecInfo: NoSupport [size.support, 3840x2160] [OMX.google.h264.decoder, video/avc]`). Note that the [Android CDD](https://source.android.com/compatibility/7.1/android-7.1-cdd.html) doesn't require devices to support 4k video decoding for H.264/AVC.

If the videos play using `MediaPlayer` but not using ExoPlayer we should investigate further. It sounds like it's device-specific, and in this case it's useful for us to have a bug report so we can see the exact device name and build number, and look at system logs. (Knowing that it's a "TV Box" or "set top box" is not enough.) I'm not sure whether you're referring to playback via MediaPlayer or ExoPlayer, but ExoPlayer's Matroska extraction functionality doesn't rely on media framework support, so it should work fine on all devices and the emulator. It may not be possible to play the extracted media, however.

To make progress on this issue it would be useful to have bug reports taken from the (non-emulator) device(s) where the videos wouldn't play. It's quite possible that the video decoders just aren't capable, as per my previous response, in which case the objective would be to fix capabilities reporting if necessary. Closing due to lack of information.  [This blog post](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a) describes various ways in which you can customize ExoPlayer's UI components. The easiest way to do what you want is to copy `exo_playback_control_view.xml` in the res/layout directory of your application and then customize it to meet your needs (e.g. by deleting the SeekBar element, or replacing it with something else - make sure what you replace it with doesn't use the same id if you do this).

You can also just implement your own UI from scratch. There is nothing forcing you to use `SimpleExoPlayerView` or `PlaybackControlView`. These are just standard UI components provided for convenience.  Please provide the information requested in the issue template. We need a link to the problematic media to reproduce and investigate the issue. How are you cutting the stream? The linked file has a Xing header but it doesn't have a table of contents so we treat the file as being unseekable. The tool you're using does not seem to be configured to rewrite a valid Xing table of contents. There may be an option to do this, or you may need to use a different program. You could try FFmpeg which rewrote the Xing header correctly when i tried it ([see instructions here](http://stackoverflow.com/questions/43890/crop-mp3-to-first-30-seconds)).  Widevine L1 is typically factory provisioned, meaning provisioning wont ever need to happen. When using L3 (either because you've forced an L1 device to L3 or because the device only has L3) provisioning typically happens once. A factory reset should cause re-provisioning. If the device is rooted then you can also cause re-provisioning by doing:
```
adb shell rm -r /data/mediadrm/IDM1013
```
which will reset Widevine modular DRM back to its factory state.  Thanks for the thorough issue description.

Reading the specification, I think that last sample should be 'sliced' to duration zero. Unfortunately our handling of edit lists is incomplete (see the comment in parseStbl) and it looks like making your suggested change would mean that we could include a whole audio sample that should be completely removed. Why does this track have an edit list at all? It sounds like it isn't needed, and if it were removed you wouldn't encounter this edge case. Does it work if you just pass `-use_editlist false` to FFmpeg, or are you relying on edit lists for something else? I'll submit a fix that includes the sample for edits in non-audio tracks. This issue will be updated once it's merged. Thanks.  Using TextureView is fundamentally less efficient than using SurfaceView, so if you're pushing against the limitations of the device you should expect a slightly choppier experience. The rotation issue using SurfaceView is already tracked in the issue you reference.  @yshrsmz Please could you provide the output of `adb bugreport` taken just after seeing the error?

@cre8ivejp Do you know how this is related to the support library? Is it somehow related to the MediaRouter bug mentioned on the [support library revisions page](https://developer.android.com/topic/libraries/support-library/revisions.html#25-1-1), or is it a different issue? Thanks. Closing due to inactivity.  Looks like the other issue you filed (#2359). We need the problematic media to investigate, as requested in the issue template. Closing due to lack of information.  Wrapping the second source in a ClippingMediaSource should work for this. See #1988 for more info. We have a way to do this with [Timeline.getDefaultPositionUs](https://github.com/google/ExoPlayer/blob/f276eb2/library/src/main/java/com/google/android/exoplayer2/Timeline.java#L319) but it's not convenient to use at the moment unless you have a custom MediaSource where you can update the Timeline when the desired start position changes.

I'll mark this as an enhancement to track providing an easy way for apps to set the start position, perhaps via ExtractorMediaSource or ConcatenatingMediaSource.  We're only accepting bug fixes into V1 at this point. Move to V2 for shiny new things ;). Not being able to use multidex for various (unspecified) reasons doesn't sound like an ExoPlayer problem. This is something you'll likely need to resolve locally. Sorry.  See https://github.com/google/ExoPlayer/issues/1988.  If you're using `SimpleExoPlayerView` you can get the `SubtitleView` by calling `simpleExoPlayerView.getSubtitleView()`. Once you have that view, there are a bunch of methods you can call on it to change the font and style. See the [documentation here](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ui/SubtitleView.html), in particular methods like `setStyle`.  The most important piece of information we need here is exactly how you're building the MediaSource from the 9 files you provided via email (there's no MergingMediaSource anywhere in the sample code provided above). Can you please provide exact code for constructing a MediaSource out of the 9 files provided that will reproduce the issue? I think this is an issue with MergingMediaSource/MergingMediaPeriod, where the following sequence happens:

1. Child A stops loading and calls onContinueLoadingRequested. This request is bubbled up to ExoPlayerImplInternal.
1. ExoPlayerImplInternal calls continueLoading on the MergingMediaPeriod. Child B is furthest behind, so the load is routed to Child B.
1. Child B finishes loading.

Step (1) and (2) is (sort of) working as intended. The biggest issue is that (3) should trigger a new onContinueLoadingRequested call to trigger Child A, but this doesn't happen. This results in the stuck buffering state. A secondary issue is that (2) doesn't do a good job of allowing parallel loading of the two children in the case that they're ExtractorMediaPeriod's. An easy way to fix this is to modify the ExtractorMediaPeriod.onLoadCompleted method [here](https://github.com/google/ExoPlayer/blob/r2.2.0/library/src/main/java/com/google/android/exoplayer2/source/ExtractorMediaPeriod.java#L340) to be:

```
  @Override
  public void onLoadCompleted(ExtractingLoadable loadable, long elapsedRealtimeMs,
      long loadDurationMs) {
    copyLengthFromLoader(loadable);
    loadingFinished = true;
    if (durationUs == C.TIME_UNSET) {
      long largestQueuedTimestampUs = getLargestQueuedTimestampUs();
      durationUs = largestQueuedTimestampUs == Long.MIN_VALUE ? 0
          : largestQueuedTimestampUs + DEFAULT_LAST_SAMPLE_DURATION_US;
      sourceListener.onSourceInfoRefreshed(
          new SinglePeriodTimeline(durationUs, seekMap.isSeekable()), null);
    }
    callback.onContinueLoadingRequested(this);
  }
```

Which is just adding the last line to the end of the method.  Seems like a very minor issue. @marcbaechinger - We should probably prevent the controls from hiding whilst the user is interacting with them, and time the hiding of the controls from the point at which the touch ends.  This is on our todo list, but is not something we currently support. Isn't the video id normally retrievable from the body of the key request? In which case one option would be to have a single license URL and have the server get the video id from the request body. That's correct until we add support (we don't have an ETA, but we're aware of the fact we're not adequately supporting this use case).  Could you explain why unity needs this (i.e. what it does with the event)? Thanks! How does each video frame actually get from ExoPlayer to Unity? Presumably you've provided a `Surface` of some kind to `ExoPlayer`, which was provided by Unity, and `ExoPlayer` is calling `releaseOutputBuffer` to release each video frame to the `Surface`. Is that correct? I wouldn't be surprised if there's some async step somewhere buried in the `Surface` implementation though, because the actual drawing in Unity is likely to be on a different rendering thread owned by Unity? In which case aren't glitches always going to be a possibility, unless you go quite a bit deeper in terms of how you implement the integration?

Ignoring the above, `onVideoInputFormatChanged` is definitely too soon, since it's triggered when the format going *into* the decoder changes, where-as you want an event when the format coming *out* of the decoder changes. What properties actually differ in the different representations that you're adapting between? If video width/height changes then you should get a `VideoRendererEventListener.onVideoSizeChanged`, which will be fired at a more appropriate time. Is `MediaCodecVideoRenderer.onOutputFormatChanged` invoked at all on the format change, or is the format identical as far as the output side of the decoder is concerned, in which case it wont be invoked?

If it's not invoked then I think we'd need to record the presentation timestamp of the first input buffer to be queued after each change of input format. Probably as some kind of map from timestamp to format. Then we could look at the presentation timestamp of each output buffer being released. When an entry is found in the map, we could fire an event passing the mapped format, and remove the entry. Maybe you could have a try at implementing this approach to see if it works? If it does then we can consider merging a change. Note that a good solution should avoid excessive long to Long boxing/unboxing.  There are a few existing issues about playing multiple videos at once (e.g. #1286). Please search existing issues before filing a new one.  - I don't think it's realistic to expect the software VP9 extension to be able to decode 2K or 4K in real time (depending on bit-rate), so I think a low frame-rate would be expected.
- When not using the extension I suspect it's a device specific issue, particularly since you say Pixel C is working well. You should report it to whoever provides the Android build on the device (typically the device manufacturer). You may also just be hitting limitations of the device. The bit-rates you're using (up to 23Mbit/s) are extremely high. HEVC and VP9 are more appropriate codecs for 2K/4K, if the boxes support them. It's most likely a device specific issue, as per my comment above. Please report it to whoever provides the Android build on the device.  Please read [this blog post](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a#.bbjreh63z). Beyond that, this is something you'll need to figure out by yourself or seek help about on StackOverflow.  Closing because the report does not include any of the information requested in the issue template. Please provide complete information as requested in the template if you want us to take a look. Tracked in #2390  I'm not really sure what the issue you're facing actually is. `Aes128DataSource` is not part of the public API. It's package private to the HLS package and is not meant for use externally. Seeking specifically for HLS with AES-128 should work fine because the media is segmented into separate files.

Please clarify exactly what you're trying to do and why it's not working. It's not possible to say where the problem is because there's insufficient information provided (including how we can easily reproduce the issue or details of the changes you've made). There are no known issues with regular HLS with AES-128 playbacks. If you're inserting the key/IV and instantiating the source correctly then I wouldn't expect issues with your approach either. It's unlikely we'll have time to look into this unless you're able to share a (smallish) patch to the demo app that reproduces the issue, along with sample content to reproduce the problem. It sounds likely to be an application specific issue rather than a problem with ExoPlayer.

I can't think of a reason why you shouldn't share a patch publicly. You can send links to the media to `dev.exoplayer@gmail.com` if you'd prefer for those not to be public. If you want us to look at this then please provide a small patch to our existing demo app, as opposed to a whole new demo app that we're not familiar with. I took a look at this. I think your AES-128 logic is correct and working. It looks like the media (after removing the AES-128 encryption) violates this part of the HLS spec:

> Each Transport Stream Segment MUST contain a PAT and a PMT, ideally at the start of the Segment, or have an EXT-X-MAP (Section 4.3.2.5) tag applied to it.

Out of the segments that I looked at, only the first one contains the PAT/PMT. This is why seeking to 0 (or anywhere within the first segment, so up to 10s) works, whilst seeking after 10s does not.

If you remove the AES-128 encryption from all of the segments and try and play them individually (i.e. as regular TS files) I think you'll find that only the first segment plays (in any player) due to lack of PAT/PMT in the other segments. The solution here is to fix the media.  Thanks for reporting this. For the sample manifest I'd expect playback to work due to the presence of `cenc:pssh` elements. It shouldn't be necessary to look at the `mspr:pro` elements. According to [Microsoft's doc](https://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjfrffjsujRAhUTOsAKHcbED-oQFggaMAA&url=http%3A%2F%2Fdownload.microsoft.com%2Fdownload%2F7%2F7%2F6%2F7762455C-D254-4C84-BE17-16B0C60E31FD%2FMPEG%2520DASH%2520PlayReady%25201.2%2520-%25202014-10-08.pdf&usg=AFQjCNGcL0LPPQgkIksJUMocue_H2Ig4aw&sig2=FZBLgxEgFxWlm_JrlFG3_g&bvm=bv.145822982,d.d2s):

> In the case of PlayReady, a PlayReady Header Object (PRO) [PRHEADER] can be contained in a
cenc:pssh element, an mspr:pro element, or a ‘pssh’ box to enable license acquisition. The
mspr:pro element is defined by Microsoft PlayReady, and includes only the PRO [PRHEADER]
information, not the box structure included in ‘pssh’ and cenc:pssh. Including both mspr:pro
and cenc:pssh will enable old players including a player based on Silverlight, and new players
including web pages using script to play protected DASH presentations on HTML5 browsers. 

Which suggests that "new players" typically look at the `cenc:pssh` elements only. Could you clarify whether your manifest is missing `cenc:pssh` elements, and if so why? I think including one is the correct thing to do. If your manifest includes `cenc:pssh` elements then I think something else is wrong. There's no particular reason why we can't parse `mspr:pro` elements as well if necessary, but I'd like to make sure we understand exactly what's going on fiest. If they don't have an option, you should probably request that they add one (they should probably be including both cenc:pssh and mspr:pro by default as suggested by Microsoft's doc for compatibility with both old and new players. The change ref'd above adds support for the `mspr:pro` element in V2, although I agree they should be including `cenc:pssh` as well regardless. We have no plans to backport. We're pretty much back-porting bug fixes only to V1. This is not a bug fix. There's also not much demand (this is the only issue requesting support for the element), should be fixed on the serving side anyway (by including a `cenc:pssh` element), and can be resolved by moving to V2.  What API level is the device you're testing on? I *think* `SurfaceView` should work correctly during the drag on Nougat, so one option you have is to use `SurfaceView` on Nougat+ and `TextureView` on earlier API versions. This will at least mean you transition over slowly as more devices get Nougat.

If I remember correctly, there's a workaround you can use on earlier API levels where you obtain the `SurfaceView` from the `SimpleExoPlayerView` like:
```
SurfaceView surfaceView = (SurfaceView) simpleExoPlayerView.getVideoSurfaceView();
```
Then during the drag, you can repeatedly call:
```
surfaceView.getHolder().setFixedSize(w, h);
```
to force the surface to resize itself properly. Maybe give that a try and see if the approach looks promising? You'll have to figure out exactly where to get `w` and `h` from. It might be enough to use `surfaceView.getWidth()` and `surfaceView.getHeight()` for the width and height. Worth a try anyway. I think it's very unlikely you'll manage to swap the view type just for the drag in a way that's anywhere close to being seamless (i.e. I wouldn't spend too much time, if any, trying). You're best bet is probably TextureView before N and SurfaceView after N, if the `setFixedSize` hack doesn't work well enough.  There's insufficient information provided here for us to efficiently investigate your issue. It sounds most likely to be a bug in application code. Please explain exactly how to reproduce the issue with the demo app, ideally by providing a patch, pull-request or link to a forked version of the demo app that reproduces the issue so that we can see exactly how you're creating and passing the surface. I couldn't see anything obviously wrong, but the app you've provided is pretty complicated and it's not realistic to expect us to fully understand and debug it. I did a quick test modifying the ExoPlayer demo app to use `setVideoSurface(new Surface(surfaceTexture))` and it appears to work fine, so I still think this is an issue with your application code. You can find my test modification [here](https://github.com/google/ExoPlayer/commit/8a3fa03b27fc869e6ca232cf434b613fea32184b).

If you're able to provide a small modification to *our* demo app that more clearly demonstrates there's an issue with the ExoPlayer library rather than your own code, please do so and we'll take a more detailed look. Closing for now as I can't see anything to suggest it's a library issue at this point. By the way, the surface definitely appears to be passed to the video renderer. The renderer appears to initialize the codec and the logs don't indicate any failures. So if I had to guess I'd say it's more likely than not that the video frames are being rendered to the surface provided, and that it's either the wrong surface, that it's not being rendered properly in the 3D scene, or that it's somehow being occluded by something else. That's where I'd start if I were debugging this, in any case.  Play the URL stream in the demo app. Check whether the text track is being exposed. If so, you can select it from the drop-down list. Have a look at the demo app's code for a precise guide on how to achieve this in your own app.

If the text track is not exposed, then check with other players whether the media is correct. If other players do correctly play the stream, please file a new issue including all the information requested in the issue template for us to look at.  As requested in the issue template, please send the stream to dev.exoplayer@gmail.com. I suspect that the stream is dynamically adding a new track in the TS(you can easily notice this), though it is quite strange that the iOS player fails in this case. Please give the last release version a try, as there have been many changes. If my theory is correct, it should still fail, but it minimizes the risk of running into an already fixed bug.

It is really hard for us to evaluate the failure with this amount of information. Please read the issue template. Providing a way for us to reproduce the issue usually answers most questions. If the new track happens to be SCTE 35 Splicing information, the bug will be fixed in the last release version. I don't think it's likely to be SCTE-35 given: `length=3; index=3`. This indicates the number of sample queues has changed from 3 to 4. I think I'm right to say that we'd expect 4 queues for this media (audio, video, cea608, id3). Of these id3 is generated up front, video and cea608 are generated together when video is found, and audio is generated when audio is found. So I think the only consistent explanation is that the first segment(s) do not contain or declare any audio. They should do so, even if it's silence.

If the above theory is correct then this issue is the same as https://github.com/google/ExoPlayer/issues/2234. Closing on the assumption the theory is correct, since working test content has not been provided.  Please provide a full bug report. It would be helpful if you could stop filing issues without providing all of the information requested in the issue template. We're really not able to help with these issues without complete information, and having to respond to each one requesting it is not an efficient use of time.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> I'm somewhat unclear what the aim is here, or exactly what the desired workflow is. If you want to depend on a local snapshot, why not just build the .aar and then do "File -> New -> New Module -> Import .JAR/.AAR Package" in Android Studio? Please clarify. Thanks!  I think `getContext()` in your application code (above) is returning `null`. You then end up passing `null` to `newSimpleInstance`, which results in a `NullPointedException` when that context is used during player construction.

I'm not certain when `getContext()` returns a null value, but you can start figuring this out [here](http://stackoverflow.com/questions/18154905/why-is-my-context-in-my-fragment-null). I suspect the fragment is either not yet attached, or no longer attached, to the activity when you're executing the code (above).

Closing since this appears to be an issue in application code.  My suggestion is to try DASH, SS or HLS. If your files are entire presentations (i.e. not chunked), this is not supported. You would have to create your own media source, choose the URL based on available bandwidth, solve ranged requests and splicing, etc. Basically, you need to reinvent adaptive streaming. If you choose to go down this path, have a look at the existing adaptive media sources.  It sounds from the issue description like this was a device specific issue that Samsung resolved in their Android 7 update for Note. If that's the case there's probably not much we can do to help, and you should advise affected users to take the Android 7 update for there device. Have I misunderstood, or does that sound right? Closing due to lack of clarification.  The stream plays back with exactly the same interference in VLC as well, so I think this is likely to be an issue with the content. A 50% success rate doesn't exactly confirm the media is correct. Two players that do not share common code, failing in the same way, where they do not fail for other streams of the same type, still implies the media is more likely or not at fault.

The stream is now broken (times out), so we're unable to test further. If it can be made to work reliably then we may do a small amount of additional investigation.  Did you look at this (top result for this error + VP9 on Google): https://groups.google.com/a/webmproject.org/forum/#!topic/codec-devel/B0UQTGK-4dc Duping to #2339  Please provide the information requested in the issue template if you want us to look at this. We cannot investigate problems of this kind without complete information, including a full bug report (instructions on how to obtain one are in the issue template you clicked through when filing this issue). @erenbakac, the error indicates the license request is too large for this device. 
Can you try a license request with fewer optional parameters in getKeyRequest for this device? 
Starting in Android N this should not be an issue.   Thanks for clarification in your previous response (although it seems to have disappeared somehow). I don't think you should modify `MimeTypes` as part of this change. I'm not convinced it's a good idea to change `FragmentedMp4Extractor` either.

It feels to me that you're trying to work around a bug in the underlying CDM implementation on older versions of Android. The correct place to do that is probably `DefaultDrmSessionManager.acquireSession`. There's already a workaround in that method for the Widevine CDM, so you can add a second workaround there that changes `schemeMimeType` if all of the following are true:

1. `Util.SDK_INT < 23`
1. `C.CENC_UUID.equals(uuid)`
1. `MimeTypes.VIDEO_MP4.equals(schemeMimeType)`

That way the workaround will be applied correctly regardless of where the media is coming from (e.g. if another extractor implementation is used). `Util.SDK_INT < 26` then, since that would suggest not all devices with `SDK_INT == 25` will include the change, but that all devices with `SDK_INT == 26` or greater will? Thanks! I simplified this somewhat in https://github.com/google/ExoPlayer/commit/3bb08e58f68ee1cc5a288b601c5fec3068cd83da. I think it's equivalent, except that it always replaces schemeMimeType to "cenc" on API level 25. I think this is fine - my main concern was just not applying the workaround indefinitely, which is achieved by capping to not apply on API levels above 25. Let me know if I accidentally broke something!
 Also corrected some naming in https://github.com/google/ExoPlayer/commit/5fe5076c8686a09608bbef8797e1c16fa8d80fd8. I don't dislike it. It just seems unnecessarily complicated to use it just on one API level.  This is not currently supported. Out of interest, what's the actual use case for this? If the user is manually changing what they're trying to view via some UI, then it's in my opinion quite confusing for the actual switch to be delayed. If you're performing some kind of automatic adaptation, is the content in the two adaptation sets the same or different, and why is the switch necessary?  You should implement your own `MediaDrmCallback` instead, which is one of the arguments passed when creating your `DefaultDrmSessionManager` instance. There are multiple ways in which you can implement the details. One option would be to do something like:

```
public final class MyMediaDrmCallback {

  private final HttpMediaDrmCallback wrapped;

  ....

  @Override
  public byte[] executeKeyRequest(UUID uuid, KeyRequest request) throws Exception {
    byte[] response = wrapped.executeKeyRequest(uuid, request);
    // Unwrap the raw response, optionally parsing additional JSON you've added.
    return rawResponse;
  }

} 
```  The relevant error is `java.net.ProtocolException: Unexpected status line: ICY 200 OK`. See #890 and #1211. We don't directly support ICY but you can try using the [okhttp extension](https://github.com/google/ExoPlayer/tree/release-v2/extensions/okhttp).  Handling of audio focus is pretty much independent of the player, as far as I know. You listen to the broadcast, and then call whatever methods you want to change the state of the player (e.g. pausing it, stopping it, or whatever logic you require) when a broadcast event is received.

This is exactly the same as what you'd do if you were using MediaPlayer or any other third party player, also, so unless I've missed something I suggest you look for general tutorials on the issue.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> There is already a pull request for this (#2219)  If the timeline is empty any index you pass to `getWindow()` will result in an out of bounds exception. You should check whether the timeline is empty before you query it, by calling `Timeline.isEmpty`.  This isn't something we'll be supporting. You can add support in your own fork if you really need it. I suspect there are much better ways you could set this up on the server side that would avoid this problem.  Please see [this FAQ](https://google.github.io/ExoPlayer/faqs.html#does-exoplayer-support-emulators).  As per title. That's the intention, yes. We're not promising to add support, but it's something we're actively looking at. This issue will be used to track any work in this area. If we do add support, it's likely that we will require the Accessibility element to be present in the AdaptationSet for CEA-608 to be enabled. It sounds like you're already adding it.

It would be preferable, although likely not required, if you could also fill out the `value` of the Accessibility element to indicate the language (and multiple channels, if applicable). This is described in the DASH-IF-IOP document also. Specifically, the most useful thing for the client would be if you used values of the form:
```
CC1=eng;CC3=deu
``` This functionality should be working now in `dev-v2`. Please give it a try.

There's still one technical issue where events might go missing when seeking short distances and/or toggling the cea-608 tracks on and off, so the issue is kept open to track fixing that. It's unlikely to be much of an issue in practice, however. Marking as fixed. We'll track the remaining minor issue separately.  The file format can't be detected because the stream starts with an HTML fragment that looks like some kind of error from PHP. That stream starts with an ID3 tag, whereas the first one has what looks like an error message in an HTML fragment at the start.

The Mp3Extractor currently sniffs only as far as the (maximum) MPEG audio frame size before giving up. Is it possible that the server that provided the first file is misconfigured?  Could you please provide said stream to dev.exoplayer@gmail.com?  No e-mail was received. Are you sure you sent it? Make sure you include the issue number in the subject. I'd say "actually sent e-mail this time". I will have a look.

Edit: Nope, you had actually sent it. Since it didn't include the information requested information in the issue template it got archived. Thanks for the patience! Can you please provide a bugreport? Looks like device specific. That is the logcat output. What I need is a bugreport: [Look here.](http://stackoverflow.com/questions/26789107/how-to-write-an-android-bug-report-to-a-file-via-adb)
 Could you please clarify a bit what the issue is? I have been able to reproduce the stream in three different devices turning on random adaptation seemingly without issues. How noticeable is the frame drop and what does 
> the player lag

mean? I don't have access to the mentioned device. Have you been able to reproduce in another device? Also, considering 
> individual streams are working fine 

Have you made sure that, when playing with the default player, adaptation was happening? I noticed through the provided bug report that you don't seem to be using exoplayer but rather a wrapper. Have you tried reproducing in the exoplayer demo app? It would be good if you tried the last release version: 2.2.0 at the time of writing. Kindly provide the bugreport for this as well, please. Please read the issue threads #678, #488, and related. One of them explains how to blacklist decoders. Try blacklisting OMX.amlogic.avc.decoder.awesome, and see what happens. An alternative is checking what decoder MediaPlayer is using. It's likely that a software decoder is also available.
In any case, I would also suggest you follow up with the manufacturer of the device. I cannot reproduce the issue on any device I have, and I don't count with enough information to make an educated guess of what the problem is. 

Please do update if you find out additional information. The issue template provides a lot of useful information to speed things up. Closing due to lack of updates.  The sample code you've provided lacks the very piece you're asking for help on (i.e. any handling of audioSessionId), which makes it quite difficult for us to help. That means it hasn't been set yet. You can use `player.setAudioDebugListener` to listen for the ID being set (the ID will be passed via `onAudioSessionId` when it is).  There isn't enough information here for us to help. Please provide detailed reproduction steps with the ExoPlayer demo app if you believe this to be an issue. If the elevated memory issue occurs only in your own app, it's likely an application specific problem (e.g. because you're not properly releasing the player) and you'll need to debug why that is yourself.  This is a duplicate of #2025. The quickest way to fix this is adding a .vtt extension to each media segment. We will also provide a fix for this for Webvtt tracks soon. My best advice, if you can't modify the stream(by the way, this should be trivial for the producers), is to switch to Exoplayer V2. A fix will be available in the next push, as it's already available internally. If you can't wait until next push, in HlsMediaChunk (r2.1.1) you should change the 

`
    if (lastPathSegment.endsWith(WEBVTT_FILE_EXTENSION)
        || lastPathSegment.endsWith(VTT_FILE_EXTENSION)) {
`

condition in `buildExtractorByExtension` to 

`
if (MimeTypes.TEXT_VTT.equals(hlsUrl.format.sampleMimeType)
        || lastPathSegment.endsWith(WEBVTT_FILE_EXTENSION)
        || lastPathSegment.endsWith(VTT_FILE_EXTENSION))
`

This is for the last V2 available subversion. If you don't want to migrate to V2 you'll have to work out an equivalent solution, since we have not decided whether we are going to backport this. At this point, we only backport straightforward bugfixes. Unfortunately, this is not the case here. I will write back to let you know what the plan is, or close should we decide not to backport the fix.  Background/screen off playback has been discussed on this issue tracker several times before -- in future, please search for an answer in previous issues before filing a new one. In this case, I suggest reading #930, #1826, #2276 and #421.  See #2343. For HLS playback in V2 we always start playing out the first track in the master playlist, even if it's not present at all in the track selection. For example if the track selector has chosen a different track as a fixed track selection.

We should not start playing out the first track in the case that it's not present in the track selection at all. Aside from being confusing, it could cause failures (e.g. if the first track in the master playlist happens to be one that exceeds device capabilities). This is now fixed. Yes, it's still needed internally. Outside is sufficient to get the format you want. Internally we may request a fraction of a segment from some other format, determined by `InitializationTrackSelection`, but any media loaded as a result wont be played out.  There is a fix on the way which adds "-DHAVE_SYS_PARAM_H" to LOCAL_CFLAGS in extensions/flac/src/main/jni/Android.mk file.Until it arrives to github repository you can try to add it to your local copy manually.

@PaulWoitaschek, because of issues with FLAC format and the way android platform handles it we can't use native support. Right. Android doesn't currently expose a FLAC decoder through MediaCodec by default (individual OEMs may choose to add one). It instead extracts+decodes FLAC to PCM as a single step under MediaExtractor, which ExoPlayer doesn't use. It's the only format for which this unusual setup is used in the platform, as far as I know :(.

The "proper" fix would be for the platform to expose a FLAC decoder through MediaCodec in the same way as decoders are exposed for other format types. If anyone's sufficiently motivated, you could perhaps propose such a change via AOSP.  They should be being displayed. If you would kindly:

1. Play the stream in the demo app.
2. Make sure the text track is selected.
3. Make sure there are cues in the WEBVTT files.
4. See if the subtitles are displayed.

If subtitles are displayed, then you are forgetting something in your app. Most likely, the text track is not being selected.

If subtitles are not displayed, please send a link to your stream to dev.exoplayer@gmail.com and make sure it is not geo-restricted or invalid. The links you published give me 403 or 410. Thanks! Have a look at DemoApp for a precise guide on how to do it. It's not much code. [This medium post](https://medium.com/google-exoplayer/exoplayer-2-x-track-selection-2b62ff712cc9) might also shed some light on track selection. Unfortunately, the issue tracker is not a good place for stack exchanging.  This doesn't sound like an ExoPlayer issue (I didn't even know what MediaBrowserServiceCompat was until I looked it up). I suggest you ask this question on StackOverflow.  Yes, it is. Otherwise there's no way to track changes in the live window. If you really want the player to be torn down completely then you should stop (or release) the player and prepare it again when it's next required.  There is no utility function in ExoPlayer to do this at the moment I'm afraid. It might be possible to provide an alternative mode in ClippingMediaSource that will snap to the start point to a keyframe. That would likely be the most efficient way to do this, as then it won't be necessary to have a separate loading step to find the keyframe. The duration of the period would only be set in the Timeline after the keyframe position becomes known. (We probably won't get around to doing this for a while unless it's needed for something else.)

If you know what media will be played in advance you could construct a ClippingMediaSource with the right start time. There are lots of ways to find keyframes; I use a command like `ffprobe stream.mp4 -show_packets -print_format compact | grep -Po 'pts_time=\K([0-9.]+)(?=\|.*flags=K_$)' | head -c -1 | tr '\n' ','`.

  Duplicate of #1845  Please provide a full bug report. The issue template explicitly states that a snippet of logcat is not sufficient. As per the issue template, we require:

> A bug report taken from the device just after the issue occurs, attached as a
  file. A bug report can be captured using "adb bugreport". Output from "adb
  logcat" or a log snippet is not sufficient.

So please run "adb bugreport" shortly after reproducing the issue, and attach the resulting output to this issue (e.g. as a file). Thanks for the bug report. Does playback work if you simply comment out the `throw e` line [here](https://github.com/google/ExoPlayer/blob/r2.1.1/library/src/main/java/com/google/android/exoplayer2/mediacodec/MediaCodecUtil.java#L210)? I'm not really sure there's a better solution to this problem.

If this does allow playback to proceed, we can add a device + decoder specific workaround to suppress the failure. Please confirm either way. Closing for now, in that case.  > Is this the only/clearly preferred way of tuning the track selection (for HLS streams) or is there a way to "front load" the process? (With ExoPlayer 1, for example, we had our own HLSRendererBuilder where we could reverse the list of Variants before the Renderer was even built).

It's helpful to distinguish between synchronous and asynchronous track selection. By synchronous I mean the tracks are selected on the playback thread prior to media being buffered. By asynchronous I mean a message is passed to the application thread and a selection is then passed back to the playback thread. With asynchronous selection there's a small "gap" during which the wrong selection is being buffered, and so it's less efficient at the start of playback. For selection during playback (e.g. as a result of user interaction) asynchronous selection is fine.

Your `HlsRendererBuilder` approach was synchronous. The asynchronous API in V1 was `ExoPlayer.setSelectedTrack`. In V2 `setSelectionOverride` is asynchronous. The synchronous API is `TrackSelector.selectTracks`, which is invoked directly on the playback thread. So you should customize what happens when `TrackSelector.selectTracks` is invoked to recreate what you had previously. It's a really complicated API to implement, and I doubt you'd want to implement it directly. Instead, it's probably sufficient for you to extend `DefaultTrackSelector` and override `selectVideoTrack` (and possibly `selectAudioTrack`). Alternatively, if you only need to specify constraints like a maximum video dimension, you can use the built in parameter functionality of `DefaultTrackSelector`. Do this prior to playback as below. The constraints you specify will then be applied during selection.

```
trackSelector.setParameters(trackSelector.getParameters()
    .withXXX()
    .withYYY());
```

> Is ExoPlayer.EventListener.onTracksChanged() a good place to call setSelectionOverride()? I wasn't able to find any earlier place where I already had access to the tracks...

As above, it's preferable to use `setSelectionOverride` for changing tracks during playback only. Even during playback, it's still preferable to change tracks by replacing the `DefaultTrackSelector` parameters if this is sufficient for your needs.

> Is it correct/safe to call DefaultTrackSelector.getCurrentMappedTrackInfo() inside of onTracksChanged()?

Yes.

> And finally, I know that this has been discussed in other issues like #281 and more recently in #1848 but is there now a preferred way to force ExoPlayer to start with a particular track in an HLS stream?

It's easy to do this for DASH. I don't think it's possible to do this with HLS at this time. It's complicated in the HLS case by the fact we need to fetch a media chunk to determine what the tracks are. The factory (`adaptiveVideoTrackSelectionFactory`) passed to the `DefaultTrackSelector` constructor is really intended to build an adaptive track selection containing all of the tracks provided to it, not select only one of them. If you want a fixed track selection you should probably pass null as the argument, which will force `DefaultTrackSelector` to make the "best" fixed selection whilst respecting the parameter constraints that you've set. Currently "best" means "highest pixel count". We should probably look at bitrate if the video resolution is unknown. If the behaviour isn't what you're looking for then you could override `DefaultTrackSelector.selectFixedVideoTrack` to do something different.

Note 1: What you're doing will probably work fine, it's just not doing things in the way the API was intended to be used. It's also quite fragile to rely on the ordering of the tracks. I don't think we guarantee anywhere to preserve the ordering (although I can't think of a reason why we'd be likely to change it).

Note 2: If you're making a track selection that doesn't contain the first track at all, and still see the first track being played out before adapting to your selection, I'd consider that a bug. I've filed #2353 to track the final note above. We'll merge a change to use bitrate as a tie-breaker for fixed track selection early next week.  I don't really see why you'd want to do this. If it's just to make the UI appear differently (i.e. as though the content consists of multiple pieces of media with << and >> skip buttons) then it seems more appropriate to customise the UI, not fake a structure for the media that's not really true. As above, I think you should implement this kind of thing in your own UI above the player. The timeline represents the structure of the media being played. Faking the structure of the media just because the default UI components use it isn't the right solution. @skuppa Can you use a ConcatenatingMediaSource joining several ClippingMediaSources, where each of those has the required start/end position and wraps an ExtractorMediaSource with the required URI? Note that for seamless video playback you'll need to ensure that the start time of every ClippingMediaSource is a keyframe (see #1988). It's fine to load the same URI more than once in different sources.  The problem seems to be that the manifest still contains the representations which include the deleted videos. When bit-rate switches, it may instruct the player to load a deleted video. I guess that is why the   **onLoadError** happens. 
Maybe the fast way to test is manually remove the representations related to the deleted videos from MPD and see if it works.
 Officially supporting offline mode is tracked by https://github.com/google/ExoPlayer/issues/2643.  Ping @vigneshvg ; Thanks. Yep, little swamped at the moment. This is in my radar. Will look into it shortly. It looks like something changed between versions 1.6.0 and 1.6.1 of libvpx. A quick fix for this would be to change the instructions to explicitly check-out v1.6.0 (we should do this anyway because the instructions as it is checks out tip-of-tree master and the generate_configs script could easily break).

I will update the instructions to do that.  This isn't supported, so you'll need to keep references to the instances so that you can pass them to the remove method. Having a method to clear all listeners may end up being more confusing than it is helpful, since components like `SimpleExoPlayerView` add themselves as listeners to the player, and it's quite likely that a developer calling such a method wont realize they're detaching the view from the player by doing so.  > Hmm, having written that...maybe the answer is a lot simpler. When I change adaptation sets could I be selective and only put one track index in the tracks array passed to 'MappingTrackSelector.SelectionOverride' ...then later on, when playback has started, add the other higher bitrate tracks? Or would this then cause a re-buffering?

Yes, it would cause re-buffering.

> Is there a way to say 'start buffering the lowest bitrated representation, rather than the level currently being enjoyed?'

There's no trivial way to enable this, but you can do what you want with a bit of work:

- If you look at what the demo app does when it creates its `DefaultTrackSelector` instance, it passes an `AdaptiveVideoTrackSelection.Factory` as an argument to the constructor. Once the set of tracks has been chosen by the selector, it invokes this factory to build a `TrackSelection` containing the individual selected tracks.
- The `TrackSelection` built by the factory is what controls the adaptation between the individual selected tracks. `AdaptiveVideoTrackSelection.Factory` creates instances of `AdaptiveVideoTrackSelection`.
- You can provide your own factory that provides your own implementation of `TrackSelection`. You'd probably want to fork `AdaptiveVideoTrackSelection` into your own codebase and then adapt it to meet your needs. In particular, you can force the selected track to the lowest quality in your forked implementation until `bufferedDurationUs` first exceeds some (presumably relatively small) value, or something along those lines. Hi @SteUK :
How about your test result?
I gave it a try as per @ojw28 ’s kind suggestion.

Please refer to the patch named “Implement-Slow-start-to-accelerate-adaptationset-switch.zip”; where I tried to customize AdaptiveVideoTrackSelection accordingly.
[Implement-Slow-start-to-accelerate-adaptationset-switch.zip](https://github.com/google/ExoPlayer/files/762958/Implement-Slow-start-to-accelerate-adaptationset-switch.zip)

The name "slow start" comes from GStreamer since it always starts from the representation with lowest bitrate when creating a new stream.

There are 2 considerations when I created it.
1.	A function named getLowestSelectedIndex() is used to select (start from) the representation with lowest bitrate when a new instance of  SlowStartAdaptiveVideoTrackSelection is created. Since slowStartAdaptiveVideoTrackSelection is created only when we have a new setSelectionOverride, it insures a faster transition between adaptationsets.
2.	Since the transition roughly includes 2 stages: 
A: render the first frame.
B: have sufficient buffer.
A value named "slowStartInitializationPeriod" is set to equal to 
“public static final int DEFAULT_BUFFER_FOR_PLAYBACK_MS = 2500” of DefaultLoadControl.java. It ensures us to keep on downloading the representation with lowest bitrate until we have had sufficient buffering data to drive playback starts.

The patch below is the way I launch the new trackselection.
[laucher.zip](https://github.com/google/ExoPlayer/files/763023/laucher.zip)
 I tested it by the steps below:
1.	Install http://www.rejetto.com/hfs/ on my laptop.
2.	Connect the mobile phone and the server (my laptop) to the same WIFI.
3.	Rewrite the test MPD to simulate 2 video adaptation sets(as the multi-view case you described).
4.	Switch the video tracks and record how fast the transition is; compare the effects of original AdaptiveVideoTrackSelection with the new SlowStartAdaptiveVideoTrackSelection

Please refer to the modified MPD below for testing also.
[car-20120827-manifest_dual_video_adaptationset.zip](https://github.com/google/ExoPlayer/files/762947/car-20120827-manifest_dual_video_adaptationset.zip)

The result shows the in average under connection status of network speed is about **17.71 Mbps**. It seems to make the transition faster but is still not seamless.
In conclusion, it makes **26.3%** speedup to render the first frame & **87.35%** to start playback (after having sufficient buffer)
![test_result_0209](https://cloud.githubusercontent.com/assets/14846473/22775003/aa45946a-eee4-11e6-80c1-e895b3ecbf21.png)

What’s your next step for accelerating it? 
It is an interesting problem. Once you have made a scheme to improve it, I will be glad if you could share it.

Thanks.

  This should be fixed on `dev-v2` as a side effect of https://github.com/google/ExoPlayer/commit/7f967f305718bc2c9ee679fdd7d014eccef0356b. There will be one further cleanup change, but that wont affect functionality.  @replystreamingwidevine please provide full bugreport, logcat, and a link to working and non working content.

thanks, @replystreamingwidevine Please send it to dev.exoplayer@gmail.com. @replystreamingwidevine, thank you for the logcat but can you please provide the ADB Bugreport. That would have more data. 
thanks. @replystreamingwidevine I have reviewed a bugreport with 'Dumpstate : 2017-01-31 11:40:01" heading. If this is indeed a Bugreport that you sent in email, I do not think it captures the behavior for this issue, I am not finding and CTR decryption errors.

 Oh i see, apologies there is a bit of a file shuffle on this side.
Let me go review that log.  I noted that you have opened a case with the WV CWIP Team.  
We should continue the troubleshooting there.
I will post a follow up with findings. This has been assigned for followup with the device manufacture. No updates as of 2/20/17. Sadly no new updates from OEM, there are issues with reproducing the issue on their side.  This is working as intended. As you mention, you are using a LoopingMediaSource, so the player doesn't end because the media goes on "_forever_".

Could you please clarify:
>  if i remove " exoplayerview.setPlayer(exoplayer);" only audio playing not repeating

If you remove that line the player actually reaches ENDED state? Or it doesn't but the audio doesn't loop, as it would with the line.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  It would be helpful if you could include all of the information requested in the issue template. For example a proper bug report and (more importantly in this case) the specific version of ExoPlayer that this reproduces with. For the final paragraph: If you provide complete information and can demonstrate that particular issues affect more than a single media file, then we'll investigate and put fixes in place. Without the information requested in the issue template we're not able to do this. I can reproduce the problem. It's not helpful that both audio files include a text track that's approximately (not exactly) twice as long as the audio. This is why the content appears to have a duration of around 45 minutes when the actual audio is only about 22 minutes long. VLC also ends up with the duration being way too long for the same reason. This is also the reason why the player gets confused and stuck in the buffering state.

The actual stack trace that follows when you manually skip to the next entry in the playlist is a bug, which we will use this issue to track. We'll push a fix for the stack trace shortly, which will allow the user to advance to the next item without failure. We'll also push a fix for an issue that prevents backward seeking within the same item once in the "stuck buffering" state.

The stuck buffering state itself is due to the media containing an incorrect duration for the text track that's for some reason present. Not getting stuck in this case is tracked by #1874, but when that issue is fixed the behavior will be to play out about 20 minutes of silence up to the stated duration of the media. This is the technically correct thing to do. The only way to avoid this is to fix the media itself. The failure is fixed in `dev-v2`. #1874 tracks not getting stuck in the buffering case, as described in more detail above.

> Any quick workaround like suggested by you in previous comment of always allowing media to play for stated duration can be implemented?

Either way, a user isn't going to want to sit around for 20 minutes waiting for the next item to start. Fixing #1874 is quite technically difficult; it's not a quick workaround. We'll follow up on that issue in due course, but it's not going to be a quick thing.  @vigneshvg - Please comment again. Thanks. @supercairos - Are you planning on updating this change?  I'm a little unclear what you're after here. If you're passing the uris into the player, don't you already know what they are? You can query the index of the media currently being played using [ExoPlayer.getCurrentWindowIndex](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ExoPlayer.html#getCurrentWindowIndex--). Is that sufficient to resolve your question? Thanks! Yes, keeping the array yourself is the best way to do this.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> @andrewlewis PTAL. Thanks. Looks good, thanks!  All looks good. See one inline comment. Once addressed we'll get this merged.  Please see #26.  This is explained [here](https://google.github.io/ExoPlayer/supported-devices.html). That's correct, yes. It's your decision of course, but is it worth the extra work to target the ~2% (and decreasing) fraction of Android devices that are still running < 16? See the distribution of Android devices [here](https://developer.android.com/about/dashboards/index.html).  See #1782. Please pay attention to the issue template next time.  This is not something we currently support. @dcower - Is that correct? What's the metadata generally used for? Would it be the audio renderer that needs it, if anyone, or is there value to exposing it all the way out to the user of the player? @ojw28 That's correct -- this is not currently supported in ExoPlayer.

The metadata is used to signal the presence of spatial (ambisonic) audio in 360 media. AFAIK it's only used to signal that for upload to YouTube/Facebook/etc.; i.e., it's used as an ingest/upload format.

There are a few options I see here:
1) Don't support it. Just use the channel count to determine if the media has spatial audio (not ideal, but works pretty well if you only care about 360 videos); I think this allows instantiating a different audio renderer, similar to 3) below.
2) Support it and plumb metadata to the audio renderer only. This requires the audio renderer implementation to have some kind of fallback handling logic if you want to also support non-spatial audio, I think.
3) Support it and plumb metadata to the user of the player. This allows the player to instantiate a different audio renderer depending on presence of spatial audio. It sounds like this metadata is used for ingest/upload, and so I think option (1) (not supporting it) sounds fine. Closing.  We'll take a look at this if you provide us with an easy way to reproduce. Please can you provide the exact changes you've made to the demo app either as a pull request, or as a change in a fork you've made of the project? That way we can see exactly what you've done and reproduce without any guesswork. Thanks!  You can query this information from the player's timeline. The demo application does this [here](https://github.com/google/ExoPlayer/blob/r2.1.1/demo/src/main/java/com/google/android/exoplayer2/demo/PlayerActivity.java#L430). Static means VOD. Dynamic means Live.

We should probably add an `ExoPlayer.isDynamic` method for convenience, so you don't have to query the timeline directly. I'll do this.  Looks like a memory leak. I'll have a look. By the way, next time please provide all the information required in the issue template. In this case, I am interested in the exoplayer version you are using. Have you reproduced the issue in the DemoApp? If so, would you kindly send a link to the stream to dev.exoplayer@gmail.com? Thanks, I'll wait. If you manage to reproduce, please send the stream to the provided e-mail address.  Can you please send a link that reproduces the issue to dev.exoplayer@gmail.com? Don't forget to include the issue number in the subject. After 5+ hours of playback, the heap hasn't gone over 50MB for most of my streams. So I will need a stream that reproduces the issue, or I will have to close the issue due to lack of information.  After playing a encryption-clear stream for a day, I noticed the heap size was 80MB. I suspect using encrypted streams increases the leakage size. Good news is I have already found a cause for the issue for which I will soon push a fix. After that, please try reproducing again and let me know the outcome.  We cannot investigate unless you provide the information requested in the issue template. You can change the resize mode of `SimpleExoPlayerView` (see [here](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ui/SimpleExoPlayerView.html#setResizeMode-int-)). For example:
```
simpleExoPlayerView.setResizeMode(AspectRatioFrameLayout.RESIZE_MODE_FILL);
```
You can do the same in your layout file too, using:
```
app:resize_mode="fill"
```
See [here](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a#.1jf453366) for more details.

Note that fill mode will distort the aspect ratio of the video, so it's unclear to me whether that's really what you want. If the video is at a very wide aspect ratio the stretching will be significant. Are you sure you don't want to keep the video in its correct aspect ratio, which is the default, and just set a black background to add black letter-boxing (rather than green and white)?  It should be pretty easy to implement this yourself, if you (or anyone else) feels like sending us a pull request ;)... Yeah, I was just looking. Looks good. I've added a few comments. Once addressed we'll get it merged. The change has been merged. I made some further changes in https://github.com/google/ExoPlayer/commit/5aff31c0615dd05cabb4a92b17d658aeff742da7 and https://github.com/google/ExoPlayer/commit/48099ee52eeaa871ddc9c65d2898790b8e35a728. These changes are completely untested as I don't have any media containing ID3 chapter information, so it's highly likely I've broken something. Please could you take a look, test and fix any issues I've introduced. If you could also provide some test content for us, that would be great. You can either attach it here, or email it to dev.exoplayer@gmail.com.

Thanks! Obvious bugs fixed in https://github.com/google/ExoPlayer/commit/c828d9b0bf13e430b442a4a0bf801fe7b6e808b5. It would be cool to add support for MP4/M4A chapter metadata too, if you fancy giving it a go. It's probably just a matter of adding support in `extractor.mp4.MetadataUtil` to parse out the ID3 frames. I found some test media here: https://auphonic.com/blog/2013/07/03/chapter-marks-and-enhanced-podcasts/ @rustyshelf - I think you're correct that MP4 files don't use ID3 tags, however the approach we've taken for MP4 metadata has been to convert it into the equivalent ID3 tags as far as is possible. It usually is possible since the metadata atoms in MP4 are in most cases equivalent to ID3 tags. See `extractor.mp4.MetadataUtil`. For chapter metadata, my guess is that the `chpl` atom is used, as defined in [this spec](https://www.adobe.com/content/dam/Adobe/en/devnet/flv/pdfs/video_file_format_spec_v10.pdf). It looks like such an atom could probably be converted into a `ChapterTOCFrame` and N `ChapterFrame`s. Each `ChapterFrame` would have a single `TIT2` sub-frame holding the title.

@geekygecko - No worries! Thanks for the contribution. Please feel free to add additional tests via a new pull request. Sub-frames are indeed not accessible currently, heh! I intended for them to be private and for there to be public `getSubframeCount` and `getSubframe(index)` methods (since just making the array public directly doesn't enforce immutability), but somehow forgot to actually add those methods. I'll fix that shortly. Thanks! That would be great, thanks! Interesting. We currently just expose the chapter data as a text track for that case. Which we probably shouldn't be doing! We're planning a release for some time this week. Closing this as fixed. The MP4/M4A change looks like it could be quite difficult. I propose we file a separate enhancement request for that as and when someone actually wants it, as was also proposed by @rustyshelf further up this issue.  The information provided here isn't detailed enough for us to help. Please provide all of the information requested in our issue template (including a full bug report captured with "adb bugreport"). Thanks.  Is it equivalent to do drawable-anydpi-v21? It might be clearer just because it still keeps the API level in the directory name. Thoughts? Thanks. We'll merge this shortly.  I don't think session sharing is the correct thing to be using. We recently pushed a helper class to make this easier to our dev branch, [here](https://github.com/google/ExoPlayer/blob/dev-v2/library/src/main/java/com/google/android/exoplayer2/drm/OfflineLicenseHelper.java). To acquire an offline license:

1. Create an instance of helper.
1. Call download on the helper (from a non-UI thread, since it will perform some networking). This will acquire an offline license. The return value is a keySetId, which you need to persist for use when performing the offline playback.

You can also use the helper to renew or release an offline license that has been acquired.

To play the content using an acquired offline license:

1. Create a DefaultDrmSessionManager (this has been renamed from StreamingDrmSessionManager because it now supports the offline use case too).
1. Call setMode(MODE_PLAYBACK, keySetId), passing the keySetId for the offline license.
1. Build the player as normal. It should use the offline license for playback. Contributions should be made in the form of pull requests (see [here](https://github.com/google/ExoPlayer/blob/release-v2/CONTRIBUTING.md)). It's unclear exactly what you want to contribute though, since I suspect the changes you made were in your application code rather than in the ExoPlayer library. The dev branch was migrated to trunk a couple of days ago.

I don't think we really recommend pre-fetching of licenses for streaming. Lots of streaming services have restrictions on simultaneous streaming sessions, so if the user has multiple devices pre-fetching has a tendency to incorrectly stop playbacks on other devices. Maybe that doesn't affect your case, but it's an example of the kind of things that can go wrong when you start doing this. If you license server is fast, it's unclear how large the benefit of pre-fetching licenses is. I'm sure it's possible to do, but it's not something we're going to be spending time providing guidance for at this point in time.  Without any reproduction steps (e.g. a link to sample content that fails) we're not able to investigate the issue. Please file an issue with clear reproduction steps and test content if you want us to take a look.  I'm not sure that's exactly what Akamai mean ("derived from" != "equal to"). What you've done sounds sensible (it seems preferable to keep doing that to ensure you don't run into similar issues with other players). We'll change the logic in ExoPlayer to handle this case also. Thanks.  Please don't spam the issue tracker. This is a duplicate of #26.  We most likely don't handle ID3 comment frames that contain no text properly. If you could send us the problematic file we'll take a look and push a fix. You can email the file to `dev.exoplayer@gmail.com`. - The SCNDL sample fails because we don't handle ID3 comment frames that contain no text properly.
- The other file appears to have a reasonably large chunk of data that doesn't look like MP3 start about 1MB into the file. Any idea what that is or where it came from? Have you tried running this file through an MP3 validator tool? We'll push a fix to correctly handle the SCNDL file to the `dev-v2` branch shortly. You could probably adjust the media to play without the fix by removing or editing the ID3 metadata. As per my comments above, there's an empty comment frame in the ID3 metadata. An ID3 editor isn't necessarily the best way to see exactly what data has been written into the file.

I tried using tagmp3.net to make the "Comment" field non-empty and was then able to play successfully. We'll push a fix to correctly handle empty comment frames in a sec, in any case.  Duplicate of #1845.  The fact your app ran out or memory whilst using ExoPlayer doesn't in itself indicate an ExoPlayer issue. It's the sum of all memory used by your app that results in the OOM; the exact point at which the OOM occurs can be irrelevant, and your app is equally likely leaking memory elsewhere.

12-15 also sounds like. . .barely anything, although without a baseline (e.g. number of playbacks / installs) it's impossible to say. . . Closing because there's no evidence this is really an ExoPlayer leak.  There's nothing we can do to investigate this unless you provide the information requested in the issue template (specifically in this case, a sample stream that reproduces the issue). We don't rely on ID3 metadata for the duration; we read or derive it from the actual media stream. I'm struggling to work out what your issue is. The player indicates the duration correctly for me, and it also corrects reports the TLEN ID3 tag. When playing the sample in the demo app I see the following logged:

```
01-09 10:33:03.660  3092  3092 D EventLogger: audioDisabled [42.76]
01-09 10:33:05.549  3092  3092 D EventLogger: state [0.00, true, I]
01-09 10:33:05.642  3092  3092 D EventLogger: state [0.09, true, B]
01-09 10:33:05.642  3092  3092 D EventLogger: sourceInfo [periodCount=1, windowCount=1
01-09 10:33:05.642  3092  3092 D EventLogger:   period [?]
01-09 10:33:05.642  3092  3092 D EventLogger:   window [?, false, false]
01-09 10:33:05.642  3092  3092 D EventLogger: ]
01-09 10:33:05.642  3092  3092 D EventLogger: loading [true]
01-09 10:33:05.694  3092  3092 D EventLogger: sourceInfo [periodCount=1, windowCount=1
01-09 10:33:05.694  3092  3092 D EventLogger:   period [1.17]
01-09 10:33:05.694  3092  3092 D EventLogger:   window [1.17, true, false]
01-09 10:33:05.694  3092  3092 D EventLogger: ]
01-09 10:33:05.695  3092  3092 D EventLogger: Tracks [
01-09 10:33:05.695  3092  3092 D EventLogger:   Renderer:1 [
01-09 10:33:05.695  3092  3092 D EventLogger:     Group:0, adaptive_supported=N/A [
01-09 10:33:05.695  3092  3092 D EventLogger:       [X] Track:0, id=null, mimeType=audio/mpeg, channels=1, sample_rate=24000, supported=YES
01-09 10:33:05.695  3092  3092 D EventLogger:     ]
01-09 10:33:05.695  3092  3092 D EventLogger:     Metadata [
01-09 10:33:05.695  3092  3092 D EventLogger:       TSSE: description=LAME 32bits version 3.99.5 (http://lame.sf.net)
01-09 10:33:05.695  3092  3092 D EventLogger:       TLEN: description=1389
01-09 10:33:05.695  3092  3092 D EventLogger:     ]
01-09 10:33:05.695  3092  3092 D EventLogger:   ]
01-09 10:33:05.695  3092  3092 D EventLogger: ]
```

Where `1.17` is the duration derived from the actual media stream and `1389` is the duration in the ID3 metadata. You haven't really explained what the problem is, since playback works fine. What's the actual issue you're encountering as a result of the duration values disagreeing by ~200ms?

If you need precise duration you should add an appropriate header to the MP3 (e.g. a VBRI header) or use a more appropriate container format (e.g. MP4). I don't think that would happen unless both (a) you're streaming and your server isn't including a content length header in the response, and (b) the mp3 doesn't include an appropriate header (e.g. a VBRI header) indicating duration. You can fix either issue to get duration reported correctly. Adding ID3 metadata isn't relevant to this problem.  This is a duplicate of https://github.com/google/ExoPlayer/issues/2093.  From what I understand, you want to avoid adapting into a specific subset of the available representations. Have you looked into the TrackSelector hierarchy? This is one of the things you can do with it. I'd also suggest trying stack overflow or a similar Q&A.  It sounds like the apk is fine, given it's working when installed normally. Which would indicate that it's not a proguard issue. It's most likely that the native library just isn't being loaded successfully. If I were you, I'd start digging [here](http://stackoverflow.com/questions/20906445/unable-to-load-so-library-files-when-making-a-system-application), which looks like a similar issue. If you do a Google search for "android system app native library" there are a whole bunch of results that look related, also :).  I doubt we'll be gracefully handling the transition to play the MPEG-TS stream any time soon, but we should figure out the data isn't an HLS playlist and bail out with an error, rather than consuming the stream until we OOM. It looks like it would be straightforward to include the resolved URL in the exception. The exception type will be some kind of "unexpected content" exception. Note that it wont tell you *what* the content is, only that it's *not* an HLS playlist. That should be sufficient though. You can try `ExtractorMediaSource` with the resolved URL. If you get an `UnrecognizedInputFormatException` when you try that then something else has happened (e.g. you've probably hit a paywall rather than media content).  @andrewlewis should be able to provide some high level advice on syncing ExoPlayer instances across devices, although it's not something we support directly and so beyond that is something you'll have to figure out for yourself.

Regarding the content itself:
- I wouldn't class 10Mbit/s as low bit-rate.
- It's slightly unclear whether you're playing MPEG-TS/UDP or an MP4. You'd probably want to be using HTTP + a seekable media format like MP4 or MKV to do this kind of thing. There is no need to make every frame a key-frame if you do this. I'm not sure I have enough information about what sort of streams are being played to address this properly, but maybe this will be useful anyway:

> Resetting ExoPlayer causes a delay and creates a freeze frame of course until it starts again.

I assume you're not trying to play audio. Here are some (untested) suggestions on what to try: you probably want to make sure the players buffer for as little time as possible, so you could try setting the player's buffer size to zero ([minBufferMs/minRebufferMs](https://github.com/google/ExoPlayer/blob/release-v1/library/src/main/java/com/google/android/exoplayer/ExoPlayer.java#L128)). With this change, if a player buffers it will return to the READY state more quickly. To get perfect synchronization across devices you will also need to make the player media clocks have the same position. You can do that by synchronizing the devices' clocks accurately (e.g. using NTP) or equivalently periodically calculating the offset from the correct time in your own code. Then use this time to provide a MediaClock to ExoPlayer by making your video renderer implement MediaClock, providing a position that is consistent with the synchronized clock time. Then every device will have almost exactly the same player position so should show frames at the same time. You will also need some way to make sure the clock position and sample timestamps are consistent (e.g. loop the input stream and if the clock position modulo the input stream duration is 'too far' from the sample timestamp do a seek -- though note that this only works if the input needs to be looped and is seekable which may not apply for your use case).

> Resetting a player (meaning releasing the old one) sometimes causes an ANR.

This sounds like an unrelated issue. If you want us to investigate this, please file a new issue including all the information in the issue template, especially a bug report.

> Quality of the stream is lacking mostly because of the "every frame is a keyframe" restriction.

By making every frame a keyframe, you prevent the video compression algorithm from taking advantage of temporal redundancy, so the quality will be much lower at the same bitrate. You can increase the bitrate to deal with this, though I agree with Olly that 10 Mbit/s sounds quite high already for playback on a mobile device. Alternatives include increasing the keyframe interval slightly (so artifacts from dropped and out-of-order packets are visible for a short time), or using periodic intra refresh, which refreshes the whole picture gradually. Glad to hear you were able to get the devices synchronized.

Regarding the bitrate: 10 Mbit/s sounds like a relatively high bitrate for a mobile application, but if this is a kiosk-style use case where you can choose the device(s) and test them then it probably doesn't matter. If you increase the bitrate to get better quality, playback as a whole will gradually get more computationally expensive (especially with MPEG-TS) but you can just pick a value that works!  This doesn't look like it's an ExoPlayer question.  I'm not certain where you're trying to merge this eventually, but this is a stale branch. Got it; sorry about that. For this particular scenario send a pull request directly to the release branch.  The stream doesn't play in VLC either, giving the following output:

```
[0x1e26118] main libvlc: Running vlc with the default interface. Use 'cvlc' to use vlc without interface.
[0x7f8450c043a8] httplive stream: HTTP Live Streaming (radios-ec.cdn.nedmedia.io/radios/ec-alfa.m3u8)
[0x7f8450004088] ts demux error: MPEG-4 descriptor not found
[0x7f8450e1eb88] packetizer_mpeg4audio packetizer: AAC channels: 2 samplerate: 22050
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4097
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4097
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4097
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7f8450004088] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4097
```

I'm guessing the stream is faulty. It does play in Safari, so the audio is definitely in there somewhere. It's possible Safari is working around some issue with the stream. @AquilesCanta - Fancy digging further? And since VLC also fails (and also plays nearly all other streams successfully). The stream is not valid: The chunks(at least some of them, which I peeked) declare an H264 elementary stream but include no associated packets, including only the audio.  The content you're trying to play is buggy. It was generated at a time when we (and everyone else) only had draft versions of the DASH and MSE specs. I've been asking for it to be deleted, since it really doesn't serve as reference media at this point.

Closing since this is a content issue.  The stream doesn't contain ID3 metadata. There's ICY metadata in the response headers, but this is not something we support. We've no plans to support ICY metadata, sorry. It's probably not that hard for you to make a small extension to ExoPlayer that adds support, if you're sufficiently motivated to do so ;).  The content is broken in the same way as described in https://github.com/google/ExoPlayer/issues/2101 and https://github.com/google/ExoPlayer/issues/1152. You should report the issue to the content producer, and ask them to fix it.

Having said that, we've seen this reported a few times now. We'll likely add a workaround to allow playback to proceed (you will still see a warning message logged every few seconds).  It can, but you need to enable it yourself when creating the data source factory. Are you using `DefaultDataSourceFactory`? If so you'll need to:

1. Use the constructor that takes another `DataSource.Factory` as the third argument.
1. Pass a `DefaultHttpDataSourceFactory` as that third argument. When you create your instance of `DefaultHttpDataSourceFactory`, set `allowCrossProtocolRedirects` to true.

If you know usage will be HTTP only you can just use a `DefaultHttpDataSourceFactory` directly. Although I don't think the issue you're seeing is due to a cross protocol redirect. It looks to me more likely that the server just isn't configured correctly, as per [this article](https://developer.android.com/training/articles/security-ssl.html#CommonProblems).  This issue only occurs in a fairly niche set of circumstances. There definitely need to be multiple changes in the sets of renderers that are required throughout the playlist. We'll merge a fix sometime this week. I *think* the change above fixes this, but please give `dev-v2` a try and confirm. Thanks!  It's quite arbitrary to assume time=0 corresponds to when the user happened to join the playback. What are you actually trying to do? If you're just trying to show an increasing time in your UI and want it to start at 0, then (a) why is this useful, and (b) why don't you just take the first position queried after playback starts, and subtract that value from each position before displaying it? It doesn't sound like a great setup to be serving archived content in the guise of live streams in the first place. That aside, I'd suggest you go with (b).  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok -->  There are lots of existing issues about this. Please search the issue tracker. See for example https://github.com/google/ExoPlayer/issues/1216. Thanks!  SimpleExoPlayerView registers itself as the video listener, so when you set it you're replacing SimpleExoPlayerView as the listener. Hence SimpleExoPlayerView doesn't receive the events that it needs.

Why do you need to register a video listener when also using SimpleExoPlayerView? You should use `setVideoDebugListener` to gather QOS statistics, and let the view use `setVideoListener`.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> Wont this crash applications running below API level 21 unless (a) they're using the support library and have configured their gradle file appropriately, or (b) they're using Android Studio? I don't think we want to place these kind of limitations on how the library can be used.

Can we add the vector drawables under `drawable-v21`, but also retain the existing png drawables for backward compatibility? Thanks!  Everything that's *not* final or private is effectively part of ExoPlayer's API, since it can then be overridden. Making everything non-final would hugely increase an already very large API surface area. This is problematic because (a) it adds things to the API that we really haven't given enough thought to, and (b) gives developers many different ways of doing the same thing rather than more strongly pushing them toward what we think is the best way of doing something.

We had problems in the past where application developers would implement features by overriding random methods that were then removed as a result of things like internal code restructuring. In such cases the developer is then stuck being unable to upgrade or having to re-implement their features using more appropriate APIs that already existed for what they were trying to achieve. So I think marking things as final and exerting some level of control over this kind of scenario is a good idea.

If you just want to do some local debugging, why not edit the files directly? I'm not exactly sure I follow this. Isn't the order of the renderers important? Can't you just swap them around to prefer one or the other? Providing a different TrackSelector implementation is also one possible solution to this problem. I'm not sure I follow. I think you should put the MediaCodec based renderer first and always prefer passthrough if available. What issue are you facing when doing that? I'm still not sure I follow this. Isn't a track selection renderer specific already? I'm not sure it makes sense to say you set a track selection and then after that a renderer was selected? I'm planning to close this next week unless further clarification is provided.  As per title, we should make it easier for applications to make use of ExoPlayer (for local playback) and the cast SDK (for remote playback) together. In particular, we should make it easier to attach ExoPlayer's UI components (e.g. PlaybackControlView/SimpleExoPlayerView) to a remote playback during casting.  We don't have plans to support FF/RW, no. We may decided to implement it for HLS/DASH specifically in the case that trick play streams are provided in the playlist/manifest, but we don't have any immediate plans.  This will be much easier to review and merge as a sequence of smaller pull requests. Could we start with just the extractor change (and corresponding tests / test assets)? That looks pretty easy to get merged as a first step.

I posted a comment about the decoder part that probably requires some work. It's fine to send the decoder pieces a single pull request, although it might be simpler if you send two; one adding basic support for plain text SSA, and the second adding in all the styling stuff. Up to you! Thanks. > Having slept on it I realise that if I put a into the text the decoder doesn't need to keep the styles. Is that the sort of thing you have in mind?

I'm not sure what this means; I think there might be some words missing from your sentence :)?

> Should separate lines of text in a subtitle go into separate cues?

Not unless they're positioned differently. If it's just a line break then it should be a single cue that contains a line break. This change is more likely to be merged if it's split into two pieces. One that adds basic support without styling in an obviously correct way, and a second that adds styling. I still don't think the styling stuff in this change is in line with what we do elsewhere, and I think it'll make things much easier if we can merge the non-controversial bit and discuss that separately.  See https://github.com/google/ExoPlayer/issues/2166. There is nothing we can do to help given such a vague statement, sorry.  It's definitely possible. There's no requirement that you use any of the UI components provided by the ExoPlayer library, which means you can be in complete control of the UI if you prefer. Are you referring specifically to the ability to display an image instead of the video in `SimpleExoPlayerView`? If so then will https://github.com/google/ExoPlayer/pull/2273 do what you need? Note that you'll need to implement the "cast bit" (i.e. using the cast SDK to start the remote playback, and stopping the local playback) yourself. We're likely to merge #2273 into the dev branch shortly. 

To answer your question: There's no built-in support until that change is merged, but that doesn't mean there's no way of doing what you're trying to do. As per my previous response, you can implement your own player view and put whatever views and logic you want into it (this is what YouTube do). You can then use your view instead of `SimpleExoPlayerView`. A simpler approach might be to override `exo_simple_player_view.xml` to include an extra view for containing the image. This [blog post](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a#.756b2q9j4) may be helpful. The normal model for casting is that you send the media URL to the receiver application on the remote device and it streams the content directly. Hence the player on the mobile device should be completely stopped, and so you wouldn't need to mute the audio. You'd need to hook the player controls up to the cast SDK to have them control the remote device playback (which is something we'd like to make easier in the future, but for now is something you need to do yourself).

I don't know if the cast SDK also supports casting of a Surface or something. If it does and if that's the model you're using, then yes I guess you'd set the audio volume to 0 when in this mode. You'll need to work out how to manually hook up the playback control UI to the cast SDK to do that. When you're casting in the normal model there is no local playback on the device, so the playback controls aren't just going to work.

I've filed https://github.com/google/ExoPlayer/issues/2283 to track supporting this more easily/directly. Note that we've merged #2283 into the dev branch also.  It's deliberately not focusable. This is because the FF/RW buttons above it are focusable and can be used to seek more easily. TV devices often have controllers with physical FF/RW buttons, and these can also be used.

That said, we should definitely make sure it works correctly if it is made focusable, which isn't currently the case. We may also consider making it focusable by default if this makes seeking in long duration content easier (I'm not sure whether it does or not).  The view is made up of multiple elements, so it's not really clear from your description what you're trying to do. What kind of animation are you trying to add, and which parts of the view do you expect it to apply to?

If you're attempting to apply a simple fade in/out to all of the playback controls, then I can see that as something we should reasonably support. If you're trying to do something more complicated than that, you'll likely need to implement your own view.

Please clarify; thanks! It will need a source change.  Using MediaPlayer without a foreground service is not advisable and will not work properly in certain circumstances. In particular, if the user launches another application and the OS decides to kill a process to free up resources (e.g. memory) for it, there's nothing stopping the OS from killing your process. A foreground services *is* the mechanism for applications to indicate to the OS that they're doing something important whilst not necessarily being in the foreground, and hence that they should not be killed. You really should be using one.

MediaPlayer and ExoPlayer are the same in that neither do anything "special" with respect to background playback. Either can be used to play audio in the background. In both cases a foreground service is required to ensure correct behavior.  There's not a lot we can do to help here unless we're provided with the information requested in the issue template. What would help is if you included the information requested in the issue template. Else we will close this issue as we have no way to reproduce it.  Closing because none of the information requested in the issue template was provided.  Please update the class level javadoc of SimpleExoPlayerView to include information about the new attribute (around line 67/68). Otherwise this looks great, so we'll get this merged once that's done. Thanks! Looks good, thanks!  The duplicate `onPositionDiscontinuity` calls are already fixed in the `dev-v2` branch by https://github.com/google/ExoPlayer/commit/4bb8793203ff280189d15849669304ebb138d814.

I cannot reproduce the other cases you mention. In the demo app `onPlayerStateChanged` events are logged to logcat with the tag EventLogger. I only see a single transition to `STATE_ENDED`, and no duplicate when subsequently seeking back to 0. I suspect you're changing `playWhenReady`, which is a state change and will cause `onPlayerStateChanged` to be invoked.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok -->  No. ExoPlayer does support extraction of AMR tracks in ISO BMFF containers, though.  I think there's a one line bug (we're forgetting to mask the value [here](https://github.com/google/ExoPlayer/blob/r2.1.1/library/src/main/java/com/google/android/exoplayer2/trackselection/MappingTrackSelector.java#L352)) that causes this. We'll push a fix shortly. Thanks! I doubt it, particularly if you're seeing it work successfully with more channels. I suggest you report the issue directly to nVidia.  The Demo App in Library is better than ExoPlayer guide page We'll update the guide to fix this, thanks! I've fixed the first issue raised. The second appears to work fine for me. It's possible "this" isn't a `Context` in your code, in which case you'll need to tweak it slightly. In any case, it's trivial to look up what the constructor arguments should be in the [documentation](http://google.github.io/ExoPlayer/doc/reference/).  This was an intentional change to avoid having negative buffer timestamps when prerolling media for transitions into clipped periods (see [ClippingMediaSource](https://github.com/google/ExoPlayer/blob/f276eb2/library/src/main/java/com/google/android/exoplayer2/source/ClippingMediaSource.java) and [ClippingMediaPeriod](https://github.com/google/ExoPlayer/blob/f276eb2/library/src/main/java/com/google/android/exoplayer2/source/ClippingMediaPeriod.java)).

Could you explain why this is problematic for your use case? Renderer implementations should not rely on buffer timestamps starting at a particular offset. Thanks. If you need a temporary workaround you can make `BaseRenderer.streamOffsetUs` visible to subclasses and subtract it from `positionUs` and `bufferPresentationTimeUs` to get the timestamp of the buffer read from the source.

Note: this won't work perfectly across period transitions if you're playing a playlist, but I doubt this applies because you would have seen output buffer timestamps that are not consistent with the source before this change in that case. That workaround doesn't work correctly for playlists, as described above. We'd need to expose the offset or source buffer timestamps in a way that works for all sources to fix this properly.

You could poll the player's current position which should match the source, but maybe you need closer synchronization than that. Or you could implement a custom MediaSource/Renderer and feed samples through the player to offset them, like we do for subtitles (then you wouldn't need to make any modifications to the video renderer for this). The original frame PTS is propagated to the [render|drop|skip]OutputBuffer methods as an additional argument, as of https://github.com/google/ExoPlayer/commit/88fddb42eca2da65297cfacd1a068aadc47b2614.  It looks like the Shield's PlayReady module can't allocate the memory it needs. I'm following up with nVidia. This is a memory leak issue that nVidia have already identified and fixed. I believe the fix will be included in their next software update for Shield. Thanks! We're not able to comment on the schedules of device manufacturers. Although according to the internet, the [answer is now](http://www.anandtech.com/show/11080/nvidia-releases-android-70-update-for-2015-shield-tv-adds-amazon-video-app).
  Yup, we will do this. Thanks!  Good question! Subtitles are really complicated because they can be packaged in different units of granularity and with different ways of conveying timing information. Roughly speaking, an input buffer received by a subtitle decoder consists of a timestamp (`timeUs`) and the subtitle data to be decoded (`data`). There are four cases that can occur:

1. `data` contains all of the cues for the media and also their presentation timestamps. `timeUs` is the time of the start of the media. The subtitle decoder receives a single input buffer.
1. `data` contains a single cue to be displayed at `timeUs`. There are no timestamps encoded in `data`. The subtitle decoder receives many input buffers.
1. `data` contains cues covering a region of time (e.g. 5 seconds) along with their presentation timestamps relative to the start of the region. `timeUs` is the time of the start of the region. The subtitle decoder receives many input buffers.
1. As above, but the timestamps embedded in `data` are relative to the start of the media rather than the start of the region. This case is tricky and best avoided.

For a side-loaded SSA file you'd have case (1). For SSA embedded in MKV, it looks like they way it's embedded means you'd have case (2) if you were to just pass the sample data through without changing it. Note that `timeUs` is being set to `blockTimeUs` already. The changes you're making to the sample data in the extractor appear to be turning it into case (4), since you're also embedding `blockTimeUs` into the sample data. Each region happens to be the duration of a single cue.

What you should do is:

- In the extractor, change the sample data so that you get case (3) rather than (4). It's much easier to handle. This basically means the embedded time should be 0 rather than `blockTimeUs`. If you look at the SubRip case in the MKV extractor you'll see that it does exactly this. The SubRip case also defers writing so that the end time can be set properly, which is something you likely want to copy.
- In the decoder you're supposed to create a new `Subtitle` instance for each decode call, rather than appending to an existing instance. Fix this.

Having made these changes, for the SSA embedded in MKV case you should end up with each call to decode producing a new `Subtitle` with a single cue at time 0. The reason this works is that the event timing in a `Subtitle` is relative to `timeUs` of the buffer, which is being set to `blockTimeUs`. The fact that event timing in `Subtitle`s is relative to `timeUs` is also the answer to your original question ;). When the decoder receives a new input buffer with a larger `timeUs` than the previous one, the value passed to `getCues` will go down. Great; glad you're making progress!  You can't expect a response when you provide so little information. "Couldn't select audio track" doesn't explain what you're doing. You don't explain what happens when you try and use your previous approach with r2.1.1 (e.g. does it not compile, does playback fail, if so how etc). You have also provided pretty much none of the information requested in the issue template (e.g. a bug report, media to test with, proper reproduction steps etc).

Closing. Please file a new issue containing detailed information if you want us to take a look.  If this is still a problem, please file a new issue including all of the information requested in the issue template (e.g. a full bug report, detailed reproduction steps that show AC3 not working, etc).  Closing because the issue does not include the information requested in the issue template. Please file a new issue including all of the requested information if you want us to investigate further. Thanks!  It looks like the first failure is that ExoPlayer doesn't find the track in the moov atom, which is part of the initialization chunk. The problem could be with the content (i.e. the initialization chunk failing the declare the track or declaring it incorrectly), or an issue with ExoPlayer (i.e. the extractor failing to handle the way in which the track is declared). It's likely the 416 issue occurs only as a knock-on effect of the first failure.

We don't need full access to the content to investigate, but we do need you to provide the problematic initialization chunk. You can either attach it to this issue, or email it to dev.exoplayer@gmail.com. The initialization chunk is invalid. Specifically, the `stsd` box is empty. It should contain an XMLSubtitleSampleEntry() (i.e. an `stpp` box) for the text track. Please take a look at "ISO 14496-12 Amendment 2: Carriage of timed text and other visual overlays", which describes what should be present. Whether non-compliant media plays successfully often comes down to exactly how the player is implemented. In this case the reason ExoPlayer is looking in the `stsd` box in to determine the type of the track. The track type is also defined in the DASH manifest (`codecs="stpp"`), and so it's likely that the web players you mention get the track type from the manifest and don't bother looking in the `stsd` box for it. Doing this would make sense if you were building a DASH only player. However for regular FMP4 playbacks there is no DASH manifest. Looking in the `stsd` box is the only way to go in that case. Since ExoPlayer supports regular FMP4 playbacks (and FMP4 in SmoothStreaming / HLS) in addition to DASH, and since we don't want to maintain multiple code paths, we look in the `stsd` box in all cases.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> So there's good news and bad news.

:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.

:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.

*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*

<!-- need_author_consent --> CLAs look good, thanks!

<!-- ok -->  I don't think it is used for audio tracks in any default configuration. It's also not well suited to audio adaptation; and if audio adaptation is desired a different implementation would probably be preferable. Please clarify and point out the circumstances in which it is used for audio tracks if you think this is incorrect. Closing due to lack of information. I'm not convinced that actually happens. There's another issue [here](https://github.com/google/ExoPlayer/issues/1975) that says, I think correctly, that audio isn't adaptive by default. If there are multiple audio tracks then current behavior is to pick one and stick with it. The default track selector only ever selects a fixed track for audio [here](https://github.com/google/ExoPlayer/blob/r2.1.1/library/src/main/java/com/google/android/exoplayer2/trackselection/DefaultTrackSelector.java#L657).

Can you provide an example of a stream where, if added to the demo app, performs an adaptive audio playback?  Yes, the lines

```
W/MDRMOemCrypto: duplicate SPS NALU (skipping)
E/MDRMOemCrypto: ERROR: multiple PPS NALUs (not supported by video decoder)
```

are suspicious and suggest that this decoder is not handling the adaptation SPS/PPSs correctly. Are you able to provide a link to the content (here or emailed to dev.exoplayer@gmail.com), or at least the clear subsamples with those NAL units? @akavrt Are you in control of the encoding for these streams (or do you have a contact of someone who is)? Whilst I would expect most decoders to handle this case, it does seem that the video streams are invalid in that they appear to be mixing avc1 and avc3 together. One of the following should be true:

1. A stream should claim that it's avc1, include SPS/PPS units in the init segment and *not* in the media segments.
1. A stream should claim that it's avc3, *not* include SPS/PPS units in the init segment, and prepend it in the media segments instead.

The problematic streams claim to be avc1, include SPS/PPS data in the init segment, but *also* prepend it in the media segments as well, which is why duplicate SPS/PPS units are being submitted to the decoder. I don't think this is valid. I think *not* in point (2) above should probably be *may optionally*. In which case your streams would likely be valid if they claimed to be avc3 rather than avc1. We'd still fail to play the content on Nexus Player if the streams claimed to be avc3, but I think it would become trivial for us to fix in that case. It still seems preferable to not include SPS/PPS in both places, however.

If you have a way to reliably know when you're going to be playing an asset with this issue, a quick fix for the failure is to delete [this line](https://github.com/google/ExoPlayer/blob/r2.1.1/library/src/main/java/com/google/android/exoplayer2/extractor/mp4/AtomParsers.java#L675) to drop the SPS/PPS in the init segment and therefore remove the duplication. Note that this will break playback of correctly generated avc1 streams, so you should only do this for assets that you know definitely have the issue.

Please do report back any findings from the CDN team. Thanks! As per my response above, an avc1 stream should include SPS/PPS units in the init segment and *not* in the media segments. Your streams contain SPS/PPS in both the init segment *and* in the media segments, which is not correct.

The player prefixes the SPS/PPS from the init segment onto the media segment when an adaptive switch because that's the correct thing to do for avc1, because SPS/PPS is not expected to already be prefixed in the media segment itself. This is why the duplication occurs.

If your streams are avc1, the correct fix is to remove SPS/PPS from the start of the media segments. It's possible the encryption tool is adding them, yes. Unless you know otherwise, it's also possible that the original streams are avc3 and that the encryption tool is incorrectly transforming the avc3 box into an encv->avc1 box rather than an encv->avc3 box.

The initial SPS/PPS is passed in the `MediaFormat` that gets passed to `codec.configure` in `MediaCodecVideoRenderer.configureCodec` (and if no adaptation switch occurs it's not necessary to send it again). The excess SPS/PPS in the media segments are precisely what *is* causing the problem? If you mean why doesn't failure occur right at the start of playback, when the decoder receives SPS/PPS via configureCodec and also at the start of the first media segment, then my guess would be that the SPS/PPS that's pushed in via configureCodec is submitted in a separate initialization buffer.  Thanks for reporting this. I have an idea what might be going wrong but haven't been able to reproduce it yet -- please could you provide steps to reproduce the issue, ideally in the demo app? (e.g. modify PlayerActivity.java to pass the player a LoopingMediaSource which wraps an ExtractorMediaSource playing the local OGG file.) I still wasn't able to reproduce this on a Nexus 6P, preparing the demo app player with a LoopingMediaSource wrapping the WAV file you emailed. Please could you provide specific steps to reproduce in the demo app? Got it -- it is important to loop a small number of times. Thanks.  It looks correct how it is currently to me, and it's desired that the `getAndSet` return the previous value. The correct behavior is to update paramsReference to the new params and call invalidate if the old and new params are different, which I think the current code does correctly. In your updated version the condition looks like it will always evaluate to false. Please clarify.  This is not currently supported. Marking as an enhancement.  I think you're confusing two separate topics. Use of the EXT-X-DISCONTINUITY tag allows various properties of the media segments to change (e.g. the encoding parameters). It does not make it OK to reset the media sequence number to zero, which is something that should never happen. The spec says this about the media sequence number:

> Its value MUST be incremented by 1 for every Media Segment that is removed from the Playlist file; it MUST NOT decrease or wrap.  Clients can malfunction if each Media Segment does not have a consistent, unique Media Sequence Number.

Marking this as a bug, but only because the exception isn't handled and causes process death, which isn't desirable. We'll fix this to cause a playback failure instead. For the stack trace to occur as in this issue report, it looks like a required condition is:
```
newPlaylist.mediaSequence < oldPlaylist.mediaSequence
```
And also that:
```
newPlaylist.isNewerThan(oldPlaylist)
```
Else the method would have returned at [this line](https://github.com/google/ExoPlayer/blob/r2.1.1/library/src/main/java/com/google/android/exoplayer2/source/hls/playlist/HlsPlaylistTracker.java#L365).

Are you still able to reproduce the issue? If so, can you take a look and see why `isNewerThan` is evaluating to true in this case? The only way I can see this happening is if the new playlist not only has media sequence set back to 0, but if it also has an end tag appended when the old one didn't. Is that what's happening? If not, what is? Thanks. Our plan for handling this case is to create a new playlist consisting of the old playlist with the end tag appended (i.e. just treat the playback as though it's ended).  That parameter is working fine for me. Please provide detailed reproduction steps, if possible using the demo app. Thanks. Closing due to lack of information.  See #2160.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> This is not a bug; it's fine to use referential equality with strings if you know that it's sufficient, which is the case here. It's true that it's less confusing just to use `.equals`, but we're accepting bug fixes only into V1 at this point, which this is not.  If you're not able to reproduce with the sample app (or a modification of it) then that would point toward an application specific issue, and I'm not sure there's much we can do to help. There's no reason inflating views from xml would result in different behavior compared to creating views in code. Please feel free to update this issue if you do find a way to modify the demo app that causes the issue to reproduce there! > DEFAULT_BUFFER_FOR_PLAYBACK_AFTER_REBUFFER_MS is set to 30000 ms, which is completely legal value for re-buffering.

It's not a sensible value to use unless you also change some of the other values defined there. By just changing that constant you're saying you want the player to maintain between `15s` (`minBufferUs`) and `30s` (`maxBufferUs`) of buffer, but that you don't want the player to actually start playing unless there's `30s` (`bufferForPlaybackAfterRebufferUs`) of media in the buffer. The player is doing what you've told it to do: It maybe has a buffer of `20s` (fine according to the first two constants) and is not resuming playback (fine according to the third constant). To avoid getting stuck this way, you should ensure that `bufferForPlaybackAfterRebufferUs` is smaller than `minBufferUs`. We should probably assert this and document it more clearly, so we'll use this issue to track that. But the root cause is you're not setting configuring the load control with sensible values.

> If you decrease value of DEFAULT_BUFFER_FOR_PLAYBACK_AFTER_REBUFFER_MS to 5000 ms then there is no issue. But in real applications this parameter rarely has such low value.

I'm not sure how you've reached the conclusion that real applications rarely use the default values. I suspect the vast majority of applications *do* use the default values. Google's streaming services (YouTube, Play Movies etc) use low values too. Setting the value as high as 30s would require the application to download a pretty huge buffer in some cases (e.g. 30MB in the case of an 8mbit/s stream). It's generally not desirable to require such a large buffer prior to resuming playback.

Re-opening to track adding sanity assertions and documentation only. Buffering is anything preventing the player from making progress. In the case of switching the view, the player may switch into a buffering state for a very short period of time until the first frame is dequeued from the newly acquired video decoder and rendered to the newly provided surface. There's usually no network re-buffer involved. Note there are other scenarios (apart from swapping the view) that could also result in the stuck state, if the constants are not configured sensibly.  Does the issue reproduce using ExoPlayer's demo app, or just your own? Do you have other physical devices on which the app behaves correctly? We don't have a Xiaomi Mi Box to test on, but I'll try and get hold of one.

In the meantime, if you're seeing different behavior on Mi Box than on other devices, you should probably report the issue to Xiaomi as it's likely to be a device specific issue with the platform software in that case. @sergeymild - I doubt it's the exact same issue. Either way, please provide sample content + a full bug report (i.e. the output of `adb bugreport` captured whilst reproducing the problem) if you want us to investigate. Thanks! I was able to reproduce this. It appears to be a bug with the H264 decoder on Xiaomi devices (OMX.amlogic.avc.decoder.awesome), where the output format of the decoder has the wrong dimensions. @agr111 - Is it possible for you to report this issue directly to Xiaomi? Thanks! Actually, I have a contact route. I'll send it their way and see if they can provide an update. Thanks! [Internal ref: 34270647] My understanding is that Xiaomi will be fixing this in an upcoming platform update. I don't have an ETA.  We don't support adaptation between audio+video and audio only tracks, sorry. This is mainly because DASH and SmoothStreaming do not have the same kind of concept due to the fact that audio and video tracks are de-muxed. To support this kind of adaptation for HLS only would require our HLS implementation to deviate significantly from the code that we use for DASH and SmoothStreaming, and this is not something we're keen on doing.

As an aside, I don't really understand *why* adaptation to audio only is part of the HLS spec. In most cases I'd argue that buffering is preferable to the video disappearing. For the minority of cases where it's not, it seems simpler just to include an extremely low bitrate video track than to require players to handle the video track disappearing. I guess the reasoning behind my last paragraph is that it wouldn't even be necessary to encode the TV stream. You could just encode a thumbnail as a very low bitrate video track. Each segment could probably just have a single key-frame. That key-frame could even just be solid black. Seems like it would have been easier. Unfortunately it's just not how things are :). Oh well!  resizeMode means resolution shows user in device monitor?

or video frame?

if the latter, I think video frame's size is dependent on encoder

so it can't change

It is only my opinion, other person comment please if i was wrong about this issue - As it happens there's also `SimpleExoPlayer.setVideoDebugListener`, which receives changes in the video size as well. It's not really intended to be used this way, but you could probably use it to do what you want.
- I can't really think of a valid reason for setting the resize mode dynamically; it doesn't sound right. Can you explain what you're trying to achieve? If you really do need to do this then it's likely we just need to add another resize mode that does what you need directly, so that you don't have to mess about changing it dynamically. I don't think the approach in your pull request is the best way to achieve this. I have a feeling it might not always work properly, but I don't have any hard data confirming this yet.

In any case, if you're using `SimpleExoPlayer` and `SimpleExoPlayerView` in their default configuration (i.e. the video is being rendered to a `SurfaceView`), then you should use [`SimpleExoPlayer.setVideoScalingMode`](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/SimpleExoPlayer.html#setVideoScalingMode-int-) to achieve this. The value you need to pass is `C. VIDEO_SCALING_MODE_SCALE_TO_FIT_WITH_CROPPING`. You should set the `resizeMode` of the `SimpleExoPlayerView` to `fit`. What does "doesn't work" mean exactly? It should work. Setting `resizeMode` to `fit` should make the view the correct size at least. What are you seeing being drawn into the view if not cropped video? `resizeMode` actually needs setting to `fill` rather than `fit`. I then see expected behavior on Pixel, but not on the emulator. Does it work for you?  I think the ID3 data in the file is technically in violation of the ID3 spec. According to the spec a PRIV frame should look like:

```
<Header for 'Private frame', ID: "PRIV">
Owner identifier        <text string> $00
The private data        <binary data>
```

However the file contains a PRIV frame that's completely empty apart from the header. Given the spec, I think there must always be at least one byte following the header (i.e. the $00 terminator on the owner identifier).

That aside, I think the parsing code we have will fail anyway if there's no private data, and fixing that issue will happen to "fix" playback of this file as well. So I'll do that. Thanks! Actually, the parsing code handles the case of zero length private data fine.

I'm not sure it's worth spending time fixing this when it's only a single file that's been found to be affected. If you're aware of a tag editor that's being used to generate lots of these files then please let us know, and we'd consider a workaround.

For the affected user, you can advise them to simply upload their mp3 file to http://tagmp3.net/ and then download the resulting file without changing any of the data. This resolves the issue with the file.  As:   #2237

http://programmingmemojohnchang.blogspot.tw/2016/12/improve-performance-of-short-seek-of.html

**Please read in detail by the link above which includes the result of experimental comparison.**

For streaming, current flow may hit the case where the whole pre-loaded data will be flushed. However, sometimes it is unnecessary to do so.

Once all pre-loaded data is flushed, ex: for 4K movie, a painful delay will happen.
Also, it hurts for users which do not have unlimited network data plan.

Some MPEG DASH links such as YouTube are made by the max interval between key frames about over 5 seconds. Hence the short seek may probably hit the claimed case.

To improve it, we may take the same way applied in GStreamer which filters the decoded-only samples at renderer.

The experimental result shows:

1. We save a lot of data download compared to original flow.
2. The seek response time is near to be the same as seamless; compared to originally it has a noticeable delay.

Please estimate if it works.

Thanks.  This issue is a good example of why you should include all of the information requested in the issue template (e.g. test content, a full bug report, clear reproduction steps with the ExoPlayer demo app). There's not enough information here for us to help.

The log line referenced in the title of this issue is expected and hence not actually an error, despite the somewhat confusing fact that it's logged as one by the underlying platform.

Please file a fresh issue containing all of the information requested in the template if you want us to take a look. Thanks.  This is an issue with the source content. The first media segment doesn't include any audio. In the second media segment this additional track belatedly "appears". The first media segment should include audio, even if it's silence. You should ask the Red5 people to fix this.  Note: Interestingly, seeking does appear to work up to about 20:15.
Correction: It looks like playback becomes more prone to getting stuck the closer to 20:15 you get (and is completely broken from that point onward). The stream in question does not work correctly on 2.0.4. On 2.0.4 seeking does not cause the player to get stuck, but audio/video sync is completely broken if you seek to the middle of the content. It's a couple of minutes out at least. On 2.1.0 audio/video sync is corrected because we started looking at the ID3 header, as a player is supposed to do, but the knock-on effect is that the player can get stuck buffering.

I think the root cause is that the segment durations listed in the audio media playlists are rounded to integer precision. The sum of the durations then drifts over time. Since it's a long piece of media, by the middle the drift very significant. This may not be a technical violation of the HLS spec, but does mean it's impossible for any player to efficiently seek in the media. The best guess for which segment to seek to for this media will result in a player requesting a segment that's multiple minutes off from the requested seek position. A player can then "guess" again based on this information, to eventually find the correct segment, however this is really quite inefficient. We do not do this in ExoPlayer currently, which I think is why the playback then gets stuck.

The best solution here would be to correct the media playlists to use floating-point precision segment durations. This will resolve the issue when using ExoPlayer, and *also* make seeking more efficient when using any other player. Recent versions of the HLS spec say this about segment durations:

> Generally, durations SHOULD be decimal-floating-point, with enough accuracy to avoid perceptible error when segment durations are accumulated.

We should also think about handling this kind of media better in ExoPlayer, but it's likely to be treated as a low priority issue because the media is quite sub-optimal to start with, and we tend to focus more energy on efficiently supporting well prepared content, rather than on handling all legacy cases where the experience will be sub-optimal no matter how well we handle it. I'm unsure what your point is, since I didn't say they were invalid. I explicitly stated they weren't a violation of the spec. But the fact they're valid doesn't make them in any way a sensible idea, for the reasons specified above (i.e. it's impossible for *any* player to perform an efficient seek if the accumulated error grows significant).

Note also that versions prior to V4 are not relevant to this issue, since EXT-X-MEDIA wasn't added prior to V4. Whilst it might be common for older streams to use integer precision EXTINF durations, I doubt it's common at all for streams to use integer precision EXTINF durations and EXT-X-MEDIA tags, which is when this becomes a bigger issue for ExoPlayer.  Please see #2208. This is fixed in r2.1.1, which we just pushed (~5 minutes ago :)).  The [demo app](https://github.com/google/ExoPlayer/tree/r2.1.0/demo) already does this. I suggest you take a look at it to see how it works.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  This is fixed already in r2.1.0, to the extent that it's possible. If playback works with r2.1.0, then problem solved. If playback still fails with r2.1.0 then the device simply isn't capable of decoding 1080p (this is probably not the case for the device in question).

Note that it's more usual for different bitrate streams to also be encoded at different resolutions when creating DASH content, so as to better support low end devices. There definitely exist many lower end Android devices that cannot allocate video buffers large enough for 1080p content. If you're producing this content, you may wish to consider this and adjust your content accordingly. Decoding the video stream requires an MPEG-2 decoder. It seems the Samsung Galaxy S4 does not declare support for this format (hence the `supported=NO_UNSUPPORTED_TYPE` logging).

See the [supported media formats page](https://developer.android.com/guide/topics/media/media-formats.html) and the [CDD](https://source.android.com/compatibility/cdd.html) for information on the minimum video decoding capabilities you can rely on. Android TVs are 'strongly recommended' to have MPEG-2 decoders, but emulators often have poor support for media so it is best to test on real devices.  `ExtractorMediaSource` is probably what you're looking for. Note also that `ExtractorRendererBuilder` was never part of the ExoPlayer library. It was part of the demo app only.  Thanks for the detailed issue report. Unfortunately I'm unable to reproduce the issue. The sample content appears to play successfully in the demo app using r1.5.13 and r2.1.0. Are you certain that the attached content is problematic? Are you able to reproduce the issue with the demo app? Thanks! I think this issue occurs because the third segment randomly doesn't contain any audio (not even a silent audio track). Either that or the TS extractor isn't finding it. It's unclear whether this is valid within the HLS spec, even with a discontinuity insertion, but it's not something we've seen before and is not something we handle. I doubt we'll prioritize handling this case, either.

It does appear that V2 plays the sample without failure, but I'm unsure whether this is down to chance or it actually handling the case properly.  We roughly adhere to the [Google Java style guide](https://google.github.io/styleguide/javaguide.html). You may also find [this](https://github.com/google/styleguide) useful. The code across the project is pretty consistent, so I'd say the best style guide is the code that's already there :).  This happens for on-demand content that includes EXT-X-PROGRAM-DATE-TIME tags. The sample timestamps are (possibly correctly) grounded by program-date-time in this case, but the timeline still thinks the start of the media is at timestamp 0. Hence the huge numbers. We'll take a look. This is fixed on the `dev-v2` branch. The fix is a little. . .strange, but hopefully it'll do the trick.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  I'm not aware of any.

You could build one on top of Android's standard [Visualizer](https://developer.android.com/reference/android/media/audiofx/Visualizer.html) API, passing the audio session ID being used by ExoPlayer. If using SimpleExoPlayer you can query the audio session ID using [SimpleExoPlayer.getAudioSessionId](https://github.com/google/ExoPlayer/blob/r2.1.0/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java#L398), and also listen to changes to the ID using [SImpleExoPlayer.setAudioDebugListener](https://github.com/google/ExoPlayer/blob/r2.1.0/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java#L439).

It's probably also possible to modify ExoPlayer to add direct hooks on which a Visualizer could be built. We don't provide any hooks currently, however.  The ExoPlayer demo app works fine for playing content from local directories, so I think this is likely to be an issue with your code. I'd suggest you try reproducing with the demo app, and filing an issue with specific reproduction steps if you're able to recreate the issue there.

A few other things to keep in mind:
- You may need to be careful with URI encoding/escaping when the URI contains underscores, as is the case here. You might want to try playing a file without any underscores in it to see if that works. That would indicate an encoding/escaping issue.
- You may need to request the READ_EXTERNAL_STORAGE permission at runtime, depending on what API level you're targeting. The demo app does this [here](https://github.com/google/ExoPlayer/blob/r2.1.0/demo/src/main/java/com/google/android/exoplayer2/demo/PlayerActivity.java#L317).  This is a question about building Android user interfaces. It's not particularly ExoPlayer specific, and falls outside the scope of this issue tracker. Sorry.  This is not something we support, sorry.  We're unable to offer support debugging individual applications, sorry. If you uncover a specific ExoPlayer issue during your own debugging, please provide detailed repo steps, ideally with our demo app or a modification of it to which you can make the source code available.  In my humble opinion, there might exist a scenario where not present is not the same as undefined. I would rather consider having a method utility isLanguageDefined() or similar that handles this for you, if needed. I wouldn't normalize this before the application level. But I guess it's open for discussion. @andrewlewis , @ojw28, @erdemguven  ? As @AquilesCanta notes, there's a difference between the media explicitly stating the language is undetermined ("und") and the media not saying anything about the language at all (null). I think it's correct that these cases are distinguished by the language field being set differently.

"und" is a well defined language code, and there are other special codes such as "mul" and "mis". See [here](https://en.wikipedia.org/wiki/ISO_639) for more details. Android platform APIs such as [Locale](https://developer.android.com/reference/java/util/Locale.html) also make use of it. So I don't think "und" deserves a public constant unless we also add constants for all other language codes too, and I'm not sure we really need to do that.  We're not particularly familiar with DrmToday. I'd suggest asking them for assistance on this, since what properties to set is dependent on what their requirements, not on ExoPlayer.  Exactly what the seek bar should look like is a matter of opinion, and the desired choice may vary between apps depending on exactly what the use case is. As an example, for live TV broadcast where a live stream has been running for months continuously, the UI as shown would be useless.

ExoPlayer exposes all of the information necessary to create your own player UI components that behave as described. We don't have any short/mid-term plans to support this directly. How to display the seek bar in these cases doesn't have a single right answer. There are different approaches, as noted here, and the one that's "best" is likely dependent on both the app, the nature of the streams being played and personal preference. "What Apple did" is not a ground truth for correctness. I suspect that a user will largely expect what they're used to from using other apps (or your app if they're a regular user). There are existing players that take both of the approaches described in this thread.

As an example of where our approach might be more useful than the one you describe, consider a case where the event duration is much longer than the seekable window. For example consider a 24 hour event where the seekable window is 15 minutes. In the approach you describe only 1% of the width of the seek bar would be usable for seeking, making it very hard to seek accurately within the 15 minutes. In our approach 100% of the width of the seek bar would be usable and accurate seeking would be possible.

As above, if you require different behaviour there's nothing stopping you from implementing your own player UI components to do this. If you were to modify the library player UI components to support an alternate mode that behaves as you describe, enabled via some kind of setLiveSeekMode(mode) method, then we would also consider accepting a pull request should you wish to contribute that back to the project.  You should implement your own audio renderer and use it instead of `MediaCodecAudioRenderer` (or extend MediaCodecAudioRenderer if it's possible to adjust it to meet your use case).

If you're using `SimpleExoPlayer` then the way to pass a custom renderer is to extend `SimpleExoPlayer` and override `buildAudioRenderers` to add your own renderer rather than the ones that are added by default. If you're using `ExoPlayer` directly (via `ExoPlayerFactory.newInstance`) then you just inject it when you create the player instance.  My object implements Simple ExoPlayer for DASH Stream

So, I try to find simple exoplayer example (using basic exoplayer library and method)

but I can't find that..

Please help, The demo app code is too hard to me I implemented simple dash stream player using SimpleExoPlayer API

Add My code this comment, soon Here is my code

This code is very simple example for playing one Dash Stream (by Big Bunny MPD)

    // Variables
    private static final String VIDEO_URI =
            "http://www-itec.uni-klu.ac.at/ftp/datasets/DASHDataset2014/BigBuckBunny/4sec/BigBuckBunny_4s_onDemand_2014_05_09.mpd";

    private SimpleExoPlayer player;
    private SimpleExoPlayerView simpleExoPlayerView;
    private Handler mainHandler;
    private TrackSelection.Factory videoTrackSelectionFactory;
    private TrackSelector trackSelector;
    private LoadControl loadControl;
    private DataSource.Factory dataSourceFactory;
    private MediaSource videoSource;
    private Uri uri;
    private String userAgent;
    private static final DefaultBandwidthMeter bandwidthMeter = new DefaultBandwidthMeter();

    // Activity onCreate
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_dash_play);
        simpleExoPlayerView = (SimpleExoPlayerView)findViewById(R.id.player_view);
        userAgent = Util.getUserAgent(this,"SimpleDashExoPlayer");
        createPlayer();
        attachPlayerView();
        preparePlayer();
    }

    // Create TrackSelection Factory, Track Selector, Handler, Load Control, and ExoPlayer Instance
    public void createPlayer(){
        mainHandler = new Handler();
        videoTrackSelectionFactory = new AdaptiveVideoTrackSelection.Factory(bandwidthMeter);
        trackSelector = new DefaultTrackSelector(videoTrackSelectionFactory);
        loadControl = new DefaultLoadControl();
        player = ExoPlayerFactory.newSimpleInstance(this,trackSelector,loadControl);
    }

    // Set player to SimpleExoPlayerView
    public void attachPlayerView(){
        simpleExoPlayerView.setPlayer(player);
    }

    // Build Data Source Factory, Dash Media Source, and Prepare player using videoSource
    public void preparePlayer(){
        uriParse();
        dataSourceFactory = buildDataSourceFactory(bandwidthMeter);
        videoSource = new DashMediaSource(uri,buildDataSourceFactory(null),new DefaultDashChunkSource.Factory(dataSourceFactory),mainHandler,null);
        player.prepare(videoSource);
    }

    // Parse VIDEO_URI and Save at uri variable
    public void uriParse(){
        uri = Uri.parse(VIDEO_URI);
    }

    // Build Data Source Factory using DefaultBandwidthMeter and HttpDataSource.Factory
    private DataSource.Factory buildDataSourceFactory(DefaultBandwidthMeter bandwidthMeter){
        return new DefaultDataSourceFactory(this, bandwidthMeter, buildHttpDataSourceFactory(bandwidthMeter));
    }

    // Build Http Data Source Factory using DefaultBandwidthMeter
    private HttpDataSource.Factory buildHttpDataSourceFactory(DefaultBandwidthMeter bandwidthMeter){
        return new DefaultHttpDataSourceFactory(userAgent, bandwidthMeter);
    }

    // Activity onStop, player must be release because of memory saving
    @Override
    public void onStop(){
        super.onStop();
        player.release();
    }


In activity xml file just use below,
 
<com.google.android.exoplayer2.ui.SimpleExoPlayerView android:id="@+id/player_view"
       android:layout_width="match_parent"
       android:layout_height="match_parent"/>

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

I'm first at ExoPlayer Development, so this code may be strange

Even so, example code run without problems  @masiha68 
This example is only for one dash stream
So, you must modify this code if you want to use the other format  We'll merge a fix for this shortly, thanks! This is fixed on `dev-v2`. We will likely do a bug-fix-only `2.1.1` release next week.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> @vigneshvg - Could you take a look at this? Thanks! Note: The fix will have to be submitted into dev-v2, not release-v2. Is there a specific case that this patch fixes?

From what i see in other uses of the decoder (ffmpeg [1], android platform [2]) they are all hardcoded to 960*6 = 5760. Considering that the existing calculation seems to make sense.

[1] https://github.com/FFmpeg/FFmpeg/blob/master/libavcodec/libopusdec.c#L142
[2] https://android.googlesource.com/platform/frameworks/av/+/master/media/libstagefright/codecs/opus/dec/SoftOpus.cpp#461 @supercairos - Could you comment on the above? Thanks.  We've got quite a few DASH live streams from multiple providers that we use for internal testing, and seeking is working correctly. It sounds more like a content issue to me. Please provide a test stream. Also, 404s when seeking near the end of the live window indicates media being advertised by the manifest before the chunks are actually available from the server (which is likely a violation of the DASH spec). @moacir69 dev.exoplayer@gmail.com The way this manifest is structured requires the client to be able to accurately synchronize against server time. This is normally achieved by the manifest including a `UTCTiming` element, however in this case the manifest does not include one. As a result, the best the client can do is assume that it's own clock is synchronized with the server, which is typically not correct.

The issues you're seeing are a result of the client and server clocks being slightly out of sync. This issue can be illustrated more obviously by changing the time on the client (i.e. mobile device) to be 1 hour ahead of the actual time. You'll find that playback is completely broken in this case.

The solution is for the manifest to include a `UTCTiming` element, which will allow the client to synchronize with server time rather than relying on the local clock.  Not that I know of. See our [FAQs](https://google.github.io/ExoPlayer/faqs.html) for information about emulator support.  I think this is fixed already (within the last 10 days).  StackOverflow is a more appropriate place to ask general questions. This tracker is for issues and feature requests.  Spun out of @scroozer's comments on #1705. Looks like it's just a matter of allowing the key to be passed through the constructor and down into ExtractorMediaPeriod where the load DataSpecs get instantiated? This was fixed by the change ref'd above.  I'm using in my app several instances of `SimpleExoPlayer` to play audio file (Wav file). I want play all these files simultaneously. Now there is some small gap between these files. Is it possible to have only one instance of `SimpleExoPlayer` with more sounds files? 

I tried use `MergingMediaSource` but it's now working. It's playing only one Wav file (not all).

Code:

            MediaSource ms = new MergingMediaSource(audioSources.toArray(new MediaSource[audioSources.size()]));
            mMediaPlayerTracker1.addListener(myExoPlayerEventListener);
            mMediaPlayerTracker1.prepare(ms);

            //Adding MediaSource to the ArrayList
            audioSources.add(new ExtractorMediaSource(
                        Uri.fromFile(new File(path)), mediaDataSourceFactory, defaultExtractorsFactory,
                        mainHandler, exoEventListener));

Thanks for help or advice! To achieve this you'd probably need to be mixing the audio prior to feeding it into a single AudioTrack instance. This is not a use case we currently support, sorry!  You're probably using a media format that is not seekable. Please see the [supported formats page](https://google.github.io/ExoPlayer/supported-formats.html). If you want to seek to a non-zero position, you need to use a container format that supports seeking. Hah, yes, I was just writing a comment suggesting you tried that :). Glad you solved the issue!
  It's not clear to me what the issue is. I think you missed part of the stack trace. Please file a new issue including all the information required in the issue template. Read each item carefully, please.  - H262 in MPEG-TS is supported by ExoPlayer at the container level. The extractor has a [H262Reader](https://github.com/google/ExoPlayer/blob/release-v2/library/src/main/java/com/google/android/exoplayer2/extractor/ts/H262Reader.java)). I would expect this to work in the HLS case also, although I've never tried this.
- The device will still of course need an H262 decoder to decode the frames. If the device has one then playback should just work. If it doesn't, you'd need to bridge onto a software decoder. Bridging onto FFMPEG video decoders is not currently supported, and is tracked by https://github.com/google/ExoPlayer/issues/2159.  We'd definitely like to make sure RTL captions work properly :). Please could you provide some sample media so that we can investigate the issue? It would be helpful if you could provide some information about a particular point in the video where the issue occurs, and the expected output at that position (similar to the example you provide above). This will help us verify a fix.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> Heh, thanks!  I'm not sure I'd describe that as a simple use case. Sounds pretty complicated to me ;). It's not something we support directly. It likely is possible to customize ExoPlayer for this use case, but we're unable to provide guidance; sorry.  ExoPlayer doesn't support this I'm afraid. Generally, it is not feasible to play compressed videos backwards in real time, due to the way they are represented.

The encoded representation of each video frame generally depends on one or more reference frames, except for key-frames which can be decoded independently. To decode a given frame you first need decode its reference frames, and to decode those you need to decode their reference frames, and so on (until you reach a key-frame). A common case is for each frame to depend on its preceding neighbor. So it can be much more expensive to decode an arbitrary frame compared to the cost of decoding a key-frame. The same cost contributes to the buffering pause you might see when doing a frame-exact seek.

Common workarounds for this problem are (1) (re-)encoding the source video with only key-frames so any frame can be decoded for about the same cost, at the expense of decreased coding efficiency, (2) decoding and storing whole groups of pictures in advance of playing them, which can use a lot of storage (unless you re-compress the decoded frames independently) or (3) playing only those frames which can be decoded efficiently (normally just key-frames) which means that playback won't be smooth. Various other tricks/tradeoffs are possible. Dear mehraf:

I guess your request is to playback all frames smoothly as andrewlewis's suggestion (2)

> (2) decoding and storing whole groups of pictures in advance of playing them, which can use a lot of storage (unless you re-compress the decoded frames independently) 

According to my previous experience it is possible to achieve it. 
Previously I made this function by mediacodec: 
https://developer.android.com/reference/android/media/MediaCodec.html

You could check the result at the link below:
original movie:
https://www.youtube.com/watch?v=gGaXMwQAEK8

The new feature created based on MeidaCodec
**(play baseball in reverse direction is actually  very funny!):**
https://www.youtube.com/watch?v=BKZdIb8H3CU

You could compare & check the effect by these two movies.

I attach my original design document as the link below:
https://drive.google.com/file/d/0B7H5vJR3qWj0WFRnMG9fQmV0bE0/view?usp=sharing

Here are some important tips to achieve it.
1. **DispMaxqueue** means the max storage you have for your codec (as andrewlewis  suggested)

> ... which can use a lot of storage...

for me, I ported a software H264 decoder, control the max internal buffer to be about 10~20 MB.
You should carefully set it according to your case.

2. A check should be made to see if the movie to be playback satisfies the limit of storage or not.
As my experience, for MP4, we could easily (for most cases) detect the max GOP.
However, for MPEG2-TS it is not the case.
So we only support MP4 previously.

3. Most of the devices with HW codec, as my HTC X9, could keep ~ more than 1 seconds frames within the codec. Hence a reorder if the max GOP is less than 1 second may be possible.
I think if you do **NOT** care the movies which are compressed by strange settings (as those with irregular huge GOP size), you could complete it (as I know, some cameras ex, from SONY(but about 3 years ago), make the rule by compressed in regular GOP size = 1 second (or 0.5 second); for this case it is nice to complete the reverse playback functionality).

**In short, technically it is NOT hard to do but the only problem is the irregular source movies.** 






  ExoPlayer has some extensions wrapping software decoders. To get started, follow the instructions in the extension README files: [ffmpeg extension README.md](https://github.com/google/ExoPlayer/blob/release-v2/extensions/ffmpeg/README.md), [vp9 extension README.md](https://github.com/google/ExoPlayer/blob/release-v2/extensions/vp9/README.md), etc.

The [demo app documentation](https://google.github.io/ExoPlayer/demo-application.html#including-extension-decoders) describes how to play your media in the demo app using the extensions, so you can follow those instructions to check you've built the extensions correctly.

If you're using SimpleExoPlayer in your app, pass `true` for [preferExtensionDecoders](https://github.com/google/ExoPlayer/blob/dca4d16/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java#L127) to use extension renderers in preference to MediaCodec-based renderers.

Note: software video decoders are likely to be less efficient than the decoders in the platform, so you might want to use the extensions only on pre-API 23 builds.  What format are the subtitles provided in? If they're in a standard subtitle format that we support, you can just use ExoPlayer's built in subtitle support. Marking this as a general enhancement to support triggers are certain positions in the media. It's unclear whether that's really what the original poster wants, but request was never clarified.  You should make sure you've acquired WiFi and wake locks to ensure playback whilst in the background (see https://github.com/google/ExoPlayer/issues/930). You'll also need to make sure you're running in a [foreground service](https://developer.android.com/guide/components/services.html#Foreground).  I have just cloned and imported the project and it worked just fine. You should ask in a general Q&A exchange. It's most likely some local config.  We'll push a fix for this tomorrow; thanks!  - Anything that's public is by definition part of the public API.
- We don't guarantee not to break the public API between versions (minor or major). We do of course don't break the public API just for the sake of it, either ;).
- Where we do make breaking changes, you can expect them to be in the form of compilation errors (e.g. a change to a method signature means your code no longer compiles). So they should be immediately obvious on upgrading and fairly easy for you to resolve.
- You can expect for us not to silently break things (e.g. leave a method signature unchanged but completely change what each variable means). So if you upgrade and your code does still compile, you can reasonably assume the APIs on which you depend were unchanged. Of course, this does not guarantee no bugs were introduced in the new version. We recommend thoroughly testing your application following taking of an upgrade.  - Gapless playback should just work if the input media has the right metadata and you're using the default MediaCodecAudioRenderer (see this [blog post](https://medium.com/google-exoplayer/exoplayer-2-x-new-audio-features-cfb26c2883a#.rddn60fl6) for more information).
- For looping use LoopingMediaSource (see the [blog post on MediaSource composition](https://medium.com/google-exoplayer/exoplayer-2-x-mediasource-composition-6c285fcbca1f#.xzxs2k1sg)).

We don't have direct support for playing multiple tracks at the same time in sync, or handling any capture operations. These are not really in scope for a general-purpose media player. For playing multiple tracks in sync you likely need to mix the audio yourself before writing one stream to an AudioTrack. If you need to use ExoPlayer for reading media you could probably implement a custom audio renderer passed to each player which handles mixing and writing to the same AudioTrack (you'd need to handle receiving data from each player on its own playback thread, and then exposing the playback position from your mixed AudioTrack). This may or may not work and would likely get quite complicated, so the best solution is probably to implement a custom player using AudioTrack and the other low-level media APIs in the Android framework!  Please see #1894. I think changing the ffmpeg parameters configuration should be enough for you to fix the issue without the need of FLAG_ALLOW_NON_IDR_KEYFRAMES.   This looks like an issue with MediaPlayer not ExoPlayer.  This is working as intended. As a matter of fact, when switching variants, the next loaded chunk should not introduce any new samples, ideally. This is due to the fact that the HLS spec does not require key frames to be provided at the start of the chunk.

Let's say we just load the next chunk. Any samples preceding the first key frame(which may be anywhere in the chunk) have to be discarded, as they cannot be decoded. If said samples have new content, you will perceive a gap in playback. Hope this made things clear. Other adaptive protocols, like DASH, don't present this inconvenient. Not only does the HLS spec not require segments to begin with key-frames, it doesn't require segment boundaries to be aligned across variants either. To switch variant without downloading any overlapping content, the spec would have to require both of these properties. Other adaptive streaming standards (i.e. commonly used DASH profiles, and also SmoothStreaming) do require these properties, and hence switching variants when using them is significantly more efficient than it is when using HLS.

We recommend using DASH for adaptive streaming. It's more efficient in quite a few ways, this being just one example.  We do not support bitmap subtitles. Support for them is tracked by #179 (I'm not sure whether BDN is another variant of bitmap subtitle format, but I think we can use the same issue for tracking).  This is not supported yet. Let's use this issue to track the enhancement. The work is not finished (which is why the issue hasn't been closed). It's not expected to work at this point. Note from ref'd issue: An additional test stream is available at http://se-mashup.fokus.fraunhofer.de:8080/dash/assets/adinsertion-samples/events/inband/dash.mpd This functionality should be working now in `dev-v2`. Please give it a try.

There's still one technical issue where events might go missing when seeking short distances and/or toggling the event tracks on and off, so the issue is kept open to track fixing that. It's unlikely to be much of an issue in practice, however. Marking as fixed. We'll track the remaining minor issue separately.  It looks like it should work. It's unclear what the question is here? If you've written some code, it's your responsibility to test whether it works (and debug it if not). The purpose of this tracker is not for us to do it for you. #2166 is tracking making this easier; until then you'll have to do something similar to the code you've written above.  Please can you clarify exactly what your use case is? What operation do you want to do, exactly, and how synchronous does it really need to be? Is onInputFormatChanged called on MediaCodecVideoRenderer? The format on the input side of the decoder has quite a bit of extra data in it, so that's more likely.

If it is then the listener will see an invocation of onVideoInputFormatChanged, which might be good enough. If it's too early, you could do something like:
1. Set a flag in onInputFormatChanged
2. When an input buffer is queued, if the flag is set, clear it and record the timestamp of that buffer. This will be the timestamp of the first buffer after the format switch.
3. When an output buffer is rendered, look at its timestamp. If it matches the timestamp recorded in the previous step then it's the first buffer after the format switch. Generate an event and do what you want in response to it.  This is a duplicate of #1978, #1921 and #2163. @AquilesCanta Pretty sure you're right that the streams are in violation of the spec. I wonder though: Am I right in thinking that we started looking at continuity counter as part of the work to handle "noisy input" properly? Is there any other reason to look at them? If not, maybe we should disable the continuity counter checks for HLS via (another :/) flag, given they appear to be causing problems. Handling "noisy input" doesn't seem necessary for HLS because HTTP/TCP is used. Note: If we do this, it will need to be V1 and V2, since I continuity counter checks have been added to V1 as well.  We already support seamless looping in ExoPlayer V2. So I think all you need is the ability to play a section of a video (rather than the whole thing), which you could then loop. The ability to play a section of a video is tracked by #1988, so please follow that issue.  We did intend to write a migration guide, but in practice it's difficult to do so because ExoPlayer can be used in so many different ways.

For "standard" use cases it should be pretty easy to figure out what to do by (a) using the resources above, (b) looking at what changed between V1 and V2 in the developer guide, and (c) looking at what changed in the demo app between V1 and V2. The demo app is probably the most useful thing to look at. Since standard use cases don't require too much work, it's also a viable option just to start from scratch with V2. If you just want to get some videos playing, or similar, then it shouldn't be much work at all, and will likely prove easier than trying to figure out the mapping between the two.

For "advanced" use cases it'll probably be the case that you get more or less all the way there but are stuck on some specific point (e.g. a customization that you made to V1 which you can't work out how to migrate across). For those issues, if really stuck, you'll probably have to ask specific questions.  Please see: https://github.com/google/ExoPlayer/issues/2053  I am not sure it's selecting the same track everytime, from what I can understand from that (not formatted) manifest, there are three variants with the same resolution and bandwidth. I have a feeling that's related to the issue. I will have a more thorough look but, if possible, please provide all the information required in the issue template. If you don't want to make the link public, please send it to dev.exoplayer@gmail.com. I am getting a 403. Can you fix this? I'll try to recreate the issue myself, then. Until then, I'd suggest trying to attach a debugger and working out the root cause of the issue.  That doesn't look right, so I suspect it's a bug. @andrewlewis can confirm. FlacOggSeeker and OggSeekMap also have unwanted side effects. We'll be merging fixes for these two (and also for Mp4Extractor) into `dev-v2` shortly. The changes will not be backported to `dev-v1` because it requires touching quite a lot of files. Fixed in `dev-v2`.  - There is no class called `RendererBuilder`, as far as I can see?
- You can implement your own `HttpDataSource.Factory` to generating instances of `DefaultHttpDataSource` that are configured the way you want them, so I think this is possible. If you don't think this works, please clarify why not.
- We could make it easier by allowing request properties to be set on `DefaultHttpDataSourceFactory`. Would that help? I'm really struggling to follow what you're referring to. There are no classes called RendererBuilder, ExtractorRendererBuilder or HlsRendererBuilder in ExoPlayer V2. It shouldn't be much work at all to implement your own `HttpDataSource.Factory` to support this case, as per my second bullet point above, until we add support to the default one. Note: We should also add support for the okhttp and cronet factories.  Closing because the issue lacks nearly all of the information requested in the issue template. We cannot investigate issues such as this one if you do not provide sufficient information.  Please email `dev.exoplayer@gmail.com` with a test stream. We need one that works without using a VPN, however, if you want us to investigate the problem. It sounds like the manifests you're generating are invalid, particularly since you're also seeing the issue using Shaka player. Confirmed the manifests are invalid. Following up via email. Thanks!  This is a proper bug report! Thanks! I'll look into it. This is a duplicate of #1978. The media breaks the spec because each chunk seems to reset the continuity counters, as explained in the referred issue. As a workaround (though not okay for real scenarios) you can force the creation of a new TS extractor for every chunk. That will fix the issue here, but might break a case where a sample is spread across different chunks, which I think is allowed.  Heh, yes :(. We'll fix this. Thanks! Fixed in: https://github.com/google/ExoPlayer/commit/e5bc00e  We still don't have proper support for multiple CC services. This is on the map, possibly Q1. I will use this issue to track this. I am working on adding multiple CC support now. However, the stream you provided doesn't seem to include 608, but rather DVB subtitles. I am speaking about the file provided [here](https://github.com/google/ExoPlayer/issues/2056#issuecomment-263528470). If you have a TS with multiple 608 channels, or even better, an HLS stream, please send a link to dev.exoplayer@gmail.com. This should speed up the process. 

As far as I know, HLS does not support dvb subtitles.  It's not possible to obtain direct links to YouTube videos for playback by ExoPlayer. YouTube's Android Player API is the officially supported way of playing YouTube videos in Android applications. * The IMA MediaSource has been pushed to our `dev-v2` branch [here](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ima). Please note the "known issues" section of the README. It will take a bit of time before it lands in a stable release because those issues still need to be ironed out.
* @rpattabi - ExoPlayer and the YouTube API are pretty different things. I wouldn't say it's duplicated effort. In particular I think YouTube's API uses ExoPlayer internally, and so it's more that YouTube's API is providing a higher level functionality. This is necessary because YouTube has monetization and analytics reporting requirements. I doubt it would be possible to enforce those requirements in a `MediaSource` implementation. In particular, ExoPlayer is so open I suspect it would be fairly trivial for someone using such an implementation just to remove all ads, which would make content producers who've opted to monetize their content on YouTube pretty unhappy.  We don't have any plans to provide an Ffmpeg video renderer. I'm fairly sure it's perfectly feasible to implement one, although note that in many cases software decoders will not be as power efficient or as performant as those provided by the platform. Marking as an enhancement in any case. - Last time I checked, libx264 was only for encoding (not decoding) but FFmpeg does have an H.264/AVC decoder so it makes sense to wrap that. This would also give the option of using other codecs. Some parts of the existing ffmpeg audio extension could be reused.
- The [vp9 extension](https://github.com/google/ExoPlayer/tree/release-v2/extensions/vp9) may be a useful starting point.
- For scaling and color space conversion: it might be necessary to do some experiments to find out whether this is best done inside FFmpeg or separately on its output. In general, the decoder has to keep full-resolution frames in memory. The vp9 extension has [code for doing color space conversion](https://github.com/google/ExoPlayer/blob/8d122a1/extensions/vp9/src/main/java/com/google/android/exoplayer2/ext/vp9/VpxRenderer.java) from YCbCr to RGB using GLES, rendering to a GLSurfaceView. You'll probably find what you're looking for in the `Format` that's provided to the renderer. Specifically in `Format.initializationData`.  As far as I can tell, the chunk is just not encoded correctly. The chunk contains 125 samples. It looks like extraction fails at sample index 62 (i.e. the 63rd sample) because it doesn't start with a valid 4 byte NAL unit length. 1. I added some logging to `FragmentedMp4Extractor`, attached the debugger, added some useful breakpoints etc. No magic was involved :).
1. Nope. The serving side should just not serve broken media under any circumstances.  The issue is similar to https://github.com/google/ExoPlayer/issues/2034, and occurs because the VP8 decoder under-reports its true capabilities. On `dev-v2` you can enable "try and play anyway" behavior with:

```
trackSelector.setParameters(
    trackSelector.getParameters().withExceedRendererCapabilitiesIfNecessary(true));
```

on your `DefaultTrackSelector` instance. Note: The exact reason this video is rejected is because it has an odd height (397px). If you control the encoding side you should probably try and ensure the video dimensions are even for best compatibility. There were issues with Chrome and odd dimension VP8 as well (search Google for: `vp8 odd resolution`), for example. We've enabled this by default in `dev-v2` now, so you wont even need to do it yourself. We're looking to wrap up a release this week.  This is the same as #2007. Please track that issue.

p.s. Please include all of the requested information in the issue template in any future reports. Thanks.  I think it would be necessary to implement a WMA extractor to support this case.

I'm not convinced this is supported by Google Play Music on all devices though, since WMA isn't mentioned on Android's [supported formats](https://developer.android.com/guide/appendix/media-formats.html) page, and as far as I know Google Play Music simply calls through to the underlying platform functionality. I tried playing a WMA file in Google Play Music on Pixel, and playback fails. Are you sure this support isn't on a subset of devices only (i.e. where the device manufacturer has opted to add WMA support as an additional feature)? The good news, I guess, is that if we did implement a WMA extractor then it would provide support across all devices. It's low priority for us, however. @otnvshi - That page doesn't say that Google Play Music supports playback of WMA on Android devices. It's saying is that you can upload WMA, which is different. The file is almost certainly transcoded to MP3 at time of upload, and subsequent streaming playbacks will be of the MP3 rather than the original WMA file. Direct playback of WMA files is not supported on Android, as outlined above.  Do those devices have a Layer II decoder? Can you play the stream using a MediaPlayer-based app started with `adb shell am start -a android.intent.action.VIEW -d "<url goes here>" -t "audio/*"`?

See the [supported formats](https://google.github.io/ExoPlayer/supported-formats.html#sample-formats) page for more information. Did you find that the file plays using MediaPlayer but not ExoPlayer? If so, please provide a link to the content and tell us what device you a testing on. I suspect the device doesn't expose its decoder properly. The sample works fine for me on Pixel, so it doesn't seem like an ExoPlayer issue.  Please provide the information requested in the issue template:

"- A link to content that reproduces the issue. If you don't wish to post it publicly, please submit the issue, then email the link to dev.exoplayer@gmail.com including the issue number in the subject line." This is an issue where we fail to correctly read a metadata string with length zero and no terminating NUL byte. Fixed in https://github.com/google/ExoPlayer/commit/9ea8b02.  As stated in the stack trace, the NullPointerException occurred in com.zeasn.hungama.player.WidevineEpsMediaDrmCallback on line 62 in postToLicenseProxy. This class is not part of ExoPlayer, so you'll need to debug your code and figure out what value is unexpectedly null. Closing since the failure is in application code, not ExoPlayer, as per the response above.  Already fixed in `dev-v2`: https://github.com/google/ExoPlayer/commit/b2222f8cb7bc44d2f69c6037a015923623b7d46a If there's video in this stream then the HLS master playlist is in violation of the spec. From the HLS spec:

> If an EXT-X-STREAM-INF tag or EXT-X-I-FRAME-STREAM-INF tag contains the CODECS attribute, the attribute value MUST include every media format [RFC6381] present in any Media Segment in any of the Renditions specified by the Variant Stream.

In your master playlist `CODECS="mp4a.40.2"` is indicating that only audio is present. The video codec needs to be listed there too. If the video codec were listed, we'd play the video part of the stream correctly.  For context, the most common ways ExoPlayer might play these surround sound formats are:
1. If the MediaCodecAudioRenderer was passed AudioCapabilities that indicate that the HDMI connection supports AC-3, E-AC-3, DTS or DTS-HD output, it will write encoded data to the AudioTrack for decoding on the receiver (passthrough).
1. If passthrough is not available and the device has a decoder for the format accessible via MediaCodec, the audio can be decoded on-device and output as PCM. I've only seen this in a few devices and sometimes the decoder is limited (e.g. only decoding stereo).
1. If you have compiled the ffmpeg extension with the required codec, and have set SimpleExoPlayer to prefer extension decoders (or equivalent), the audio gets decoded to multi-channel PCM. (Note that ffmpeg only gets priority if extension renderers come first, for example, if [EXTENSION_RENDERER_MODE_PREFER](https://github.com/google/ExoPlayer/blob/894ae1a/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java#L117) is set.)

MediaCodecAudioRenderer decides whether to use passthrough based on the AudioCapabilities passed in, so you can override what formats are available there. If you pass in an AudioCapabilities that [only allows stereo](https://github.com/google/ExoPlayer/blob/8d122a1/library/src/main/java/com/google/android/exoplayer2/audio/AudioCapabilities.java#L36), passthrough won't be used. I think that gives the control you're looking for. (Aside: I think a platform setting was added for Android TVs in Android N which allows the user to override whether passthrough is always on or off. That setting should override what encodings are included when AudioCapabilities is constructed based on the HDMI capabilities.)

So you can implement a setting to force passthrough on/off by providing the relevant AudioCapabilities to MediaCodecAudioRenderer, and, if you put MediaCodecAudioRenderer before FfmpegAudioRenderer, passthrough should be used in preference to on-device decoding if possible.

If you need to get further control over what renderers are used (and information on what formats are in the input) you could experiment with providing your own TrackSelector. See MappingTrackSelector/DefaultTrackSelector as a starting point.  One possible explanation for passthrough not working is not passing an AudioCapabilities instance to the renderer, but that seems to be different from the change you're describing (and SimpleExoPlayer does this already, in case you're using that).

Could you paste the exact diff and we can take a look? Thanks. Yes, we check the audio renderer capabilities before trying to use it, so the error is unexpected. Is it possible that some override was in place that caused ExoPlayer to try to use passthrough even if the HDMI connection didn't advertise it (e.g. an Android TV platform setting to force passthrough)? If you can find some way to reproduce the original error, please provide some instructions so we can investigate further.

We don't yet support TrueHD though we might add support soon as [AudioFormat.ENCODING_DOLBY_TRUEHD](https://developer.android.com/reference/android/media/AudioFormat.html#ENCODING_DOLBY_TRUEHD) was added in API 24. I'm not sure about DTS-HDMA. My understanding is that [ENCODING_IEC61937](https://developer.android.com/reference/android/media/AudioFormat.html#ENCODING_IEC61937) can be used to support arbitrary passthrough formats. If the original issue does not need further investigation I will mark this as an enhancement to add support for those encodings. MediaCodecAudioRenderer is used for passthrough. FfmpegAudioRenderer should always decode to PCM.

Support for passthrough of other formats using IEC 61937 is an enhancement. I will use this issue to track that. Thanks! Was the stream extracted from a Matroska file? That container uses the same codec identifier for DTS and DTS-HD. Currently we just assume DTS for Matroska tracks. To make this work properly we'd need to inspect the samples to distinguish these two formats, which is inconvenient in the current model. Does it work if you just specify `MimeTypes.AUDIO_DTS_HD` in the MIME type in [MatroskaExtractor.java](https://github.com/google/ExoPlayer/blob/d74166c/library/src/main/java/com/google/android/exoplayer2/extractor/mkv/MatroskaExtractor.java#L1421)?

I'm afraid we don't have a sensible way to do this other than forking the Extractor at the moment. You can pass your own ExtractorsFactory to ExtractorMediaSource.  This issue tracker is primarily for issues. If you have general questions, Stack Overflow is a more appropriate venue. For your particular question, [this blog post](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a#.3d3wiofh7) is a good place to start.  I don't think negative times are SubRip compliant. Please feel free to refer to a source that says something different, if you've found one. What would it mean for a subtitle to occur during negative time? Thanks! It seems preferable for you to ask whoever writes the tool you're using to fix the bug (i.e. by clipping the output to t=0). Making non-compliant additions to other software as a means to work around the problem is not the best approach, and furthermore, it encourages the creation of more tools that generate non-compliant output.

Unless you can find a reference that says negative timestamps are allowed, I think you should report the issue to whoever created the tool and get a fix on that side. I see what you mean now. You're right. To clarify:

- Whilst we don't want to actively handle the case (i.e. treat it as valid), we don't want to flat out fail playback either. I was thinking this was covered by the "Skipping invalid timing" logic case, but I guess the failure actually occurs in `parseTimecode` and is not currently handled gracefully. We will update the code to handle this by skipping to be consistent with the other cases.
- Re what's really allowed. Yes, I'm equally unsure of whether there exists a definitive spec. The closest to authoritative sources I could find were the [WebVTT timestamp definition](https://www.w3.org/TR/webvtt1/#webvtt-timestamp) (which I think was based derived from SubRip) and [Wikipedia](https://en.wikipedia.org/wiki/SubRip). Both of which appear to not allow negative timecodes.
- As a general point I disagree with the "parsers should be as permissive as possible" comment. Parsers being overly permissive encourages content production tools to be non-compliant, and you end up in a cycle where a large proportion of content is non-compliant and where it's no longer possible to implement a viable parser just by reading a specification and doing what it says. Marking as bug; we will resolve by skipping as per first bullet point. Thanks!  This is fixed already in `dev-v2` and will be in the next release. See:

https://github.com/google/ExoPlayer/blob/dev-v2/extensions/okhttp/build.gradle#L40

Thanks!  We don't plan to merge this change. It would, however, be helpful if more information than just "violation" could be provided. Specifically, where does the code originate from? Thanks! Thanks for the information.

@drhill - Please be aware that under the terms of the CLA, any contributions must be original works. Details of any third party licenses or restrictions must be included in the submission. You can follow the personal/corporate CLA links [here](https://github.com/google/ExoPlayer/blob/release-v2/CONTRIBUTING.md) for more details. Thanks! Just to clarify: The default position for this project is that we only accept original contributions where the contributor (or their company in the case of the corporate CLA) owns the copyright/IP. Although the CLA makes provision for "submitted on behalf of a third-party" contributions, we are unlikely to accept such contributions in practice, even if the source and licenses are accurately stated. We will definitely not accept contributions of any work covered by LGPL/GPL.

@wm4 - If you could refrain from making drive-by comments about "Google things" based on a pull request that was not created by us, that had a signed CLA, and that we at no point indicated we were going to merge, that would be nice. Thanks for your compliment regarding our speed of development, however! We are of course grateful for you pointing out the issue with this pull request, and please continue to do this if you see other pull requests that you believe are similarly problematic. This is valuable information. Please include a reference to the original source when doing so, so that we can take a look.

Regarding the latter point: Making drive-by comments insinuating poor development practices is not pointing anything out. It's at best unnecessary, particularly if you don't care and haven't taken time to verify your statements. Since you have no issue with this particular project, this doesn't seem like the correct place to be making such points, either. Thanks!  The sample stream works fine for me over both WiFi and mobile data. It's unlikely to be an ExoPlayer issue. What you're seeing is a [http 503 response status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#5xx_Server_Error), which suggests something about your mobile connection is causing the issue. Are you sure there aren't any restrictions on mobile data enabled on the device (e.g. no background data + you're running your app in the background). In such cases I imagine the platform could choose to simulate a 503 status response as a way of indicating that connectivity isn't available.

As an aside, it doesn't look like you're using `OkHttpDataSourceFactory` to me. The stack trace shows `DefaultHttpDataSource` is being used. Closing since this is unlikely to be an ExoPlayer issue.  We're only back-porting bug fixes, not features. And only the subset of bug fixes that can be easily back-ported (i.e. we're not back-porting bug fixes that would require also back-porting other structural changes).

You might want to look at migrating to ExoPlayer V2. You may find you no longer need the ExoMedia layer, since ExoPlayer V2 provides higher level APIs of its own (e.g. SimpleExoPlayer, SimpleExoPlayerView).  This doesn't look related to ExoPlayer. Someone's reporting a similar problem in a context unrelated to ExoPlayer [here](http://stackoverflow.com/questions/32679260/i-cant-start-android-studio-on-windows-8). You should probably ask this question on Stack Overflow or on the Android Studio issue tracker.  @drhill Thanks for the contribution! Out of curiosity, what is the failure mode without the workaround? Do some tracks just not get played, or does the AudioTrack throw, or something else? Some further clarification on this would definitely be appreciated; let us know when you're able to try it out again! How were you able to determine that 4 different channel configs were broken, given only one test video ;)? We're pretty keen to get a fix in, but we'd like to make sure it's targeted accurately. Thanks! Looking better! Re the earlier point about whether this affects passthrough or whether it also affects standard PCM output:

- I'm assuming your test results above are for the passthrough case where ffmpeg isn't used. Is that correct?
- For the case where you decode to PCM using ffmpeg that you state is also problematic: Are you sure this isn't a more general problem elsewhere? In particular, this could also be caused by `FfmpegDecoder.getChannelCount` returning a channel count that differs from the actual number of channels it produces on the output side. If you could verify whether the issue is Shield specific when using this path, or whether the same issue is present on other devices too, that would remove ambiguity.

Thanks! Could you clarify what error is thrown (a stack trace would be helpful)? Thanks!  There are a whole bunch of useful setX methods on SubtitleView (looks like there are 9 such methods). It seems inconsistent to expose only one of them, but exposing all of them is going to bloat SimpleExoPlayerView quite a bit. How about a `getSubtitleView` method that just returns the view? That doesn't feel ideal either, but is perhaps the "less bad" option? We'll try and merge some of these at roughly the same time (it makes handling them a little easier for us), but this looks ready to go when that happens. Thanks!  I don't understand why this is needed. The `ComponentListener` is already passed to the `buildXRenderers` methods where needed. See [here](https://github.com/google/ExoPlayer/blob/894ae1a310cc2e1fe5ba59eb56c7fbeb751036d5/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java#L625). What's the use case where you need some other way to access it?  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> Thanks! I will get this merged into V2 as well.  On the serving side I don't think you need to do anything fancy. Hosting an mp4, mkv or webm file in a standard way should be fine. You should check your server supports partial range requests, but beyond that I don't think you need to do anything special

On the client side, we don't provide out-of-the-box support for things like downloading part of the media to disk beforehand. So that's something you'd need to look at building yourself, if you really need to do this. In case it wasn't clear, when I said you don't need to do anything fancy, I explicitly meant you don't need to be looking at anything related to progressive downloading, at least as it's described [here](https://en.wikipedia.org/wiki/Progressive_download). Comments like

> The offset parameter name differs for various servers so it must be specified in player settings.

on that page suggest what it's describing isn't really standardized. Just hosting the file in a normal way on a server that supports partial range requests is the best thing to do.

I'd be pretty surprised if Amazon servers don't support partial range requests by default. It's a pretty standard thing. If it weren't supported then their servers wouldn't support something as simple as resuming a partial download, for example. I don't know what an "active server" is, but I doubt you need one. You're probably just not using a suitable container format for your media (e.g. mp4, mkv or webm). Search for "seek" in our [supported formats](https://google.github.io/ExoPlayer/supported-formats.html) page for more information.  First, please try fixing the duration declared in the manifest (~60 secs) to be closer to the actual chunks duration (~6secs, at least for chunks 1 and 3. I haven't tried the others). After that, please open a new issue describing exactly what the steps to reproduce are, and what the expected result is.  Without a working way to reproduce this, there's really nothing we can do to help. I don't really understand why you'd ever get a 304 unless your server is misconfigured or unless you've made an invalid modification to the ExoPlayer client code.

Please provide a non-georestricted test stream that reproduces the problem in the ExoPlayer demo app if you wish for us to take a look. Closing due to inactivity.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> CLAs look good, thanks!

<!-- ok --> Urgh, sorry. This needs to go into `dev-v2`, not `release-v2` branch. Please open a pull request to that branch instead, and we'll get it merged. Thanks!  I am going to assume that by mpeg-ts you mean a raw ts file using an extractor media source. This makes sense considering onLoadCompleted implements an AdaptiveMediaSourceEventListener method which only applies for adaptive media. Even if it did, I don't think onLoadCompleted would provide any generally useful information as it doesn't handle the download throttling done by extractor media source. You should also clarify if you are looking for the download bandwidth or the TS flie bitrate (as defined by the PCR, for example). I assume the first. You can have a look at BandwidthMeter and see how it's done. I hope this provides some insight. You should try asking this kind of questions in a general Q&A exchange. The issue tracker is reserved for bug reports and feature requests, mainly.  It's quite complicated in practice because there are quite a lot of cases where you would want to exceed the display resolution. For example if two resolutions are available, one of which is far less than the display resolution and the other of which is a little bit larger. As another example, all use cases involving UIs that don't fit the video (e.g. UIs that clip instead, 360 video, VR). Conversely, in many other situations you probably want to constrain to something below the resolution of the display. For example any UI where the video is windowed, when streaming over a metered mobile network and so on. Whether the orientation of the application may change is also relevant.

Given these complexities, we leave it to the application to decide what the right thing to do is. You can pretty trivially constrain to the display viewport using `DefaultTrackSelector`. Specifically, use `DefaultTrackSelector.getParameters` to get the current parameters. Then call methods like `Parameters.withViewportSizeFromContext`, `Parameters.withMaxVideoSize`, `Paramaters.withExceedVideoConstraintsIfNecessary` to configure the behavior to be to your liking. Then pass the resulting parameters back to the selector via `DefaultTrackSelector.setParameters`.  Issue does not provide sufficient information.  What CPU usage do you see using `MediaPlayer`? Note that you need to add the usage from the app and from the `mediaserver`, which is a separate process. The `ExoPlayer` case most likely has higher usage in the app and lower usage in `mediaserver` vs `MediaPlayer`.

As additional questions: What do you expect the CPU usage to be, and why are you concerned by the usage? CPU usage isn't an issue by itself; it's there to be used after all. It only really becomes a problem if there's a measurable knock-on effect, for example higher power draw. `ExoPlayer` does draw more power than `MediaPlayer` for audio-only use cases on some devices, but this is because `MediaPlayer` has an unfair advantage that's not exposed through a public API for us to use (yet!). We don't think CPU utilization is a major factor. Power consumption is also in line with some other (non-`MediaPlayer`) based solutions. Closing due to inactivity.  It would not be hugely difficult to make a variant of `ConcatenatingMediaSource` that did this, provided all of the sub-sources are on-demand with known duration. It's not something we support currently, however. Marking as an enhancement. If you were to use `ConcatenatingMediaSource` as a starting point, the adjustment that you'd need to make is to change `ConcatenatedTimeline` so that it exposes a single window containing all of the individual periods, rather than a separate window for each of the sources that you've concatenated. It's probably quite fiddly to get this just right. The Javadoc for [`Timeline`](http://google.github.io/ExoPlayer/doc/reference/index.html?com/google/android/exoplayer2/Timeline.html), `Timeline.Window` and `Timeline.Period` would be a good place to start.

It may also be possible (and quite likely easier) to support this just at the UI layer, by editing `PlaybackControlView` to consider the progress bar as extending over all windows, rather than just the current one. You can use `ExoPlayer.getCurrentTimeline` to get the media `Timeline`. Once you have the `Timeline` you can query the number of windows and their properties (including duration). Hence you can iterate over the windows exposed by the timeline and add the durations. If it's possible that one of the pieces of media might have an unknown duration, you should take care to handle this case correctly. The end result should also be unknown in that case. We recently pushed support for showing a multi-window time bar in [DefaultPlaybackControlView.setShowMultiWindowTimeBar(boolean)](https://github.com/google/ExoPlayer/blob/9d20a8d41c0cc6cd01b20bd72f3bbfcf11e49cae/library/ui/src/main/java/com/google/android/exoplayer2/ui/PlaybackControlView.java#L353) to `dev-v2`. The implementation iterates over all windows in the timeline when calculating a position/duration, rather than just using the current window.

Important caveat: this mode relies on knowing the durations of all periods in the timeline, so, for example, it won't work properly when using `ConcatenatingMediaSource` until all periods have been prepared. As a workaround, if you know the durations of the constituent sources in advance you can wrap each one in a `ClippingMediaSource`, so that its period duration is known immediately. We might provide an alternative way to do this later (TBD).  Thanks. Happy to merge this once the comment is addressed. Note that you need to sign an appropriate CLA before we can merge the change. See [here](https://github.com/google/ExoPlayer/blob/release-v2/CONTRIBUTING.md). Please let us know once you've done this. Thanks! Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->  Can you confirm that normal seeking (prior to having reached the ended state) work successfully, and that the player does indeed transition properly into the ended state? Thanks!  Yes If you have an issue, please file a new issue including all of the information in the issue template. We cannot efficiently investigate or answer questions without the requested information. Thanks!  This is an intentional change that is part of adding support for seeking in HLS live, and brings the reported position in line with DASH and SmoothStreaming. `getCurrentPosition` reports the position within the current seek window. Since the seek window is itself moving with time for a live playback, the position will not increase indefinitely. You can read about timelines, windows and periods [here](http://google.github.io/ExoPlayer/doc/reference/index.html?com/google/android/exoplayer2/Timeline.html).

If you want a time that increases indefinitely then you want the time relative to the start of current period, rather than the current window. You can calculate this in several ways using the `Timeline`, one of which is:

```
long position = player.getCurrentPosition();
// Adjust position to be relative to start of period rather than window.
Timeline currentTimeline = player.getCurrentTimeline();
if (!currentTimeline.isEmpty()) {
  position -= currentTimeline.getPeriod(player.getCurrentPeriodIndex(), period)
      .getPositionInWindowMs();
}
```

Where `period = new Timeline.Period()` is a holder object that you probably want to allocate once and re-use.  I would have thought you should do the request to obtain the media URL before you create the player (i.e. entirely in your own code). Can you use a custom [DataSource](https://github.com/google/ExoPlayer/blob/f8ed4cf/library/src/main/java/com/google/android/exoplayer2/upstream/DataSource.java) to do this? You could provide arbitrary identifier URIs for content to MediaSources, then resolve these in your custom DataSource's open method, which would then then delegate to e.g. DefaultDataSource. You could also make your own `MediaSource` as a solution to this problem. The DASH, SmoothStreaming and HLS `MediaSource` implementations all make "top level requests" of this kind to retrieve the manifests that contain the individual stream URLs. So they're in some ways similar, albeit quite a bit more complicated than what you'll need.  Closing because it ignores the issue template. The issue template provides a *very* clear list of six bullet points indicating what you should include to allow us to efficiently investigate your issue. This issue addresses two of the six bullet points only. It's just not an effective use of our time to try and guess what you mean and fill in all the gaps. If you want us to investigate an issue, please fill out the issue template properly. Thanks! As a general notes:

- We don't know what "correct" means because we don't know what behavior you're expecting.
- We don't know how the behavior you observe differs from the expected behavior because you haven't told us, hence we cannot easily tell if we're reproducing the issue or not.
- We don't know what content to use to try and reproduce the issue because you haven't included it in your description.
- Please include a bug report as requested in the issue template.

Please stop filing issues that do not contain sufficient information.  Closing because it ignores the issue template. We're not going to spend time guessing things like what version of ExoPlayer you're using. If you want us to investigate an issue, please fill out the issue template properly. Thanks!  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---

- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla --> We will backport the same fix that was available in V2 to V1. Please consider moving to V2 to get bugfixes as soon as possible and a superset of features.  SubtitleView supports a bunch of style customisation options, and is what you should be using. Closing this issue because it doesn't contain a concrete question or a detailed description of an issue.  See https://github.com/google/ExoPlayer/issues/2974.  Without any information about the underlying cause of the issue to which you refer, it's impossible to say. Sorry!  Thanks for reporting. A (probable) fix will be provided in the next push. Please, reach us out again if this still gets reported after the fix.  This is almost certainly content specific. It's possible the content is malformed. Unless you provide us with sample content that reproduces the issue, as requested in the issue template, we can't investigate further.  ExoPlayer assumes that when you take the drm init data (i.e. the PSSH) from any stream in the content and make a license request with it, you'll get back all of the keys required to play the content. That means if you've encrypted audio and video using different keys, a license request using the drm init data from one of the streams should return both keys required for playback. It shouldn't be particularly hard for a real license server/proxy to do this, given it will already have to be doing things like checking that the user has purchased whatever content they're trying to play, etc. There are also some considerable benefits, for example being able to maintain a single DRM session for both audio and video rather than having to maintain separate sessions for each one, and allowing the client to make fewer requests. Nevertheless, it's fair to say not everyone likes this decision; there's a detailed discussion [here](https://github.com/google/ExoPlayer/issues/1824).

For your particular case I think it's just that you've used different keys, but since the Widevine test server doesn't know what you've done it's unable to respond with both keys as a real license server/proxy would. If you use the same key for both audio and video, or implement a real license server/proxy to behave as described above, then playback should work fine.

Asides:
- If you disable video or audio in the demo app during the first 10 seconds, you'll find playback completes successfully for the stream that's still enabled if that one happens to be the one that the PSSH data was retrieved from for the license request.
- It's also possible to provide a custom `DrmSessionManager` implementation to support the separate keys case where the license server doesn't behave as above. The default `StreamingDrmSessionManager` implementation returns the same `DrmSession` from each `acquireSession` call, which is effectively making the assumption that keys for all streams will be loaded into the single session. You could provide an implementation to return a separate `DrmSession` for each call instead, which would not make such an assumption.  It looks like the DASH stream you're trying to play is invalid. Since you've opted to ignore the issue template and not provide any of the requested information, we are unable to assist. Please file an issue including the requested information if you wish for us to investigate further. Thanks.   We're not able to provide legal advice. I can however state some facts that may assist you, which are accurate as of the time of writing:

- The core ExoPlayer library does not contain an AAC decoder. It calls into Android's `MediaCodec` API to access a decoder provided by the underlying OS. Including the core ExoPlayer library does not result in your application binary containing an AAC decoder.
- Building and including some of the ExoPlayer extensions, for example the [FFmpeg extension](https://github.com/google/ExoPlayer/tree/release-v2/extensions/ffmpeg), may result in decoders being included in your application binary.  Please search for existing issues. Specifically, https://github.com/google/ExoPlayer/issues/1152. It's likely this case is the same. The sample "works" in 1.5.8, but if you look at logcat you'll still see that error being spammed repeatedly into the logs. The reason it still plays successfully is because there was another bug that would then cause the extractor to retry from the wrong position in the data. This bug has since been fixed.

The issue is exactly the same case as in #1152. It looks like the content provider has at some point inserted extra UUID atoms that are 61 bytes long between the `trun` and `mdat` atoms in the streams. To do this in a valid way, they should have also updated the `data_offset` in the `trun` atom to be 61 larger than it was before, but they haven't done this. Hence the stream is invalid, and you should report this to the content provider.

```
atom	size
---------------------
trun	1712	<-- Contains data_offset whose value is 61 smaller than it should be.
uuid	44
uuid	61	<-- Hypothesis is that this atom was inserted later.
mdat	1873
```

You can pretty easily work around this issue by commenting out [L940-L953](https://github.com/google/ExoPlayer/blob/r2.0.4/library/src/main/java/com/google/android/exoplayer2/extractor/mp4/FragmentedMp4Extractor.java#L940) (inclusive) of `FragmentedMp4Extractor` (line numbers accurate for `r2.0.4`).  We keep audio fixed and adapt the video only. It would be possible (but more complicated) to adapt both at once. In V2 you could do this through implementing a custom `TrackSelection`. We may support this directly at some point, since being able to adapt audio at the same time as video is pretty important for countries where low bandwidth connections are common. We don't have any short term plans, however.  Live vs Event differentiation is not supported, yet. For Live vs Event you can check the current Timeline, but that's not HLS specific.

For playlist type support you can track #805. Have a look at the `onTimelineChanged` callback (`Exoplayer.EventListener`). The second argument recieved should be an instance of HlsMediaPlaylist. There you can look at the `playlistType`.  Please see https://google.github.io/ExoPlayer/supported-formats.html. If you were to do this yourself, you'd probably need to implement some kind of binary search into the media until you find the seek position.  You'd need to provide at least a full bug report for us to help debug that. Probably you'll need to share your custom class somehow, or provide an equivalent class that we can test in the demo app to reproduce the issue. Looking at your implementation, I'm not convinced that your approach is best suited to the problem you're trying to solve. Rather than requiring the application to proactively append child sources, I think you should instead just expose a timeline consisting of some arbitrarily large number of periods. Then when `createPeriod` is called for a period whose child source does not yet exist, you should ask the application to build you one. Conversely, when you're holding a child source whose periods have all been released, you should release it and null out any references you hold to it.

The suggested approach is basically the opposite of what you have now, with the source pulling data from the application rather than the application pushing data into the source. I think you'll find this approach works much better.  It's unclear exactly what you mean here (and the bug report attached doesn't look like a full bug report, as would be obtained from `adb bugreport`). The video plays fine for me in both `1.5.12` and `2.0.4` demo apps.

Please could you clarify what you mean by "freeze"? Please also provide logging and a full bug report when reproducing this issue with the demo app, rather than your own, since we know what logging to expect in that case and not in the case of your app. Thanks! @Nelsun - You should ask your HLS provider to include RESOLUTION attributes in the master playlist, or use a provider who does this. Whilst technically optional in the HLS spec, inclusion of RESOLUTION attributes has been recommended for over three years now. If the master playlist contains these attributes then the player will not need to reconfigure the decoder in this way. If the master playlist doesn't contain them then the player has no way of knowing what the maximum resolution is in advance of switching to the highest resolution variant, and may have to perform a (potentially quite expensive) buffer re-allocation when this occurs.

@gbadoual - You'll likely find including RESOLUTION attributes in your master playlist also resolves the issue for your stream.

@Nelsun - I'm not sure I follow your null pointer exception description. It sounds like something we should definitely fix though. Could you provide more details to help? As I understand it:

1. Resolution of stream changes. canReconfigureCodec returns false and codecReinitializationState is set to REINITIALIZATION_STATE_SIGNAL_END_OF_STREAM.
1. feedInputBuffer will queue an input buffer with BUFFER_FLAG_END_OF_STREAM and will update codecReinitializationState to REINITIALIZATION_STATE_WAIT_END_OF_STREAM
1. In drainOutputBuffer, a the buffer with BUFFER_FLAG_END_OF_STREAM is read. This causes processEndOfStream to be called.

It's here that I get lost. processEndOfStream should re-initialize the codec when it calls maybeInitCodec, so codec shouldn't end up null after this call. Could you take a look and see why this isn't happening? The only thing I can think of is if the surface isn't valid, but that sounds pretty strange? Please also provide a stack trace for the null pointer exception. Thanks. I'm pretty sure there is a check somewhere in V1. The reason playbacks are able to avoid the codec re-instantiation in most cases is because HlsChunkSource hard-codes 1080p in the case that resolution tags were missing ([here](https://github.com/google/ExoPlayer/blob/r1.5.13/library/src/main/java/com/google/android/exoplayer/hls/HlsChunkSource.java#L646)). I think that approach is quite problematic, since it can cause unnecessary playback failures on devices not supporting resolutions that high.

We should probably do as you suggest for V2. A few caveats:

- I don't think there is a single maximum resolution in all cases. For example a decoder might support 1980x1080 and 1080x1980, but not 1980x1980. 
- I don't think it's wise to set at the very maximum value. It seems likely that the larger the value you choose, the more likely it is that decoder instantiation may fail (e.g. due to memory constraints). It seems reasonable that if someone's delivering >1080p they should probably be up to date enough with the HLS spec to be setting resolution tags, for example. > I think we can just avoid setting the Max values when RESOLUTION field is missing. Platform decoders will allocate memory for the maximum supported resolution by default.

I don't think that's correct. As I understand it decoders operate in two modes. Decoders that operate in "legacy" mode they allocate for (width,height) or (maxWidth,maxHeight) if specified, and playback *will* fail if those dimension are exceeded. This is why it's not valid to avoid setting the max values in such cases. Decoders that operate in "non-legacy" mode are able to dynamically re-allocate their output buffers when adaptation occurs, and so don't fail in this case. I suspect these decoders might ignore (maxWidth,maxHeight) even if specified, although I'm not certain about this. Anyway, due to "legacy" mode decoders, not setting the max values and also not re-initializing the decoder when the resolution increases is not an option.

We have a change under review that will set (maxWidth,maxHeight) to something sensible, based on standard video resolutions, the aspect ratio of the initial video and the capabilities of the decoder. It'll get merged into `dev-v2` today or next week. The change above sets the maximum resolution to something sensible when the variant resolutions are unknown. Please give it a try on the dev-v2 branch. We're planning to cut a 2.2.0 release containing this fix sometime next week.  This is not something we'll be supporting. It doesn't seem appropriate to be writing to the file simultaneously whilst it's being read. What should the player do if it gets to the "end" of the file, given it has no way of knowing whether you'll append more data to it, for example?

If you really need to do this you should write a custom `DataSource` implementation that's capable of handling this case, and with the assumptions programmed in that you want to hold. You can use `FileDataSource` as a starting point, so it's probably not that much work. The `DataSpec`'s length is probably already unset for this use case. The way the rest of the player finds out about the actual length of the data is through the value returned from `DataSource.open`, so you need to make sure you return unset from your `open` implementation rather than the length of the file, which is what `FileDataSource` returns.  There's no real need to create a player to obtain offline DRM keys, but the implementation for actually doing this isn't trivial. It's something we intend to support in the future. Marking this as an enhancement. Actually, this is already tracked by #876. Closing this one.  Is the issue specific to the content you're trying to play, or does it happen with all content? Try playing [this video](http://redirector.c.youtube.com/videoplayback?id=604ed5ce52eda7ee&itag=22&source=youtube&sparams=ip,ipbits,expire,source,id&ip=0.0.0.0&ipbits=0&expire=19000000000&signature=513F28C7FDCBEC60A66C86C9A393556C99DC47FB.04C88036EEE12565A1ED864A875A58F15D8B5300&key=ik0) in your setup, which is just a plain MP4, and see if it reproduces in that case. I'm able to reproduce the issue (it actually reproduces with streams of any type). The problem occurs because we're not waiting for the next key-frame before rendering to the new surface, which we should be doing.

Fixing the issue will remove the corruption, but it will also mean that the switch wont be immediate any more because we'll be discarding frames from the point at which the switch occurs up to the next key-frame. The alternative is to re-buffer from the previous key-frame, which isn't great either.

In general instant surface switching is quite difficult due to the way the decoder is tied to the surface at the platform level. There are (non-trivial) solutions. See #318 for details. The change above makes V2 behave in the same way as V1 does. Which isn't ideal, as noted in my Nov 29th update to this issue above. It does at least remove any reason to not update from V1 to V2, since there is no longer anything that could be considered a regression here.

I'll go ahead and close this. Please give it a try on the dev-v2 branch. We're planning to cut a 2.2.0 release containing this fix sometime next week.  No, this is not supported in V2. Someone would have to implement an ExoPlayer AVI extractor. It's unclear there's really any significant demand for it, however.  I don't think the content is valid. How can all three video representations have the same initialization segment if they're different profiles and resolutions, as is the case in this manifest? The initialization segment contains data like the video resolution, for example, which is definitely different in each representation:

```
<AdaptationSet segmentAlignment="true" bitstreamSwitching="true" maxWidth="1920" maxHeight="1080" maxFrameRate="24" par="16:9" lang="und">
   <SegmentTemplate initialization="sintel_set1_init.mp4"/>
   <Representation id="1" mimeType="video/mp4" codecs="avc3.42c01e" width="640" height="360" frameRate="24" sar="1:1" startWithSAP="1" bandwidth="621839">
    <SegmentTemplate timescale="12288" media="360p-700k-baseline30-$Number$.m4s" startNumber="1" duration="24576"/>
   </Representation>
   <Representation id="2" mimeType="video/mp4" codecs="avc3.4d401f" width="1280" height="720" frameRate="24" sar="1:1" startWithSAP="1" bandwidth="1035490">
    <SegmentTemplate timescale="12288" media="720p-1300k-main31-$Number$.m4s" startNumber="1" duration="24576"/>
   </Representation>
   <Representation id="3" mimeType="video/mp4" codecs="avc3.640028" width="1920" height="1080" frameRate="24" sar="1:1" startWithSAP="1" bandwidth="2051075">
    <SegmentTemplate timescale="12288" media="1080p-2600k-high40-$Number$.m4s" startNumber="1" duration="24576"/>
   </Representation>
</AdaptationSet>
``` Confirmed the content is broken. Each video representation should have its own initialization segment. The single shared segment (`sintel_set1_init.mp4`) appears to be for a 360p stream, so it's invalid to use it for the 720p and 1080p representations too.

It's possible that some decoders manage to do the right thing anyway, but it's not required that they do so. On Pixel the content plays correctly, but the reported resolution is still incorrect because it's set incorrectly in the initialization segment when playing the 720p and 1080p representations.  Well, someone has to be posting (or similar) to generate position updates. You're effectively stating that this code should be implemented by ExoPlayer rather than in client code, however logically it's the client that knows the points at which it wants a position update, which may be non-periodic, periodic, periodic at different rates for different pieces of the client etc. Hence it makes sense for the client to be in charge of generating any events that it needs. It's really not that hard or error prone to trigger an event at periodic intervals. The application developer is free to implement it in whatever way they want, if they think some alternative approach is clearer ;).

Note also that ExoPlayer V2 does a lot more for you than V1 does, so if you're looking for an easier API you should probably think about migrating. > Since the player manages its own getCurrentPosition property it already knows when to emit onPositionChanged events

The player updates its internal position at a very high rate, because this is required for media playback. I doubt you'd want to be spammed with an event every 10-20ms.

> Constantly checking getCurrentPosition (e.g. with the help of Timer().scheduleAtFixedRate) is a very inefficient solution. 

I'm not really sure what this means. You should just query it at exactly the rate you want (e.g. every second). It's in no way inefficient (although I doubt `Timer` is the right thing to use here; `Handler.sendMessageDelayed` seems more appropriate). > I could use RxJava's throttle method to set the required rate though.

Ok, but at this point your request is basically equivalent to asking that we poll you every 10-20ms so that you can ignore us 99% of the time. This is far less efficient than just posting something to yourself to perform updates at the rate you actually need.

> I said it is "inefficient" because I thought about the case when the player is stopped and the client continues polling the current position. So the developers also need to monitor the player's stopped states to cancel the timer or remove handler's delayed messages, right?

You could do that, and it wouldn't be particularly hard, yes.

> Timer was used just as an example. However, I'm curious, why do you think Handler is more appropriate here? Is it related to the code style or the performance?

Search "Timer vs Handler Android" or similar on Google, and you'll probably find that consensus is to use Handler. Aside from any pros/cons, Handler is the "Android way".  Changes to support this were added underneath `MediaDrm` in Android Nougat. We also need to make some changes in ExoPlayer on top of that. > Does this mean that even when you add support in ExoPlayer, it will only apply to Nougat and up?

Yes, which is partly why we're not prioritising this work right now. Widevine would need to provide a standalone library for Android to get this working on older versions of Android, which they don't do currently. Note that even if they were to do this, it would likely only be L3 security, which may or may not be acceptable for your use case.

> Without the changes to MediaDrm, is it possible to implement it using transmuxing? Widevine on iOS is doing something similar, in the opposite direction -- transmuxing DASH chunks to HLS TS chunks.

What you refer to as transmuxing must presumably include decryption. So the same answer as above applies; Widevine would need to provide a library for this to be supported.

DASH/FMP4/CENC (in "cenc" mode) is the recommended approach for delivering protected content to Android devices. Unfortunately Apple mandated use of the `cbcs` CENC scheme for fMP4 in the HLS RFC. Support for `cbcs` was also only added in Nougat. Prior to Nougat the `cenc` CENC scheme is the only one supported. The work to support HLS/WV/CENC(cbcs scheme) and HLS/WV/CENC(cenc scheme) is probably more or less the same, with some small caveat around propagating the right scheme information. However this work is not a priority for us at the moment because the standard compliant cbcs scheme is only going to work from Nougat.

More generally, why would HLS/WV/CENC(cenc scheme) be a sensible thing to do? Such streams presumably wont work on iOS devices. If the streams you're generating are for Android devices only, you should really just generate an MPD instead. Choosing to attempt HLS/WV/CENC(cenc scheme) is attempting to use a worse, less well supported and non-standard-compliant approach that's pretty much equivalent to DASH anyway; why would you not just use DASH at that point ;)?  Reverse engineering websites to extract content URLs is not in the scope of this issue tracker.  It's unclear what this question is asking. ExoPlayer doesn't have an event called `OnPrepared`. ExoPlayer V2 doesn't even have the concept of a preparing state. In general, it's unclear what "preparing" actually means. It might be clearer if you instead phrase your question in terms of exactly what you're trying to achieve. Thanks!
 You should [add a listener](http://google.github.io/ExoPlayer/doc/reference-v1/com/google/android/exoplayer/ExoPlayer.html#addListener-com.google.android.exoplayer.ExoPlayer.Listener-). Its [onPlayerStateChanged](http://google.github.io/ExoPlayer/doc/reference-v1/com/google/android/exoplayer/ExoPlayer.Listener.html#onPlayerStateChanged-boolean-int-) will be invoked when a state change occurs.

Note that if all you're trying to do is start playback as soon as possible, there's no need for you to wait until preparation happens for you to tell the player that you want it to start playback. Just call `setPlayWhenReady(true)` straight away, and it'll start playing as soon as possible. If you really need to do this, it seems pretty easy for you to track whether you're preparing or not. Why can't you just do something like:

```
boolean isPreparing;
...
isPreparing = true;
player.prepare(...);
...
```

```
@Override
public void onPlayerStateChanged(boolean playWhenReady, int playbackState) {
  if(isPreparing && playbackState == ExoPlayer.STATE_READY){
     // this is accurate
     isPreparing = false;
  }
}
```

It's still unclear why you need this in the first place. What do you actually want to _do_ when you see that the player has been prepared?  I see the same error using r1.5.12 as well. It appears the server is rejecting the requests based on the User-Agent. I was able to play the stream successfully in both r1.5.12 and r2.0.4 by using a different User-Agent. Specifically in r2.0.4, by replacing [this line of code](https://github.com/google/ExoPlayer/blob/r2.0.4/demo/src/main/java/com/google/android/exoplayer2/demo/DemoApplication.java#L36) with:

```
userAgent = Util.getUserAgent(this, "ExoPlayerDemo").replace("ExoPlayerLib", "Blah");
```

I've no idea why the server would be configured this way. If it's not your server and you've released something that's sending it a significant amount of traffic, then it's possible they've crudely blocked your requests by returning 401 for all requests that have `ExoPlayerLib` in their User-Agent.

In any case, this isn't an ExoPlayer bug, so closing.
  I am assuming you are speaking about the stream type for [DTS-HD Master Audio](https://en.wikipedia.org/wiki/DTS-HD_Master_Audio). I say this because of the mentioning of **descriptor** at the end of the title. Unfortunately there is not much I can do to help you. We support SCTE 35's splice info section and it is signaled with stream type 0x86 as well (See SCTE 35 - 2016, Section 9.5.1). Since we don't support the mentioned audio format(does it use its own container format? Do we already support its container format? You would do well to provide a sample stream), it's not something we can do much about: It's in the _user private_ stream_type values interval (0x80-0xFF).

If you need to support this, I guess the best option is to implement your own TsPayloadReader.Factory, catch any streamType == 0x86 requests and redirect any other to the default one. Can you think of any better solution?

The question now is how to discriminate them if both streams (splice info and DTS-HD) are present in the same program. I assume something can be done by using the 0x06 stream type, but that's in the muxer side. It would be nice to find out.
 But that's what the factory is for: creating your own streamType->TsPayloadReader mappings. What are you suggesting? I will have a look at the samples later to see whether we can extract them. 
 Is there anything we can do about this? Can we close otherwise? **For any future similar issue:** You should implement your own factory if you need a different behavior from the one provided by the default implementation. You may want to wrap it, if the changes are small.  See the documentation for handleBuffer. The method may partially consume the buffer that is passed in, in which case the same buffer will be passed on the next call to continue writing it. Then isNewSourceBuffer will be false, so the passed in buffer will be used (not your replacement buffer).

In this specific case, it looks like your added code will advance the position of the input buffer to the end, so no bytes will by written on the second call, even if it your replacement buffer was only partially consumed on the first call.
  There are arguments that mobile carriers prefer this kind of traffic pattern over their networks (i.e. bursts rather than drip-feeding). Which is an important consideration given ExoPlayer is used by some very popular services. It may also be more battery efficient.

Whether these arguments are still valid is something we should probably take another look at fairly soon, since the information we used when making this decision is 3-4 years old now. We should also figure out whether we should adjust the policy dynamically based on network type (e.g. even if the arguments are still valid, they may only hold for mobile networks and not for WiFi).

Let's use this issue to track taking another look.

@rlatapy-gpsw - You can't have "unlimited" buffer. Devices are fundamentally constrained. If you want this kind of functionality you probably need to implement media downloading. ExoPlayer doesn't implement download functionality (at least for now), but will happily play downloaded media once you've downloaded it. In any case, your question is different, let's not use this issue for it.
  This enhancement is the same as #1988.
  Thanks for the report. I think the problem is more that `nextSubtitleEventIndex` needs resetting to `INDEX_UNSET` in a few places.
 This is fixed in `dev-v2`.
  Closing because insufficient information was provided. Please file a new issue containing all of the information requested in the issue template if you want us to investigate further.  Could you please provide the mentioned file? If you don't want to post it here, you can send it to dev.exoplayer@gmail.com.
 The test video plays fine on devices I have, so this is likely a device specific capability issue (i.e. the same issue as #2034). I'll leave this issue open for now and follow up when we have more logging in place so that we can capture logging from your test device and use it to put a workaround in place.
 Please could you:
1. Check out the latest `dev-v2` code.
2. Try and play the affected video.
3. Paste the output of `adb logcat -s "MediaCodecInfo"` here. You should see a bunch of lines containing "FalseCheck" the detail exactly why the stream wasn't played.

Thanks!
 @Andoctorey - You need to test with the very latest `dev-v2` code to see the logging. The output you've suggests you're using a slightly older version. For vertical videos, which both videos in this issue are, this should be resolved by: https://github.com/google/ExoPlayer/commit/2e3ffe1e94a16d6ca1e5c09f818969cbcd531abe

Marking as fixed. Please let us know if you still see the issue on the very latest `dev-v2` branch. Thanks.  You can have a look at the Demo App to see how to instance a ExtractorMediaSource. Unfortunately, the issue tracker is not intended to be used for general general Q&A. If you are not able to play your media in the demo app (as Avetri says, #1488 is possibly related) you will need to provide all the information requested in the issue template. Make sure you don't create a duplicate, please.
  There's quite a lot of information about portrait videos and rotation in https://github.com/google/ExoPlayer/issues/91. Have you read through that thread? If so and if that doesn't answer your question, you'll need to provide more information (as explicitly listed in the issue template) for us to help.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  The player is trying to open the local URL "file:///":

```
com.google.android.exoplayer2.upstream.FileDataSource$FileDataSourceException: java.io.FileNotFoundException: /: open failed: EISDIR (Is a directory)
```

Please share a link to the manifest and we can take a look.
 @ranjan000 Please could you email the link to a non-geo-restricted stream to dev.exoplayer@gmail.com?
 I get an "Access denied" error when trying to access the provided stream. Specifically, I get the following response:

```
<HTML><HEAD>
<TITLE>Access Denied</TITLE>
</HEAD><BODY>
<H1>Access Denied</H1>

You don't have permission to access "XXX-CENSORED-URL-WAS-HERE-XXX" on this server.<P>
Reference&#32;&#35;18&#46;f881bb8&#46;1479332317&#46;a414cc
</BODY>
</HTML>
```

Which suggests that it's geo-restricted or else requires some form of authentication. Please clarify and provide a non-geo-restricted stream.
 Closing due to lack of information.  You can already use `AspectRatioFrameLayout` in a custom layout xml. Removing the `final` modifier is only needed if you actually need to extend the class to add extra functionality. Could you please describe what additional functionality you need; it may well be that extending the class isn't the best solution to whatever you're trying to solve. Thanks!
 I'm still not entirely sure I understand what you mean, sorry! Please can you provide an example (e.g. the actual layout file that you're using, and the simpler one that you could use if `AspectRatioFrameLayout` were non-final?

Also, I doubt removing a single view from the hierarchy will provide any measurable performance gain here, unless you're repeating this layout many times (e.g. as the layout for a grid or list entry where many entries are visible simultaneously).
 - Hmm. That seems like a strange way to be structuring things. Why does your `VideoView` class need to extend anything? Why not just inflate a layout xml directly, and pass the inflated root to a standalone helper class?
- I suspect that `TextureView` is so much more expensive than the extra view you're saving, that you'll see no measurable performance gain whatsoever. Have you actually seen a measurable performance difference, and if so what data are you looking at?
 `SurfaceView` is expensive too, relative to the cost of the extra view. My point is that there's little point in optimising something that's a very low percentage of the total cost. Unless you can actually measure the difference, you should in my opinion just leave the additional view in your hierarchy and be done with it.
  Looking at what `PlaybackControlView` does is the right place to start. Beyond that, this is something you'll have to work out for yourself.
  Yes. We don't expect lack of positioning support to cause this kind of issue though.

@bit101 - When you see the captions becoming corrupted, do you find that toggling the text track off and on again resolves the corruption? I think I'm seeing this. 608 is stateful. I suspect the problem is that this stream is really multiple videos separated by #EXT-X-DISCONTINUITY tags, that we need to flush the 608 decoder state when the discontinuity tag is encountered, and that we're not doing this.
 I've been unable to reproduce this with the latest from `dev-v2`. It's quite hard to tell whether I've just been lucky or whether the issue is resolved there. Also, the live stream currently seems to be displaying only a static image. Please test using latest `dev-v2` and report back.

As an aside, the server providing this stream seems unable to serve it (to me in the UK at least) fast enough, so the player frequently has to pause and buffer. We've made some further changes in `dev-v2` around 608 handling, including positioning and some styling support. I'm not able to reproduce this issue any more.  Does the stream play well on the same device using other player (i.e. MediaPlayer)? If not, then the device probably does not support the any of the formats (as stated in the provided log message).

We can have a look if you provide a stream for us to look at. Since it's a TransportStream, you should be able to cut the file at any reasonable(in terms of file size, say 30MB) point and provide us with that.  
 Also, what device are you trying to play this stream on? AC-3 playback requires either HDMI passthrough on Android L (or later), or an on-device decoder for AC-3. Also, not all Android devices have MPEG-2 video decoders. This stream is unlikely to be playable on many non-TV devices.
 For playing the audio with ExoPlayer, you can either rely on HDMI passthrough or use the ffmpeg extension (you'd need to compile FFmpeg with an AC-3 decoder).

We don't provide an extension for decoding MPEG-2 video. According to the [CDD](https://source.android.com/compatibility/7.0/android-7.0-cdd.html), including an MPEG-2 decoder is "strongly recommended" for Android TV devices.

VLC probably bundles software codecs for MPEG-2 video and AC-3 audio decoding so it doesn't rely on platform support for decoding these formats.
 As an aside, note that even if you get this stream working, you still wont be able to seek within it. See [here](https://google.github.io/ExoPlayer/supported-formats.html). We have no plans to enable seeking in containers that do not efficiently support it. Note also that a software MPEG-2 decoder is likely going to perform significantly worse and consume more battery than the hardware backed H264 decoders available on many Android devices (some newer devices also have hardware backed H265 and VP9 decoders).

If this is for playback on mobile devices (as opposed to an AndroidTV) then I'd suggest you investigate the alternative path of transcoding your streams into a more suitable format. Searching for "mythtv transcode" on Google suggests there are tools and resources to do this.
 To answer your specific questions above:
- Yes, the demo app will use the ffmpeg extension if you use the `withExtensionsDebug` or `withExtensionsRelease` build variant. Note that you still need to manually build ffmpeg and the JNI binding. Instructions are [here](https://github.com/google/ExoPlayer/tree/release-v2/extensions/ffmpeg).
- If you want to use a software MPEG-2 decoder then you'll need to build a new extension that bridges onto it. You could use the VP9 extension as a starting point.
 Read the top of that page about the difference between container format and sample format. We support MPEG-TS as a container format, but the underlying platform still needs to provide a decoder for the sample format for playback to work. See the second column of the table [here](https://developer.android.com/guide/appendix/media-formats.html#core) for supported sample formats, which does not contains video/mpeg2 (aka H262, I think).

Note that in addition, it's necessary for us to do some work to support each sample format at the container format level (e.g. parsing the correct decoder initialization data). It looks like we've already done that for H262 in MPEG-TS, [here](https://github.com/google/ExoPlayer/blob/r2.0.4/library/src/main/java/com/google/android/exoplayer2/extractor/ts/H262Reader.java).

So in summary, if the device doesn't have a video/mpeg2 decoder you'll need to implement an extension that provides one, as per my previous response above.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Thanks for reporting this issue. I'm not sure this is the right solution though. The use of `videoFormat.rotationDegrees` in particular may not be correct, since the decoder may or may not have already applied the rotation. We may need to make it so that `onVideoSizeChanged` is always invoked after the `Surface` has been replaced (which is what we do for `onRenderedFirstFrame`).

As a related point: `SimpleExoPlayerView` should probably apply any unapplied rotation in the case that `TextureView` is being used, which it doesn't currently do.

I've filed https://github.com/google/ExoPlayer/issues/2077 to track fixing these issues. I'll close this pull request, since the fix will look different to the proposal here.
 I think this should be fixed in `dev-v2` by the change ref'd in #2077, although I didn't test this specific case. Please give it a try and let us know if there's still an issue!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  The audio track has a `.mp3` box in the `stsd` which we don't currently handle. The code is specified for [Quicktime](https://developer.apple.com/library/mac/documentation/QuickTime/QTFF/QTFFChap3/qtff3.html#//apple_ref/doc/uid/TP40000939-CH205-BBCGEBEH) and it looks there is also an alternative way to signal it using an [ObjectTypeIndication](http://mp4ra.org/object.html).
 @vishnuganta22 I don't have a concrete ETA for this yet, but the (unreviewed) patch is very simple:
1. Add `TYPE__mp3 = Util.getIntegerCodeForString(".mp3");` in [Atom.java](https://github.com/google/ExoPlayer/blob/d922a21/library/src/main/java/com/google/android/exoplayer2/extractor/mp4/Atom.java).
2. Add a check for `|| childAtomType == Atom.TYPE__mp3` in [AtomParsers.java](https://github.com/google/ExoPlayer/blob/d922a21/library/src/main/java/com/google/android/exoplayer2/extractor/mp4/AtomParsers.java#L630).
3. Set the MIME type in [parseAudioSampleEntry](https://github.com/google/ExoPlayer/blob/d922a21/library/src/main/java/com/google/android/exoplayer2/extractor/mp4/AtomParsers.java#L859): `else if (atomType == Atom.TYPE__mp3) { mimeType = MimeTypes.AUDIO_MPEG; }`.
 Thanks for the pull request, but we have already submitted this change internally. The feature should be available next time we push to `dev-v2`.
 This should be fixed in `dev-v1` and `dev-v2`.
  Could you please provide a link for us to reproduce ourselves? Also, is it possible for you to try your URL in v2 to see whether an error is thrown?

If you don't want to make the url public, please send it to dev.exoplayer@gmail.com
 Stream was provided by email.
 Sorry for the delay on this. How long does the stream have to play for the error to occur? Have you tried the URL in V2? I have been playing for some time now and did manage to reproduce the issue. But I also observed that the media sequence of the playlist gets reset to 0 from time to time. According to the spec, this is not allowed. I also suspect that in this process the server closes all connections, or some server related error occurs. If you find evidence pointing towards Exoplayer doing something wrong, please open a new issue. I suggest making sure the server is providing a stable stream. An easy way of doing this is tracking the state of the playlist with:

` wget -qO- <stream url>`  You can use SimpleExoPlayerView as a starting point/example (see https://github.com/google/ExoPlayer/issues/2017#issuecomment-259633365).
  Please run `adb bugreport` just after seeing the ANR and attach the output to this issue, or email it to dev.exoplayer@gmail.com. Then we can investigate why the playback thread is blocking release. Thanks.
 - Is it intentional that you have 8 ExoPlayer instances at the same time? That looks a little suspicious to me.
- The ANR itself is likely caused by `"ExoPlayerImplInternal:Handler" prio=5 tid=39` which is blocked in `android.media.MediaCodec.native_stop`.
 - Yes, it does. It's waiting for the playback thread to exit, which is blocked in `android.media.MediaCodec.native_stop` as per my previous reply. It looks like the call is hung in the platform.
- 8 instances seems pretty unusual. Most devices only have one or two hardware decoders available. Are you actually playing 8 pieces of media at once? What's the actual use case? It's difficult to say whether it's causing problems or not.
 If I were you I'd make sure there's only ever a single instance, if you're only using playing one thing at once. It'll give you more confidence that there's no contention for decoders, memory etc. I'm not sure whether it will make the platform hang less likely (or get rid of it) though.
 > Does ExoPlayer's underlying architecture run on top of the same as MediaPlayer?

"It depends". There's more than one `MediaPlayer` implementation, and device manufacturers may have made further modifications (including completely rewriting it) on their own devices. Generally speaking though, if you're running a relatively recent version of Android, and definitely if you're using a Nexus or Pixel device, then yes they both make use of the same decoders.

> It seems that some people having an issue releasing MediaPlayer claimed that not calling stop helped them: https://code.google.com/p/android/issues/detail?id=959

It looks to me like everyone in that thread except for one person is saying that removing stop didn't help.

> I wonder if removing the call to stop would help.

There's no real guarantee that `MediaPlayer` and `ExoPlayer` stop methods are doing the same thing thing with the underlying decoders. It's quite a big stretch to make this kind of inference. I doubt it'll help.

In my opinion there's probably not a huge amount you can do about this kind of issue. If it's very infrequent, you're probably best off not spending a whole amount of time looking into it.
  This is an issue for the ExoMedia project. EMAudioPlayer is not part of ExoPlayer.
  Please provide the URLs you're trying to play and we will take a look. (Note that we don't support seeking in [some container formats](https://google.github.io/ExoPlayer/supported-formats.html#standalone-container-formats).)
 We don't support seeking in MPEG-TS. You will need to use a different format if you require seeking (see the linked page in my previous comment).

See #966 for some more background on this.
 Please read #966 for an explanation of why we support seeking with HLS and not plain MPEG-TS.
  I don't know of any specific issues around playing interlaced video on Pixel devices. We can take a look if you provide a URL to the problematic media.
 Closing due to lack of required information. See also #2013 which may be relevant.
  @crossle Please could you provide steps to reproduce this, including what media you were playing, in case it's needed to investigate further? Thanks.
 Sorry for the delay on this. I think I have an idea of what the issue is. I will provide a fix which will reference this issue.
  We support forced text tracks but only for HLS streams and Matroska files at the moment. (You can check [Format.selectionFlags](https://github.com/google/ExoPlayer/blob/94cc606/library/src/main/java/com/google/android/exoplayer2/Format.java#L169) in the TrackGroup's formats for [C.SELECTION_FLAG_FORCED](https://github.com/google/ExoPlayer/blob/94cc606091799b4a03593582528a600494979b15/library/src/main/java/com/google/android/exoplayer2/C.java#L202).)

However, it looks like we don't support forced subtitles for DASH. Could you give some details about how you want to signal forced subtitles, and/or provide some sample media? Thanks.
 - We also support specifying selection flags on side-loaded subtitles. The application is responsible for providing the `Format` in that case, and so can specify any desired flags.
- For DASH specifically, we could easily support a standard way of indicating forced subtitles in DASH manifests, were one to exist. I can't find such a feature in the DASH spec, however. If you're aware of one then please let us know, and we can look at adding support for it.
- We don't (easily) support both forced and non-forced subtitles being enabled simultaneously. I think that commonly, non-forced subtitles will also contain the forced subtitles as well, meaning you don't need to worry about enabling more than one track at once.
 I think something like that would be the right approach. It feels like something that should be standardized properly though, if we're to officially support it. Is this something you could suggest to [DASH-IF](http://dashif.org/)?
 If you do end up using your own custom element in the manifest, then it should be relatively straightforward for you to support it in ExoPlayer as follows:
1. Extend `DashManifestParser`. Let's say you call your extension `CustomDashManifestParser`. In this class, override whatever methods you need to override to parse the custom element or attribute, and set the flags on the text `Format` accordingly.
2. When creating a `DashMediaSource` instance to provide to the player, inject an instance of your `CustomDashManifestParser` through the constructor that allows you to do this*.

\* Such a constructor doesn't currently exist, but we'll push a change in a week or so that adds one ;).
 You'd still need to extend the parser as well because you'll need to actually use the (non-standard) content of the element in the parser to set the appropriate flag on the `Format`. If you want to add some kind of hook for `Role` elements in `DashManifestParser`, so as to make the extension easier, then that sounds like a sensible idea.
 It's now possible to inject a custom parser, so the approach described above should be a viable solution for this issue. Feel free to send a pull request if additional flexibility is needed in `DashManifestParser`. FYI - We'll be pushing a change to `dev-v2` shortly that parses `Role` elements and sets `C.SELECTION_FLAG_DEFAULT` if the element looks like:
```
<Role schemeIdUri="urn:mpeg:dash:role:2011" value="main"/>
```
This is sufficient to cause the track to be selected by default by `DefaultTrackSelector`. The change is on `dev-v2` already, and is ref'd above. We don't have a good ETA of our next release as of now. It would be helpful if you could explain exactly what it is you're trying to do from a functionality perspective (reading the value of the tag is the means by which you're trying to do it, but does not explain what it is you're trying to do or why). What are the possible values of the tag for your use case? Why isn't `Format.selectionFlags` sufficient for this? Presumably "main" is SELECTION_FLAG_DEFAULT and "forced" is SELECTION_FLAG_FORCED. What does "alternate" mean, and is it just no flags (i.e. 0)? I think you need to justify the need for flexibility with specific examples. If we were to just add extra flexibility everywhere in case it's useful, without understanding why, then everything would become very bloated and we'd have no way of knowing that we really made the right choices.

Without specific examples it remains unclear why selection flags are not sufficient. If there are a few specific extra flags you need, for example, then it's probably more suitable just to define some extra flags than it is to add a free-form string field. Well defined flags are also much clearer to handle for someone implementing a TrackSelector without necessarily knowing all the details of the MediaSource. You can OR multiple flags together to represent that. That's why they're flags rather than consecutive numbers ;).  Please, see #2055.
  I doubt the above response is relevant.
 Have you seen #2014 ? Is this related? If so, this is a duplicate. If that's the case, please close the issue. If not, please clarify, as I might be misunderstanding the problem. 
 Also, if not a duplicate, try to provide a link for us to have a look. That makes everything easier. You can do so at dev.exoplayer@gmail.com. 
 This is a duplicate of #2014 . It won't work perfectly well, but it would be interesting to try @Avetri 's patch(above). The language would be incorrectly set, but I wonder how it would work. @bene25 , give it a try and let me know the outcome. Or send your HLS link to dev.exoplayer@gmail.com. You are probably using an older Exoplayer version, that's why.  Please read the issue template and provide all of the requested information. We can't debug this with the information provided.
 We will almost surely need to reproduce the issue ourselves. If you cannot share the stream publicly, you can -as stated in the issue template- send it to dev.exoplayer@gmail.com. 

Just in case, if the chunk is too big, that could be the reason behind the error. If that's the case, try splitting it into smaller pieces or using a ExtractorMediaSource for the chunk directly. The stacktrace is not enough for us to figure out what the error is.
 I can confirm it's reproducible. I will look into it further later since it's most likely an unintended behavior. I would suggest using a shorter segment duration, if within your possibilities.
 There's no benefit to using HLS for your use case. Furthermore, your use case is not what HLS is designed for; Apple recommend 6 second chunk segments for HLS.

We're unlikely to fix this. I think you need to reconsider the approach you're taking. Why are you using HLS at all for this case? Why do you need an HLS playlist? Why don't you just play the underlying 1GB file directly using `ExtractorMediaSource`?
 I'm afraid we are unlikely to be able to help you play the media in its current form (definitely not in the near future). For HLS, we load chunks in full so they can't be too big. For MPEG-TS it's not feasible to support seeking (see #2061).

As an aside: even if we did load these 1 GB chunks in HlsMediaSource seeking would be very inefficient for most use cases, as we would need to read and discard samples from the start of the chunk up to the requested seek timestamp.

You will need to get streams in a different format, or implement your own MediaSource which constructs a time-to-byte mapping for these MPEG-TSs, but we don't have resources to provide guidance on this.
  I'm not sure `HttpUrlConnection` exposes a way of getting the IP address at the platform level. Do you really need the actual IP address, or is the URL after redirection sufficient?
 I understand what you're trying to do. I'm saying the actual IP address doesn't appear to be exposed by the underlying network stack, which is why I'm asking whether the URL after redirection is sufficient.

The CDN I have most experience working with redirects to URLs that are machine specific, so having the resolved URL would be fine. I suspect most (all?) CDNs do this, and if so the resolved URL should be fine for your purposes. Can you confirm whether or not this is the case?
 Updated the title to reflect the enhancement we'll provide for this.
- The URL after redirection will be reported through `AdaptiveMediaSourceEventListener`'s `onLoadCompleted` method, and possibly `onLoadCanceled` and `onLoadError` in the case that the cancelation or error occurs after the connection has been established.
- We may also want to report the URL after redirection via something in `ExtractorMediaSource.EventListener`.
  Why would you expect it to change? One of the key selling points of playlist support in V2 is that we can perform transitions seamlessly, which means there wont be any changes to player state across the transition ;). You should probably be looking at `onPositionDiscontinuity` instead, which is invoked on seeks and also when transitioning from one song to the next.
 Yes. So use `onPositionDiscontinuity`. From there, you can use `ExoPlayer.getCurrentWindowIndex` to see the index of the track being played. If it's changed then you can refresh the UI accordingly.
  Merging two sources that expose audio tracks is possible in the sense that the merged source will expose all of the audio tracks provided by its children. However it is not possible to then have more than one of the audio tracks enabled at a time, which is what you're wanting to do. I'm not sure whether Android even provides a way to accurately synchronize two `AudioTrack` instances together. If not, you'd likely have to do something yourself to merge the audio track data before feeding it into a single `AudioTrack`.

I don't think this is something we have any plans to support.
 It's possible merge audio and video sources, yes. We don't provide any functionality for adding a delay to one of the sources, however. It might be possible for you to implement it yourself with a custom `MediaSource`, but we don't have the cycles to be able to provide guidance about how you might go about doing this; sorry.  We don't currently support extracting URLs from these sort of playlist files; see also #1947. ExtractorMediaSource throws an exception because it doesn't recognize the file as a valid media container: MP3, Matroska, MP4, etc..

It should be fairly straightforward to request and parse the playlists in your app, then pass the contained media URLs to ExoPlayer.

Note: you can quickly verify that the contained URLs play using the ExoPlayer demo app, like this: `adb shell am start -a com.google.android.exoplayer.demo.action.VIEW -d "http://wmfe.iad.streamguys1.com:80/wmfe"`, where the URL comes from the `m3u` file linked above.
 @amintabar If you're playing HLS streams, use HlsMediaSource (not ExtractorMediaSource).  I see where you're coming from, but in practice, if you do this, you end up having a to add a whole bunch of blocks that look like:

```
if (chunk instanceof MediaChunk) {
  // Cast to MediaChunk and do something
} else {
  // Do something else
}
```

So pragmatically it ends up being quite a bit cleaner just to define them in `Chunk`.
  See #1961. If not applicable(the playlist does contain the #EXT-X-ENDLIST tag), open a new one including the information required in the issue template.
 @ojw28 pointed out this is not working correctly. I will reproduce myself and will use this issue to track the bug. Sorry for closing it prematurely!
 This is indeed a bug in dev-v2 but has been fixed in the internal version. Wait for the next push and  it will be fixed. Will keep this open until confirmation. Thanks for reporting!
 This should be fixed in `dev-v2`.
 Just saying "doesn't work" isn't particularly helpful to us. Are you certain `#EXT-X-ENDLIST` is being appended correctly?
 Can you provide the exact ways to reproduce the issue?

- Are you seeking yourself directly to the end or do you let it play until the end is reached by itself? I assume you are not seeking at all, considering this is not supported in V1, but just in case.
- Is this a master playlist or a media playlist?
- If a master, how many media playlists are being handled at the same time? Do you have multiple audio renditions? Subtitles?
- The playback starts as live (i.e. It doesn't have an ENDLIST tag) but eventually you add it?

I will need some more context as I am not able to reproduce the issue. If you could attach a debugger and see what's going on in the player when it is stuck in buffering state that would also be great. I am now going to create and play a live stream, and see if I can reproduce. But if I cannot and no further information is provided, I will have to close the issue. Closing due to lack of information.  ....Why?
 The overall line delta of this change appears to be positive (+432, −409) and I also think it's missing a whole file (PlayerActivityBinding), which will make it more positive when added. So it's unclear how this is minimizing anything. Have I missed something? I think I'd also argue that it makes the demo app harder to understand, since someone trying to do this will now also have to look up how an (optional) data binding library works...
 At best this is personal preference. At worst it's more verbose than it was previously (the overall line delta is already positive, and will be significantly positive once the missing class is added), is less performant, and is (presumably?) subject to fewer compile-time checks. Closing.
  This issue tracker is for issues with the library. We don't have bandwidth to debug your own customizations; sorry. It's your responsibility to make sure you implement customizations correctly and to resolve any issues with them.

That said:

Note that there is no guarantee `audioTrack.write` will actually write all of the data passed to it. That's why it returns the number of bytes written. It's quite likely you're accidentally applying your processing multiple times to the same audio in the case that it doesn't fully consume the data. `handleBuffer` is documented in detail (in V2 at least) explaining the contract and that the same `ByteBuffer` might be passed multiple times in the case that the data is not fully consumed.

In the "easier way" described above, the code is invalid because it will always advance the position of `buffer` by `remaining()`, even though fewer bytes might be written to the underlying platform AudioTrack. If fewer bytes are written, you've advanced the position too far, since the contract of `handleBuffer` is that the position is advanced by the number of bytes that were written successfully.
  Consult Android's [supported media formats](https://developer.android.com/guide/appendix/media-formats.html) pages and the [CDD](https://source.android.com/compatibility/cdd.html) for information about what formats should be playable on all Android devices/builds. Your best bet may be to encode the video as an H.264/AVC or VPx bitstream, as these formats are relatively widely used. You should be able to configure FFmpeg to encode those formats efficiently but I'm afraid it's outside the scope of this forum to provide specific guidance on encoding tools.

You mention it's playable in other players (and I'll assume you mean on Android). Those players may use their own bundled software decoders, which could explain the discrepancy. That said, if you find that the problematic stream can be played using the platform [MediaPlayer](https://developer.android.com/reference/android/media/MediaPlayer.html), please share a link to the media and a bug report from the device, as requested in the issue template, and we'll take a look.
 This appears to be the same as #2007. Please follow that issue for updates. Thanks!
 As noted in my previous comment and on #2007, you may wish to switch to H.264/AVC instead (if possible). The capabilities reporting for that codec seems to be more reliable.
  It's unclear what you're actually reporting here. Please provide more details, sample media and a full bug report.
  Yes, it does.
 The `TrackSelector` creates track selections. For the adaptive video case the selections are produced by the `AdaptiveVideoTrackSelection.Factory` in the code above. As long as you have similar code to the code in the demo app (and above), it will be enabled by default.
  You'll need to provide sample media for us to reproduce this. Please also try using ExoPlayer V2.
 If the issue is fixed in V2 then I'm not sure we'll be able to help much beyond advising you to update. @AquilesCanta might have some insight into what's been fixed, but unless it was an isolated change (unlikely for HLS, since we've been making some pretty big changes there) we're unlikely to back-port. I'm getting a 509. Any chance of fixing it? I can't reproduce the issue in either version. Have you found out any more useful information? I have tried random adaptation in V2 and reducing network capabilities and manually adapting to lower bitrate variants in V1. If you have figured out the root cause or have some extra insight for us to look into, please share it.  Closing due to lack of information.  This is not an ExoPlayer issue (which is what this tracker is for). I suggest you ask questions such as this one somewhere like StackOverflow. You might also find [this](https://medium.com/@cinemur/exoplayer-for-building-powerful-vr-players-for-cardboard-and-gearvr-73ec7e83dd5c#.npf2tldsi) an interesting read?
  The link indicated as faulty above plays fine for me using ExoPlayer on both a Pixel and an emulator. There's nothing in ExoPlayer that's specific to particular types of mobile connection. We simply use the underlying http stack in both cases. So whatever this is, it's unlikely to be an ExoPlayer issue. You'd need to provide a full bug report (as requested in the issue template) for us to have a chance of further narrowing down the cause.
 Sounds likely to be an issue with Cyanogenmod, so I'd suggest you report it there, or to OnePlus. Alternatively, if you're able to provide a bug report shortly after encountering the issue, it might be possible for us to provide a workaround.
  Closing due to lack of information. ExoPlayer doesn't support downloading, so if you're trying to support it then it feels like obtaining the URLs to download is something you'd need to implement yourself.
  Track #87. Something similar is done in that line of work.

As a note, what you suggest is not that easy. That I know of, there is no restriction over the duration of the chunks except that they are shorter than the target duration of the media playlist. So, multiplying is not an option. Not generally, at least. And this is assuming you load every playlist at the start of playback. Ideally, you wouldn't load a playlist until you need it, but even if not, you might be loading a playlist that has drifted from the other one. Note also that sequence numbers need not be aligned.

To sum up, wait for live window seeking support. We do something relatively similar to what you ask. Thanks for the suggestion!
  Could you clarify whether you saw the issue running on Cyanogenmod or stock Android? It looks like a platform bug rather than an issue with ExoPlayer.

See also the [FAQ about using ExoPlayer on emulators](https://google.github.io/ExoPlayer/faqs.html#does-exoplayer-support-emulators).
 The built in gallery app playing the video successfully doesn't say much about whether this is a platform bug or an ExoPlayer bug. The two may well use different decoders, exercise the underlying platform in different ways etc.

The failure (to instantiate a decoder) strongly suggests an issue with the version of Cyanogenmod that you're using rather than an issue in the ExoPlayer. If you can reproduce the issue with the official version of Android that the device shipped with then please let us know. Otherwise please report the issue to Cyanogenmod developers.
  Given (a) The underlying AudioTrack simply plays what it's given, and (b) ExoPlayer feeds regular audio data to the underlying AudioTrack correctly, any artifacts being introduced must be purely a result of the processing that you're performing on the buffers.

This is something you'll need to resolve for yourself, I'm afraid. There's no particular reason why you'd need to change the buffer size; it should be more than large enough already.
  The device is indicating that it doesn't support the media you're attempting to play. It's likely the device is under-reporting its capabilities, given your observations. We'll need to get hold of a device and have a look.
 V2 performs more checks than V1 did, so that isn't inconsistent with the diagnosis. We don't want to revert the additional checks because they prevent playback failures on other correctly functioning devices.
 I don't think disabling the additional checks is a good solution in the general case. What we'll do is:
1. Add some additional logging that indicates exactly which check is failing and why, so that we can see this in future bug reports.
2. For each report received we'll add a corresponding device specific workaround, if appropriate.

I suspect we'll pretty squash out the most common cases (and I'd be surprised if there are a huge number of cases required; most likely the majority of these issues are with older, possibly lower end devices).
 Please could anyone encountering this issue:
1. Check out the latest `dev-v2` code.
2. Try and play the affected video.
3. Paste the output of `adb logcat -s "MediaCodecInfo"` here. You should see a bunch of lines containing "FalseCheck" the detail exactly why the stream wasn't played.

Thanks!
 The change ref'd above will have helped with vertical videos (i.e. where height > width). We're still working on ways to mitigate this issue for other videos. Thanks! We'll shortly be pushing a change that will allow you to enable exceeding of reported decoder capabilities. It will be off by default, but it will be possible to enable it with a single line of code.

Once this change has landed you'll be able to enable the option to get back to what V1 did. We'll then consider either:

1. Enabling the option described above by default.
1. Writing a list of known overrides into the code as problematic cases are reported.
1. Writing a list of common-sense overrides into the code to achieve the same result, but hopefully more broadly than device specific overrides. For example we may put an override in place that assumes devices are able to decode H.264 at least up to their own display resolution (this is nearly always, if not completely 100%, true). That kind of override would solve the case immediately above this post, since the display resolution of the device is 2048 x 1536. Fixed in `dev-v2`.  This is most likely a duplicate of #1978. Shouldn't be reproducible in the last version of Exoplayer. Please reply to this issue if this is not the case. Confirmation would also be appreciated.  Is this for streaming? If so then I agree this sounds like a (server-side) configuration issue. You can configure a license policy in a way that will cause the `MediaDrm` session to generate an `EVENT_KEY_REQUIRED` periodically before the current keys expire. Each time this event is fired `StreamingDrmSessionManager` will perform a new request, which will then obtain keys valid further into the future.

It's quite common for services to issue keys that are valid for durations far _less_ than the duration of a typical viewing session, and rely on the behaviour described above to have the client periodically obtain fresh keys during playback. For example a movie service may configure their license policy so that keys are valid for 5 minutes, with `EVENT_KEY_REQUIRED` fired every minute. The client will obtain fresh keys every minute and hence the keys it currently has will always be valid for between 4 and 5 minutes into the future under normal operation. If a request fails it will retry a minute later, and it will get 3 further chances before the current keys finally expire and playback fails. This approach allows for an indefinitely long viewing session, whilst also allowing the service provider to revoke the client's access pretty quickly.

If you have a Widevine contact, you'd be best off reaching out to them to get more information about how to configure licenses in the way described above. I think "renewal delay" might be what you're looking for, but I'm not certain. Alternatively, it might be obvious now you know roughly what to look for ;).  I don't really understand the StackOverflow post. If the poster can create a DataSource instance to do what they want, it should be trivial for them to implement DataSource.Factory just to provide instances that are created that way, like:

```
  DataSource.Factory factory = new Factory() {
    @Override
    public DataSource createDataSource() {
      // Create and return source with headers set as required.
    }
  };
```
  Closing issue because it fails to provide adequate information, as is requested in the issue template.
  `EventLogger` in the demo app is already a good example of a class that's registered to receive a whole bunch of internal events from the player, so you should take a look at that as a starting point.  Note that in ExoPlayer V2 quite a bit of extra useful information is propagated to `EventLogger`, so if you're really keen on getting as much information as possible then you should think about migrating.
 > the layer () the chunk belongs to

It's unclear what "layer" means here. You get a `Format` via `onLoadCompleted`. Does the `Format.id` provide what you need?

> the remaining buffer size (in seconds)

You can use `ExoPlayer.getBufferedPosition` (subtract the current position to get the buffered duration).

> frame per second decoded

It's unclear what this really means, or what it's for. On average the frames per second decoded is going to be identical to the frames per second of the format.

> chunk full url

The original url is available in V2. It's not available in V1. It shouldn't be surprising that updating to a newer version gets you more functionality ;). Also reporting the redirected url is tracked by https://github.com/google/ExoPlayer/issues/2054 and will be a V2 only feature.

> bitrate

If you're talking about the bitrate of the stream, then same response as for "frame per second decoded" as above. If you're talking about the actual available bandwidth, if you're instantiating a `DefaultBandwidthMeter` then you can pass a listener to it in order to receive bandwidth estimates.

> chunk position

It's unclear what this means. `onLoadCompleted` receives start and end media times for the chunk.
  Please file a new issue containing complete information as requested in the issue template if this is still a problem for you.  > But didn't go well.

Why not? I would expect this approach to work fine.

> how can i both play a video and a background music?

If the video and background audio have the same duration then you can merge them with MergingMediaSource. If they have different durations then this is not something we support. You could use a separate player instance for the background music. Closing due to lack of clarification.  The reason behind this bug is that the vtt segments don't have a .vtt/.webvtt extension. This means that the stream does not get past preparing state for V1. 

I couldn't find any constrains over the extension of the segment URIs in the spec so this is an actual bug. For the time being, if you can rename the webvtt segments so that they end with .vtt, that will fix the issue for you.
  Could you elaborate on what you mean by "flakey"? Are the files truncated?

We may need an example file to investigate further so please share one, and also the output of `adb logcat` just after you see the error. Thanks.
 I think having elegant failures for this kind of thing in general would require us to do a huge amount of validation on the stream as we're processing it, which is quite expensive, would add a lot of weight to the codebase, and probably doesn't represent a good trade-off.

If this problem is widespread across multiple providers then we could consider an elegant failure specifically for this case. I somewhat doubt it is though. I'd expect most streaming providers to catch this kind of thing when testing a new ffmpeg version, before adopting it. I'd also expect a streaming provider to fairly quickly re-transcode any bad content they've produced, thereby rectifying the issue. Is that accurate, or do you really think we need to be handling this? If so, please provide concrete numbers / data to justify :).

As an aside, could you link to the corresponding ffmpeg issue? It would be good to know the root cause and the exact versions of ffmpeg in which the issue was present.
 As per above, unless the issue is affecting multiple providers or is widespread, I think we're happy to assume the problem will be cleared up as that provider re-transcodes any bad content they've produced. Closing for now. A link to the ffmpeg issue would still be appreciated.
  This is not enough information for us to help you. Please open a new issue including ALL the information required in the **issue template**. Please double check you include everything listed there.
  See #55.
  Have you seen #1967 ? For me, the first chunk doesn't play in any of exoplayer versions, unless I set FLAG_ALLOW_NON_IDR_KEYFRAMES.

The second one does not play on Nexus 5X(any exopayer version), but I think it's related to decoding capabilities.

I wonder if you are setting the FLAG_ALLOW_NON_IDR_KEYFRAMES flag for version 1.5.7 but not for 2.0.4. Please check if that's the case. If not, let me know and I will explore further.
 Closing due to inactivity.  `InfoListener` was never part of the ExoPlayer API. It was only defined in `DemoPlayer` in the demo app, not in the core library. In V2 the equivalent to copying `DemoPlayer` is to use `SimpleExoPlayer` directly, which is in the core library. It defines a bunch of listeners that you can register that provide equivalent functionality.
  We don't use or support `android.media.MediaController` in ExoPlayer V2. It was never an API that was particularly well suited for `ExoPlayer`. Even in V1 it was expected that most people would not use it, with `PlayerControl` documented to say:

> This class is provided for convenience, however it is expected that most applications will implement their own player controls and therefore not require this class.

In V2 we've introduced our own UI components (`SimpleExoPlayerView` and `PlaybackControlView`). They will be really easy to customize from the next release (r2.0.5), and we'll hopefully have a chance to write a blog post detailing this.
  There are so many different variants of what it means to "support subtitles". You need to be more specific with exactly what media + subtitles you're trying to play (ideally providing links to sample content).
 Here's a basic example for playing subtitles from a `Uri srtUri` alongside some other `mediaSource` providing video/audio:

```
Format textFormat = Format.createTextSampleFormat(null, MimeTypes.APPLICATION_SUBRIP,
    null, Format.NO_VALUE, Format.NO_VALUE, "en", null);
MediaSource textMediaSource = new SingleSampleMediaSource(srtUri, mediaDataSourceFactory,
    textFormat, C.TIME_UNSET);
MediaSource mediaSourceWithText = new MergingMediaSource(mediaSource, textMediaSource);
```

I've hardcoded the language to "en", but this is only used for signaling information on available tracks to your app. See the demo app source to find out how `mediaDataSourceFactory` is constructed.
 SimpleExoPlayerView already has a SubtitleView, so if you're using that (without any customization) it should just work once you provide the right source to the player. If not, consider switching to using SimpleExoPlayerView, or read its source to see how the TextRenderer is hooked up to the SubtitleView and replicate it in your app.

I was able to play the WebVTT URL successfully using the code above but changing the MIME type from `MimeTypes.APPLICATION_SUBRIP` to `MimeTypes.TEXT_VTT`.
  `SimpleExoPlayer` is itself just a wrapper around a regular `ExoPlayer`, so the easiest way to answer this kind of question is to take a look at what `SimpleExoPlayer` does [here](https://github.com/google/ExoPlayer/blob/r2.0.4/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java) :). You're probably building what is effectively a wrapper of your own, so it might even make sense for you to use `SimpleExoPlayer` as a starting point for your own wrapper, and simplify and modify it to suit your exact use case.

We should possibly consider adding support for this to `SimpleExoPlayer` directly. It's a little awkward because that configuration parameter is specific to `SurfaceView`, but we could probably find a way that's not too confusing. Is that the only reason you can't use `SimpleExoPlayer`, or are there additional reasons on top of that?
 I'm unsure what you mean by:

> For example, why I'm not getting method setVideoSurfaceView(), though it is defined as public in given code.

`setVideoSurfaceView` is only available on `SimpleExoPlayer` and not on `ExoPlayer`, so it's expected that you wont see it if you're using non-simple `ExoPlayer`.

It does sound like we should provide a way for you to set the scaling mode and still use `SimpleExoPlayer`, for convenience. We'll use this issue to track this.
 To keep things in perspective, it's only "almost useless" if you're one of the minority of developers who actually wants to specify the other scaling mode. You can also take what's there and modify one line of code, so there's still clearly added value in the other ~850 lines of code in that class compared with you having to write everything from scratch. Or to put it another way, "almost useless for you" != "almost useless" ;).

We will look at this quite soon, however. We're going to add a `setVideoScalingMode` method to `SimpleExoPlayer` (and also `C.MSG_SET_SCALING_MODE` that you can send directly to the renderer, if not using `SimpleExoPlayer`). We don't have any plans to change the default.  `FORMAT_EXCEEDS_CAPABILITIES` indicates that the device has a decoder for the format, but this decoder indicates that it doesn't support the specific properties of the track. For audio this typically means the decoder doesn't support the sample rate + number of channels. For video this typically means the decoder doesn't support the resolution + frame-rate (and profile, in some cases).

If the device reports its capabilities accurately then attempting to enable a track labelled in this way is expected to fail. You can try and force enable it regardless if you think you know better, but as per the previous sentence, you should expect failure in absence of any other information you have.

For the sample stream that does work if you enable it regardless, I suggest you report the issue to Amlogic. If you're using the same streams as #2014, the issue is that the AudioCapabilities for their `audio/mpeg-L2` decoder does not claim to support `channels=1` and `sample_rate=48000`.
  Note: Streams were sent via email.

The problematic streams are HLS media playlists. I don't think it's officially supported in HLS to mux multiple streams of the same type into TS chunks. This is supported in straightforward MPEG-TS because the pid for each stream is guaranteed to be constant, but in HLS the pid can change from one chunk to the next, which makes supporting multiple streams of the same type problematic in the general case. HLS officially supports multiple audio via HLS master playlists with [alternate renditions](https://tools.ietf.org/html/draft-pantos-http-live-streaming-20#section-4.3.4.2.1).

So I think this is working as intended. If you wish to distribute HLS variants of these streams, you'll need to work out a way of generating them to use alternate renditions as per the HLS spec. Note that this approach is also significantly more efficient, since the client will only need to download the enabled audio track, rather than all seven of them.

@AquilesCanta - Is there a valid general case way to support this, or is the above correct? Would there be value in providing an option to enable support for this case (i.e. by forcing mapByType to false) if the application knows that the PIDs in the content wont change?
 First things first: This behavior is expected. As stated, we don't support TS variants with multiple muxed tracks of the same type. Initially, the idea was to add support for multiple closed caption tracks only. Unlike with closed captions, the main obstacle for supporting multiple audios in the same TS(in HLS) is that they cannot be described by the manifest. That is, the #EXT-X-MEDIA tag only provides instream id for closed captions. That being said, [the spec also states](https://tools.ietf.org/html/draft-pantos-http-live-streaming-20#section-6.3.3)(read the whole section of the spec for some context):

> Clients SHOULD be prepared to handle multiple tracks of a particular type (e.g. audio or video). A client with no other preference SHOULD choose the track with the lowest numerical track identifier that it can play.

I used to think this was not supported for some reason I don't remember. Even though ambiguous, this suggests the contrary. I also found an hls stream producer that allows muxing multiple audios in each variant. I will mark this as an enhancement but it will probably pass some time before I  get to this. The best option, as @ojw28 says, is providing different streams for each language, and let the client download whatever it wants to play.
  Note: Test stream sent via email. The stream provided works fine for me both on Pixel and running in a Nexus 5X Virtual Device (i.e. emulator). It sounds like a device specific issue that should be reported to Amlogic. Please do this.
 It's really unclear what this issue is really saying. How are you determining that the device does not "fulfill de-interlacing". Please provide details of exactly what you're seeing, ideally including some photos of any visual artifacts that are visible as a result. Thanks.
 There is no deinterlacing functionality implemented in ExoPlayer itself and it is not feasible for us to implement it in a way that works in general.

Decoders can do arbitrary post-processing and in the past we have seen devices where you can customize what post-processing is applied by setting device-specific MediaFormat keys. You might want to investigate this further if you only need to support certain devices where you have documentation about the decoders, and what MediaFormat keys they will accept. You can override MediaCodecVideoRenderer.configureCodec to customize how the codec is configured.

Since Android L, devices have been able to expose "tunneled" codecs that may apply additional post-processing compared to untunneled decoding, but ExoPlayer does not currently use these. (Tracked by #1688.)

For the specific stream/device combination mentioned in previous comments, do you have any evidence that the device should be able to play it without the artifacts shown? For example, does MediaPlayer play it without artifacts?
  We fixed a whole bunch of memory related issues in early 2.x releases. Please try with 2.0.4.
  There's not enough information to help here, beyond the obvious suggestion of making sure that the address is accessible. Perhaps try playing it using another player.

If you can narrow down the issue to a bug relating to ExoPlayer, please file a new issue providing all the requested information.
  To try out the ffmpeg extension from the demo app:
1. Follow the [instructions](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ffmpeg) to build the native part of the extension.
2. Show build variants in Android Studio and select the `withExtensionsDebug` build variant of the demo app module.
3. Play some media where the audio is in one of the formats that can be decoded by the FfmpegAudioRenderer. Note that the set of supported formats will depend on the options you used to configure FFmpeg in step 1.

To test out playback of your own streams in the demo app, follow the instructions in the developer guide about [playing your own content](https://google.github.io/ExoPlayer/demo-application.html#playing-your-own-content). Note that the 'prefer extensions' flag will determine whether the FfmpegAudioRenderer is used in preference to MediaCodecAudioRenderer if both renderers support the audio format.
 @SrujithPoondla If you want to write a custom video renderer that uses FFmpeg, I'd recommend taking a look at the [vp9 extension](https://github.com/google/ExoPlayer/tree/release-v2/extensions/vp9) as a starting point. That extension wraps libvpx but you could use a similar structure for wrapping FFmpeg. I'm not aware of any tutorials about how to do this I'm afraid.

As a general point, video decoding in software is normally inefficient compared to using decoders that are part of the platform. I'd advise against this approach, especially if your app targets mobile devices.
 @nguyenxvn I'm afraid I don't have any information on building FFmpeg on Windows. Maybe [FFmpeg's compilation guide](https://trac.ffmpeg.org/wiki/CompilationGuide) will help.  @Shyri can you provide a adb log? if the content cannot be shared there may not be much the OEM can do to troubleshoot.
 @wvpaf - Friendly ping. Can we follow up on this? Thanks. Hi I was hoping to have more detailed update, however there is good news.  

According to the OEM representative the issue has been fixed for few 'familys' of Exynos powered devices.  
But have not been able to get updates on what versions will be needed nor how the fixes would be applied. As far as I know it should always be possible to force to L3, and L3 does not require manufacturer specific integration work. There were some ExoPlayer releases that didn't handle L3 provisioning correctly, but it should work with the latest 1.x and 2.x releases. There does seem to be a few issues with the Exynos chip based devices. The OEM has investigated and believes they have a fix. however we do not have any details on the fix, what the defect/solution is, nor what version of the software with the fix. 

Do you observe the macroblocking with the widevine exoplayer test content?  If no reply i will close next week. Closing  The issue here is most likely that the underlying platform is under-reporting the capabilities of the device to play video of type `video/mp4v-es`. On Pixel the decoder for this type is `OMX.qcom.video.decoder.mpeg4`, which claims to support a maximum decode width of 896px. Since the video provided is 1280px wide, ExoPlayer determines that the format exceeds the capabilities of the decoder and hence plays only the audio component.

If I force ExoPlayer to play the video stream regardless then it works fine, suggesting that the underlying platform should be reporting that it's capable of playing video in this format with larger decode widths, on Pixel at least.
- Perhaps we should be more optimistic and have ExoPlayer always try and play the stream in the case there's only one stream available and it exceeds the device capabilities. We'll have a think about this.
- You can experiment with force enabling the video stream for yourself by editing [this part of TrackSelectionHelper](https://github.com/google/ExoPlayer/blob/f8a8302f7bbcb47997095bd97b086eb633e49972/demo/src/main/java/com/google/android/exoplayer2/demo/TrackSelectionHelper.java#L150) so that the if block is always executed. You'll then be able to select the video stream in the demo app by clicking on the "Video" button.
- As an aside, if you control the way these videos are encoded, I'd strongly recommend you use H264 instead.
 Not currently. We're thinking about it, but it's unclear whether it makes sense.
 Please could you:
1. Check out the latest `dev-v2` code.
2. Try and play the affected video.
3. Paste the output of `adb logcat -s "MediaCodecInfo"` here. You should see a bunch of lines containing "FalseCheck" the detail exactly why the stream wasn't played.

Thanks!
 Fixed in `dev-v2`.  Please see: https://github.com/google/ExoPlayer/issues/318
 This really is already covered in some detail by #318. It's unclear what "not common and not universal" means, or why you can't use the solution provided in that issue. It's also unclear what "not stable" means; there's not enough information here for us to help.
  Are you certain the black bars on the left and right aren't "burnt in" to the video stream itself? If so then this is working as intended, and the black bars need to be removed from the video stream to fix the issue.
 In that case it sounds very much like a Philips specific issue. Have you reported it to Philips? If not, you should probably do so. If you provide a full bug report from the device then we can try and route the issue internally as well.
 How do you know the issue affects TVs where the width is less than 720 if you've only seen the issue on one TV? If you've seen the issue on multiple devices, please provide full details in all cases. We really need full bug reports from the affected devices as well, if you want us to investigate this issue further.
 Please see my latest comment above (the one you just replied to was ~21 days ago; I'd suggest opening this issue on the GitHub website in case email is messing things up).
 Closing because a bug report was not provided.  In the latest `dev-v2` branch you can set headers directly on the `DefaultHttpDataSourceFactory` that you're building in your code, like:

```
DefaultHttpDataSourceFactory factory = new DefaultHttpDataSourceFactory(.....);
factory.setDefaultRequestProperty(key, value);
return factory;
````

This feature will be available in `2.2.0` (scheduled for release this week) and above.  We have recently added an `HlsDataSourceFactory` interface. By implementing one, you can provide a custom `DataSource` for media chunks, playlists and keys. In your case @xbezdeka, it might make sense to take advantage of this to avoid matching the Uri yourself. Let me know if you run into any issues.  There is insufficient information here for us to debug the issue. Please read the issue template and file a new issue with all of the requested information if you wish for us to look at this.
  See #55 .
  Read the issue template, please. This does not provide enough information for us to help you.
  See #55.
  If this is using an emulator, please see [here](https://google.github.io/ExoPlayer/faqs.html#does-exoplayer-support-emulators). If this is using a real device, please read the issue template and provide all of the requested information, since there's no enough here to debug the issue.

For what it's worth, the sample plays fine for me on Pixel and with a Virtual Device with configuration: Nexus 5X API 23.
 As above, please read the issue template and provide all of the requested information if you want us to look at this. Probably file a new issue at this point.  This is not something we support directly. What do you need this functionality for?
 We're tracking adding support for this in https://github.com/google/ExoPlayer/issues/2054, so I'll go ahead and close this one.
  We abandoned plans to do this for HLS simply because it's significantly harder to get it right when dealing with TS chunks in HLS than it is when dealing with FMP4 chunks in DASH.

If you want optimal performance you should consider switching to DASH, since it's a much better thought out standard. For HLS, I think the best support we'll be providing here is to enhance our on-disk caching support, which at least means the player only has to hit the disk (and therefore will re-buffer extremely quickly) rather than the network. This enhancement is tracked in https://github.com/google/ExoPlayer/issues/420.
 It's possible, but it is harder to support than in the DASH and SmoothStreaming cases.

HLS is fundamentally less efficient and harder to deal with (e.g. through use of MPEG-TS [normally], muxed streams [normally], lack of key-frames at the start of chunks [common], multiple separate playlist files [always] and ambiguities in the spec that cause difficulties when performing stream switching [dependent on HLS version]). 

We're primarily focusing performance optimization on DASH. We endeavour to provide a robust HLS implementation, but we're unlikely to focus a large effort on optimizations it. Providers who really value delivering optimal performance are moving to DASH anyway (albeit on various timelines).
  Closing because issue completely ignored the issue template and does not provide sufficient information.
  This issue report is not useful (it's unclear whether you're side-loading the WebVTT or whether it's embedded in the MP4, for example). Please provide clear reproduction steps + sample media that reproduces the problem if you want us to look at this.
 The link provided does not work for me (returns a 500 response code). Either it's broken, or it's probably geo-restricted to some region that we're not in. Please could you provide a working sample? Thanks.
  I see the same error in the 'Design' view in Android Studio but I think it's safe to ignore -- the app runs fine. If you are seeing the error somewhere else, please clarify.

It looks like we can fix the error anyway by checking !isInEditMode() before accessing the captioning manager.
 We've fixed this by checking [View.isInEditMode()](https://developer.android.com/reference/android/view/View.html#isInEditMode%28%29). The patch will appear in `dev-v2` next time we push a set of changes. Thanks for reporting this!
  You need to make an `HlsMediaSource` rather than an `ExtractorMediaSource`, since this is an HLS stream. I added the following in `media.exolist.json` and it played back fine:

```
      {
        "name": "Sample",
        "uri": "http://pl.youku.com/playlist/m3u8?vid=443361910&type=flv&ts=1477376095&ep=dCaTHUqKUM4J4CTdij8bZyWwIiQIXPoO9RyDgNtjANQmQem7&sid=64773760957221209bfc7&token=2914&ctype=12&ev=1&oip=2067486600",
        "extension": "m3u8"
      },
```
 That stream plays fine for me in the demo app on a Pixel XL (using `adb shell am start -a com.google.android.exoplayer.demo.action.VIEW --es extension m3u8 -d "http://pl.youku.com/playlist/m3u8?vid=444086626\&type=3gphd\&ts=1477636300\&ep=cSaTHU%2BOUM0A5SPWjj8bby7gcCUGXPoO8h%2BNh9RgBtQlT%2Bu7\&sid=3477636300096128224b9\&token=1734\&ctype=12\&ev=1\&oip=2067486600"`).

Please file a new issue providing all the information requested in the issue template, and attach a bug report output by `adb bugreport`.
  We don't support CRYPTO_MODE_AES_CBC yet. Note that it's only available from API level 24, so it's not hugely useful right now given the low penetration of the Android N release. We do have support going forward, so marking as an enhancement.
 Unsure. @wvpaf, do you know?
  @satyan Sorry for not replying sooner. Please try out ClippingMediaSource and let us know if this does what you need. You can compose ClippingMediaSources in a ConcatenatingMediaSource or LoopingMediaSource as required.

An important caveat is that when playback transitions into a ClippingMediaSource the player needs to do a frame-exact seek operation. To avoid buffering you therefore need to ensure that the clipping start point corresponds to a synchronization sample/key-frame. (Playback should work even if you don't do this, but transitions from one source to the next will not be seamless.) It will be available in 2.2.0.  Please try adding

```
|| "CIPAMRNBDecoder".equals(name)
```

to [MediaCodecUtil:225](https://github.com/google/ExoPlayer/blob/dev-v2/library/src/main/java/com/google/android/exoplayer2/mediacodec/MediaCodecUtil.java#L225).

If that resolves the issue, we'll merge the fix. Thanks!
 We don't have access to a device of this type, so you'll need to test and let us know yourself. Thanks.
 It takes a couple of minutes at most to check out the ExoPlayer source code, modify it and test a sample using the demo app. It feels like this should be worth it if you want the bug fixed! Or at the very least, that this is not an unreasonable request.
 Thanks! So to clarify, the suggested workaround fixed the issue for you, correct (looks that way, just making sure)? If so we'll merge the fix.

`SimpleExoPlayerView` is deliberately designed to maintain the aspect ratio of the video. Do you want to deform (i.e. stretch) the video to fill the window, or do you want to keep the video aspect ratio unchanged but clip the edges?
 Fixed in `dev-v1` and `dev-v2`
  We need more information to diagnose this, and in particular a full bug report output by `adb bugreport`. Please provide all the information requested in the issue template.
 Closing due to insufficient information.
  Please provide all the information requested in the issue template. To diagnose this it would be useful to have a bug report taken just after seeing the artifacts (output by `adb bugreport`). It looks like it might be device-specific as I couldn't reproduce it on a Nexus 5 running L.
 This stream appears to be in H.264/AVC High profile level 3.0, which is higher than the minimum video decoding requirements for non-TV devices (see [the CDD](https://source.android.com/compatibility/android-cdd.html#5_3_video_decoding)), so there is no guarantee that it can be decoded.
 If you want to, you can try playing the file using MediaPlayer. If the issue persists, it is quite likely it is a matter of decoding capacity. Let us know if the issue doesn't reproduce with MediaPlayer, please.
  I think this is a duplicate of #1764 (and should be fixed by the latest Samsung platform update).  `dev-v2-id3` was a feature branch for ID3 development, which has now been merged and is being deleted. This PR needs to be made to `dev-v2`. Closing the PR because this is required for the feature branch to be deleted.
  Same as https://github.com/google/ExoPlayer/issues/87
  It's unclear exactly what this is requesting, but it's not possible to configure the amount of parallelism used by ExoPlayer during buffering (and we have no current plans to add this feature).  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  We believe the issue is in the media. From what we observed, one of the chunks is not a direct continuation of the previous chunk. That is, the continuity_counters do not follow. This means that the extractor detects a discontinuity, a seek() is called and the issue is observed. The HLS spec [Section 3](https://tools.ietf.org/html/draft-pantos-http-live-streaming-20#section-3) requires consecutive chunks to be in strict continuation:

> Each Media Segment MUST carry the continuation of the encoded from the end of the segment with the previous Media Sequence Number, where values in a series such as timestamps and Continuity Counters MUST continue uninterrupted.  The only exceptions are the first Media Segment ever to appear in a Media Playlist, and Media Segments which are explicitly signaled as discontinuities (Section 4.3.2.3). **Unmarked media discontinuities can trigger playback errors**.

To sum up, to avoid the issue you will either have to add a #EXT-X-DISCONTINUITY tag between the non-contiguous chunks or fix the continuity counters.
  I don't really understand what the problem is. Is the app crashing? Is it just a warning? The reason we ask for a full bug report in the issue template, instead of one line of logs, is that it lets us answer these questions without having back-and-forth.

It's known that `PlaybackParams` doesn't exist on API level 17. But then we don't use it on API level 17 devices either, so it shouldn't be a problem.
 I would suspect playback is failing for some completely unrelated reason, and you're just mis-assigning the blame to that one line of logging.
 The failure is not related to PlaybackParams. The crash is right at the bottom (very last line) and all the useful information about it is cut off. A snippet of log output is not a full bug report, and our issue template clearly explains what tool to use to obtain one. We wrote it for a reason!

In any case, it looks like a native crash. It's probably a Cyanogenmod issue, so you should report it to them in the first instance.
  The fix for #1506 is in ExoPlayer V2 and the problematic sample provided in that issue plays correctly. You'll need to provide a problematic file to allow us to reproduce and debug further. Please can you provide one here, or email one to `dev.exoplayer@gmail.com` (updating this issue once sent, so we know to retrieve it). Thanks!
 Thanks. We're failing in the special case where the OGG file only contains a single page that's not a header. It's pretty easy to fix this; we'll push a fix soon.
 Fixed in `dev-v2`
  Please provide all information requested in the issue template.
 The HLS link provided gives me a http 403 (forbidden). It's likely that it requires authentication or is geo-restricted (e.g. based on IP address). Providing a stream that we're able to access will help us investigate the issue.

Is the stream audio only (i.e. no video)? If so then I think I know what's changed, and this is really a feature request to adapt audio (for DASH, SmoothStreaming and HLS) in the case that there's no video stream.

Please confirm whether the stream is audio only and if possible provide a working URL, and we'll take a look at getting this done. Thanks! Please also confirm exactly what "newer version" you're using. "newer version" and "older version" are not useful descriptions when filing issues. Please always state the exact version numbers used ;). The clearer and more complete the issue is, the more likely we'll look at it quickly. We moved HLS behavior to be in line with what we do for DASH/SmoothStreaming in V2. For DASH/SmoothStreaming we didn't ever adapt audio only playbacks by default, and so we moved HLS behavior to be in line with that.

It's somewhat unclear whether this should be considered a bug of enhancement, so I've marked with both labels for now. I am working on this. Sorry for the delay on the answer.

 I don't have permission to access the link provided above. Is it possible for you to fix that? You can always send private links to dev.exoplayer@gmail.com. Received, thanks. Support for audio only adaptation should be available soon. Fixed in `dev-v2`. Please give it a try! Please provide a stream URL rather than an apk if you're still seeing an issue. Thanks. Adaptive audio is being enabled properly for the test stream in the demo app. If you look at the logging, you can see all five streams being selected:
```
EventLogger:     Group:0, adaptive_supported=YES_NOT_SEAMLESS [
EventLogger:       [X] Track:0, id=0, mimeType=audio/mp4a-latm, bitrate=31000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:1, id=1, mimeType=audio/mp4a-latm, bitrate=63000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:2, id=2, mimeType=audio/mp4a-latm, bitrate=96000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:3, id=3, mimeType=audio/mp4a-latm, bitrate=128000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:4, id=4, mimeType=audio/mp4a-latm, bitrate=195000, channels=2, sample_rate=44100, supported=YES
EventLogger:     ]
```
So I'm unclear what the issue is that you're seeing. What do you expect to happen that's not happening? Perhaps you've concluded that the format is not adapting because you're not seeing any logging that looks like:
```
EventLogger: audioFormatChanged [3.48, id=1/15, mimeType=audio/mp4a-latm, channels=2, sample_rate=44100]
```
This is because the five streams are so similar, they actually look identical to the audio renderer that reports these format change events. If you want to see when the format changes, you can use `onDownstreamFormatChanged` instead, which is triggered whenever the format changes for this use case. If you add a log line to that method in `EventLogger` in the demo app, you should be able to see the format changing. Hi Vivek

I tested with the stream you sent me with ExoPlayer v2.4.0. 
I added some log statements to make track selection visible:

When testing without network throttling I see this selection:

measured effective bitrate is 25030856 so it always select the highest quality available:
195000 -> .../segment4_4_a.ts   (_4_a.ts)

```
04-28 15:36:09.693 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: getSelectedIndex: 0
04-28 15:36:09.693 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: bitrateEstimate: 33374476
04-28 15:36:09.693 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: effectiveBitrate: 25030856
04-28 15:36:09.693 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: available formats: 5
04-28 15:36:09.694 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: check format with bitrate: 195000
04-28 15:36:09.694 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: updateSelectedTrack: 0
04-28 15:36:09.694 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: getSelectedIndex: 0
04-28 15:36:09.698 7322-7753/com.google.android.exoplayer2.demo D/adaptiveaudio: http://hungcomiosta-vh.akamaihd.net/i/VideoS3/r/ms2/20673450/4/1235/Purza_,32001,64001,96001,128001,196001,.mp4.csmil/segment4_4_a.ts
04-28 15:36:10.510 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: getSelectedIndex: 0
04-28 15:36:10.510 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: bitrateEstimate: 33374476
04-28 15:36:10.511 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: effectiveBitrate: 25030856
04-28 15:36:10.511 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: available formats: 5
04-28 15:36:10.511 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: check format with bitrate: 195000
04-28 15:36:10.511 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: updateSelectedTrack: 0
04-28 15:36:10.511 7322-7749/com.google.android.exoplayer2.demo D/adaptiveaudio: getSelectedIndex: 0
04-28 15:36:10.514 7322-7753/com.google.android.exoplayer2.demo D/adaptiveaudio: http://hungcomiosta-vh.akamaihd.net/i/VideoS3/r/ms2/20673450/4/1235/Purza_,32001,64001,96001,128001,196001,.mp4.csmil/segment5_4_a.ts
```


When throttling the network it selects segments for 128000 bitrate. It checks the highest quality, recognizes it's above the available bitrate, checks he next which is ok and hence selected:

128000 -> ../segment2_3_a.ts  (_3_a.ts)

```
4-28 15:40:23.522 7322-7888/com.google.android.exoplayer2.demo D/adaptiveaudio: effectiveBitrate: 178064
04-28 15:40:23.522 7322-7888/com.google.android.exoplayer2.demo D/adaptiveaudio: available formats: 5
04-28 15:40:23.522 7322-7888/com.google.android.exoplayer2.demo D/adaptiveaudio: check format with bitrate: 195000
04-28 15:40:23.522 7322-7888/com.google.android.exoplayer2.demo D/adaptiveaudio: check format with bitrate: 128000
04-28 15:40:23.522 7322-7888/com.google.android.exoplayer2.demo D/adaptiveaudio: updateSelectedTrack: 1
04-28 15:40:23.522 7322-7888/com.google.android.exoplayer2.demo D/adaptiveaudio: getSelectedIndex: 1
04-28 15:40:23.529 7322-7891/com.google.android.exoplayer2.demo D/adaptiveaudio: http://hungcomiosta-vh.akamaihd.net/i/VideoS3/r/ms2/20673450/4/1235/Purza_,32001,64001,96001,128001,196001,.mp4.csmil/segment2_3_a.ts
``` When playing the stream with the APK you sent I see you are using ExoPlayer 2.3.1.

> Init ExoPlayerLib/2.3.1

Can you please upgrade to 2.4.0 and confirm? Hello @hungamademo. I'll try to help as best as I can:

> does not suppoet adaptive streaming by default

This is the intended behavior, so if it's not happening please file a new issue (don't forget to complete the issue template).

> We need to change it manually(by enabling random adaption mode).

Note that random adaptation has only debugging purposes (as far as I know). It basically randomly chooses a variant every time a selection update is made. This is not intended to be used on production environments. However, the fact that turning it on triggers adaptations, suggests that the adaptive selection is actually being made.

Since the default bitrate (See AdaptiveTrackSelection#DEFAULT_MAX_INITIAL_BITRATE) is 800kbps, it is likely that the highest audio quality is the default every time and, unless the network bandwidth is not sufficient, adaptation will not be triggered. You should be able to test this by throttling down the network speed.

The stuttering you observe in Random adaptation may be caused by the fact that HLS requires overlapping segments downloads for adaptation (unlike DASH, for example). So, when the chunk downloader needs to choose the next segment to download, if random, it is likely that the variant gets changed, resulting in the buffered position not advancing. Let me know if you don't think this is the case.

A fast way to check adaptation is working is making sure the lowest bitrate variant is selected at first. For example, by using a low `maxInitialBitrate` in the AdaptiveTrackSelection.Factory you pass to the `DefaultTrackSelector`. Let me know if you run into any issues. Just recapping what we've already said in this thread:

1. "Random adaptation mode" is for debugging purposes only, and adapts at random ignoring current network conditions. You should not be enabling this, ever, for actual playbacks. It's for testing purposes only.

2. ExoPlayer `2.4.0` and later *do* support adaptive streaming by default, without the need to do anything like enabling random adaptation mode. When using the demo app, this is indicated in the logs by `[X]` appearing next to multiple streams such as in the sample log output below. Do you see this type of logging in your own tests, or not?
```
EventLogger:     Group:0, adaptive_supported=YES_NOT_SEAMLESS [
EventLogger:       [X] Track:0, id=0, mimeType=audio/mp4a-latm, bitrate=31000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:1, id=1, mimeType=audio/mp4a-latm, bitrate=63000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:2, id=2, mimeType=audio/mp4a-latm, bitrate=96000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:3, id=3, mimeType=audio/mp4a-latm, bitrate=128000, channels=2, sample_rate=44100, supported=YES
EventLogger:       [X] Track:4, id=4, mimeType=audio/mp4a-latm, bitrate=195000, channels=2, sample_rate=44100, supported=YES
EventLogger:     ]
```

3. @marcbaechinger tested your streams under network variation and found that adaptation did occur. Please can you clearly explain how you're concluding that adaptation is not occurring in your testing?

There is currently no good evidence in this discussion thread that supports the claim that adaptation is not working. Our tests show that it is working. Please answer the specific questions above, else there's nothing actionable for us to do here. So playback *is* adaptive, and your issues are actually:

1. The initial selection is different to what it used to be.
1. Adaptation is too slow during playback.

Tackling each of these:

*  Make sure you're using a singleton instance of `BandwidthMeter` like the demo app does. This means that for all but the first playback, the bandwidth estimate derived from previous playbacks will be carried over to make the initial selection for the next playback. For the very first playback we have no bandwidth information, and so make the selection using an assumed bandwidth (800kbit/s by default). You are most likely instantiating an `AdaptiveTrackSelection.Factory` somewhere in your code. You can pass a different assumed bandwidth value through the 8-argument constructor if 800kbit/s is too high for your use case, which will result in a different initial selection, and hence solve this problem. It is true that basing initial selection on an assumed bandwidth is a change in behavior from earlier versions of ExoPlayer, which simply selected the first variant in the master playlist.
* For slow adaptation, I'd guess that the way you're simulating the bandwidth restriction is not indicative of what real networks do. I'd suggest instrumenting your application so that you can see how adaptation is performing for real users under real conditions.  It's quite difficult (although not impossible) to retain video playback across an activity recreation. The fact it's crashing in this way suggests that you've implemented an incorrect solution, that's trying to do something with a Surface after it has been destroyed. This is not an ExoPlayer issue as such.

I'm a little dubious about the correctness of the post you reference. Does the author provide a working example somewhere, and if so does that work for you? Are you able to recreate a working solution using MediaPlayer based on that post?

We do intend to try and provide some guidance in this area at some point, but it'll take us some time; it's not particularly easy to address.
  Sounds edit list related.
 This appears to be the same as #1659. We do not support prerolling when applying an edit list. The first sample (which is a synchronization sample) gets discarded when applying the edit.
 This is not really in scope for this issue tracker, but: to cut the video so it starts at an arbitrary position you will either need (1) a player that can pre-roll from the key-frame before the cut point, or (2) to re-encode (at least) the first group of pictures up to the key-frame following the cut point.

In case (1) the player has to decode almost a whole group of pictures in the worst case which will introduce a pause when playback starts. How bad this may be depends on key-frame interval and device performance.

In case (2) the player doesn't need to do anything special but (if you need to do this for arbitrary inputs) it might be difficult to restrict transcoding to just the first group of pictures and still produce a valid stream.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Thanks for the pull request. This is fixed already in `dev-v2` (see #1965) so I will close this.
  This will break any links to the existing location. I think there needs to be a better reason if it's to be moved.
 The file might be linked from anywhere. The most obvious other example I'm aware of (which granted, we could fix) is: https://bintray.com/google/exoplayer/exoplayer#release

In general, it's not possible for us to find and fix such links. Does naming the file CHANGELOG.md follow a standard naming convention that's widely accepted and documented somewhere? If so, can you point us to it. We might consider making the change in that case. We'd likely want to leave something in RELEASENOTES.md (e.g. a link to the new location, so it's not just a 404). If not, this seems little more than personal preference to me, in which case we're best off leaving it where it is.
  The recorded stream appears to have no IDR frames. Try setting [FLAG_ALLOW_NON_IDR_KEYFRAMES](https://github.com/google/ExoPlayer/blob/release-v2/library/src/main/java/com/google/android/exoplayer2/extractor/ts/DefaultStreamReaderFactory.java#L36) to treat non-IDR I frames as synchronization samples. This makes the video play on my test device.

See also #1894 and #1660 which are similar.
  Could you please download a ts chunk with these characteristics and send it to dev.exoplayer@gmail.com? So that I can have a look and test it myself.
 Sample content received; thanks!
  Sorry about that. For some reason Android Studio isn't showing this as a lint error (`./gradlew library:lint` does). This issue affects devices running Jellybean. We'll push a fix shortly.
 This is fixed in the `dev-v2` branch by the change ref'd above. We'll push a r2.0.4 release sometime this week containing the fix.
  This issue is being closed because it does not adhere to our issue template, and/or because it omits information requested in the issue template that is required for us to investigate the problem efficiently. The issue template can be found [here](https://github.com/google/ExoPlayer/blob/release-v2/ISSUE_TEMPLATE).
 
If you’re able to provide complete information as requested in the issue template, please do so below and we’ll re-open the issue. Thanks!  ExoPlayer doesn't support RTMP. See [these closed issues](https://github.com/google/ExoPlayer/issues?q=is%3Aissue+rtmp+is%3Aclosed).
  Please provide a bug report, as is explicitly requested in the issue template.
 The player is confused by text tracks in the stream, which have samples right at the start of file with timestamps corresponding to very near to the end of the stream. This is. . .unusual, but we should handle it.
 This will be fixed in `dev-v2` shortly. As an added bonus, you get album art as well when playing this stream in the demo app ;).
 Fixed in `dev-v2`.
  Please provide a full bug report, as is clearly requested in the issue template, along with the other requested information.
 Wowza should be appending the EXT-X-ENDLIST tag to their live stream HLS media playlists when a live streams end, else this is how the player will behave. I suggest you continue to raise this issue with the Wowza team.
 I believe ExoPlayer handles this case correctly already. This issue appears to be specifically about Wowza streams, where the tag is incorrectly not appended.
  Same as https://github.com/google/ExoPlayer/issues/1959.
  This is not something we support.
 Note: Depending on what exactly you mean by the decibel value, I'm not even sure that the question makes sense. What would you expect the reference point be?
 It's still unclear what you mean by decibel. That aside, this is not something we support directly. It would probably be possible for you to extend ExoPlayer's audio renderer and AudioTrack classes to add support for this, but this is not something we intend to support in the core library.
  Fixed. Thanks. We're still "fixing" that file manually as part of the release process, since we haven't had time to work out how to fix it automatically. I just got it wrong this time around!
  Preparing is treated as buffering in V2. Existence of the preparing state didn't generalize well to the introduction of playlist support. I don't think it's necessary for you to be able to distinguish between the two states. Do you have a use case where this is necessary?
 State changes in ExoPlayer are asynchronous by design, so there will be a small delay between performing an operation that changes the state and the state change taking effect and becoming visible. You still haven't explained why this is a problem for you.
  I'm a bit confused by this issue. If you want complete control over the final measurement of the view then you shouldn't be using `AspectRatioFrameLayout`. By definition it doesn't provide you with exact control.

I'm uncertain whether you understand correctly what the deformation fraction does. Would you tweak it to be larger or smaller, and what would you expect this to achieve?
 When you are using RESIZE_MODE_FIT which is the default mode you can control the dimension/measurement of the SimpleExoPlayerVideoView. The view itself is not resized in this case but only the video inside the view.
  Sorry for prematurely closing this. As @ojw28 correctly pointed out, if you are using Exoplayer, you could add an extension for decoding H.264 through software in a similar way as the [VP9 extension](https://github.com/google/ExoPlayer/tree/release-v2/extensions/vp9) does. I am sure someone else will be able to provide more accurate information about this.
 Right, you could implement an extension that bridges onto a software H264 decoder with main/high profile support. You can look at the VP9 extension for a general example of how to do this. It would be up to you to implement something similar that bridges onto a suitable H264 decoder. Note that software decoders are in general less efficient than the hardware supported decoders that are often exposed through MediaCodec. There's no guarantee that a software decoder would be able to decode fast enough, particularly at high bitrates. Power consumption will also likely be higher. This is something you'd have to experiment with and make your own decision on.

I think the example you provide is main profile. I would expect the vast majority of devices to handle main profile, despite it not being required by the Android CDD. YouTube use main profile quite liberally, although they do have a fallback I believe. Are there popular devices that you're seeing not support it? Would it be an option to also provide a low quality baseline profile stream as an additional Representation, for such devices?
 I would expect Nexus 6 to support main profile. Looking again at your sample, it appears to have DRM. Are you sure it's not a DRM issue / limitation that you're hitting? We'd need a test stream or at least a full bug report to debug further.
 Can you share the clear stream so we can take a look? I think it should be supported on a Nexus 6.
  Please stop filing issues that ignore the issue template (see also https://github.com/google/ExoPlayer/issues/1929). We will close all such issues without investigating.
  In this case I think correct media should duplicate Subtitle S in both Sample A (03:59.500 -> 04:00.000) and in Sample B (04:00.000 -> 04:02.000).

It seems wrong that a sample's content should extend beyond its duration. Such an approach wouldn't work properly when seeking, even with the fix you propose. For example if a seek occurred to 04:01.000 then the player would start from Sample B, and so wouldn't show Subtitle S even though it should be made visible. This is obviously not a major issue, but is an explanation of why it would be sensible for specifications to require duplication of the subtitle.

If you can point to a spec that indicates what you're doing is valid, please let us know.
 Note - When I say the seeking issue wouldn't be major, that really depends on how far subtitles extend beyond the end of their containing samples. If they extend a long way, it would be a more significant problem.
  Yes, you're right. Note that you can just not implement it that way in your own app ;). We'll get around to fixing the demo app to do something more sensible at some point, but it's not high priority.
  I doubt this is a memory issue. You'd expect a memory issue to cause an out-of-memory failure and result in process death, rather than playback stopping at random. We'll take a look.
 While I can repro this on Nexus 5x but it works on a Pixel phone. Not sure this is memory related.

Track: "WV: Clear HD (WebM, VP9)" of the category "Widevine DASH: WebM, VP9".

## Nexus 5X (Android 7.0)

Playback freezes when it tries to render the HD variant. When I restrict playback to the 720p variant by choosing only the smaller variant in the video track selector dialog, it plays fine. 

## Pixel (Android 7.1.1)

HD content plays fine. No issues.
 It sounds like the device just isn't capable of playing the 1080p variant at 1x speed. We attempt to filter streams that a device isn't able to play, but it's borderline impossible to get it right all of the time. There are multiple dimensions (resolution, bitrate, profile etc) to consider, and performance of software decoders can depend heavily on what else the device happens to be doing at the time. Even something like temperature can make a difference since it potentially affects CPU throttling.

Going forward, we probably need to start thinking about dynamically tracking whether the device is keeping up, and switching back to a lower quality automatically if not. This will also help automatically drop quality in cases where the playback rate is increased beyond the rate at which the device can decode.
 This is unrelated to your network connection. It's to do with the processing capability of the device + decoder. Decoding 10Mbit/s VP9 using a software decoder, which is what will happen on Nexus 5X, is very computationally expensive. IIRC Pixel has a hardware VP9 decoder, so will perform much better.

It should also be noted that 10Mbit/s is excessive for VP9 at 1080p. According to Netflix 5.8Mbit/s is sufficient for excellent quality 1080p streams, and that's using H.264. From memory, I think VP9 is supposed to save ~30% for equivalent quality, so 4Mbit/s should be sufficient. I'll follow up internally to find out why the streams in question are encoded at such a high bitrate.
  It looks like 'pls' is a type of playlist file, rather than an actual media stream. If you download the example link and open it in a text editor, it consists of only the following:

```
[playlist]
NumberOfEntries=1
File1=http://stream2.srr.ro:8000/
```

The underlying stream `http://stream2.srr.ro:8000` plays back fine in ExoPlayer. So it should be possible for you to write a super simple parser for pls files that extracts the underlying stream and passes it to ExoPlayer. This is not something we'll be supporting directly, though, so you'll need to do it in your own application code.
  There are multiple ways to add items to the demo app, which are documented [here](https://google.github.io/ExoPlayer/demo-application.html).

I'm not really sure what you're getting at here. I tried playing the sample to which you link and it plays fine in the demo app, with the time advancing correctly up to 1 minute. What do you mean when you say the "PTS will roll back to 0 again"? Are you referring to some internal state?
  We are unable to bump the gradle version at this time due to a bug in the recent version that prevents our tests from running successfully. See in the [file history](https://github.com/everalbum/ExoPlayer/commits/ce29611eee453223d23c33e88a721c46a4d2ecc5/gradle/wrapper/gradle-wrapper.properties) that we've already tried. We're trying to follow up with the Android plugin for gradle people to find out if and when they're going to fix this.
  If you're using `PlaybackControlView` then you may well be running into https://github.com/google/ExoPlayer/issues/1908, which is fixed in `dev-v2` and will be in the next release.

Either way, it should be possible for you to analyze the leak yourself to at least diagnose the problem. Android Studio provides tools to allow you to find out what the leak is and what the chain of references is that leaks it. Specifically, once the leak has occurred:
1. Trigger a few GCs to make sure everything is GC'd that can be.
2. Dump the Java heap (purple icon with the downward arrow on the screenshots you've provided above)
3. Look at it ;). Specifically, look at the number of instances of classes for which you know there should be some known number (often 0). A good example is SimpleExoPlayer, which you would probably expect there to be 0 of after you've exited the activity in which playback occurs. If there are unexpected instances, you can select each one in turn, and look at the reference tree to work out the chain of references to a GC root that's stopping it from being GC'd. That's your leak.
 I was able to reproduce. I didn't spend any time debugging, but did try using `dev-v2` tip-of-tree, and the issue is no longer present. So it looks like whatever the leak was, it's fixed. We'll be pushing a new release early next week.
  The easiest way to do this is simply to make some other completely black view visible on top of the surface. We already do this in `SimpleExoPlayerView` prior to the first frame being displayed, and also in the case that video output is disabled (either via the selector in the demo app, or as a result of playlist transition from video to audio-only). The black view is called `shutterView` in that class.

We should probably provide a mechanism to manually close the shutter, and/or a mechanism to close the shutter automatically at the start of a `prepare()` call. Although whether you'd want to do the latter depends a bit on what the `prepare()` call is for. If you're calling `prepare()` to retry after a playback error, for example, then you probably wouldn't want to close the shutter, since you'd expect to be resuming playback from the currently visible frame anyway.
 Marking as an enhancement to track supporting this with `SimpleExoPlayerView`. Note however that you can pretty easily do something like this in your app without any library support. The `SimpleExoPlayer.VideoListener.onRenderedFirstFrame` callback will indicate to you the correct moment to hide the black view and hence make the surface visible.
 We'll probably implement this by closing the shutter in `prepare` if the third argument is `true`. Currently this argument is called `resetTimeline`, but it would be renamed to a more general `resetState` as part of the change. Note that the one-argument version of `prepare` sets the third argument to `true` by default.
 I _think_ this is probably fixed in `dev-v2`. Could you give it a try and see?
 Just passing true as the last argument to prepare should be sufficient. You can simulate something equivalent in the demo app by:

1. Changing the third argument to prepare be true in PlayerActivity.
1. Start playback of a piece of media.
1. Toggle device into airplane mode using pull down drawer. Wait for playback to fail.
1. Seek to a different position. Turn airplane mode off using pull down drawer.
1. Hit the "Retry" button.

You should see the view go black. The first frame displayed should be from the new position (i.e. from thew new source provided to the player in the final step). This is what I see, which suggests this is working correctly? I don't think it's relevant. The above is just a way to trigger the same behavior. If you still have an issue then please provide as source a way for us to reproduce the issue, ideally as a small patch to the demo app.  We don't currently support seeking in live HLS. This is indicated in the `Timeline` exposed by the player. If you use the demo app, you'll see that the seek controls are disabled when playing such media.

Adding support for seeking in live HLS is tracked by https://github.com/google/ExoPlayer/issues/87.
  Can you provide a stream that reproduces the issue?
 Unfortunately, I cannot reproduce the issue. 

If you are completely certain that the same media, on the same device used to work with MediaPlayer, I can only guess that the codec capabilities advertising is not correct or we are not correctly doing the check. In any case, I haven't heard of these things happening before. 

If you need/want to confirm this quickly, you could try disabling the codec profile/level check and see if things change. In the meantime, I'll see if I can get one of these devices to reproduce the issue.
 I'll see if I can reproduce it myself with one of the listed devices.
 Both links play well in Galaxy Tab 4 Active, in the Demo App, last exoplayer version. Can you confirm that you can reproduce the issue on this device in the Demo App? Considering the profile is high for the video, it wouldn't surprise me if some of the devices don't support it. But in that case, the streams shouldn't play in MediaPlayer either. Let me know if you have any more data, but I won't be able to help you if I cannot reproduce this. And I certainly cannot try every device you list, unfortunately.
 There's not enough information provided in this issue for us to investigate. We'd likely need a full bug report captured from the device shortly after experiencing the problem. Please file a new issue if you're able to provide one. Thanks!  I don't know, sorry. I don't think it's an ExoPlayer bug though. You should probably report the issue to the tool bar.  I could be wrong, but I don't think the doc you reference is relevant to SmoothStreaming. We've always unconditionally assumed that SmoothStreaming uses length-delimited NAL units (i.e. without start codes, or AVC1 style in the doc you reference). Hence FourCC="H264" does not seem to imply start-code-delimited NAL units in SmoothStreaming, else the samples we have wouldn't work.

The parser is definitely failing to parse your content, but without a sample stream or at least a working snapshot of it, we'll not be able to debug the issue. Please provide sample media if you wish for us to take a further look.
 I don't really understand your comment about the FMP4 parser needing an enhancement. The source media is broken, and the fix is to correct the media to be valid.
  This is quite far outside the scope of the issue tracker. We're not familiar enough with the Android-FFmpeg library to provide an answer. It seems almost like this is more a question for the Android-FFmpeg developers than it is for us.
  This is not something we'll be supporting with the in-memory buffer. We're working on enhancing on-disk caching to help better support this case. This is tracked by https://github.com/google/ExoPlayer/issues/420.
  There are two timers in the video stream that drift apart. We stay in sync with the bottom one. This is the same as what Safari does. If you're expecting the playback controls to stay in sync with the top timer rather than the bottom one then I think you're expecting the wrong thing.

Please clarify.
 Ah. This is device and setup specific rather than HLS specific, I think. Please see the comment about Nexus Player on [this](https://google.github.io/ExoPlayer/supported-devices.html) page, which I think is what you're seeing.
 No worries!
  This is not a feature supported by ExoPlayer, sorry! It's probably possible to do, but how is something you'd need to work out for yourself.
  AES-128 encrypted HLS streams should work in an identical way to clear HLS streams. As an application developer, you shouldn't need to do anything differently. So please just refer to playback of clear HLS streams in the demo app.
 I'll have a look.
 By chance, have you managed to play the provided stream with any other player? I strongly suspect it's malformed. We cannot help you with that, unfortunately. Please close if that's the case.
 The source media looks broken to me too. It doesn't play in Safari or VLC either. Just looking at the manifest, every segment appears to be referencing a different (but overlapping) part of the same file (002test.ts), which definitely doesn't look right.

If you are able to produce HLS AES-128 content that works in Safari/VLC but not in ExoPlayer, please file a new issue. For this one, it looks like the media is just broken.
  If the HLS playlists contains incorrect information, surely this is a content issue rather than a player issue. Please clarify.
 There is nothing wrong with the media playlists, I think. The issue is that PTS's from different variants do not match. When exoplayer adapts(in your case, it happens in the second segemnt), the discontinuity is found in the audio timestamps, making the player think that the position has changed drastically. Unfortunately, there is not much we can do about this. Having no issue with other players does not necessarily mean it will not occur, as it depends on adaptation.

I say we cannot do much, because this is against the spec, and supporting media that is not spec compliant is bad for everyone. In the last available version of the [HLS spec, section 6.2.4](https://tools.ietf.org/html/draft-pantos-http-live-streaming-20#section-6.2.4):

> Each Variant Stream MUST present the same content.
> 
> Matching content in Variant Streams MUST have matching timestamps.
> allows clients to synchronize the media.
  Please read the issue template and provide all of the requested information as a new issue. It _explicitly_ states in the issue template that a snippet of logcat is not sufficient and that we require a full bug report. Also, if this is media specific, we'll need media that reproduces the issue.
  Yes, this is on our list. It's not high priority at this point, but we'll get to it.
 This is blocked on the release of support library 25.1.  Since no additional information was provided, I will assume this is a duplicate of #1894.
 This thread is not getting an answer because it contains close to no information. That's why it was closed. The other thread did get an answer.
  - I'm pretty sure all devices should work with L3. If you know of devices where forcing L3 with setPropertyString doesn't work, please let us know exactly which models and OS versions. Ideally please provide bug reports so we can take a look.
- A MediaDrm should default to its best supported level without you having to do anything. Once you've instantiated a MediaDrm instance, you can query its level using mediaDrm.getPropertyString("securityLevel"). If the best supported level is L1 then it should be possible for you to force L3 by calling mediaDrm.setPropertyString("securityLevel", "L3"). If the best supported level is L3 then you will not be able to force L1. You can query the level again after an attempt to force to see whether forcing was successful. Forcing of level is only allowed prior to opening any sessions.
 @mlondon1 we reviewed a HTC10 log provided via email. The 'error' above is really a warning and is not the root cause.

The log did not indicate that the error is not in the DRM space.
Here is an interesting bit:
10-11 14:14:52.222: E/ExoPlayerImplInternal(14445): Caused by: com.google.android.exoplayer2.upstream.HttpDataSource$InvalidResponseCodeException: Response code: 404

Could this be an issue in serving up the content? Alternatively we have seen an issue similar to this and the cause was an inaccurate URL in the MPD.

Does this content fail on all devices or just some devices? 
 @wvpaf / @jefftinker - This looks quite serious. When the CDM requests provisioning, we generate the URL to which the request is sent as:

```
String url = request.getDefaultUrl() + "&signedRequest=" + new String(request.getData());
```

But we're seeing 404s. This seems to occur across all devices.
 @mlondon1 - How are you implementing `executeProvisionRequest`? If you've taken ExoPlayer's default implementation, please try nulling out the request properties so that the `Content-Type` property doesn't get set. This appears to fix the issue when I tried it.
 Yes. This is an unfortunate issue that I think we introduced as part of our work to support Cronet via an extension. Investigating possible fixes now.
 The change above fixes V1, where the issue was introduced as a regression in r1.5.11. We'll push a fix for V2 tomorrow.
 This should now be fixed in `dev-v2`. Sorry for the inconvenience. We'll cut another release probably early next week that includes the fix. If you could verify the fix in the meantime, that would be great.
 It seems highly unlikely to me that the failure in the response above is related to this issue. The fix for this issue is only touching provisioning requests and POST requests via DefaultHttpDataSource. Changing these areas shouldn't in any way result in failures in the media decoder.

You can probably verify the issue is unrelated pretty easily by patching the change in and out. I doubt this will reliably toggle the failure on and off. I doubt you'll find going back between tip-of-tree dev-2 and r2.0.2 makes any difference either, since I don't think we've changed anything that would have caused this as a new issue. It would be good if you could check though.
 That seems similarly unlikely. There were only [19 commits](https://github.com/google/ExoPlayer/pull/1905) in 2.0.2 and I can't see any of them affecting this. If this is really true, please can you binary search to the exact commit that broke things (should take <=5 attempts) so that we can take a look and fix whatever the issue is.

There are other important fixes in r2.0.3 (just released). I wouldn't recommend basing anything on earlier revisions of v2.
 Thanks. The issue looks like some kind of device specific thing to be honest. I wouldn't be surprised if it reproduces on all ExoPlayer releases (including v1).
  You'd still need to ensure you call your additional methods before any call to `ExoPlayer.prepare`, else you'd have no guarantee that the properties are set prior to requests being triggered. Are you doing this?
  Yes, I have a pending change that fixes this already, and makes a bunch of other tweaks to the new UI components.
  You should really follow up with the device manufacturer if this is a device specific performance problem, and if you have a relationship with them.
  Hi. Thanks for you report. 
Can you please check this post about what information we need to handle an issue? Many thanks: https://medium.com/google-exoplayer/howto-1-reporting-an-issue-56a3ed73a5
 Have you checked that this is not related to #1978?
 Also, can you reproduce the issue in V1?
 Closing as this is most likely a duplicate of #1978 and no update has been made.   For non-technical reasons this is currently non-trivial for us to do, unfortunately. If someone else were to set this up then I think it would be possible to add links from relevant parts of our documentation (e.g. [this page](https://google.github.io/ExoPlayer/demo-application.html)), although I'd need to double check this.  We'll push a fix for this shortly; thanks!
  I think it is trivial; just seek back to 0 (by calling seekTo(0)) and that's it? Note that the ExoPlayer demo app allows you to seek back after the state has transitioned to ended (with the player controls).
  I tried this by adding the following entry into `media.exolist.json` in the demo app:

```
      {
        "name": "Test",
        "uri": "file:///android_asset/video_guide.mp4"
      },
```

and also adding an asset of that name to the app, and it worked fine on 7.0.

Please try and reproduce using the demo app. If you cannot reproduce, this is likely some error in your application code. If you can, please provide clear reproduction steps and a full bugreport (captured with `adb bugreport` shortly after encountering the issue).
 Please can you read the [issue template](https://github.com/google/ExoPlayer/issues/new) (i.e. the default contents of the issue box) and provide all of the information requested? You're not providing sufficient information for us to help you.
  Thanks!
  The logging is indicative of you not clearing the old `Surface` from the player when it's destroyed. It looks like an application error, in that you're most likely failing to do this.
  If it's possible to provide an easy way to reproduce this as source (e.g. a patch for the demo app, or a link to some other open source demo app that reproduces the issue) that would be great.

The only way I can see this happening is if the ExtractorMediaSource has been released whilst one of its child ExtractorMediaPeriods has not, and I can't see an obvious way that can happen.
 Which versions of ExoPlayer are you seeing this with? Unless you've shipped your app extremely recently I'd expect you to be seeing a different stack trace (even if only by the line numbers).
 Could you point me to your app in Play Store? I'd be interested in trying to reproduce. Thanks!
 @mopsalarm - I checked out master and switched the dependency back to vanilla ExoPlayer. Should that be sufficient to get the app into a state where it might reproduce the issue (albeit that reproducing might be difficult). Do you know in which fragment the issue occurs, and what browsing steps I'd need to take to get to that fragment in the app? If not, that might be something you could usefully log in future releases, if you fancy helping to chase this down.
 Actually, it reproduces relatively easily (e.g. within 5 minutes or so) if you unleash the monkey on your app, like:

```
adb shell monkey -p com.pr0gramm.app -v 500
```

(repeat until crash occurs).
  @mlondon1  can you help provide a full ADB log? 

Other SS tablets have suffered from HDCP issues, which may cause the decrypt errors reported, and upgrading to L seem to fixed the other issue.
 Closing due to inactivity.  This is tracked by https://github.com/google/ExoPlayer/issues/87.
  @cjveldkamp can you please provide some offending test content we can take a look. 
 @cjveldkamp any updates, else will close as stale. Closing due to lack of activity.  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
  This is demonstrated by the app here:
https://github.com/PerkmannM/Demo-ExoPlayer
 Notes:
- The leak is much less severe if you upgrade to ExoPlayer r2.0.2 (it's still there though).
- Until we fix this, you can easily work around the problem, or at least mitigate it, by explicitly clearing the player from the view when it's no longer required, like: `mPlaybackControlView.setPlayer(null);` (for example on `onStop`).
  Is this using the ExoPlayer demo app, or something you're written yourself?
  Checkout how we do it in V2 `MediaCodecInfo.isCodecSupported(String codec)`. We currently implement the checks for h264 and h265, I think, but the idea is the same for all of them. If you have any further questions, please try a general android Q&A.
 Yes, 

> MediaCodecUtil.maxH264DecodableFrameSize() decides the maximum decodable frame size according to the codec profile level. 

is true, but this is only so for older APIs in v2. Can you give v2 a try? It does the correct checks for h264 and h265, but is conservative about other decoders, and returns true if not. Also, what format are you trying to play? 
 >  Is it possible to make use of only some classes or functions from V2?

I wouldn't advise that, really. You can try, however, backporting some of the stuff from V2 and patching whatever you need.

> We are decoding and playing H246-based HLS stream

I assume you meant H264. It is strange as the calculation should be correct. Try attaching a debugger and see what's going on, another approach I'd recommend is playing the problematic streams in v2 and see if the behavior is any different. This doesn't require any effort from your part, but if we manage to reproduce it we might be able to help you.
 If the HLS master playlist includes RESOLUTION tags then I think streams beyond the device's capabilities are filtered out already (in V1 and V2).
 Please provide sample content for us to investigate this further and a full bug report captured shortly after the issue occurs.
 Closing as no sample content or bug report was provided.
  The stack trace provided above is a network timeout, which is expected if you have a bad network. Since you haven't provided any of the information requested in the issue template, we wont be investigating further. Please provide complete information as requested in future issues.

Please also see the part of [this page](https://google.github.io/ExoPlayer/supported-devices.html) that refers to Fire OS. Thanks!
  If you could spend some time debugging this kind of issue prior to filing, it would sure help reduce our workload and allow us to focus on getting actual work done ;).

In this case, the stack trace indicates the failure happens [here](https://github.com/google/ExoPlayer/blob/f8a8302f7bbcb47997095bd97b086eb633e49972/library/src/main/java/com/google/android/exoplayer2/source/dash/manifest/DashManifestParser.java#L227) because we're trying to parse the AdaptationSet id attribute as an integer, but in the manifest it has value "v1". The DASH specification (ISO 23009-1) defines id like this:

```
<xs:attribute name="id" type="xs:unsignedInt"/>
```

So the manifest is broken rather than the player (the AdaptationSet id element should be an integer).
  Yeah. Note this is only the demo app. It doesn't stop other applications from using some other version of okhttp.
 Oh wait, it's not the demo app. I don't think it's correct to make this restriction. If we did so and if some other library required some other version of okhttp, wouldn't it become impossible to resolve which version to use in any case where a developer wanted to use both?

I think the specific version of a dependency should be specified in the gradle file corresponding to whatever the end product is (typically an app). Each library should define its dependency versions as loosely as possible, to give the developer of the end product flexibility in version choice and to minimize the chances that it'll be impossible to resolve a suitable version.
 As per above, I'm think the version should be specified in the gradle file corresponding to whatever the end product is. Gradle should be clever enough to pick the version specified there when resolving dependencies.

If libraries are overly specific in which version they require then dependency resolution will fail instead, which isn't what you'd want to happen. For example if one library specifies a dependency on `3.4.1` and some other library specifies a dependency on `3.4.0`, there is no way that Gradle can resolve a suitable version. Conversely, if each library specifies `+` Gradle can resolve a suitable version. Furthermore, if each library specifies `+` and the application specifies `3.4.1`, Gradle can still resolve a suitable version and in addition the version is suitably constrained so as not to cause unpredictable behavior.
 Hmm. May well have been wrong about this. Discussing / figuring out exactly what best practice is :)...
  ExoPlayer doesn't use `MediaExtractor`. We instead implement our own set of extractors directly in the library (they're all in [this package](https://github.com/google/ExoPlayer/tree/release-v2/library/src/main/java/com/google/android/exoplayer2/extractor)). We did this for quite a few reasons, one of which was to allow us to more easily fix extractor bugs and support a wider range of file types.

It would be possible for you to replace your use of `MediaExtractor` with pieces of `ExoPlayer`, although there's no simple way just to swap them around. `ExtractorMediaSource` is probably the closest to what you want.

It's somewhat unclear why you're working directly with `MediaCodec`. Using ExoPlayer's extractors and feeding the data into `MediaCodec` sounds exactly like what full `ExoPlayer` just does for you...!
 Yeah, ExoPlayer is likely not optimized for this use case. It remains an option for you to use just the extractors provided by the ExoPlayer library, but exactly how to do this is dependent on your other application code, and is something you'll have to figure out for yourself.
  Yes, this is a bug. I will use this to keep track of this. Also there is a possibly related issue that dates from the v1 older versions reproducible thorugh the Apple master playlist advanced (TS) stream by alternating from text option 2 to 1.
 My fault.
 Note - A similar issue also affects `MergingMediaSource` in the case that two of the children being merged contain tracks of the same type, and if the track selection for a renderer is switched from a track belonging to one child to a track belonging to the other in a single selection step.
  We'll fix the crash (thanks!), but we wont be adding support for streams of this type (FLV containing MP3 and H263). We would accept a high quality pull request to add support, if someone external feels like spending time on it. We only support FLV at all due to an external contribution, and that contribution only supported FLV containing AAC and H264.
 Fixed to the extent it's going to be fixed. Feel free to send a pull request to add MP3 / H263 support in FLV if you feel motivated enough to create one!
  I only looked at the first stream. It starts consistently within a second for me. I think the only explanation for the delay you're seeing is because you have a much lower bandwidth connection.

The reason long duration MP4 files take longer to start is because they contain an sample index in the header at the start of the file. If you have a longer duration video there will be more samples, so the header will be larger. In the case of the first stream the index is about 5.5MB, so the player has to download at least that much data before it can start playback.

There isn't much we can do about that. If you control the media, you can choose a file format that's better suited to the combination of long duration, low bandwidth and fast startup. Specifically, fragmented MP4 (FMP4) distributes the index into smaller fragments within the file to avoid having a huge header at the start. I have a feeling Matroska and WebM do the same, but I could be mistaken.
 Packaging of content is outside the scope of this issue tracker. There are plenty of resources on the internet to help with that kinda thing, also!
  This is an interesting edge case. When a seek occurs to the end we always fetch the last chunk of each stream so that we can render the "end state" of the media. In this case demuxed subtitles are also enabled. The subtitles have a shorter duration than the video. This can result in the following happening on seek specifically in the case that there's already an ongoing load of a video chunk:
1. Seek happens. As a result we trigger async cancellation of the ongoing video chunk load.
2. We evaluate which streams we should trigger loads for next. Since we're immediately after a seek both video and subtitle streams are eligible, but we're still waiting for the video chunk cancellation to finish. Hence the subtitle chunk load starts. The video chunk load is deferred to be started when the asynchronous cancellation of the previous load completes.
3. The subtitle chunk load blocks waiting for the video load to start due to the nasty timestamp adjustment necessary for playback of HLS content.
4. The cancellation of the previous video load completes. We evaluate which streams we should trigger loads for next. At this point what should happen is that the video chunk load should start, but the subtitle stream now has a next load position that's earlier than the seek position was. As a result we conclude loading of subtitles is "behind" and that we should avoid loading anything else until it's caught up. The result of which is that we're stuck.

There are a whole bunch of ways we could fix this. TBC which one is best!
 Note - This issue only occurs for (a) HLS, (b) with demuxed subtitles enabled, (c) when playing media where the subtitles are shorter than the duration of the video chunks, and (d) when a video load is in progress when a seek happens. (d) is relatively likely given (a), (b) + (c), but that combination is probably fairly rare to start with. So it's not super high priority for us to fix, although regardless we'll get to it soon.
  It got replaced with the 2.x reference, but I've restored the 1.x reference to http://google.github.io/ExoPlayer/doc/reference-v1/ and will also add a link at the top of the deprecated developer guide (https://google.github.io/ExoPlayer/guide-v1.html).
  Not currently, but it's trivial to add a `getPlayer` method. We will do this.
  I will be closing this now. I think the file contains I-Frames, which are not actual IDR frames. We will possibly investigate this in the future, but it's not high priority right now. As @gpinigin said, try using the FLAG_ALLOW_NON_IDR_KEYFRAMES flag.
 Have a look at DefaultTsPayloadReaderFactory. The constructor's `flags` argument allows you to set the flag. No library code needs modification. You will need to provide your own ExtractorsFactory(instead of the default one), replacing the default TsExtractor constructor for the one that takes a PayloadReaderFactory. If you know you are going to play only Ts files, you can just provide that one. Filed https://github.com/google/ExoPlayer/issues/2657.  Without links to sample content we're not able to assist debugging this issue. I think the default buffer settings should be fine for 2mbit/s and 6mbit/s.
 You can send links to `dev.exoplayer@gmail.com` as in the issue template, thanks!
 Links provided do not appear functional. Closing due to lack of information. As per my earlier reply, the default buffer settings should be fine for 6mbit/s streams.
  Where in your code are you registering the listener? There should be a call to `SimpleExoPlayer.setVideoListener` somewhere?
  There's nowhere near enough information here for us to debug your issue. You're not even indicating which app you're talking about, and haven't provided a full bug report as is explicitly requested in the issue template. Please provide complete information, or we'll have to close this. Thanks!
  The provided URL no longer provides a valid playlist. Can you fix this?
 Closing due to lack of required stream.
  I've created https://bintray.com/google/exoplayer/extension-okhttp. Bintray need to approve its addition to jCenter, I think. Hopefully this should happen pretty soon.
 This is a dupe of https://github.com/google/ExoPlayer/issues/1157, so closing this.
  This is already fixed in 'dev-v2'.
  This is discussed in some detail in #26. @andrewlewis is planning to take a look at playback rate changes on earlier API levels, so I'm assigning that issue to him.
  It's very unclear what the question is here. Please clarify.
  @ChernyshovYuriy I understand that CWIP team got you answers. Please reopen if that is not the case.
  @ChernyshovYuriy  it would be helpful to get an ADB bugreport from the device so we can tell why the provisioning request failed.
 Are you a CWIP partner? if so please use the CWIP portal to submit logs. Else we will need to get you a good email address. 
 @ChernyshovYuriy I understand that CWIP team got you answers. Please reopen if that is not the case.
  Please see https://github.com/google/ExoPlayer/issues/1842. We don't have any plans to change the artifact id for v2.
  1 - It's working as intended that the track isn't selected by default. If you want this behaviour, you should specify `C.SELECTION_FLAG_DEFAULT` as the `selectionFlags` argument when creating the format for the text track. `DefaultTrackSelector` also exposes methods for selecting the track.

2 - This is a bug. We'll push a fix shortly.
 A fix has been pushed to `dev-v2`. Thanks!
  What are the chances of moving to version 2, where DefaultHlsTrackSelector does not exist? We are backporting bugfixes to v1. This is going to get more attention if you manage to reproduce it in v2. Please, give it a try and let me know.
 I'm unsure about this (it sounds pretty strange). Why is SurfaceView not a solution for you? It generally gives better performance and smoother frame release.
 Any news about this? I cannot reproduce on a Sony Bravia TV.
 Closing due to inactivity.  Yeah. If you're going to implement your own solution, it seems reasonable that you should be able to start with `SimpleExoPlayerView` and modify it to your needs (or understand how it works and do the same thing).

The shutter is a black view that's made visible over the surface prior to anything having been drawn into it. It can help avoid rendering artifacts prior to the first frame being drawn.
  You shouldn't be calling methods on UI components to query the state of the player (in V1 or V2). If you want to query the state of the player, query it directly.

A one minute investigation as to what that method did in V1 (by looking [here](https://github.com/google/ExoPlayer/blob/release-v1/library/src/main/java/com/google/android/exoplayer/util/PlayerControl.java#L84)) would tell you that it's equivalent to calling `getPlayWhenReady` directly on the player.
  This sounds like a general programming question, not an ExoPlayer issue. Please ask such questions somewhere like stack overflow. Thanks!
  The manifest you link to is quite badly malformed. The main problem is the video AdaptationSet. It's claiming that each video segment is `(49152 / 12288) = 4` seconds long. There are 24 of these segments, which gives a total duration of 96 seconds. This is 36 seconds longer than the top level presentation duration of 1 minute, which shouldn't happen. Each segment actually only contains about 2.5s of video. The result is that ExoPlayer thinks it has more video buffered than it actually does, the video buffer underruns, and playback stalls. Seeking will also not function efficiently, if at all. If the manifest works in V1, this is more by chance (and it definitely wont be playing in an efficient way).

Note that the duration of audio `(192000 / 48000) * 16 = 64`, also exceeds the top level presentation duration, which again should not happen.
  I have an idea of what the problem is, but please provide a piece of media that reproduces the issue, as stated in the issue template. 
 Okay, I will try to provide a fix for this by the next push hoping it is what I suspect. Once the fix is pushed we will close this. If you run into this again, just open a new issue. Make sure you include media to reproduce this! Thanks for reporting this.
  As per title, playback of a playlist can get stuck in the buffering state if the end times of the tracks of the sample coming to an end are uneven.
  I don't think I agree that it's more difficult. SimpleExoPlayerView didn't exist in 1.x, and so you effectively had to implement something equivalent in application code (in the demo app, this was done in PlayerActivity and in the corresponding layout file).

So even once you take v2, pretend SimpleExoPlayerView doesn't exist and re-implement it all yourself in your activity, you're still only at equal difficulty to v1. And once you've done that, it's just as easy as it was in v1 to replace the controller.

So I don't think the assertion that it's more difficult is valid. There is of course value in making it demonstrably easier, however, so using this issue to track that.
 We've made it somewhat easier to customize the UI components in `dev-v2`. We'll post a blog post on our [blog](https://medium.com/google-exoplayer) when we push a release that includes them. As per above, you still have the option of implementing your own UI components as well.
  If you're getting a 412 status code, that sounds like a server issue?
 Got it. I think our current implementation works on the assumption that any segments listed in the manifest are available from the server. We'd probably need to add support for parsing and applying LiveBackOff to support your content; there's no existing way to achieve something equivalent in the library.
 We don't infer future URLs. I'm not sure I fully understand why you'd list segments that aren't available in conjunction with LiveBackOff. Suppose LiveBackOff=10 and there are 10s worth of segments not yet available. Why wouldn't you just list 10s fewer segments and set LiveBackOff=0? It feels like it would be pretty much equivalent, and less error prone.
 > I'm working now to provide evidence to the content provider that the segments in the manifest are not valid and ideally this should be fixed on server side. any ideas how can I do that?

It should be pretty easy to verify that the most recent segments aren't available (just by fetching the latest manifest, parsing the latest segment, and trying to request it).

> What we see on other players though is that they handle those live streams nicely without error and smooth playback.

As an aside: Just because it works doesn't mean its handled nicely. Players may do things like hammer the server until the segment becomes available, for example. Unless you're looking at the requests using Wireshark or something, it's quite hard to validate how nicely (or otherwise) the case is handled.

> Also spend some time looking into the the exoplayer code. Saw that in SmoothStreamingChunkSource class there is a property called currentManifestChunkOffset.
> Will setting it with a default value make a difference and work around the current issue?

There is no way to work around this issue.
  The first of those is correct, and works fine for me. Note that you need

```
repositories {
  jcenter()
}
```

somewhere in your gradle file(s) too.
 NB - Updated response above to say jcenter rather than mavenCentral.
  Please see #1648.
  `DefaultDataSourceFactory` is the right thing to be using. It sounds like the `Uri` you're using isn't correctly formatted. Where _exactly_ is the asset located, and what `Uri` are you using to try and play it?
 There's some terminology confusion here, because there's also an `assets` directory, which is different to `res`. See [here](https://developer.android.com/studio/projects/index.html):

> Contains file that should be compiled into an .apk file as-is. You can navigate this directory in the same way as a typical file system using URIs and read files as a stream of bytes using the AssetManager . For example, this is a good location for textures and game data.

This folder is generally suitable for media. If that works for you, place your file in that directory and use `DefaultDataSourceFactory`. The Uri should be of the form `asset:///video_1.mp4`.

If you really do need to use `res/raw`, then (1) why?, and (2) you can use the newly added `RawResourceDataSource` class. You'll need to make your own factory for it. You can build a suitable Uri using `RawResourceDataSource.buildRawResourceUri`.
  This is not supported. I would have thought that if FFMPEG doesn't support adding UtcTiming elements, it probably doesn't support generating manifests that are formatted in such a way so as to require their presence. It wouldn't really make sense to do one without the other.

It sounds like you should be filing a feature request on FFMPEG in any case, or else manipulating the output of FFMPEG to manually insert the UtcTiming element yourself.
  I think this should be fixed in `dev-v2` now, but please give it a try and verify. Thanks! Great; thanks for verifying!  @teokw90  the full logcat the snippet does not show much. What movie/content was attempting to playback? Does it work on other devices? @teokw90 any update on the logs? Closing due to inactivity.  You need to provide the information requested in the issue template for us to help you. Specifically, we'll need a link to the media that you're trying to play. Also, please try and reproduce in the ExoPlayer demo app, which will normally tell you why if video is present in the media but not being played.
 I didn't test this, but try replacing this:

```
if (videoPlayer == null) {
  Handler mainHandler = new Handler();
  TrackSelection.Factory videoTrackSelectionFactory =
      new AdaptiveVideoTrackSelection.Factory(bandwidthMeter);
  TrackSelector trackSelector =
      new DefaultTrackSelector(mainHandler, videoTrackSelectionFactory);
  LoadControl loadControl = new DefaultLoadControl();
  videoPlayer = ExoPlayerFactory.newSimpleInstance(getContext(), trackSelector, loadControl);
}
```

with something that disables all of the video renderers, like:

```
if (videoPlayer == null) {
  Handler mainHandler = new Handler();
  TrackSelection.Factory videoTrackSelectionFactory =
      new AdaptiveVideoTrackSelection.Factory(bandwidthMeter);
  DefaultTrackSelector trackSelector =
      new DefaultTrackSelector(mainHandler, videoTrackSelectionFactory);
  LoadControl loadControl = new DefaultLoadControl();
  videoPlayer = ExoPlayerFactory.newSimpleInstance(getContext(), trackSelector, loadControl);
  for (int i = 0; i < videoPlayer.getRendererCount(); i++) {
    if (videoPlayer.getRendererType(i) == C.TRACK_TYPE_VIDEO) {
      trackSelector.setRendererDisabled(i, true);
    }
  }
}
```
  You're probably not detaching the `Surface` that the player is rendering to before it's destroyed. This results in the player trying to draw into a surface that's no longer valid, which may result in failure.

Please ensure you're listening to `surfaceDestroyed` and clearing the surface from the player in a blocking manner. You can refer to the V1 demo app for an example of this. In V2 `SimpleExoPlayer` manages this task for you, which makes things easier.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  We've pushed some much cleaner code for doing this to `dev-v2`, which can be found [here](https://github.com/google/ExoPlayer/blob/f194df114967e7f2a1dd42bd54d6a060b436dc2e/library/src/main/java/com/google/android/exoplayer2/drm/OfflineLicenseHelper.java#L190).  I was able to play a file containing a # symbol, but you have to be careful when you construct the `Uri` that gets passed to `ExtractorRendererBuilder` because # is a reserved character. See [this page](https://en.wikipedia.org/wiki/Percent-encoding) for details. The way the demo app works, you can add a sample to `Samples.java` by percent encoding the # as %23:

```
/storage/emulated/0/netease/cloudmusic/Music/Jerry Martin - SIMnata %2315.mp3
```

If you're creating the `Uri` using your own code, you'll have to come up with an equivalent way to create a valid `Uri` that points to the file.
  @erdemguven - Any chance you can apply some of your JNI skills and take a look at this? Thanks!
 Hi dlewanda, 

By any chance are you using robovm? If so please try the code without robovm (compile with the default android toolchain). Also please send a full bug report.
 Please send 

> A bug report taken from the device just after the issue occurs, attached as a
>   file. A bug report can be captured using "adb bugreport". Output from "adb
>   logcat" or a log snippet is not sufficient.
> 
> With your device attached, from the command-line (where ADB is accessible) run:
> 
> `adb bugreport > bugreport.txt`
> 
> This will generate a text file with the above name in the same directory, which you can then attached to the issue. Be patient, it may take 10 seconds or more to generate.
 Hi Dave, It looks like, somehow jOpusOutputBuffer object is passed to GetDirectBufferAddress (instead of the "data" field of the same object) on line 101 in file extensions/opus/src/main/jni/opus_jni.cc.

Unfortunately I don't have any idea about why/how it happens and can't say more without seeing the code.

You can try to debug it by putting some 'printf's to that function and one before (opusInit). In opusInit opusDataField should be initialized and in opusDecode function it's used to get the 'data' field of OpusOutputBuffer class. Which is then passed to GetDirectBufferAddress function.
 I also tried on the demo app with opus extension on a android 6.0.1 phone and it works fine.
 You're welcome.
  I'm pretty sure you can login with any regular Google account, so just using your personal GMail account, opening one if necessary, should be sufficient.

I agree this is not a good distribution mechanism. @kul3r4 - Can we please have a discussion with the Cronet team and come up with something better?
  It's important to understand what you mean by "exact position". Some people understand this as the exact timestamp of the video frame that's currently displayed, however note that in general positions reported during playback are at a finer granularity than video frame and are based on audio time.

Please could you clarify exactly what your use case is? Thanks.
 Got it, but note that even if we fixed things to behave in the way you expect, I'm not sure we'd want to guarantee that the seek call wouldn't re-buffer all of the currently buffered media. Why don't you just add a check that omits the call to seekTo if X is within ~100ms or so of the current position?
 I think you should just go with your fudge/workaround, and longer term try and resolve the constraints that are requiring you to seek instead of resume in the first place. In V2, we don't even have `onPlayWhenReadyCommitted` (although it's possible that it'll make a comeback).
 Closing this as working as intended. The behaviour is only problematic for the original poster due to what sound like unfortunate design decisions. A fudge/workaround is known, and in the medium to long term these decisions can probably be undone ;).  Please can you provide more information on how this is reproduced in the ExoPlayer demo app?

When I exit the playback activity and force 2x GCs via Android Monitor in Android Studio, the memory level reliably returns to its base level (I'm unsure why 2x GCs are required, but that's more likely a property of how the GC works than of ExoPlayer).

Also, if you do multiple playbacks successively, does the memory level increase indefinitely? I would expect that to happen (eventually resulting in an OutOfMemory failure) if there were genuinely a leak. Please clarify. Thanks!
 Please try with the demo app. Without further information, the memory leak you're seeing could be caused by anything in your application. A graph like the one above is not sufficient evidence the leak is ExoPlayer's fault.

As long as you're releasing the player and nulling the reference to it, that should be sufficient. You should do it before `onDestroy` though; doing it there is too late. You should be releasing resources in `onPause`, or `onStop` if the SDK version is 24 or higher. This is what the demo app does.
 We've pushed a change to `dev-v2` that will release memory proactively when `release` is called on the player.

Note however that you will only be experiencing this issue in the first place if you're leaking the player instance. We're unable to reproduce this problem in the demo app, and suspect that it's an issue in your own application code. You should work out where this leak is and fix it, since although our change will significantly reduce the size of the leak, the leak will still be there and you may be leaking other objects too.
 Thanks for the very helpful reproduction steps. I've filed #1908 to track this issue. I'll also post a workaround there.
  I think you're somehow managing to include a null entry in the array of `TrackRenderer` instances passed to `ExoPlayer.prepare`, which would be a bug in your application code. Either way it should be pretty easy to attach a debugger and work out why the player is trying to invoke a method on a null `TrackRenderer`.
 We will push a fix for this tomorrow.
  @andrewlewis - Fixing this may well link into getting all the `onPrepared` calls onto the right thread.
 The issue is best understood by looking at `MergingMediaSource.createPeriod`:

```
periods[i] = mediaSources[i].createPeriod(index, mergingPeriod, allocator, positionUs);
```

Some `createPeriod` calls ends up calling `onPrepared` synchronously, so such calls happen before `periods[i]` is actually set to the period being created. `MergingMediaSource.onPrepared` relies on all entries in `periods` being non-null at the point where `pendingChildPrepareCount` is decremented to 0.
  @replystreamingwidevine  Samsung device is this happening on?  Is it multiple models? Can you provide the names so we can investigate on our end?  Does this work on some Samsung devices? Can you provide some of those model names?

Also are you passing in large optionalParameters in to MediaDrm.getKeyRequest?
 no update in 4 weeks closing.
  Please provide all of the information requested in the issue template, otherwise we don't have enough data to help. Thanks!
  What version of ExoPlayer is this with? Please could you try with a recent version and report back (i.e. 1.5.11 or 2.0.0)?
 stitch.mp4 appears to be invalid: the number of video sample timestamps that can be derived from the `stts` box is different from the actual number of samples, so the player throws an error due when it asserts that the sample metadata is valid.

We might replace the assertion with a logged warning at some point, but the proper fix is to write the MP4 container correctly.
  It is not something we support. I'd suggest asking about this in a general programming Q&A site, like stack overflow. Also try to provide more information about why you need it.
  HLS playbacks using ExoPlayer are no longer constrained to start with the track given by `InitializationTrackSelection` (this is true at least for the `dev-v2` branch and for all future releases (i.e. 2.3.0 onward). Playback will start using whatever track is selected by the `TrackSelection`, in line with behaviour for DASH and SmoothStreaming.  Hello @b95505017, thanks for your contribution. 

The spec does not define the name attribute for the EXT-X-STREAM-INF. Why do you want it in? It is in v1, but I removed it from v2 as part of a refactoring process for the mentioned reason.
 Are they important providers or just providers who are doing it wrong (or both)? Can you provide examples so we can gauge importance. We'd rather not support something that's not in the spec without good reason. Also, do Apple support this, given it's not part of the spec?
 It's not useful if it's only one provider who's doing it wrong, as per my response above. Please provide concrete examples of providers who are violating the HLS spec in this way, and we'll assess whether it's something we want to support or not. Else we wont be supporting this. Thanks!
 We already parse NAME from EXT-X-MEDIA tags, as per the part of the spec to which you refer. This pull request parses NAME from EXT-X-STREAM-INF tags. The HLS spec does not define the NAME attribute on EXT-X-STREAM-INF tags.
  You're likely going to need to provide sample content and a code snippet showing how you're building the MediaSource for us to investigate this. Thanks.
 What workaround did you use? It would be good to merge something a workaround back into the core library, to handle the case generally on this particular device.

I would suspect that changing `MediaCodecRenderer` to return true from one (not sure which) of `codecNeedsFlushWorkaround`, `codecNeedsEosPropagationWorkaround` and `codecNeedsEosFlushWorkaround` on this device, for a particular decoder, may well do the trick?
 Please do play around with the workarounds mentioned if you have a chance. Thanks!
 Closing due to inactivity, although it would be preferable if you could try the workarounds mentioned so that we can merge something back into the core library if possible. Please update this issue if you have a chance to do this. Thanks!  @wvpaf - Could you follow up with the device manufacturer about this; it sounds fairly badly broken? Can we also check that this device will definitely fail some tests we now have in place to prevent this kind of thing going forward?
 @ojw28 thanks for sending our way. we will attempt to get an example of this device and run it up and see what can do. 
 Hi @lkcanal,
Can you verify an upgrade to M resolved the issue? 
  Including a CODECS attribute but failing to include the audio format in it is a violation of the HLS spec, as stated in #1832. The broken media should be fixed to resolve this issue.
  Good question. The package name of the classes in the library _has_ been changed, which allows v1 and v2 to co-exist in an application. We have however published the library using the existing name, which I think is what you're referring to. This makes it awkward to include two versions of the library at once using gradle, which is probably what you're referring to. It is possible, according to [this post on stack overflow](http://stackoverflow.com/questions/29374885/multiple-version-of-dependencies-in-gradle).

This was oversight as much as anything. Internally we use a different build system, and so we didn't encounter this issue. We'll have a think about whether it makes sense for us to adjust things. For now, you can either try and instructions linked above, or you can just get v1 as a jar and include it directly, and reference v2 in your gradle file. The latter is probably the easiest approach, particularly since you'll likely want to freeze the version of v1 that you're using when you start also bundling v2, and since this is a transient state where you'll be removing v1 from future builds once you're satisfied that v2 is stable.
  Closing due to insufficient information.
  This issue has been fixed in ExoPlayer 2 (i.e. 2.0.0 and greater). See #1285 for some more discussion about the issue.
 It really depends on what the bug is and what's required to fix it. If back-porting a fix requires us to also back-port a bunch of huge architectural changes that allowed the fix to happen, then it's not viable (and is approximately equivalent to you updating to v2 in any case). This is one such case, so you will need to update to v2.
  > however Shaka fails to handle the AdaptationSet while parsing the manifest

I think a player should probably ignore an AdaptationSet that it doesn't know how to handle, or at least have the option of doing so. If Shaka is failing for this reason then you should probably open a bug on that project asking if it would be more correct for them to ignore it.

> I'm trying to understand if this is a valid usage of DASH or if we should define our own app namespace in the XML and put the information under a custom element. Going with the latter I would definitely have to modify ExoPlayer a bit or somehow parse the XML a second time which complicates things a bit.

The DASH spec is very permissive (most often you'll find that something is just not defined, rather than being explicitly forbidden). So it feels quite likely that you're not explicitly violating the spec. You'd have to convince yourself though; I didn't look in any depth. On the assumption that you're not explicitly violating the spec, I guess the remaining risk would be that handling of contentType="image" AdaptationSet elements could be defined in some future version of the spec or in the DASH interop guidelines, in a way that's different to what you've done.

Note that if you control server and client side (for all clients), then interoperability becomes less important. There's nothing forcing you to use XML style DASH manifests at all in this scenario. You could use your own binary format (e.g. for efficiency) and include whatever you like. If using ExoPlayer in such a scenario, the extra client work you'd need to do would be to parse your binary format into a DashManifest instance for the player (whilst also doing whatever else you want to do with additional data in the response). Injecting a DashManifest might be awkward (not possible?) in V2 right now, but we'll add that functionality back in as/when someone needs it.
  We'll provide a fix for this shortly. It's unclear whether the second issue is the same, however.
 Tomorrow.
  Please could you try with ExoPlayer 2.0.0. Format selection and filtering is implemented consistently across different types of playback, so I'd expect that you'll find the content doesn't play either as DASH or regular MP4 in that case. The demo app will also show you a toast indicating that the format is not supported, if that's the case. Either that or playback will work for both DASH and regular MP4.

If playback doesn't work and you see the toast, this is because the device doesn't advertise support for resolutions that high, so we're "doing the right thing" in that case. In V2 it's not too hard to override the selection and try and play the stream anyway, if you think you know better.

Note also that it's unusual to provide DASH content consisting of a single representation at a very high resolution. Is this actually a real use case? Why aren't there lower resolution alternates?
 The post above appears to say exactly what the original poster says. I'm not sure there's any additional information, so my response above still holds. "Capable of UHD" isn't really well defined. What you really mean is  that the device is capable of UHD the exact way it's encoded in the videos you're playing, which isn't the same thing. If you're at or close the limit then the device may not advertise support (because it might not be possible to reliably determine whether playback will be successful or not).

Either way, as per the response above, please try using V2 (using the demo app is fine) and see what behavior is like with that.
  I'm don't think we have a good answer to this question/problem. It sounds very application specific, to the extent that it may or may not be an issue with application code (although if it is, it's curious that the resulting behavior appears to be device/os specific).

In any case, we can't prioritise investigating the issue vs other stuff, so closing. It sounds like you have a solution to the problem. You could also investigate TextureView, which may behave better for your use case.
  There is nothing special about how you'd treat ExoPlayer vs any other library when it comes to this issue, so you should just follow the advice [here](https://developer.android.com/studio/build/multidex.html).
  We were seeing a lot of cases where developers were using extension as a convenient but lazy (or worse, incorrect) way of achieving custom behaviors. In such cases it was often better to use composition or some other approach instead, or to make modifications to the library to support the functionality they desired more directly (in a way that all users of the library can benefit from). We made quite a few classes final to try and guide developers in these more correct directions. Note that we did this in V1; this isn't something that's new in V2.

We're happy to remove final modifiers where developers can demonstrate a specific use case for which we agree extension is the best way. So please file specific issues indicating what you're trying to do, and which classes would need to be non-final for it to be possible, and we'll consider them on a case-by-case basis.
 Why don't you just fix the broken media?
 That doesn't mean they shouldn't fix it. Assuming the content is owned by the same provider for whom the app is being produced, it's difficult to understand why they would refuse to amend the content...
  It's a violation of the HLS specification for the `CODECS` attribute to omit the audio format. As stated in the [spec](https://datatracker.ietf.org/doc/draft-pantos-http-live-streaming/?include_text=1):

> If an EXT-X-STREAM-INF tag or EXT-X-I-FRAME-STREAM-INF tag contains
>    the CODECS attribute, the attribute value MUST include every media
>    format [RFC6381] present in any Media Segment in any of the
>    Renditions specified by the Variant Stream.

Note that even if the audio is fully demuxed, its format must still be included since the `EXT-X-STREAM-INF` tag specifies demuxed rendition (called `aac` in the case you provide).
  This is most likely an issue with Rockchip's H265 decoder implementation, and you should report it directly to them.

That said, if you're able to provide us with test content using broken and working variants, we can take a more detailed look. It may also be possible for us to come up with an workaround for the issue.

Thanks!
 Closing due to insufficient information.
  A lot of lint warnings are warnings rather than errors because it's not possible to identify the case they're warning about in a way that avoids false positives. If there were a way to identify this kind of leak without false positives then it would likely be made an error rather than a warning.

In the cases you mention we do not believe there are unintentional leaks. The explicit suppression of the warning via an annotation is an indication that someone thought about it and decided this to be the case. If you know of specific cases where you believe we're preventing objects from being garbage collected that should be eligible for collection, please file an issue including details of the specifics. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Yes, this is indeed a bug. We will fix this and provide better documentation regarding the expected behavior of the merging media sources in the next push. Thanks for reporting this.
 To clarify somewhat: When using `MergingMediaSource` the expectation is that you're merging items that have the same duration. If you were to do that and then loop the merged result (example below) then I'd expect playback to proceed correctly.

```
MediaSource audio = new ExtractorMediaSource(Uri.parse(".../audio.mp3")...);
MediaSource video = new ExtractorMediaSource(Uri.parse(".../video.mp4")...);
player.prepare(new LoopingMediaSource(new MergingMediaSource(video, audio)));
```
  I don't think SimpleExoPlayerView does anything special. By default it's just `match_parent` for width and height. If you're trying to put the view into something that allows vertical scrolling then I think you need to set an explicit height on the view. Similarly, if you're trying to put the view into something that allows horizontal scrolling then I think you need to set an explicit width on the view.
 I'm not sure the proposed solution is the right one. `AspectRatioFrameLayout` is intended to resize itself within the bounds available to it, not to resize itself in a way that causes it to exceed those bounds. Treating height==0 as a special case that's allowed to break this contract doesn't feel right to me.

Note also that since the aspect ratio of the video only becomes known asynchronously (once it's been parsed from the stream), the proposed solution would cause the view to resize itself, possibly when the user is interacting with the list, which is likely to be quite a janky experience. If there's an error loading the video then the aspect ratio will never become known and so the view will never gain height.

The best solution probably depends on what your use case is. If you're building something like the Facebook feed, for example, you'd probably want to put the video aspect ratio into the feed data returned from your server. That would allow you to size the views correctly up front, avoiding both the need for any asynchronous resize and the issue described here. If you're building something like a list of movie trailers, it would probably be reasonable to assume 16:9 and accept letter-boxing for wider-than-16:9-trailers. If you have some other case, perhaps neither of these apply.

If you could detail your exact use case, including where the data is coming from that you're using to construct your list of items, that would be good. Thanks!
 Got it. It sounds like we should enhance `AspectRatioFrameLayout` to support this case, but it should probably be something that's requested explicitly, rather than it being assumed if `height == 0`.

We should probably have a mode parameter that can be set to the current behavior (fit within bounds), or to one of two new modes: One that considers the width fixed and resizes the height (which is what you want). The other that considers the height fixed and resizes the width (for horizontal scrolling). 
 @marcbaechinger - Is this something you'd be able to take a look at? Thanks!
 @amosyuen - You can now specify a resize mode on `SimpleExoPlayerView` or `AspectRatioFrameLayout` to achieve the behavior you need. You can do this either in layout xml or programatically. See the change linked above for details.

If you want to place the view into a vertically scrolling container (e.g. a list), you can do so in a way that allows the height to resize as required using:

```
android:layout_width="match_parent"
android:layout_height="wrap_content"
app:resize_mode="fixed_width"
```

For horizontal scrolling you'd use:

```
android:layout_width="wrap_content"
android:layout_height="match_parent"
app:resize_mode="fixed_height"
```
  This was never a feature of the ExoPlayer library. It was a feature of the demo application in previous versions, however note that it wasn't implemented anything like how you'd need to implement background playback in a production quality app. In particular, you'd need to use a foreground [Service](https://developer.android.com/guide/components/services.html) to implement this kind of functionality properly, so as to avoid things like the possibility of your process being killed during background playback.

We removed the feature from the demo app so as to avoid developers believing what we'd done was the right way of implementing background playback.
  Thanks. We can reproduce this, and will push a change shortly.
 This should now be fixed on `dev-v2`, but please give it a try to double check. Thanks!
  From my point of view, within a single period, expected behavior is that your license server should respond with all of the keys required to play the content when provided with a request that contains the PSSH for one of the streams. This is how other providers I'm aware of have implemented this. I believe it's achieved due the presence of some kind of content id in the request, which can be mapped server-side onto the required set of keys.

Note that multiple license requests is obviously inefficient and more error prone, so that's not a great solution. A single request containing multiple PSSHs is similarly inefficient if the PSSH data is embedded in the streams rather than being provided in the manifest. If the data is in the manifest then that sounds fairly sensible, but I doubt we'll be supporting it given other providers I'm aware of have adopted the solution described above.
 Thanks for the info!

> Having PSSHs in the manifest is the right way to do it and supported by industry standards. Any content that does it otherwise only has itself to blame for any inefficiency.

Aside from the rest of this discussion, I'm not sure that's quite fair given many providers deployed DASH before the IOP included the recommendations to which you refer. I guess this is a problem inherent with situations where you have an extremely general spec being retrospectively constrained. Which is unfortunate!

> I feel it may be unwise to design player features to target merely the present-day implementations of DRM services and content workflows that are not keeping pace with modern industry specifications.

This is true. We do need to make any changes in a measured way though. In particular we should not make changes that penalize providers who already use ExoPlayer and who have deployed DASH in a way that now violates the IOPs. It's not realistic to expect content providers to repeatedly adjust their workflows, servers and content every time a new IOP recommendation is introduced, particularly if they have large content libraries that are expensive to re-transcode. There is significant cost and delay involved in practice.

> Certainly, expecting behind-the-scenes magic to take care of key transfer seems entirely incorrect - if the device requests key A, there seems little reason to expect that it also receives key B. Any implementation that does so seems more of a hack than anything standards conforming.

Is there anything specific in the IOP that states this? I don't think either of the sections quoted above say that key request / response should not work in the way I describe. My understanding is that this is partly scheme specific. In particular, my understanding is that the approach I describe is readily supported by Widevine, but not by PlayReady. The reporter of https://github.com/google/ExoPlayer/issues/968 was able to adjust their Widevine proxy to return all keys. If Widevine provide support for this then it seems reasonable for ExoPlayer to rely on the approach, given Widevine is the de-facto DRM solution for Android devices.

@wvpaf - Could someone from Widevine comment on this also. You're the real experts here. Thanks!
 > Fair point about backward compatibility - that is indeed quite an unavoidable consideration and expecting content to be changed is not something that would make anyone happy. I don't think there is anything with so drastic implications here, though.

There's also client compatibility to think about. I'm not sure there's a way to efficiently support your proposed behavior on existing Android devices. Specifically, I'm not sure it's possible to avoid having multiple DRM sessions and multiple separate key requests with the way MediaDrm and the Widevine CDM are implemented currently. IIRC the number of concurrent MediaDrm sessions is also quite limited on some of the first devices that got MediaDrm support.

> Intrusions of "custom business logic" like "for Widevine, actually ignore the key IDs in the request and just use the content ID" just serve to fragment and overcomplicate the picture.

This is unarguably true, but pragmatically it's important to weigh up how much it really matters. Is it _really_ a huge burden, or in practice is it an hour or two of work and an annoyance that it's not an elegant solution? By all means get some clarification on the issue, but in the short and medium term (possibly even long term if existing clients cannot easily support your expected behavior), I suspect you'll end up having to accept how things are done currently and take the complexity hit on the serving side.
 Any other questions or actions? Or can this be closed?
  According to [this](http://stackoverflow.com/questions/19306819/android-resources-key-collision) there shouldn't be a build problem as a result of a naming clash, so I wonder if you need to do a full clean or whether there's something else wrong with your setup.

Having said that, I think we want to avoid naming conflicts and the app having to choose which resource to retain should one occur. Hence we'll add prefixes to the assets in ExoPlayer to avoid conflicts going forward.
  If you're trying to set it globally then you might have some luck calling the static method `HttpsURLConnection.setDefaultSSLSocketFactory()` prior to use of ExoPlayer? Calling the non-static variant is not something we expose, but it would be pretty straightforward for you to make your own `DataSource` implementation that does support this use case.
  Issue reports that omit nearly all of the requested information in the issue template are not efficient for us to investigate, therefore closing. Please file an issue containing complete information, thanks.
  It's hard to diagnose based on the information available in the logs. Decryption is failing but it's unclear why. We'd likely need a full bug report, test content and a working license server to reproduce the issue in order to investigate further. Please share this information here, or email it to `dev.exoplayer@gmail.com` if you prefer. If sending via email, please update this issue when sent so that we know to look for it; thanks!

As an aside, I don't know what "P7" and "P8" mean. They sound like details specific to your media to me?
  Depends on the case. As you correctly state, in this case it is an m3u8, which maps to HLS. 

For manifest based formats (HLS, SS, DASH), if the file has an extension, you can use that to try the corresponding RendererBuilder(.m3u8, .ism, mpd; note there are more alternative extensions).

In the case of standalone files, like mp4, flv, mp3, etc. the best way is to sniff them with the extractors and see which one returns true (this is already done by the ExtractorSampleSource, you don't have to do it yourself).
Ultimately is a matter of content, unfortunately, so the ground truth is what the file contains: The url you provide is m3u8 media playlist (it is not strictly HLS, but it's the same renderer builder). You will need to take a look yourself. It would not be hard to build a "universal sniffer", I think. 
  ExoPlayer doesn't support RTMP, sorry.
  I think Widevine are the right people to help you with this. DRM is reasonably transparent to ExoPlayer. The encryption + packaging is done by the content provider and the decryption is done by the Widevine CDM. ExoPlayer is the layer in the middle, but we don't really do anything special for DRM aside from pipe the data to where it needs to go.

The nature of the issues you're seeing suggest a encoding/encryption/packaging issue to me. What tool are you using to package the media, and are you able to play it successfully elsewhere?
 @wvpaf - Are you able to provide some assistance here? Thanks!
 Following up off thread; thanks.
 Closing as issue was resolved by updating to ExoPlayer V2. Thanks!  For ExoPlayer V2, you can create and host a JSON file that defines your test vectors, as described [here](https://google.github.io/ExoPlayer/demo-application.html). You can then simply share a link to this file. Following the link on an Android device with the demo app installed (e.g. by clicking it in the browser or email client) will load your samples into the demo app.

I think we prefer this approach to having all third party test vectors loaded into the demo app by default. If we were to also provide a web page that has links to the files provided by all third parties, that would be very convenient for anyone wishing to try them out. Each third party would be free to update their test vectors by themselves, since it's just a case of modifying the JSON file that they're hosting themselves. What do you think?

Either way, we're definitely interested in being provided with 3rd party test vectors for our own internal testing, particularly if you can do so by defining them in the JSON format to make loading it super convenient for us ;).
 Note: If there's no alternative to your `X-AxDRM-Message` header then we can add an optional `drm_headers` field (or repeated `drm_header` field, or similar) to our JSON format for you.
 The ability to specify headers has been pushed. The following example lists two of the Axinom test streams. If you host this file with a name ending in `.exolist.json`, then clicking a link to it on an Android device should open the ExoPlayer V2 demo app for your content (provided the demo app is installed).

```
[
  {
    "_comment": "Source: https://github.com/Axinom/dash-test-vectors",
    "name": "Third party: Axinom DASH test vectors",
    "samples": [
      {
        "name": "v7 (Clear)",
        "uri": "http://media.axprod.net/TestVectors/v7-Clear/Manifest.mpd"
      },
      {
        "name": "v7 MultiDRM SingleKey (Widevine)",
        "uri": "http://media.axprod.net/TestVectors/v7-MultiDRM-SingleKey/Manifest.mpd",
        "drm_scheme": "widevine",
        "drm_license_url": "http://drm-widevine-licensing.axtest.net/AcquireLicense",
        "drm_key_request_properties": {
          "X-AxDRM-Message": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ2ZXJzaW9uIjoxLCJjb21fa2V5X2lkIjoiYjMzNjRlYjUtNTFmNi00YWUzLThjOTgtMzNjZWQ1ZTMxYzc4IiwibWVzc2FnZSI6eyJ0eXBlIjoiZW50aXRsZW1lbnRfbWVzc2FnZSIsImtleXMiOlt7ImlkIjoiOWViNDA1MGQtZTQ0Yi00ODAyLTkzMmUtMjdkNzUwODNlMjY2IiwiZW5jcnlwdGVkX2tleSI6ImxLM09qSExZVzI0Y3Iya3RSNzRmbnc9PSJ9XX19.4lWwW46k-oWcah8oN18LPj5OLS5ZU-_AQv7fe0JhNjA"
        }
      }
    ]
  }
]
```

We may establish a page containing links to test media provided by third parties, provided we get more than one of them! So please let us know if you host a file somewhere. Thanks!
  Hmm, the sample plays back fine for me on the same device + build combination.
 Not that you should ever have to do this, but does rebooting the device resolve the issue for you?
 It is possible that the `saiz` issue was the root cause of the problem. I'd suggest re-testing in any case, since we're unable to reproduce.
 Could you give the V2 demo app a try. For maximum reproducibility, it's preferable if you could avoid making any changes to the demo app. You can define your content to be loaded into the demo app using a `.exolist.json` file as described in https://github.com/google/ExoPlayer/issues/1812.

It's somewhat unclear how minimal the modification is between the demo app and what you're testing with is (log tags `MediaPlayerFragment`, `BaseMediaPlayer` etc suggest some non-trivial modifications). The test content still plays fine for me (in V1 demo app with minimal modification, and in V2).

Thanks.
  To play MP3s you would normally use ExtractorSampleSource (in v1) or ExtractorMediaSource (in v2). These use Mp3Extractor internally, but for simple use cases your app does not need to refer to Mp3Extractor directly.

If you are using the release-v1 branch, take a look at [ExtractorRendererBuilder](https://github.com/google/ExoPlayer/blob/release-v1/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java#L64). For dev-v2, the ExtractrorMediaSource is created [here](https://github.com/google/ExoPlayer/blob/dev-v2/demo/src/main/java/com/google/android/exoplayer2/demo/PlayerActivity.java#L351).

If you are trying to do something specific with Mp3Extractor (not just play an MP3), please provide some more details.
 It sounds like the issue you are seeing is that when you seek to a particular timestamp you hear media from a different timestamp. Seeking in variable bitrate MP3 streams is generally not exact, because the file header doesn't always provide a complete map from timestamp to byte offset. See also https://github.com/google/ExoPlayer/issues/328#issuecomment-168532148.

The fix is to use a different container with complete metadata, like MP4 or Matroska.
 Andrew's comment above is correct. If the media contains insufficient information to seek accurately, ExoPlayer will not seek accurately. There exist plenty of container formats that allow for accurate seeking.
  It's not that ExoPlayer uses more memory, but rather than the memory is allocated in a different place. When using ExoPlayer, the media buffer is allocated within your process. When using MediaPlayer, the media buffer is allocated within one of the MediaServer processes.
 You can change the size of the media buffer by tweaking parameters such as that one, yes. Obviously making the media buffer smaller increases the probability of re-buffers for network streamed media. If you're playing local content only it's generally safe to reduce it quite a bit. Note that when using MediaPlayer you have no control over the size of the buffer allocated in the MediaServer process.
  @erdemguven - Please take a look. Thanks!
 Hi nezen, currently flac files are seekable only if they include a SEEKTABLE metadata block. In this case your flac doesn't have one. 
You can check flac files to see if they have a seektable:
`metaflac --list sample.flac | grep SEEKTABLE`
To add 100 seek points to a file
`flac -S 100x sample.flac -o new_sample.flac`
Please see `flac` documentation for other options.
 MediaPlayer uses libflac for seeking which just do a search in flac file for the correct position if there isn't any seektable. Currently ExoPlayer doesn't do that for flac files.
 @burix, sorry, it isn't. And why not? I receive a lot of user complaints about media the user cannot seek.  As above, seeking in FLAC already works if there's a SEEKTABLE metadata block. We may support seeking (by searching) in other FLAC files at some point, but it's a long way down our list of priorities right now. If you're able to contribute a pull request that adds support, please feel free to do so. Okay it sounded as if the feature was not wanted for some reason. I barely understand all this audio stuff but would really welcome if someone tackles this :)  @cdrolle is looking at 708 support. Could you comment on the above? Thanks!
 We have some 708 code internally that hasn't been pushed to GitHub yet. @cdrolle - Can you make it happen? Thanks. The change above adds what we've done so far, but it's not yet a complete solution. Some further work is needed on the MediaSource/Extraction side to expose the CEA-708 tracks so that they can be selected for rendering.  It's quite difficult to debug this kind of thing, since it's more likely than not that the issue is somewhere in your code (which we don't have :)). I'm not sure why the resolutions would be relevant I'm afraid.

One thing that can cause this kind of failure is when two decoders end up having a reference to the Surface at the same time, so you should definitely double check that this isn't the case. You must null out the Surface from one player (or release the player) before passing the Surface to the next player. If you're nulling out the Surface you must do so using a blocking message rather than a non-blocking one to ensure thread safety. Releasing the player is blocking as standard, so that approach would be fine.

As an aside, playlists in ExoPlayer 2.x will probably help you a lot with this use case.
  @hezhisu - We would appreciate it if you could not cross-post exactly the same question in multiple issues. It creates more work for us, and will not lead to your question being answered more quickly. Thanks.
  I have not dived into the code, but the line 

`player.setSelectedTrack(DemoPlayer.TYPE_TEXT, DemoPlayer.TYPE_TEXT);`

looks weird. I would have expected something like 

`player.setSelectedTrack(DemoPlayer.TYPE_TEXT, 0);`

maybe. I would recommend you debug a little bit, I am sure the answer will come to you quickly. Unfortunately, the issue tracker's purpose is to keep track of bugs and feature requests. I would recommend asking these kinds of questions in a general programming Q&A like stack overflow. Good luck with this.
  @AquilesCanta PTAL, thanks.
  Please provide the information required. This is not enough for us to figure out what the problem is. If you are not playing video, then there is nothing for audio to synchronize against.
 Please, provide a stream to see if we can reproduce the issue.
 I will give it a look, I assume it has something to do with seeking in mp3.
 Same as #1810. Please do not cross-post.
  Closing due to lack of useful information (a stream url).
  Is this reproducible on any other devices? If not, it is probably a device-specific bug with how the video is being rendered, so please check if there are any system updates available for the device. 

To investigate further we need a link to a stream that doesn't play correctly.

If I understand correctly, streams using MP4 containers play fine but playing streams in MOV containers are broken as in the screenshot. Is there anything else that is different in the streams that are broken besides the container format, e.g. the video codec or resolution? For what it's worth, there are some native crashes earlier in the bug report (which seem to be from VLC) and appear to relate to the video dimensions, like `DEBUG   : Abort message: 'frameworks/av/media/libstagefright/ACodec.cpp:3664 CHECK_LE( rect.nTop + rect.nHeight - 1,videoDef->nFrameHeight) failed: 1087 vs. 1080'`. I guess it's possible that this is related if you were trying to play the same stream in VLC.
 This looks like a pine64 specific issue with the H264 video decoder. My best guess it that the decoder fails to handle rotation metadata correctly. Portrait videos are typically encoded as landscape videos with a piece of metadata indicating that the content should be rotated for playback. This rotation value is provided to the decoder at playback by ExoPlayer. It appears that the decoder on this specific device is not able to perform the rotation correctly.

Please report the issue directly to the device manufacturer and see what they make of it. Thanks!
 Closing since this issue isn't something we can fix in ExoPlayer; it would be up to the device manufacturer to provide a fix via a platform software update.  Are you certain the device can access the private address in the URL? Can you play other URLs to FLAC streams using ExoPlayer or MediaPlayer?

To help diagnose this it would be useful to have a public URL to the file, posted here or emailed to dev.exoplayer@gmail.com, and a bug report taken just after trying to play the stream (output by `adb bugreport`).
 That stream plays fine for me when I add it to the samples in the (otherwise unmodified) demo_ext app on the latest code in the dev-v1 branch. Could you try reverting your modifications, check that the stream plays and then figure out how your code differs?

If you find that it doesn't play in the unmodified demo_ext app, please post a full bug report (instructions in my previous comment) and I will reopen the issue and investigate further. Thanks.
  This question seems to be starting from a strange point (i.e. thinking there should be additional Widevine test vectors, without knowing what they should be), so it's unclear what the motivation is here really. Widevine created the current test vectors themselves, and so without information suggesting otherwise, I think it should be assumed that they cover the core use cases properly.
 Right, but why do you want/need to test something if you don't want to use it for any other purpose? It feels like there's little or no value in doing that.

I don't think there's really a bit difference between the path you'd take to produce test and production Widevine content. In fact, if you were using Widevine in production you'd explicitly want the test content to use the same path as the production content. I think you'd probably need to setup your own Widevine license proxy/server. We have test keys/policies/servers, but these are intended for our own test content, rather than being for arbitrary use.
 that question is outside the scope of this issue tracker. My understanding is [Shaka Packager](https://github.com/google/shaka-packager) can be used to package Widevine protected content. I'm not sure about setting up a Widevine license/proxy server. I suspect that most people who have a need to do this take Widevine's [CWIP program](http://www.widevine.com/cwip/). Some streaming-as-a-service companies will do the hard work for you.
  Sorry for the delay in looking at this; it looks useful :)! Will take a look shortly.
 Ah. There are other version 2 tags that we'd need to handle as well (TXX, COM, GEO, PIC). Can we keep handling of version 2 tags for a separate change; this one has enough stuff in it already (arguably it should already be split into smaller changes).
 I've created a new branch for this work here: https://github.com/google/ExoPlayer/tree/dev-v2-id3

If you could send the next pull request to that branch, that would be great! If you don't have time to address the comments here any time soon, please just send the original change as a pull request to that branch as it was, and we can help with the comments as part of a more iterative process.
 Merged into `dev-v2-id3`. Work should continue there, and will be merged to `dev-v2` when ready. Thanks!
 Thanks for the confirmation. I've pushed some improvements in https://github.com/google/ExoPlayer/commit/110c8f6f1f4459b05e9943fd6f5c8521735bbfbe.
 @cbfiddle - Hi. We're approaching being able to merge this into `dev-v2` (we're also making `SimpleExoPlayerView` automatically show the album art, if present :)).

One question: For AtomParsers, where did you find the list of possible types? I'm really struggling to find anything authoritative for some of them. For example I seemingly can't find anything about "�trk". Are you sure it exists? Ditto for quite a few of the others.
 http://www.sno.phy.queensu.ca/~phil/exiftool/TagNames/QuickTime.html seems relatively authoritative. No "�trk" though. Only the variant starting with ©.
 Ah, I think you're just handling the possibility of the © symbol getting mangled. Is that likely in practice? 
 I edited my posts (because they were wrong - heh). Sorry for the confusion. Read the web version here: https://github.com/google/ExoPlayer/pull/1797

I think my only remaining question is how likely the copyright symbol is to be mangled. Hopefully we can get this merged back into `dev-v2` in the next few days :).
 @cbfiddle - Thanks for all the work you did on this! We've just merged the changes back into `dev-v2`. You should find album art is automatically displayed in the demo app as well, including for MP4. I'm going to post a couple of comments on this closed request, just in case you're keeping your own fork and/or are interested in a few things I discovered :).
  I'm not sure what default behavior you're trying to override. If you're saying that ExoPlayer is requesting the chunks from the wrong URLs then should fix the manifest or the location of the chunks so that they're not wrong?
 Can you put it in the query part of the URL, rather than a header? If so you could probably do that within the boundaries of the DASH spec. If not, you'll have to modify ExoPlayer at the source level to get what you want. There are a whole bunch of places you could do this. It shouldn't be particularly hard to figure out, but it's not something we support or provide an explicit convenient hook to override for.
 That would be one way of doing it, yes. There's no particular need to use okhttp though. You could create a `DataSource` that wraps some other `DataSource`, re-writes the URL, and then passes the re-written `DataSpec` on the the wrapped `DataSource`. Such an approach would work with any wrapped `DataSource` (e.g. default/okhttp/cronet).
 The two posts above should serve as a good starting point for working out a way of doing this. If you want to change the URLs where they're built, look in the dash package.
  Closing because question is vague and does not provide sufficient information for us to be able to help.
  The issue template clearly requests significantly more information than you've provided, including a full bug report (output of `adb bugreport`) and additional information about the types of devices on which you see this issue, and the versions of both Android and ExoPlayer. Please provide this information. Thanks.
 @guillaumemarc  Can you monitor the CPU usage on the device, you may be overrunning the device's capabilities.
 Hi have not received an update in 4 weeks will close if no new data.
  The reason behing the fake CEA reader (and the type mapping originally forced as well) is HLS. If I remember correctly, the muxed CEA tracks could appear intermittently, which was problematic. Once we add support for multiple CC channels, this should be fixed. For the time being, considering we don't have any way of handling the lifecycle of the CC channels (even if it is a single one), we cannot add the flag just yet. I am tracking this enhancement with #1677.
  Can you provide a specific example of a sample video that causes this error (whether it's already listed in the demo app or not)? If you've added it to the demo app yourself, please specify how. Please also let us know specifically which version of ExoPlayer you're using ("latest" is ambiguous).

For what it's worth, the MediaFormat in the log above isn't what I'd expect to see for a regular MP4 file, because the maximum dimensions (1280x720) are set differently to the actual dimensions (640x360). This normally occurs for adaptive media only, which a regular MP4 file is not.
 Thanks. Please can you attach a full bug report (i.e. the complete output of `adb bugreport`) captured shortly after experiencing the error.
 Also, is your app (the one that works) using an earlier version of ExoPlayer? Do you know which, if so.
 I think our caching of decoder information has led to a lack of useful information in that bug report! Please try disabling caching by modifying [this line](https://github.com/google/ExoPlayer/blob/r1.5.10/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java#L121) from:

```
List<DecoderInfo> decoderInfos = decoderInfosCache.get(key);
```

to

```
List<DecoderInfo> decoderInfos = null;
```

Once you've build and installed the demo app updated to include this change, please take another bug report shortly after reproducing (which with any luck will show the root cause!).
 Hm, actually, it's not entirely clear that it will show the root cause after you do that. You might want to attach a debugger and work out why `getDecoderInfosInternal` in `MediaCodecUtil` is returning an empty list when called with `key.mimeType` set to `video/avc` (which I think it is doing)?
 It's probably easiest to try with the first test video in the demo app, since then we can reproduce and see exactly what you're seeing. The logs indicate the failure occurs trying to find the `video/avc` decoder. Note `video/avc` in:

```
com.google.android.exoplayer.ExoPlaybackException: com.google.android.exoplayer.MediaCodecTrackRenderer$DecoderInitializationException: Decoder init failed: [-49999], MediaFormat(1, video/avc, -1, -1, 640, 360, 0, 1.0, -1, -1, null, 135502033, false, 1280, 720, -1, -1, -1)
```

Is it definitely returning a list containing two items for `video/avc`? If so, I don't understand why the failure occurs. Follow the stack up to `MediaCodecTrackRenderer.maybeInitCodec` and work out why it's throwing the error.
 :)
  You'll need to provide a working and non-geo-restricted link for us to debug this, either here or by emailing `dev.exoplayer@gmail.com` (if you email it, please update this issue to say that you've done so so that we know to look for it).

Please also provide a full bug report (i.e. the full output of `adb bugreport`) captured shortly after the stream gets stuck. This is clearly requested in the issue template.
 Closing due to insufficient information.
  The buffers passed to `processOutputBuffer` hold decoded audio. This will usually be 16 bit PCM, but in certain cases can be 8/24/32 bit PCM instead. You can look at `MediaCodecAudioTrackRenderer.pcmEncoding` to determine which case you're seeing.

As an additional note: The semantics of `processOutputBuffer` are somewhat unclear in ExoPlayer V1, which might be causing some confusion. In ExoPlayer V2 we've tightened this up significantly. See the documentation [here](https://github.com/google/ExoPlayer/blob/dev-v2/library/src/main/java/com/google/android/exoplayer2/mediacodec/MediaCodecRenderer.java#L956).
  The HLS specification is very clear that you must include EXT-X-DISCONTINUITY-SEQUENCE in this case. Specifically:

> If the server wishes to remove segments from a Media Playlist containing an EXT-X-DISCONTINUITY tag, the Media Playlist MUST contain an EXT-X-DISCONTINUITY-SEQUENCE tag.

And:

> If the server removes an EXT-X-DISCONTINUITY tag from the Media Playlist, it MUST increment the value of the EXT-X-DISCONTINUITY-SEQUENCE tag so that the Discontinuity Sequence Numbers of the segments still in the Media Playlist remain unchanged.

And:

> Clients can malfunction if each Media Segment does not have a consistent Discontinuity Sequence Number.

The fact that EXT-X-DISCONTINUITY-SEQUENCE is pitched in the HLS spec as being useful to the client where there are multiple variants/renditions doesn't mean you can ignore MUST clauses in the specification in the single variant case. Adding EXT-X-DISCONTINUITY-SEQUENCE to be spec compliant will resolve the issue you're seeing.
 EXT-X-DISCONTINUITY was added to the spec quite a long time before EXT-X-DISCONTINUITY-SEQUENCE. It's likely that the iOS player doesn't rely on EXT-X-DISCONTINUITY-SEQUENCE as much as ExoPlayer does as a result. It's unclear to me whether or not the spec was under-specified (to the extent that the player had to "guess what to do" in some cases) prior to the addition of EXT-X-DISCONTINUITY-SEQUENCE, but I suspect this is likely to be the case.

EXT-X-DISCONTINUITY-SEQUENCE has been in the spec for around 3 years now. Given it's relevant to live streams (only), it's not like there's a huge volume of legacy content that omits the tag. It seems reasonable for us to require that live stream providers aren't more than 3 years behind the spec.
 Yep. So I think you can consider ExoPlayer as not supporting live streams prior to version 6 in the specific case that they include EXT-X-DISCONTINUITY tags. As per above, it seems reasonable that live stream providers shouldn't be using a spec that's 3 years old.

ExoPlayer will successfully play on-demand streams and live streams that do not include EXT-X-DISCONTINUITY tags that are prior to version 6.

We could proactively detect the unsupported case and fail with a clearer warning, if you feel that would be worthwhile.
 We now plan to provide best-effort support for EXT-X-DISCONTINUITY without EXT-X-DISCONTINUITY-SEQUENCE. We'll likely do this sometime in Jan. The latest `dev-v2` branch should handle this issue properly in most circumstances. There may still be issues if there are discontinuities very close together and if high latency is experienced refreshing the HLS playlists.  Make sure you provide all information required in the issue template. 

Please provide a link for us to reproduce the issue. Unfortunately, this is not enough. You can send them to dev.exoplayer@gmail.com if you don't want to post them here.
 The failure suggests your mp4 files are malformed. They appear to contain atoms/boxes that define their size incorrectly. As above, we'd need access to the content to tell you exactly what's wrong.
 Providing the manifest without any of the underlying media segments is not useful to us, particularly given we believe the issue is in the underlying media segments. Please provide complete test content, meaning that we can take whatever link or files you attach and actually try and play them. Else we are unable to assist. Thanks!
  This is something we'd need to add support for. Marking as an enhancement.  It should be noted that this kind of manifest is pretty obscure. I'm fairly sure they could just omit the `SegmentTimeline` completely and inline the segment duration into the `SegmentTemplate` element, like:

```
<SegmentTemplate timescale="12288" duration="24576" media="ED_512_640K_MPEG2_video_$Time$.mp4" initialization="ED_512_640K_MPEG2_video_init.mp4"/>
```

As an aside, I notice you've filed a few bugs about obscure DASH manifests. What's the actual use case your developing for? The DASH spec is so broad it's not generally a sane goal to play arbitrary DASH manifests. Pretty much every service provider generating DASH manifests will do so according to a significantly more constrained (and sensible!) DASH profile.
 No worries, and thanks. Please do continue to report edge cases as you find them. It's useful to know of cases we don't support even if we decide that we don't want to support them. For this one, we will aim to support the case, but it's low priority since I'm not aware of any providers who are generating this style of manifest in production.
 You can definitely omit the SegmentTimeline. There is no requirement for one to be present and there's typically only value in including one if some segments are of different duration than others.

We're aware that r=-1 is valid according to the spec, as already indicated above. We will get around to fixing it at some point, but would still advise omitting SegmentTimeline in the case where all segments are of equal duration, since its inclusion in unnecessary.
 It's not particularly viable to implement the whole DASH spec. It's also not a great way to invest time because it contains a long tail of features that are rarely used.

I don't think I fully understand the case you describe, because I don't see that missing frames would result in the segment duration changing. Every time you shorten the duration of a segment the player will move slightly closer to the live edge, because you're misrepresenting time. That would most likely have undesirable consequences (e.g. the player will end up maintaining a smaller buffer than it intended). It seems preferable to have a segment with the correct real-time duration that has some frames missing. I'm also not sure how `r=-1` helps, since it doesn't help with specifying that a segment has a different duration. Please can you clarify exactly how this case works? Thanks! Note: I've also seen quite a few providers generating live stream MPDs that use only SegmentTemplate and so have a completely fixed segment duration. Which suggests this is not an insurmountable problem.  - When using DRM, you need to make sure the player is configured to use the correct license server. You'll most likely need to modify `WidevineTestMediaDrmCallback` to make it hit `https://license.uat.widevine.com/cenc/getcontentkey/widevine_test`, since that's not its default. In ExoPlayer V2 this can be configured where you add the sample to the demo app.
- We'd need a full bugreport captured from second phone shortly after attempting playback in order for us to tell you for sure why the playback isn't successful.
 As stated previously, we need a full bug report taken shortly after reproducing the issue. Log snippets are not sufficient. A bug report is the full output of `adb bugreport`. See [here](http://stackoverflow.com/questions/26789107/how-to-write-an-android-bug-report-to-a-file-via-adb) if you need more information.
 Having said that, in this case the error is pretty clear:

```
com.google.android.exoplayer.ExoPlaybackException: Media requires a DrmSessionManager
```

You need to be instantiating and injecting a `DrmSessionManager` instance into the renderers when you build the player. You can take a look at how it's done in `DashRendererBuilder` as a guide. You'll need to modify `ExtractorRendererBuilder` to do something similar, if you're trying to play a DRM protected mp4 using the demo app.
 In the most recent bug report, the player is simply enable to find the URL that you're pointing it to:

```
09-01 16:23:32.909 19069 19069 E EventLogger: com.google.android.exoplayer.upstream.HttpDataSource$HttpDataSourceException: Unable to connect to http://192.168.1.253:8080/huoguoen.mpd
```

I'm going to go ahead and close this, since it's unclear that it's tracking an actual issue.
  The emulator does not properly support media applications unless running a system image with API level 23 or greater. This is a shortcoming of the emulator, not an ExoPlayer issue. Where possible, we recommend testing media applications on physical devices rather than emulators.
  This will be fixed in version 1 as well in the next push.
  I think the mpd file is invalid. It contains SegmentBase and SegmentList elements at the same depth in the manifest, but according to the DASH spec (ISO 23009-1):

> At each level at most one of the three, SegmentBase, SegmentTemplate and SegmentList shall be present.

So this is a violation of the DASH spec:

```
<SegmentBase>
  <Initialization sourceURL="bunny_4s_50kbit/bunny_50kbit_dashNonSeg.mp4" range="0-862"/>
</SegmentBase>
<SegmentList duration="4">
  <SegmentURL media="bunny_4s_50kbit/bunny_50kbit_dashNonSeg.mp4" mediaRange="863-22486"/>
  ....
```

A spec compliant manifest would only have the `SegmentList` element, probably like this:

```
<SegmentList duration="4">
  <Initialization sourceURL="bunny_4s_50kbit/bunny_50kbit_dashNonSeg.mp4" range="0-862"/>
  <SegmentURL media="bunny_4s_50kbit/bunny_50kbit_dashNonSeg.mp4" mediaRange="863-22486"/>
  ....
```
  HLS is known broken in V2 currently. We'll be pushing fixes shortly.
 Hi, guys, I am currently taking care of Hls support. We are currently working on this but we have quite a lot going on. I already have in mind a few ways of minimizing the BLWEs you are getting, but it is blocked by some new features being added. Once those go through, I will address this directly. I am curious about the fixes you speak about, @jonasevcik. If you don't mind giving me an overview, we can consider adding them to Exoplayer.
 > added support for #EXT-X-PLAYLIST-TYPE - it made seeking possible in live events

This will have to wait until we add live seeking support in HLS, its on the map so it is a matter of time. Once I get to it, we can look at the possibility of integrating it. In the meantime, we have to wait for a few other more basic things to get pushed. 

> live playlists (not live events) start playback 3 chunks from end of the playlist

This is already happening in V2, right? Have you checked the released version?

> live playlists (incl. live events) are refreshed periodically according to the specs

This is already happening while playing in V2. Haven't thought about:

> playlist is not being refreshed while playback is paused

I haven't given much thought to the paused case, as it will be directly addressed once live HLS seeking is supported. I will keep in touch once I get to HLS live seeking. Please give the latest v2 release a try and let me know if something I said is incorrect. Thanks for your interest.
 > It is too late to start refreshing while playing the last chunk. Then, if you don't meet the condition regarding min. refresh time, you'll get stuck. Live playlists should be refreshed periodically regardless current playback position. That's the main design flaw.

Can you please clarify? This 

`if (chunkIndex >= mediaPlaylist.segments.size())`

has nothing to do with the playback position. In other words, refreshing the playlist does not have anything to do with playback position. We used to poll the playlist, if I remember correctly. We now only poll it (with half the target duration time interval) when we get to the live edge while loading the chunks. If I am playing the end of the live window(far away from the live edge), I don't see a reason to reload the media playlist (can you provide an example? Or maybe a fragment from the spec? I didn't see a paragraph that states that it is mandatory to refresh the media playlist periodically).

On the other hand, If I am playing near the live edge, then the loading position will get to the live edge as well, at which point we might need to refresh the playlist periodically to see whether a new segment has been added. This, we do at the rate advised by the spec (half the target duration). Also, playing too near the live edge is not advisable as per: 

> If the EXT-X-ENDLIST tag is not
>    present and the client intends to play the media normally, the client
>    SHOULD NOT choose a segment which starts less than three target
>    durations from the end of the Playlist file.  Doing so can trigger
>    playback stalls.

Is there something I am missing here?

Also,

> Current implementation doesn't look for changes in downloaded playlist, so refreshing happens at rate >= target_duration / 2. If the playlist wasn't changed, refreshing it at the rate of target_duration is sufficient.

This is suggesting we increase the refresh interval, not decrease it, when we finally get a different playlist. I assume

> If the playlist wasn't changed, refreshing it at the rate of target_duration is sufficient.

meant "If the playlist was changed". This is indeed out of spec but, in my opinion, with no greatly negative consequences (does not mean you will get stuck or anything like it, we are just refreshing twice when the spec says we should refresh once). Will look into it, though.
 > I don't see a reason to reload the media playlist

Aside from anything else (or what the spec says), this will be required for seeking in the live window. We should IMO be prioritizing doing this (i.e. moving the playlist refresh logic up to HlsMediaSource, making it periodic and exposing the live window).
 >    `if (oldMediaPlaylist.mediaSequence == newMediaPlaylist.mediaSequence) {`

I think this condition is not ideal, as one of the media playlist might have been refreshed at a different moment, meaning that there is a small offset that breaks the condition (even if sequence numbers are aligned).

Also, due to splicing, this 

> `return previousChunkIndex + 1;`

should probably be

>  `return previousChunkIndex;`

If your media playlist contain the `EXT-X-PROGRAM-DATE-TIME` tag, we will use that for adaptation in the future and that should be the definitive solution.

Something that changed that seems to be causing some trouble is that we stopped considering sequence number alignment when adapting. Which is not a bad idea, in general, because the spec specifically states that it should not be assumed true when adapting. If you know your contents include aligned sequence numbers, you can give it a try and let me know the results. 
 We are going to try to be more conservative in out HLS implementation to minimize the occurrences of this exception. I will use this issue to track this. An enhancement will be pushed soon, around next week.
 Guys, a commit message will appear in the thread when something is done about this. We will probably push a temporary tentative fix soon. This is being worked on.
 @AquilesCanta - Can we close this as fixed in 2.2.0?
@adnanutayim - Any JWPlayer issues should be reported to JWPlayer. We have no control over what version of ExoPlayer they're using and of any local modifications they make, so cannot provide direct support. This issue should remain open to track a default player behavior on BLWE. 

An update: We have done some work to fix spurious errors actually caused by falling behind the live window. I am not sure those were affecting you guys, but still. The demoplayer now reinitializes the media source when a BLWE is thrown(d303db975ed770528e5b51a243c5023a54c8ec6e). For now, this is what you should do when you get this behavior. Have a look at the demo app to have a guide. Let us know if you have any feedback.  @kiall - We asked for clarification about the CLA with respect to this pull request in https://github.com/google/ExoPlayer/issues/179 and didn't receive any. We wont be looking in detail until clarification is provided.

Note that a piece of the functionality needed to support bitmap subtitles, which overlaps will some of this pull request, is likely to be merged via https://github.com/google/ExoPlayer/pull/2219. The plumbing of bitmaps through the text package is likely to be merged in via https://github.com/google/ExoPlayer/pull/2219. We've no objection to that change, and it's more or less required to enable support for bitmap subtitles to be added via extension in a clean way. Whether we then support DVB directly so that you don't need to do any extension is being re-assessed as per https://github.com/google/ExoPlayer/issues/179. It's more likely than it was in the past, at least.

The CLA question isn't about whether the CLA is signed. It's about whether the code is an original creation as per the requirements of the CLA. One of the changes in this PR has "Original C code taken from VLC project" written in it. I don't think you can take GPL licensed code, make a derivative work and then remove the license. In general we can only accept original code where the contributor owns the copyright/IP and where it's not subject to other licenses. If you believe the PR does meet the requirements of the CLA, please clarify. Thanks! Re the more general overlay suggestion: As soon as you're thinking about arbitrary overlays (e.g. live discussions) you probably also want things like custom animations, fades, clever interactions with playback controls etc. I don't think it's possible to come up with a general way of modeling that, short of reimplementing the whole Android UI layer. So I think the best way to support such cases is just to provide a way for applications to insert custom layouts on top of the video where they can do whatever they like. Which is something you can already do. There are a whole bunch of ways to do this, for example overriding exo_simple_player_view.xml (see [this post](https://medium.com/google-exoplayer/customizing-exoplayers-ui-components-728cf55ee07a#.vbhj26dbk) for details). @tresvecesseis - We've now merged the rendering changes required to support DVB subtitles (i.e. the required changes to `Cue` and `SubtitleView`). It would be cool if we could get to the point where adding DVB support is as simple as inserting the additional subtitle decoder when building an `ExoPlayer` instance. Specifically, this means I think it would be fine for you to propose a pull request that merges:

* The addition of the DVB mime type to MimeTypes.java
* Any remaining change necessary to Format.java for your use case
* Changes to TsExtractor that expose the track

This will result in the track being visible, but ignored as not supported by default. Enabling support in your app will not require any changes within the library itself. You'll be able to simply pass your own `SubtitleDecoderFactory` that's capable of instantiating a `DvbSubtitleDecoder`. The `DvbSubtitleDecoder` will live in your application code (or some other library, as appropriate).

Note: This is conditional on the TsExtractor changes being an original work (as far as I can see the LGPL derivative work is only in the decoder, but please confirm if you're interested in doing this).

Note2: We would also accept a pull request to add a `DvbSubtitleDecoder` subject to it meeting the CLA requirements and being an original work. But that can be considered separately.

Thanks! We will be pushing the visibility change soon. @tresvecesseis - You may still want to consider sending a pull request to contribute the TsExtractor changes, despite the fact you're able to inject them via `TsPayloadReaderFactory`. The main benefit to you would be that if we ever significantly change any of the interfaces that you're implementing, we'd take care of also updating the contributed code. Where-as if it's injected by the application, this will be your responsibility. Up to you of course :). @tresvecesseis - I don't think that's a concern. We have readers for things like AC3, for which most Android devices do not provide decoders, and so we already handle this case (without leaking). I think the first block of code you provide is the right thing to do. If that's not working, take a look at `SubtitlePainter.setupBitmapLayout` and see if you can figure out why. Ah, interesting. Yes, we probably need the PAR in the Cue class. I'm not convinced we need logic to confine the rectangle inside the screen though. Subtitles (including the PAR) should be configured properly... We've moved a lot of files in `dev-v2`. If you could re-base this on top that would be helpful! Thanks. Thanks. We're working on getting this change merged. @kiall - Any idea why the audio tracks in those samples have the codec ID set to `A_MPEG/L3`? Shouldn't the codec ID be set to `A_MPEG/L2`? This results in the track selector trying to play the track on devices that have an MP3 decoder but don't support MP2. In the case that both are supported but the decoders are distinct, ExoPlayer will feed the audio to the wrong one. Not sure if I've misunderstood something... Merged. Thanks a lot for your help on this! I ended up doing quite a bit of cleanup, but it was all pretty mechanical. Mainly making things static and final where-ever possible. For parser code in particular, if you can get to the point where you have:
```
private static SomeObject parseSomeObject(SomeInput input)
```
where `SomeObject` is immutable, it makes everything a lot easier to understand. You know the only side effect of calling the method is consuming some input, you know that the parsed object wont be mutated once it's created, and you also know that the parse method doesn't forget to set any fields in the object it's creating. The cleanup change is: https://github.com/google/ExoPlayer/commit/156bc52c8f73154bd847c352acb9227324ff7301.

Note: The last commit on this pull request got clobbered as part of the cleanup. Sorry about that. Please feel free to send a new pull request that makes the equivalent change on top of the cleaned up code! If the media indicates that a subtitle should be enabled by default then we enable it. Else we don't. So yes, this is expected behavior where the first sample doesn't mark a subtitle track as default and the second sample does. For MKV I think tracks are marked using FlagDefault (see the spec [here](https://www.matroska.org/technical/specs/index.html)).  The case has gone away in ExoPlayer V2. It still applies as far as I'm aware in V1.
  Thanks for the pull request. We're no longer accepting feature contributions into V1 because nearly all of our work has shifted to V2. Please feel free to propose a PR to the `dev-v2` branch if and when you need to do so.
  Have you tried V2? I am not sure, but since there have been many loading policy changes, you might note improvement. Give it a try and let me know how it goes.
 Our HLS implementation is now undergoing some major design changes that should come into effect before long. So please have some patience if you encounter strange behaviors(specially regarding live content). The bug fixes have priority, so that should be solved shortly. Looking forward to your reply.
 Quality switches up a lot faster in V2 (playing the demo app test streams I quite often see switching up as quickly as ~3s on a fast WiFi connection).  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Interesting. To check we're thinking along the same lines, this is effectively a platform bug, isn't it? VideoCapabilities shouldn't claim that the device doesn't support decoding 720x1280 video if it can successfully decode this resolution. It sounds like the device is under-reporting its capabilities for the vertical video case?

I'm not saying we shouldn't work around the issue; just trying to check we have consensus that the platform is doing the wrong thing? I'll follow up internally if so and make sure this doesn't happen going forward.
 By the way, do you see this issue on any devices other than the Sony M4?
 CLAs look good, thanks!

<!-- ok -->
 Did you have a chance to test on any more devices? I'm pretty sure it is a platform issue; the fact that video is normally horizontal doesn't mean the API can ignore input for which this isn't the case.

There's a danger that flipping the dimensions around might cause other issues (e.g. if there exists a device can legitimately play horizontal video but not vertical video with the dimensions flipped). So I think we do need results from other devices before deciding what to do here. If the issue is isolated to the M4 then we should add a workaround just for that device.
 Hi. We decided to merge an equivalent change, having found some more occurrences of this issue. The change is https://github.com/google/ExoPlayer/commit/2e3ffe1e94a16d6ca1e5c09f818969cbcd531abe and has been merged into `dev-v2`. Closing this request. Thanks for your input!  Please see https://github.com/google/ExoPlayer/issues/426. The key points being:
- The warning is harmless and can be safely ignored.
- The warning will go away in V2.
  Thanks for reporting this; we'll push a fix to V1 and V2 shortly.
  @cdrolle FYI
 I'm aware there are some issues with the merged result of this. We'll get those fixed up shortly! Thanks for the contribution btw!  Well... I can see three causes for this:
1. In older devices, the main profile might be unsupported. [This(#683)](https://github.com/google/ExoPlayer/issues/683) and [this(media format support)](https://developer.android.com/guide/appendix/media-formats.html) might be helpful.
2. The playlist is live. Does the issue reproduce instantly? If not, the encoding might be somehow sporadically changing so reproducing this might not be straightforward. I don't think this is too probable. If it always reproduces instantly, i'd go for the other two and ignore this one.
3. Emulators are not the ideal place to test video playback. For example, I was not able to reproduce this on a nexus5.

In version two, for playlist such as this one, in which there is only one entry, the resolution parsing will not be a problem as the width and height will be taken directly from the contents instead of the platylist.  Though this is still not available.

I will use this issue to track h264 profile support checks in version 2. This will not help you too much, but at least we will show a toast instead of throwing runtime error. I will write back once I add these checks, so you can try this on v2 and let me know the results.
 Yes, the toast happens in the Demo app. No exception is thrown, though. You can have a look at the TrackSelector hierarchy to learn more about track support.
 Well... The checks for AVC codecs have now been added to version 2. Please give it a try(once it gets pushed) and let me know if there are any issues.
 No, no. They are two different issues. #1464 declares an incorrect resolution(being the manifest resolution not supported), but the video is actually fine to play. This issue(#1772), on the other hand, refers to codecs that are not supported on the running device. There is no workaround for this, unfortunately, besides providing your own software decoder. Note that this is not a simple task.

In the near future, in HLS, for the first variant listed in the manifest we are going to use the actual resolution of the video, instead of the one listed in the playlist, so this workaround(#1464) won't be needed anymore. Just FYI. Can we close this issue now?
 I thought you had said:

> For S3 and TESLA stream working normally and for others (S4, LG G3 and emulator Nexus 6 API 23) there is only audio with toast message.

When does the toast appear and when does the exception you pasted occur?
 This should finally be fixed in the next push.
  I don't think the log snippet you post is relevant (which is one of the reasons why our issue template explicitly requests a full bug report, not just a snippet).

That aside, this is likely an issue with the emulators rather than with ExoPlayer, so you should file issues on them rather than here. The official Android emulator does work well with ExoPlayer for recent API levels, I believe.
 The relevant bit is:

```
Unable to connect to http://cdn3.viblast.com/streams/hls/airshow/playlist.m3u8
```

which suggests your app and/or the emulator doesn't have network connectivity (or else is broken in a way that causes ExoPlayer to experience a lack of network connectivity). This is an issue with your setup or the emulator, not with ExoPlayer.
 If you look at the DefaultHttpDataSource implementation, you'll see that all it does is call through to the platform network API (HttpURLConnection). In this case the platform network API is saying that it cannot connect, and DefaultHttpDataSource is then propagating that error up. So it's not even like it's something in the ExoPlayer layer that's failing.
  Please provide a full bug report, as is explicitly requested in the issue template.
 Please attach the output of "adb bugreport" rather than "adb logcat". Is this running on an emulator or a physical device?
 The emulator didn't do a good job of supporting video playback until pretty recent API levels. From your original post, it sounds as though it works properly once you get up to API level 20, although support probably becomes more robust the further you go beyond 20.

This is an issue with the emulator rather than with ExoPlayer. I don't think there are any plans to fix it for the older API levels, unfortunately. Playback is expected work correctly on real devices.
  Which emulator are you using? I was under the impression ExoPlayer performs pretty well on the official emulator running a recent API level.

If part of the player slow when running on an emulator then I'd guess it's the actual video decode. In which case you're looking for a way to replace the video decoder with something that discards the input buffers and produces dummy output buffers (cheaply). I can't think of a way of doing this without modifying ExoPlayer directly. If you're willing to do make direct modifications, you could change MediaCodecTrackRenderer to call MediaCodec via an interface you define, and then create a dummy decoder implementation of that interface.
 releaseOutputBuffer renders an already decoded video frame to the surface, and returns the output buffer holding it so that the MediaCodec can re-use it. I don't think you can remove just that call, if that's what you mean. A MediaCodec instance has a fixed number of input and output buffers (see the first diagram [here](https://developer.android.com/reference/android/media/MediaCodec.html)). If you just "stop releasing output buffers" then the MediaCodec wont have any free output buffers available to decode into. This will effectively block the decoder and hence the flow of data that you're trying to retain. Note also that it's unclear what the expensive computation is that you're trying to avoid. I would have thought decoding is the expensive part (i.e. between queueInputBuffer and dequeueOutputBuffer), not rendering (releaseOutputBuffer).

Either way, if you want the flow of data to remain unchanged, you probably need to replace the whole MediaCodec instance with a dummy implementation that you've written yourself.
  You're playing an unseekable live stream (i.e. seeking is not possible). Hence this is working as intended.
  I wouldn't classify 11 as "many". Do you have any concrete numbers on what the overhead actually is? I really wouldn't expect it to be significant; there's nothing fundamentally expensive about the extractor classes from an instantiation point of view.

As an aside, note that you can pass just the extractors that you want to use to the constructor, if you know what type of media you're playing in advance.
 Without numbers there's nothing to justify that claim. I suspect the instantiation overhead is negligible (certainly compared to something like inflating a view hierarchy or instantiating a decoder, both of which I'd expect to be at least an order of magnitude more expensive).

Note also that it's non-trivial to simply make the sniff implementations static. Some of the sniff implementations require large parts of the extractors, so you'd end up making a whole bunch of stuff static (which you'd then end up having to instantiate again inside of a concrete extractor should the sniffing be successful).

Closing for now. Feel free to reopen if you have concrete numbers showing a significant overhead.
  This sample works fine for me in 1.5.10 using the demo app. Closing because we cannot reproduce, and because most of the requested information in the issue template was not provided.
  @ojw28 if you have received the logcat and videos related to this issue?  If so, can you attach them here?
 Logs shared internally. @KiminRyu, it would be helpful if you could provide a full bugreport (captured with "adb bugreport") shortly after encountering the issue, rather than just logcat output. Thanks!
 @KiminRyu, please attach the requested troubleshooting information to this case so the issue can be investigated.   Thanks.
 The logs have been received outside of this case. Team is reviewing them now, not finding any errors/issues with the decryption process. Asking the OEM to take a look.
 Samsung believe sthis issue is addressed but would like to verify playback. 
Can you share any content, not geo restricted for them to test?
 @wvpaf - Do we know what platform build we expect the Samsung fixes to land in? It would be good if we could share this; otherwise it's difficult to distinguish between "still broken" and "fixed from build X as expected" :). Thanks! @ojw28 no details from the OEM representative yet other than SS has a fix. Waiting on details. Looks like S2 Tabs will get update in Feb/Mar, no word on S7.  @KiminRyu - thank you for the update. Yes Samsung will need to issue updates to those devices too.  Yes, it was written that way to allow extension (for provider specific extensions to the DASH manifest)..
  To investigate further we need more information (as requested in the issue template). In particular, we need a link to a problematic stream, steps to reproduce the issue in the ExoPlayer demo app and, since this appears to be device-specific, information about the device and build on which you saw it. Does the stream play correctly in the platform's built-in media player?
 Please provide the output of `adb bugreport` taken just after reproducing the issue. It would be useful to know an example of an affected device. "android boxes" and "TV box" are not specific enough.

Can you play the stream using a MediaPlayer-based app started with `adb shell am start -a android.intent.action.VIEW -d "<url goes here>" -t "video/*"`?

As a guess at what might be causing this: perhaps this device's decoder is buggy and does not support this video height (770 pixels) properly. If you scale the stream to 768 pixels high does it play correctly?
 Closing due to inactivity. Please file a new issue providing the information requested above if you weren't able to resolve this.  Please provide all the information requested in the new issue template, including a full bug report taken from the device just after the issue occurs. You can capture one using `adb bugreport`.
  - I've fixed the three broken links (CONTRIBUTING.md will remain broken on the dev branches for now, but we'll push fixes for those too in due course).
- As for your issue, did you build the extensions correctly? You need to build them all as per their instructions. For example the VP9 instructions [here](https://github.com/google/ExoPlayer/tree/release-v1/extensions/vp9).
 Closing due to lack of response. Please follow the build instructions for the extensions, as per the second bullet point in my previous reply. Thanks!
  Thanks for reporting this!
 I'm not sure it's relevant when the GC happens. The point is that the DataSource implementations shouldn't be affected by it. We have a fix internally, and will push to GitHub soon.
  It looks like the H.265/HEVC codec is failing to initialize. There are a few possible causes, including: running out of device resources (due to an app not releasing the codec), a device-specific bug with the codec or an issue with the input media.

To investigate further, please could you try to reproduce this in the ExoPlayer demo app? Do other H.265/HEVC videos play reliably on this device? On what devices can you reproduce the error?

It looks like the provided bug report doesn't include the error stack trace, so if you provide another bug report taken just after seeing the issue we would have more information to go on. Thanks!
 Closing for now since we don't have a bug report that captures the issue. It sounds a lot like underlying platform flakiness, so even if we do get a bug report it feels fairly unlikely that we'll be able to do much about it, although please do provide one if you see the issue again. Thanks!
 For what it's worth, the symptom that playback is broken for all apps after seeing this error and that it's fixed by reboot sounds very similar to what you might see when leaking a codec on some devices running older Android builds. You can sometimes identify this case by inspecting a bug report (depending on the device), but your bug report linked above seems to be inaccessible now. It might be worth checking your code to make sure you're releasing the player correctly. But an alternative explanation is simply platform flakiness as noted above.
 @scn46 If I understand correctly, you're using an unsupported build/device combination, so it's not surprising that you see instability. The underlying error is probably unrelated to ExoPlayer and trying to work around it is outside the scope of this issue tracker. I think your expectation is incorrect. The decoders we use are in the platform. If the unsupported build/device combination doesn't have correctly functioning decoders, then it's not possible for YouTube/ExoPlayer to still function. As an analogy, your expectation is kind of like expecting a game to work on a PC that has broken graphics card drivers installed.

Please let us know if you can reproduce the problem on a supported version of Android.  I wrote a comment before which is now deleted. If you read it, forget about it. As @ojw28 correctly points out, we start the preparation of the HlsMediaPeriod as soon as we have loaded samples from the Chunk, and this happens as we get the data downloaded. My bad. In other words, there is no need to have the whole chunk downloaded to start playback.
  If you are using version 1, have a look at the DemoPlayer class of the DemoApp. If you are using version 2, have a look at the PlayerActivity and go from there. Unfortunately, this issue tracker is reserved for bugs and feature requests, I'd suggest you try another Q&A such as StackOverflow for generic programming questions.
  We're unable to provide support for Amazon's fork. Some FireOS devices have specific issues around audio timestamps / tracking of playback position. I believe their fork is intended to work around some of those issues, but this is something you'd have to follow up with them directly on (i.e. file an issue on their fork's issue tracker, not here).
 The underlying problem is an issue with FireOS / FireOS devices, hence the need for you to contact Amazon directly. Note that FireOS is not Android; it's a heavily modified fork of Android. It seems they've failed to retain proper compatibility with Android in parts of the media stack on some of their devices; hence this kind of issue.
  The initial volume is always 1.0, and the volume is thereafter only changed if you change it. So you should always know in application code what the volume is.

For V2, we can pretty easily add a `getVolume` method to `SimpleExoPlayer` so that you don't have to keep track of it yourself.
  For V2, this is answered in https://github.com/google/ExoPlayer/issues/2855.  This stream plays fine for me. It's quite high quality (1080p), so it's possible the device you're testing on isn't capable of playing it. Please provide complete information (e.g. a full bug report) as is clearly requested in the issue template.
  For devices running API 23 or later, you can send a message to the MediaCodecAudioTrackRenderer setting the playback parameters, where you include the pitch. 

You can have a look at:

https://github.com/google/ExoPlayer/blob/r1.5.10/demo/src/main/java/com/google/android/exoplayer/demo/player/DemoPlayer.java

To see how messages are sent. Note that this will not work in older versions of android.
  I am not sure what the case is here. @andrewlewis is the voice of knowledge here but what you are trying to play is a ts that includes an ac3 encoded audio stream? Or does the playlist refer to .ac3 segments?

As a side note, unless you are using extensions, codec support depends solely on the platform on which you are playing. You can find some information about this here:

https://developer.android.com/guide/appendix/media-formats.html

Depending on the circumstances, you might want to pass the samples through. Please clarify.
 Unfortunately, this is not Exoplayer specific and you will have to look somewhere else. I can point you to

https://developer.android.com/reference/android/media/MediaCodecList.html

and

https://github.com/google/ExoPlayer/blob/r1.5.10/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java
 Regarding adding codecs to the library: some of the [extensions](https://github.com/google/ExoPlayer/tree/release-v1/extensions) provide TrackRenderers that bundle their own software decoders, so they don't rely on platform codecs. The ExoPlayer v2 development branch has [an extension that wraps the FFmpeg library](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ffmpeg). It is possible to compile FFmpeg with support for decoding AC-3 audio.
 We don't have any detailed documentation on using the FFmpeg extension I'm afraid, but the v2 demo app will use it if it is built.

At a high level, you would need to follow the [instructions to build the extension](https://github.com/google/ExoPlayer/tree/dev-v2/extensions/ffmpeg/README.md), making sure to enable whatever decoders are needed when you configure the library. Then switch to one of the demo_ext build variants in Android Studio and play your stream from the demo app (by adding it to the samples list or starting the demo app with an intent pointing to the stream to play). We will provide some more details about this last step in an upcoming post on the [developer blog](https://medium.com/google-exoplayer) soon.
  Thanks for reporting this. A separate fix has now been merged to address the broken link.
  Is it possible to provide some (fairly minimal) patch to the ExoPlayer demo application that allows the issue to be reproduced there? If someone can get us that far then we'll take a look. Until then I don't think we have clear reproduction steps. @mufumbo - ExoPlayer powers a *huge* number of playbacks every day across many apps on Samsung devices, including millions of YouTube playbacks, and we just don't see lots of users reporting that they see a blank screen instead of the video. We also test with the demo app regularly on Samsung devices and do not see this issue. That suggests there's something that your app(s) do specifically that causes the problem to reproduce, which is why we're requesting a patch to the demo app that also allows the issue to be reproduced there. This will help us to reproduce the issue and to understand the conditions necessary for it to occur.

If I had to guess, I suspect the demo app will work fine on a Huawei P8 as well, and that we'll need proper reproduction steps to debug on that device as well. But I'll try and track one down to give it a try (we do not have a wardrobe containing an unlimited number of every Android device ever shipped ;)). I got hold of a Huawei P8 Max running Marshmallow (we don't have the normal P8, or one running Lollipop). Playback appears to work perfectly in the demo app. So I think we're still stuck on needing proper reproduction steps as described above.  This is tracked by https://github.com/google/ExoPlayer/issues/87. We'll be working on this in 2.x.
  On what information are you basing your assertion that this is enabled by default on some Samsung phones? Can you point to a source?
 So you're saying it's defaulting to true across a significant number of devices, but also there are no discussions about the subject on the internet? That doesn't feel like it's particularly plausible to me. Nor would I expect a sudden burst of complaints if this were the root cause.

That aside, I think as long as you're a foreground service you probably aren't subject to background data restrictions. You should be a foreground service regardless, otherwise your process might be killed (which is a more likely cause of playback stopping). Is your service a foreground service? If not, make it so. See [here](https://developer.android.com/guide/components/services.html#Foreground).
 I don't think you have sufficient evidence to blame the background data restriction. I could be wrong about all of the following, but I think you'd need to confirm that I am before you could justify the claim:
- I don't think foreground services count as "background" for data restriction purposes.
- There is no evidence this setting is enabled by default on a significant number of devices. Lack of evidence suggests that this isn't the case.
- It's unclear that ECONNREFUSED is the result of background data restriction. The fact that it takes 8000ms to hit the failure (according to the logs) kind of suggests that it's not, since if the failure is due to a local constraint there's no reason why it couldn't occur immediately; short of it being implemented in a pretty strange way.
- How have you validated that the server isn't refusing the connection?
 Interesting. I did find these posts suggesting similar issues:

http://forums.androidcentral.com/samsung-galaxy-s7/655691-music-streaming-stops-pauses-galaxy-s7.html
https://community.spotify.com/t5/Android/Stops-playing-on-Samsung-Galaxy-S7-Edge/td-p/1311156

It surprises me that a foreground service counts as being in the background for data restriction purposes, but your results suggest this is the case, on some devices and/or OS combinations at least :(. The posts above suggest that it's not specific to ExoPlayer or your app. You could try experimenting with explicitly holding [WiFi locks](https://developer.android.com/reference/android/net/wifi/WifiManager.WifiLock.html) and [Wake locks](https://developer.android.com/reference/android/os/PowerManager.WakeLock.html), to see if that makes a difference, but I doubt it. If the system setting is indeed restricting data to the app then I doubt there's a lot you can do. If it were possible for apps to simply ignore a system setting, it would kind of defeat its purpose.

The fill/drain behavior you note in your second post is intentional. In theory batching requests in this way is nicer for network providers, although I don't really know how much truth there is to that statement these days. You can tweak this behavior by passing non-default values to LoadControl.
 Closing as answered; although if you have further insights we'd be interested to learn of them!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Closing for the reasons discussed in #26.
  I suspect this is fixed in the latest `dev-v2`. Please let us know if you still see the problem. Thanks!
  I think the fix for this doesn't rely on N. It would involve checking [AudioDevice](https://developer.android.com/reference/android/media/AudioManager.html#getDevices%28int%29) encodings (via [AudioDeviceInfo.getEncodings()](https://developer.android.com/reference/android/media/AudioDeviceInfo.html#getEncodings%28%29)), and listening for changes using [AudioDeviceCallback](https://developer.android.com/reference/android/media/AudioDeviceCallback.html). These are all available on M.

@arnoutvandervorst That encoding constant is for use by applications that want to output media that is not in one of the supported encodings (AC-3, E-AC-3, DTS or DTS-HD), so I think it's not directly related to this issue.
 To clarify, the underlying issue here is that ExoPlayer relies on HDMI audio plug info to detect passthrough capability, rather than checking AudioDevices (which should include USB audio devices).
 @arnoutvandervorst This will require some changes to ExoPlayer's AudioTrack wrapper, to detect the encoded output capability. Once that's done, hopefully it will be possible to enable passthrough automatically when the input format can't be decoded and the default output audio device supports it.
 No updates at the moment but it's on our radar to look at it soon.  It shouldn't be too complicated moving to 1.5.9(or 1.5.10) if you the DemoApp as guide, I think. The interfaces haven't changed that much. Version 2 would be a different story. What is the complication specifically? HlsRendererBuilder contains most of the details related to HLS.
 As per the answer above, it should be fairly straightforward if you look at the difference between the demo app going from the version you're upgrading from to the current version.
  If this is for caching, please search the existing issues for more information. 

Otherwise, this doesn't really relate to ExoPlayer, so it's outside the scope of this issue tracker. I suggest asking on a more general forum like Stack Overflow or similar.
  Closing since this is neither an issue nor a feature request. Unfortunately we don't have time to answer questions of this type on the issue tracker, sorry.
  Concatenation (at least by our definition) means play one and then the other. For playing two things at once, you need to be using MergingMediaSource. Please give that a try!
  There is quite a lot of discussion around this issue in https://github.com/google/ExoPlayer/issues/91. Please give that thread a read.
  The logic is intended to (approximately) match the behavior of the platform extractor (see [MP3Extractor.cpp](https://android.googlesource.com/platform/frameworks/av/+/master/media/libstagefright/MP3Extractor.cpp)). I think the search is intended to avoid false positives -- it is very unlikely that frames would be found at the correct offsets in random data. Do you think it should work in a different way?
 The fewer frames we check, the more likely we are to hit a false positive. I don't think we want to reduce the number of frames that we look for, but we should allow a match having seen fewer frames specifically in the case that end-of-stream is encountered during sniffing.

Is there any chance you could attach the file you're trying to play, or send it to `dev.exoplayer@gmail.com`? Please update the issue if you send it to the latter, so we know to check. Thanks!
 @peddisri - Please could you provide us with the file?
 This is fixed in `dev-v2`.
  Agreed the device is likely too slow for 4K content.  You mention seeing this error when playing an MP3 file "from internet or device memory" -- just to check, have you definitely seen this with local streams? What stream were you playing the one time you managed to reproduce the error? If you see it again, please take a full bug report with `adb bugreport`. Thanks.
 Closing due to lack of information.
  Hi @cjveldkamp, 
It seems that S7 has had issues keeping up for smooth playback and encryption would impact playback as well.  Have you been able to test a track encoded at a lower bitrate?
Also an ADB bugreport would be helpful.
thaks.
 Hi @cjveldkamp, 
From the log the device seems to close down the h/w TEE driver : 
08-11 09:33:04.136  3138  3583 I TeeDriverClient: driver client closed [hardware/samsung_slsi/exynos8890/mobicore/ClientLib/src/driver_client.cpp:113]

We will need to dig deeper.
 Hi Christian, The information you have provided is sufficient for now. This is not the only issue we've seen with the S7, we are continuing to investigate internally.
 Samsung believes this issue is addressed but would like to verify playback. 
Can you share any content, not geo restricted for them to test?
 Let's use #1764 to track what's left to be done here. @cjveldkamp - If you are able to share test content and/or a demo apk that can be used for testing, please update #1764 rather than this one. Thanks!  I replied erroneously before(deleted now). As @ojw28 pointed out, it is likely you will have to implement your own MediaController to that end, as those values are hardcoded and the listeners made private. I would suggest you try getting some help in a general Q&A site, as the issue tracker is for bugs and feature requests.
  Please see #26
  The issue template requests quite a bit of information that is not provided here. As such, there is nothing we can do to efficiently help with your issue. Please file a new issue including the requested information if you want us to take a look. Thanks.
  Please can you upgrade the version of ExoPlayer that you're using to `1.5.9`. Does the issue still reproduce once you do that? If so, please provide the logcat output again, since we started logging something more useful in `1.5.9` for this type of failure. Please also provide a full bug report as requested in the issue template.
 @manishmips28 - Please provide the requested information, or we'll close this issue. Thanks.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Any update on the CLA? Thanks.
 You're showing up on the CLA signers list for me now, so if you say you signed it again, I think the bot will be happy! Give it a try.
 CLAs look good, thanks!

<!-- ok -->
 Nope, we'll take care of that. Thanks!
  Yes.
 @billyjoker, you don't need to do anything special on the ExoPlayer side. Try adding your HLS sample with subtitles to the demo app. You can follow the demo app to write your own player. The one called 'Apple 16x9 basic stream' (already in the HLS section) has webvtt subtitles. Try it out.  There is not enough information in the title for us to know what the issue is. You should also replace the issue template with the information it requires. Please, open a new issue.
 Yes, this is already supported in exoplayer. You can chose any of the subtitles declared in the master playlist. This same HLS sample is available in the DemoApp, so you can have a look there.
  Please could you provide the following information?
- Version of ExoPlayer (commit number)
- Full stack traces from the exceptions, including any cause
- The output of `adb shell getprop ro.build.fingerprint`

Thanks.
 Please also provide a full bug report using release `1.5.9` of ExoPlayer, shortly after reproducing the issue.
 @biezhihua - Please provide the requested information, or we'll close this issue. Thanks.
  The error is indicative of incorrect media data being fed into the extractor, which implies your custom DataSource isn't implemented correctly. Closing since this is not an issue with ExoPlayer itself.
  This should be fixed in `dev-v2`. There is still a known issue relating seeking and reuse of MediaPeriods which will likely be fixed in the next push. Thanks.
  The [Android Compatibility Definition documents](https://source.android.com/compatibility/cdd.html) specify for each Android version what formats devices must be able to decode.

Support for decoding H.265/HEVC streams has been required since Android 5.0 Lollipop.

Note that OEMs can support whatever formats they want above the minimum requirements of the CDD. So, for example, there may be devices running Android 4.4 builds that can actually decode H.265/HEVC, even though it's not required.
 @AquilesCanta suggested that this might be an issue with capabilities reporting. You could try stepping through the code in [MediaCodecUtil](https://github.com/google/ExoPlayer/blob/r1.5.9/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java#L154) to see if there is decoder for MIME type video/hevc present on this device, and, if so, why it does not have the required capabilities on Android 4.4.
  It's a known issue with V2 that live playbacks are currently starting from the wrong place. We'll fix this shortly.
 This is fixed in latest `dev-v2`.
 What does HLS with WebVTT have to do with this issue, which is about the position of playback in DASH Live streams?
 Please file separate issues for unrelated problems. Please also include all of the information requested in the issue template when you do so; thanks.

There are a whole sequence of changes that fed into fixing this, some tangentially, so it's not really a case of there being a single change. Hence there is no simple diff that "fixed this". I can point you to the code that determines the default playback position, which is here: https://github.com/google/ExoPlayer/blob/dev-v2/library/src/main/java/com/google/android/exoplayer2/source/dash/DashMediaSource.java#L387
 SmoothStreaming: Yes. HLS: Eventually but not yet. We prioritize DASH, as stated [here](https://medium.com/google-exoplayer/test-8b62d50362ef). A feature often lands for SmoothStreaming at the same time as for DASH because the two are quite similar (and share quite a lot of code). A feature lands later for HLS because the HLS spec is a mess.
  Let's track this using #1865 (they're the same).  @watemotion Thanks for reporting this. I don't have a test device on Jelly Bean, so please could you try out the following workaround? Add `|| Util.DEVICE.startsWith("gee")` to the condition [here](https://github.com/google/ExoPlayer/blob/r1.5.9/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java#L226) and let me know if that solves the issue, or if you see some other error. Thanks!
 Great. This issue should be updated when we push the fix.
  Please ask general questions on StackOverflow. This issue tracker is for issues and feature requests. Thanks!
  This works fine on a test device I have, so it seems like it's probably Android version dependent. As per the issue template, please provide details about the devices and Android builds on which the issue reproduces (and preferably a full bug report).
 The sample provided works fine on a Nexus 5X I have running MTC19T (same build as in the bug report attached above). Are you certain it reproduces on that device/build combination?
 Oh right, I can reproduce this now. I was assuming you were sideloading the `.srt` subtitle file (playback works correctly if you do this), but you're actually embedding the subtitle in the `.mp4` file.

There's a pretty big piece of information missing here. How are you doing the embedding? You must be using some kind of tool or utility to do that. Are you sure that the tool works correctly in the case of the input containing font tags? It's notable that QuickTime player doesn't appear to play the captions in your sample video quite right either. If I had to guess, I'd say the media is not formatted correctly.
 It should be possible for you to write a short program that strips out the HTML tags, which you could then run on the srt file before you give it to Handbrake. That's probably the easiest way you could fix this.
 The issue here is that we're treating tx3g samples as though they're just text, where-as they actually consist of:
- 2 byte length N
- N bytes of text
- Optional additional boxes defining, for example, styling information.

We'll push a change shortly that parses just the N bytes of text, which will fix your issues (but wont color the subtitle as specified by the font tags). At some point in the future we'll also parse the tx3g styl box, which contains the color (and other styling information).
  Yes, sorry I hadn't seen this. As @stelma says, there is a parameter for this in the DashChunkSource constructor. You could try changing the value you are using. You should try posting this kind of questions in a general Q&A site, as the issue tracker is preferably reserved for bug reporting and feature requests.
  Most likely the subtitles include their own embedded styling information, which is applied in preference to the locally configured style by default. If you want to disable application of embedded styling information you can call `subtitlesLayout.setApplyEmbeddedStyles(false)`. Please give this a try and let us know if it helps! Thanks.
  Note: We'll be merging something that consolidates the compileSdk/targetSdk/buildTools versions into `dev-v2` shortly.
  Hi, please search existing issues as there are quite a few that refer to this. For instance, [#26](https://github.com/google/ExoPlayer/issues/26) and [#1216](https://github.com/google/ExoPlayer/issues/1216), which also contains some references, I think. Otherwise, you might want to try some programming Q&A, like stack overflow.
  Thanks for reporting this! It's most likely a device/platform issue rather than an issue in ExoPlayer, but we should be able to work around the problem fairly easily.

Did you test this on any other devices at all? If so, could you let us know whether behavior was as expected, or whether this issue was the same across all devices. Thanks!
  It's pretty easy to implement the first request yourself (create a class a bit like `ConcatenatingMediaSource`, but with the ability to add/remove). We should be providing a standard component for this. Let's use this issue to track.

Please file a separate enhancement request for the other request; the two aren't related.
 @b0g8, We are going to work on this topic soon to provide a build-in solution for a media source which supports adding/removing. Subscribe to this thread and you'll get informed as soon as we are ready :) No, still working on it. It is currently undergoing internal reviews. @oikmar Days, most likely. @AOrobator Gapless playback is already supported when you use ConcatenatingMediaSource. @AOrobator The new media source will definitely also support gapless playback.   The application is ultimately responsible for providing the `contentId`, but you'd be correct to observe that there's no way to do this currently in V2. We'll enable this again (and possibly use the hash of the Uri as a default if the app doesn't specify one explicitly).
 @danrossi - The caching functionality is not yet packaged up in a way that enables for easy offline use (it is possible to build offline functionality on top and Google Play Movies does this, but it's quite complicated and custom to the individual app). We are working toward more comprehensive offline support, but it's very complicated to cover all use cases and so will take some time.
@scroozer - What type of media are you trying to play (i.e. what MediaSource implementation are you passing to the player)? Right, there is currently no way to specify a key for that use case. I filed #2201 to track supporting this. I believe this is fixed now. You're able to pass your own `DashManifestParser` into the `DashMediaSource` constructor, and you can pass your own custom contentId to the `DashManifestParser` constructor. Even if you don't specify a custom contentId, I think we now end up using the content uri by default.  I think this is working as intended. You're providing the player with various different streams of media of different lengths, and it's playing them until they end, which is what it's supposed to do. If it's not intentional that there's a minute of captions extending past the end of the video, then the content shouldn't include them.
  See #426.
  Unfortunately there's not much we can do about errors like this. It's likely caused by running out of resources or a platform bug (or both).

Bundling your own software decoder will help reduce the dependency on the platform so you should see more consistent behavior on different Android versions. It sounds like you have found that FFmpeg works fine.

Please be warned that software video decoders can be quite inefficient and slow compared with decoders that are hardware-accelerated. Also, decoding multiple high resolution videos can consume a lot of memory, as each video's reference frames need to be stored as bitmaps. So make sure you play the lowest resolution videos possible for your use case.
  This is basically boils down to being able to smoothly switch/detach/attach surfaces, which is discussed and tracked in https://github.com/google/ExoPlayer/issues/677.

Note: You appear to be using ExoPlayer V1 still, which is pretty old at this point. You should consider migrating to V2, where we've been making improvements in this area.  This seems likely to be an error in your integration code. Specifically, you're likely passing null as `target` to `ExoPlayer.sendMessage` or `ExoPlayer.blockingSendMessage`. You'll need to debug your code to find out why this is happening, and fix it. I assume the issue does not reproduce in the ExoPlayer demo app.
 Please copy the error that you see when using the demo app. I'm pretty sure you wont see the one pasted above. If possible, please provide a full bug report as requested in the issue template.
 See #1190. Redirection from HTTPS to HTTP will only work if you enable cross-protocol redirects. See also [DefaultUriDataSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/DefaultUriDataSource.java#L93).
  1.3.3 is very old. Please try a more recent version and file a new issue if you're still having problems.
  As stated in the ISSUE_TEMPLATE, please provide a link to the media that causes the error. Otherwise it is practically impossible for us to know the cause. You can send it to dev.exoplayer@gmail.com, if you would rather not post it here. Don't forget to include the issue number.
 Closing due to lack of requested information.
 @londhenamdev What version are you using? Is it possible that you are using an old one? I cannot reproduce it and I am inclined to think the issue is the lack of alignment of the segments (which is not a problem in version 1.5.10, I think). Please, try updating exoplayer and if the problem persists, open a new bug, providing all the information required in the issue template.
  We'd be happy to take a look at this if you provide a test stream for us to reproduce the issue with. Please provide one if you wish for us to do so (either here or by email to dev.exoplayer@gmail.com)
 Are you able to provide test content, or shall we close this? Thanks!
 Closing due to lack of sample.
  This should be fixed in `dev-v2`. There is still a known issue relating seeking and reuse of MediaPeriods which will likely be fixed in the next push. Thanks.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Change appears to do the right thing; thanks! I've posted some cleanup related comments.
 This was closed as a side effect of branch restructuring. Please send a new pull request to `dev-v1` once your corp CLA is sorted. Sorry for the inconvenience!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 This pull request was closed automatically as a result of branch restructuring. Apologies for the inconvenience.

I don't think we really want to take pull requests where the result is known to be broken though (i.e. "audio is jumpy" doesn't sound like it would be of sufficient quality for us to accept). Note also that in V2 there's a new `SimpleExoPlayer.setPlaybackParams` method, which should cleanly cover most use cases on API level 23 and above. 
 I believe the majority of the demand for this feature is for playbacks that include audio. For example podcasts, audio books, and educational videos such as recorded lectures. I don't see the video only case as high priority at this time.

I disagree that jumpy audio is better than nothing. As an application developer I'd rather ship a feature that works properly or not ship that feature at all. As a maintainer of ExoPlayer, I don't feel comfortable adding an API that we'd know to be broken and have no plan to ever fix (see below).

You're correct to note that a full solution prior to API level 23 requires a non-trivial effort and most likely integration with a third party library. This is not something we plan to support and we would be unlikely to accept a pull request adding such functionality. A dependency on a third party library would be unacceptable for the core ExoPlayer library. If it were structured as an extension that would be more acceptable, but we'd be unlikely to accept a pull request regardless due to the ongoing maintenance cost. Note also that a dependency on a third party library may be problematic from a licensing point of view.

So, I think the current situation, where we solve the problem for playbacks with audio on API level 23 and above, is pretty much where we want to be right now. It's never been our goal to be all things to all people, and staying focused on supporting core use cases really well is our priority. There is of course nothing to prevent a sufficiently determined application developer from implementing this feature on top of ExoPlayer for their own use.
 Just to respond to the points above:

> Yes video without audio is less common, but if variable speed is left unimplement for this case, applicaions based on ExoPlayer would behave inconsistently with/without audio, even on API >= 23, which would surprise users.

Most applications based on ExoPlayer aren't ever required to play video without audio. Consider YouTube, Facebook, any premium streaming service. All content in such cases is transcoded to include audio (even if the audio is silence). So video only media is a fairly niche use case, the primary example that springs to mind is security camera live streams. Applications that need to play video with audio _and_ video without audio _and_ expose variable speed playback is a very niche use case. Application developers can opt to not expose variable speed playback if they think the inconsistency is confusing, or can opt to do their own integration with a third party library to deliver the feature consistently if they think it's really important.

> Similarly, applicaions based on ExoPlayer can change speed on API >= 23 but can't otherwise, which is also broken from users' perspective. I don't known whether this is more broken than jumpy audio, but I'd rather implement it and leave the choice to applicaion developers and users (our project definitely need it).

It's quite common for applications to expose additional features on newer devices. Newer devices can support HD video whilst older ones can't. Ditto for things like 360 video, VR and 60fps content. Many games wont work at all on older devices. I think most users understand this.

Thanks
  Playback of streams that use more than one slice per frame should work I think. Could you provide a test stream so we can reproduce the issue (either here or by email to dev.exoplayer@gmail.com)? Thanks!
 An ADB Bugreport might be helpful as well. 
 @Sky35, any updates?
 @gvidda, Since the devices are exhibiting different issues it would be best to create separate cases with supporting information like ADB bugreports and links to content to reproduce. 
Thanks.
 Closing because no bug reports have been provided.  This question does not relate directly to ExoPlayer, since we don't implement player UI at this point in time. Please ask your question somewhere like StackOverflow instead.
  We looked into this a while back but found little performance difference between tunneling and normal playback on devices we tested. I think the primary claimed benefits at the time were around improved audio/video synchronization and allowing OEMs to customize decoding and rendering. We also found some device-specific bugs with stability and seeking, though these are likely fixed now.

Could you give some examples of devices where this feature is needed to play 4k streams? If there is sufficient demand may be able to prioritize it. Thanks!
 Yes, this is being worked on. As of the most recent commit `MediaCodecVideoRenderer` and both audio renderers are capable of tunneling.

To complete support we need to (a) check whether both audio + video renderers provide support for tunneling for the media being played, (b) generate an audio session ID for tunneling if they do (there's a static method in `C` to do this), and (c) propagate it to `onEnabled` of the renderers (there are currently TODOs in these classes to allow the ID to be propagated). There's some nuance involved in having to potentially disable and re-enable the renderers if you're in a playlist and they're toggling between having tunneling disabled/enabled, which we'll need to figure out.

We hope to complete this work soon. We don't give exact time estimates. Note that if you want to add the feature internally in the meantime then you'd be best off building on the changes so far, since they do all of what's required aside from the remaining piece described above. My current plan is to have a track selector make the decision (since it knows about the capabilities of each renderer). The selector effectively becomes more of a binder that decides (a) which tracks to select, and (b) what the corresponding renderer configuration should be when using the selection. I'm not completely sure it's the best approach, as opposed to something directly in ExoPlayerImplInternal, but we're weighing it up internally and hope to have some news fairly soon. I'm not sure any special logic is required for skipping output buffer processing. In any case, the solution appears to work fine end-to-end as it is.  You can do what the demo app does and handle the rotation yourself, which avoids Activity recreation and hence the need to save and restore state. This is achieved [here](https://github.com/google/ExoPlayer/blob/r1.5.9/demo/src/main/AndroidManifest.xml#L44).

Else, you'll have to use the normal save/restore functionality provided by Activity to do this. ExoPlayer provides sufficient methods for you to query, for example, the current playback position, and restore it again in the recreated Activity.
  We're removing background support from the demo app in v2 because it's not a fully fledged solution (to do that you'd need to move the player to an Android [Service](https://developer.android.com/guide/components/services.html), which is quite a lot of work).

In the v1 demo app, if the video and audio are demuxed (as is the case for DASH and SmoothStreaming) then the player stops downloading the video component. If they're muxed then there's no way to avoid continuing to download both.
 In HLS audio is typically muxed with the video, in which case there's no alternative but to download both with in the background. HLS does also support demuxed streams though, in which case the player would only download the audio. We recommend DASH for this use case, since supported streams are always demuxed.
  > Unfortunately the getPhysicalWidth/getPhysicalHeight are not returning 4k values on Sony devices, the reason is that Sony renders the Android UI on 2k resolution on all UHD compatible sets

I don't really understand this statement. The whole point of those APIs is for them to report the actual display mode, which can have a different resolution to the UI (which is returned by getWidth/getHeight).
 Following up internally to work out if (a) the API is broken as I think it is, (b) whether it's Google or OEM code that's responsible if so, and (c) when we can expect it to be fixed. Once we have some clarity here, we'll extend the workaround if appropriate to API levels up to the one in which we expect a fix to land.

We believe a similar issue may also affect nVidia SHIELD, so we're taking a look at that too.
 We will be updating the Sony specific check to apply up to (and including) API level 24. We expect that getPhysicalWidth/Height will work correctly from API level 25 onward. We will also be adding a similarly specific check for nVidia SHIELD devices.
 Changes have been pushed to `dev-v1` and `dev-v2` to fix this.
  We have not received any other reports of this issue, which suggests that it doesn't happen in all cases. If you play the first HLS sample in the demo app you can observe that it displays the elapsed time in the video, and that it remains in sync with the progress bar throughout playback up to 30 minutes.

You'll need to provide some concrete reproduction steps that we can use if we're to investigate this issue, starting from the ExoPlayer demo app and providing sample content that can be used to reproduce the problem.

Also, please could you clarify what ground truth you're using (i.e. what are you assuming is the "correct" elapsed time, and how is this calculated)? There are multiple ways that you could derive elapsed time during an HLS playback, so it's unclear what assumptions you're making. Thanks!
 We'll close this soon if no further information is provided. Thanks.
 Please file a new issue when you're able to provide a sample stream for us to look at. It seems equally likely to me that the source content, once ads have been stitched, is inaccurate (e.g. because the sum of the segment durations in the HLS media playlist drifts over time, which would be a violation of the HLS spec). Other players not experiencing this issue is not necessarily indicative of an ExoPlayer bug.
  This error doesn't occur if I add the stream to the ExoPlayer demo app, so it's likely something you've done in your integration. Are you injecting two audio renderers or custom renderers of some kind into the player?

Please provide reproduction steps if you're able to cause the failure inside the ExoPlayer demo app. Else we'll have to close this as an issue with your integration. Thanks!
 A `MediaClock` is a component that drives the playback position. In regular ExoPlayer use, `MediaCodecAudioTrackRenderer` is the `MediaClock` and drives the playback position. Other `TrackRenderer`s ensure that they render in sync with the `MediaClock`. In the case that there's no audio, `ExoPlayerImplInternal` has a component that can act as the `MediaClock` instead.

In your case you have two active `MediaClock`s at once, because you're using `MediaClockAudioTrackRenderer` and you're also using the custom renderer, which implements `MediaClock`. You cannot have two active `MediaClock`s at once, because that implies there could be two different playback positions simultaneously, which makes no sense. Hence we fail the playback.

If you're not interested in audio, one solution is to not inject an audio renderer. Note that we're unable to assist with any issues specific to the custom renderer, if you still have issues after doing this.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 This pull request was closed automatically as a result of branch restructuring. Apologies for the inconvenience.

I think @andrewlewis had some concerns about side effects of this change, which perhaps he can comment on.
 Thanks. This is a very cool workaround! However, I don't think we should integrate it into the core library:
- As you point out, it is not safe to rely on devices having an encoder for the format they need to decode.
- As written, there are no guarantees that the encoded frame will be compatible with the stream to be decoded. For example, the encoded frame might use a different profile than the stream to play.
- Applications using this will need to signal out of band how many frames to drop before the frame has been completely refreshed. To avoid this limitation in general, it would be necessary to parse the incoming stream and figure out what macroblocks had been refreshed, which would involve integrating an H.264/AVC bitstream parser into the library.
  A fix will be available for both exoplayer versions in the next push.
  This question does not really relate to ExoPlayer, so it's outside the scope of this issue tracker. I suggest asking on a more general forum like Stack Overflow or similar.
  The KeyCompatibleMediaController in [PlayerActivity.java](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java#L728) should handle fast forward and rewind. Hopefully you can use that as a starting point.
  You can specify the period (playlist item) to seek to if you use [seekTo(int periodIndex, long positionMs)](https://github.com/google/ExoPlayer/blob/dev-2.X.X-experimental/library/src/main/java/com/google/android/exoplayer2/ExoPlayer.java#L293). For example, to seek back to the start of the first period you can call seekTo(0, 0). The other variant of seekTo seeks relative to the start of the currently-playing period. There are still a few known issues with seeking in playlists, which we're working on, so this may not work perfectly yet.
  I will look into this, could you also provide a link or a file to reproduce the issue? If you'd rather not make it public, please send it to dev.exoplayer@gmail.com
 Yes, I am sorry about this delay. I am currently working on our HLS implementation. As soon as I finish this work, I hope next week, I will get to this. 
 This will be supported in the next push, both versions. Sorry for the delay.

NOTE that we still don't support multiple CC services, but I plan to add this relatively soon. Thanks for the samples.
 I will probably reach out when I start working on multiple CC services support. Thanks!
 Assuming this is fixed based on the comments above.
  I'd suggest trying a general programming Q&A. We avoid answering these kind of queries in the issue tracker.
 Courtesy of @andrewlewis:
https://developer.android.com/training/system-ui/status.html
 I am sorry but I don't quite understand what I have to see in the provided screenshot. Even with more information, as I said before, it is not the purpose of this issue tracker to debug apps from users. Unfortunately, unless you have a bug to report or a feature request, we cannot help you as this is not exoplayer specific.
 ExoPlayer doesn't create or display any notifications. Where-ever that notification is coming from, it's not coming directly from the ExoPlayer library.  I think the manifest is most likely valid according to the DASH spec. The segments aren't aligned, but the `AdaptationSet` doesn't indicate that they are aligned (i.e. `segmentAlignment=true` is not set on the `AdaptationSet`).

ExoPlayer requires segment alignment, which is why you're seeing the failure. I believe DASH-IF IOP also has this same requirement, which in turn implies shaka-packager should be aligning the segments. I've updated the issue you filed on shaka-packager to indicate this. On the ExoPlayer side, we have no plans to support non-aligned segments in DASH. Issue https://github.com/google/ExoPlayer/issues/1049 is tracking failing in a more graceful/obvious way when we encounter non-aligned segments.

Please update this issue if the shaka-packager folks disagree with the above, since I probably wont see updates to that issue without being prompted. If I hear nothing, I'll mark this as a duplicate of https://github.com/google/ExoPlayer/issues/1049 in a few weeks.

Thanks!
  There was a Sony specific performance issue for Sony devices running Android 5.1.1, so if your Xperia is running that version of Android this could be the problem. As far as I'm aware, they fixed it in more recent OS updates. See https://github.com/google/ExoPlayer/issues/816 for details.

I'm not sure about the Tab. You haven't provided sufficient information, as is requested in the issue template, for us to investigate further.
 Closing due to lack of information requested in the issue template.
  If the device has a decoder for AC-3 MediaCodecAudioTrackRenderer will use it, but few devices support this.

MediaCodecAudioTrackRenderer also supports passthrough of AC-3 audio if the device is running Android Lollipop (or later) and is connected to a receiver via HDMI. See also #722.
  Please see: https://github.com/google/ExoPlayer/issues/280
  For capturing the current frame, see #418.

You can reuse the player by calling stop, seekTo and prepare, passing renderers reading the new source. Note that there are some caveats to using more than one platform video decoder at a time; see #1286.
  I can reproduce the issue. It appears to occur only (or at least primarily) with encrypted content, where-as clear content appears to be unaffected. I'm not sure there's much we can do to fix this at the ExoPlayer level. I'll pass the issue to Samsung; hopefully they'll be able to fix it in a future platform update. I was testing using 2.x and the issue still reproduces. I think the problem is in the underlying platform, and I wasn't able to come up with a workaround for it. I've reported through an internal channel and therefore there's no publicly visible ticket/reference. I'd suggest your customers' file their own ticket with Samsung, referencing this issue. Samsung tell me this is fixed in their Nougat update.  As discussed, it's probably fine to merge a workaround for this but I will follow up to check if it's safe to treat DTS-HD as DTS first.
 This pull request was closed automatically as a result of branch restructuring. Apologies for the inconvenience. Please send a new pull request to `dev-v1` if appropriate, I'm not entirely sure where the discussion above ended up. Thanks!
  The server is returning HTTP status code 403 (Forbidden). Please check the server is configured correctly. Does the stream play in other players?
 https://github.com/google/ExoPlayer/issues/625 may also be relevant.
 Closing due to inactivity.  This is not something we support, so you'll have to take ExoPlayer as source and modify it to your needs if you want to do this.
  You mean something like setting playWhenReady as false?
 Playback is started only if `setPlayWhenReady(true)` is called on the player (which happens [here](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java#L371) in the ExoPlayer demo app). If you don't this behavior, simply don't make that call.
  This pull request was closed automatically as a result of branch restructuring. Apologies for the inconvenience.

As per the comment on #1629, we're accepting only bug fixes into V1, which this is not. Please feel free to send a pull request to `dev-v2`. Thanks!
  Rolling into #1661, since the two will probably be handled together.
  This is on our radar and we'll use this issue to track the feature. We don't have a concrete plan at the moment, but we'd like to provide support. We're not yet decided on exactly what level of support we'll provide (e.g. we may opt to support only demuxed media chunks, as we do for DASH and SmoothStreaming).

We'd also hope that the majority of streaming providers will be able to serve DASH manifests in addition to HLS playlists, with both pointing to the same media chunks. In this scenario we'd recommend using DASH manifests for ExoPlayer.
 Some functionality added with https://github.com/google/ExoPlayer/commit/6306c269885475c0a6f947245ed19b60a010ccfd.
 r2.0.2 is the earliest one that includes it, but many features and bugfixes have been added in the later versions. I strongly suggest you update to the latest exoplayer version, if possible.  The link provided gives me a 403 (forbidden) http response. Is it still active? If it is, is it geo-restricted somehow in a way that means we can't access it from the UK? Can you provide a working link that we can reproduce the issue with? Thanks!
 The stream doesn't play because it doesn't contain any IDR frames. As per Apple's [technical note](https://developer.apple.com/library/ios/technotes/tn2224/_index.html), streams should have at least one IDR frame in each TS segment.

If you want to manually force ExoPlayer to play this stream, you can do so by ORing `TsExtractor.WORKAROUND_ALLOW_NON_IDR_KEYFRAMES` into `workaroundFlags`, computed in `HlsChunkSource` and passed to the `TsExtractor` constructor. We have no plans to enable this by default though, since it can cause corruption on seeking for compliant streams. The correct fix is to make sure the content includes IDR frames.
  This file contains an edit list that skips the first video frame, which is a keyframe. Due to a limitation in how we handle edit lists (see [this comment](https://github.com/google/ExoPlayer/blob/a4b4c49446ce4938fe7c6721bb155554a37bd95f/library/src/main/java/com/google/android/exoplayer/extractor/mp4/AtomParsers.java#L261)) we don't send the frame to the decoder, which means samples are dropped up to the next key-frame. This is why video starts playing late.

I'll use this issue to track the enhancement of extracting and skipping samples from the latest keyframe at/before an edit up to the start of the edit. I think it's unlikely we will prioritize it in the near future, unless we find that this type of stream is common. Thanks!
  What @zsmatyas says is correct. We can use this issue to track Mp4 + CEA subtitles support. The clcp handler type is also a concern, but I am sure we can add it to the list of supported handlers as an alternative to "subt". However, it will most definitely be added to version 2.
 This feature will be available soon, in the next v2 release.
  In ExoPlayer V2 you can implement your own `TrackSelection` and use it in place of `AdaptiveVideoTrackSelection`. In your implementation you can choose to switch quality faster if you wish to do so. You can configure ExoPlayer V2 to use your custom implementation by passing a `TrackSelection.Factory` to the `DefaultTrackSelector` constructor that instantiates instances of it.  The stack trace is likely unrelated (see #426).

More information is needed to diagnose this. Can you reproduce the failure to complete preparation when playing the same media in the demo app? Please attach a bug report or send it to dev.exoplayer@gmail.com.
 Closing due to lack of information.
  The ExoPlayer [demo app](https://github.com/google/ExoPlayer/tree/release-v2/demo) allows the user to select which quality is played (click the "Video" button during playback and you'll get a selection dialog). I suggest you take a look at the the demo app code to see how it works.  The stream seems to have one IDR frame at the start, so when playing it starting an some offset (rather than the beginning) we never see an IDR frame, and no frames are output as a result. I don't think we have a simple way to add support for this stream, and it seems like a fairly niche use case.

To make this sort of stream play in ExoPlayer, you can either (1) add IDR frames to the stream (their spacing will determine the worst-case startup latency; note that adding IDR frames defeats one of the purposes of intra refresh, to keep the bitrate and quality reasonably constant) or (2) modify the code in H264Reader to set the synchronization sample flag on the first frame even if it's not an IDR frame. With option (2) you will also need to drop some frames after decoding in the renderer, to avoid showing partially-refreshed frames at the start of playback. The number of frames to drop depends on the intra-refresh interval.
 I think MediaCodec will handle this stream fine if you could find a way to queue an IDR frame before the predicted frames. You could try queuing a hard-coded blank IDR frame at the correct resolution before the sample data. You would still need to drop partially-refreshed frames.

What pull request did you have in mind? I think we wouldn't want to make non-trivial changes to support this because it is quite a rare use case. Using FFmpeg for video decoding is going to be resource intensive and battery-inefficient.
  The link provided is either dead or geo-restricted, so we're unable to test. I'm pretty sure this is fixed in ExoPlayer V2 though. Try with the latest code from `dev-v2` branch (note: V2 is not yet completely stable, but you should be able to validate this).
 I was able to reproduce in V1, but not in V2. I'm pretty sure the issue is fixed. Were you testing using the very latest `dev-v2` branch when you were able to reproduce the issue?
  Do you see the same failure on other devices or only this one?

Going on the assumption that it's device-specific, please could you let us know what the full build fingerprint is (output by `adb shell getprop ro.build.fingerprint`). If you have the device available please try adding another workaround like 1fa9b9235641916baf976b4246f06e86051ad641, but targeting the codec named `OMX.MARVELL.AUDIO.AACDECODER` and the relevant device name, then let us know if that resolves the issue. Thanks!
 What actual device is this, and who's it made by? I can't seem to find any conclusive answer by searching for "xo4".
 We need more information to know that the workaround you've provided is the correct one. Please provide the fingerprint as mentioned by @andrewlewis above, and also a link to the device in question.
 We'll close this soon unless further information is provided. Thanks.
  I found many related issues (particularly with icecast, though I don't know it this is the case here). Considering there is almost no information available for me to help you, I would suggest looking through older issues. If you cannot solve yours after this, open a fresh issue including all the information required in the issue template.
  This is not currently supported (you'd have to prepare the player again, which would cause re-buffering before the playback resumed). Marking as an enhancement for the future.
  Next time, please provide more context to your question, it is impossible for us to help you with this amount of information. The items of the issue template are not optional unless they don't apply (not the case, now).

This is not related to exoplayer. If you search for "Can't create handler inside thread that has not called Looper.prepare()", the first three entries will probable be helpful. Since no code was provided to recreate, I am not able to tell which. The issue tracker is not for generic programming questions, I'd try stack overflow instead. Good luck!
 I don't think DemoPlayer has anything to do with your issue, I strongly suggest revisiting your threading model, I would suspect that you are creating several instances of DemoPlayer in the same thread, and that each instance is trying to call Looper.prepare(), then the error is quite self explanatory. You can check that prepare() has been called before, and only call it if needed. You need to solve these issues yourself, as this is not the purpose of the issue tracker. You can find all information you need by searching for these error messages.
  I may be wrong, but I think this is fixed in V2, coming soon. I don't know what you mean with "opening", though. I don't think the data source gets closed at any point between prepare and enable. @ojw28 and @andrewlewis have the last word on this, however.
 Yes, this should be addressed in V2.
 Fixed in `dev-v2`, which is the V2 development branch.
  One possible explanation is that the video is in H.264/AVC Main profile. Since Android 6.0, devices must support the Main profile, whereas in earlier versions of the Android compatibility definition only Baseline profile was required. You should serve media in a format that is supported by all devices your app runs on. What is the exact video format?
 We need more information about the sample format. If you share the sample media (posting a link here, or emailed to dev.exoplayer@gmail.com with the issue number in the subject) we can take a look.
 Closing due to lack of information.
  Would you kindly provide the URL to the media (you can use dev.exoplayer@gmail.com if you don't want it to be public) so that I can see what the problem seems to be? The provided solution is appreciated but I would like to get to the bottom of the issue, since I fixed a somewhat similar issue not long ago.
 @AquilesCanta - Is this fixed in V2? I don't think we need to backport to V1, but we should make sure things are working correctly in V2.
 Yes, this will be fixed in V2 once the new Hls patch gets pushed. 
  The demo app acts as a working example for HLS playback. Have a look at the [Developer guide](http://google.github.io/ExoPlayer/guide.html), [PlayerActivity.java](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java) and [HlsRendererBuilder.java](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java). One approach is to use the demo's PlayerActivity as a starting point then remove parts which you don't need.
  There isn't enough information provided to help us with this issue, sorry. If it's working in the demo app, I suggest you start there and try and figure out what you've done differently.
  @jefftinker / @wvpaf  - This looks similar to b/23713420. Do we have an update on that?
 Hi,
There seems to be a fix in M MR1 according to the defect referenced by @ojw28  on 6/29.  We don't support Widevine for TS based DASH. We don't support TS based DASH at all, even without content protection. Note that the industry is moving away from TS; with Apple moving to add fMP4 within the HLS spec.

See the [supported formats](https://google.github.io/ExoPlayer/supported-formats.html) page for a good overview of what formats and format combinations ExoPlayer supports.
  Exact same comment as https://github.com/google/ExoPlayer/pull/1637 applies.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks, however we're ramping down development of v1 and so only intend on accepting bug fixes at this point. If there's an equivalent change in the 2.x.x experimental branch, feel free to propose a change there.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  AudioTimestamp is only used if the device is running >= API 19, so it is strange that you're seeing this error. Please send a full bug report captured with `adb bugreport` to dev.exoplayer@gmail.com.
 Which ExoPlayer version number does the stack trace correspond to?
 The stack trace mentions AudioTrack.java:1282, but that line doesn't exist in r1.5.6. Did you modify the code?

In [AudioTrack.java](https://github.com/google/ExoPlayer/blob/r1.5.6/library/src/main/java/com/google/android/exoplayer/audio/AudioTrack.java) we only use AudioTimestamp on devices where SDK_INT >= 19 (see [the check here](https://github.com/google/ExoPlayer/blob/r1.5.6/library/src/main/java/com/google/android/exoplayer/audio/AudioTrack.java#L259)). So I'm not sure how this is occurring if android.os.Build.VERSION.SDK_INT is set correctly.

To investigate further I think we need verified steps to reproduce this in the demo app, not just stack traces from your app from the developer console.
  I think the demo app has the exact behaviour that you're trying to replicate, so I suggest you look at that. I'm not sure whether it resolves the issue for you, but you should be getting and setting position directly on the player, not on the player control.
  When it's ready. Note however that there's already a [dev branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental) that you can play around with, and it already features some [Playlist samples](https://github.com/google/ExoPlayer/blob/dev-2.X.X-experimental/demo/src/main/assets/sample_media.exolist.json#L335) that demonstrate seamlessly transitioning from one piece of media to another. We'll obviously be documenting this properly as we get close to doing a full 2.x release.
  @Muthukathiravan, I'm afraid there's not enough information to know what is being asked. When you create an issue, please give a full description and any applicable information requested in the issue template. Otherwise we will just close the issues, because we can't help!

This is not the right place to get general advice on how to implement your app.
  This is not supported currently.
  We don't take changes directly into the master branch. We currently take bug fixes only in `dev` and other changes into `dev-2.X.X-experimental`
  I doubt this has anything to do with ExoPlayer. It seems more likely to be something related to the attachment of the debugger, which is outside of our control.
  We do not support Verimatrix. The industry as a whole is converging to fMP4 and Common Encryption, and this is what we'll be supporting going forward. We already provide support for DASH / SS with Widevine / PlayReady. Apple recently announced fMP4 and Common Encryption support as an enhancement to the HLS spec, and we'll look at supporting this in due course.
  If there are multiple de-muxed audio tracks, it doesn't make sense for the player to buffer all of them just in case the user changes their selection. Hence when the selection is changed there's a short delay whilst the newly selected stream is buffered. This is working as intended.  We don't have anything to do with Xamarin.Android; you'll have to ask them directly.
  I'm pretty sure that the key request contains a unique device identifier that you can use on the server side. We don't generate the license request ourselves; it comes from the Widevine CDM. I assume the structure of the key request must be documented somewhere. @wvpaf can probable assist.

As an aside, note that you're also free to add your own device identifiers to the request in your `MediaDrmCallback` implementation.
 @mromer,  
To answer your original questions.
Exoplayer does not add anything to the license request that is done in the CDM.
However, there is a certificate serial number in the license request at the proxy level, that can be used to identify the device.

For the 2 requests using privacyMode, you can set privacyMode by MediaDrm.setPropertyString("privacyMode","enable"), the CDM will then generate the additional request for the service certificate and no other action is needed from the player. 
 @mromer  do you have your answers?  I am assuming so, and will close this case.
Please do reopen if further clarifications are needed.
  @AquilesCanta - This reproduces on latest V2. Unsure why. @dreich, I have observed that sometimes (I didn't manage to reproduce at first, that´s why I say sometimes), the keyframes in the TS contain invalid PTS values in contrast with (values that map around two and a half hours later) non-key frames. I was able to notice this with other transport stream inspectors and have double checked the PES syntax to corroborate ExoPlayer is not doing something wrong. I think the media is malformed. If you have reason to think otherwise, please let me know. 

As a side note, VLC does play the stream, but this wouldn't be the first time VLC plays malformed media.   There's no obvious reason why this wouldn't work, but I don't think I've ever seen a SmoothStream/Widevine sample to know exactly what the manifest and chunks should look like.

@wvpaf - Do we have any sample streams?
 Hi @tomaskukol , 

there is no path for MSS and WV. MSS is proprietary and will not inter-operate with WV.
  From the issue template, please include the following:
- A bug report taken from the device just after the issue occurs, attached as a
  file. A bug report can be captured using "adb bugreport". Output from "adb
  logcat" or a log snippet is not sufficient.
 @wvpaf - Could you take a look at this? Thanks!
 Hi there,
We took a look and will need to reach out to the OEM/SoC as the log looks like : 
error @ 12:27:48.097 results in error 500 from the PR server. 
....
06-22 12:27:48.094   492 16120 D DRMAgentDelegate: Silent acquisition initiated
06-22 12:27:48.097   492 16120 E drmagent-jni: [nativeGetMetaData] getProperty failed
06-22 12:27:48.097   492 16120 E drmagent-jni: [nativeGetMetadata] Error occurred
06-22 12:27:48.098   492 16122 D AbstractDRMLicenseAcquisitionHandler: Acquiring license from: http://essdrmproducts-eu.insidesecure.com/test-portal/device/portal?b=6369643d6369643a44423143394243352d453942382d343546352d424239462d4233413936323344303841304061757468656e7465632e746573742e706f7274616c2e636f6d7c7c616369643d6576616c7c7c6d3d505253696c656e744c6963656e736544656c6976657279
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: License Acquisition URL from header: http://essdrmproducts-eu.insidesecure.com/test-portal/device/portal?b=6369643d6369643a44423143394243352d453942382d343546352d424239462d4233413936323344303841304061757468656e7465632e746573742e706f7274616c2e636f6d7c7c616369643d6576616c7c7c6d3d505253696c656e744c6963656e736544656c6976657279
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Web Initiator: supported
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Detected the TTS license server, will reconstruct the LA_URL accordingly
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Force time-based license: No
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Number of Minutes: 5
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Force OPL license: No
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: OPL Level: 100
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Force Interval license: No
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Number of Second: 30
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Play enablers: 
06-22 12:27:48.098   492 16122 D LicenseAcquisitionHlpr: Force Acknowledgement: No
06-22 12:27:48.099   492 16122 D LicenseAcquisitionHlpr: New URL: http://essdrmproducts-eu.insidesecure.com/test-portal/device/portal?PlayRight=1
06-22 12:27:48.155  5872  5881 I art     : Wrote stack traces to '/data/anr/traces.txt'
06-22 12:27:48.155  7035  7040 I art     : Thread[5,tid=7040,WaitingInMainSignalCatcherLoop,Thread*=0xb7721ad8,peer=0x12c000a0,"Signal Catcher"]: reacting to signal 3
06-22 12:27:48.197   492 16122 W AbstractDRMLicenseAcquisitionHandler: Error while retrieving license: 500 : Internal Server Error (at http://essdrmproducts-eu.insidesecure.com/test-portal/device/portal?PlayRight=1)
06-22 12:27:48.198   492 16122 D AbstractDRMLicenseAcquisitionHandler: Received PlayReady Soap Error: 
06-22 12:27:48.198   492 16122 D AbstractDRMLicenseAcquisitionHandler:     FaultCode: soap:Server
06-22 12:27:48.198   492 16122 D AbstractDRMLicenseAcquisitionHandler:     FaultString: The length of the challenge is 0
....
 Moacir, yes that looks right, the 500 error is a result of the invalid challenge provided to the server.

Looking at the log segment you posted in the original report, it looks like the challenge buffer you are providing may be too small:

06-22 10:54:31.847 4919-4919/? D/PRClient﹕ getKeyRequest:: playready_licacq_generate_challenge(1st) returned 0X8007007a BUFFERTOOSMALL, iLicenseQuerySizeLocal = 7102. Continue with good buffer size.

Can you check if increasing the buffer size allows the challenge to be returned?
 @moracir69, our DRM team does not integrate the PlayReady solution. 
You will need to reach out to the Razer folks to get more help. 
  As per https://github.com/google/ExoPlayer/issues/1588, PlayReady supports the streaming use case. Also, this issue fails to provide most of the information that's explicitly requested in the issue template, so closing.
  What is "referer value"? Please provide significantly more information. Thanks.
 I get what a referrer is for, but how you specify it is probably specific to your serving infrastructure and what type of media you're playing. Hence there's insufficient information here for us to help. It could be as simple as just setting a query parameter in the URL that you're trying to play before passing it to ExoPlayer.
 You're not providing sufficient information for us to understand the problem, sorry. You need to explain clearly _how_ the referrer value is being sent. If it's just a query parameter on a URL, you should just change the URL that you give to ExoPlayer.
  Hi, i'm developing a wrapped player from Exoplayer project and my problem is when i play videos with portrait orientation, i have a black screen but the audio is ok. If i change to landscape the video appears correctly and the playback is ok. What it could be happenning ? It occours with whatever kind of video format. Thanks!
 In fact i've checked that Exoplayer demo application works in portrait mode flawlessly... i've asked for a possible cause of my bug
 @Ood-Tsen i did as you said and the log says:

```
Build configuration: [sf] [libui] [libgui]
Sync configuration: [using: EGL_ANDROID_native_fence_sync EGL_KHR_wait_sync]
DispSync configuration: app phase 0 ns, sf phase 0 ns, present offset 3468000 ns (refresh 16666667 ns)
Visible layers (count = 10)
+ Layer 0xb6201000 (com.android.systemui.ImageWallpaper)
  Region transparentRegion (this=0xb6201160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb6201008, count=1)
    [  0,   0,   0,   0]
      layerStack=   0, z=    21000, pos=(-360,0), size=(1440,1280), crop=( 360,   0,1080,1280), isOpaque=1, invalidate=0, alpha=0xff, flags=0x00000003, tr=[1.00, 0.00][0.00, 1.00]
      client=0xb626ecc0
      format= 2, activeBuffer=[1440x1280:1920,  3], queued-frames=0, mRefreshPending=0
            mTexName=12 mCurrentTexture=0
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[1440x1280], default-format=2, transform-hint=00, FIFO(0)={}
+ LayerDim 0xb6208000 (DimLayer)
  Region transparentRegion (this=0xb6208160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb6208008, count=1)
    [  0,   0,   0,   0]
      layerStack=   0, z=    21009, pos=(-180,-320), size=(1080,1920), crop=(   0,   0,  -1,  -1), isOpaque=0, invalidate=0, alpha=0x00, flags=0x00000001, tr=[1.00, 0.00][0.00, 1.00]

      client=0xb583d380
      format= 0, activeBuffer=[   0x   0:   0,  0], queued-frames=0, mRefreshPending=0
            mTexName=5 mCurrentTexture=-1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[1080x1920], default-format=1, transform-hint=00, FIFO(0)={}
+ LayerDim 0xb62a2000 (DimLayer)
  Region transparentRegion (this=0xb62a2160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb62a2008, count=1)
    [  0,   0,   0,   0]
      layerStack=   0, z=    21049, pos=(-180,-320), size=(1080,1920), crop=(   0,   0,  -1,  -1), isOpaque=0, invalidate=0, alpha=0x00, flags=0x00000001, tr=[1.00, 0.00][0.00, 1.00]

      client=0xb583d380
      format= 0, activeBuffer=[   0x   0:   0,  0], queued-frames=0, mRefreshPending=0
            mTexName=7 mCurrentTexture=-1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[1080x1920], default-format=1, transform-hint=00, FIFO(0)={}
+ Layer 0xb62b5000 (SurfaceView)
  Region transparentRegion (this=0xb62b5160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb62b5008, count=1)
    [  0, 415, 720, 1549]
      layerStack=   0, z=    21055, pos=(0,415), size=( 720,1134), crop=(   0,   0, 720,1134), isOpaque=1, invalidate=0, alpha=0xff, flags=0x00000002, tr=[1.00, 0.00][0.00, 1.00]
      client=0xb62a7200
      format= 4, activeBuffer=[ 854x 480: 896,7FA30C03], queued-frames=0, mRefreshPending=0
            mTexName=649 mCurrentTexture=3
            mCurrentCrop=[0,0,854,480] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[720x1134], default-format=4, transform-hint=00, FIFO(0)={}
             [00:0xb61ff480] state=DEQUEUED, 0xb6186290 [ 854x 480: 896,7FA30C03]
             [01:0xb5842b00] state=DEQUEUED, 0xb583c2e0 [ 854x 480: 896,7FA30C03]
             [02:0xb5842b80] state=FREE    , 0xb583c830 [ 854x 480: 896,7FA30C03]
            >[03:0xb61ff080] state=ACQUIRED, 0xb6186330 [ 854x 480: 896,7FA30C03]
             [04:0xb5842980] state=DEQUEUED, 0xb583c8d0 [ 854x 480: 896,7FA30C03]
             [05:0xb61ff980] state=DEQUEUED, 0xb61861f0 [ 854x 480: 896,7FA30C03]
             [06:0xb61ff400] state=FREE    , 0xb6186150 [ 854x 480: 896,7FA30C03]
             [07:0xb5842100] state=DEQUEUED, 0xb583c880 [ 854x 480: 896,7FA30C03]
             [08:0xb61ff100] state=DEQUEUED, 0xb61865b0 [ 854x 480: 896,7FA30C03]
             [09:0xb5842700] state=FREE    , 0xb583c920 [ 854x 480: 896,7FA30C03]
             [10:0xb5842780] state=DEQUEUED, 0xb583c970 [ 854x 480: 896,7FA30C03]
             [11:0xb5842a80] state=DEQUEUED, 0xb583c150 [ 854x 480: 896,7FA30C03]
             [12:0xb61ff580] state=DEQUEUED, 0xb61861a0 [ 854x 480: 896,7FA30C03]
             [13:0xb5842180] state=DEQUEUED, 0xb583c0b0 [ 854x 480: 896,7FA30C03]
+ Layer 0xb623d000 (com.devbrackets.android.exomediademo/com.devbrackets.android.exomediademo.ui.activity.VideoSimplePlayerActivity)
  Region transparentRegion (this=0xb623d160, count=2)
    [  0, 415, 720, 952]
    [  0, 1280, 720, 1549]
  Region visibleRegion (this=0xb623d008, count=1)
    [  0,   0, 720, 1280]
      layerStack=   0, z=    21060, pos=(0,0), size=( 720,1280), crop=(   0,   0, 720,1280), isOpaque=0, invalidate=0, alpha=0xff, flags=0x00000000, tr=[1.00, 0.00][0.00, 1.00]
      client=0xb62a7200
      format= 1, activeBuffer=[ 720x1280: 736,  1], queued-frames=0, mRefreshPending=0
            mTexName=648 mCurrentTexture=2
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[720x1280], default-format=1, transform-hint=00, FIFO(0)={}
             [00:0xb5842380] state=FREE    , 0xb583c1f0 [ 720x1280: 736,  1]
             [01:0xb5842400] state=FREE    , 0xb583c290 [ 720x1280: 736,  1]
            >[02:0xb5842480] state=ACQUIRED, 0xb583c330 [ 720x1280: 736,  1]
+ Layer 0xb62a9000 (FocusedStackFrame)
  Region transparentRegion (this=0xb62a9160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb62a9008, count=1)
    [  0,   0,   0,   0]
      layerStack=   0, z=    21061, pos=(0,0), size=(   1,   1), crop=(   0,   0,  -1,  -1), isOpaque=0, invalidate=0, alpha=0x4d, flags=0x00000001, tr=[1.00, 0.00][0.00, 1.00]
      client=0xb583d380
      format= 1, activeBuffer=[   0x   0:   0,  0], queued-frames=0, mRefreshPending=0
            mTexName=3 mCurrentTexture=-1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[1x1], default-format=1, transform-hint=00, FIFO(0)={}
+ LayerDim 0xb6206000 (DimLayer)
  Region transparentRegion (this=0xb6206160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb6206008, count=1)
    [  0,   0,   0,   0]
      layerStack=   0, z=   100999, pos=(-180,-320), size=(1080,1920), crop=(   0,   0,  -1,  -1), isOpaque=0, invalidate=0, alpha=0x00, flags=0x00000001, tr=[1.00, 0.00][0.00, 1.00]

      client=0xb583d380
      format= 0, activeBuffer=[   0x   0:   0,  0], queued-frames=0, mRefreshPending=0
            mTexName=4 mCurrentTexture=-1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[1080x1920], default-format=1, transform-hint=00, FIFO(0)={}
+ LayerDim 0xb62a0000 (DimLayer)
  Region transparentRegion (this=0xb62a0160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb62a0008, count=1)
    [  0,   0,   0,   0]
      layerStack=   0, z=   100999, pos=(-180,-320), size=(1080,1920), crop=(   0,   0,  -1,  -1), isOpaque=0, invalidate=0, alpha=0x00, flags=0x00000001, tr=[1.00, 0.00][0.00, 1.00]

      client=0xb583d380
      format= 0, activeBuffer=[   0x   0:   0,  0], queued-frames=0, mRefreshPending=0
            mTexName=6 mCurrentTexture=-1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[1080x1920], default-format=1, transform-hint=00, FIFO(0)={}
+ Layer 0xb62c3000 (StatusBar)
  Region transparentRegion (this=0xb62c3160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb62c3008, count=1)
    [  0,   0, 720,  50]
      layerStack=   0, z=   151000, pos=(0,0), size=( 720,  50), crop=(   0,   0, 720,  50), isOpaque=0, invalidate=0, alpha=0xff, flags=0x00000000, tr=[1.00, 0.00][0.00, 1.00]
      client=0xb626ecc0
      format= 1, activeBuffer=[ 720x  50: 736,  1], queued-frames=0, mRefreshPending=0
            mTexName=10 mCurrentTexture=1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[720x50], default-format=1, transform-hint=00, FIFO(0)={}
             [00:0xb61ff600] state=FREE    , 0xb623f2e0 [ 720x  50: 736,  1]
            >[01:0xb61ff680] state=ACQUIRED, 0xb62dd1f0 [ 720x  50: 736,  1]
+ Layer 0xb6240000 (NavigationBar)
  Region transparentRegion (this=0xb6240160, count=1)
    [  0,   0,   0,   0]
  Region visibleRegion (this=0xb6240008, count=1)
    [  0, 1184, 720, 1280]
      layerStack=   0, z=   201000, pos=(0,1184), size=( 720,  96), crop=(   0,   0, 720,  96), isOpaque=0, invalidate=0, alpha=0xff, flags=0x00000000, tr=[1.00, 0.00][0.00, 1.00]
      client=0xb626ecc0
      format= 1, activeBuffer=[ 720x  96: 736,  1], queued-frames=0, mRefreshPending=0
            mTexName=45 mCurrentTexture=1
            mCurrentCrop=[0,0,0,0] mCurrentTransform=0
            mAbandoned=0
            -BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[720x96], default-format=1, transform-hint=00, FIFO(0)={}
             [00:0xb5842500] state=FREE    , 0xb6186dd0 [ 720x  96: 736,  1]
            >[01:0xb61ff500] state=ACQUIRED, 0xb6186380 [ 720x  96: 736,  1]
             [02:0xb5842080] state=FREE    , 0xb623ff60 [ 720x  96: 736,  1]
Displays (1 entries)
+ DisplayDevice: Built-in Screen
   type=0, hwcId=0, layerStack=0, ( 720x1280), ANativeWindow=0xb6174608, orient= 0 (type=00000000), flips=126563, isSecure=1, secureVis=0, powerMode=2, activeConfig=0, numLayers=4
   v:[0,0,720,1280], f:[0,0,720,1280], s:[0,0,720,1280],transform:[[1.000,0.000,-0.000][0.000,1.000,-0.000][0.000,0.000,1.000]]
mAbandoned=0
-BufferQueue mMaxAcquiredBufferCount=1, mDequeueBufferCannotBlock=0, default-size=[720x1280], default-format=1, transform-hint=00, FIFO(0)={}
>[00:0xb62ac280] state=ACQUIRED, 0xb6075b00 [ 720x1280: 736,  1]
 [01:0xb62ac400] state=FREE    , 0xb6075ba0 [ 720x1280: 736,  1]
SurfaceFlinger global state:
EGL implementation : 1.4
EGL_QUALCOMM_shared_image EGL_KHR_image EGL_KHR_image_base EGL_QCOM_create_image EGL_KHR_lock_surface EGL_KHR_lock_surface2 EGL_KHR_fence_sync EGL_KHR_cl_eventEGL_IMG_context_priorit
y EGL_KHR_gl_texture_2D_image EGL_KHR_gl_texture_cubemap_image EGL_KHR_gl_texture_3D_image EGL_KHR_gl_renderbuffer_image EGL_EXT_create_context_robustness EGL_ANDROID_blob_cache EGL_
ANDROID_framebuffer_target EGL_KHR_create_context EGL_KHR_wait_sync EGL_KHR_gl_colorspace EGL_ANDROID_image_crop EGL_ANDROID_recordable EGL_ANDROID_native_fence_sync EGL_ANDROID_imag
e_native_buffer EGL_KHR_gl_colorspace
GLES: Qualcomm, Adreno (TM) 320, OpenGL ES 3.0 V@84.0 AU@05.00.02.006.020 (CL@)
GL_AMD_compressed_ATC_texture GL_AMD_performance_monitor GL_AMD_program_binary_Z400 GL_EXT_debug_label GL_EXT_debug_marker GL_EXT_discard_framebuffer GL_EXT_robustness GL_EXT_texture
_format_BGRA8888 GL_EXT_texture_type_2_10_10_10_REV GL_NV_fence GL_OES_compressed_ETC1_RGB8_texture GL_OES_depth_texture GL_OES_depth24 GL_OES_EGL_image GL_OES_EGL_sync GL_OES_EGL_im
age_external GL_OES_element_index_uint GL_OES_fbo_render_mipmap GL_OES_fragment_precision_high GL_OES_get_program_binary GL_OES_packed_depth_stencil GL_OES_depth_texture_cube_map GL_
OES_rgb8_rgba8 GL_OES_standard_derivatives GL_OES_texture_3D GL_OES_texture_float GL_OES_texture_half_float GL_OES_texture_half_float_linear GL_OES_texture_npot GL_OES_vertex_half_fl
oat GL_OES_vertex_type_10_10_10_2 GL_OES_vertex_array_object GL_QCOM_alpha_test GL_QCOM_binning_control GL_QCOM_driver_control GL_QCOM_perfmon_global_mode GL_QCOM_extended_get GL_QCO
M_extended_get2 GL_QCOM_tiled_rendering GL_QCOM_writeonly_rendering GL_EXT_sRGB GL_EXT_sRGB_write_control GL_EXT_texture_sRGB_decode GL_EXT_texture_filter_anisotropic GL_EXT_multisam
pled_render_to_texture GL_EXT_color_buffer_float GL_EXT_color_buffer_half_float GL_EXT_disjoint_timer_query
  Region undefinedRegion (this=0xb60937ac, count=1)
    [  0,   0, 720, 415]
  orientation=0, isDisplayOn=1
  last eglSwapBuffers() time: 274.683000 us
  last transaction time     : 30.520000 us
  transaction-flags         : 00000000
  refresh-rate              : 59.999999 fps
  x-dpi                     : 315.309998
  y-dpi                     : 315.649994
  gpu_to_cpu_unsupported    : 0
  eglSwapBuffers time: 0.000000 us
  transaction time: 0.000000 us
  frame avg:            39.201 ms       (13 triggers)   (29 frames)
  frame longest:        50.267 ms
  compose avg:          2.244 ms        (0 triggers)    (30 composes)
  compose longest:      4.731 ms
  post avg:             0.349 ms        (0 triggers)    (30 posts)
  post longest:         1.038 ms
  render avg:           3.598 ms        (0 triggers)    (30 renders)
  render longest:       5.799 ms
  layers: (4:10)                (FocusedStackFrame (0xb62a9000): 0:660)* (DimLayer (0xb6206000): 0:368)* (DimLayer (0xb6208000): 0:4)* (DimLayer (0xb62a0000): 0:184)* (DimLayer (0xb6
2a2000): 0:277)* (StatusBar (0xb62c3000): 0:10533) (com.android.systemui.ImageWallpaper (0xb6201000): 0:2084)* (NavigationBar (0xb6240000): 0:4075) (com.devbrackets.android.exomediad
emo/com.devbrackets.android.exomediademo.ui.activity.VideoSimplePlayerActivity (0xb623d000): 0:168) (SurfaceView (0xb62b5000): 30:122)
  triggers:             (rate: 753:41858)       (compose: 0:41) (post: 0:255)   (render: 6:364) (29:116174 frames)      (30:126582)
VSYNC state: disabled
  soft-vsync: disabled
  triggers:             (rate: 0:2)     (10445 sw vsyncs)       (0 skipped)     (0:300314 vsyncs)   (1:348328)
  numListeners=22,
  events-delivered: 381763
    0xb585b038: count=-1
    0xb585b060: count=-1
    0xb585b088: count=-1
    0xb585b0b0: count=-1
    0xb585b178: count=-1
    0xb6038f60: count=-1
    0xb618e1c8: count=-1
    0xb618e308: count=-1
    0xb618e330: count=-1
    0xb618e358: count=-1
    0xb618e3f8: count=-1
    0xb618e470: count=-1
    0xb618e4c0: count=-1
    0xb618e5b0: count=-1
    0xb618eb00: count=-1
    0xb62c01f0: count=-1
    0xb62c0290: count=-1
    0xb62c0420: count=-1
    0xb62c05b0: count=-1
    0xb62c0600: count=-1
    0xb62c0970: count=-1
    0xb62c0dd0: count=-1
h/w composer state:
  h/w composer present and enabled
Hardware Composer state (version 01030000):
  mDebugForceFakeVSync=0
  Display[0] configurations (* current):
    * 0: 720x1280, xdpi=315.309998, ydpi=315.649994, refresh=16666667
  numHwLayers=5, flags=00000000
    type   |  handle  | hint | flag | tr | blnd |   format    |     source crop (l,t,r,b)      |          frame         | name
-----------+----------+------+------+----+------+-------------+--------------------------------+------------------------+------
      GLES | b6186330 | 0000 | 0000 | 00 | 0100 | ? 7fa30c03  |    0.0,    0.0,  854.0,  366.1 |    0,  415,  720, 1280 | SurfaceView
      GLES | b583c330 | 0000 | 0000 | 00 | 0105 | RGBA_8888   |    0.0,    0.0,  720.0, 1280.0 |    0,    0,  720, 1280 | com.devbrackets.android.exomediademo/com.devbrackets.android
.exomediademo.ui.activity.VideoSimplePlayerActivity
      GLES | b62dd1f0 | 0000 | 0000 | 00 | 0105 | RGBA_8888   |    0.0,    0.0,  720.0,   50.0 |    0,    0,  720,   50 | StatusBar
      GLES | b6186380 | 0000 | 0000 | 00 | 0105 | RGBA_8888   |    0.0,    0.0,  720.0,   96.0 |    0, 1184,  720, 1280 | NavigationBar
 FB TARGET | b6075b00 | 0000 | 0000 | 00 | 0105 | RGBA_8888   |    0.0,    0.0,  720.0, 1280.0 |    0,    0,  720, 1280 | HWC_FRAMEBUFFER_TARGET
Qualcomm HWC state:
  MDPVersion=440
  DisplayPanel=9
HWC Map for Dpy: "PRIMARY"
CURR_FRAME: layerCount: 4 mdpCount: 0 fbCount: 4
needsFBRedraw:YES  pipesUsed: 0  MaxPipesPerMixer: 4
 ---------------------------------------------
 listIdx | cached? | mdpIndex | comptype  |  Z
 ---------------------------------------------
       0 |     YES |       -1 |      GLES |  0
       1 |     YES |       -1 |      GLES |  0
       2 |     YES |       -1 |      GLES |  0
       3 |     YES |       -1 |      GLES |  0


Overlay State
==========================
Ctrl(mdp_overlay) id=1 z=0 fg=0 alpha=255 mask=-1 flags=0x20000
        src(msmfb_img) w=736 h=1280 format=13 MDP_RGBA_8888
        src_rect(mdp_rect) x=0 y=0 w=720 h=1280
        dst_rect(mdp_rect) x=0 y=0 w=720 h=1280
Data(msmfb_overlay_data) id=1
        data(msmfb_data) offset=0 memid=20 id=0 flags=0x0 priv=0
Attached to dpy=0

Ctrl(mdp_overlay) id=-1 z=0 fg=0 alpha=0 mask=0 flags=0x0
        src(msmfb_img) w=0 h=0 format=0 MDP_RGB_565
        src_rect(mdp_rect) x=0 y=0 w=0 h=0
        dst_rect(mdp_rect) x=0 y=0 w=0 h=0
Data(msmfb_overlay_data) id=0
        data(msmfb_data) offset=0 memid=-1 id=0 flags=0x0 priv=0
Attached to dpy=0

Ctrl(mdp_overlay) id=-1 z=0 fg=1 alpha=255 mask=-1 flags=0x848000
        src(msmfb_img) w=896 h=480 format=17 MDP_Y_CBCR_H2V2_TILE
        src_rect(mdp_rect) x=0 y=0 w=854 h=366
        dst_rect(mdp_rect) x=0 y=415 w=720 h=864
Data(msmfb_overlay_data) id=0
        data(msmfb_data) offset=0 memid=-1 id=0 flags=0x0 priv=0
Attached to dpy=0

Ctrl(mdp_overlay) id=-1 z=0 fg=0 alpha=0 mask=0 flags=0x0
        src(msmfb_img) w=0 h=0 format=0 MDP_RGB_565
        src_rect(mdp_rect) x=0 y=0 w=0 h=0
        dst_rect(mdp_rect) x=0 y=0 w=0 h=0
Data(msmfb_overlay_data) id=0
        data(msmfb_data) offset=0 memid=-1 id=0 flags=0x0 priv=0
Attached to dpy=0

Pipes used=4


================
Allocated buffers:
0xb583c0b0: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c150: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c1f0: 3680.00 KiB |  720 ( 736) x 1280 |        1 | 0x00000900
0xb583c290: 3680.00 KiB |  720 ( 736) x 1280 |        1 | 0x00000900
0xb583c2e0: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c330: 3680.00 KiB |  720 ( 736) x 1280 |        1 | 0x00000900
0xb583c6a0: 7200.00 KiB | 1440 (1920) x 1280 |        3 | 0x00000900
0xb583c830: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c880: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c8d0: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c920: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb583c970: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb6075b00: 3680.00 KiB |  720 ( 736) x 1280 |        1 | 0x00001a00
0xb6075ba0: 3680.00 KiB |  720 ( 736) x 1280 |        1 | 0x00001a00
0xb6186150: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb61861a0: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb61861f0: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb6186290: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb6186330: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb6186380:  276.00 KiB |  720 ( 736) x   96 |        1 | 0x00000900
0xb61865b0: unknown     |  854 ( 896) x  480 | 7FA30C03 | 0x82006900
0xb6186dd0:  276.00 KiB |  720 ( 736) x   96 |        1 | 0x00000900
0xb623f2e0:  143.75 KiB |  720 ( 736) x   50 |        1 | 0x00000900
0xb623ff60:  276.00 KiB |  720 ( 736) x   96 |        1 | 0x00000900
0xb62dd1f0:  143.75 KiB |  720 ( 736) x   50 |        1 | 0x00000900
Total allocated (estimate): 26715.50 KB
```
 Thanks!! Finally i've figured out it was a scale problem!!
  Please provide the information requested in the issue template.
 Please read the issue template and provide the requested information.
  Widevine is supported on API levels 18 and higher, as per the [supported formats](https://google.github.io/ExoPlayer/supported-formats.html) page. It's likely just an issue with the emulator; please try on a real device instead.
  Are you trying to play a Smooth Streaming stream? Please could you provide a link to the media?
 We need a link to the stream you're trying to play to diagnose this, as requested in the issue template.

If the stream really is HLS, are you building renderers to play an HLS stream? Using [HlsRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java) for example?
 The name "E" looks wrong. Why is this using the same port as RTMP? We need to have the URL to a stream that is publicly visible to debug this. If you can't share it on this issue tracker please email it to dev.exoplayer@gmail.com.
 That looks wrong too. There is no domain name.
 This issue is really very confusing. Closing.
  You're more than welcome to create your own extension to do this, but we're unlikely to provide official support (i.e. we're unlikely to merge it into this repository or ensure that it continues to build against new versions of ExoPlayer). It's unclear that there's sufficient demand, and every edge case feature we decide to take slows down the speed at which we're able to iterate on the core use cases we should be prioritising. If sufficient demand is demonstrated, we could reconsider.
  HTTP status code 404 indicates that the resource was not found on the server. Please check the URI being requested is correct.
  A fix will be available soon, thanks for the report.
  Does the cause returned by `e.getCause()` contain the information you need? (See the [javadoc for ExoPlaybackException](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer2/ExoPlaybackException.html).)
 As per #1552, it's not possible to provide a complete list of errors, but I agree it would be useful to have documentation listing the most common/important exceptions. I'll use this issue to track adding the documentation. Thanks!
  I think this is because the server you're hitting is using ICY rather than HTTP. There are various related issues [here](https://github.com/google/ExoPlayer/issues?utf8=%E2%9C%93&q=is%3Aissue%20ICY%20). Thanks for sharing!
  Unfortunately, we cannot help you. This issue tracker is reserved for ExoPlayer specific bugs, enhancements and such. You should try a general programming Q&A exchange site, like stack overflow for example. Good luck!
  I'm pretty sure this is resolved in `dev-v2`.
  Will look into it now, sorry for the delay.
 Unfortunately, I will not be able to help you, there are a few issues. The first and most important, is that the live window is too small (less than 10 seconds most of the time) with only 3 segments[1] forming the playlist. At the slightest buffering, you will always end up falling behind. The second issue that I found is that the playlist refreshing is taking up to 4 seconds, sometimes. This means that if you were playing the "most recent" segment of the playlist (optimistic assumption) and you refresh to get the next segment, due to [1], you will end up playing the oldest segment of the new playlist, after that, you are one delay away from falling behind, if you didn't during the playlist refresh. The first approach and likely the only solution would be to increase the live window to a 30 seconds period, if it is within your reach. Good luck!
  This is a feature request to avoid immediate retry for live streams in ExtractorMediaPeriod. It would be better to wait until the player can make no further progress before triggering the retry.  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Is a new Period started in the manifest when these reconfigurations occur? I think this might be a requirement for doing this kind of thing (i.e. I don't think it's valid for the server just to be adding and removing Representation elements without starting a new Period).
 I disagree with the assertion that defensive programming is better, in this context. I think I might write a blog post explaining exactly why at some point, since this has come up a few times, but roughly speaking:
- If a media spec specifies something, it's probably specified for a reason.
- If someone then produces content that violates the spec, if all media players follow the model you suggest, then the "solution" is for players to "handle" the spec violation.
- This encourages the production of more content that violates the spec, and so violates something that was specified for a reason.
- This in turn undermines the original media spec, and also makes it borderline impossible for someone to write a new player from scratch that plays the majority of media, because you end up with all kinds of unofficial "stuff" that players become expected to handle even though said "stuff" doesn't exist anywhere in an official media spec.

I don't think we'll be fixing this unless you can show that it's allowed by the DASH spec. I believe a new Period element has to be used for such reconfigurations.
 The [DASH IF Recommendations](http://dashif.org/w/2015/04/DASH-IF-IOP-v3.0.pdf) implies in 3.2.12 that you should probably be starting a new Period to remove or add Representation elements in an Adaptation Set.

You should probably find a way of avoiding having server restarts underneath streams that users are actually consuming (short of something going fairly badly wrong, in which case failure would be expected). I don't think there exist technical reasons why this should not be possible.
  I don't have enough information to give you a definitive answer, but I can tell that you are not trying to play an HLS stream, so you should just use an ExtractorSampleSource(no HlsExtractorWrapper or TsChunk involved, chunks are related to adaptive streaming, such as HLS). If you just pass along the default extractors, the correct extractor should be selected automatically. 
 If what you said is right (about the pat referencing pid 256, and the pmt for that program having pid 257), then you are not going to be able to play it with Exoplayer's TS extractor (as you said, this is not spec compliant). It is not something we are going to add a workaround for, either. If there are no extra discrepancies, you could try replacing the pid for every 257 pid pmt to 256.
 Unfortunately we don't have a H.263 reader and I don't think the 264 one would work. But of course you can try it out with some small hardcoding. We could write one but it is not a priority right now considering the upcoming v2 release (feel free to contribute with one if you think it is similar to the h264 one).

I updated the title to avoid misleading people. I will use this issue to track H263 support in transport stream.
  What is the error from ExoPlayer? Please share a full bug report (`adb bugreport`). I couldn't find any clues about what is going on in those logs.

Note: We have very limited resources to deal with device-specific issues like this, and we can't efficiently investigate bugs on devices/builds that haven't passed Android CTS. I'd recommend you follow up with Broadcom directly.
 To add to the above: What you're asking with these issues is equivalent to asking us to assist with the platform media bring-up of this specific device. This is beyond what we're able to assist with, and is the responsibility of the device manufacturer (either yourselves of Broadcom, depending on exactly what your relationship is with them).

Please do not file further issues relating to this device unless you can show that it correctly passes Android's compatibility test suite, and is generally stable and able to run other apps such as YouTube successfully. Details about the compatibility test suite can be found [here](https://source.android.com/compatibility/cts/). Once the device passes CTS, at this point it becomes reasonable for you to open issues on this issue tracker. Please attach CTS test results showing the device passes when you do so.

Thanks!
  Isn't the problem more that the server is using ICY, which isn't _real_ http? I wouldn't say it's a problem that an http stack can't handle a non-http protocol. It's more an added bonus if it does ;)...
 Closing since we do not officially support ICY. As per @cyberjaime45 above, there exist pretty trivial solutions to make it work if you wish to do so.
  Smoothly switching between surfaces (including detaching and re-attaching) is tracked by https://github.com/google/ExoPlayer/issues/677.  This looks like a duplicate of #890 and #1211. Try using the [okhttp extension](https://github.com/google/ExoPlayer/tree/master/extensions/okhttp).
  We don't support SHVC. The answer in https://github.com/google/ExoPlayer/issues/1571 applies here too.
 You're correct; it would be possible to bundle a software decoder that supports SHVC streams into your app, and use it from ExoPlayer. Such an approach would be similar to how we support bundling of FLAC, Opus and VP9 software decoders via [extensions](https://github.com/google/ExoPlayer/tree/master/extensions). We don't have any plans to do this to support SHVC though. Also note that software decoders are typically less able and significantly less power efficient than the hardware decoders that can be provided by the underlying platform on many devices.
 @arianaa30 - I think those are questions you'll have to answer for yourself ;). It's not something we're interested in supporting directly at the current time.
  This is likely to be unrelated to PlayReady and related to your observation above, which is a failure to handle the audio and video elements changing order.

That said, it feels like a server that does this is really just asking for trouble. It should be trivially for a server to keep them in the same order, and it's likely that doing so will minimize the risk of client failures.
  This is an example of raw PCM audio in mp4. The audio samples end up with incorrect timestamps. The samples are rechunked, but it appears the timestamps are wrong even if rechunking is disabled. 

@dcower - Could you take a look at this? It's quite possible that the audio timestamps are incorrect in the media itself, but it could be that we've missed something.
 Closing this on the assumption the media was broken. If it was just the one file and you don't know how to generate a file that's similarly broken, this seems quite likely. If you're aware of a way to generate such files please let us know, and we'll take a closer look.
  Please provide sufficient information, as is clearly requested in the issue template, if you wish for us to investigate this kind of issue.
  Thanks for the detailed feedback. My best advice to help you get started is to get a test URL for an example of your DASH content, add it to `Samples.java` in the demo app, build and play. So it should be pretty easy to get something that's able to play your content, using the demo app as a starting point.

Your comments on the size of the demo app and its complexity are justified, and this is something we're actively addressing for 2.x The number of classes in the demo app in 2.x currently stands at 6 (previously it was 12), and will likely fall to 5. This is partly achieved by the promotion of the code in the `player` package in the demo app to the library itself, and the creation of a new `SimpleExoPlayer` class.

We'll be pushing 2.x to GitHub soon, and will update this issue once that's done.
 Note - To keep track of 2.x updates, you can follow us on [Medium](https://medium.com/google-exoplayer).
 @nitro52 - You can add a URL to an existing collection in ExoPlayer's demo app too. Samples are listed in `Samples.java`, so you just need to modify that. For 2.x this class will turn into a JSON file and it'll be possible to load any such file into the demo app without making any code modifications.

@brianwernick - Sure, feel free to mention it on issues where it's relevant, such as this one ;). It's a cool project!
 The demo app has been significantly reduced in size in ExoPlayer V2. For V1 we had:

```
$ find v1/demo/src/main/java/ -name '*.java' | xargs wc -l
   207 v1/demo/src/main/java/com/google/android/exoplayer/demo/SampleChooserActivity.java
   268 v1/demo/src/main/java/com/google/android/exoplayer/demo/Samples.java
    65 v1/demo/src/main/java/com/google/android/exoplayer/demo/SmoothStreamingTestMediaDrmCallback.java
   213 v1/demo/src/main/java/com/google/android/exoplayer/demo/EventLogger.java
   199 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java
   204 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/SmoothStreamingRendererBuilder.java
   600 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/DemoPlayer.java
    88 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java
   266 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/DashRendererBuilder.java
   762 v1/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java
    62 v1/demo/src/main/java/com/google/android/exoplayer/demo/WidevineTestMediaDrmCallback.java
  2934 total
```

in V2 we have:

```
$ find v2/demo/src/main/java/ -name '*.java' | xargs wc -l
  419 v2/demo/src/main/java/com/google/android/exoplayer2/demo/SampleChooserActivity.java
  336 v2/demo/src/main/java/com/google/android/exoplayer2/demo/TrackSelectionHelper.java
  439 v2/demo/src/main/java/com/google/android/exoplayer2/demo/EventLogger.java
   24 v2/demo/src/main/java/com/google/android/exoplayer2/demo/DemoApplication.java
  574 v2/demo/src/main/java/com/google/android/exoplayer2/demo/PlayerActivity.java
 1792 total
```

If you ignore the files that are essentially just scaffolding (i.e. not interesting), the lines of code goes from 2246 to 910.
  I suspect it's by design that PlayReady is only supported using `KEY_TYPE_STREAMING`. It seems questionable that you'd ever need support for `KEY_TYPE_OFFLINE` given AndroidTV devices are not really expected to have intermittent internet connectivity. Please could you clarify what the use case is?
 The first log snippet that you've posted pretty much says `KEY_TYPE_OFFLINE` isn't supported for PlayReady: 

> PlayReadyDrmPlugin﹕ getKeyRequest failed since only streaming type is supported

My assumption is that this is by design, since PlayReady is only supported on AndroidTV devices, and, well, why would an AndroidTV device ever be in airplane mode?
 Given it appears that the PlayReady plugin only supports streaming type licenses, I doubt there's support for the model you're suggesting. It's likely that the Widevine plugin does support this model across all AndroidTV devices, but I'm not 100% certain. @wvpaf could confirm.

The model you're suggesting sounds pretty flaky in general, to be honest, since it sounds like the user has no way to recover the license if something goes wrong (if there were a way to do this, you could use it to solve the issue that you describe). All services I'm aware of use `KEY_TYPE_STREAMING` for AndroidTV, and have a server-side solution that knows what each user has purchased. If you have this information, then you can issue streaming licenses accordingly.
 @wvpaf - Could you confirm that the above is correct? Thanks!
 As noted above, lack of internet connectivity is not really a valid concern for AndroidTV devices.
 ojw28@ we will have to check with the implementers of the PlayReady on Android. However, I do believe you are correct the implementation is limited to Android TVs and does not have a use case for downloaded offline content.
 Verified. PlayReady is Android TV only, and there is no 'offline use' case.
  If it's only a specific piece of content that's broken, and if we don't have a link to that content, how are we supposed to follow up on this... ;)?
 I think the problem is that your WebVTT playlists are missing #EXT-X-ENDLIST tags. The lack of this tag indicates that additional segments might be added to the playlist in the future. As a result the WebVTT part of your media is (correctly) treated as a live stream that may have additional media appended to it.
 So to conclude: Adding #EXT-X-ENDLIST tags to your WebVTT playlists should resolve the issue. You should do this regardless, since the lack of this tag may well lead to players repeatedly refreshing the WebVTT playlist in case new segments have been added. 
  That error means your DataSource is either returning invalid media, or violating the contract of the DataSource interface. I can't see anything obviously wrong with your approach, but since this is a custom component we're unfortunately unable to provide direct assistance.
  Please can your try with the current dev branch? This was probably fixed by 82ef94e. Thanks!
 Closing assumed fixed on `dev` and in `1.5.9`. Please re-open if this is not the case.
  How frequently do you see the discontinuity logging? I haven't been able to reproduce the issue so far, having left the stream playing for 10 minutes.
 This looks the same as #1285. Did you try the workaround there?
 Users with insufficient bandwidth are going to get re-buffering every ~5 seconds or so anyway, even with a permanent fix. So it's not like a permanent fix is going to solve the problem for this class of users. If they fundamentally don't have sufficient bandwidth for the stream, it's not going to play without interruption.

I think (although I'm not 100% certain) this is fixed in a more satisfactory way in 2.x, but it'll be at least another few weeks until we start pushing this version to GitHub, and we have no plans to backport a fix to the current release.
 If you use the temporary fix as described in #1285 then the stream will come back. If you don't then it might not, as is clearly documented on that issue. I'm not really sure what your question is. We're going to implement a more permanent fix in 2.x. It's not available on Github yet. Even the permanent fix wont allow users with insufficient bandwidth to play content without interruption.
  As stated above, this is a duplicate of [#179](https://github.com/google/ExoPlayer/issues/179). Thanks, @rvergara1.
 Re-opening specifically to track support for bitmaps inside of SMPTE-TT. Marking as an enhancement. Realistically, I don't think we'll be looking at this any time soon, so if someone wants to work on a pull request please feel free. It's probably not too hard to add support now that plumbing of bitmaps through Cue and SubtitlePainter is in place. Yes, I think it's just a case of adding support in TtmlDecoder (and related classes in the text/ttml package). I don't think there are any gotchas, and you shouldn't need to change anything outside of that package.  ExoPlayer does this by default if you prepare a player but don't call `setPlayWhenReady(true)`. So the solution is to:
1. Create your new player when coming back to the foreground.
2. Seek to the position you wish to restore.
3. Prepare the player.

Note that seeking before preparing is more efficient then doing it the other way around (which could cause additional and unnecessary network requests to be made).
  Hi. Thanks for the PR. Some questions:
- Is a stream that has a Xing header without a table of contents definitely always CBR?
- Is this written anywhere authoritative, or do you know of any other players that make this assumption?
- Do you have any information on how common streams with a Xing header and not TOC are?

The current logic in ExoPlayer is not completely consistent because we treat streams with Xing headers as seekable but CBR if they are missing a frame count, and unseekable if the the frame count is present but the TOC is missing.

The Android platform in contrast treats files with Xing headers missing either the frame count or the TOC as unseekable ([see XINGSeeker.cpp](https://android.googlesource.com/platform/frameworks/av/+/master/media/libstagefright/XINGSeeker.cpp#47)).
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Please re-send this pull request into the `dev-v2` branch if it's still applicable. I'm not sure this is a good idea though; if media is supposed to be seekable, then it should contain sufficient information to enable seeking without the player having to resort to heuristics. @andrewlewis - any comments on the above?
 Given the lack of authoritative information on how to interpret this case, I think we should continue to be conservative and treat it as a signal that the file is VBR so can't be seeked without a TOC.

If we see that the vast majority of files with a Xing header lacking a TOC are actually CBR, then we should make this change, but I don't think we have enough data to justify it yet.

Applications that can control the input media format and care about seeking should use another container, as VBR MP3 seeking is approximate anyway.
  This is not something we support directly. It should be pretty easy for you to locate the point in code where ExoPlayer feeds data to the `AudioTrack`, though, and work back from there...
  Please file an issue where the entire description isn't in the title, and containing the information requested in the issue template. Thanks.
  Please can your try with the current dev branch? This was probably fixed by https://github.com/google/ExoPlayer/commit/82ef94e30b7daa1825b7f88aa48680e7257df5a9. Thanks!
 You'll need to provide a full bug report (captured with `adb bugreport` for us to look at this further).
 I think it's a device specific issue. The device doesn't appear to have passed Android's compatibility test suite (https://source.android.com/compatibility/cts/) and also appears to be running a fairly strange looking eng build. You should report the issue to the device manufacturer.

In addition, we'll see if we can come up with a workaround for the issue, and will add additional logging to determine exactly which decoder is causing the issue.
 The underlying device/platform is broken. You should report the issue to the device manufacturer. It should be pretty straightforward for them to work out what's going on and correct it. This error occurs in platform code, and should not happen:

```
E/ExoPlayerImplInternal( 4670): Caused by: java.lang.IllegalArgumentException
E/ExoPlayerImplInternal( 4670):     at android.media.MediaCodecList.getCodecCapabilities(Native Method)
E/ExoPlayerImplInternal( 4670):     at android.media.MediaCodecInfo.getCapabilitiesForType(MediaCodecInfo.java:211)
```
 The code underneath the platform is throwing an `IllegalArgumentException` as shown above, which seems to be at odds with your statement that it's fine. Please clarify. Is it possible for you to root cause exactly why this exception is being thrown?
 Some extra logging was added in https://github.com/google/ExoPlayer/commit/a68d5243828289568977c1dc91fd9baf20336682 that will print the name of the decoder for which the query fails. It would be good to know what this is, so if you could attach another bug report that would be great.
 Sorry, but this is something you need to talk to Broadcom about. If the underlying support just isn't there in the platform, then ExoPlayer isn't going to work.
  Please provide sample content if you want us to look at this. Thanks.
  Same as https://github.com/google/ExoPlayer/issues/1569.
  > Setting xmlParserFactory.setNamespaceAware(true); in the constructor of MediaPresentationDescriptionParser fixes this, i.e. the xlinks are fetched and the whole MPD is parsed properly.

I don't think this is true. There's no logic anywhere in ExoPlayer for fetching xlinks; it's not functionality that we currently support.
 Cool :). I'm not sure whether we'll be implementing xlink support any time soon, by the way. It seems preferable that the server side should inline the proper content, rather than pushing (a) complexity, and (b) extra request round trips, onto the client.
  Playback from a `file://` URI should work fine. The sample you link to doesn't appear to play successfully when using the `http://` URI either. It's most likely a content issue, in which case we'll follow up with the owner.
 The sample content contains very strange edit lists in three of the video representations. We're following up with the content owner to find out whether they think the media is valid, and will either get the media fixed or patch ExoPlayer depending on the outcome of that discussion. Thanks!
 We believe the content is broken. When it was generated the DASH and MSE specs weren't even finalised, so it likely contains more mistakes in addition to the one that's causing the playback failure in ExoPlayer. I've suggested that the content owners might like to either fix the content, or remove it.
  SurfaceView rendering wasn't properly synchronized with view animations in older releases of Android (by design, since I believe there were some trade-offs involved). So it's expected that you may see the content of a SurfaceView lagging slightly behind an animation in the way that you describe. This has been fixed in N, where SurfaceView rendering is properly synchronized.

If this is causing issues in your app then TextureView can be used on M and earlier to resolve the problem. For N and later you should prefer SurfaceView if possible, because SurfaceView provides get better video frame release timing, tighter AV sync, and lower power consumption.
 This is now on our new [FAQ](https://google.github.io/ExoPlayer/faqs.html) page.
  @Ood-Tsen is correct. Note that since ExoPlayer uses Android's platform decoders, any support would likely rely on changes being made to the underlying platform, in addition to changes to ExoPlayer.
  We do not support SRD, and have no plans to do so at the moment.
 As for features more generally, the DASH spec is so broad I doubt there exists a player that implements all of it. In particular full multi-period support is missing from many players (ExoPlayer currently provides support only if the available representations are unchanged across periods, although we'll be providing full support in 2.x). I suspect there's a really long tail of obscure DASH features that barely any players support.
 I'm not aware of any (which isn't the same as saying one doesn't exist). It feels like quite a niche feature, so it seems doubtful that a player would have prioritised implementing it.
 This is something you'll have to figure out and answer for yourself; it's not something we'll be spending any time on ourselves. The code is all open source and there for you to look at!
  I think the provided bitstream is probably invalid. The initialization segment contains a PPS with id 0, which we use to configure the codec, but some slices in the sample data appear to refer to a picture parameter set with id 1. I did not find any in-band PPSs near the start of the sample data.

Also, avplay/libav outputs errors about a missing parameter set with id 1 when playing the stream, though its decoder's error concealment seems to hide the errors well and the output looks okay.

Please could you check that the bitstream is valid?
  A proper fix for this will be available in the next push. If you need it urgently, you can safely remove the failing assertion.
  This tracker is for bugs/enhancements relating to ExoPlayer. For general advice on how to use the library from your app, read the [Developer guide](http://google.github.io/ExoPlayer/guide.html) and take a look at the [demo app](https://github.com/google/ExoPlayer/tree/master/demo). If you still have questions I suggest you try asking on Stack Overflow or similar.
  If you look in the demo app, the `EventLogger` prints a log line more or less when the representation changes, [here](https://github.com/google/ExoPlayer/blob/dev-1.5.6-rc/demo/src/main/java/com/google/android/exoplayer/demo/EventLogger.java#L120). If you trace this back, you'll see that the event that's fired is `ChunkSampleSource.EventListener.onDownstreamFormatChanged`.

The only difference between this event and exactly what you're asking for is that `onDownstreamFormatChanged` is fired when a new format is _fed_ to the decoder, where-as what you're asking for is an event when a new format is _output_ from the decoder and rendered on screen. In practice, the difference will be small. If you're just looking to do some logging when the format changes, `onDownstreamFormatChanged` should be more than good enough for your needs. It's probably good enough for most UI changes that you might like to do in response to a format change as well.

If `onDownstreamFormatChanged` isn't suitable for your needs, please describe your use case in more detail. Thanks!
 What kind of transformation do you need to do?
 Sounds fancy ;). I'll try a different question. What information would you need about the format in order to apply the appropriate transformation? Or in other words, if there were an `exactlyWhatYouWantCallback(x)`, what's the minimum amount of information `x` that you need?
 Got it. It's somewhat awkward, because detailed information about the format doesn't propagate as far as you need it to. In some cases where the representations are similar formats, the decoder might not even output a format changed event.

I don't suppose your representations happen to be all different resolutions? If so then you could probably use `onVideoSizeChanged`, which should be spat out at the correct time. Note however that since events are delivered to a different thread, you'd still have a somewhat uncertain delay between the event being dispatched and it being processed on the receiving thread.

The best solution is probably going to require you to hook directly into the player's rendering thread. You could, for example, extend `MediaCodecVideoTrackRenderer` by:
1. Override `onInputFormatChanged` to note changes to the input format.
2. Override `onQueuedInputBuffer` to note down the presentation timestamp of the first buffer queued after each input format change.
3. Override `processOutputBuffer` or some of the methods that are called by that method. You should be able to see when a buffer is released whose timestamp is >=\* to the one noted for each input format change. At this point, you can fire an event.

\* In most cases you'd expect == to work, but you might be unlucky if the first frame is dropped, plus we've seen some funky decoders that like to "tweak" the timestamps a little (we now have device acceptance tests to prevent this, for new devices).
 Cool, let us know how you get on! I'm not actually sure whether you'll find sufficient information in the format objects that `onInputFormatChanged` receives, so that's something to be aware of. In 2.x this should be resolved (there are no longer separate `Format` and `MediaFormat` classes in 2.x, and the formats propagated to the renderers should include sensible format identifiers).
  This error is not ExoPlayer specific; it's an SSL failure in the network stack encountered when trying to open a connection to the server. I suggest you start your investigation [here](https://developer.android.com/training/articles/security-ssl.html#CommonProblems).
  We don't need sub-millisecond granularity, so this is not an issue. Microseconds are used as the unit only for consistency.
 There's no benefit in making the calculation more accurate just for the sake of it. Once you're below the level at which the most discernible human can perceive a difference, there's nothing to be gained from going further.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  @itaykinnrot - Are you able to reproduce this in the ExoPlayer demo app, without modification? If so, please file a new issue along with a full bug report (captured just after reproducing the issue) and the other information requested in the issue template. Also, what version of ExoPlayer are you using?

We've never managed to reproduce this ourselves. Whilst it's obviously easy to fix the NPE by adding a null check, we worry that this is simply masking some other underlying issue. Specifically, how is `enabledTrackCount` greater than zero if the playback is exiting, which is a condition for the stack trace posted in the description to occur.
 We need to understand how it actually occurs, not just when. For example, what's the sequence of execution that causes the player to get into this state? We have no way of determining whether this is the correct patch to fix the root cause if we don't fully understand how the issue arises.
  ```
kunalbhatia : can you provide the source of the URL's you are trying to play?  It seems that these may not be valid. 
```
 Hi @kaunalbhatia, Can you do as @pgrov asked? Thanks. 
 @kunalbhatia  Any update?
 Your update seems to confirm that your expressplay server requires the application/octet-stream content type so the issue can be closed. Is that correct or are there any further questions?
 It's possible the server simply requires the Content-Type header to be set to something (but doesn't really care what) for POST requests. We've seen the same requirement for sending POST requests on the client side when using the Cronet network stack. For this reason, we've actually just made a change in ExoPlayer V2 that sets Content-Type to application/octet-stream by default for Widevine provisioning/key requests. This change will be pushed to GitHub soon.

@jefftinker - Is application/octet-stream that be the most correct value for Content-Type, given we need to set it to something, or is there a more suitable value?
 I actually concluded that Content-Type shouldn't be set for provisioning requests, since a provisioning request doesn't have a body. We do now set Content-Type for key requests to `application/octet-stream`, which fixes this issue. 

Closing. If Content-Type really should be set to application/json for provisioning requests, even though they don't have bodies, please let me know. Thanks!
  I get a 403. Can you fix this?
 Just wondering, have you managed to play the file with another player? I haven't. I will look into it, but I have a feeling that it may be malformed. 
 Well, as a matter of fact the last URL you sent does not correspond to an mp4 file (I wasn't able to determine what it was either). Considering the stack trace you sent before, the dropbox.com url and the first one don't point to the same file. Please, open a new issue providing a correct media sample. If possible, make sure they don't expire. That way, the information exchange may become useful to third parties reading this thread.
  This is not something we support.
  @pantos27, it looks like getBufferedPercentage and getDuration work fine with the latest version of ExoPlayer. Please update the issue if you still have the problem.  minimumUpdatePeriod in the manifest is set to 42 years, which indicates to the player that it doesn't need to refresh it. The way the manifest is setup, that's probably correct. SegmentTemplate is used, which means the player doesn't need to update the manifest to work out the location of segments as they come available. The UTCTiming element allows the player to synchronize its clock (approximately) with that of the server's.

So, I can't see anything obviously wrong. We'd need a working test stream to provide further assistance.
  This is most likely fixed in the latest V2 release, since we've made some fixes for AV sync that apply to the multi-audio HLS case. If the issue remains, please provide test content so that we can reproduce. I didn't know there was an issue related to this, otherwise I would have added a reference. Do try the latest version and let us know if the issue persists. Unfortunately, this fix is not available in V1.   Please open a new issue including all the requested items in the issue template. Namely, a way to reproduce the issue ourselves.
  The injectable/extensible nature of ExoPlayer and the dependencies it has on the Android platform mean that it's not feasible to provide a single authoritative list of all errors.

Each exception class should be documented sufficiently well for you to understand what that error is, however. If you're aware of specific exceptions that are being thrown where there's insufficient information to help you understand when they can happen, and in the unlikely event that the exception stack trace doesn't help you to resolve your uncertainty, then please let us know.
  Making this type of change to the core `ExoPlayer` API is problematic due to the way that renderers are injected. The `setVolume` method simply wont work when using any audio renderers other than `MediaCodecAudioTrackRenderer`, for example. The dependency (as a result of the cast) from `ExoPlayer` to `MediaCodecAudioTrackRenderer` is also problematic from a structural point of view' `ExoPlayer` isn't supposed to care what type of renderers have been injected into it.

In 2.x we'll be adding a `SimpleExoPlayer` that extends `ExoPlayer` and injects a set of renderers suitable for most use cases. We'll be adding a `setVolume` method to that class.
 Or to paraphrase: It's by design that `ExoPlayer` doesn't look like `MediaPlayer`. It's deliberately more bare-bones and agnostic as to what components are injected into it. In 1.x `DemoPlayer` in the demo app is the API that looks like `MediaPlayer`. In 2.x it will be `SimpleExoPlayer` that looks like `MediaPlayer`.
  I think @marcbaechinger wrote that particular block of code. @cdrolle is also looking at EIA-608 in general.
 We've made some recent changes in `dev-v2` around 608 handling. Marking as obsolete, assumed fixed. There aren't any reproduction steps for us to verify.  This error could be caused by more or less anything, including a memory leak in your own application code. The point at which an OutOfMemoryError is thrown is often not indicative of the root cause. You need to analyze your application's memory usage to determine what's happening. Some information on how to do this is provided [here](https://developer.android.com/studio/profile/investigate-ram.html). Actually, for this specific case it's likely you're hitting https://github.com/google/ExoPlayer/issues/2301, which was fixed in 2.2.0 and above. If you're already using 2.2.0 or greater, the advice above still holds.  Thanks!
  I'm not exactly sure what you're trying to say. The log shows the connection repeatedly failing (as you say, the network is bad). The player is retrying in an attempt to make the connection successfully. This is working as intended.

Please clarify exactly what behavior you're seeing _and_ what you expect the behavior to be and how the two differ. Thanks!
 I'm not sure it's accurate that there are multiple requests in flight at once. We always cancel the previous request before we start another one. Are you sure this is the case? How exactly are you verifying that there are multiple requests in flight, and that the previous one has not been canceled prior to completion?
 It's also not accurate to state that the entire video has to finish downloading before playback starts. I observe playback starting whilst the download is still in progress for the dizzy sample.
 Thanks for the update. I haven't used it recently, but in the past I found that using [Stetho](https://code.facebook.com/posts/393927910787513/stetho-a-new-debugging-platform-for-android/) with ExoPlayer's OkHttp extension was a pretty good way of analyzing network requests.
  Although your manifests contain EXT-X-DISCONTINUITY-SEQUENCE tags, they are being incremented incorrectly. I will provide a more complete analysis tomorrow, but I believe this to be a media issue, not an issue with ExoPlayer.
 I added some debugging lines to `HlsPlaylistParser` to print out data from each manifest as it's received. The playlists being served aren't quite right. Here's a sequence I observed. For each segment line in the logging below, the first number is the assigned discontinuity sequence number. This is defined in the HLS spec as:

> A segment's Discontinuity Sequence Number is the value of the EXT-X
>    -DISCONTINUITY-SEQUENCE tag (or zero if none) plus the number of
>    EXT-X-DISCONTINUITY tags in the Playlist preceding the URI line of
>    the segment.

The second number is the assigned media sequence number.
1. First response. So far so good:

```
EXT-X-DISCONTINUITY-SEQUENCE    0
EXT-X-MEDIA-SEQUENCE    62
(Segment)   0   63
EXT-X-DISCONTINUITY 
(Segment)   1   64
(Segment)   1   65
(Segment)   1   66
(Segment)   1   67
EXT-X-DISCONTINUITY 
(Segment)   2   68
```
1. Second response. `EXT-X-DISCONTINUITY-SEQUENCE` has been incremented without any corresponding removal of an  `EXT-X-DISCONTINUITY` tag. This is invalid. Note that the segments now (incorrectly) have different discontinuity sequence numbers than previously:

```
EXT-X-DISCONTINUITY-SEQUENCE    1
EXT-X-MEDIA-SEQUENCE    63
EXT-X-DISCONTINUITY 
(Segment)   2   64
(Segment)   2   65
(Segment)   2   66
(Segment)   2   67
EXT-X-DISCONTINUITY 
(Segment)   3   68
(Segment)   3   69
```
1. Third response. The  `EXT-X-DISCONTINUITY` tag has now been removed but `EXT-X-DISCONTINUITY-SEQUENCE` has not been incremented:

```
EXT-X-DISCONTINUITY-SEQUENCE    1
EXT-X-MEDIA-SEQUENCE    63
(Segment)   1   64
(Segment)   1   65
(Segment)   1   66
(Segment)   1   67
EXT-X-DISCONTINUITY 2
(Segment)   2   68
(Segment)   2   69
```

The problem here is that the `EXT-X-DISCONTINUITY` tag should have been removed in the second response rather than in the third response. Alternatively, `EXT-X-DISCONTINUITY-SEQUENCE` should have been incremented in the third response rather than in the second response. I think you probably want to do the first of these, since it doesn't make sense for an `EXT-X-DISCONTINUITY` tag to appear before the first segment, as is the case in the second manifest.
 Note that in addition to breaking playback in ExoPlayer, this issue likely causes more subtle failures when switching variants during ad breaks across other HLS players.
 I don't see much noise when I run the test URL through `mediastreamvalidator`. There are quite a few warnings there about incorrect mimeTypes, target durations and bitrate declarations, but it seems to me they're all things you should be fixing rather than ignoring? The bitrate declaration issues appear to be particularly concerning. According to the validator the actual bitrate of each variant is over 2x what the master playlist is declaring?

To answer your question, no, I don't know of another tool that I would recommend. I debugged the issue with discontinuity sequence insertion manually. I would however encourage you to ensure you have regression tests in place to validate correct behavior going forward.
  To clarify, this doesn't actually cause the process to crash. Playback fails in a controlled way and the player's listener is invoked with the exception. Is that correct?

If so, then is the complaint here that the exception isn't useful (e.g. you'd prefer something like a `NoSupportedVariantsException` or similar)? This is something we'll be handling more gracefully in V2. It seems non-urgent to fix it in V1.
 I think this is fixed in V2. Please let us know if this is not the case.  Not yet, we are working to have live seeking finished first. Sorry for the delay. This is fixed by the change ref'd above.  This doesn't sound like an ExoPlayer issue...
  How do you expect players to handle this type of playlist? I think most clients would pick either AVC or HEVC, rather than mixing the two during playback. On Andorid at least, it's not easy to support seamless switching between AVC and HEVC because they use completely different decoders.

If you do expect a client to pick either AVC or HEVC, then why not just have a separate playlist for each?
 Hi ojw28,

Thanks for your response. People might choose HEVC and AVC for high and low profiles, respectively. Decoder has to be reinitialized during stream switching, so why it makes difference for switching between HEVC/AVC and AVC/AVC?

Thanks,
 Decoders on Android support seamless adaptive reconfiguration, but switching between AVC and HEVC would require releasing one decoder and acquiring another. The former is likely to be significantly smoother than the latter.

I'm still not entirely convinced about the justification for mixing AVC and HEVC. If a device supports HEVC and if you're going to provide HEVC transcodes, then why would you serve the low quality variants in AVC when you could save a whole bunch of bandwidth by serving them using HEVC too?
 Hi ojw28,

Understood. Thanks for the explanation.

Thanks,
Hoi Ming
  As per previous comments on https://github.com/google/ExoPlayer/pull/1521 and the HLS spec, you need to include  EXT-X-DISCONTINUITY-SEQUENCE tags in this type of HLS media playlist. The KCRA media playlists do not contain this tag. This is a spec violation, and is the reason you're observing these issues.
 Please file a separate issue, since the issue here is very much that the tags are missing.
  We'll clean this up a little bit for consistency, but otherwise looks good. Will merge as is; the listener method name might change shortly, so be on the lookout for that. Thanks!
  There are quite a few issue threads about this. You can start with [#1216](https://github.com/google/ExoPlayer/issues/1216). There are many ways to do this. If after some research you still don't manage to get what you want, open a new issue with more specific constrains on what you want.
 Note: we recently pushed support for playback speed adjustment to `dev-v2`. Please see the recent updates to #26 for more information.  It's kind of unclear what the expected behavior should be. I can envisage some services not wanting the sticky behaviour. It seems preferable to get a definitive answer (e.g. an addition to the DASHIF interoperability document), rather than adding a player option...
 Closing since no further information was provided, and since I'm not aware of any specification or interoperability point that indicates sticky behaviour is the right thing to do.

I also don't think it's impractical to come up with a serving solution that avoids the issue described. For example, in the steps described above, the player must have obtained a URL before the first step. You could perform the redirect at that point. The player would already have an appropriate CDN URL at step 1, and could just use that directly.
  The problematic stream seems to use H.265/HEVC Main Profile Level 4. The [CDD](http://static.googleusercontent.com/media/source.android.com/en//compatibility/android-cdd.pdf) states that devices must support up to Main Profile Level 3, and that Android TVs must support up to Main Profile Level 4.1. So it is not safe to rely on non-TV devices being able to play this stream, in general.

The stream plays correctly on Nexus Player and Nvidia Shield, so I thought the distortion might be due to playing a stream with an unsupported profile/level using the software decoder OMX.google.hevc.decoder. However, the CodecCapabilities for that decoder actually include Main Profile Level 4 so this should should be playable.

Separately, we should add capabilities checks for H.265 to reject the stream instead of trying to play it on devices that do not advertise support for this profile/level.
 This stream has two PPSs, but we currently only send the first one to the decoder.
 It's a bug in ExoPlayer that we don't currently send all the parameter sets the decoder. I'm working on a fix. As far as I know, the stream and decoder are fine.
 This should be fixed in dev.
 @wachiu I just tried playing the stream in 1300_blocky.zip again and couldn't spot any blockiness using the code on the dev branch. Could you describe where/when the video is blocky? Are you playing the same stream? What device/build are you using?
 @wachiu On Nexus 10 running LMY49J I don't see the distortion that was visible before the fix referenced above. There are some subtle blocking artifacts but they look like the distortion introduced by some lossy block-based video compression algorithms, not error concealment in the decoder. What Android build is the device running? Do you see the same distortion when viewing the video using VLC/ffplay/similar?
 @wachiu Possible but unlikely. Could you first double-check that you can see [this code](https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/extractor/ts/H265Reader.java#L457) on the branch you're using? If so, please send a screenshot of the distortion to dev.exoplayer@gmail.com and I will see if I can see the same thing.
 @wachiu Please could you file a new issue, and include a link to a H.265/HEVC DASH test stream we can use to reproduce the issue? I will rename this issue to indicate that it's specific to HLS. Thanks.
  Please take a full bug report with `adb bugreport` and attach it to this issue.
 The emulator you're attempting to run your app on claims that it can't play any of the variants (even the 480x270 one). Depending on whether the emulator is supposed to support video playback, this is either working as intended or a bug specific to that emulator. Please file an issue on whoever provides the emulator (AndroVM?).

Note that this issue is unlikely to reproduce on real Android devices. We recommend testing video playback on physical hardware.
 Do you see the issue on all devices, or just that one (and the emulator)? Could you also provide a working media link? The one at the top of this issue used to work, but is now returning 403 (FORBIDDEN). Thanks!
 Is it a single device not working on 4.1.1, or multiple devices running 4.1.1?
 Also, on the affected device, what value does `MediaCodecUtil.maxH264DecodableFrameSize` return?
 As a test only, does playback work properly if you hardcode `maxH264DecodableFrameSize` to return `384000` instead? The device's decoder is advertising a very low level of video support, which is why playback doesn't proceed correctly. It's likely a device specific issue; and is unlikely to be a widespread problem.

We could/should add a few sanity checks around maxH264DecodableFrameSize to guard against incorrect values. Specifically, we could check that the value we get is at least the frame size recorded by the device's camera, since there pretty much aren't devices that can't play back video recorded on the device!
 It's resolution rather than bitrate that's the problem. If you were to go low enough then it would work, but it would provide a poor user experience. Instead, we should be doing more to correctly determine the true capability of the device. Please test having `maxH264DecodableFrameSize` return `384000` on the affected device.
 Please can you provide more detailed information. The very top line of a stack trace isn't exactly useful. Please provide a full bug report taken just after the issue described above occurs.
 Please take the bug report immediately after reproducing the issue (I would expect to see some relevant logging in the report if so - so either you didn't do this or something weird is happening capturing bug reports from this device). Either way, we really need the full stack trace of the "Internal track renderer error." issue you're seeing with this modification in place (either in a bug report or pasted here). That looks like a secondary issue, so the way forward here would be to fix the secondary issue, and then try again with the modification in place.
 Please pull the very latest `dev` branch, which will include the change ref'd above. Then modify `MediaCodecUtil` in the same way you have been doing (to return `384000` from `maxH264DecodableFrameSize`). Does playback then work correctly? Thanks!
 You should definitely not use that as a general solution. It will restrict the ability to play higher quality streams unnecessarily on many devices. We'll have a think about what the long term fix should be.
 Can you please provide another working link (I'm guessing they're expiring). Or better, attach or paste a sample master playlist into this issue. Thanks.
 I would expect that source to play something on the physical device (i.e. the 144p variant) even without making the test workaround. Is that variant new since the issue was originally filed?
 Note that the emulator is effectively irrelevant for the purposes of launching the app, since real users don't run apps in emulators. So it's really the physical device that we're interested in.
 The change above should "fix" this issue by assuming that all devices are capable of playing back H264 videos whose total number of pixels don't exceed 172800 (480*360). This should allow the affected device to play the lowest quality variant that you're generating.

It would be further possible to add device specific workarounds to maxH264DecodableFrameSize conditional on Util.DEVICE, Util.SDK_INT and so on, which could enable them to play higher quality variants if we can verify that they really are capable of doing so. We probably wont do this unless we see significant demand and/or find a really popular device that would benefit from us doing this.
  Please file issues that contain all of the information requested in the issue template. Specifically, we need a link to the content and a full bug report taken from the device to investigate this type of issue. Please file a new issue if you're able to provide complete information.
  Please file issues that contain all of the information requested in the issue template. Specifically, we need a link to the content and a full bug report taken from the device to investigate this type of issue. Please file a new issue if you're able to provide complete information.
  It's not within the scope of this project to ensure that emulators are performant enough and/or behave correctly during media playbacks. You'll need to file an issue on the emulator that you're experiencing problems with. In the meantime, we recommend always testing on real devices.
  We don't have any control over Facebook's application.
  I added this sample to the demo app in Samples.java as:

```
new Sample("Test", "http://vod-mp4.sfanytime.com/trailers/17000/17347/17347_download_eng_LOW_920.ism/Manifest.ismv", Util.TYPE_SS)
```

and it plays just fine for me. Are you specifying the correct `TYPE_SS` constant?
 Ah, when using `TYPE_SS` ExoPlayer actually ends up fetching the SmoothStreaming manifest at:

```
http://vod-mp4.sfanytime.com/trailers/17000/17347/17347_download_eng_LOW_920.ism/Manifest
```

which it then plays correctly. It's unclear why you're trying to play the fragmented mp4 directly? Given it lacks a segment index, it's not (efficiently) seekable. The SmoothStreaming manifest provides seeking information, so it's better to play it that way.

ExoPlayer doesn't play the fragmented mp4 directly for a couple of reasons:
1. We don't parse the `mehd` box, which leads to ExoPlayer determining an incorrect duration for the stream. We can fix this.
2. There aren't any `tfdt` boxes in the stream. Which I believe is normal for `piff`, but isn't something we handle currently. We can probably do so.

Even if we fix the above, we still wont allow seeking in this type of media. So you should really play the SmoothStreaming manifest, rather than the video file directly.
 This is fixed in dev. Note that seeking wont work. This is intentional for this type of media, as described above.
  We blacklist known broken decoders in `MediaCodecUtil.isCodecUsableDecoder`. We should add a case there to blacklist the (device + api-level + codec) combination, which will then solve the issue for all users of the library.

What are the values of `Build.DEVICE`, `Build.MODEL`, `Build.MANUFACTURER` and `Build.VERSION.SDK_INT` for the affected device?
 Yes, you can implement your own [MediaCodecSelector](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecSelector.java). The default implementation just calls through to `MediaCodecUtil.getDecoderInfo`. You could implement a variant that does the same thing, except in the special case where you want something other than the default behavior. For such cases have your implementation call `MediaCodecUtil.getDecoderInfos` instead, and then iterate through the list until you find a decoder that you're happy to use and return that one.
 We've blacklisted the decoder in the dev branch. Please check on the device, since we don't have one on which we can reproduce.

It's quite possible that there are other device names used for variants of this hardware, but since we don't know what they are, we have no good way of expanding the blacklist to cover them at this point in time. Hence marking this issue as fixed.
  It feels quite unlikely that you're seeing that exact error on both an emulator and a real device. It might be failing on both, but I suspect the errors will be different. Please attach separate bug reports for the emulator and the phones, taken using `adb bugreport`. Note that copy/pasting sections of log is not sufficient. Thanks!
 It's the real device we're interested in (emulators often have spurious issues related to media playback), so we'll take a look once that bug report is provided.
 The stream plays fine for me on a Nexus 5 (build KTU84Q) running the demo app from the `dev` branch.
 Closing due to lack of information (i.e. a full bug report from a real device).
  I don't understand what you're trying to do. What does it mean to "access or evaluate a chunk"? What are you actually trying to do specifically? Please provide more detail.
 When do you need this information? When the chunk starts to load, finishes loading, starts to be played, finishes playing? What are you actually trying to do with this information?
 You can see when the format being played changes by listening for invocations of `onDownstreamFormatChanged` on the `EventListener` that you pass to the `ChunkSampleSource` constructor. Note that this will only be invoked when the format changes, not for every chunk. If this is insufficient for your needs then you'll need to make some modifications directly to the ExoPlayer library to get at what you need.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Please see inline comments. You also need to ensure your GitHub account is properly associated with your corp CLA (the googlebot will pick this up if it's the case).
 CLAs look good, thanks!

<!-- ok -->
 Can you provide some sample content that contains these ID3 frames?
 Oops, I think I might have merged this before you'd finished! In any case, I've done some cleanup in https://github.com/google/ExoPlayer/commit/388acb39d2145257ef52ebc76bf89d8e49309f5b, including generalizing the TIT2 frame parsing to work for any T000-TZZZ excl TXXX frame.
  Do you have DRM protected playbacks working in streaming mode for the same content? If not, you should get that working first. If so, how is the DRM initialization data transferred during playback? If you're not seeing `open` called on your `DrmSessionManager` then that indicates that there is no DRM initialization data (e.g. a pssh box for FMP4) present in the media.
 As stated in the message above, I think the problem is that there's no DRM initialization data (e.g. a pssh box) in the media. When you're streaming this isn't a problem because the pssh boxes are being delivered out-of-band in the DASH manifest's ContentProtection elements. There are multiple possible solutions:
1. Use DASH for offline playbacks too. Since your manifest appears to use relative URLs only, this is probably as simple as downloading the DASH manifest alongside the files and then using the exact same code as you do for the streaming case, but pointing to the offline manifest using a file uri. This is probably going to be the simplest approach. It lets you use the same code for both streaming and offline, rather than having an alternative code path requiring `ExtractorSampleSource` for the offline case.
2. Embed the pssh boxes inside the media instead of or in addition to the ones provided in the manifest. This would work fine, although embedding pssh boxes in the media reduces the ease with which they can be modified/changed.
3. We could allow sideloading of `DrmInitData` into `ExtractorSampleSource`. You would have to offline the data from the ContentProtection elements of the manifest, then at playback time build a `DrmInitData` from that data and sideload it into the `ExtractorSampleSource` instances.

I'd suggest you try the first option to start with and see how you get on.
 Interesting. I'll take a look.
 Aside: Regardless of what we discover, it's probably still going to be easier for you to use DASH for offline playbacks. Having one code path is going to be easier to maintain and test than having two ;).
 @andrewlewis - Our mp4 sniffer is failing to detect the video file as fragmented. The condition we're using is that there's a `moof` atom in the first ~4K of the stream, but in this case the first `moof` is quite a bit further than that at ~12K (see capture above). The sniffer's logic for detecting unfragmented mp4 also looks a bit suspicious. Ideally we'd want it to return false for fragmented mp4, but given it's only looking at the first 128 bytes I'm not sure that it'll do that (which means we effectively rely on the extractors being tried in a particular order).

Is there some way we can be more sure when sniffing, to determine for certain whether a file is fragmented or not? Just increasing the sniffing length would "fix" the problem for this media, but doesn't seem like a good solution in general.

@replystreaming - This is a general media type detection issue. When the media is correctly handled as fragmented mp4 we call into the DrmSessionManager as you'd expect.
 @andrewlewis has a fix already that also looks for the `mvex` atom. We'll push it to the dev branch this week.
 This is fixed in dev.
  It would be good to find out why the playback head position is jumping backwards. Please do send a link to the media and details of what platform build you're using on the Shield device. Thanks!
 When seeking we create a new AudioTrack. On this device, the new passthrough AudioTrack's playback head position increases from the old AudioTrack's playback head position for a short time before jumping back to zero, but we expect it to start from zero.

I've followed up with Nvidia, as it seems to be specific to this device.
 Let's leave this open to track the bug, in case we have a fix or clean workaround in ExoPlayer.
 We expect the fix will be included in the next system update.
 That's a question you'd have to direct to nVidia. You may also be able to get some idea or expectation by looking at their historical release cadence.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 This doesn't look right to me. It will cause a new adjuster to be created for every chunk (since every chunk has a different start time). That's not correct behavior.

It's invalid for an HLS live stream to be re-using the same discontinuity sequence numbers over time. It sounds like your streams are omitting EXT-X-DISCONTINUITY-SEQUENCE from the media playlists, which must be included as per the HLS spec if an EXT-X-DISCONTINUITY is ever removed from the media playlist.
 CLAs look good, thanks!

<!-- ok -->
 See comment above.
 That edge case is pretty unlikely to happen, and I don't think it would cause the behaviour you're seeing. I still suspect the source media is broken, since we haven't had other reports of this issue. You'll have to file an issue and provide links to sample content if you want us to do some debugging. It's not viable to discuss how to solve something in a pull request when we don't understand what the problem is ;)...
  - An enabled `MediaCodecVideoTrackRenderer` with no surface will not be able to consume video from the buffer. This will result in buffered video accumulating over time. Eventually, this will cause playback to hang due to the buffer being entirely filled with unconsumed video. So there are fairly fundamental issues with what you're trying to do, in addition to the issue you mention.
- To solve this, you should be disabling the video renderer when the app moves to the background and enabling it again when the app moves back to the foreground. The demo app does this [here](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/DemoPlayer.java#L277). Note that since the `isEnded` checks are only performed on enabled renderers, the video renderer is not then taken into account to determine whether playback has finished.
 We may well do something to make it more obvious/automatic (probably the latter) in 2.x. Out of interest, what type of media are you playing (DASH/SmoothStreaming/HLS/SomethingElse)?
 I also noticed never initializing the videorenderer and just the audiorenderer will trigger the state_ended callback when the activity is paused and the service is running in the background. 

Tested on a Nexus 6p running 6.0.1.
 I'm pretty sure this if fixed in 2.4.0. An enabled MediaCodecVideoTrackRenderer with no surface is now able to consume video, and the ended state should be reported correctly. There are some related issues around seamless switching from no surface to a surface (and from one surface to another) that are largely platform limitations. We'll use https://github.com/google/ExoPlayer/issues/677 to track trying our best to overcome those.  All changes need to go into the dev branch. I've made the change there. It'll be merged to master shortly. Thanks!
  The reasoning is that creating a `DecoderInfo` for a decoder that doesn't exist wont ever work, and you should be able to get a `DecoderInfo` for any decoder that does exist from `MediaCodecUtil.getDecoderInfo` or `MediaCodecUtil.getDecoderInfos`. Do you get the `DecoderInfo` that you need if you call:

```
MediaCodecUtil.getDecoderInfo(MimeTypes.AUDIO_RAW, false);
```

If so then perhaps we should create a `MediaCodecUtil.getPassthroughDecoderInfo` to formalize this, and use that approach for `MediaCodecAudioTrackRenderer` as well rather than using the approach that only works if you're in the same package.
 I think we'll add a `MediaCodecUtil.getPassthroughDecoderInfo` to handle this case.
 Added
  Please include all of the information requested in the issue template. We cannot debug this type of issue with partial information.
 Closing due to lack of information. We'd at least need a proper (and complete) list of affected device model names and builds to debug this further, ideally with separate bug reports (or at least logs of the failure) for each one. The two logs above actually appear to show different failures. Please file a new issue including sufficient information, if you want us to take a more detailed look.  Have you looked around for solutions to the exception ("Trust anchor for certification path not found")? It feels to me like something is wrong with your setup.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Change looks fine but there's something missing with the CLA. If your GitHub account is correctly associated with your corp CLA then googlebot will pick that up. Please take a look and see what's wrong there.
 CLAs look good, thanks!

<!-- ok -->
 This was going into the wrong branch (all changes should go into `dev`) and so has been reverted. I've cherry-picked the changes into `dev`.
  I can't play this in Safari, Chrome or VLC either. I think the stream is just broken. Are you able to play it successfully in any of the above?
  Hmm. I'm not sure about this. I have a 24-bit WAV sample that played fine with the code how it was, and sounds like white noise after this change. Are you certain this change is correct, and have you tested it on some samples of your own?
 Sample: http://download.naimlabel.com/test/30803470-8b73-43fa-a29b-570964d7eef7.aspx
 Yeah, I gave yours a try. I'm pretty sure the sample is little endian. I don't think it would have played correctly before this change if this were not the case. It's possible there's an off-by-one error in the Wav extractor for the 24-bit case, or something like that.
 Right, there's a "pad byte" at the start of the data chunk in WAV, which we're probably not skipping properly.
 Urgh, ignore me, there isn't. It's probably something like that though. I'll investigate!
 NB - There was a bug in WavExtractor that was causing data to be read from the wrong place. Will push a fix shortly. So all is good!
  Without providing test content that reproduces this issue, we can't debug it. Please link to test content here, or email it to `dev.exoplayer@gmail.com`. Thanks.
 Closing due to lack of required information.
  @marcbaechinger - Could you take a look at this?
- Where does the `-8000` come from in VorbisReader.read? Could this be made a named constant or could an inline comment be added for clarification?
- After seeking to `length-8000` readGranuleOfLastPage finds lots of page headers, but none for which the condition to exit the while loop is true. I think the file probably doesn't have "last page" flag set on the last page. It should be trivial to handle this case by breaking out of the loop if the input's position ends up being equal to its length after the body for the previous page has been skipped?
 Yes, that's right. Seems to have no last page. I'll take a look. Thanks.
  This almost certainly has nothing to do with ExoPlayer. You're getting an http 501 response code from the network stack (under ExoPlayer) only when doing something in particular in your app code (above ExoPlayer). You'll have to investigate this by yourself, unfortunately.
  The segments appear to be in ADTS format, but we try to use TsExtractor because their file names end `.ts`. The stream plays if I make HlsChunkSource [use AdtsExtractor](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/hls/HlsChunkSource.java#L485). Try changing the segment file extensions from `.ts` to `.aac`.
  I'm not sure about OkHttpCilent and OkHttp3.CacheControl, but for everything else you should be creating fresh instances (i.e. they should not be shared between players).
 Yes, that message can be safely ignored (I'm not really sure why the platform prints that, but oh well!).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 We really don't want HLS specific stuff propagating through classes like `BaseChunkSampleSourceEventListener`, documented with "if this is HLS". This approach doesn't scale and is generally quite confusing.

It feels like it would be better if `HlsChunkSource` were to have a listener interface for this. You could modify `MediaPlaylistChunk` to stash the raw data, and invoke the listener to pass the data from `HlsChunkSource.onChunkLoadCompleted`.
  As far as I know the UDP protocol used by newer GoPro cameras is proprietary, so I wouldn't expect it to "just work" with ExoPlayer. @ptran-gpfw can probably provide more insight.
  The ExoPlayer library does not provide UI components; you need to build those yourself and have them call in to ExoPlayer (e.g. to start/stop playback when your play/pause buttons are pressed).
  Please provide a full bug report as requested in the issue template.
 Actually, it looks like this is a content issue. Specifically, the absence of `EXT-X-DISCONTINUITY-SEQUENCE` tags in the individual media playlists. These are required for live streams where segments are removed from the media playlists, as per the HLS spec:

> If the server wishes to remove segments from a Media Playlist containing an EXT-X-DISCONTINUITY tag, the Media Playlist MUST contain an EXT-X-DISCONTINUITY-SEQUENCE tag.  Without the EXT-X-DISCONTINUITY-SEQUENCE tag, it can be impossible for a client to locate corresponding segments between Renditions.

Once these are added and your stream is spec compliant, I think you'll find that it works correctly (I was able to validate this by adding some hacks to simulate the tags being present).
  It's probably a bug, but we really need a link to a playable sample to be able to efficiently debug the issue.
 I'm pretty sure it's not the case that WebVTT on live streams is broken in general in ExoPlayer, so we really will need a test stream to take a look at this. We might be able to use a proxy to get around geo-restriction, but either way we definitely need a URL.
 Mail `dev.exoplayer@gmail.com`. Thanks.
 Thanks for the link! This should be fixed on the dev branch by the change ref'd above.
 @ChernyshovYuriy The issue with your approach is that you are trying to parse a m3u8 with a WebvttParser, therefore the error "Expected WEBVTT. Got #EXT3MU". This is unrelated to this thread. It is not a good idea to use a closed issue either, as it could easiliy get ignored. If you still don't manage to solve this, please open a new issue providing all the information requested in the issue template.
  Do you have access to a spec that defines the correct behavior in this case? I mean, "relative timing" and "absolute timing" in, say, WebVTT cues. If you have knowledge of such spec, that would be really helpful.
 Note - There doesn't seem to be anything explicit in the DASH IF guidelines about this. The DASH IF test vectors for subtitles are all single segment only, so that doesn't help either. For the latter, I filed a request for multi-segment test vectors [here](https://github.com/Dash-Industry-Forum/Test-Vectors/issues/44), but if it's not actually well defined what the timestamps should be, it's likely that'll need to be clarified somewhere first!
 As per the response in https://github.com/Dash-Industry-Forum/Test-Vectors/issues/44, the DASH IOP document indicates that for live you're expected to embed WebVTT in ISOBMFF. According to the IOP non-embedded WebVTT should be used in the single segment on-demand case.
  Please provide the information requested in the issue template. We need a link to a specific stream that fails to play. Did you find that none of the sample streams in the demo app played on a Samsung Galaxy S7 running Android 6.0.1?
 Most likely the device doesn't advertise support for the format/resolution/frame-rate combination that you're trying to use, in which case this is working as intended. If you're using DASH you should definitely not be only providing 4K representations. There should be SD and HD representations as well in the same adaptation set. This allows playback to proceed successfully using the subset of representations that are supported by the device.

Note that the UHD samples in the demo app do only include 4K representations, but these are test streams; not an example of what you should actually ship in production to end users.

If you believe there's something else going on, then as above we'll need a link to a specific stream. This is also true if you believe that the device is capable of playing the streams, in which case it's probably a device specific issue where it's under-reporting its own capabilities.
 For what it's worth, I tried the demo app on an S7 running 6.0.1 and all of the clear UHD samples work fine for me. There's a subtle caching problem that can cause these samples to break if you try and and play the secure UHD samples before the clear ones, which will be fixed shortly, but it's unclear whether that's the problem you're running into.
 The subtle caching problem was actually fixed already in dev, by 5f016c9df9631838ffb38e959299346870b517cb.
 Closing because insufficient information was provided. We've fixed one subtle issue related to this, as described above.
  Hey! Thanks for this. Looks pretty good. Could you please:
- Link to some sample content that's 24 bit PCM.
- Make 8 bit PCM work too ;)? It should be very similar. Note that ENCODING_PCM_8BIT is not guaranteed to be supported by all devices. We probably want to attempt to use it, but if constructing the AudioTrack results in IllegalArgumentException then we should re-sample to 16 bit instead.
 Cool; thanks! I've been able to clean this up a little and also enable 8/24-bit PCM coming through WavExtractor. Both seem to work just fine. It should be fairly easy to support different PCM bit depths contained in MP4 as well (just how easy depends on where the bit depth is located in the container).

My cleanup will temporarily strip the 32-bit stuff out again, but will give a good platform to work on top of. No need to adjust this PR. I'll merge it, then merge my cleanup on top, and we can go from there!
 Built on top of in https://github.com/google/ExoPlayer/commit/de1ccea3756f28a74922db3a15c92bd68cde3829.
 - Please send a pull request to fix the byte being dropped. Thanks!
- buffer is used (e.g. through the Util.SDK_INT < 21 code path), so I think it does need to be set to resampledBuffer. buffer.position(offset) is a no-op when the buffer is the resampled one, since through that code path the offset is set to resampledBuffer.position(). The handling of position/limit is pretty confusing in AudioTrack right now. We've cleaned it up quite a bit for 2.x. We hope we'll be able to push an initial version toward the end of next week (or possibly the start of the week after).
  That's a server error (http response code 501). See [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#5xx_Server_Error). It's something that would most likely need to be rectified on the serving side. I'm not sure why the server only responds in this way for the second playback; that's something whoever is providing the server will have to look at.
  Unfortunately we cannot work with a hexdump of your file. We will need the file itself. You can either post it here or send it to dev.exoplayer@gmail.com. However, I can see that the sync byte is not at the beginning of the file, so if you remove the first four bytes (so that the file starts with 0x47), it should work. If this solves your issue, let me know through this issue. If it doesn't, open a new issue including all required information in the issue template.
 Well, considering this addition breaks the 188 packet size restriction, I wouldn't say it is M2TS format. If these four extra bytes appear for every TS packet then no "actual TS parser" will be able to sync, and therefore, to read. 

I am wondering though where you got this file from (considering @Avetri knows about it). On top of that, the timestamp included there is to facilitate access according to what? If it was playback time, I would have expected the first four bytes to include a value near 0. 
 As per the response above, if you're adding data between transport stream packets then it's not a valid transport stream. I'm not sure why you expect this to work?
 I'm not sure what you're trying to show by pasting part of a Wikipedia article into this issue with no accompanying comment. Please clarify?

Also, I think you're misreading the specification. The packet structure is _not_ 4-random-bytes-to-do-what-you-like-with followed by a 188-byte-body. The packet structure is a 4-byte-packet-header followed by 184 bytes of payload. The 4 byte header consists of the fields starting from the sync byte and ending at the continuity counter, as defined in the table above (note that these fields total to 4 bytes in length). The remaining 184 bytes is the payload data (optionally preceded by an adaptation field).
 A link to https://en.wikipedia.org/wiki/.m2ts would have cleared up a lot of confusion on this issue a long time ago... Please can you provide some working sample content in this format to `dev.exoplayer@gmail.com`. The link to sample content above no longer works.
 We need proper sample content to look at this. The patch above is incomplete in that it doesn't implement seeking or duration parsing, both of which should be possible using the extra four bytes in this type of media.
 Until someone provides sample content for this enhancement, we wont be looking at it.
  Please search existing issues before filing a new one, as mentioned in the issue template. This issue looks the same as [this one](https://github.com/google/ExoPlayer/issues/86).
  I don't really understand this change. If a track as a bit depth other than 16, then we should be parsing it from the container and propagating it properly through the format object. The AudioTrack may also need to do re-sampling in the case that the bit depth is not supported. @dcower was planning to look at this at some point.
 As per my response above, the solution to this needs to be more like the one you've used in #1490.
  I think this is custom enough to be outside the scope of this issue tracker, sorry.
  These videos include a non-zero rotation (i.e. the video is recorded horizontally, but the video includes metadata indicating that it needs to be rotated to 90 degrees to make it vertical for playback). Unfortunately, it appears there's an issue in the underlying platform when using `VIDEO_SCALING_MODE_SCALE_TO_FIT_WITH_CROPPING` together with this type of video. I'm not sure there's much we can do to fix it in ExoPlayer. I'll file an issue on the platform team.
 Internal bug ref: 28512795  If you want to be able to seek, you should use a container format that's better designed for seeking (e.g. MP4). Not supporting seeking in containers that don't make seeking easy, such as FLV, is working as intended. See #860 for a little more detail.
  It's probably possible to extend ExoPlayer to do this kind of thing, but it's beyond the scope of the library directly and of this issue tracker.
  I don't think we'll ever provide a way to retrieve the exact frame number. In some types of media it's not possible to determine the exact frame number. It's more sensible to try and retrieve the exact frame timestamp. There's already a solution explaining how to do that on the StackOverflow question you're linking to, however, so why not just do that?
  Is playback actually failing? We already retry a number of times to handle this case, and I don't have any issues in the demo app switching between WiFi and mobile networks. If you want to retry harder, you can specify a higher-than-default value for `minLoadableRetryCount` when instantiating each `ChunkSampleSource` instance.
 Right, increasing `minLoadableRetryCount` should fix it in that case. The policy is that we'll retry for as long as we have buffer to be playing, and once the buffer has run out we'll fail only if we've also retried at least `minLoadableRetryCount` times. Note that this means failure is less likely if more of the media is buffered, since you'll most likely be able to fit more retries in before the buffer runs out.

As a final note, increasing `minLoadableRetryCount` will increase the time that it takes for legitimate failures to be propagated to the user (the player will appear as though it's buffering until it gives up in such cases). So you probably don't want to increase it too much.
  There are two things to mention: The TTML subtitles have an incorrect mime type, we use "application/ttml+xml" for regular ttml, like the one in the provided manifest. The adaptation set uses application/mp4, which we use for a spec that allows ttml embedded in mp4 files. For this reason, the player tries to parse the ttml file as it was an mp4 file. Which results in parsing errors. Leaving this aside...

The WebVTT cues include absolute cue timing, instead of relative timing, which we currently correctly support. **If you want a quick solution**, you can change the cue timing so that if a webVTT file corresponds to the period [x, x + segment_length), the cue starting time will state something of the form:

```
absolute_start_time - x --> absolute_end_time - x
cue text
```

This means you should basically subtract the starting time of each segment to every cue timestamp. So, if you have a cue like

```
0:07:00.000 --> 0:07:15.000
Hello
```

In a WebVTT file that starts at the minute 7 of the playback, it would become:

```
0:00:00.000 --> 0:00:15.000
Hello
```

Concerning absolute timing, we are still deciding what we will do here because we haven't found a specification that addresses this (feel free to point us to one, if available). We are tracking this in [#1493](https://github.com/google/ExoPlayer/issues/1493), so please follow it to see how this turns out.
  I'm confused by this statement. There's only one call to `onTransferEnd` in `OkHttpDataSource`, and it's [here](https://github.com/google/ExoPlayer/blob/master/extensions/okhttp/src/main/java/com/google/android/exoplayer/ext/okhttp/OkHttpDataSource.java#L216). Furthermore, `closeConnectionQuietly` does not call `close` on the instance. Please clarify?
 - `closeConnectionQuietly` doesn't do anything that would be externally visible outside of the class/instance. I'm unclear why you're suspicious of that method really. It only contains two lines of code. Two of them just null variables, and the other is calling close on an internal object?
- The same `TransferListener` can be shared by multiple `DataSource` instances. Depending on what media you're playing, it may be that there are multiple requests being made. I'm really not sure what the issue is that you're reporting here.
 `DataSource` instances are not single use. It's valid for a `DataSource` to be used for more than one request.
 Your assumption that playing a piece of media maps onto a single network request is incorrect. Some media playbacks will require many (typically bounded) network requests. Each one of them counts as a transfer. Hence this is working as intended.

Depending on exactly what you're playing, it's either necessary to make two requests due to the nature of the media, or it's an inefficiency in ExoPlayer (note: it's not "broken" as such and neither is it doing anything that the player is not legitimately allowed to do; it's just not as efficient as it could be). The main cause of the latter is tracked by #1041.
  - Yes, HLS will use FormatEvaluator in V2.
- I suspect there isn't a really good reason behind the chosen values being different, however it should be noted that there are differences between DASH/SS and HLS that could require legitimately different fractions. Specifically (1) In DASH/SS audio is demuxed and will require some bandwidth of its own, where-as in HLS the audio is typically, but not always, muxed with the video (2) For HLS it's necessary to download overlapping chunks when switching qualities, where-as for DASH/SS this is not required.
 I don't think they've been fine tuned, if that's your question. In reality the optimal number is dependent on the serving side as well as the client. For example the cost of an edge cache miss on the serving side likely affects what the optimal number should be. So it's unlikely that a single best value exists, and if you have the time and the urge it's worth doing your own studies to try and find the best value for your specific use case.
  - For de-muxed audio and webvtt specifically, it should be pretty easy to do better (i.e. assume AAC or WebVTT respectively if the file extension is missing). It's trivial to explicitly pass `HlsChunkSource` a type indicating whether it's being configured for one of these two cases.
- Another option would be to sniff the media format, but I'm not sure we really want to do that; it seems fairly heavy for this case.
- Note that there's nothing stopping a service from always putting a file extension on the end of the URL, whether it's being generated dynamically or not. It seems preferable that a service should do this, since it makes the client side logic significantly simpler.
 Marking as a bug to resolve the first bullet point, although we probably wont do it until we release 2.x. I would suggest reaching out to the stream provider to request that they append file extensions to their URLs as well.
 Fixed for WebVtt segments in: https://github.com/google/ExoPlayer/commit/497651c7b9b70ea4d84568506b88a9c682845fe6. https://github.com/google/ExoPlayer/issues/2025 tracks fixing this issue for other segment types.  This is not a proper issue report. Please read the ISSUE_TEMPLATE file and provide every corresponding item. On top of this, please see [#426](https://github.com/google/ExoPlayer/issues/426).
  No, there's no built in support for that. Although you could easily implement your own `TrackRenderer` and inject it into the player that could dispatch events at specified times during playback.
 The equivalent would be implementing your own Renderer (subclassing BaseRenderer) and using `positionUs` passed to [`render`](https://github.com/google/ExoPlayer/blob/ce55d1a/library/core/src/main/java/com/google/android/exoplayer2/Renderer.java#L208). Providing a more convenient API for this is also tracked by https://github.com/google/ExoPlayer/issues/2189.  You can set the volume by sending [MSG_SET_VOLUME](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecAudioTrackRenderer.java#L78) to the MediaCodecAudioTrackRenderer like this: `player.sendMessage(audioRenderer, MediaCodecAudioTrackRenderer.MSG_SET_VOLUME, volume);`, where volume is a Float between 0 and 1.
  @zsmatyas - Yes, sorry about the delay with the PR. We're pretty busy working on some major changes for a 2.x release right now.
 Have you managed to reproduce this issue recently? It should be fixed by now, I think. Do you still have HLS streams with multiple CC services? If so, please send us a link. We are currently working on support for this. I'll be closing for now. Please open a new issue if you run into any issues with 608/708 issues.  What's wrong with `ExoPlayer.getCurrentPosition` and `ExoPlayer.getDuration`?
  Can you check if the device is secure boot?
To do, boot into 'fast boot', so adb reboot or power on with up & down vol button held.

You should see 'secure boot enabled'. 
 If the device is not secure booted this behavior is expected.
 There are various failures that will lead to a  : 'Illegal state exception: Failed to handle key response: DRM vendor-defined error: -2902 (-2902)'

The original bugreport for this case the error right before is : ' WVCdm   : CryptoSession::LoadKeys: OEMCrypto_LoadKeys error=10012'. 

Following up with the OEM out of band.
 no new update from the OEMs. 
 @wvpaf - Did we ever hear back from the OEM? If not, please add the "wont fix" label and close, since I don't think there's anything we can do to fix this in ExoPlayer. We have not. Sorry.  I'm sorry, but this is an issue tracker; it's not the right place to ask general how-to / tutorial questions, particularly when the scope of the question is outside of the immediate scope of the project. This has already been answered in #1440. If you're still struggling, StackOverflow would be a more suitable place to ask your questions.
  General Q&A questions should be asked on StackOverflow, not our issue tracker. Thanks.
  General Q&A questions should be asked on StackOverflow, not our issue tracker. Thanks.
  The HLS playlists you've linked to only contain 720p and 1080p streams. HD resolutions are beyond the capabilities of low end (or older) devices. It's also a massive waste of bandwidth and battery to be streaming HD video to devices that have low resolution screens. The Sony Xperia Mini Pro has a screen resolution of 320x480, so sending it 720p is sending it 6 times as many pixels as the physical display can display.

The video isn't played because the device reports that its not capable to play it. The solution is to provide lower resolution variants in your HLS master playlist, so that one appropriate to the device can be selected for playback. ExoPlayer should also report a clearer error in this case, which is tracked by https://github.com/google/ExoPlayer/issues/1545.
 @rajeshgoswami2025 @makvasic , when you need to report a potential bug, please open a new issue. If you think the information in another issue is useful, just provide a link to it. Otherwise it is possible that it will get overlooked. Keep this in mind. 
On top of this:
@rajeshgoswami2025 : Unfortunately I cannot reproduce the issue in a device. Did you manage to achieve this? Emulators are not usually great for this. Please file a new issue with some more context (I will add some more related information below).
@makvasic : This usually happens because the declared resolution has an odd element. In this case, the manifest contains `RESOLUTION=1047x576`. As @ojw28 pointed out, it is the manifest that is malformed. The actual video is 720x576, so if you ignore that part of the manifest (or correct it, if you have access to the serving of the content), it will play correctly. 

I think both issues are related to renderer capabilities, that is why it is device dependent. I would suggest you start giving it a try to the version 2 that is a bit smarter in this aspect. If you still haven't solved your problems, please file a new issue.
  `ExoPlayer.Listener` doesn't define an `onError(Exception e)` method, it defines `onPlayerError(ExoPlaybackException)`, so I'm a little unsure as to what question you're asking.

In general though, `onPlayerError(ExoPlaybackException)` will only be called for errors where the player has failed to recover internally, or doesn't think that it will be able to. So you shouldn't retry automatically in any case when this method is invoked. It's more appropriate to display a retry button to the user, to allow them to retry manually if they wish to do so.
 - It can make sense to automatically retry for `BehindLiveWindowException` in the case that you know it's likely to occur with the source media due to the live window being too small. You'd probably want to put some kind of limit on it though; something like only allowing the automatic retry once every 30 seconds or something, so as to make sure you don't end up in an infinite cycle of retrying where the source media is completely broken.
- The last of the stack traces you posted (buffer too small) is definitely not a recoverable exception. It's a bug where the decoder input buffer isn't large enough. Attempting to play the same piece of media again will result in exactly the same failure. You should file a separate issue for that, containing all information requested in the issue template (including a full bug report).
- The first of the stack traces you posted should not really be considered recoverable. If that's happening mid-playback, it might be a device issue or an issue with the source media. It's not possible to tell with the information provided.
  Use of distinct keys is not incompatible with having a single key request per session. What's supposed to happen is this:
1. Player extracts pssh from arbitrary stream.
2. Player makes single key request with pssh data.
3. License server responds with _all_ of the keys that the user is entitled to for that content. So specifically, the response would include keys to play both the audio and video streams in this case.

The same approach is used today by many providers who use distinct keys for SD v.s. HD streams.
 Note also that alternative approaches involving separate key requests are less efficient (more network requests) and more likely to result in playback failure (more chances to fail).
 - I would argue that you should be implementing your key management system to return all keys when handling Widevine requests regardless, simply because it's more efficient and a better way of doing things. I doubt it's much work at all on the license server side to do this (i.e. if there's a content_id then use it and include the extra keys, if there isn't then don't). In which case a small amount of additional complexity seems well worth it. The generic version could require maintaining three DRM sessions simultaneously (SD, HD, Audio). In the case that PSSH data isn't in the manifest the generic case also requires either loading init chunks from all representations up front at the start of playback, or accepting that the first SD<->HD transition may not be seamless. Neither of these is ideal.
- For the specific case of using different keys for audio and video, you can actually trivially achieve this already by using separate `StreamingDrmSessionManager`s for the audio and video renderers. Currently `DashRendererBuilder` in the demo app shares a single instance between both renderers, but you could create a separate instance for each to get the separate sessions. As above, I'd recommend you don't do this though (and it wont solve the SD/HD video issue). 
  Can you reproduce the behavior in the demo app? If so, please provide the URL that stops playing and we can take a look.
 Note: As mentioned in #1459 (where you posted earlier), to play HLS properly you would need to use HlsSampleSource, not ExtractorSampleSource. Read the code for the demo app to see how to do this.
 I suggest you try asking on Stack Overflow or similar. We can't debug app-specific issues on this tracker I'm afraid. Perhaps a good approach would be to add the (known working) code from the demo app to your app, and then incrementally modify it to match your existing player interface.
  - I _think_ it's possible for NotProvisionedException to be thrown on both L1 and L3 devices. I don't fully understand the provisioning process though.
- As an aside, note that you can force an L1 device to use L3 by calling `mediaDrm.setPropertyString("securityLevel", "L3");` immediately after the `MediaDrm` is instantiated. This might be useful for your testing. It's obviously not possible to force an L3 device to use L1 :).
- On a rooted device, I believe it's possible to erase provisioning as follows:

```
adb root
adb remount
adb shell rm -r /data/mediadrm/IDM1013
```
  ExtractorSampleSource does not read m3u8 files, which is why the exception is thrown.

The URL seems to be on your private network, but you probably need to use an HlsSampleSource instead, as in [HlsRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java).
 @brdn I suggest using the demo app as a starting point, and making sure you can play your streams there. If not, please file a new issue and include the information requested in the template. We can only address issues that relate directly to ExoPlayer on this issue tracker I'm afraid.

@brucetoo The chunks are being fetched with a base URI before redirection. I will take a look at fixing this.
 Fixed in dev.
  This is a general programming question, not something that's specifically ExoPlayer related. There are several ways of avoiding an infinite loop, for example by posting a message to a different thread (or better) posting a message back to the same thread to be handled at a later time. I suggest taking a look at [Handlers](http://developer.android.com/reference/android/os/Handler.html).

Please ask general questions on StackOverflow rather than the issue tracker. Thanks!
  **A description of the issue.**
Exoplayer (r1.5.7) cannot player a Widevine asset, which can be played on shaka player v1.6.5.

**Steps describing how the issue can be reproduced, ideally in the ExoPlayer demo app.**
1. Host a Widevine encrypted DASH asset on a webserver.
2. Enter the DASH URL at Exoplayer Demo app at Android Studio 1.5.1.
3. Connect Nexus 10 (Android 5.1.1) to the PC on which the Android Studio runs.
4. Click "Run" on Android Studio and choose Nexus 10.
5. The Exoplayer demo app is started on Nexus 10 but the video cannot be played.

**A link to content that reproduces the issue.**
The tested Widevine encrypted DASH asset can be downloaded from the following FTP server.
FTP server: ftp.harmonicinc.com
filename: dash_widevine.zip
username: hlit8448
password: 5jH62BU2

**The version of ExoPlayer being used.**
r1.5.7

**The device(s) and version(s) of Android on which the issue can be reproduced, and how easily it reproduces**
Device: Nexus 10
Android version: 5.1.1
Reproducible:  Always

**A bug report taken from the device just after the issue occurs, attached as a file. A bug report can be captured using "adb bugreport".**
The bug report can be found from the following FTP server.
FTP server: ftp.harmonicinc.com
filename: bugreport_exoplayer.txt
username: hlit8448
password: 5jH62BU2
 Hi Jefftinker,
Thanks for your response. We will check the license and post updates here.
Thanks,
 Hi hoimingw@, 

Any update? Else we will assume policy settings addressed issue and will close in the next 14 days ... or so.
 @Mark86NL  as for documentation if you are part of the CWIP program there are documents there that will describe the security levels.  

However to reiterate if L3, least secure, playback is chosen then HDCP cannot be initiated. 
 Closing as the path to documentation was provided above.
Please reopen if the main question was not addressed.
  I was trying to player a Widevine encrypted asset on Exoplayer but failed, please find the details below.

Exoplayer version: 1.5.7
Android device: Nexus 10
1. It can be played on Shaka player 1.6.5
2. The Exoplayer 1.5.7 can play the sample Widevine URL in the demo app.

The tested Widevine encrypted DASH asset can be downloaded from the following FTP server.
filename: dash_widevine.zip
username: hlit8448
password: 5jH62BU2

Please note that only the first few segments for each stream were uploaded. Please let me know if you want full content.

Console output of the emulator:
```
04-21 18:46:20.822 2495-2495/? I/art: Late-enabling -Xcheck:jni
04-21 18:46:20.954 2495-2521/com.google.android.exoplayer.demo D/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: true
04-21 18:46:20.959 2495-2495/com.google.android.exoplayer.demo D/Atlas: Validating map...
04-21 18:46:21.023 2495-2521/com.google.android.exoplayer.demo I/OpenGLRenderer: Initialized EGL, version 1.4
04-21 18:46:21.040 2495-2521/com.google.android.exoplayer.demo D/OpenGLRenderer: Enabling debug mode 0
04-21 18:46:21.044 2495-2521/com.google.android.exoplayer.demo D/mali_winsys: new_window_surface returns 0x3000
04-21 18:46:21.589 2495-2495/com.google.android.exoplayer.demo I/Choreographer: Skipped 30 frames!  The application may be doing too much work on its main thread.
04-21 18:46:24.032 2495-2521/com.google.android.exoplayer.demo D/mali_winsys: new_window_surface returns 0x3000
04-21 18:46:25.607 2495-2495/com.google.android.exoplayer.demo I/ExoPlayerImpl: Init 1.5.7
04-21 18:46:25.615 2495-2495/com.google.android.exoplayer.demo D/EventLogger: start [0]
04-21 18:46:25.615 2495-2495/com.google.android.exoplayer.demo D/EventLogger: state [0.00, false, P]
04-21 18:46:25.625 2495-2495/com.google.android.exoplayer.demo D/EventLogger: state [0.01, true, P]
04-21 18:46:25.683 2495-2521/com.google.android.exoplayer.demo D/mali_winsys: new_window_surface returns 0x3000
04-21 18:46:25.908 2495-2613/com.google.android.exoplayer.demo W/VideoCapabilities: Unrecognized profile/level 32768/2 for video/mp4v-es
04-21 18:46:25.935 2495-2613/com.google.android.exoplayer.demo W/VideoCapabilities: Unrecognized profile/level 32768/2 for video/mp4v-es
04-21 18:46:25.968 2495-2613/com.google.android.exoplayer.demo I/VideoCapabilities: Unsupported profile 4 for video/mp4v-es
04-21 18:46:25.987 2495-2495/com.google.android.exoplayer.demo D/EventLogger: availableRange [true, 0, 647018666]
04-21 18:46:25.998 2495-2495/com.google.android.exoplayer.demo D/EventLogger: state [0.38, true, B]
04-21 18:46:26.013 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [0.40, 853, 0.00, 3412000]
04-21 18:46:26.019 2495-2495/com.google.android.exoplayer.demo D/EventLogger: audioFormat [0.40, stream_1, 2]
04-21 18:46:26.097 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [0.48, 37479, 0.04, 6814363]
04-21 18:46:26.689 2495-2638/com.google.android.exoplayer.demo I/OMXClient: Using client-side OMX mux.
04-21 18:46:26.736 2495-2495/com.google.android.exoplayer.demo D/EventLogger: decoderInitialized [1.12, OMX.google.aac.decoder]
04-21 18:46:26.742 2495-2495/com.google.android.exoplayer.demo D/EventLogger: state [1.13, true, R]
04-21 18:46:26.753 2495-2495/com.google.android.exoplayer.demo D/EventLogger: state [1.14, true, B]
04-21 18:46:26.778 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.16, 25861, 0.01, 6814363]
04-21 18:46:26.826 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.21, 25978, 0.02, 12989000]
04-21 18:46:26.880 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.26, 24395, 0.01, 12989000]
04-21 18:46:26.925 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.31, 25949, 0.01, 13839466]
04-21 18:46:26.970 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.36, 24039, 0.02, 12989000]
04-21 18:46:27.031 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.42, 26002, 0.02, 12989000]
04-21 18:46:27.081 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.47, 25918, 0.01, 12989000]
04-21 18:46:27.130 2495-2495/com.google.android.exoplayer.demo D/EventLogger: bandwidth [1.52, 24294, 0.02, 12989000]
04-21 18:46:27.850 2495-2613/com.google.android.exoplayer.demo E/ExoPlayerImplInternal: Internal track renderer error.
                                                                                        com.google.android.exoplayer.ExoPlaybackException: android.media.MediaCodec$CryptoException: Unknown Error
                                                                                            at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:704)
                                                                                            at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:513)
                                                                                            at com.google.android.exoplayer.SampleSourceTrackRenderer.doSomeWork(SampleSourceTrackRenderer.java:129)
                                                                                            at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:434)
                                                                                            at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:213)
                                                                                            at android.os.Handler.dispatchMessage(Handler.java:98)
                                                                                            at android.os.Looper.loop(Looper.java:135)
                                                                                            at android.os.HandlerThread.run(HandlerThread.java:61)
                                                                                            at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
                                                                                         Caused by: android.media.MediaCodec$CryptoException: Unknown Error
                                                                                            at android.media.MediaCodec.native_queueSecureInputBuffer(Native Method)
                                                                                            at android.media.MediaCodec.queueSecureInputBuffer(MediaCodec.java:956)
                                                                                            at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:694)
                                                                                            at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:513) 
                                                                                            at com.google.android.exoplayer.SampleSourceTrackRenderer.doSomeWork(SampleSourceTrackRenderer.java:129) 
                                                                                            at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:434) 
                                                                                            at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:213) 
                                                                                            at android.os.Handler.dispatchMessage(Handler.java:98) 
                                                                                            at android.os.Looper.loop(Looper.java:135) 
                                                                                            at android.os.HandlerThread.run(HandlerThread.java:61) 
                                                                                            at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40) 
04-21 18:46:27.850 2495-2495/com.google.android.exoplayer.demo E/EventLogger: internalError [2.23, cryptoError]
                                                                              android.media.MediaCodec$CryptoException: Unknown Error
                                                                                  at android.media.MediaCodec.native_queueSecureInputBuffer(Native Method)
                                                                                  at android.media.MediaCodec.queueSecureInputBuffer(MediaCodec.java:956)
                                                                                  at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:694)
                                                                                  at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:513)
                                                                                  at com.google.android.exoplayer.SampleSourceTrackRenderer.doSomeWork(SampleSourceTrackRenderer.java:129)
                                                                                  at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:434)
                                                                                  at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:213)
                                                                                  at android.os.Handler.dispatchMessage(Handler.java:98)
                                                                                  at android.os.Looper.loop(Looper.java:135)
                                                                                  at android.os.HandlerThread.run(HandlerThread.java:61)
                                                                                  at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
04-21 18:46:27.859 2495-2495/com.google.android.exoplayer.demo E/EventLogger: playerFailed [2.24]
                                                                              com.google.android.exoplayer.ExoPlaybackException: android.media.MediaCodec$CryptoException: Unknown Error
                                                                                  at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:704)
                                                                                  at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:513)
                                                                                  at com.google.android.exoplayer.SampleSourceTrackRenderer.doSomeWork(SampleSourceTrackRenderer.java:129)
                                                                                  at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:434)
                                                                                  at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:213)
                                                                                  at android.os.Handler.dispatchMessage(Handler.java:98)
                                                                                  at android.os.Looper.loop(Looper.java:135)
                                                                                  at android.os.HandlerThread.run(HandlerThread.java:61)
                                                                                  at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
                                                                               Caused by: android.media.MediaCodec$CryptoException: Unknown Error
                                                                                  at android.media.MediaCodec.native_queueSecureInputBuffer(Native Method)
                                                                                  at android.media.MediaCodec.queueSecureInputBuffer(MediaCodec.java:956)
                                                                                  at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:694)
                                                                                  at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:513) 
                                                                                  at com.google.android.exoplayer.SampleSourceTrackRenderer.doSomeWork(SampleSourceTrackRenderer.java:129) 
                                                                                  at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:434) 
                                                                                  at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:213) 
                                                                                  at android.os.Handler.dispatchMessage(Handler.java:98) 
                                                                                  at android.os.Looper.loop(Looper.java:135) 
                                                                                  at android.os.HandlerThread.run(HandlerThread.java:61) 
                                                                                  at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40) 
04-21 18:46:27.894 2495-2521/com.google.android.exoplayer.demo D/mali_winsys: new_window_surface returns 0x3000
04-21 18:46:27.904 2495-2495/com.google.android.exoplayer.demo D/EventLogger: state [2.29, true, I]
``` _Please_ provide all of the information requested in the issue template. Specifically, we require a full bug report. Please file a new issue with complete information. Thanks.
  ExoPlayer correctly plays local file URIs of the form `file:///path/to/media/file.ext`. Beyond that, it's unclear what this issue is asking.
  You're most likely trying to access surface after it's been destroyed. The ExoPlayer demo app does not suffer from these problems. I suggest you look at how it works. Specifically, look at how `PlayerActivity` implements `SurfaceHolder.Callback`, to ensure that the player doesn't try and use a surface after `surfaceDestroyed` is invoked.
  Are you calling `new Handler()` on a thread that has a Looper, like the main thread (see [Handler()](http://developer.android.com/reference/android/os/Handler.html#Handler%28%29))? In the demo app, the renderers are built on the main thread. What error do you see?
 I'm not sure exactly what error you are referring to, so could you paste the exception here?

As a separate issue, DataSources can only be opened by one thing at a time, so you will need to have a separate one for the second ExtractorSampleSource.
  Which version of exoplayer are you using? I can only point you to possibly related issues. But without the media, there is not much I can do. There was a bug related to this when the application of the edit list resulted in a "sample window" without any sync sample. You can also try looking it up in the issues.
 Acording to [#1336](https://github.com/google/ExoPlayer/issues/1336), 1.5.6 presented this issue I speak of. Please update the exoplayer version and see if the exception changes. If it does, you have met an Exoplayer limitation, unfortunately. The new version will provide better error description, at least.
 If the exception does not change, file a new issue including all the useful information. It would be really useful if you could provide a way to reproduce the crash.
 You updated the ExoPlayer version but the error you are getting is still `ArrayIndexOutOfBoundsException`? Are you sure that the client report is not in the older version? If this is not the case, I will not be able to do anything without a sample to reproduce the issue. From the stack trace, the crash looks exactly like the one I spoke before, but unfortunately I cannot explore every possible input for which this exception could be thrown. There are two possible ways to go:
1. Look out for an exception with message: `The edited sample sequence does not contain a sync sample`.
2. Get a media sample that reproduces the issue and file a new issue.
  Can you check if the device is secure boot?
To do, boot into 'fast boot', so adb reboot or power on with up & down vol button held.

You should see 'secure boot enabled'.

if the device is not in secure boot this behavior is expected
 This issue reports a similar to "google/ExoPlayer Failed to handle key response: DRM vendor-defined error
#1468".
  Hm. I managed to hack around and make this style of mpd work, but I'm not clear on whether the mpd is in violation of the DASH spec, or whether ExoPlayer is. The DASH spec is quite nuanced/complicated/non-intuitive when it comes to assigning start times and durations to segments in this kind of manifest, so it'll take a bit of thought.

As an aside, you should probably use the templated style of manifest regardless, since they're generally more concise.
 Marking as bug for now, although this is really pending further investigation.
 Closing for now, since it'll take us quite a long time to figure it out vs. what the spec defines. Please re-open if the combination of Wowza + DashIF determine it to be valid. Given Wowza seem to have issues with compatibility in other areas too, I'm suspecting it's invalid.
  `MediaCodecAudioTrackRenderer.processOutputBuffer` feeds each output buffer into an `AudioTrack`, but note that an `AudioTrack` has its own audio buffer (at the platform level). So if that buffer is being kept full there will be a delay between an output buffer being processed and the corresponding audio actually being output from the device.

To do what you're wanting to do you would probably need to generate the data that you need for your visualization from each output buffer, then add that data to the back of a queue with the associated timestamp `bufferInfo.presentationTimeUs`. You could then pull items off the front of the queue and render them as `MediaCodecAudioTrackRenderer.getPositionUs()` reaches the associated timestamps.
  The issue is that the first playlist included does not contain ts files but rather aac (audio only files). There are two things I suggest you to try(particularly important is the second one):
1. Try not to list the audio-only playlist first. As a caveat, It is possible that if the player tries to adapt to that option, it will break.
2. And more importantly, see https://tools.ietf.org/html/draft-pantos-http-live-streaming-13. The CODECS attribute **should** be included in the tags, which means you should have a good reason not to include it. This way, exoplayer would handle the audio only playlist itself.
  You can provide the link through dev.exoplayer@gmail.com. After that, let us know here so that we can go look for it.
 I will keep you posted. It may take me some time to work this out, but feel free to ping me if it seems I forgot about it. 
 I still haven't been able to reproduce the issue. If you want to work around it, perhaps [#911](https://github.com/google/ExoPlayer/issues/911) will provide some aid, just in case. I will try to reproduce again this week, but if I don't manage to, I will have to close this thread. If you have any extra information that may aid, please provide.
 I haven't been able to reproduce the issue even after 12+ hours playbacks. Unfortunately, I will have to close this. If you manage to limit the scope of the issue to a certain device or certain circumstances, file a new one. Perhaps the issue was related to some connection/server side problem, but if you say that it does not manage to recover, I can't think of any good diagnosis.
 It is a good hint for anyone who encounters the same issue in that device. Unfortunately is not a workaround we can add to the library, considering the specificity of the circumstances under which the problem occurs and that the workaround is not targeting the cause of the problem itself, but rather a symptom. It is unlucky that I do not count with a testing device right now. If you dig up some more information about what the cause is, let me know. Cheers.
  Can you please provide media to reproduce this issue? You can either post it here or send it to dev.exoplayer@gmail.com and then post here so that I look for it.
 Hi, thanks to the invaluable help of @andrewlewis, we have come to the conclusion that the software you are using to add the thumbnail is faulty. Long story short, the file has an invalid prefix of ~55KB which is a copy of the real ID3 info (that contains the thumbnail). If you remove this prefix, you will get an MP3 file that contains the thumbnail and is playable. Due to this error, the length of the first ID3 header ends up being too far away from the first MP3 frame header, and so the MP3 sniffer gives up before finding it.

By the way, if you could add some information regarding how you added the thumbnail, that would be great.
  If I take the demo app and enter these smaples in `Samples.java` then they play back fine, seeking included. Please describe exactly how to reproduce the issue in ExoPlayer's demo app, and provide a full bug report as requested in the issue template, or we will be unable to investigate further.
 Closing due to insufficient information.
 @malvin91, the link you include plays well in the demo app. 

Add your link to the demo app and go from there. If you need to report an issue, please don't use a closed issue. And please don't forget to include all the information requested in the issue template. Otherwise it will probably be closed automatically.
 Please try it in the demo app, first. It plays well for me.  It's likely a problem similar to the one we workaround for some decoders with `MediaCodecTrackRenderer.codecNeedsEosPropagationWorkaround`. Most likely adding the problematic codec to this workaround will resolve the issue. We can't debug further without a full bug report (as requested in the issue template).
 Thanks! You'll probably find that modifying MediaCodecTrackRenderer's codecNeedsEosPropagationWorkaround method to look as follows resolves the issue:

```
private static boolean codecNeedsEosPropagationWorkaround(String name) {
 return Util.SDK_INT <= 17 && ("OMX.rk.video_decoder.avc".equals(name)
    || "OMX.allwinner.video.decoder.avc".equals(name));
}
```

Please give that a try and let us know what you find!
 I'll make a change to our dev branch to include this workaround.
  We support AES-128 with HLS in ExoPlayer. We do not currently support DRM schemes like PlayReady or Widevine with HLS. Widevine with DASH is the recommended solution for delivering DRM protected content to Android devices. PlayReady can also be used if you're targeting AndroidTV devices.
  Thanks for this! This is something we've fixed already in our (currently still internal) 2.x branch. We're trying to minimize changes into 1.x at this stage, so I'm not sure whether we'll merge this. Will leave it open for now though.
 See comment above.
  Please see https://github.com/google/ExoPlayer/issues/1220.
  I cannot reproduce the issue. It plays fine a 10 seconds video. The log does not include any warnings either. Are you sure you posted the correct media? I will see if I can find what is wrong in the meantime.
 The problem here is that some of the segments do not contain any audio samples. For example:

```
http://d12b7ev0vur5ia.cloudfront.net/12414515055c2b33d0c98ca4a4cd4125f84f3bb902fc6f0ae597b01489752e_high_1.ts
```

where-as it should contain ~3.6 seconds worth of audio, which is the segment's duration. I'm not sure why this is, but there should be audio data in these samples, even if it's just silence.
  Hmm. Are you passing non-default values when constructing the `DefaultLoadControl`, or are you just using the constructor that only takes an `Allocator`? Have you tweaked any other values, such as the buffer segment size that's being used?
 If possible, please let us know how we can reproduce this in the ExoPlayer demo app.
 Closing due to lack of a sample that reproduces the issue.
  Please provide a proper bug report, as clearly requested in the issue template:

> A bug report taken from the device just after the issue occurs, attached as a file. A bug report can be captured using "adb bugreport". Output from "adb logcat" or a log snippet is not sufficient.
 We still don't have enough information to look into this. Your latest issue seems audio-related but your first comment was about DRM. For diagnosing DRM-related issues a full bug report (as requested above) is useful.

I am closing this issue because it is unclear what the bug is or how to reproduce the problematic behavior. Please file a new issue which includes all the information requested on the "New issue" page. Thanks!
  Please could you provide a URL to sample media that reproduces this error when played in the ExoPlayer demo app?
 For the MP3 case, could you try using .mp3 as the chunk file extension instead of .ts? The chunks seem to play correctly when using Mp3Extractor (see [HlsChunkSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/hls/HlsChunkSource.java#L492)).
 The media segment URIs have a last path segment of ".aspx". This isn't recognized so we default to treating them as MPEG TSs rather than MP3s, which means their contents are parsed incorrectly.

The easiest fix for now is to make the media segments end ".mp3". At some point we may determine which extractor to use based on the contents of the segments (like how ExtractorSampleSource's sniffs the container).

By the way, it looks like you might be using Azure to generate the streams. If so, we'd advise using DASH or Smooth Streaming instead of HLS, in which case you won't encounter this problem.
  I'm not particularly knowledgeable about how this works. Would said components need to contain the native libraries that they depend on? If so then I think we'd probably want to avoid doing that; since we'd effectively be distributing other projects in our components (which may have different licenses etc). @vigneshvg might know more. If not then it's something we should consider.
 Including the libvpx (vp9 decoder) native library would be okay from a licensing point of view. (and i think it's probably okay for Opus too).

That said, i have no idea how native libraries are distributed via maven.
 I think we're unlikely to do this. The above discussion aside, requiring the extensions should (hopefully!) be for fairly niche or advanced cases. There's an argument for "not making it too easy", since platform decoders should be preferred where feasible.
 Current status is as follows.

- Already distributed: OkHttp, GVR
- Can probably distribute: Cronet, VP9
- Unsure: FLAC, Opus
- Probably wont distribute: FFMPEG, as explained in https://github.com/google/ExoPlayer/issues/2781. I'd really be glad about flac\ opus. Building and maintaining  them was kind of a pain. In my case I had to make sure my project contains no binary files so fdroid can build them.

What makes them status unsure? If I remember correctly, the VP9 extension alone is larger than the library (without extensions). They're definitely non-trivial in terms of size. Some developers are extremely sensitive to library size, particularly those targeting emerging markets, which is why we've actually been going in the opposite direction (i.e. splitting the core library into multiple individual modules).

Reasoning for not distributing the FFmpeg extension is already provided in https://github.com/google/ExoPlayer/issues/2781. As already stated in this issue, we're considering distributing the Cronet, VP9, FLAC and Opus extensions. It's not high priority for us though. Usage of these extensions is very low vs usage of the library.  Same as [#1263](https://github.com/google/ExoPlayer/issues/1263).
  If you want us to look at your issue, please provide the information requested in the issue template. The issue as it stands ignores every single bullet point, including the last one that explicitly states: **Output from "adb logcat" or a log snippet is not sufficient.**. Please file a new issue containing complete information.
  Same as [#605](https://github.com/google/ExoPlayer/issues/605).
 Sorry, I could have clarified and save you some reading. Cheers!
  Closing due to insufficient information.
  I don't think I really understand what the request is here. Please clarify?

To address the final point, I'm pretty sure `VIDEO_SCALING_MODE_SCALE_TO_FIT_WITH_CROPPING` works correctly. Note that if you're using `AspectRatioFrameLayout` or similar then the scaling mode isn't going to make a difference; the difference will only be apparent if the `SurfaceView` has a different aspect ratio to the video. Note also that burnt in letter-boxing (i.e. black bars that are actually part of the video) count as part of the video w.r.t scaling modes.
 Nope, you didn't miss something. It appears that `setVideoScalingMode` doesn't work from the place where we're calling it. It should work, as far as I can tell by looking at the `MediaCodec` documentation. I'll follow up with the Android platform team about that.

I found that if you move the call to `setVideoScalingMode` in `MediaCodecVideoTrackRenderer` to happen in `onOutputFormatChanged`, then in that case the call does work correctly. It's probably the case that we should move the call there, seeing as that works, although I'd like to confirm expected behavior with the platform team first. In the meantime, you can give that a try at least and verify that it works as expected!
 Marking as a bug to make sure we fix/workaround application of the scaling mode.
 - We'll push a fix to correctly apply the scaling mode soon, which will be to set it on the `MediaCodec` from `onOutputFormatChanged` as described above.
- TextureView is definitely significantly worse with respect to power draw. How much worse depends on many factors, but we've seen numbers in the region of a ~30% increase in overall device power draw when using TextureView rather than SurfaceView.
 Scaling mode only works for `SurfaceView`. We'll clarify this in the documentation. If you're using `TextureView` then you should use `TextureView.setTransform` instead.
  Give the failure is in `readSample`, it's pretty unclear to me that the error is reading the initialization segment. Please provide a link to a playable DASH manifest that reproduces the issue.

Also, the issue template also contains an explicit request for useful information, like the version of ExoPlayer that you're using, which would be useful here.
 It doesn't have to be a complete test stream. It's possible to do something like create a manifest by hand that contains just enough of the media to reproduce the issue, for example, and then host just that subset of the media and the custom mpd side by side. Or similar.

You can email `dev.exoplayer@gmail.com`, although please update this issue so we know to go look for it! Thanks.

As an aside, the comment to which you reference is a typo; we'll fix that!
 Thanks! It appears the `trun` boxes in this stream are invalid. Specifically, both `first‐sample‐flags‐present` and `sample‐flags‐present` are set to true, but the specification (ISO 14496-12) states:

> first‐sample‐flags‐present; this over‐rides the default flags for the first sample only. This makes it possible to record a group of frames where the first is a key and the rest are difference frames, without supplying explicit flags for every sample. **If this flag and field are used, sample‐flags shall not be present.**
 We do support "most streams" ;). We've been supporting DASH for over 2 years now, this is the first stream that we've seen with this issue, and it appears to be a pretty clear violation of the MP4 spec.

If players become arbitrarily more permissive over time then what incentive is there for a provider to fix their streams? And if there is no incentive and providers all end up shipping subtly broken streams because players are permissive, how is someone ever supposed to write a new player from scratch? They'd need a complete list of "workarounds that everyone does even though they're not part of any spec". I'd actually rather see CAST and Chrome updated to explicitly fail on this type of stream, not the other way around.
  Accurate position.
  We do not support this mode, no. If you wish to use DRM then the recommended approach is Widevine with DASH or SmoothStreaming. You can also use PlayReady with DASH or SmoothStreaming if you're targeting only AndroidTV devices.
  The issue is that the HttpDataSource you instance there is not the same you use to obtain the chunks. With that DataSource you just obtain the Manifest. You should do the same configuration in the DataSource's used for the streams. Perhaps having a look at HlsRendererBuilder can provide some insight. 
  1. Yes. The demo app uses [DefaultUriDataSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/DefaultUriDataSource.java), which delegates to a [FileDataSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/FileDataSource.java) when supplied with a `file://` URI (or a non-asset URI with an empty scheme). If you just want to try out the functionality, you could start by adding the path to the file you want to play to the [samples list](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/Samples.java) in the demo app (making sure to pick the right type, for example Util.TYPE_OTHER for an MP4 file). Alternatively, you can start the demo player directly, using an intent: `adb shell am start -a com.google.android.exoplayer.demo.action.VIEW -d "file:///sdcard/file.mp4"`.
2. It may be related to ExoPlayer, because YouTube uses ExoPlayer for DASH playbacks. I'm not sure what information you are looking for, though. (And the way the YouTube app works specifically is probably off-topic for this issue tracker!)
 That depends on the implementation details of the YouTube app. I suggest reading through issue #420 and its linked issues to find out more about using caching with ExoPlayer.
  What is the exception you are referring to? If it relates to having two media clocks, you could try subclassing MediaCodecAudioTrackRenderer, overriding getMediaClock to return null and using this new class as a replacement for one of your audio track renderers. The other audio track renderer, which is still a MediaCodecAudioTrackRenderer, will be the player's media clock. If the exception is something else, please provide a stack trace.
 If you look at the source of the failing assertion, it is caused by calling open on an already open data source. Do you have two SampleSources using the same DataSource? Please post the code that builds the sources and renderers.
  Please send the file to `dev.exoplayer@gmail.com`(access permission was not granted to the given resource) and post back so that we go look for it.
 Same as [#1362](https://github.com/google/ExoPlayer/issues/1362). The issue here is that exoplayer only supports UTF-8 right now. I think you have two possible courses of action:
1. Convert your file to UTF-8 and serve the resulting file.
2. Programmatically do the conversion before feeding the file to the exoplayer parser.
  I don't understand the question. The watermark/logo is presumably burnt directly into each of the video streams. If that's the case then it's not possible that we would be showing the watermark/logo from one of the streams but the underlying video from the other.
 How is that possible? If the watermark changes then the player must be playing the stream that has that watermark baked into it. There's no such thing as playing a 720p stream at 360p quality, unless the device you're testing on is doing something bizarre.

Please can you provide a test URL that we can reproduce what you're seeing with, and details about the specific device you're testing on?
 This sounds like a device specific error. Please report it to the device manufacturer (it's unclear whether the device has passed Android CTS).
  Closing because insufficient information was provided.
  @tresvecesseis I'm pretty sure your case was applicable to HLS only.
@ChernyshovYuriy If your DASH content is generated such that each segment starts with a keyframe (which they should) and such that each segment contains samples whose timestamps align properly with what the manifest defines, then this shouldn't happen. Unfortunately, it's up to you to debug what's wrong with your content. Given the streams we have from other providers all work correctly, this is most likely to be a content issue.
 Actually, I think it might be possible for this behavior to occur in the DASH case (although it's not really the same problem). I'll have a look.
 This is fixed for both DASH and SmoothStreaming in `dev-v2`. I'm not sure about HLS; it would likely depend on how sensibly the chunks are encoded, but closing this issue because the sample provided was a DASH stream.
  It's not always -1 for DASH (as demonstrated if you simply modify the demo app to print the value). This callback is invoked without knowledge of what's being parsed from the data, so the frame rate needs to be indicated in the manifest (or equivalent) if it's to be set.
  Please provide more detail. Are you using DASH, SmoothStreaming or HLS? Is this for live streaming or for on-demand? I don't think the callback to which you refer is invoked for manifest load or refresh for DASH or SmoothStreaming.
 For DASH and SmoothStreaming custom data can be embedded directly into the media in EMSG boxes, which we'll be adding support for in 2.x.

I'm not really sure what the best approach would be. Can you provide an example of what these fields look like and do, how other players provide you with access, and when you expect to receive them?
 Question - Is there a reason why you don't burn your metadata into the stream as ID3, which I think would be the normal way to do something like this in the context of HLS. See [this](https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/HTTP_Live_Streaming_Metadata_Spec/Introduction/Introduction.html#//apple_ref/doc/uid/TP40010435-CH1-DontLinkElementID_11), for example. ExoPlayer already supports metadata delivery via ID3.
 I replied to the pull request. No particular objection to providing a way to do this, but not in the way the PR is currently structured.
 I believe your requirements were addressed by the patch you sent; so closing this one. Thanks!
  We don't support SAMPLE-AES. AES-128 should "just work".
  In using HLS (and as a result MPEG-TS) you've made a decision to use a container format that fragments media samples unnecessarily into hundreds (sometimes thousands) of pieces, that the client needs to merge back together. You've also chosen to use a container format that includes insufficient metadata, meaning the client has to dig around in the raw media stream to get at what it needs. MPEG-TS is fundamentally inefficiently and sub-optimal for delivery of media to mobile devices for these reasons, particularly at high bitrates. 

You should think carefully about whether you want to provide an optimal experience (by switching to DASH or SmoothStreaming and as a result using a sensible container format), or just one that's "less bad, but still quite bad", which is what optimization of HLS playbacks in ExoPlayer would achieve.

To address your specific points:
- We have no plans to change where extraction happens. We used to do extraction much later than we do now, and ironically HLS was one of the primary drivers toward us doing the extraction earlier. We frequently see HLS streams where the audio and video samples are poorly interleaved in the media chunks, which made it necessary to perform extraction further ahead in order to play corresponding audio and video samples at the same time. Note that the total amount of work done is the same either way; it's just that doing it earlier doesn't distribute the computation as evenly over time. All of this aside, we just wont be making large structural changes to handle what is essentially a legacy, albeit still widely used, format, particularly given we're seeing more and more large partners migrate to DASH (with excellent results).
- We would likely accept a pull request that optimizes AES decryption, but it's not something we're likely to spend time on ourselves in the near future. Note that any contributions must be accompanied by a CLA (meaning, for example, you can't just copy-paste code from something you've found elsewhere).
 Apple have announced that they'll be adding fragmented mp4 and common encryption support to HLS. This should make it possible to deliver HLS content in a far more suitable container format, and using content protection that Android supports efficiently. This move makes it less likely that we'll spend time on this issue, but we'll leave it open for now.
 We don't have any plans to optimize this in the near term, so I'm going to close this issue. Please see my other comments above.  Same as https://github.com/google/ExoPlayer/issues/953. Please report the issue to the device manufacturer, since I do not believe this is an ExoPlayer issue.
  Please can you provide sample media for both codec types? Ideally also for both 16-bit and non-16-bit. You can email some samples to dev.exoplayer@gmail.com if you're unable to provide them publicly. Thanks!
 Also looks good otherwise, thanks!
 [NB - Matching style of code comment applies to this one too]
 Thanks for the changes! FYI I made some tweaks in https://github.com/google/ExoPlayer/commit/841c10f12163dad34066b81b9a8058e2c77fd1ea. Let me know if I broke anything; I confess to not having tested the result...!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I added some comments to address and am unsure about the for loop. Otherwise looks pretty good to merge. Thanks!

General point - Please try and make your code match the existing style. Specifically indentation (2 char indent rather than 4, unless it's actually a broken line in which case we use 4 chars). We also always put a space either side of an operator, so always `1 + 2` rather than `1+2`.
  - Are you using `FrameworkSampleSource`? It has been deprecated for a long time and it's use is strongly discouraged, so you should really stop using it if so.
- Opus is only supported in the Android platform from 5.0 onward. See [here](http://developer.android.com/guide/appendix/media-formats.html). If you really want to use Opus and need it to work on earlier platform versions, you'll need to use ExoPlayer's [Opus extension](https://github.com/google/ExoPlayer/tree/master/extensions/opus).
 What extractor are you using? Are you using `ExtractorSampleSource` or `FrameworkSampleSource`? It might not be relevant to the issue you're seeing, but it would be good to know.
 We're pretty confused about the logging from `FFmpegExtractor`. If you're using `ExtractorSampleSource` then I don't see how that would ever be output. That looks very much like logging that would be output if you were using `FrameworkSampleSource`...
 There's unresolved confusion over exactly what components were being used here. Closing.
  The problem is that this variable bitrate file has no variable bitrate header, so we assume it is constant bitrate and calculate the wrong duration. If you play the file using the platform's MediaPlayer (using e.g. `adb shell am start -a android.intent.action.VIEW -d "file://path/to/file.mp3" -t "audio/*"`) you can see that the seek bar jumps to the end suddenly after two minutes, for the same reason.

To guarantee getting the correct duration in general, we would need to read the whole stream, which is not feasible. Alternatively, we could use 'unknown' duration for MP3 files without a header, but this would lose useful information when playing valid CBR files.

If your app is playing audio streams only, one possible workaround is to override MediaCodecAudioTrackRenderer.onOutputStreamEnded() and use that method to signal to your app that playback has ended. That won't help you provide correct player seeking controls, but perhaps you know the duration of the stream already from some other source.

We could take into account the duration of disabled renderers only in the condition to transition to STATE_ENDED in ExoPlayerImplInternal, so that when all enabled renderers have ended we would immediately transition to the ended state if there are no disabled renderers with a known duration after the current position. (Perhaps that is the fix you are hinting at in your comment.) Then ExoPlayer's behavior would match the platform. I am not convinced that it's worth the added complexity to handle input that is unsuitable for streaming, though.
 If the duration is known and there is a disabled renderer with a duration that is longer than the enabled renderers' durations, I think we want playback to continue after the enabled renderers have ended (in case the app enables that renderer, which would still have media to play).
 As author of the change I can state that the intention was more general than you believe. Fundamentally, if a piece of media has a defined duration, the desired behavior is that the player does not transition to the ended state before that duration has been reached just because one or more of the renderers happens to be disabled.

It should be noted that the vast majority of ExoPlayer usage (in terms of hours of playback) takes place in applications that are provided by providers who generate their content correctly and aren't affected by this kind of issue. Out of the relatively small fraction of usage that's left, only a small fraction of that is trying to play broken content. So in the bigger picture it doesn't make sense for us to start implementing less well defined player behaviors to try and better accommodate broken playbacks. If individual applications that happen to deal with a lot of arbitrary/broken side-loaded content want to patch the version of ExoPlayer that they include to be more permissive, it's not that huge of a technical challenge for them to do this.
  Why don't you trust the player to buffer chunks in a sensible way? Or to put it another way, why do you actually want to do this? Having enough buffer to play the next few seconds isn't a good reason in itself, and artificially keeping the buffer small significantly increases the probability of re-buffers or playback failures.
 `LoadControl` (and `DefaultLoadControl`) implements this functionality currently, so you should start there.
  In the attached file there seems to be more than 10 MB of video sample data before the first audio sample. I think the player is probably loading video samples until it has filled the allocation, but at that point not enough audio has been loaded to let the player transition to the READY state. No more samples are loaded because the allocation is full, so the player is stuck.

For reliable playback you need to produce a file where the samples are better interleaved. You should be able to control this at the encoder. Or you can re-interleave samples in an existing file to make it suitable for playback (for example, you could run `avconv -i flower.mp4 -codec copy flower-interleaved.mp4`).
 This is basically exactly the same as #1386 (going from the naming of the file, I'm guessing it's the exact same media file too)?
 @bluelabs I sometimes use avprobe in libav, which appears to be available in Homebrew (`brew install libav`).
  @wvpaf - Could you take a look?
@pykler - Please attach a full bug report, as is requested in the issue template. Thanks!
 Hi,

It seems that the resulting 'load keys' error is similar but the root error 'LoadKeys: OEMCrypto_LoadKeys error=10036' is different from the 'LoadKeys: OEMCrypto_LoadKeys error=10007'.
The WV team will follow up the SoC vendor out of band.
 @pykler, @polmabri - If either of you still have a device in this state, and you have root on the device, the SoC manufacturer has requested the following information, which will allow them to create a custom TZ image to further debug the problem:

Reboot into fastboot:
adb reboot bootloader
fastboot getvar uid 

You should see something like:
$ fastboot getvar uid
uid: 0B2C1B0112000000000000000000

Please provide the uid.
 @pykler &/or @polmabri , any update?  This case is starting to age. Any update would be helpful. Thanks,
  Depending on the sources of a demo project doesn't feel like the right thing for you to be doing. It might be the easiest thing to do, but that doesn't make it right, particularly given you're probably only interested in the DemoPlayer and Builder classes. Copying them into your own project is the best approach in my opinion.

For 2.x release of ExoPlayer we're working on substantially reducing the amount of code that you need in your application. Specifically, it's likely that all of the functionality that currently resides in the Builder classes will be merged into the core library, and much of the complexity in DemoPlayer will go away too. This should further reduce the barrier to entry.
  Please provide information as requested in the issue template.
  Using git blame and looking at the original changes and their change descriptions can often answer questions of the form "what is this code for". See [here](https://github.com/google/ExoPlayer/blame/master/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java). In this case, the code block is for AndroidTV devices.
  Passing zero as the buffer duration as in your code snippet looks fine for what you are trying to do. I think one possible explanation is that decoding the required frame sometimes just takes a long time: if you seek to a frame, the cost of decoding it depends on its distance from the previous key-frame. Key-frames can be decoded quickly but later frames are increasingly expensive to decode because (generally) all earlier frames in the group of pictures (i.e., since the last key-frame) have to be decoded first. So it is generally not feasible to make `seekTo` give you a frame instantly, I'm afraid.

There may be something else going wrong, so you could test out this hypothesis by checking that the time to decode a frame is low for frames that are just after a key-frame, and high for frames that are just before a key-frame. One way to find out the timestamps of key-frames is to run `ffprobe -show_packets -print_format compact -select_streams v:0 filename | grep flags=K`. If the speed doesn't depend on the frame position then the problem is different.

Modern compressed video formats are not designed for fast frame-level random access like this. You could try reducing the key-frame interval if you control the encoder, but this can reduce the video quality or increase the file size. Seeking will still be slower on devices with slower decoders. Depending on how smooth you need seeking to be, it may be necessary to store decoded frames as bitmaps so every frame is cheap to access, transcode to a different format or do something more complicated.

(Note: there is a small optimization possible in the case of seeking forwards by a few frames where you 'decode forwards' rather than flushing the decoder state. I don't think we do this at the moment, but it wouldn't help with rewinding anyway.)
 To clarify the final paragraph of @andrewlewis 's reply: We did actually used to implement this optimization in certain specific cases, but it was really difficult to implement properly. Our implementation was subtly broken, and it turned out that it would be _really_ tricky to get it just right. Since the improvement was marginal, only helped with a fairly niche use case, and even then only helped with fast-forward and not with rewind, we ended up removing it. I think this was a good decision.
 @vxhviet You could (for example) log the timestamps of frames as they are decoded, at the start of [MediaCodecVideoTrackRenderer.processOutputBuffer](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecVideoTrackRenderer.java#L385). If you pause the player, then tap on the seek bar you should find that the number of frames that have to be decoded depends on where you tap (on or just after a key-frame should require fewer frames to be decoded). To find out how long it's taking, you could use `adb logcat -v threadtime` to see the how much time elapses between logging a request to seek in your app and the final frame being decoded.

I don't have any other suggestions, except to store the frames in a different format which is more amenable to random access, as per my previous comment.
 @Saketme Could you clarify whether you're looking for support for the optimization @ojw28 mentioned earlier in this thread, or something different?

The time it takes to scrub to a frame partly depends on the performance of the video decoder, which varies between different Android devices. I can't think of any reason Android would be inherently slower or faster than another platform in general.  Please open a new issue including the information required in the issue_template file. As a starting point, we usually need to reproduce the issue ourselves. However, I did see errors in the provided log file such as:

`java.lang.IllegalArgumentException: Requested window android.os.BinderProxy@41584138 does not exist`

which I haven't seen before in the demo App(this does not mean it couldn't be an exoplayer related problem, of course). I wonder if you have introduced some modifications yourself or if this is a device specific issue(in which case, "some STB" is not a very detailed domain). Anyhow, we will require more information to help you. Thanks for reaching out!
  Thanks!
  The logic to which you refer allows re-evaluation of a previous format selection. This gives the player flexibility to change the evaluation should conditions have drastically changed since it was originally made. The constant controls the rate at which such re-evaluations are allowed to occur. The value was chosen fairly arbitrarily, however it's not particularly important and should have no impact on issues relating to BehindLiveWindowException or the player getting stuck in a buffering state.
  We have quite a few live test streams available to us internally, including test streams provided by a range of large media streaming providers and services. We're not able to share such streams because we don't own them and they're provided to us for testing purposes only, but we do not see these issues with the test streams we have, and neither do the partners who provide them.

I'm unclear what you're after here. It's not up to us to provide streams that prove ExoPlayer functions correctly. If you have streams that prove ExoPlayer does _not_ function correctly, then it's up to you to provide them to us in the form of clear issue reports, providing the information requested in our issue template, so that we can take a look.

Note that BehindLiveWindowException has been documented quite extensively in this issue tracker. There's currently no evidence to suggest this occurs in any case other than when the stream does not have a sufficiently large live window for mobile playback (in other words, the chunks exposed in the HLS playlists do not cover a sufficient duration of time). If you have evidence of BehindLiveWindowException occurring for streams where the window is reasonably large, please file an issue as described above.
 Note - It's also unclear whether you're talking about HLS, DASH or SmoothStreaming. The failure mode for BehindLiveWindowException is similar in all cases, although in DASH it can also be caused by incorrect server/device time synchronization for certain types of manifest (you would have to provide a test stream exhibiting the issue for us to tell you whether this is the issue).
  We have no plans to merge any DVR / seeking-in-live support for HLS until 2.x, where the architecture will look quite a bit different and should be more amenable to supporting this functionality.
  What is the one device/build where you see this? Please provide the output of `adb shell getprop ro.build.fingerprint`.
 I haven't taken a more detailed look, but it feels a look like a device specific issue. It should probably be reported direct to the device manufacturer. Where can we get one of these devices? Is it one of [these](http://www.geniatech.com/pa/android-tv.asp)?
 Closing on the assumption this is a device specific issue. Please report it direct to the device manufacturer.
  This is a `HttpDataSourceException` being printed by an event logger, which is in your own code. `HttpDataSourceException` contains a `dataSpec` that includes information such as the failing request Uri. So you could try looking at those.

ExoPlayer does already buffer ahead (by a sufficient margin given anything approaching reasonable server performance). You can further increase the buffer size by changing the `BUFFER_SEGMENTS` constants in `HlsRendererBuilder`, if you wish to do so.
  As per the HLS [specification](https://tools.ietf.org/html/draft-pantos-http-live-streaming-18), `EXT-X-TARGETDURATION` and `EXT-X-MEDIA-SEQUENCE` are tags that should exist only in media playlists, not in the master playlist.

ExoPlayer automatically detects the type of the playlist being parsed based on the tags that it sees. Since the playlist contains tags that should only exist in a media playlist, we assume that's what it is. Playback then fails because it is in fact a master playlist that contains spurious tags.

The solution is to fix the source media to adhere to the specification.
  Hey. We've pushed TTML positioning support to the dev branch now. Apologies for the overlap in work. Please take a look and let us know of any issues/shortcomings. If you'd like to contribute tests covering specifics of the support you're going to be relying on, those would be appreciated also!
  This issue doesn't provide enough information. For example:
- What does "YouTube videos" mean? YouTube videos are transcoded into many different formats, so you need to be more specific. Please provide a URL that's playable in both MediaPlayer and ExoPlayer and exhibits the behavior you observe.
- How are you measuring CPU usage?
- To which process(es) is the CPU usage attributed? For ExoPlayer, CPU utilisation is typically split between the application process and Android's mediaserver and surfaceflinger processes.
- What's your experimental setup in each case and how can we reproduce it?
- High CPU usage isn't in itself a significant problem. What negative effects are you seeing as a result of the high CPU usage?
- Please provide a full bug report, as is explicitly requested in the issue template.
  Thanks for this. We'll push a fix tomorrow.
  In the sample file the video samples are placed in the byte range [3227,66370313], and the audio samples are placed at the end, in the byte range [64844208,66394879]. For any sane streaming use case you really need the audio and video samples to be interleaved evenly throughout the file.

If the samples aren't interleaved then a player needs to issue multiple separate network requests or maintain a huge buffer. ExoPlayer does actually play this sample correctly if you increase the buffer to hold the entire video (i.e. ~70MB), but the correct solution here is to encode the media in a more sensible way.
 `avprobe -show_packets file.mp4` will output information on each media packet in `file.mp4`, including the position and size of each sample.
 > This example mp4 file (http://23.239.4.36/test_audio/kwwl_20161114195742230AC.mp4) when played through the Exoplayer 2 demo app seems to fail because of the audio renderer falling behind the video, but the gaps in byte position between audio and video data never seem to be larger than ~50-100kb (viewed using "avprobe -show_packets" ).

Playback of this file is fixed in `dev-v2` (the relevant change is https://github.com/google/ExoPlayer/commit/488c2d82708fcd547a5a0b19bd56160914ed8567).

> Perhaps the fix is to have separate buffers for each track?

We already use separate buffers for each track ;).

> One other bug my issue seems to reveal is that when multiple tracks(audio/video) are loaded to different timestamps, the buffer displays the maximum timestamp of the tracks that are loaded, when I believe it should be the minimum (if video is loaded to 45 sec, but audio is only loaded to 40, when I skip to 44 seconds all the data is not actually loaded yet even though the visual buffer indicated it was). I believe this is caused by ExtractorMediaPeriod.java getBufferedPositionUs() method referencing getLargestQueuedTimestampUs() method which doesn't make sense, and might be affecting the code elsewhere.

Yes. This is what https://github.com/google/ExoPlayer/commit/488c2d82708fcd547a5a0b19bd56160914ed8567 fixed. It's a little more nuanced than just using the minimum because the file may also contain sparse tracks. Take a look at the change if you want to see the logic for handling this case correctly.

> Let me know if you agree with my assessment. I will continue to debug the exoplayer code to see if there are improvements that may be suggested in a PR. Thanks!

I think the conclusion is that if you use `dev-v2` you should find these problems are resolved.
  _Or_, people could stop putting media into containers that can't actually house it properly?
 To "correctly" transport HE-AAC in ADTS you'd need to put a value of 5 into a field that can only represent the range [1,4]. Hence I disagree with the statement that there must be a way to transport HE-AAC correctly. There's a discussion [here](https://github.com/dailymotion/hls.js/issues/224). I would concur with the statement there, that:

> it is impossible to correctly signal HE-AAC using ADTS header

I mean, you can continue to use outdated, unsuitable, inefficient and computationally expensive container formats on Android because other industry players mandate their use, or you can deliver a high quality experience on Android devices and save the use of such container formats only for devices where they're required (which might actually help advance the industry ;)).
 - For what it's worth it is possible to transmux MP4 to TS at the CDN level, and I think some CDN providers support this. Quite a few major streaming providers have moved to DASH now, and I've yet to hear any negatives. Quite the opposite in fact.
- The CODECS attribute is optional, and even with it I don't think there's enough information to properly reconstruct the codec specific data.
- From an ideological point of view I think I'd argue that you should penalize legacy formats (and devices that require them) by preferring to evict them from the CDN, but I acknowledge that this isn't a particularly practical suggestion ;).
- Since Android doesn't support AAC Main we'll likely "fix" this issue by blindly mapping audioObjectType == 1 to AAC LC and hoping for the best. I'll push a change to do this soon.
  There is insufficient information here to answer the question (i.e. the type of media being played), but unless you're seeing an actual issue or negative impact related to this question, I would suggest that you trust that the player is doing something sensible.
  This is the issue discussed on https://github.com/google/ExoPlayer/pull/1385. As far as I can tell, this is an encoding error at this time (putting HE-AAC inside of a container that can't properly represent the HE-AAC object type). It's an open question whether we want to try and work around the issue.
  Thanks for the suggestion. We'll consider this starting from our v2 release. We likely wont adopt semver because it would result in us incrementing the major version too frequently, but we should add more structure than exists currently.
 As per above, semver isn't realistic for this project as we'd end up incrementing the major version too frequently. As of V2 we've been incrementing X.Y.Z to mean:

X - Major new release
Y - API changes / large new features
Z - Bug fixes / small new features

I think that's the best we can do given the project's current rate of evolution.  The same would be true of Id3Frame and subclasses too. Do you have a good way to access a Surface in a different process? Because unless you do, there's not much point doing anything else in this area!
 It would be good if you could verify that passing a Surface across processes actually works, before we invest time in this. Please give it a try and let us know what you discover.
 Cool! Please feel free to send a pull request to our dev branch that makes `Cue` parcelable.
 You're more than welcome to implement this and send us a pull request to add support, but I don't think we'll be doing this ourselves and there's seemingly little additional demand. Hence I'm going to go ahead and close the issue for now.
  Unfortunately, this is not ExoPlayer related: If the same song is being played, it is because the same media URI is being used for playback. This means there is an error in the application's code. This error has not been reproduced in the DemoApp previously, so I find it unlikely that the error comes from the DemoApp out of the box.

 If I were you, I would take a look at any modifications you may have introduced in the DemoApp to find what the mistake is. If you just changed the elements in the DemoApp for your songs (without introducing any new behaviours) make sure you haven't added the same URI twice. If you still have a problem, let me know!
  Based on the logging, it looks like the renderers were built using ExtractorRendererBuilder. The error is output because ExtractorSampleSource does not read m3u8 files.

HlsRendererBuilder shows how to build renderers for playing an HLS stream. If you are adding the URL in Samples.java, use Util.TYPE_HLS like [these samples](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/Samples.java#L213).

Regarding H.264 Main profile: before Android M, the Android CDD only required support for Baseline profile, so you can't rely on this format being supported in general (though it may work fine on many devices). The [M CDD](https://static.googleusercontent.com/media/source.android.com/en//compatibility/6.0/android-6.0-cdd.pdf) does require Main profile level 3.1, though.
  If the question is how to set HTTP headers: [DefaultHttpDataSource.setRequestProperty](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/DefaultHttpDataSource.java#L162) can be used to add headers to requests made by the data source. [HlsRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java) in the demo app shows how to set up renderers for playing an HLS stream at a given URL. You can use a DefaultHttpDataSource instead of a DefaultUriDataSource.
 Please could you explain in detail exactly what you are trying to do? Your first message did not mention encryption; what scheme are you trying to use? Can you clarify why you are making a request to the server if everything is downloaded?
 It's still unclear what you are trying to do. What player component is making the request you need to add headers to? What failure do you see when trying to play the video? Are you using DRM?
 Hi @AlirezaGhanbarinia, if you could please provide a description of what your problem is, that would be really helpful. Please be specific, provide as much descriptive information as you can. So far, it seems that exoplayer does not have anything to do with this scenario.
 Agreed this issue is very unclear. There's no clear description of which network request this issue is talking about, where it's being made from in code, what it's for or what data needs to be sent in it. Please fully describe what you're trying to do if you wish us to investigate further.
  Given you're doing something that no one else has asked for, the first question should be: Why do you need to do this? It feels like it shouldn't be necessary.
 Extracting it from the pssh seems like a reasonable solution to me (at least from the client's perspective).

I am aware of other applications sending up various content ids in their license requests, now you mention the use case. In such cases the content id is typically known in advance of fetching the manifest (because the content id is typically needed to construct the manifest request). Hence the common way to do this is simply to pass the content ids that need to be sent to the constructor of your `MediaDrmCallback` implementation, before even fetching the manifest. The `MediaDrmCallback` can then include the ids in the request when `executeKeyRequest` is invoked.

Not sure if the above works for you, but you could explore that path, or stick with the current solution that you have. Closing in any case, since we don't have plans to pipe this parameter directly into `executeKeyRequest`.
 I still think the setup being described here is pretty unnecessary. There definitely exist cleaner solutions to this problem. I thought the CENC PSSH data is sent up as part of the license request. If the KID is contained within the PSSH data as implied above, why can't the server just parse and extract it from what's already being sent?

The above aside, it's a little unclear how your current implementation works. What are you actually doing in onLoadStarted currently? Specifically, where are you getting the manifest from? If you're getting it from `ExoPlayer.getCurrentManifest` then you should probably use the `ExoPlayer.onTimelineChanged` callback instead, since this is the earliest point at which the manifest can be retrieved from that method.

I don't think we want to make hard guarantees about the ordering, however, so this is still risky. The only guaranteed way to get data from the manifest before the key request is made would be to extend `DashManifestParser` and retrieve what you need during the actual parsing. You can inject your extended/custom parser into `DashMediaSource` constructors. I don't really understand what step (2) is for, either. What does it do that cannot just be rolled in to step (3)? Are you asking whoever is providing the serving side to make things a little less convoluted?

Your approach of extending `DefaultDrmSessionManager` sounds like the simpler and more logical of the two options. Although thinking about this some more, can't you just extract the key directly in `executeKeyRequest`? I think the data in the `KeyRequest` is the pssh box (or just the data within the pssh box on earlier versions of Android, due to [this workaround](https://github.com/google/ExoPlayer/blob/r2.4.0/library/core/src/main/java/com/google/android/exoplayer2/drm/DefaultDrmSessionManager.java#L336)). So I think you could do this in `executeKeyRequest`:

```
byte[] requestData = request.getData();
if (Util.SDK_INT >= 21) {
  requestData = PsshAtomUtil.parseSchemeSpecificData(schemeInitData, C.WIDEVINE_UUID);
}
// requestData is now the data within the pssh box; extract the key from it
``` Ah, yes, ignore what I wrote in my previous response. The `KeyRequest` is actually generated by the platform (by calling `mediaDrm.getKeyRequest`). It's possible that different OS versions are putting different pieces of data in the request, and I'm not sure it's documented anywhere. Stick to your approach of extending `DefaultDrmSessionManager`, if that's working for you.  Please provide full information, including a link to the media and a proper bug report, as requested in the issue template and also [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
 Closing as this is quite an old issue and the required information was not provided. I can tell you that version 2 improves track selection. Try it out and open a new issue (with the required information in the issue template) if you still find complications.
  Otherwise looks fine; thanks!
  There is no difference between local and remote files. You should just follow the directory scheme used by the m3u8 file, if any. Trial and error should work, if you read the exceptions ExoPlayer throws when it fails.
  It's a little unclear whether this is the best approach v.s. just shoving something into the format to indicate that the samples are compressed, and leaving it to the renderers to do the decompression.

One argument for this is that it keeps the data compressed in the buffer that sits in between, decompressing each sample only when it's needed (similarly to how we don't decode video frames until they're needed). An argument against is that doing it in the extractor solves the problem without the downstream renderers having to do anything. I'll have a think.
  To help with this kind of issue we really need an easy way of reproducing the problem. It's unclear whether this is even an ExoPlayer issue, as opposed to either an application bug or a device specific issue. My guess would be that the latter is most likely. Please check with the device manufacturer that it passes Android's Compatibility Test Suite.
  It sounds very much like the media is broken to me. You'll need to provide us with an easy way to reproduce the issue if you want us to take a look.
 Closing due to insufficient information.
  Sorry this didn't get looked at.

It appears the license proxy returns a status code 400 now, so we're unable to investigate at this point with the information provided. Please provide test content that still works if you wish for us to investigate further. Please also test using a more recent version of ExoPlayer, since if it's a player issue it's relatively likely to have been fixed. Thanks! Closing due to lack of information.  DefaultBandwidthMeter invokes its listener with `onBandwidthSample` when a bandwidth estimate becomes available. It's true that each invocation may not correspond directly to a single chunk fetch, but then it's unclear why it would be a good idea to do that in the first place. If chunks are being downloaded in parallel in an overlapping way, then dividing the size of a chunk by the time it took to download it doesn't seem particularly useful. The bandwidth estimate you end up with ignores the fact that other data may or may not have been downloaded in parallel. Note there are other complexities too, such as if a chunk fetch failed half way through and had to be resumed via a retry mechanism.

What is is that you're actually trying to achieve? Is this for DASH or for HLS?
 I don't really understand. DefaultBandwidthMeter's estimate is a already a bandwidth estimate that's independent of chunk size. What specifically is wrong with the estimates that it provides?
  The issue template pretty clearly requests substantially more information than is provided here (in particular, it _specifically_ states not to just copy a small amount of logcat output). Closing this issue and the other one filed. Please file issues with complete information if you wish us to take a look.
  Unfortunately, to be able to help you, as a starting point, we need information to recreate the error ourselves. Please read the ISSUE_TEMPLATE provided in the root directory of the project. Other important point is that you haven't provide crucial information, like the ExoPlayer version you are using.
  There's not actually a bug here. The static MimeType methods return the constants defined in MimeTypes, so doing referential equality checks against those same constants is valid.
 It is true that it's a little error prone, as a pattern. We'll update to use the `MimeTypes.CONSTANT.equals(variable)` pattern at some point.
 Marking as a bug just so that we remember to make the change.
 The referential equality checks are gone in V2 (which isn't available on GitHub yet, but will be before too long). Closing since we've cleaned this up, and it's not actually a bug in V1.
  The file looks like it's UTF-16 encoded to me. We only support UTF-8. For what it's worth, several text editors on my machine are also unable to open this file (I could open it in emacs though). I converted it to UTF-8 and it worked fine.
 Someone could add UTF-16 srt support to ExoPlayer. It's unclear whether it's really worthwhile though. It's not high priority for us, in any case.
 We cannot merge things that take dependencies such as `org.mozilla.universalchardet`.
 Closing for now because I don't think there's sufficient demand to merit spending time on this. If anyone wishes to send a pull request that adds support without taking any external dependencies (as was the case with the suggested change above), then feel free to do so, and we'd likely be happy merging it. Thanks!
  Are they even errors? I wouldn't expect something that's only a warning to prevent the apk from being generated. Note that the classes that aren't found are android library classes that were new in API level 23. Are you sure you're not using an out of date sdk?
 You need to target API level 23.
 Actually, I'm not certain about that. Proguard's troubleshooting page has a section on this [here](http://proguard.sourceforge.net/manual/troubleshooting.html#unresolvedclass). The relevant paragraph being:

> If the missing class is an Android run-time class, you should make sure that you are building against an Android run-time that is sufficiently recent. You may need to change the build target in your project.properties file or build.gradle file to that recent version. You can still specify a different minSdkVersion and a different targetSdkVersion in your AndroidManifest.xml file.
  Thanks for the report! Your solution isn't quite complete because it doesn't take into account the possible default values for displayWidth and displayHeight (it would be valid to specify only one of them and have the other default from the corresponding pixel dimension). I'll add in some additional logic for that and merge a fix. Thanks!
  It sounds like you're on the right track already, but you'll have to figure it out for yourself or ask somewhere more appropriate like StackOverflow. Our team doesn't scale to help with this kind of request. It's also not in our remit to add documentation/comments to the codebase indicating how features that we don't support might be implemented.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  See #273. Though it is possible to create and use more than one player at the same time on many devices, unfortunately there are some devices where this doesn't work.

You may be able to work around the limitation by using software codecs, but this is very inefficient. Also, based on your comment, the device is already trying to use a `OMX.google.` software decoder, so even that appears not to work. On what device did you see the error message?
 I don't know of a reliable way that works for all formats across all devices/builds, except using a software codec.
  I will give it a look!
 The issue seems to be that the transport stream does not include any video ts packets(even though video is declared in the PMT). This is a complicated situation, because the transport stream is not valid. It would be ok for us to fail gracefully when you meet an ExoPlayer limitation, but this is not the case. If we add checks of every possible invalid input, we will end up with a very nice multimedia analyzer, but a poor player. If I am wrong, let me know. Otherwise, please check the validity of the input, whenever possible.
 One last thing, the error report in this case is not as simple as one would expect. The problem is that the player is not failing. The player is just waiting for the video packets to arrive. At any point you have to ask whether the transport stream is invalid or the awaited packets are about to arrive.
  This is the same as https://github.com/google/ExoPlayer/issues/548. We'll add Note 2 to also receive the workaround.  We do not currently support this functionality. You could build it yourself, but we're not in a position to be able to offer advice on how you might do this.
  This looks identical to https://github.com/google/ExoPlayer/issues/1295, which was fixed in ExoPlayer version 1.5.7. The log shows you're using 1.5.6, so I suspect that updating to 1.5.7 or later will resolve this issue for you :).
  The issue template asks for more information than is provided here. For example, it states that you should provide a full bugreport and not just a snippet of logcat output. Please file a new issue with complete information.
  Could you attach sample content that's affected by this issue for us to verify/test with? Or email it to dev.exoplayer@gmail.com? Thanks!
  We do not support this feature.
  This is tracked by https://github.com/google/ExoPlayer/issues/73 (which also covers handling de-muxed audio).
  You're probably reusing the same dataSource instance for multiple
renderers. This is invalid; you should be using a separate instance for
each one. Does that help?
On 8 Mar 2016 9:59 p.m., "Subinkrishna Gopi" notifications@github.com
wrote:

> I keep getting the following exceptions when I use MP4s with an external
> VTT. Though I see these exceptions, playback is uninterrupted and
> everything works as expected. Unfortunately I am not allowed to share the
> video/vtt URLs. I am using Moto X Second Gen running 6.0.
> 
> E/LoadTask: Unexpected exception loading stream
>    java.lang.IllegalStateException: Unbalanced enter/exit
>        at com.android.okhttp.okio.AsyncTimeout.enter(AsyncTimeout.java:62)
>        at com.android.okhttp.okio.AsyncTimeout$2.read(AsyncTimeout.java:209)
>        at com.android.okhttp.okio.RealBufferedSource.read(RealBufferedSource.java:50)
>        at com.android.okhttp.internal.http.HttpConnection$FixedLengthSource.read(HttpConnection.java:418)
>        at com.android.okhttp.internal.Util.skipAll(Util.java:176)
>        at com.android.okhttp.internal.Util.discard(Util.java:158)
>        at com.android.okhttp.internal.http.HttpConnection$FixedLengthSource.close(HttpConnection.java:435)
>        at com.android.okhttp.okio.RealBufferedSource.close(RealBufferedSource.java:374)
>        at com.android.okhttp.okio.RealBufferedSource$1.close(RealBufferedSource.java:362)
>        at com.google.android.exoplayer.upstream.DefaultHttpDataSource.close(DefaultHttpDataSource.java:270)
>        at com.google.android.exoplayer.upstream.DefaultUriDataSource.close(DefaultUriDataSource.java:152)
>        at com.google.android.exoplayer.SingleSampleSource.load(SingleSampleSource.java:257)
>        at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:209)
>        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:423)
>        at java.util.concurrent.FutureTask.run(FutureTask.java:237)
>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113)
>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588)
>        at java.lang.Thread.run(Thread.java:818)
> E/LoadTask: Unexpected exception loading stream
>    java.lang.NullPointerException: Attempt to invoke virtual method 'int java.io.InputStream.read(byte[], int, int)' on a null object reference
>        at com.google.android.exoplayer.upstream.DefaultHttpDataSource.readInternal(DefaultHttpDataSource.java:552)
>        at com.google.android.exoplayer.upstream.DefaultHttpDataSource.read(DefaultHttpDataSource.java:258)
>        at com.google.android.exoplayer.upstream.DefaultUriDataSource.read(DefaultUriDataSource.java:140)
>        at com.google.android.exoplayer.extractor.DefaultExtractorInput.readFromDataSource(DefaultExtractorInput.java:240)
>        at com.google.android.exoplayer.extractor.DefaultExtractorInput.read(DefaultExtractorInput.java:56)
>        at com.google.android.exoplayer.extractor.RollingSampleBuffer.appendData(RollingSampleBuffer.java:388)
>        at com.google.android.exoplayer.extractor.DefaultTrackOutput.sampleData(DefaultTrackOutput.java:256)
>        at com.google.android.exoplayer.extractor.mp4.Mp4Extractor.readSample(Mp4Extractor.java:390)
>        at com.google.android.exoplayer.extractor.mp4.Mp4Extractor.read(Mp4Extractor.java:129)
>        at com.google.android.exoplayer.extractor.ExtractorSampleSource$ExtractingLoadable.load(ExtractorSampleSource.java:732)
>        at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:209)
>        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:423)
>        at java.util.concurrent.FutureTask.run(FutureTask.java:237)
>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113)
>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588)
>        at java.lang.Thread.run(Thread.java:818)
> 
> My code is identical to that of in ExtractorRendererBuilder except for
> TextRenderer. Which is as follows:
> 
> SingleSampleSource textSampleSource = new SingleSampleSource(
>         subtitleUri,
>         dataSource,
>         MediaFormat.createTextFormat(
>                 "",
>                 MimeTypes.TEXT_VTT,
>                 MediaFormat.NO_VALUE,
>                 TrackRenderer.MATCH_LONGEST_US,
>                 "en"));
> 
> textRenderer = new TextTrackRenderer(
>         textSampleSource,
>         this,
>         mainHandler.getLooper());
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/google/ExoPlayer/issues/1348.
  I think this is something that your application should handle. Specially considering there are many different ways to solve this. Perhaps [this](http://developer.android.com/training/managing-audio/audio-focus.html) can help.
  Just listen to `ExoPlayer.Listener.onPlayerStateChanged` and count the number of times the state changes to `playbackState=STATE_BUFFERING` with `playWhenReady=true`?
  I don't think we really want to do this. The UDP data source isn't fully featured (e.g. doesn't support range requests), and I don't think our extractors recover correctly from packet loss either. It seems quite disingenuous to claim UDP support by default where in reality it's more of a "best effort" kind of thing.
 What's the failure mode now, exactly, and what does "better" mean? What would you like the failure mode to be?
 Got it. What's the actual specific exception that DefaultHttpDataSource ends up throwing in this case? I mean, if it throws its own UnsupportedProtocolException type exception, that's not particularly confusing.

We did used to do exactly what you suggested, but we moved away from that approach because it's possible to use `java.net.URL.setURLStreamHandlerFactory` to allow `DefaultHttpDataSource` to support arbitrary http-like protocols such as icy, and people rely on that behavior. See for example https://github.com/google/ExoPlayer/issues/1003.

One option would be to explicitly throw an UnsupportedProtocolException specifically for udp, I guess, although I'm not sure it's really worth the effort.
 Hm. How about we do the following:
1. Default to httpDataSource as we do currently.
2. If the open call fails, we're using the httpDataSource _and_ the protocol that we're trying to open with isn't `http`, throw an exception that indicates the protocol might be unsupported (and wraps the original exception, so that a caller can inspect it if they want).
  I see what you're doing now, but I don't think it's necessary. There was an issue like the one you describe in 1.5.4, but it was fixed in 1.5.5 in https://github.com/google/ExoPlayer/commit/813031b8452b611c1edb93571f8a1daaaebe49ad. Were you using 1.5.4 when you were seeing this issue? If so, just updating to 1.5.5 should resolve the issue.
  Unfortunately, I have just now been able to get to this. When I tried to play the streams I got a 400 for the media, in both encrypted and otherwise. The manifest is downloaded successfully, but not me media. Can you solve this?
 Just post them here. If you don't want them to be public, you can send them to dev.exoplayer@gmail.com, but still post here as that address is not the best way to get prompt responses.
 - Yes, this sounds like a device specific issue with the secure decode path. You should report it to the device manufacturer.
- It's not possible to bypass security by changing a bit of code ;). The approach you're using wont work if the content license indicates that a secure decode path is required. The fact that it does work indicates that the license is not configured to require one. Whether a non-secure output path is secure enough for your needs is something you need to decide, but it should be configured in the license, not in the client software.
- In general, the approach you're using isn't guaranteed to work even in the case where a secure decode path is not required, however there will be a safe fix or workaround that you can use instead. In order for us to tell you what this is, please could you remove your workaround and modify `StreamingDrmSessionManager.requiresSecureDecoderComponent` to include the following logging:

```
Log.e("DRM", "fingerprint=" + android.os.Build.FINGERPRINT);
Log.e("DRM", 
    "requiresSecureDecoder=" + mediaCrypto.requiresSecureDecoderComponent(mimeType)
    + ", securityLevel=" + getPropertyString("securityLevel")
    + ", mimeType=" + mimeType);
```
- If you let us know what the output of this logging is when you try and play one of the affected pieces of content, we can tell you what the correct solution is (and possibly just implement it directly in the library).
 Thanks! The line of most interest is:

`requiresSecureDecoder=true, securityLevel=L1, mimeType=video/avc`

This indicates that the device has a Widevine L1 integration and that the L1 implementation indicates that it requires a secure output path for H264. What you're observing when you force use of the non-secure decoder is that the L1 implementation does actually work using a non-secure output path in the case that the license doesn't indicate that one is required. There is however no guarantee that an implementation will support this, and it's for this reason that your workaround is unsafe. In practice implementations that I've seen do work when you do what you're doing, but there are no firm guarantees.

A better workaround is probably to force Widevine L3 rather than L1. You can do this by calling:

```
drmSessionManager.setPropertyString("securityLevel", "L3");
```

immediately after you create the `StreamingDrmSessionManager` instance. You should then see `requiresSecureDecoder=false`, and the non-secure decoder will be selected for use without you having to force anything in `MediaCodecUtil`. Please give this a try and see what you discover! If this works, and if L1 Widevine is just flat out broken on this device, we could consider forcing L3 for this device as a workaround in the library. We'll have to take a look / have a think about that though!

Note: I'd advise implementing the workaround only for devices that you know to be affected. Don't just do it on all devices!
 Please leave the issue open so that we can follow up with the device manufacturers. We may or may not decide to put a workaround directly in the library. To assist with both efforts, could you provide the list of devices that you're enabling the workaround on? Thanks!
 following up with Asus contact offline.
 @dobrusev yes when using L3, software only decrypt, that will put a large load on the device and you may experience dropped frames and stuttering especially at higher quality streams. 
 @bwidtmann   thanks for the update.  

As there is nothing we can do in the player/stack I will close this but please to reopen and add to it if other devices are found to be problematic.
  Closing because you haven't provided all of the information requested in the issue template. There's not enough information here for us to efficiently investigate the issue.
  FireOS isn't "proper Android". If this is FireTV specific then you'll probably have to report it to Amazon directly, since it's likely an issue with their FireOS implementation.
  What does this have to do with ExoPlayer? For what it's worth, that sounds working as intended to me. According to the [Javadoc](http://developer.android.com/reference/android/media/MediaCodec.BufferInfo.html#flags), `BUFFER_FLAG_KEY_FRAME` is only set on _encoded_ buffers (e.g. output from an encoder, not from a decoder).
  I think this last message (by @tao1) is not related to this bug, but rather #1315. Also, with less likelihood, could be #1308.
 @DBradyIE, unfortunately the provided media sample includes a single edit list which doesn't include any sync samples. This is currently not supported (mentioned [here](https://github.com/google/ExoPlayer/blob/267a870509bb85e2cff839ea162ae5b7c85933aa/library/src/main/java/com/google/android/exoplayer/extractor/mp4/AtomParsers.java#L261)). This could change in the future, but not for now. Better error reporting will be included in the next release. If you control the file encoding, this is easily fixable.
  This is expected behavior. The first track has `MediaFormat.adaptive` set to true, indicating that it's an adaptive track. The other tracks are non-adaptive tracks, and are there to allow explicitly overriding/disabling the adaptive behavior.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Please fix the indentation/white-space back to how it was before. Right now the files view of this change makes it look like the whole file has changed (i.e. a mess). See: https://github.com/google/ExoPlayer/pull/1334/files
  The content link gives a 404 when I try and access it. Is it no longer available and/or is it geo-restricted?
 Thanks. One point of clarification - I assume it's just a side effect of the way you've produced the test content that the manifest is marked as being dynamic even though it doesn't actually update? It causes the player to get stuck in a never-ending buffering state as it waits for more segments to be added to the manifest. If so then fine; just thought I'd check!
 I tried quite hard to "break this" and couldn't reproduce with `1.5.6`, even when I inserted a whole bunch of artificial network failures/delays etc. 

One source of this kind of issue can be if your server isn't handling range requests properly (e.g. we load part of a segment, a network failure occurs, we make a range request to load just the remainder of the segment, the server serves us incorrect data, we fail). I can't see any evidence of that happening though; your server seems to respond correctly.

I don't think much changed between `1.5.4` and `1.5.6`, but could you try again using the latest ExoPlayer demo app + your URL, and see if you can still reproduce it there?
 Not sure if this is still an issue, but closing since we weren't able to reproduce it and no further information was provided.
 @byronclark , @ranithsachin : If you really want to figure out what is going on with 4.4.1, I'd suggest you get a copy of the manifest and mp4 segments that generate the error and provide us with a link (feel free to share this info by emailing dev.exoplayer@gmail.com). We need to be deterministically able to reproduce the issue. With no more information, everything points to the media generation. Also, please file a new issue with the required information. 
  The sample isn't supported because it doesn't start at a TS packet boundary, which we currently require.

Note that even if we were to support starting at an arbitrary point in a TS packet, this stream wouldn't play on most Android devices, since decoder support is not required for either mp2 audio or h262 video, as are contained in this stream. See [here](http://developer.android.com/guide/appendix/media-formats.html).

It's an open question whether we should support starting (and ending) at an arbitrary point in a TS packet.
 Updated the issue title to reflect the possible enhancement.
 @erdemguven - Maybe I imagined it, but I seem to remember we talked about this (or something similar) a while back. In any case, perhaps we should add support at this point (I guess it's just a relatively simple change in TsExtractor?).
  According to ISO 14496-16, the duration in the `trak` box is:

> An integer that indicates the duration of this track (in the timescale indicated in the Movie Header Box).

So in this case I think that the extractor is doing the right thing and that the media is incorrect.
  Question was answered.
  Is is possible to obtain bugreport from the devices where this happens? In the meantime I will run some experiments to see whether I can guess what the problem is.
  Under what condition(s) would the optimal pixel resolution to display a video be anything other than the actual pixel resolution of the video? Stretching 404 pixels onto 405 pixels sounds like a bad idea to me. It seems more likely that the manifest was off-by-one by mistake.
 Closing.
  One of the possible reasons is that the seek is frame accurate. This means that if we seek into a non iframe, we need to decode and discard buffers until we are able to decode the sought frame. However we have perceived some difference with the ExtractorSampleSource, so we will look into this. I cannot guarantee this will happen soon, though. Perhaps @ojw28 can provide some insight later. 
 For HLS playbacks, ExoPlayer will always re-buffer when a seek is performed. We may optimize this at some point in the future, but it's non-trivial. For DASH and SmoothStreaming optimizing seeks within the buffer is more straightforward, and ExoPlayer already optimizes for these cases.
  Sorry for the delay! Feel free to ping me if I take too long to respond an issue (there were a few similar bugs available that were waiting for the fix to be pushed. I thought this one was one of them). This is a confirmed bug. Takes place in a specification gray ground. A fix will be available soon, at which point I will close. Thanks for the report.
  Have you worked around the fact that the transport stream do not contain audio stream packets somehow? Because it will not play otherwise. One quick way to see whether this is the problem is to comment out the [TS_STREAM_TYPE_AAC](https://github.com/google/ExoPlayer/blob/b228ccd8d27f66f63e1c7c9983a5002efa43a4b5/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L343) branch to avoid the AdtsReader from being created. If it plays after that, then there is your issue: the stream is malformed. Please let me know if this solves your issue.
 As per the end of https://github.com/google/ExoPlayer/issues/1056, `java.net.ProtocolException: unexpected end of stream` usually indicates an implementation error on the server side. Specifically:

> I think this is indicative of a server providing a Content-Length value in a response header that does not match the length of the response body (the body turns out to be shorter). If my understanding is correct, that kind of behavior is invalid from the server.
  Thanks!
  As stated in the issue template, please provide content to reproduce the issue. It is impossible for us to work with the stack trace alone.
 @hoseinit, please provide a URL. I need whatever you are using as "selectedVideoLocation".
 I am sorry, but we need a specific media file to check this. We will not be able to guess which format container you are working with, or which device this is. Exoplayer certainly plays files bigger that 30 MB, so I don't think that is a significant metric.

To sum up, to help you we need to be able to reproduce the problem ourselves, so we need the media sample and the device, if the file is not enough. I assure you, I cannot solve the problem with the provided information.
  Issues #26, #1216, and others refer to this. I am sure they will provide some insight.
 #571 also refers to this.
  I don't think that's correct. I'd expect the exception message to read "EACCES (Permission denied)" if the permission were missing. I think it's more likely that the path is incorrect.
  As stated in the issue template, please provide content to reproduce the issue. It is impossible for us to work with the stack trace alone.
  I don't think MediaPlayer exposes a way for you to get at the pixel width aspect ratio.
 It's parsed out of the media stream by ExoPlayer's extractor implementations. You should be able to look through the ExoPlayer source code to find where it originates from in each of the extractors.
 It depends on the individual container format. Please look at the source code to answer this question yourself for whatever container you're interested in.
  Good question. In older versions of Android secure decoders were not explicitly listed, and it was by convention that secure decoders were obtained by taking the regular decoder name and appending ".secure".

In MediaCodecUtil we assume the existence of a secure H264 decoder on these older versions of Android. Note however that MediaCodecTrackRenderer will only attempt to instantiate a secure decoder if the MediaDrm implementation states that it requires one. If this is true, it really should imply that a secure H264 decoder really does exist, because it doesn't make any sense for a device to ship with a MediaDrm implementation that requires a secure decoder if no such decoder exists. Note that H264 was the only codec for which secure decoders were being supported at this point.

My hunch would be that on the affected device the MediaDrm is incorrectly claiming that it requires a secure decoder, when in reality it doesn't. If that's the case then the fix should be to override that, so that no attempt is ever made to instantiate the secure decoder. This should be pretty easy to do, if that's the issue.

To help debug the issue, could you please modify StreamingDrmSessionManager.requiresSecureDecoderComponent to include the following logging:

```
Log.e("DRM", "fingerprint=" + android.os.Build.FINGERPRINT);
Log.e("DRM", 
    "requiresSecureDecoder=" + mediaCrypto.requiresSecureDecoderComponent(mimeType)
    + ", securityLevel=" + getPropertyString("securityLevel")
    + ", mimeType=" + mimeType);
```

Try playing some DRM protected content on the affected device, and reply to this thread with the logging that gets output.
 One other question - I assume you're not doing anything to deliberately lower the security level of the MediaDrm implementation (if you don't know what this means, the answer is no :)).
 To answer your second question - You shouldn't do that. There are a number of scenarios where that will cause playbacks to fail unnecessarily. This is true in all cases where the keys require a secure path. It may also be true in cases where they don't, since device manufacturers are not required to support this combination/mode and there are no device certification tests to enforce behavior in this case. Please follow up as per above, and we can figure out a better solution for you :).
 It appears that version of the spec was only released a month ago. It doesn't seem too surprising that software that shipped before the standard doesn't fully support it! Please follow up with Widevine directly regarding test vectors. They also have their own set of tests, so if this is something that's expected to be supported they should have tests covering this case.
  i have played back 8 channel audio streams with libopus renderer successfully in the past. can you please share the file with me and i will take a look?
 @ojw28 could you please forward it to me? thanks!
 @vigneshvg I've forwarded it. Thanks for taking a look.
  I am using Exoplayer version 1.5.6 in my application having DASH based Streaming with Widevine. It is working properly for most of my devices however for one of the Nexus5 device gives the following 

Even the Sample Application(Widevine DASH Policy Tests) is giving the same error in this device.

Exception:

```
02-26 16:50:40.034 17633-17633/com.jio.media.ondemand E/EventLogger: playerFailed [1.60]
                                                                     com.google.android.exoplayer.ExoPlaybackException: android.media.MediaDrm$MediaDrmStateException: Failed to handle key response: DRM vendor-defined error: -2902
                                                                         at com.google.android.exoplayer.MediaCodecTrackRenderer.shouldWaitForKeys(MediaCodecTrackRenderer.java:715)
                                                                         at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:649)
                                                                         at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:492)
                                                                         at com.google.android.exoplayer.SampleSourceTrackRenderer.doSomeWork(SampleSourceTrackRenderer.java:129)
                                                                         at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:431)
                                                                         at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:213)
                                                                         at android.os.Handler.dispatchMessage(Handler.java:98)
                                                                         at android.os.Looper.loop(Looper.java:148)
                                                                         at android.os.HandlerThread.run(HandlerThread.java:61)
                                                                         at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
                                                                      Caused by: android.media.MediaDrm$MediaDrmStateException: Failed to handle key response: DRM vendor-defined error: -2902
                                                                         at android.media.MediaDrm.provideKeyResponse(Native Method)
                                                                         at com.google.android.exoplayer.drm.StreamingDrmSessionManager.onKeyResponse(StreamingDrmSessionManager.java:392)
                                                                         at com.google.android.exoplayer.drm.StreamingDrmSessionManager.access$800(StreamingDrmSessionManager.java:47)
                                                                         at com.google.android.exoplayer.drm.StreamingDrmSessionManager$PostResponseHandler.handleMessage(StreamingDrmSessionManager.java:483)
                                                                         at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                         at android.os.Looper.loop(Looper.java:148) 
                                                                         at android.os.HandlerThread.run(HandlerThread.java:61) 
                                                                         at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40) 
```

The Current OS version is 6.0.1. Just wanted to understand what and where exactly is the issue. 
 I will try this with Widevine DASH Policy Tests sample, but if you could also provide a sample to reproduce this to dev.exoplayer@gmail.com, that would be good.
  Here's another mp4 parsing failure. `srcPos=-8 dst.length=8192 dstPos=0 length=8` looks suspicious ;).
 The QuickTime format has a very peculiar format for the udta atom(not present in the MP4 format). A fix/workaround will be available for this soon. Thanks for the issue!
 @dgreenhalgh, If you are 100% sure that this bug is affecting you,  @andrewlewis suggested, one quick workaround would be to remove TYPE_udta and TYPE_meta from the shouldParseContainerAtom and shouldParseLeafAtom methods in the Mp4Extractor class.

The fix will very likely be available next week in the dev branch.
  There is no functionality to do this, sorry.
  I am not perfectly sure what you want to do. But using your own `MediaCodecSelector` implementation should allow you to select the decoder you want.
  Thanks for this. It might be a couple of weeks before we get around to looking at this properly and getting it merged (holidays etc), but from a quick scan it looks very helpful!
 Added a bunch of comments. Looks like the right approach in general. I think a little fine-tuning may be required though! See in particular the comment about paint-on caption mode.
 Hm, something weird happened to this pull request that's resulted in the version I previously commented on being no longer available. Did you do a force push or similar? For future reference, it's preferable not to do this. I can't find any way of looking at the delta between that version and the latest one as a result :).

That aside, apologies for the delay. If you could remove all the debug code + code that's unnecessary for now, then I'll patch it in and give it another detailed look. Thanks!
 Not sure how else you should have done it; I don't work too much with Git/GitHub directly :)! The simplest solution is presumably just to not fast forward your fork (since it's a feature branch, there's no particular reason why you need to continually merge the latest unrelated changes into it).

I'll take a look tomorrow.
  Sorry for the delay. We need you to provide a way to reproduce the crash ourselves, as stated in the issue template.
 I will see what I can do. I will write back soon.
 Should have assigned it to myself, I lost track of this. I doubt we will be able to do anything about the error itself, considering you are not able to reproduce this (and looks device specific). It is strange, though, that it happens with any DRMed content, it would be helpful for us to know what "any" means in that context.

On the other topic, the error callback method, I doubt this is a good way for us to go. The exception, being an IllegalStateException, shouldn't be in the category of "things that should be handled". In the application point of view, I guess you want to fail gracefully, which I understand. But in the library point of view, you don't want to add error handling for states which are expected not to occur. 

Considering this is a very specific and not reproducible error, I think this should be handled on application code as elegantly as possible, I guess you could use your own patch if needed. I don't think modifications to the library of this kind are a very sound design, it would mean that the thrown exception is not appropriate. @ojw28 will have the final word on this.
 A full ADB bug report will be helpful, however if this is not easily repro'ed we will need close as 'no repro'.
 Sorry can we get an ADB Bugreport?  The log does not provide enough data.
 The fragment is not sufficient, hence the request for a full bug report. Making assumptions about which parts of the report are relevant is almost always not the best thing to do, compared to just giving us all of it ;).
 Hi @evorotilova  any full logs?  If not we will consider the case closed in  7 days.
  That's a segfault as opposed to an out-of-memory, as far as I can see. I doubt we can do much to fix it; given it's quite deep in the platform. Speaking of which, judging from the build fingerprint (`TECLAST/p90hd/p90hd:4.4.2/KOT49H/eng.hy.20140822.083942:eng/test-keys`) it looks a bit like you're running a non-standard build. Are you sure that's not the source of this problem?
 It's a segfault in the platform. It may or may not be due to the special build that's running on your device; it's not possible to say. Closing in any case. Even if this happens on release builds for this device, there's probably not a whole lot we can do to fix it.
  The dump plays fine for me using the latest release (1.5.6). Closing because I cannot reproduce, and because you haven't provided sufficient information as requested in the issue template. Please file an issue containing all of the requested information.
  Please supply test content. Thanks.
 @AquilesCanta - This is an interesting one to look at.
 Confirmed bug. The fix will be available in the next release, I guess. Many thanks!
 Will close again once the fix is available.
  If you need to use DRM, you could try forcing L3 Widevine through

StreamingDrmSessionManager.setPropertyString("securityLevel", "L3");

I wonder if this solves your issue, I will not be able to try it out myself, unfortunately, so I depend on your response.
 There is some content [here](https://medium.com/@cinemur/exoplayer-for-building-powerful-vr-players-for-cardboard-and-gearvr-73ec7e83dd5c#.52ou4a6su) that might be useful. I'd also suggest trying Stack Overflow or similar. This tracker is reserved for feature requests and bug reporting.  Sorry, we're a small team, and we don't scale to providing Q&A on this issue tracker. I'd suggest asking on stackoverflow.
  Yes, ExoPlayer supports all device architectures. If you want further assistance please file an issue that contains all of the information requested in the issue template.
 @gongshoudao It is impossible to help you with that amount of information. Also, since the issue is closed, please file a new issue including all information requested in the issue template.
 Simulators often don't fully implement the media stack that would be required on a real device. For media, it's better to test on real devices.
  Yes, this is a known issue. It stems from the fact that frames can be out-of-presentation-order in the video stream. Caption data is embedded, and so is similarly out-of-presentation-order.

The video frames are reordered by the platform's video decoder, but for caption data we have to do the reordering ourselves. Our approach is sometimes a little too aggressive at the start of playback, and can end up processing caption data too soon only to have data with an earlier presentation timestamp arrive shortly afterward.
 @bit101 - It's still theoretically possible in V2, but it's less likely to occur than it was in V1. Regardless, it shouldn't occur during ongoing playback; this issue is specifically about the start of playback (or equivalently, immediately following either seeking or buffering). Let's track your issue with #2070.
 This is much better in V2; I think it can be closed.
  Thanks for the detailed report. I tried to reproduce this with a Galaxy Note 4 running LMY47X, using the first set of steps that you describe in the demo app. Playback resumed successfully in all cases for me.

Does it definitely reproduce reliably on Galaxy Note 4? Is it 100% reproducible for you? Could you provide a bug report taken from that device, since that'll be most similar to the device I'm trying with. Thanks!
 I guess "Rebooting the device seems to stop the issue for a few hours. Not sure if attempting to play many videos causes the issue." could explain why I'm currently unable to reproduce, given I've only just turned the device on.
 The logs do not provide a lot of information, but things look to be working as intended. 
There is a timing issue, in both the logs for 'G4' and in the 'Note 4'. There is a decrypt call as keys are being loaded. This can/will cause decrypt errors.

There are methods to determine when keys are available for decrypt, these are include in API #23 and newer.  Register for MediaDRM.OnKeyStatusChangeListener. @wvpaf - After ExoPlayer calls MediaDrm.provideKeyResponse, it assumes that the keys provided will be available straight away. Are you saying that's not the case (i.e. that MediaDrm.provideKeyResponse is asynchronous and we need to wait for a callback)? Where is that documented if so, and how can we know when the keys become available prior to API level 23? If not, then I'm not quite sure what "being loaded" means. Could you clarify? Thanks. MediaDrm.provideKeyResponse is synchronous and keys will be available before the call returns for all API levels where WV is present. 
If decrypt is called before provideKeyResponse returns this will result in CryptoException.

Hope this helps.    It's beyond the scope of this issue tracker to be advising on configuration on your serving side infrastructure. If you can produce some sample content that you believe should be supported and does not play, please file an issue linking to that content and providing full details as requested in the issue template.
  Closing because information as requested in the issue template is missing. Please file a new issue with complete information, thanks.
  The file contains a Xing header that's missing a table of content. If there were a table of content, the file would be seekable.

I doubt MediaPlayer is doing a particularly good job of allowing seeking. It's most likely indexing the entire file, which will significantly increase startup latency and will not scale properly to large files. In ExoPlayer we've opted to only support seeking in files that actually contain the information necessary to seek in an efficient and scalable way.

@andrewlewis - As an aside, I briefly wondered whether we should support seeking in this kind of file using a constant bitrate assumption. I gave it a try but seeking was inaccurate by over 40s in some cases for the provided sample, so I'm pretty sure this isn't a good idea. Let me know if you disagree though :).
 We won't be making it work like MediaPlayer, for the reasons specified above.

We should allow an application to query whether the media is seekable, so that it can avoid calling seekTo if the media is not seekable except in the case where it wishes to restart the playback. I've filed #1300 to track this.
 Exactly the same case (Xing header that's missing a table of content). Just providing the source media is best. That kind of output is normally not sufficient to debug an issue by itself, and if we have the source media we can run mediainfo/ffprobe etc on it ourselves if we need to. Thanks!  We don't support key rotation yet, but it's on our radar.
  The short answer is: No.

You can play anything to which you have a direct URL to the media. YouTube don't expose direct URLs, so you'd have to use their [player API](https://developers.google.com/youtube/android/player/) instead. Of course, you could host DASH content yourself, as you're doing for HLS, which would provide you with the direct URLs you need.
  The media is strange, in that many PES packets within the stream don't declare pts timestamps. In other words, `ptsFlag` is false [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L593).

It's probably possible to work around the issue specifically where the first PES packet does declare a pts timestamp and where the frame duration is known. Fixing wont be high priority, however.
 @AquilesCanta - This would be a good thing to fix. In the case that PesReader sees a packet where ptsFlag isn't set, it sets timeUs to 0 and ends up propagating this to its payload reader. It should probably propagate a "not set" constant of some kind. This would allow the payload reader to increment time by itself in the case that it knows the sample duration (as is the case here).
 A fix will be available in the next release.
  Hm. That's odd, I would have thought that would break a whole bunch of things. Do you have a way of confirming whether that's the issue?
 You'll be able to query them directly from code one way or another. [This post](http://stackoverflow.com/questions/9332522/androids-proxy-confusing-documentation-resources) has some suggestions as to how you can do this.
 Closing due to inactivity. It's unclear that this is something ExoPlayer should be handling or concerned about.
  If you want to play H264 without any IDR frames then you need to enable the workaround flag explicitly. If you have control over the generation side of these streams then you should include IDR frames. Aside from that, this issue is the same as #1292.
 Please file a new issue. Please include all of the information requested in the issue template. Thanks!
  Neither mpeg2 nor mpa2 are officially supported by Android. If this is an nVidia Shield specific issue with their optional additional support for these codecs, then you should file the issue with nVidia in the first instance.
  What's the use case for playing 19Mbps mpeg-4 part 2 video? That doesn't seem like a particularly sensible way to be encoding video for delivery to mobile devices...
 Is this a device that you're developing directly (i.e. you control the platform software)? The root cause of the failure is that the mpeg-4 part 2 decoder isn't allocating sufficiently large input buffers. So if you do have direct control, you should fix the problem in the platform.
 For comparison, OMX.qcom.video.decoder.mpeg4 on Nexus 5X allocates 1576960 byte buffers for this video. OMX.google.mpeg4.decoder allocates 3110400 byte buffers. On the affected device, it appears that the buffers are only 65536 bytes, which is far too small.

We've worked around the issue of too small input buffers for some other formats in MediaCodecVideoTrackRenderer.maybeSetMaxInputSize, where we set an explicit minimum at the application layer to pass down. We could add the following case for mpeg-4:

```
      case MimeTypes.VIDEO_MP4V:
        maxPixels = maxWidth * maxHeight;
        minCompressionRatio = 1;
        break;
```

which would match what Google's software decoder does. We might want to set minCompresisonRatio to 2 just to try and keep the allocation size under control a little.

Please try adding the above to MediaCodecVideoTrackRenderer to see if it resolves the issue for you. Although if you do have control over the platform, it's still preferable for you to fix the issue in the platform itself, so that the buffers are sized large enough for non-ExoPlayer based playbacks.
 Good to know. If you could test the fix suggested above, that would be helpful. We can merge that independently (and in addition to) any fix directly in the platform. The platform fix would probably need to come from MTK, assuming it's their decoder.
 @andrewlewis - What do you think a sensible value for minCompressionRatio would be for this case? I'm somewhat worried about using 1; it seems a little excessive. I was wondering about using 2. It appears the decoder on Nexus 5X uses 2. Feel free to apply a fix with whatever value you deem best. Should we do something for H263 whilst we're at it, to avoid the inevitable future bug?
 minCompressionRatio of 2 sounds fine, and H.263 can probably use the same.
  I think this is fixed in the dev branch already, in https://github.com/google/ExoPlayer/commit/30a250fb43b7f0a8f9729f62ec24fbc91e09adff.
 We'll ship a release with this soon, yes.
  I don't understand what's being asked here. ExoPlayer already supports playback of URLs, and its demo application contains multiple examples of this.
  That's odd. Are you able to provide the link to the actual mpd, as opposed to just attaching it, so that we can reproduce the issue?
 We'll need a working URL to debug the issue. Whether it's the original one or a copy doesn't matter, but the media needs to exist as well as the mpd. You can email a link to `dev.exoplayer@gmail.com` if you'd rather do that than post one here. Thanks!
 The media here is pretty unusual. The player's doing what it's doing because the initialization chunk contains a sidx boxes (an index of the chunks that among other things specifies each chunk's offset and length). This is unusual. There's no value in the sidx boxes given the mpd is listing the chunks using a SegmentTimeline, and the range offsets don't apply. The player gets confused, tries to use the sidx index rather than the mpd, and playback fails. ExoPlayer should probably prefer to use the mpd in this case; I'll have a think about that. Although on your side, unless there's a good reason to keep them, you might want to consider removing the sidx boxes from your initialization segments, which should fix the issue for you.

As an additional point, it appears that this content is DRM protected beyond ~10s. I would expect the mpd to include ContentProtection elements to indicate this.
 The change above removes the need for you to comment out sidx box parsing in the extractor.
  It sounds like this device does not allow more than one player to be instantiated at a time. In general Android devices don't have to support playing more than one video at a time, and whether it works can also depend on the input videos' formats. So it is important to release a player before creating a new one.

Based on your latest reply I will close this for now, but let us know if there is still an issue that needs to be fixed.
 @Ltty Your issue may be different. Please supply a link to the stream you are trying to play (emailed to dev.exoplayer@gmail.com or in a reply here). Based on the logs, it looks like the video resolution (1500 \* 1500) may be too big for the device -- do smaller videos play correctly?
 @CAMOBAP795 The [Android M CDD](https://source.android.com/compatibility/6.0/android-6.0-cdd.pdf) (and older versions) do not require compatible devices to support playing more than one video at a time, and I don't think there are any [CTS](https://source.android.com/compatibility/cts/index.html) tests that fail if using two video decoders at a time doesn't work.

So it is not safe for apps that need to work on all Android compatible devices to rely on this feature. However, I will try to make some suggestions about using hardware codecs anyway:
- As I mentioned above, whether you can play two videos at once can depend on the videos' formats. The actual limitation may apply to the total macroblock rate of all decoded videos, for example.
- In practice you will find you can play multiple videos at once on many devices, especially 'higher end' ones (even going back to Android K generation devices). At the moment, it appears the majority of devices running builds >= L support at least two 720p playbacks concurrently, though I'm afraid I don't have any documentation to prove this.
- On Android versions before L, I have seen bad device-specific failure modes when trying to create two video MediaCodecs at the same time on devices that don't support it. For example, the device may have to be rebooted to make video work again after trying to create two video decoders, even if the app released resources correctly, or the mediaserver process may crash due to an assertion failure.
- Android M added [CodecCapabilities.getMaxSupportedInstances()](http://developer.android.com/reference/android/media/MediaCodecInfo.CodecCapabilities.html#getMaxSupportedInstances%28%29) which you can use as a hint, but, as documented, the number of concurrent instances may be lower depending on what resources are in use. To understand exactly what max supported instances value means, look at the [CTS test](https://android.googlesource.com/platform/cts/+/marshmallow-cts-dev/tests/tests/media/src/android/media/cts/MediaCodecCapabilitiesTest.java#626) for the method. (Note that this is not testing that creating > 1 decoder works -- only that the max supported instances value is accurate.) See also [MediaCodec.CodecException](http://developer.android.com/reference/android/media/MediaCodec.CodecException.html)'s new constants in Android M. These API changes should make it possible to detect the capability before trying to play multiple videos and fail gracefully if it doesn't work (note that the CTS test fails on devices which fail in any way other than ERROR_INSUFFICIENT_RESOURCE).
- See also #273 which mentions using software codecs as an alternative.
 Closing due to lack of required information on the original issue.
  I'm not referencing that issue. I referenced this issue from that issue (i.e. the other way around ;)). @atul14 - Things would be less confusing if you didn't cross-post the same thing in multiple places. Thanks!

Regarding this issue - We'll take a look when we have a chance, although any additional debugging work you can do to resolve the issue directly would be much appreciated.
 I could reproduce the issue on a Nexus 5X. I'm not convinced that this isn't just a serving issue. The stream provided above quite often returns a 400 response code rather than a valid response, so the server seems pretty flaky. How certain are you that this isn't simply a connectivity issue (hence transitioning into the buffering state whilst the client attempts to recover the connection, followed by idle in the case that recovery wasn't possible)?
 I don't think the "Discontinuity Detected" log messages are relevant to this issue. Instead, I think the issue occurs when the client's connection to the server is broken. ExoPlayer tries to recover, and one of two things happens:
- ExoPlayer fails to re-establish a connection, an error is propagated and the state transitions to idle. This is working as intended.
- ExoPlayer succeeds in re-establishing a connection, but the player gets stuck in a buffering state. There's an issue with how we're recovering the connection where the player incorrectly gauges how much media is buffered, causing it to get stuck thinking there's not sufficient media for playback to resume.

I think the second possibility is what you're seeing. If it is, you can work around the issue by ensuring that you always pass 0 as the third argument to `ExoPlayer.Factory.newInstance`. This should allow for successful recovery in the second case. Please could you test this and verify that it fixes the issue for you? We can investigate a proper fix in due course, but it might take a while.
 By the way, the issue isn't device specific. It can be reproduced across all devices fairly easily as follows:
1. Play the stream for ~5 minutes.
2. Quickly toggle WiFi off and on again using the pull-down drawer. This will force a connection break.
 Continuous stuttering will occur with the workaround in place specifically when there is insufficient bandwidth between the server and device. This is because the workaround is to set `minRebufferMs=0`. This is the duration of media that needs to be re-buffered for playback to resume after buffering. When it's set to 0 toggling between buffering and playing can occur rapidly if media is being fetched but at insufficient rate.

Once a "proper" fix is in place it will no longer be necessary to set `minRebufferMs=0`. The default value of `5000` will cause less frequent toggling between buffering and playing. Note however that playback will not be smooth. If there's insufficient bandwidth then there's not much we can do.
 This issue is fixed properly in the `dev-v2` branch.
  You must be implementing `MediaDrmCallback`. You should have your implementation of `MediaDrmCallback.executeKeyRequest` check the response. If it's an error response this method should construct and throw an appropriate exception, which you can define yourself. Playback will then fail with this exception as the cause.

To provide more detail, the way I've seen this implemented elsewhere is for the license server to return a response whose body contains a status. For example:

```
Status=X
<data>
```

Upon obtaining the response, `MediaDrmCallback.executeKeyRequest` will parse the response and look at the status. If it's ok, it'll return `<data>`. If it's not ok, it'll construct and throw an appropriate exception. Note that the license server response is something you can design yourself, so you can put any information in there that you want to include in the exception.
 You don't have to return a byte array. You should be throwing an exception, meaning you don't need to (i.e. can't) return anything.
 Great!
  This is tracked by https://github.com/google/ExoPlayer/issues/73.
  It's not really our job to debug your code, particularly when it's not even cleaned up (i.e. contains commented out code blocks that make it harder to read) and when you haven't provided a full stack trace of the failure ;). Why don't you attach a debugger, work out what the null object is, and then work backward from that, rather than filing an issue?

You're probably passing a TrackRenderer[] into ExoPlayer.newInstance that contains a null, which you shouldn't do (the demo app doesn't do this, for example).

Please include all of the information described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) in future issues. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  (a) This is easy to answer just by looking at the code? It's assigned in four methods. In each place it's assigned the value of a method input argument. In all cases the method input arguments are documented in the Javadoc. From this information, you can conclude that downstreamPositionUs is the current playback position of the player.

(b) getLargestParsedTimestampUs is documented in the Javadoc to return Long.MIN_VALUE in the case that a sample has yet to be received. In other words, there are no samples currently buffered. In such cases it makes sense to return a buffered position equal to the current playback position, since this gives a buffer duration of (buffered position - current position) = 0.
  ExoPlayer will render video to any `Surface`. You could use `SurfaceTexture`, at which point you'd have video rendered into an OpenGL ES texture. Once you have that you can do anything that OpenGL lets you do, including using a pixel shader to transform the video to black and white. How to achieve that is beyond the scope of this issue tracker, however.
  That method calls another `doSomeWork` method that you can override. Is there a reason why you can't override that one instead?
 It is vital. That's a good reason to not allow someone to override it (because they might forget to call through to super.doSomeWork). You should implement the other doSomeWork instead.
 What I'm saying is:
- The work done in `final void doSomeWork(long, long)` is vital. If you remove final from that method, as proposed here, it would allow someone to make the mistake of overriding it and failing to call `super.doSomeWork(long, long)` from their override. This is why it's final.
- You should just be able to implement `doSomeWork(long, long, boolean)` to do what you need. You shouldn't need to override `doSomeWork(long, long)`.
  ExoPlayer currently selects the first variant listed in the master playlist. If I remember correctly, this is what Apple recommends/specifies as correct client behavior. If you want to start in the lowest quality, you should technically have your server generate the master playlist with the lowest quality listed first.

The above aside, we do agree that it makes more sense for the client to make the initial variant selection locally, as opposed to the recommended behavior. We'll be moving HLS over to use FormatEvaluator in ExoPlayer V2, which will give more control over the initial selection (and over the adaptive algorithm in general).
 I'm not sure what you mean when you refer to the seamlessness of the transition. The variant transitions are already (more or less) seamless.
 If you try and switch too soon then you have very little bandwidth data on which to reliably base the decision. Also, because HLS sucks, you also need to download overlapping chunks of media. This further increases the probability of a re-buffer should you switch too soon. You're better off just selecting the variant you want up front (e.g. based on whether there's a WiFi connection, or similar) than trying to switch up very quickly. This will be possible in 2.x.
 The answer depends on what level of risk you're comfortable with, and what it means for it to be available :). It's already available in experimental form [here](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental). We have some major changes coming next week, so I'd definitely hold off until then. Once those are pushed, probably toward the end of next week, we'll remove "experimental" from the branch name. It will still be a dev branch and not a release branch. Once that happens, you'll need to decide what level of risk you're comfortable with in terms of whether you want to start looking at it or wait a little longer. At Google, we've just started to transition the first of our internal apps over, if that helps you to assess.
 Please, do open a new issue. But please be clearer about what you need. For example, why ExoPlayer.getCurrentTrackGroups() does not provide the information you need. @ram992, you can provide your own `TrackSelection.Factory` to `DefaultTrackSelector`. Before implementing one from scratch, have a look at `AdaptiveTrackSelection.Factory`. Its parameters may provide enough customization for your case. 

Can this issue be closed? It seems to me that #2353's commit fixes this. As I mention in my previous comment:

> Before implementing one from scratch, have a look at AdaptiveTrackSelection.Factory. Its parameters may provide enough customization for your case.

From what you mention, it should be enough. For general programming questions I'd suggest trying Stack Overflow.  This is tracked in https://github.com/google/ExoPlayer/issues/73. If you have any sample streams that we can use to test this, please provide them in that issue. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I really don't buy that this is worth doing. If the media is under your control and you want to allow seeking, you should _really_ use a more appropriate container format.

If the media isn't under your control then it's a little more debatable, but I'm not sure that this is a priority for us; it's unclear what the actual use case is where a user is trying to play their own MPEG-TS stream, or whether we want to go out of our way to support it. If I were to guess, I think this kind of use case would make up a tiny fraction of total ExoPlayer use.
 What benefits do you obtain from using mpegts, exactly?
 But you can't actually start playback efficiently from a specified position in middle of the stream, since MPEG-TS isn't efficiently seekable. The exception to this rule would be if you have some server-side magic where you can specify the start position using a query parameter in the request, or similar, but given this pull request I'm guessing that's not the case for your setup.

DASH, SmoothStreaming and HLS all solve exactly this problem by segmenting the media into chunks and providing manifests that index the chunks in a seekable way. Moving to one of these (preferably DASH) is the correct way to achieve what you're trying to do. What are the benefits of your mpegts solution v.s. moving to DASH, or is it just a matter of implementation complexity / having to do some work?
 Fair enough. This isn't something we'll be supporting though. In general, if you want seeking, you should use a streaming technology that's designed to support it, as opposed to using an unsuitable technology and then papering over the cracks in the client players.
  A screenshot without context isn't particularly helpful. What's that screenshot actually showing? Is it a path from GC roots, or something else? What version of ExoPlayer are you using?

@AquilesCanta fixed a resource leak in 1.5.4 (and later) in https://github.com/google/ExoPlayer/commit/c31473c5964a6f1faa75934d31d8b333e99e712c, so if you're using a version earlier than that please upgrade to 1.5.4 or later and try again.

Thanks!
 I can't reproduce a leak using the demo app. I tried playing a HLS video, exiting back to the list of samples, playing another HLS video and so on. If there were a leak, I'd expect it to reproduce in this case.

I think the most likely explanation is that your code is not calling `ExoPlayer.release` properly on each instance of the player. Calling this method will cause the thread that's acting as the GC root in the image you've provided to quit, and so the fact that it's still a GC root suggests that this hasn't been done.
 Please re-open if you can reproduce this with the demo app or provide more information. Thanks!
  Yes, you will still be able to install the app on devices running older versions of Android. The minimum API level on which your application will install is determined (only) by minSdkVersion as specified in your application's manifest. You're free to use APIs that are more recent than the specified minimum version, up to whatever you specify as the targetSdkVersion in the application manifest, but it's your responsibility to ensure that use of those APIs is correctly guarded with SDK version checks. As long as you do that everything should be fine.
  Is it possible that the device does not count with a H264 decoder?
 Yes, I think the device most likely does not have a H264 decoder. As per the [Android CDD](http://static.googleusercontent.com/media/source.android.com/en//compatibility/android-cdd.pdf), video decoders are currently optional for Android Watch devices.

I think @andrewlewis has successfully used ExoPlayer's VP9 software decoder extension on Android Watch devices, so that might be something you could look at. If you need H.264, you'll likely need to implement a similar extension that bridges onto a H.264 software decoder.
 The [vp9 extension](https://github.com/google/ExoPlayer/tree/master/extensions/vp9) (and the [demo](https://github.com/google/ExoPlayer/tree/master/demo_misc/vp9_opus_sw) which uses it) should work fine on Android Wear devices. For example, I was able to play a WebM file from the sdcard on a Sony Smartwatch 3 running that demo app, without making any special modifications. You can also use the [OpenGL-based output buffer renderer](https://github.com/google/ExoPlayer/blob/master/demo_misc/vp9_opus_sw/src/main/java/com/google/android/exoplayer/demo/vp9opus/PlayerActivity.java#L143) which improves performance slightly.

Please be careful about battery consumption: in my experimentation, software playback of a video used 100% battery in about an hour! So this is only really suitable for short clips.

Last time I checked it was not possible for an app to make direct HTTP requests from Android Wear. So media needs to be pushed as an asset or as part of the microapp, or streamed from the paired device. In the latter case, we found that the [channel API](http://developer.android.com/training/wearables/data-layer/index.html) worked well.

Aside: many Android Wear devices do actually have video decoding hardware, but support for using it is not included in the Android system images.
 Regarding testing on different watches: I wouldn't expect video decoding via the core ExoPlayer library to work on _any_ Android Wear devices, because it uses MediaCodec to access device codecs. Even though many devices have the hardware for it, the Android system image does not expose it to the system or apps.

The VP9 extension should just work (on any device), because it bundles a software decoder with the library. Thanks!
  This is supported in `dev-v2`. You'll need to build a `MediaSource` for each song that you want to play, then build a `ConcatenatingMediaSource` that wraps the 10 individual `MediaSource` instances. If you then provide the `ConcatenatingMediaSource` to the player, it'll play the tracks seamlessly, maintaining a consistent buffering policy across the transitions.
 You could implement your own `MediaSource` similar to `ConcatenatingMediaSource`, but backed by something like a `Cursor` (or whatever). Being able to lazily add to a `MediaSource` as a direct feature of the library is tracked by https://github.com/google/ExoPlayer/issues/1706.  I'm not sure I fully understand the question, but it falls outside the scope of this issue tracker. I'd suggest asking on StackOverflow.
  Yes, we should do better at this. Although it is somewhat unusual to have this requirement vs. resuming to the live edge, which is what ExoPlayer does currently. Could you clarify the use case where what you're doing makes more sense, for my education? Thanks!

p.s. As a temporary workaround to avoid seeing a frame at the current head position, you can always just put a fully black view over the top of the Surface until you've got the player where you want it and into the ready state.
 I think this is most likely fixed in V2. Please give it a try and see what you discover. Thanks!
  Sideloading subtitles in 1.5.5 is simpler than it was previously. As an example, if you have two WebVtt subtitles that you want to sideload, you can create a renderer like:

```
    SampleSource[] sampleSources = new SampleSource[2];
    sources[0] = new SingleSampleSource(...);
    sources[1] = new SingleSampleSource(...);
    TrackRenderer textRenderer = new TextTrackRenderer(sampleSources, player,
        player.getMainHandler().getLooper());
```

Use `ExoPlayer.getTrackCount`, `ExoPlayer.getTrackFormat`, `ExoPlayer.getSelectedTrack` and `ExoPlayer.setSelectedTrack` to query the available subtitles and switch between them.
  We have a few test samples with captions of this type, and they're working fine for me. Please could you provide a link to test content so that we can reproduce? Thanks.
 We don't support DRM protected captions. See https://github.com/google/ExoPlayer/issues/718.
 Although your non-working caption file doesn't look much like it has DRM protection...
 It should be fine just to share the protected version in that case; I can just disable the video and audio parts of the playback and look at why the subtitles are problematic.
 Are the segments geo-restricted to some region? I keep getting 503 errors when I try and obtain them. Thanks.
 I was able to get at them through a US proxy. I can't reproduce the problem though. To test them I did the following:
1. Checked out 1.5.5 (i.e. the current master branch).
2. Added your URL to `Samples.java` of the demo app with type `TYPE_SS`.
3. In `SmoothStreamingRendererBuilder`, commented out the blocks that create the `DrmSessionManager`, video and audio renderers, and instead set `videoRenderer` and `audioRenderer` to null.
4. Play the sample in the demo app, enabling the text track by tapping on the text button.

It appears that the captions play fine in this environment, having tested the first 5 minutes. Does this work for you? Are you able to play the full media in the demo app if you update `SmoothStreamingTestMediaDrmCallback` to obtain a suitable key response? If so, it feels like the issue must be elsewhere in application code.
 This is actually a content issue, not an issue with ExoPlayer. Spaces in URLs are a bad idea in general (i.e. just don't do it). An example of a related issue on this tracker is https://github.com/google/ExoPlayer/issues/871.

For SmoothStreaming in particular, the specification explicitly states that you're not allowed to put unsafe characters into the Url attribute. If you drill down through the spec far enough you find that the part of the Url attribute that contains a space in your manifest is defined to consist of:

> URISAFE_IDENTIFIER_NONNUMERIC: A nonnumeric identifier that is safe for use in data fields
> that are part of a URI [RFC2396].
 Well, the content _explicitly_ violates the SmoothStreaming specification. They also don't need to do a any re-encoding; they just need to adjust their manifests and rename the files?

If you have data to back up "not uncommon" and who these "different parties" are, then I guess we could discuss further. We haven't had any other major video providers come to us with this issue; most likely because they all correctly adhere to the specification.
 It may also work (and be specification compliant) to leave the files where they're located and simply update the manifests such that the url strings are already properly encoded. Don't take my word for it though!
  I can't say that there isn't one anywhere, since anyone could have written one. But I can say that there's nothing officially supported.
  The demo app has examples of playing m3u8 HLS playlists; please take a look and use that as a starting point.
  You'll need to provide more information if we're to look at this. We'll probably need an accessible test stream. Thanks.
 Please send to `dev.exoplayer@gmail.com` when you get round to it; thanks.
 The issue with the `test4.ts` file is that the H264 stream doesn't contain any access unit delimiters. We currently rely on the existence of access unit delimiters to split the stream up into frames. They aren't strictly required, according to the spec. The audio in the file is audio/mpeg-L2, which I don't think many Android devices include a decoder for.

Given the latter point and depending on what devices you're targeting, you may need to do some transcoding work regardless to get the audio into a suitable format. In which case you should probably insert AUDs into the video stream as part of that process.

I'm unsure whether it's worth the effort for us to support H264 streams that don't contain AUDs. Any thoughts @andrewlewis (context: this is in ts.H264Reader)?
 It should be possible to alter H264Reader so it doesn't rely on AUDs to identify samples. This would be consistent with our H265Reader (though detecting the start of an access unit is easier for that format). I'll take a look at implementing this.

Regarding the audio stream, if you can find a suitable software codec it may be possible to use that with ExoPlayer. The [opus extension](https://github.com/google/ExoPlayer/tree/master/extensions/opus) is an example of doing this. (Tangentially, we are currently working on reducing the amount of boilerplate code needed to implement extensions like this, but don't have a firm ETA yet.)
 You can now work around the issue described here using the `WORKAROUND_DETECT_ACCESS_UNITS` flag in `TsExtractor`.
  Is this in the context of HLS, DASH, or something else? Please could you provide a little more information about what you're trying to achieve? Thanks.
 I'm not really sure what you're trying to achieve. If you restrict the bandwidth to less than the bandwidth of the media then you'll end up with repeated re-buffers. If you restrict the bandwidth to more than the bandwidth of the media, then it's unclear what you're actually achieving?
 We don't support that, no. It's possible to extend ExoPlayer to add support for this yourself though. A nice approach would be to implement a `DataSource` that wraps another `DataSource`, calling through to its methods but blocking for as long as some flag is set. Note that you'd need to ensure sufficient synchronization around the flag.
  If you're just using normal MP4 files, it's hard to see how a player could do significantly better than what you're doing already. There's always going to be a cost when a user manually switches from one stream to another, unless you would have the player continually buffer twice as much data just in case the user switches (this is not something we support).
  Does the issue reproduce with the streams in the demo app, if you switch it over to using TextureView?
  This pull request was closed automatically because the branch it was targeting has been deleted. We continue to use issue #771 to track this feature request, but it remains relatively low priority for us, and we haven't seen much demand for it.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 This feature is not important enough to be in the demo app, sorry.
  I think the change referenced above should fix this issue, but please could you verify.
  If your code is going to have useless calls on each action, why don't you just have a boolean so that you only do it once?
  As @abhiguru says, this is already tracked in #55 (and we don't currently support RTSP).
  That doesn't look like an error that would affect all devices running 4.1.2. It looks device specific to me. Please provide more information, as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). In particular full bug reports are very useful, as is detailed information about exactly which devices are affected by the issue.
  - Having ExoPlayer handle a missing last segment without failure is tracked in #836, but yes, technically it's a content issue and the encoder should be fixed to not do this.
- The only way to recover is to prepare the player again following the failure; seeking isn't enough.
  This feature request will be addressed as part of playlist support in #1270. Seamless looping should be possible once that is implemented, though further optimizations for this specific use case may be possible.
  ExoPlayer doesn't know anything about `SurfaceHolder`; it only knows about `Surface` directly.

I haven't tried, but it should be possible to have multiple ExoPlayer instances render to the same `Surface`, provided only one of them is doing so at any point in time. When you want to switch player from A to B, you'll need to clear the surface from A using `ExoPlayer.blockingSendMessage` and attach it to B using `ExoPlayer.sendMessage`. Note that the first call needs to be blocking so that you can be sure A really doesn't have access to the surface before you give it to B.
  I don't think we want to do that. There are other things we'll likely need a `Context` for in the renderer classes in the future. Addressing a few points in the article specifically:

> Your JavaDoc is not very useful! Yes, by examining the API I can see that the Mechanic needs Context, but Context is the kitchen sink. What does the mechanic really need? (If you don’t have source code nearby than it may be hard to figure out).

I'd argue that there are cases where a caller shouldn't care what the context is being used for, and that this is one such case. Developers shouldn't need to figure out how to retrieve the refresh rate themselves; and requiring them to do this leaves room for subtle bugs that might occur should they implement it incorrectly. We could provide a util method to help, but there's still room for error. It should also be noted that there's no value in using anything other than the true refresh rate in this case, and hence providing the flexibility to do so does not achieve anything.

> Even if you don’t plan to reuse the code, the Context has high coupling with the rest of the system. Coupling is transitive, this means Mechanic inherits all of the badness through association.

This seems like a throwaway paragraph that doesn't actually mean anything. What is the "badness" and is it "inherited through association"?

The one really solid argument for injecting exactly what you need is ease of testing, but ExoPlayer is pretty far from that goal in any case. In some cases there are real test-ability vs performance trade-offs, and we opt for performance in those cases.
  You can't, I would say. 

ExoPlayer currently is focused on DASH, SmoothStreaming and HLS streaming formats. Besides this there is support for some 'standalone' formats including MP3 which are supported via ExtractorSampleSource. 

You can find the available Extractors registered in the ExtractorSampleSource here: https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/ExtractorSampleSource.java#L103
 Yes, we don't support WAV at this point in time. We may do so in the future, but it's not a priority.
 Opening this to track adding WAV support, since we have an internal contribution under review that will add support.
 Support for 16bit WAV files is now in the dev branch. Changes:
020a9759841da74e983fac48ea1d2e9af0dc7bbe
ef680e9ea6ad1148ab7142156cd5ccf010caaae6
402a00b48bbdf3ccbbda6bc8ae489a7aee3edb9e
41dbb3415022dc8c88e8bee86b27af91be1c2797
 NB - Issue most likely needs to remain open until we also support 8bit and 24bit, at which point it can be closed.
 8bit and 24bit is supported as of https://github.com/google/ExoPlayer/commit/de1ccea3756f28a74922db3a15c92bd68cde3829.
  I'm pretty surprised any device running API level 19 even has an HEVC decoder. It was only a requirement from API level 21. I think you'd be better off using H264; HEVC is pretty new, relatively speaking, and so you shouldn't expect rock solid compatibility with older devices.
 I'm pretty sure the bottom three paragraphs are all covered in existing issues in this tracker.

As for the initial issue here, it's unclear what "pause" means in the context, and you haven't provided a bug report or information about the device. Please file a fresh issue with complete information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
  That video has an H.263 stream but the FlvExtractor only supports H.264/AVC at the moment.
 I don't think there is a quick fix to make it play.

@jeoliva originally contributed the FlvExtractor and may have an idea of what is involved in implementing this. It will be fairly similar to H.264, but it looks like H.263 does not use codec specific data so the width/height would need to be read separately (using ScriptTagPayloadReader to look up width/height elements, perhaps?).
 I don't think there's sufficient demand to warrant spending time on this issue. Closing.
  Overriding `seekTo` just to listen for seek events doesn't sound like the right thing to be doing. The only thing that can invoke a seek operation is your own code, so you shouldn't need to listen for seek events as such. Why don't you just implement listener logic in your own code at the point where you call seekTo on the ExoPlayer instance? In the demo application, this would be equivalent to notifying listeners from `DemoPlayer.seekTo`.
 > I'm working on a library that needs to provide these callbacks out of the box. One of them is a callback for any seek that happens during playback, not just for one particular seek

So encapsulate the ExoPlayer instance with a wrapper, like DemoPlayer in the demo application. Once you have a wrapper you can guarantee that all seek calls must be made through it, which gives you a single point at which to implement listener logic.

> Calling DemoPlayer.seekTo, and then invoking the callback right after, might not guarantee that the seekTo has completely finished by the time the listener is invoked.

What's the use case where it's important that the listeners aren't notified early, given they'll only be early by a matter of milliseconds?
 You're right that it will happen slightly earlier if done this way, but I really wouldn't expect it to make any difference unless you have very specific requirements (i.e. an unusual use case). The listeners will be invoked on a more sensible thread, also. So yes, I'd recommend you do it that way.
 - It's unclear that the requirement you have is sufficiently strict for it to matter. The difference really should be very small. So yes, just invoke the callback in `DemoPlayer`. If that doesn't work for some reason, then feel free to reply again here with more information!
- The motivator behind making more things final to stop people from extending parts of the library that were not intended to be extended, in ways that are likely to be error prone. In my opinion what you were doing sounds pretty error prone (particularly if you were invoking listeners directly from ExoPlayer's thread rather than posting back onto the application's main thread).
  I think this was fixed in 1.5.4, in this change: https://github.com/google/ExoPlayer/commit/030f26fd2d1c381f35bbed0be0f1746be3625ed9. What version are you using?
 Closing. Please let us know if this still occurs on 1.5.4 or later.
  Most likely your custom components are not encrypting/decrypting correctly. Note that your DataSource needs to handle requests for data at arbitrary position offsets. A common source of errors when implementing this kind of thing is not accounting for padding added during encryption. You might like to try using `AES/CTR/NoPadding`, which is pretty well suited to this use case.
  I don't really understand the latest update to this issue. Does playback work with the latest ExoPlayer, or not? You should try removing the block of code yourself to see what happens.
 I'm really unclear about what this issue is actually about. If this issue still occurs on a recent ExoPlayer build, please file a fresh issue containing concrete reproduction steps, a full bug report and sample content to reproduce the issue. Thanks!
  `FrameworkSampleSource` has been deprecated for a long time. You should be using `ExtractorSampleSource`. The demo app in this repository already has samples of mp4 playback, so you should use that as a starting point.
  When using the url you posted above the video plays fine. I see a firework drawing a ladder into the sky. 
Hence this seems to be a network issue to me. Apparently connecting to the server failed. 
 I was able to play the video successfully too, although playback takes a _long_ time to start.

It looks like the root cause for the slow startup is that your server lacks support for range requests. The file contains an index at the end of the file that the player needs before it can start playback. Since your server doesn't support range requests, the only way that the player can get at the index is to download the whole file, which is why the playback takes such a long time to start. Similarly, when seeking, the best the player can do is download the whole file up to the seek point.

For media, you really need a server that supports range requests properly (i.e. the server should respond with a 206 response code and partial content, not a 200 response code).
  `java.net.UnknownHostException: Unable to resolve host "servidor.i-live.com.ar": No address associated with hostname` looks like the cause. Make sure the device has a working internet connection and the app has permission to access the network. Is it just this specific stream that doesn't load, or do other URLs not load either?
 The sample plays fine in the ExoPlayer demo app, if I add it to Samples.java as:

```
    new Sample("Test",
        "http://servidor.i-live.com.ar:9234", Util.TYPE_OTHER),
```

As Andrew notes, the root cause looks like a networking issue rather than an issue with the player. Please also try with the latest version of ExoPlayer, to be sure.
 I doubt there is anything we can do to work around this particular issue.
  Can you provide some more information? I would like to reproduce the leakage. Are you releasing the player between runs? You are instantiating a new surface per stream, I guess?
 Agreed; this issue is unclear. ExoPlayer doesn't own the surface (or SurfaceTexture). The surface is injected into the player by the application, which can also clear the surface. Please clarify.
 Closing due to lack of information.
  I think it's just a typo, and should say: `if {@link #getPlayWhenReady()} returns true`. Thanks for pointing it out!
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Hi. Please could you provide some test content that requires this change? Thanks!
 Thanks! Looking at the spec, it appears PRIV and GEOB can both be repeated as well, and for the general case we need to assume repeats are possible too.

So I think we need to come up with a representation that supports repeated frames in general. Preferably a lightweight one; I don't really want to end up with each Id3Tag being a Map<String, List<Object>>.

What about if we make `Id3Parser.parse` return `List<Id3Frame>`. `Id3Frame` would be an abstract base class containing the frame type as a `public final String`, and would be extended by `PrivFrame`, `GeobFrame`, `TxxxFrame` and `BinaryFrame`. Seems simple enough and allows arbitrary repetitions of any frame type.
 That would be great; thanks! Don't worry about backward compatibility by the way; we're generally ok with breaking compatibility for the purposes of making the library better.
 Changes look good; thanks. Please can we go back to having public final fields though?

If you could revert the indentation changes too, and make the indentation consistent with the rest of the library, that would be great. Not to worry if you can't be bothered though; we can do it after it's merged if needed. NB - Regular indent is 2 spaces, or 4 spaces when breaking a line.
 Thanks!
  Can you please provide a stream?
 VLC thinks it only has one audio track. How have you determined that it has two?

Perhaps you're misunderstanding what a track is in this case. A track is a completely separate audio track, so for example if you have English and French audio tracks, then in that case there would be two tracks. If you're interested in audio channels (e.g. mono vs. stereo vs. surround), you should query that using `ExoPlayer.getTrackFormat` to get the format, then look at `MediaFormat.channelCount` to get the number of channels.
 Sorry for the delay, but @ojw28  is right. There are only two tracks in this file. A video track and an audio track.
  For Dash and SmoothStreaming TextRenderers are setup in the [DashRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/DashRendererBuilder.java#L241) or [SmoothStreamingRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/SmoothStreamingRendererBuilder.java#L184). These are using a TextTrackRenderer [which have SubtitleParsers for various formats registered](https://github.com/google/ExoPlayer/blob/267a870509bb85e2cff839ea162ae5b7c85933aa/library/src/main/java/com/google/android/exoplayer/text/TextTrackRenderer.java#L70). So for DASH/SS you need to register your parser there.

For HLS WebVTT and EIA608 is setup in the [HlsRendereBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java#L152).
 It sounds like the parser is being registered correctly, since its `canParse` method is being invoked. What you're missing is that whatever extractor(s) you're using also need to support the format. For example, if you're playing MP4s then you'll need to modify `AtomParsers` to create an appropriate format for the track, probably adding an additional `else if` [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/mp4/AtomParsers.java#L549).

Note that if you do this, we'd probably be interested in accepting a pull request if you're able to send one.
 You will need to update `WebmExtractor` in that case. It's probably pretty trivial to do so; you'll need an extra case in the switch statement in `initializeOutput`, and an additional entry in `isCodecSupported`.
 Did you manage to get this working?
 Ok, thanks for the update! Closing for now.
  Try using the allocation and buffer size values from [ExtractorRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java#L65). In the code snippet you pasted, the player will never be able to make progress because the individual allocation size is equal to the extractor sample source's requested buffer size.
 The actual cause of the player getting stuck is that the player can't buffer enough data to transition from buffering to ready, because the ExtractorSampleSource requested buffer size is too small.

In case it is of interest, here is what I think is happening in detail: during buffering the (paused) AudioTrack buffer fills with PCM data, and then no more decoded output is handled. Therefore, the decoder can't handle more input, which means that no samples are read from the ExtractorSampleSource, which in turn means that allocated size does not drop below the requested buffer size passed to ExtractorSampleSource. That stops the source buffering any more data, and so the buffered position never exceeds the minimum required buffer size to transition to ready.

The buffer size used in ExtractorRendererBuilder is chosen to be comfortably large enough to let the player keep making progress, though you can probably use a smaller value for audio-only playback. See also #680.
 Here is the updated link: https://github.com/google/ExoPlayer/blob/release-v1/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java#L65
 ExtractorMediaSource replaces ExtractorSampleSource in v2. As an aside, you might also find the new SimpleExoPlayer class useful for simplifying building the player.

It sounds like you want to control how much media the player buffers before it starts playback. Take a look at [this constructor in DefaultLoadControl](https://github.com/google/ExoPlayer/blob/69af389/library/src/main/java/com/google/android/exoplayer2/DefaultLoadControl.java#L98), and in particular the `bufferForPlaybackMs` parameter. Passing a lower value here should make the player start sooner, but note that there is a tradeoff between startup time and resiliency to transient network slowness.  There's something a little strange about your code. If you're just trying to play a single MP4, why do you have two sample sources. The first one being:

```
ExtractorSampleSource videoSampleSource = new ExtractorSampleSource(
              Uri.parse("http://media.achievr.co.s3.amazonaws.com/achievrWelcome.mp4"),
              dataSource, allocator, VIDEO_BUFFER_SEGMENTS * BUFFER_SEGMENT_SIZE);
```

and the second one being:

```
ExtractorSampleSource audioSampleSource = new ExtractorSampleSource(
              Uri.parse(url), dataSource, allocator, AUDIO_BUFFER_SEGMENTS * BUFFER_SEGMENT_SIZE);
```

For playing an MP4 you should have a single sample source, that's used by both the audio and video renderers. There's an example [here](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java#L64). I'm somewhat confused as to why you don't use `ExtractorRendererBuilder` without modification; it should do what you need.

As a final point, the actual crash you're seeing is because both of your two sample sources are using the same `DataSource`. A `DataSource` can only be used by one thing at once, so if you really do need two sample sources, you should have two `DataSource`s as well.
  You will have to provide us with sample content that reproduces the issue in order for us to take a look. Please either post it here, or mail it to `dev.exoplayer@gmail.com` (and update this issue so we know to look out for it :)). Thanks!
 Closing due to insufficient information.
  Try using `ExoPlayer.getTrackCount` and `ExoPlayer.getTrackFormat`.
  - I'm unsure whether that means one core is 100% busy or whether one core is 23% busy. I would definitely test your assumption though; I've seen stats reported in a way where four cores being busy is reported as 400%. It should be pretty easy to verify one way or another by creating a test app that busy loops 4 threads, or something like that?
- DASH uses a _much_ more efficient container format than HLS (fmp4 rather than mpeg-ts). There are significant benefits to switching to DASH; one of which is reduced CPU consumption due to having to do significantly less work extracting the media samples from the container. You can read more about this [here](https://google.github.io/ExoPlayer/2015/05/08/the-benefits-of-dash.html). It may be that we can make optimizations to our mpeg-ts extractor, but CPU consumption will always be higher due to the inherent disadvantages of mpeg-ts.
- It's unclear whether the high CPU utilization actually matters. It's definitely not related to `BehindLiveWindowException`. If you search this issue track (including closed issues) then you should find information about that exception.
  We don't provide such a callback. It's probably most appropriate for you to extend MediaCodecVideoTrackRenderer somehow, but it depends a bit on exactly what you're trying to do / what your use case it.
 Ah, I see. I haven't tried, but can't you just use [SurfaceTexture.OnFrameAvailableListener](http://developer.android.com/reference/android/graphics/SurfaceTexture.OnFrameAvailableListener.html) for this? [This](https://www.virag.si/2014/03/rendering-video-with-opengl-on-android/) might also be useful to take a look at.
  This is tracked by #87.
  Please read [reporting an issue](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) and provide the information described there. We would at least need to know the device and Android build you are using, and what stream you are playing, preferably in the demo app. Thanks!
 Please paste the output of `adb shell getprop ro.build.fingerprint` so we know the exact device and build you are testing on.
 What Samsung device is it? Please run `adb shell getprop ro.build.fingerprint` and paste the output.
 Your DASH manifest lists a video width of 853 pixels. This couldn't be played because VideoCapabilities.areSizeAndRateSupported returns false due to the [odd width](https://android.googlesource.com/platform/frameworks/base/+/master/media/java/android/media/MediaCodecInfo.java#1300).

However, it looks like the video is actually 720 pixels wide. The stream should play correctly if you replace "853" with "720" in the manifest.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Please reply with "I signed it!" to have the CLA bot perform the check again. Thanks.

Also, thanks for this; we had been trying (and failing) to figure that one out; so it's great that you've solved the problem for us!
 The account/email you're using needs to match the one for which there's a CLA in place. Please could you make it so; either by adding the address you're using to the CLA, or adding the email address for which there's a CLA in place to your GitHub account, or similar. Thanks!
 Do you understand exactly what the specification means by "successive frames"? Video frames are quite often re-ordered during decode, and it's unclear whether successive is defined before or after this re-ordering occurs.
 CLAs look good, thanks!

<!-- ok -->
 As a heads up, we're exploring a way of fixing this in Eia608TrackRenderer, which looks like it might fall out a little nicer. Will provide another update in due course.
 Hi. We've pushed https://github.com/google/ExoPlayer/commit/b3c954968f1fb94f5d8daf81b6994e7fa0e42e38, which we think addresses this issue. Closing the pull request, but please verify that the issue is resolved for you on the dev branch. Thanks for your help tracking this down!
  I have seen adaptive bitrate selection working many times. It starts with the lowest quality and adapts if the bandwidth allows for it. It depends on network condition and the bitrate of the variants in the HLS. Also the variants are filtered to discard variants which can't be played on a given device. 

So the number of variants available depends on the device and the point in time at which to switch up/down depends on network condition. Please check the sample in the ExoPlayer Demo app under 'HLS'. Try the sample 'Apple master playlist'. It starts with the lowest quality (on screen it is named 'Gear 1') and then switches to better quality ('Gear 4' in my case which is the highest quality). It's quite easy to spot the switches.

Can you provide more information for the first question? What url would you like the user to set? Actually, there is only one relevant URL for HLS, the one of the playlist. All further urls to network resources are derived from the playlist. When you want to set a new playlist URL you have to start again and create a new instance. But this can not happen 'during playback' inmy understanding. 
  The question itself falls quite far out of scope for ExoPlayer. It's definitely possible to do what you're attempting, and [this guide](https://www.virag.si/2014/03/rendering-video-with-opengl-on-android/) might be of use to you. It uses MediaPlayer, but you could pretty easily replace MediaPlayer with ExoPlayer in the sample. Beyond that, you'll probably have to work out how to get there yourself ;).
  I wasn't able to reproduce this on a Galaxy Note 4 (trlte), though the device is running 4.4.4. I hope to be able to test on 5.1.1 tomorrow.

Please could you provide full steps to reproduce the issue in the demo app? What stream are you playing? Does the glitch happen during the first two minutes of playback every time? Thanks.
 I wasn't able to reproduce this on a Samsung Galaxy Note 4 (trlte) running 5.0.1 or 5.1.1 when playing MP3 streams via the demo app.

To diagnose this we will need the information described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Also, do you have any idea what change between 1.5.2 and 1.5.4 caused the change in behavior you saw?
 Closing due to lack of required information.
  There's insufficient information here for us to do anything. Please read [this](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) and file a new issue containing all of the information listed there.
  The closest analogy would be to look at dropped frames, which occur when the player ends up being late trying to render them. You can listen for dropped frames using `MediaCodecVideoTrackRenderer.EventListener.onDroppedFrames`. Note that if you don't want batched reporting, you need to set `maxDroppedFrameCountToNotify` to a suitably small value in the `MediaCodecVideoTrackRenderer` constructor.
  So, for the sake of simplicity let us say you have only a Video track. In that case, of course, the playback speed of video is linked to real time (since there is no audio). I am tempted to think that the provided gist is a bit of an overkill. 

I would try to use an implementation of MediaClock that suits your needs. Something like the  StandaloneMediaClock but with your adaptations to support the desired playback speed. You shouldn't find yourself in need of playback implementation modifications. Keep me updated if you feel you have met an Exoplayer limitation.
 Does it work well? Have you tried just using your own MediaClock implementation instead of modifying the renderer?
 Yes, I had just read the class name and not walked through the code. Sorry about it.
 Mobile devices are simply not able to decode video at arbitrarily high speeds (note that network speed may also become an issue if you're streaming the content, since you're basically doubling the bitrate requirements). It's likely that if you use a lower quality video format, you'll be able to increase the multiplier further.
 I think you're just exceeding the capabilities of the device.

There are more complicated schemes where you actually serve sped up streams from the server, and then have the client switch to the correct speed stream according to the setting. That's pretty much the only option if you want to be able to do this without increasing network/decoder requirements as you increase the speed multiplier. We don't support such schemes directly, but it's probably not too hard to do so. It's probably more work on the serving side.
  The error has nothing to do with okhttp. The fact you've commented out the line of code that instantiates audioRenderer means that you're passing a null renderer to the player, which isn't a valid thing to do.
  Maybe the bandwidth requirement is so high that it's not possible to buffer the minimum amount of media for playback to resume. Try using `ExoPlayer.newInstance(rendererCount, 0, 0)` to create your ExoPlayer instance. Does that allow playback to resume (albeit in a choppy fashion)? You can also try bumping up `BUFFER_SEGMENT_COUNT` in `ExtractorRendererBuilder` in the demo app.

If neither of those help, I would suspect that your server does not properly implement range requests.
 Closing, since it seems like the answer is to either lower the bitrate of the content, increase the buffer size or decrease the minimum buffer duration. Providing a better (i.e. more obvious) failure mode when the buffer is too small is tracked by #583.
  See https://github.com/google/ExoPlayer/issues/1077.
  I don't think this is an ExoPlayer specific issue. I'd suggest you ask for help on StackOverflow, or similar.
  `Unexpected status line: ICY 200 OK` looks similar to #890. Is this still reproducible if you use [OkHttpDataSource](https://github.com/google/ExoPlayer/blob/master/extensions/okhttp/src/main/java/com/google/android/exoplayer/ext/okhttp/OkHttpDataSource.java)?
 OkHttpDataSource is part of [an extension](https://github.com/google/ExoPlayer/tree/master/extensions/okhttp), not the core library, and it looks like it is not published to jCenter at the moment unfortunately. You could try checking out the source and using that instead as per [the instructions here](https://github.com/google/ExoPlayer#as-source).
  It looks like the actual problem is `DecoderInitializationException: Decoder init failed: AACDecoder`, but we currently don't use this decoder on platform API version < 21 ([see the check here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java#L193)).

Are you using an up to date version of ExoPlayer? Please also paste the output of `adb shell getprop ro.build.fingerprint`.
 This is most likely fixed by using an up to date version of ExoPlayer, where the check mentioned by @andrewlewis most likely fixes the issue. Please let us know if not.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  I don't think there is much we can do without having a test stream that reproduces the issue. Please reply if you can provide one.

For what it's worth, it seems likely that the stream fed to the decoder was corrupted (OMX_ErrorStreamCorrupt) or lacked some required initialization data.
  The initial question states that there's a way to do this already, so it's unclear what the question is beyond asking whether there's a second way of doing the same thing? If that is the question then the answer is no, although in 2.x the setting of a buffering policy should become much simpler and less specific to the different types of playback that ExoPlayer supports.
 Closing this. Note that in 2.x this becomes far more straightforward. Rather than the buffering configuration being set in separate places for DASH/HLS/SmoothStreaming/Other, it's now managed by a single LoadControl injected when creating the ExoPlayer instance.
  You can also use `HttpDataSource.setRequestProperty`.
  There's nowhere near enough detail to diagnose this, but if the cameras are video only (no audio) then you're probably hitting #758, and you'll need to wait for that to be fixed.
  ExoPlayer synchronizes everything based on the platform's reported audio track position. This is retrieved through `AudioTrack.getTimestamp`, or (less good) `AudioTrack.getPlaybackHeadPosition` if `AudioTrack.getTimestamp` isn't available. If A/V sync is gradually lost, the most likely cause is that the device is reporting an audio track position that's slowly drifting relative to what's actually being played, so that eventually the reported positions are wrong enough for the user to notice the loss of sync.

One workaround you could try is to modify this method to always return false:

https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/audio/AudioTrack.java#L1175

That would disable the use of `AudioTrack.getTimestamp` and force ExoPlayer to go through the legacy path of using `AudioTrack.getPlaybackHeadPosition` instead, which might help if the first one drifts but the second one doesn't. If that doesn't work, it feels like something that would be hard to fix other than directly in the OS.

Please report back with any findings!
 We do not recommend use of the Amazon port. At this stage pure ExoPlayer should work on the majority of FireOS devices, except for certain specific use cases (e.g. if you're using PlayReady). The Amazon port is likely to lag behind pure ExoPlayer, and we do not vouch for the quality of the changes made there (as an example, there's one change merged into the port that we explicitly rejected from ExoPlayer because it didn't look like a good idea).
  Please see your corp inbox. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Is it a "real" Android device. What device is it, exactly, and what version of Android does it run?
 If the device is reporting what it supports correctly then the device would encounter an issue if we _didn't_ filter the video streams. What value is the device reporting as maxH264DecodableFrameSize, and what profile levels does it find in the `maxH264DecodableFrameSize` method?
 You understand correctly; yes. You're also correct to observe that we end up deriving a device's capabilities in a somewhat overly conservatively way. Things are much better on API level 21 and above, where we use a different platform API that provides us with better information.

For your case, if the default format is always pretty low quality for the streams that you're playing, then it should be relatively safe to do what you're doing. You could optionally enable your fix only on known bad devices, for example by inspecting http://developer.android.com/reference/android/os/Build.html#MODEL.

We'll be making it easier to select tracks that we don't think the device can play in the future, since we've received a few other requests for the ability to override our logic. Leaving this open to track this as an enhancement.
  This is already possible by listening to ChunkSampleSource or HlsSampleSource. In the demo app, these events end up in `EventLogger.onVideoFormatEnabled`, where they're printed to the log.

Having said that, we might do more to allow the current selection to be queried more easily (to be determined).
 In the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental) there's a new `SimpleExoPlayer` that provides a `getVideoFormat` convenience method, along with `DebugListener.onVideoFormatChanged` for listening to changes. When using `ExoPlayer` directly, `VideoTrackRendererEventListener.onVideoInputFormatChanged` can be used to listen to changes in the format of video being consumed by a renderer. So all in all, 2.x.x should address this pretty comprehensively.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Hello Eyliss, thank you very much for your contribution. I don't have the last work in this matter so you will have to wait. My impression, however, is that there are a few things that require reviewing. For example, indentation is not consistent with the rest of the library. This is the unimportant part. The fading feature, in my opinion, is something that escapes the purpose of the library: It is application specific, and seems to be implementable on application level. Finally, most importantly, the changes affect the very core of the library. It is unlikely that such a feature should take place in the interface of the library. One of the core design concepts is to keep the Player track-agnostic (roughly), while this codes assumes the existance of an audio track renderer. I do really appreciate your contribution though. It is @ojw28 who has the final call. To sum up, I think this code belongs in the application.  
 Agreed. It looks like you could take what you've implemented in ExoPlayerImpl and move it to application code. If your application happened to be the demo app, it looks like you could fairly trivially move it to the DemoPlayer class instead, where it's more appropriate to assume the existence of an audio renderer.
 Eyliss, some people have suggested that the DemoApp was already too bloat and complex. And since there is no "playlist" feature in the DemoApp, perhaps there is not much sense in fading in and out. That would be forcing videos to have a certain "fading" component which would be part of the video already, if the author intended it to be that way.

If you really want to contribute to the project, help is always welcome. You can check the open issues for feature requests or bug fixes.
  Dupe of https://github.com/google/ExoPlayer/issues/1434.
  In the short term we should probably just skip pssh boxes that have version > 0.
 Correction. Parse version 1, ignore version 2+.
  I hadn't seen this. I will have a look.
 ExoPlayer does not cache to disk. Whilst it's beneficial for an individual application (in isolation) to cache media to disk, I'm not convinced that it's good for Android device health if many applications all start doing this by default as a result of using ExoPlayer. Hence our decision not to support this as a conveniently built in feature at the current time.

It's not true to say that ExoPlayer aggressively loads all segments into memory. ExoPlayer operates within a specified memory budget that the application controls. For HLS playbacks in the demo application, the memory budget is specified [here](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java#L57).
 I will close this issue as things have changed quite a lot in this matter, particularly in v2. If this issue persists in the new versions, please file a fresh one.
  The player doesn't skip the "mysterious music" caption for me. The captions are a little early, but they appear to be displayed at the times listed in the caption file itself, so that feels like an issue with the content to me?

There is a separate issue where seeking backward isn't working with this setup in the demo app, which I'll fix, but otherwise everything looks fine from the player's point of view.
  Please search existing issues before filing new ones. See https://github.com/google/ExoPlayer/issues/26.
  The stack trace above doesn't match up with the line numbers in any releases that we've made. What version of ExoPlayer are you using?
  It's working as intended that ExoPlayer doesn't follow cross-protocol redirects by default. In this case there are three redirects (not ideal to start with). The second one is a cross-protocol `http->https` redirect.

```
http://www.ivoox.com/03-nuestros-horrores-favoritos-h-p-lovecraft-el-templo_mn_10103734_api_1.mp3
-> http://files.ivoox.com/listen/10103734
-> https://archive.org/download/TyNM2015Otros/03NuestrosHorroresFavoritosH.p.lovecraft-ElTemplo.mp3
-> https://ia801505.us.archive.org/13/items/TyNM2015Otros/03NuestrosHorroresFavoritosH.p.lovecraft-ElTemplo.mp3
```

Ideally I think you should fix your serving infrastructure to not perform cross-protocol redirects. However if you can't do this, you can enable following of cross-protocol redirects in ExoPlayer by passing `allowCrossProtocolRedirects=true` to the `DefaultUriDataSource` constructor.
  Closing due to insufficient information. Please open a new issue containing complete information if you wish us to investigate.
  - Longer segments should work fine, provided the configured buffer is large enough. Perhaps it's not in this case, if you're using a very high bitrate.
- We don't support adaptive audio, although the application can select the representation that it prefers.
 Your streams don't work from my location ("The Amazon CloudFront distribution is configured to block access from your country."). Please could you provide ones that aren't geo-restricted? Thanks!
 The segments in the media aren't aligned properly, so this is a content issue. Below are the segment start and end times for the video representations in your manifest. Note that the representations with ids 10 and 8 have a significantly shorter first segments (8s) than the others (10s), which means that the segments are misaligned throughout by around 2s. Adaptation will be seamless between formats 8 and 10, and separately between formats 6, 4, 3, 2 and 1. But when switching between these groups a discontinuity is expected due to the misalignment.

The solution is to fix the content, I'm afraid!

```
Format 10
21000   8029011
8029011 18039022
18039022    28049022
28049022    38059033
38059033    48069044
48069044    58079055
58079055    68089066
68089066    78099077
78099077    88109088
88109088    98119100
98119100    108129111
108129111   118139122
118139122   128149122
128149122   138159133
138159133   148169144
148169144   155509822

Format 8
21000   8029011
8029011 18039022
18039022    28049022
28049022    38059033
38059033    48069044
48069044    58079055
58079055    68089066
68089066    78099077
78099077    88109088
88109088    98119100
98119100    108129111
108129111   118139122
118139122   128149122
128149122   138159133
138159133   148169144
148169144   155509822

Format 6
21000   10021000
10021000    20021000
20021000    30021000
30021000    40021000
40021000    50021000
50021000    60021000
60021000    70021000
70021000    80021000
80021000    90021000
90021000    100021000
100021000   110021000
110021000   120021000
120021000   130021000
130021000   140021000
140021000   150021000
150021000   155562666

Format 4
21000   10021000
10021000    20021000
20021000    30021000
30021000    40021000
40021000    50021000
50021000    60021000
60021000    70021000
70021000    80021000
80021000    90021000
90021000    100021000
100021000   110021000
110021000   120021000
120021000   130021000
130021000   140021000
140021000   150021000
150021000   155576555

Format 3
84000   10084000
10084000    20084000
20084000    30084000
30084000    40084000
40084000    50084000
50084000    60084000
60084000    70084000
70084000    80084000
80084000    90084000
90084000    100084000
100084000   110084000
110084000   120084000
120084000   130084000
130084000   140084000
140084000   150084000
150084000   155667333

Format 2
189000  10189000
10189000    20189000
20189000    30189000
30189000    40189000
40189000    50189000
50189000    60189000
60189000    70189000
70189000    80189000
80189000    90189000
90189000    100189000
100189000   110189000
110189000   120189000
120189000   130189000
130189000   140189000
140189000   150189000
150189000   155772333

Format 1
189000  10189000
10189000    20189000
20189000    30189000
30189000    40189000
40189000    50189000
50189000    60189000
60189000    70189000
70189000    80189000
80189000    90189000
90189000    100189000
100189000   110189000
110189000   120189000
120189000   130189000
130189000   140189000
140189000   150189000
150189000   155939000
```
 I manually added some logging to the ChunkIndex constructor in ExoPlayer, and then used the demo app to select different video formats. There's probably a better way though; I imagine tools exist :).
  Please see #426 (it's a good idea to search existing issues before filing a new one, also!). Thanks!
  I'm a little confused. Are you basically saying that you don't want playback to fail, ever, even if there's no network? Specifically, you'd rather the player enter an indefinite buffering state until network is restored?
 You could probably stop the player from failing by passing a large value as minLoadableRetryCount to the HlsChunkSource constructor. You don't need to do anything special to have the player enter a buffering state when the buffer runs out; it'll do this automatically. So I think passing a large minLoadableRetryCount is all you'd need to do to achieve what you want.
  All of the underlying work necessary to enable this has been done, but I think things haven't quite been wired together yet (probably just an oversight on our part :)). @AquilesCanta to take a look.
 It is indeed a bug. The fix will be avaialble soon, I believe.
  This is very vague. Please see [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) for the kind of information we need to be able to diagnose this kind of issue.
 Closing due to insufficient information.
  I am not sure whether there is a Sample including WebVTT or SRT in the DemoPlayer, but you can always include your own Sample. What are you specifically looking for?
 Well, if you want to use an external Subrip file as subtitle, I don't think that the DemoPlayer provides support out of the box. @ojw28 Can provide some better insight on this if am wrong. 

What you can do is use a different `SampleSource` for the `TextTrackRenderer`. Have a look at the `ExtractorRendererBuilder`. You can feed a `SingleSampleSource` instead of a `ExtractorSampleSource`, which takes your SRT's URI, instead of the original audio/video file's URI. Thank @andrewlewis for this. I hope this helps.
 The first post in #753 has some code that demonstrates roughly how to set this up.
 Elio, for bitrate you can use MediaFormat.NO_VALUE and for trackId you can use any value you like. If you are using any modern IDE, you can check for references and see what these values are used for and then give yourself an idea of what to use.
 Does the logcat show anything? Also, does the screen show a "text" box or only video and audio? Give yourself a few minutes of looking around. You can check out [#753](https://github.com/google/ExoPlayer/issues/753), posted above your comment. Finally check if using the same data source is not the problem. It is a good idea to start from working code and try to figure out how to add stuff. 

An observation: The github issue tracker exists to file bugs, feature requests and such. For general coding questions, I would suggest that you use stack overflow or similar sites. Good luck!
  This question is about a custom extension to ExoPlayer, not part of the core library, and hence is beyond the scope of this issue tracker.
  I agree this is broken, but I'm also confused by the spec. Having read a PES packet how are you supposed to determine whether you have a complete ID3 tag, without having to wait for the next PES packet in the metadata stream and look at its data_alignment_indicator? Waiting doesn't seem viable, since you may have to wait for an indeterminately long period of time. There might not even be a next PES packet, since you might have just read the final ID3 tag in the stream.

One approach would be to assume that you have a complete ID3 tag unless the PES packet was of length 65535. The spec doesn't mandate this, but it's probably a reasonable assumption (and is true for the sample you posted). That still doesn't solve the edge case where an ID3 tag happens to be exactly the length that would result in the wrapping PES packet being of length 65535, however :(.

Thoughts?
 I guess we'll just have to parse the ID3 header as well. Which is highly unfortunate, but oh well. Unless you can think of a better way?
 Even waiting for the next packet with DAI=1 in the normal case is bad, since you might end up waiting for an arbitrarily long period of time before it arrives. It does seem like a shortcoming.

Fortunately it's actually pretty trivial to read the size of the complete ID3 tag. At the end of each PES packet you can then check whether you've read that number of bytes. We'll do that.
  Please see https://github.com/google/ExoPlayer/issues/1074.
  Is it possible that you're opening `MediaDrm` sessions and failing to close them again. How do you actually reproduce the issue? Is it possible to reproduce the problem with the ExoPlayer demo app?
 The issue would more likely be that previous playback sessions have not closed their DRM sessions. A failure then occurs because there are so many sessions held open due to previous playbacks, that it's not possible to create another one.
 How are you determining whether there are any other open sessions? Can you reproduce this with the demo app, or only your app? If you can't reproduce with the demo app then it's likely an app specific issue.

I'm not aware of any way to reset DRM sessions at the global level, unfortunately (that's not to say that there isn't one - just that I'm not aware of one!).
 As above, does this reproduce using ExoPlayer's demo app, or just with your own applications? Thanks!
 I feel there's something missing from the description provided here. I'm pretty sure that the call stack at the top of this report only occurs at the start of playback. I don't think seeking would ever cause you to hit that code path, unless playback were to fail and you were then to start a subsequent playback.

If the playback is failing with some other error, and then the stack at the top of this report only occurs on a subsequent playback, then it's the initial failure that we need to be looking at, rather than the one that's been provided. Please provide more details around exactly what you're seeing, and corresponding logs, preferably in the form of a full bug report taken with `adb bugreport`, not `adb logcat` and not just the stack copied from an IDE.

Thanks!
 If this is still reproducible, please provide an ADB Bugreport.

thanks.
 @casajavi , any updates? almost 3 months without a ADB Bugreport.  Will monitor for a couple more weeks then will close.
  You mean from the DemoPlayer manifest?

EDIT: Sorry for the confusion. I am now removing the permissions from the Manifest. 
  This is already tracked by the combination of #1145 and #366.

FLAC is somewhat awkward, because Android doesn't have a FLAC decoder at the platform layer. If I remember correctly it provides an extractor that converts FLAC to PCM, but doesn't expose just the decoder part through MediaCodec. Hence our currentl plan is to support FLAC as an extension, where you'd need to bundle the FLAC decoder as a library inside of your apk. It's something we're working on, but we don't have an ETA.
  Yup, I have a fix for this that will be merged today.
 Should be fixed by https://github.com/google/ExoPlayer/commit/d804446b343401f6487f62777c9a23173f79309b.
  The documentation not only documents which looper you should pass, but also exactly where you should get it from? See below.

```
   * @param playbackLooper The looper associated with the media playback thread. Should usually be
   *     obtained using {@link com.google.android.exoplayer.ExoPlayer#getPlaybackLooper()}.
```
  The only thing I can think of that would cause this is if your DASH manifest has the 1080p representation in its own adaptation set at the top, and then a second adaptation set containing 720p/480p. Is that correct, and is there a reason why all three aren't in the same adaptation set?
 What are the two codecs, out of interest? Is the 1080p representation H265, or something like that?
 It's typically not possible to seamlessly adapt between different codecs. The auto track generated in this case is 720p and 480p only, and so it's pretty ambiguous whether the player should default to the H265 stream or to the two H264 streams (unless you have something in your manifest that indicates this somehow).

I think from the delivery point of view, it would be more normal to have H265 streams in 1080p, 720p and 480p, and then H264 streams in 720p and 480p. You'd then get an auto H265 track that can switch between the three H265 representations and would be enabled by default on devices with H265 decoders, and a separate auto H264 track, which would be the default on devices without an H265 decoder.
 ExoPlayer should already select a compatible audio track. What tracks do you have in your manifest, and on what device, and what selection are you seeing?
 Yes, it's possible to switch resolution. On Jellybean and some KitKat devices there might be a slight (~50ms) freeze in the video across the resolution switch, but nothing worse than that. On newer devices it should be completely seamless. There's definitely no buffering involved.
  It works fine for me on Nexus Player and Nvidia Shield, using encoded audio passthrough to an audio/video receiver.

Note that we can only play AC-3 if the device has a built-in decoder for the format (few do), or there is an HDMI connection advertising AC-3 passthrough support.
 The demo app handles changes in audio capabilities by restarting the player. The notifications come from the [AudioCapabilitiesReceiver](https://github.com/google/ExoPlayer/blob/0c060f429f459f1bac2ffa0963cadc45ce35b73a/library/src/main/java/com/google/android/exoplayer/audio/AudioCapabilitiesReceiver.java#L97). You could try logging the audio capabilities there and checking whether they match up with what you are expecting. In particular, at number 6 in your steps to reproduce, the capabilities should look something like `AudioCapabilities[maxChannelCount=8, supportedEncodings=[2, 5, 6, 7, 8, 9]]`, and the demo player should be restarted when encoding 5 (ENCODING_AC3) is supported.

When I tried following those steps, audio playback did work correctly (though I found that my AVR switches away from the correct input if the devices are turned on in a certain order). Is this 100% reproducible for you, or does it seem to depend on the exact timings of the devices restarting?
 The listener is only called when HDMI is connected/disconnected and the audio capabilities change. I think it's fine to reset the player as in the demo app, because this is not likely to happen during playback (except when attaching/detaching an AVR, in which case playback may need to be restarted taking into account the new audio capabilities).
 Note: We have a low priority goal to handle changes in capabilities automatically inside of the player. Probably wont happen for a while though.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  If you want your listener to be invoked then you need to pass a something other than null as the value of `eventHandler` in the `MediaCodecVideoTrackRenderer` constructor.
 Whilst I agree in general that we need more detailed documentation, the Javadoc seems pretty clear in this specific case:

```
eventHandler - A handler to use when delivering events to eventListener. May be null if delivery of events is not required.
eventListener - A listener of events. May be null if delivery of events is not required.
```

Choosing to pass null implies that you're not interested in having your listener receive events.
  We don't support recording.
 Plans can change, but as of now we have no plan to support this feature. V2 has the [same method](https://github.com/google/ExoPlayer/blob/r2.1.1/library/src/main/java/com/google/android/exoplayer2/audio/MediaCodecAudioRenderer.java#L312) and it looks to me like you can still override it, so I'm not sure the StackOverflow issue is particularly relevant.

Developers are free to fork part or all of ExoPlayer to implement what they need, where we don't provide a feature directly. Note that adding hooks to allow a feature to be conveniently developed as an extension is in itself a feature. Note also that a developer is free to propose a pull request that adds such hooks to the project.  When filing new issues, please read the Issue Template and provide all useful information. For instance, the stream that produces the failure.
  I'm unsure as to what you're asking here. Please clarify. Thanks!
 Please try explicitly hiding the SurfaceView itself. You could also try using TextureView, which will likely behave more as you expect.
  Try using TextureView instead of SurfaceView. You'll most likely find that it performs better when subjected to animations like scrolling.
  The question is not perfectly clear. For starters, I can tell you need to detect whether connectivity has changed (this may help http://stackoverflow.com/questions/3767591/check-intent-internet-connection). If you just want to stop using the network, then you could release the player.

Otherwise, if you need something more complex, you could think about implementing your own DataSource. Perhaps something like a toggable HttpDataSource. I could be misunderstanding your problem, but I will not be able to solve the application specific issues, unfortunately. 
  What does "before" mean and what is "after" in this case?
 I don't think this is particularly likely to be related to an ExoPlayer change. The second log is just a network timeout (i.e. bad network connectivity or the server not responding quickly enough). The first one looks like the underlying media server is in a bad state on the device.
  FYI - This is probably the same as #398?
 Closing as a duplicate of #398. I'll follow up on that bug.
  Most likely the player is starting before the surface has been created. You'll probably find that you can fix the problem by making sure `player.setPlayWhenReady(true);` doesn't occur until after `surfaceCreated` is invoked in `PlayerActivity`.
 Using a TextureView instead of a SurfaceView seems to fix the problem in your demo. I think it is likely that the transition animation is not completely compatible with SurfaceView as configured in your app.

Note that TextureView is less efficient than SurfaceView in battery consumption, and video playback may be slightly less smooth.
 It looks like the player has the surface and is rendering frames just after the transition, but the view is black despite that. At the moment the video does become visible, `ViewRootImpl: changeCanvasOpacity: opaque=false` is logged; I am not familiar with how transitions work, but SurfaceView is 'special' and I think it is plausible that it is incompatible with translucent activities. On the other hand, this opacity change happens about two seconds after the transition completes, so maybe there is something else going on.

I'd recommend using TextureView, but please update this thread if you investigate further and find a workaround that makes SurfaceView work. Thanks!
 Adding `surfaceView.setZOrderOnTop(true);` in the player activity's onCreate method actually seems to make this work with SurfaceView. With that change the transition is still not completely smooth, but you might be able to fix that by preparing the player when the transition completes instead of in onResume.

Could you confirm that you see the same behavior?
  Could you provide some more information? I am not sure I am understanding the issue.
 I assume this thread is supposed to be replaced by issues/1163. So I am going to close it.
  That log snippet doesn't include the original cause part of the stack trace, so isn't enough for us to do anything with. There are also multiple hardware variations of the S5. Please provide complete information as documented [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
 Closing due to insufficient information.
  I'm unclear as to why you don't just take the existing demo app and replace each instantiation of `DefaultUriDataSource` so that instead of this:

```
DataSource dataSource = new DefaultUriDataSource(context, bandwidthMeter, userAgent);
```

You have this:

```
DataSource dataSource = new DefaultUriDataSource(context, bandwidthMeter,
    new OkHttpDataSource(okHttpClient, userAgent, null, bandwidthMeter));
```

Is it just a matter of documenting this more clearly somewhere?
 Let's use this issue to track adding documentation to the corresponding [Readme](https://github.com/google/ExoPlayer/blob/master/extensions/okhttp/README.md) file.
 This enhancement is tracking:
1. Adding a sentence or two to the extension readme.
2. Making the extension available via BinTray.
 In theory it should now be possible to include the OkHttp extension for V2 with:

```
compile 'com.google.android.exoplayer:extension-okhttp:r2.0.1'
```

Bintray link [here](https://bintray.com/google/exoplayer/extension-okhttp). This doesn't seem to work for me at the moment. I'm unclear whether there's just a propagation delay in it being indexed, or whether something else is wrong. If anyone has any ideas, please let me know!
 Yeah. Hence the "This doesn't seem to work for me at the moment" part...!
 I don't really understand what you mean. Have you spotted an error? If so, please clarify exactly what it is.
 Ah, I see the problem now. I'm not entirely sure how to fix how that gets generated as part of the release process, but will take a look.
 It's possible that using https://github.com/bintray/gradle-bintray-plugin instead of https://github.com/novoda/bintray-release resolves this problem. It looks like https://github.com/novoda/bintray-release/issues/16 is tracking supporting this case for bintray-release.
 I've manually fixed the POM file for now, so the following should work:

```
compile 'com.google.android.exoplayer:extension-okhttp:r2.0.1'
```

Note that if you tried whilst it was broken, you may have to do some cache wiping on your machine. File->InvalidateCaches in Android Studio wasn't sufficient for me. Neither was deleting the .gradle directory in the project. I had to also delete the .gradle directory from my home directory (disclaimer: I don't know what the consequences are of doing that; so do it at your own risk; I just know that it helped for me!).
 The OkHttp extension can be built from [source](https://github.com/google/ExoPlayer/tree/156381b938559ecb4d7533fb82fd3f29be22cdd7/extensions/okhttp) for V1, but we will not be distributing it via Bintray/JCenter. We do distribute it for V2.  Sorry, but since this is very much a custom modification, it falls pretty far outside the scope of this issue tracker. I'd suggest asking on StackOverflow, or similar, where someone may be able to help.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks!
  There is a demo app in this repository that shows how ExoPlayer can be used in an application. Converting your code is something you'll have to figure out for yourself (or ask on StackOverflow).
  You should try and contact the device manufacturer in the first instance; it looks like a device specific issue. Although I'm not sure where to do that.

In terms of fixing the issue, what are you actually trying to achieve? You could just use H264 on this device, which probably isn't affected.
  Would I be correct in thinking this is a sporadic failure, rather than something that happens all of the time?
 There's a public sample built into the demo app that you can try (and it works as far as I know).
 Ah, I didn't spot the live part of your comment. There's an Azure live stream provided by Microsoft, that works fine for me:

```
    new Sample("Azure live",
        "http://b028.wpc.azureedge.net/80B028/Samples/a38e6323-95e9-4f1f-9b38-75eba91704e4/"
        + "5f2ce531-d508-49fb-8152-647eba422aec.ism",
        Util.TYPE_SS),
```

I also have some private test streams that work fine. Please could you provide a link to an affected stream?
 NB - I just updated the link in the response above, so copy it again if you tried within the last minute!
 That suggests malformed media. There's nothing we can do to assist unless you can provide a test stream.
 Please provide the stream to `dev.exoplayer@gmail.com`. Thanks.
 I think the `data_offset` field in the `trun` boxes is set incorrectly in the underlying fragmented mp4 streams. You'll probably need to fix this to get your samples playing in ExoPlayer.
 If I've diagnosed this correctly, the only reason the streams worked previously is because we were completely ignoring the data offset. It's likely that some other platforms do this too, but it's essentially incorrect and causes parsing failures for legitimate streams that contain non-sample data at the beginning of their mdat boxes.

As a local workaround, it appears you can simply comment out the block of code where the failure occurs.
  We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 This really needs to be split up into two distinct pull requests, since it's delivering two separate things. Please could you open 2x new requests, one for FLV and one for RTMP? For the FLV one, can you please provide a link to some sample media that we can use to test the implementation in the pull request description? Thanks.
 Thanks! It's definitely worth splitting the FLV change into a separate request, since that one looks a lot more straightforward for us to merge than the RTMP change.
  Sounds pretty much the same as #603 (the power of search ;)). Does the workaround described there work in this case also?
 Note. An equivalent and simpler workaround on the dev branch would be to have `MediaCodecTrackRenderer.codecNeedsFlushWorkaround` return true for S5 Mini. Please test locally and see whether that helps.
  The log spam is coming from the platform and there's nothing we can do about it. I'm not sure about performance; it's likely also a platform issue, unfortunately.
 You could try `http://developer.samsung.com/forum/en`. 
  I think whatever IDE you're using is just not doing a good job of telling you what the actual problem is. There are probably a few small specific things, similar to the one you've already fixed, that are broken. What's the first compilation error when you try and build?
 You've fixed the `onAudioTrackUnderrun` error though, right? Please paste the exact error you see in the gradle build output (not just red underlines, or whatever, that the UI might be showing you).
 Right, so some methods have changed and you need to update your code to match the new method signatures. If you click on those errors, I think the IDE will land you somewhere in your own code where you're calling a method whose signature has changed. You'll need to adjust your code to match the new signatures.
  You'll need to update TtmlColorParserTest as well (`testHexCodeParsing` will fail as a result of this change, I think, because the test is also wrong)?
 I'm not sure; try and see. You can always send a new pull request if necessary. Thanks!
 I merged this as it is because I'd like to land this change in front or another one that's coming. I'll fix the test shortly.
 Tests fixed in https://github.com/google/ExoPlayer/commit/7974a61476e9f494076fad1f8ee8f6df0a87c83c. Thanks again for the fix!
  According to [Wikipedia](https://en.wikipedia.org/wiki/Opus_%28audio_format%29#Containers), .opus files are Ogg containers with Opus payloads. ExtractorSampleSource does not yet have an extractor for Ogg, which is why an UnrecognizedInputFormatException is thrown, but we are working on adding this functionality.

If you have control over the input file, I suggest using WebM instead for now.
 This is work in progress and unfortunately we don't have an ETA yet. 
The Opus URL you pasted is a live stream. Are you interested in live stream only?
 @marcbaechinger added work-in-progress Ogg support in 89ce1ccedff72da5c7efc4ee50bfcd2d67e45818. The extractor doesn't yet support seeking, but should work otherwise.
 Ogg is seekable as of cd8df78162cddc041f9633caec4de9db4e7450cc, so we can close this.
 We'll be pushing Flac in Ogg support very soon. We don't have any immediate plans to support Opus in Ogg; although we do support Opus in other containers, like MKV. I filed #1447 to track Opus in Ogg support.
  The stream declares an Audio stream in both PAT and PMT but never includes any payload. I would suggest either removing the PSI entries or adding audio payload. I don't think this kind of transport streams will have support in the near future.
  A few questions:
- Where is the PSSH header in the original stream that you provided? There doesn't seem to be a PSSH box in the moov atom of the streams and it's not in the manifest either, so I'm not sure exactly where you're expecting to find it in order to make the license request?
- Once you add the PSSH header into the manifest the error changes to an error obtaining the license from the license server. This is presumably because the demo app is hitting Widevine's test server rather than your own. What's the license server URL that needs to be hit to play this content?
 ExoPlayer currently expects:
1. The `pssh` box either in the `moov` atom in the init segment, or alternatively in the mpd.
2. A `sinf` box under `moov->trak->mdia->minf->stbl->stsd->sample_entry`, where `sample_entry` is `enca` or `encv`in your audio and video streams respectively. Within the `sinf` we expect to find `schi->tenc`, which contains the keyID.

So your problems are:
- You have the `pssh` box in the media segments rather than the init segment. As you've observed, this can be resolved by putting it in the mpd instead.
- We're not finding the keyID in the place specified above, so the player is ending up trying to use a key consisting of all zeros. I'm guessing the keyID is also in your media segments rather than in the init segment? Where abouts; it's quite possible that we should be handling that.
 Yes, you're correct. We don't look at SampleToGroupBox. We should start looking at it, and I'd rather fix that than have people work around the problem ;). Do you know the specification that says exactly where that lives and how it works? I could only find vague descriptions. I also scanned through the boxes in your media and I couldn't seem to find it. 

I'm unsure as to whether that's sufficient to enable key rotation. It's unclear to me exactly how the rotating keys should be loaded into the MediaDrm session. But I guess that's something that should be looked at later.
 Which standard exactly (if it's an ISO standard then it should have a number), and what revision of it? Thanks.
 This is fixed in the `dev-v2` branch by https://github.com/google/ExoPlayer/commit/cf363f9e97a3fab647e1b9806a0f36c9717f7637.

Regarding the comment above, the issues are a little different. The sample in this issue puts the PSSH data in the MOOF but does (as far as I could tell) the keys do not rotate. For #1298 we'd need to support key rotation. Most of the plumbing for key rotation is in V2, but our default `DrmSessionManager` implementation does not provide full key rotation support yet.
  There is insufficient information here to help. Please see [this](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) and file a new issue with all of the requested information. Thanks!
  I think tt is quite straightforward. Have you tried...
- File -> Open
- Navigate to directory containing exoplayer.
- Select exoplayer, and click OK.

It will open multiple projects, one of which is the library itself. Another useful project is the DemoPlayer, which is basically an App that shows the usage of ExoPlayer. Shout if you have any trouble.
  - We'll take a look at this, thanks.
- As an aside, why are you sending 4K content to a device that has a resolution significantly below 4K, and a screen whose physical size is such that it's debatable whether the user will be able to perceive a difference v.s. a lower quality? Isn't that just wasting the user's battery + network? Shouldn't you be restricting stream resolution to 1080p on the S6?
  Integration with another library is something you'll have to figure out for yourself, or possibly ask on StackOverflow for help. This is not an issue or question directly related to the ExoPlayer library.
  You can extend `MediaCodecAudioTrackRenderer` and override onAudioSessionId. The ID is passed in right there as the argument, so you don't need to be calling anything from there.
  @jeoliva - Can you please take a look at this?
 The video is not available. Please could you provide a working link to a sample that reproduces the issue? Thanks!
 Seems to be dead again. Is it possible to provide one that will remain valid for more than ~30 minutes?
 Note: Another 30 minute one will be ok for the time-being, whilst you figure that out!
 Thanks. There are two problems:
- The media specifies its duration explicitly as 0.0. We currently take this to mean the media really is of 0 duration, but we should probably treat a value of 0 as unknown instead.
- The audio specific config parsing is incorrect.

I have fixes. We'll push them to the dev branch sometime this week.
 Fixed on the dev branch.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  ExoPlayer does not support this currently. Adding support is tracked by #420.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Happy to take this change, but it would need to go into the google:dev branch, not google:master. You'll also need to sign a CLA as above. Closing this request since it's into the wrong branch, but please do open a new one into google:dev. Thanks!
  Question is unclear. Closing.
  That's just a networking error indicating the resource wasn't found, as described [here](https://en.wikipedia.org/wiki/HTTP_404). So it looks like a networking issue to me. ExoPlayer just uses the standard Android network stack, so it feels like this isn't an ExoPlayer issue.
 I'm sorry, but a 404 is categorically a network error indicating the resource wasn't found. It's possible that the live stream you're attempting to play is missing various chunks and that a different player may, by chance, not select to try and play those chunks. In any case, you should look at the specific URL for which the 404 occurs, and see if your device is able to request that URL successfully in something like a regular browser.
  This has been fixed in 2.x by https://github.com/google/ExoPlayer/pull/1803.
  Does the PlayReady sample in the demo app work correctly on whatever device you're testing on? What device is it that you're testing on, out of interest?
 - Do you have a (real) AndroidTV device to test with at all, such as Nexus Player, nVidia Shield TV box or one of the Sony TVs? We've seen compatibility issues with Fire TV devices where they haven't implemented Android APIs correctly. That's not to say that's definitely what's happening here, but it would be a good test on a device that we know does things properly.
- As for the demo app sample, could you give this another try? The error suggests to me that the server was down, but it seems to be working for me. It's possible that was a temporary issue. If you're still seeing the same error, it would be interesting to see what happens if you navigate to `http://playready.directtaps.net/pr/svc/rightsmanager.asmx` in a web browser.
 I'd suggest you follow up with Amazon about this issue.
  Yes
  Track selection is pretty complicated. The current approach we take has quite a lot of niggles:
- Filtering of tracks by device capabilities doesn't happen in the right place. It should happen in the renderer, since the renderer is the thing that knows what it's capable of playing. Right now filtering happens way upstream for DASH/SS/HLS and not at all (beyond checking that a decoder exists for the mime type) when using ExtractorSampleSource.
- For DASH/SS/HLS the application doesn't have a nice way to select an adaptive track containing a subset of tracks that match the device capabilities. This would be a good thing to support.
- We don't expose tracks that aren't supported to the application. This would obviously be nice so that the application knows what happened. Right now a video playback where we only support the audio stream looks identical to an audio playback to the application.
- The player makes an initial track selection and the application can only change it asynchronously (i.e. via a process that involves messages passing between threads). This leads to inefficiency in the case that the application wants to select a different track. It also causes problems if the initial track selection for some reason doesn't work (and the application knows this somehow), as you mention above.

We do have plans to fix all of this, although it's quite a lot of work. I'm envisaging something like:
- DASH/SS/HLS components will not filter tracks based on device capabilities or generate an adaptive track. They'll just list all individual tracks. Tracks will be marked with a group if they can be used with other tracks in an adaptive way.
- TrackRenderers will mark tracks as playable or not based on their capabilities.
- All tracks will be exposed through ExoPlayer's API, including those not playable for information purposes.
- We'll allow synchronous track selection on the player thread by letting the application inject some kind of track selector object. This is complicated, because it needs to consider all renderers at once to be most useful, rather than having a separate selector for each renderer separately. This is because in practice dependencies might exist. For example an application might want to enable a subtitle track only if an audio track doesn't exist for a certain language. Note that the injected track selector object will also be of use for things like full multi-period support in DASH where formats can change moving from one period to the next. In such cases I'd envisage the selector that has been injected being invoked again for the next period.
 - The injected track selector would be able to select multiple tracks belonging to the same group to enable adaptive playback, if the renderer says that it supports it.
 This is fixed in the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental). You can implement a custom [TrackSelector](https://github.com/google/ExoPlayer/blob/dev-2.X.X-experimental/library/src/main/java/com/google/android/exoplayer/TrackSelector.java) to perform arbitrary selection of tracks for each renderer.

Note that for nearly all use cases, using [DefaultTrackSelector](https://github.com/google/ExoPlayer/blob/dev-2.X.X-experimental/library/src/main/java/com/google/android/exoplayer/DefaultTrackSelector.java) and just implementing a custom [TrackSelectionPolicy](https://github.com/google/ExoPlayer/blob/dev-2.X.X-experimental/library/src/main/java/com/google/android/exoplayer/TrackSelectionPolicy.java) should be sufficient.
  The best approach is to create your own `DataSource` to perform the decryption. Your implementation should wrap another `DataSource` that can be used to read the still encrypted data (you'd probably wrap a `FileDataSource`, but there's no reason not to allow wrapping of arbitrary sources). The read method of your implementation should read encrypted data from the wrapped source, decrypt it, and return the decrypted data to the caller.
  Thanks!
 Oops. This was on the wrong branch. I've cherry-picked it to dev in https://github.com/google/ExoPlayer/commit/9cdd246a555d3e0bd4d304293353b3644167fff9, and will be reverting it from master.
  See #1157.
  That is in case you want to reproduce local files through the DemoPlayer. By default, there are none. But if you added any to the Samples list, and the permission was not asked for, the app would crash.
  The initial description doesn't sound quite right to me. That aside, whether the front 8 seconds are unencrypted or not shouldn't make much difference (i.e. I don't think it would cause this kind of issue). I think we'd need further information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) to investigate further.
 ExoPlayer 1.3.1 is way too old for us to be supporting. Please try and reproduce with 1.5.3. Thanks!
 That seems unrelated to this issue, but you should ask for advice on a mailing list associated with the tool that you're using to package your content.
  Are you certain this is only after proguard? Last time this kind of issue was reported it turned out it occurred without proguard too. If it is only with proguard, you'll have to try playing around with your proguard configuration to see what causes it.

I'm not sure exactly what version of Android you're running, or on which device. The stack trace:

```
Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'boolean java.lang.String.equals(java.lang.Object)' on a null object reference
    at android.media.MediaCodecInfo.getCapabilitiesForType(MediaCodecInfo.java:2150)
```

Doesn't match up with 5.0 source code (there's no call to `String.equals`) that [here](https://android.googlesource.com/platform/frameworks/base/+/android-5.0.2_r3/media/java/android/media/MediaCodecInfo.java#2150)
 I don't think it's possible for an OEM to have modified the code, no. Is it possible that the line numbers have changed because they're not being retained through your proguard step?
 Please can you provide a stack trace that at least has the line numbers retained. When every line says "Unknown Source" it's very difficult to work out what's going on. You should be able to retain line numbers, with:

```
-renamesourcefileattribute SourceFile
-keepattributes SourceFile,LineNumberTable
```

As per [this thread](http://stackoverflow.com/questions/10158849/android-proguard-return-line-number).
 MediaCodecInfo may be customized in the platform on this device (it most likely is given the stack trace). But your proguard config shouldn't make any difference to that class since it's not part of your apk.

The stack trace suggests that the value of `supportedType` passed to `getCapabilitiesForType` from `MediaCodecUtil.getMediaCodecInfoInternal` is probably null. However this should be impossible because the line above in `MediaCodecUtil.getMediaCodecInfoInternal` dereferences `supportedType`. If it were null, it should fail on that line.

All I can think of is that proguard manipulates `MediaCodecUtil` in a way such that the resulting code then hits a bug in either dexopt or the vm on the affected devices. I have seen one other instance of this, where a proguard optimization generated code that was then subjected to an incorrect dead code elimination optimization. In that case the result was a seemingly impossible index out of bounds.

If the device has both Art and Dalvik, and if you're particularly interested in testing this theory, you could try switching in developer options. If it's a dexopt or vm thing then the issue would normally only affect one of them :).
  - Do the bundled clear samples play properly in the demo app? What about the bundled encrypted ones?
- Can you test an equivalent clear sample to the one you're having difficulty with. Does that one work?
- When playing the affected stream in the demo app, if you bring up the controls, is there a Video button at the top listing some available video streams, or are the Audio and Logging buttons the only ones that are present?
 - H265 is only supported on Android 5.0 and above. See [here](http://developer.android.com/guide/appendix/media-formats.html).
- The CIPVorbisDecoder issue is fixed in the `dev` branch by https://github.com/google/ExoPlayer/commit/9bcd1069b110fc7158f630905d93ecbe17da5a1f.
- Most devices that are not AndroidTVs do not support PlayReady.

I think you'll need to provide a sample stream for us to debug this further. If you're using H265 then it would be working as intended that you don't get video due to the first point above. Please also try and play an equivalent clear stream; which will separate out DRM issues from anything else.
 It's possible the device just isn't capable. As above, we can't really help more until you (a) test an equivalent clear stream to properly determine whether it's a DRM issue or just an issue with the content, and (b) provide a sample stream once this is done.
  This happens because ExoPlayer assumes that there will be at most one video frame per PES packet in the stream. The sample provided packages multiple video frames into each PES packet, which results in us assigning the same timestamp to all of them.

The fix will be to parse the frame-rate from the stream and use of it to increment the frame timestamp in this case.
  It feels like it should be possible for an MKV to define the duration regardless of the fact that it doesn't contain audio/video?
 I wonder if using `-t` can be used to set the duration. See [here](http://ffmpeg.org/ffmpeg-all.html#Main-options). Just a thought.

Closing as something we shouldn't need to handle :).
  Please provide a sample stream. The failure is pretty deep in the network stack underneath of ExoPlayer. It could be a server or client side issue, I'm not sure just from looking at the stack trace.
 If you're using http and have the ability to do so, it would also be an interesting exercise to try the same stream over https. Doing so will limit the ability of a man in the middle, for example a network provider, to modify the request/response. If this does turn out to be provider specific, then I'd take a wild guess from the stack trace that the provider is trying to compress the response but is in some way not getting it quite right.
 I think the conclusions so far are:
- There's some kind of bad interaction between the way ExoPlayer requests data and handles the responses and certain carrier networks. The bad interaction appears to be related to the compression of the response.
- Disabling gzip as @ciribob describes is a workaround. Using https rather than http almost certainly also resolves the issue.

What's unclear is whether the bad interaction part is the carrier network doing something that's "flat out wrong" or whether it's doing something that's "a bit strange but technically valid". We should take a look at that and try and work out exactly what's going on.
 I am not able to reproduce the bug on Vodafone UK, using an HLS feed. It does not mean anything, so I will keep trying. If you could provide something for me to work on, that would really help. The issue is that the GZip header (https://www.ietf.org/rfc/rfc1952.txt [Page 5]) is failing to include the fixed values ID1 ID2, so I would go for the "flat out wrong" hypothesis. If it works on Wifi, the man in the middle theory seems the most according. Can you try HTTPS and attempt to recreate? 
  This should be fixed by 6eea4bdbc34dea86a5bc81991163ca26761cae83. Please reopen if it still doesn't work as expected. Thanks!
  You will have to provide a sample stream that reproduces this issue for us to investigate. As noted on another issue you filed, we cannot investigate issues when insufficient information is provided. There's a list of information that's useful when reporting issues [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
 The stream does not contain any IDR frames. Each segment should contain at least one IDR frame (preferably near the start) as documented [here](https://developer.apple.com/library/ios/technotes/tn2224/_index.html#//apple_ref/doc/uid/DTS40009745-CH1-MEDIASTREAMSEGMENTATION-USE_AT_LEAST_ONE_IDR_FRAME_PER_SEGMENT__PREFERABLY_AT_THE_START_).

Note that you can make the sample play by setting `idrKeyframesOnly = false` in the `H264Reader` constructor, however this should not be required for HLS and doing this can cause visual corruption (and in the worst case decoder crashes) on other correct streams.

So it seems this is an encoding issue with the media.
  Do you mean a completely external webvtt file, or a file listed in the HLS master playlist using a EXT-X-MEDIA tag? If completely external, I don't understand how your code would work, given you're not provided a URL for the WebVtt file anywhere. If listed in the HLS master playlist, this is not yet fully implemented, and is tracked by #151.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Re the CLA - There's a problem where your commits are under your gmail (personal?) account, but your CLA signature is under your work account. You need to either get your gmail account added to the corp CLA, or use your work account for your commits. Thanks!

Re the change - Hoping @andrewlewis will take a look, although it may take a while to clear the holiday backlog.
 CLAs look good, thanks!

<!-- ok -->
 The changes look good to me. Thanks for fixing this!
  If that's all on one line in the file then I think that's invalid. Each tag should be on its own line. Please clarify. Ideally, please provide the link to the actual m3u8 file. Thanks.
 I get `The page you are looking for is not found.` when I click on that link. Either it's broken or it's geo-restricted to a country that I'm not in?
 The problematic lines are:

```
#EXT-X-VERSION: 2
#EXT-X-MEDIA-SEQUENCE: 811
#EXT-X-TARGETDURATION: 10
```

Per the HLS spec, there shouldn't be spaces between the colons and the values. The provider should fix their HLS m3u8 to comply with the spec. We could of course be more lenient and allow this kind of mistake, but unless it's a widespread issue we'd rather have the provider fix the content.
  This isn't related to #1099. The video track of this file has an elst (edit list) box that defines a single edit with segment_duration set to 0 and media_time set to 3000. We interpret this edit as removing all samples from the track, meaning `trackSampleTable.sampleCount == 0` in `Mp4Extractor.processMoovAtom`. This results in the video track being dropped.

@andrewlewis - ISO 14496-12 added an example not present in earlier revisions of the spec that suggests this is a way to subtract 3000 from the sample timestamps. That would map the first sample timestamp to 0, which seems right. The spec kind of suggests this is only valid in fragmented MP4, although it's written in quite a confusing way. There is also this related [document](https://w3c.github.io/media-source/isobmff-byte-stream-format.html) (search for edts). Thoughts?
 In fragmented MP4, where we don't currently apply edit lists, a 0-duration edit makes sense because there may be no samples yet in the media timeline, and the edit can provide an offset for timestamps in subsequent fragments (as described in the specification). But for unfragmented MP4 files all sample timestamps are known so it is not necessary to specify an edit without a duration.

I'm not certain that the current implementation of dropping samples that aren't explicitly included by an edit is correct, but I guess it would be strange if the last edit was treated specially, so I'm tempted to leave this as-is. Alternatively, we could add special handling for the case of one 0-duration edit, and assume that no-one would deliberately make an edit list that removed all samples.
  ExoPlayer does not currently support this. Please clarify why you need to use UDP as opposed to TCP. I asked this on https://github.com/google/ExoPlayer/issues/966, but you didn't answer. Thanks!
 Note that https://github.com/google/ExoPlayer/pull/1083 will probably also (partially) fix this issue. Do you feel like generating and sharing some test files? I imagine the following cases are of interest:
1. Start of the stream not aligned with a TS packet boundary.
2. Some fractional number of TS packets removed from the middle of the stream.
3. End of the stream not aligned with a TS packet boundary.
4. All three of the above in a single stream.
 All three of the samples above appear to play fine in `dev-v2`, so closing as fixed!
 :). FYI the fixes are probably in the `dev-v1` branch too, although I didn't check. If they're not already, they likely will be soon (i.e. next week), since we've been back-porting most of our fixes around MPEG-TS robustness.
  ExoPlayer just renders into the `Surface` (typically encapsulated by a `SurfaceView`) provided by the application. How that `Surface` (or `SurfaceView`) is laid out in an application is the application developer's responsibility.
  The native crash is happening in platform code. I think in the platform's HEVC video decoder, but I'm not certain about that. You should report the issue to the device manufacturer. The video plays correctly on my Nexus 5X.

Does playback work correctly if you blacklist the HEVC decoder, which you can do by adding the following code block [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java#L185):

```
if ("OMX.rk.video_decoder.hevc".equals(name)) {
  return false;
}
```

This is likely dependent on whether the device ships with Google's HEVC software decoder in addition to the one being blacklisted. If this does fix the issue then we might consider a more targeted version of this fix (i.e. targeted at the specific device / api level in addition to the decoder name) as a workaround.
 That implies the device does not ship with Google's HEVC decoder in addition to the blacklisted one. You'll need to report the issue to the device manufacturer.
 A device only needs one decoder, however if there are two then blacklisting one can result in the other one being used (i.e. there's more redundancy there in case one of them is broken). I don't know why some work and others don't. It could be related to profiles/levels/resolutions, however regardless the failure shouldn't be in the form of a native crash. Please ask the device manufacturer.
  ExoPlayer supports MPEG2 embedded in MPEG-TS (see https://github.com/google/ExoPlayer/pull/915) and in WebM (see https://github.com/google/ExoPlayer/pull/858), where the device has an appropriate decoder. I'm not sure exactly what the media is that you're trying to play, but my guess would be that `MPEG-PS` is something that we don't support. Please provide more information if appropriate. Thanks!
 In general we'd prefer to do a great job of supporting a smaller set of formats than a less good job of supporting a wider set. When selecting formats, we prefer to support formats that are either widely used for streaming video over the internet, or forward facing and well suited to this purpose. MPEG-PS doesn't seem to fall into either category.

That being said, we would consider accepting a patch if someone were to send one, provided that it's simple and easy enough for us to maintain (and ideally complete with accompanying tests to help verify that we haven't broken it).  Adding support would likely consist of creating an MPEG-PS implementation of the `Extractor` interface. I didn't look into it, but it might be that MPEG-PS is similar or closely related to MPEG-TS. It may even just be the "contents of the TS packets" in MPEG-TS. If that's the case it might be simple to add support with relatively little code. If you/anyone wishes to investigate, please feel free to do so. We don't have resources to do this ourselves, however.
  What version of ExoPlayer are you using, and what's your Proguard configuration? Is it possible for you to reproduce the issue by applying your Proguard configuration to the ExoPlayer demo app, or not?
 Are you sure the same thing doesn't happen when using your HLS URL on the Genymotion Emulator _without_ proguard too? We've had quite a few issues that occur only on Genymotion Emulator and not on real devices; from which I conclude it doesn't do the best job of emulating when it comes to media.
 Most likely the Genymotion simulator just claim that it cannot support any of the variants available in the HLS playlist. You'd have to report the issue to Genymotion.
  What @se-bastiaan suggests should work. There's not a lot we can do to investigate further unless you provide complete information as listed [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).

You should also ensure that the asset really is being included in your apk. Are you able to play it using `MediaPlayer` or any other media library?
 Please see list of complete information linked above, and provide it. Thanks.
 Try using larger parameters for the default allocator and the ExtractorSampleSource in: 

```
ExtractorSampleSource extractorSampleSource=new ExtractorSampleSource(Uri.parse("song.mp3"),dataSource,new DefaultAllocator(1000),5000);
```

Such as the ones used in the DemoPlayer.

In particular, the ExtractorSampleSource gets blocked after lacking allocated space. You can check the ExtractorSampleSource class. To sum up, try

```
ExtractorSampleSource extractorSampleSource=new ExtractorSampleSource(Uri.parse("song.mp3"),dataSource,new DefaultAllocator(64 * 1024), 64 * 1024 * 256);
```

Just like in the DemoPlayer. 

Regards.
  @AquilesCanta - Fancy looking at this? `SmoothStreamingManifestParser` needs to be updated so that the `Format` returned by `TrackElement.getFormat` includes the language, which it doesn't currently. Note that `type` is an example of something that's parsed at the stream element level but used in the track element level, which is more or less what needs doing in this case too.
 @bartsidee - Your fix looks good to me (aside from the one comment in just added to one of the ref'd changes). Are you planning to send a pull request to fix this, or do we need to do it ourselves? Thanks!
 Fixed in https://github.com/google/ExoPlayer/pull/1155.
  Are you a developer of Viber, or a user? This sounds like an application issue, not an ExoPlayer issue. It's up to the application to pause/release the ExoPlayer instance at an appropriate time.
 This is an application developer decision; not something related to ExoPlayer.
  This issue is tracked by #1041.
  There's insufficient information here for us to debug the issue. Please open a new issue containing all of the information listed [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) and we'll take a look.
  `Aes128DataSource` is package private to the HLS package, and as such is not part of ExoPlayer's public API. It's intended for internal use only, and is not a fully functional `DataSource` implementation. See the comment at the top of that class:

```
 * Note that this {@link DataSource} does not support being opened from arbitrary offsets. It is
 * designed specifically for reading whole files as defined in an HLS media playlist. For this
 * reason the implementation is private to the HLS package.
```

If you wish to play something that you've encrypted, it's your responsibility to implement and use a `DataSource` that provides correctly decrypted data (including from arbitrary offsets).
  The root cause is the first of the errors:

```
com.google.android.exoplayer.ExoPlaybackException: java.lang.IllegalStateException: Buffer too small (65536 < 121807)
```

It appears that this device has a VP8 decoder (`OMX.MTK.VIDEO.DECODER.VPX`) that only allocates 65K input buffers, which isn't large enough. Does playback work successfully if you blacklist this decoder, which you can do by adding the following code block [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecUtil.java#L185):

```
if ("OMX.MTK.VIDEO.DECODER.VPX".equals(name)) {
  return false;
}
```

Once this block is added, you'll see one of (a) successful playback - in which case we'll apply a fix along these lines but targeted only at affected devices, (b) playback of audio only - in which case there's no alternate VP8 decoder and we'll have to do something else, (c) a different failure - in which case please provide details.

Thanks.
 Unfortunately it's not possible to replace the buffers when using platform provided decoders, because the platform is responsible (and requires ownership of) the allocations.

The next thing you can try is explicitly setting a `maxInputSize` on the format for VP8. In `WebmExtractor` [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/webm/WebmExtractor.java#L1160), try experimenting with adding the line:

```
maxInputSize = XXX;
```

where XXX is some sufficiently large value (it will need to be at least `121807` based on the logs you provided - and most likely more). You could try '1048576', perhaps. It shouldn't be necessary to do this, but if it works then the decoder at least respects explicit requests for larger buffers. We could explore a workaround along these lines if it looks promising.
 Having to specify the size up front is an unfortunate limitation of the Android platform decoders, and is why this problem exists in the first place.

@andrewlewis What do you think about having `MediaCodecVideoTrackRenderer.maybeSetMaxInputSize` do something for VP8/9 as well? I'm not sure what though. I think I'm right in saying that we don't have the max input size up front for WebM like we do for MP4, so setting it in the extractor probably isn't an option.
 The max input size can be set to match the default input buffer size of the software VP8 decoder, which should be safe.
 just a heads up, we have had to update the input size in the software decoder when issues came up with high resolutions like 2k and 4k. it might change again in the future.

nevertheless, here's where it is computed right now: http://androidxref.com/6.0.0_r1/xref/frameworks/av/media/libstagefright/codecs/on2/dec/SoftVPX.cpp#51
  Regardless of what version of Android the device you're using is running, you need to be developing with the latest version of the Android SDK (Android 6.0 API 23). It sounds like you have an older version of the Android SDK installed on your development machine.
  This is tracked by https://github.com/google/ExoPlayer/issues/676.
  This question is not about functionality supported directly by ExoPlayer, and therefore falls outside the scope of this issue tracker. Closing.
  `DataSource` is the interface through which ExoPlayer reads media data, so that's the interface that you'd need to find a suitable implementation of. We do not provide an implementation that wraps an `InputStream` directly because the `InputStream` interface doesn't provide suitable random access behavior for media playback, which is required both for seeking and because some media formats place data at the end of the file that must be read at the start of playback.

However, where you're able to create `InputStream` instances at a given offset into the data, or where you know that `InputStream.skip` will behave in an efficient way for the type of `InputStream` that you're handling, it is possible to implement a `DataSource` around `InputStream` instances. `AssetDataSource` and `ContentDataSource` are two examples for reading assets and data from content providers respectively.

Where are your `InputStream` instances coming from exactly, and where is the media located that you're trying to play? Knowing that would help suggest a suitable approach.
 To put it another way, why are you trying to use an `InputStream` as opposed to just giving ExoPlayer the URL directly? `InputStream` is not a suitable interface for full featured media playback.
 If you're feeding content from the `InputStream` directly into the `AudioTrack` then the data being provided by the `InputStream` is probably unpackaged and decompressed audio. ExoPlayer handles packaged and compressed media.

It's very unclear what the goals are here. What's wrong with your existing `AudioTrack` based solution? Given the apparent constraints (unpackaged and decompressed media through an `InputStream`), there's not really a huge amount that a library like ExoPlayer could do beyond feeding the data into an `AudioTrack`, which is what you're doing yourself already.

The constraints themselves sounds flawed/bad, although without knowing the exact use case it's hard to say. I'd suggest revisiting those, and if they really are sensible, sticking with an `AudioTrack` based solution.
  Depends on what the link is pointing too. A link is a link; I don't think there's anything special about Akamai links that would require specific support.
  It should also be noted that there are significant negative battery consumption implications around that kind of approach, so we don't really want to encourage widespread use of such techniques. In general:
- If your app is for playing long-form content this kind of approach really is best avoided. I know it's pretty annoying, but it's better for the user if you design around the limitation.
- If you app is for short-form content and you really need it, the approach can be useful.

It may be that [setOutputSurface](http://developer.android.com/reference/android/media/MediaCodec.html#setOutputSurface%28android.view.Surface%29) provides a more efficient solution from API level 23, although it's still slightly awkward if you don't have a surface at all for some period of time during the transition.
 There are quite a few variables that make providing a single answer quite difficult. The implications are hardware dependent, and obviously variables like how bright the screen is make a big difference (e.g. if the screen is as bright as possible then the screen will account for a far greater % of battery consumption than if the screen is as dim as possible).

With the above in mind and therefore treating the numbers with some caution, I have seen data that suggests using TextureView rather than SurfaceView can result in +30% consumption during playback (the results showed ~390mA draw with TextureView v.s. ~295mA with SurfaceView). I can't say how controlled the experimental setup was. Note that the approach described above for solving this issue will be at least as bad as TextureView; probably worse.

I was mainly referring to TV shows / movies by "long-form content". Although I guess what's really important is the total time users are likely to be engaged with your application. If users are likely to be watching video for extended periods, they'd be better served if you preserve their device's battery life in preference to fancy UI transitions.
 If you do gather some data around battery consumption then it would be great if you could share it here, when you have something (ditto for anyone else on this thread :)).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I believe we've fixed this in `dev-v2`, [here](https://github.com/google/ExoPlayer/blob/dev-v2/library/src/main/java/com/google/android/exoplayer2/extractor/ts/TsExtractor.java#L180), so closing this.
  The link doesn't work for me (in VLC, safari, just using wget etc). I think it's either dead or geo-restricted. Please can you provide a working link? Thanks!
  There is insufficient information to allow us to debug the issue. We'd probably need a link to the problematic content to tell you what's going wrong.
 The first thing that's apparent is that the manifest appears to declare muxed media (audio and video combined in a single stream). ExoPlayer only supports demuxed media where the audio and video are provided as separate streams in their own adaptation sets.

It's also unclear what your use case is. Given your manifest only defines a single representation, why are you using DASH in the first place as opposed to just using a (non-fragmented) mp4?
 The new mpd looks more in line with what ExoPlayer is expecting. Just saying it doesn't work isn't sufficient information to help debug the issue, however, especially given you haven't provided access to the actual media (only the manifest has been attached) or a bug report or log output. It sounds very much like a setup issue with your test content.

Please provide full information as specified [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) including a full bug report and a publicly accessible link to the media in a playable form. Ideally you will also have verified that your media plays correctly with some other player, such as [shaka](https://shaka-player-demo.appspot.com/).
 We really don't have time to be providing consulting services; sorry. It's up to you to generate your own test content. If you can verify that your test content works correctly in some other player (i.e. that it's an ExoPlayer specific issue) or provide more detailed information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) then we can take a look. Otherwise it's up to you to resolve the issue.
  Are the parameters actually part of the DASH specification, or a specification of some kind? If not, it's unclear why you expect ExoPlayer to recognize them?
 Thinking about this some more, most likely the query parameters cause the server to provide a different manifest as the response, in which case ExoPlayer wouldn't need to do anything special to handle them and they wouldn't need to be part of a standard.

I'm not really sure if you're constructing the URL correctly, but it's more common to use `UriBuilder` and methods like `UriBuilder.queryParam`, as opposed to just concatenating strings together, to ensure that the resulting URL is correctly escaped.
 A sample stream would be useful. Please send a link to `dev.exoplayer@gmail.com`.
 I tried adding samples to the ExoPlayer demo app's `Samples.java` with and without the starttime and stoptime parameters. Without them I got a live stream. With them I got the requested window, so everything seems to work correctly as far as I can tell.

This was using the current dev branch, although I'm pretty sure you'd see the same correct behavior with the current master branch too (1.5.3). Are you using an older version? If so, please update. If you can reproduce the issue within the demo app using 1.5.3 or the dev branch, please let us know.
  Is it possible to put the media in a container where we support seeking, like MP4 or Matroska?

As noted in #227, to seek in a .aac file (using AdtsExtractor) it seems that it is necessary to build an index having read the whole file, which is generally not efficient.
 @pottabathini Is there a specific problem you are seeing? Seeking in unfragmented MP4 and fragmented MP4 files with indices should just work.
 I'm afraid this is working as intended for the current design. See #896 and #978.
 We do not currently support retaining data before the current playback position, so you will always see buffering when seeking backwards, even in the latest version. Sorry if I wasn't clear in my previous reply.

As per the linked issues, we might change this behavior so that we only discard data if it's possible to buffer more future media (for example, if the entire stream fits in the buffer) but this is not a priority at the moment.
 Correction: we might change this behavior so that we only discard data if it's _impossible_ to buffer more future media (for example, if the entire stream fits in the buffer)
 Please ignore the previous correction! I meant to write that: we might change this behavior so that we only discard data if it's possible to buffer more future media. If the entire stream fits in the buffer, there is no more media to buffer, so the played data wouldn't be discarded.

Sorry the current functionality is suboptimal for your use case.
  - As per previous replies about this, MPEG-TS streams by themselves do not contain the information necessary to perform seek operations (short of downloading and parsing the entire stream in advance). We have no plans to support any non-standardized mechanisms for seeking within such streams.
- Using `seekTo(0)` to flush the player in step (3) seems like the right thing to do to me.
- If you want the absolute positions in the stream then you could try locally removing the addition of `timestampOffsetUs` that occurs [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/ts/PtsTimestampAdjuster.java#L83), and have `timeUs` be returned without adjustment instead.
- ExoPlayer supports MPEG2 embedded in MPEG-TS on devices that have a suitable decoder, as of 1.5.3. Note that since MPEG2 isn't required for Android devices, the correctness of an MPEG2 decoder likely wont be covered by Android's compatibility test suite. Hence it's entirely the responsibility of a device manufacturer to ensure that the decoder works correctly.
 If we were to allow passing of a "don't offset" value that could be passed to the `PtsTimestampAdjuster` constructor as the value of `firstSampleTimestampUs`, would that work for you? Note that you'd need to construct the `PtsTimestampAdjuster` in your own code, then construct a `TsExtractor` that uses it, then explicitly pass the extractor to the `ExtractorSampleSource` constructor.
 Opening again to track adding that (only).
  You could choose which renderer builder to use based on the URL like in [PlayerActivity.inferContentType](https://github.com/google/ExoPlayer/blob/1855a5a97d5fb6e6733d390148923a4c559f6546/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java#L693). This is not as robust as ExtractorSampleSource's sniffing, which looks for structure in the input stream, but may be good enough for some use cases. I don't think we have any plans to sniff playlists/manifests like we do for media containers, at least in the near future.
  The `MediaCodecVideoTrackRenderer` constructors take a `videoScalingMode` parameter. See [here](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/MediaCodecVideoTrackRenderer.html#MediaCodecVideoTrackRenderer%28android.content.Context,%20com.google.android.exoplayer.SampleSource,%20int%29). You should experiment with passing `MediaCodec.VIDEO_SCALING_MODE_SCALE_TO_FIT_WITH_CROPPING` as the value for that parameter.

You probably don't want to use `AspectRatioFrameLayout` in conjunction with this parameter. It might make more sense to use `SurfaceView` directly, or alternatively use `AspectRatioFrameLayout` but set the ratio based on the desired ratio of the view, not the ratio of the video.
  The timestamp information in the stream's video track seems to be incorrect: the last sample_delta in the stts box has the value 0xFFFFFFB8, which would imply an incorrect (too large) duration for the final sample. Although the ISOBMFF specification technically permits the value, ExoPlayer throws an exception as it can't be represented using a positive 32-bit signed integer.
 Closing as a media error.
  `BehindLiveWindowException` can occur when playing a rolling live stream in which segments become unavailable after a certain period of time. For example a rolling live stream might allow you to request segments that correspond to the most recent 2 minutes of the broadcast, but not earlier. In this case we'd say that the window is 2 minutes in length.

`BehindLiveWindowException` is thrown if the player attempts to request a segment that's no longer available. This is typically caused by:
- The window being unreasonably small. In this case the fix is to increase the window on the server side.
- Repeated client buffering causing the client to be playing further and further behind the broadcast until it eventually ends up trying to request segments that are no longer available. In this case the fix might be on the server side if the buffering is due to a server issue. If it's not a server side issue, it's most likely just that the client doesn't have sufficient bandwidth. In this case preparing the player again will allow playback to resume. It should be possible to do this immediately (the 3 - 5 second you mention wait shouldn't be necessary). In previous releases something equivalent happened internally, but we opted to propagate the error and require that applications do this instead. This was for consistency reasons, and also because some applications may wish to do something different. See https://github.com/google/ExoPlayer/issues/765 and the change referenced there for details.
 Note that most often the exception is caused by a combination of the two bullet points above. Repeated client buffering will only cause the exception if the accumulated duration of time spent buffering exceeds the window size minus the duration that the client wants to buffer locally (typically ~30 seconds). So the larger the window size, the less likely this is to occur. I would say that you'd want the window to be at least ~2 minutes.
 You should really just fix the source content as per my initial response.
 That doesn't sound right. Are you sure the variable you're looking at isn't the chunk size, for which a 10 second limit would make sense.
 Window refers to (N-1)*(SegmentDuration), where SegmentDuration is 10s in your case, and N is the number of segments that are listed in each HLS playlist at any given point in time.

I suspect you'll find that your HLS media playlists only contain 3 or 4 segments at any given point in time. Increasing that would help you to avoid this issue.
 I'd be surprised if Akamai don't allow configuration of the window length. How many segments are listed in your manifests, at any given point in time?
 Right, and that's your problem. It should be possible for you to increase that somehow.
 You should request that the provider resolves the issue. Fundamentally you're trying to play a live stream with a live window of insufficient size. It's a content issue. It can only be properly resolved by the content provider.

The best thing that you can do on the client is as @kientux suggested further up this thread, which is to automatically restart the playback each time it fails in this way. This will not be seamless, but it will allow playback to continue once it buffers again.
 So get the provider to fix their content. It's simply not a client side problem if content is not provided correctly.
 Specifically for HLS, it seems this issue can be caused by non-aligned segment numbers across the different variants, in addition to the other causes mentioned above. Non-aligned segment numbers are permitted by the HLS spec, and therefore failing to correctly handle this case is an ExoPlayer issue. It's kind of tracked by #699, but it would be good to have a bug explicitly tracking this resulting in `BehindLiveWindowException`.

@huykn - Are you able to provide test content that reproduces this issue? If so, please file a new issue on this tracker providing all of the information requested in the issue template, and we'll use that to track the problem.
 Note - There are fixes in `1.5.10` and `2.x.x` releases of ExoPlayer that help a second root cause of BehindLiveWindowException in HLS playbacks.
 - Is this still actually an issue in `1.5.10`? As per my comment above, we pushed some fixes as part of that release.
- I wouldn't class a non-productionized sample command as evidence of best practice. Note that they're actually overriding the default value (=5) of their own tool with a smaller value (=3) in that sample command, without giving any justification. I can't see any good reason for doing this, where-as it _will_ definitely make failures/re-buffering/content-jumps more likely across all platforms by restricting the window length to be unnecessarily short.
 @ishantsagar - Please don't cross post on multiple issues (i.e. this one and https://github.com/google/ExoPlayer/issues/1782), it adds unnecessary noise to the issue tracker. Please continue this discussion on the open issue, which is https://github.com/google/ExoPlayer/issues/1782.
  I am in quite a situation here, my audio and video delay are both working fine if I am rendering both audio and video.
But if I stops rendering the Video and only Audio renders at that time audio delay is not working. To make it work I tried to change  the handleBuffer(ByteBuffer buffer, int offset, int size, long presentationTimeUs) method in Audio Track.java class.

I added & Subtracted offset to each of the different times like, bufferstarttime, startMediaTimeUs, presentationTimeUs but none of this is working. I also tried by adding an offset into GetPositionUs method but all this is changes the seek position that's it. They don't delay the audio

As per above comments if I do the change in MediaCodecVideoTrackRenderer, then also it won't be in effect because I am not rendering Video at all. 
 Below are my use cases:

1) I am playing MPEG DASH streaming on a tablet, in that case my Audio and Video both are played on one tablet and I can able to delay both of them by changing TimeStamps and releaseFrameTimings as per my comment. This is working fine

2) I want to play MPEG DASH streaming in such a way, that the Video will render on the Monitor and Audio will render on the tablet. For this, what I did is, stop rendering the Video on the Tablet. But in that case when I hit the play button, Audio on the tablet is going ahead than the Video on the monitor. That's the reason I want to delay the audio by couple of milliseconds so that I can make both Audio and Video in sync.

Sorry for my less understanding language, but I hope you get an overall idea what I want to do and why I want to delay the audio.
 - What's the use case for (1)?
- I don't really understand (2). Are the two devices playing independently, or is the monitor connected to the tablet? If they're separate devices then what's the synchronization mechanism between the two?
 1) use case for first point is I want to delay both audio and video on a tablet only.

2)Yes, those are two devices independently. Two devices are working in this manner. On first device user will press the Play button, so that a message will go to the second device to play the video and as soon as second device started playing video it will send Audio TimeStamps (bufferinfo.PresentationTimeUs) to the first tablet and that tablet will receive those timestamps and adjust their position accordingly and will play the audio at that timestamps.
 I did it. I exchanged the Timestamp (Bufferinfo.presentationTimeUs) which is used in ProcessOutputBuffer method of MediaCodecAudioTrackRenderer.java class. I multicast this timestamp from the second device (which is playing AV both) and receive it on first device (which is playing audio only), but when I do fast forward on second device, it's not adjusting the audio on first device instead it adjust only the progress bar timestamp.
 I still don't understand what the point of (1) is. I understand what it does, I just don't understand why you would want to do it. If you're playing only on the tablet and you're delaying both audio and video, isn't that more or less the same as delaying neither of them? What are you actually trying to achieve?

For (2) I think you'd find it easier if you were to make the video on the monitor render earlier rather than having the audio on the tablet render later.
 @Bastian35022, I see that whenever we fast forward it's calling the seekTo function of MediaCodecAudioTrackRenderer then TrackRenderer and then SampleSourceTrackrenderer.java class. Thank you very much for your help.

Now the same way I am calling the seekTo function, if I found a sudden jump in the Timestamp which I am receiving on first device, from MediaCodecAudioTrackRenderer class, but it crashes the app. Is there anyohter way or place where I can call this seekTo() function to sync the audio?
 @ojw28, I want to delay the video & Audio for (1) point because, there is a bluetooth latency in android and this latency is different for different devices.

for (2), i can do that, but where I am stuck is if user FF the AV on monitor it should sync the audio on tablet which is not happening right now. For that i need to go with @Bastian35022 suggested way.
 @Bastian35022, I called the player.seekTo() in player activity where I am receiving time stamps and when the difference between the two time stamps is greater than 2 seconds. And it seems it's working correctly and adjusting the audio chunks accordingly.

Thanks a lot for all your help guys...
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I'm not sure what the intention is here, but this doesn't look right.
 CLAs look good, thanks!

<!-- ok -->
 Please see comment above.
 It's working as intended that the listener is invoked when playWhenReady changes. It's even explicitly documented that way [here](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/ExoPlayer.Listener.html#onPlayerStateChanged%28boolean,%20int%29). So I don't understand what this change is actually trying to achieve. It looks like it's removing correct behavior.
 Closing this because I still don't understand what it is. I'm almost certain this isn't necessary, and no-one else has reported any issues with how the listener works.
  The device is telling the player that it's not capable of playing some of those streams, so we don't try and play them. I think this is working as intended as far as ExoPlayer is concerned.

This issue is most likely device specific. What device are you reproducing the issue on exactly, and on what version of Android? Could you paste the output of `adb shell getprop ro.build.fingerprint` on the affected device into this issue.
 It appears that by default, Android requires dimensions to be even rather than odd. See [MediaCodecInfo line 1080](https://android.googlesource.com/platform/frameworks/base/+/android-5.0.0_r2/media/java/android/media/MediaCodecInfo.java#1080) and the use of `mWidthAlignment` and `mHeightAlignment` in the [MediaCodecInfo.supports](https://android.googlesource.com/platform/frameworks/base/+/android-5.0.0_r2/media/java/android/media/MediaCodecInfo.java#975) method.

It's unclear whether the dimensions you're using are really supported, or just "happen to work". I'll follow up internally. Orthogonal to this, it feels like it would be preferable for you to use even dimensions when transcoding your content. I didn't read through in detail, but if you do an internet search for "h264 odd dimensions" you get a bunch of results suggesting that it's a bad idea.
 Also, is it possible for you to provide a sample stream (here or to `dev.exoplayer@gmail.com`). It would be interesting for us to take a look at what's in the stream itself.
 The dimensions in the manifest do appear to be wrong. The actual widths are 1280 (where the manifest claims 1277) and 1920 (where the manifest claims 1921). The heights appear to be correct. So yes, correcting the manifest will fix this issue for you.

Supposing that your video streams really did have odd dimensions and that the manifest were correct, it's unclear whether we did the right thing in filtering. I suspect we did and that such dimensions really aren't supported, but I'll find out to make sure. Leaving this issue open for now to track this.
 I didn't hear back about the odd dimensions, but closing seeing as we've not actually seen any streams that (genuinely) have them.
  Thanks for sending this! It looks like an interesting approach :). Playlists are definitely something that we want to do a better job of handling. As you may have observed, there are a few things that make this difficult currently. The main issue is that ExoPlayer doesn't support changes to the available tracks and track formats during a playback. For general purpose playlist support as well as for DASH multi-period manifest support (which is conceptually similar), this is a must. We don't want to disallow playlists from containing both MP3 and MP4 files, for example.

I think the first step toward supporting general purpose playlists will be to allow dynamic binding of SampleSource to TrackRenderers. Moving from one piece of media to the next would then involve (approximately speaking) rebinding the TrackRenderers to a new SampleSource, and at this point we'd permit the set of tracks to change. On top of that, for a general purpose solution, we'd also like to properly manage buffering when transitioning from one piece of media to the next (e.g. if the policy is to keep 30 seconds of media buffered and we're 10 seconds from the end of one piece of media, we should have buffered 20 seconds of the next piece).

We're hoping to get to looking at this pretty soon now, so stay tuned. As a final note, in general I don't think it will be viable/sensible to prepare all of the sources up front. That would be a significant amount of work for long playlists. This implies that the application would need to obtain media durations from elsewhere (or alternatively accept that the durations are unknown). I think this is reasonable though; something like a music application most likely already has a database containing this kind of information.
 There are multiple APIs that you can use to achieve gap-less looping audio:
- For small sound effects (<1MB), `SoundPool` can do this.
- You can use two `MediaPlayer` instances and its `setNextMediaPlayer` API.
- You can use `AudioTrack` directly and feed in the looping audio yourself.
 Two MediaPlayer instances should work. Are you sure the media you're playing contains gapless metadata information? If the media isn't designed to be looped in a gapless way, it's not going to be gapless.
 That doesn't sound particularly scientific ;). The metadata that needs to be included is format dependent. You'll need to do some research I think. A good place to start would be [here](https://en.wikipedia.org/wiki/Gapless_playback#Format_support).
 Support for playing sources back-to-back is landing in 2.x. Many of the required parts are already in place. For example [SampleSourceProvider](https://github.com/google/ExoPlayer/blob/c2b89d62859c67e4a589de1f6b6acfd605d5099f/library/src/main/java/com/google/android/exoplayer/SampleSourceProvider.java). We don't intend to add support to 1.x, therefore closing this pull request.
  Are these issues specific to the devices you mention? If you, you'll need to report them to Huawei.
 If you can provide sample content that reproduces the issue, we can try on a different device.
 Given you don't know what the root cause of the problem is, it doesn't seem right to be trying to fix it by replacing pieces of code with other pieces of code. You need to diagnose what's actually wrong so that you make sure you're fixing the right thing, I think?

You can download the stream locally and email it as an attachment to `dev.exoplayer@gmail.com` if you'd like us to take a look. Otherwise, we don't have enough information to debug the issue.
 We don't have the information to look at this, so closing.
  You'll need to provide links to the media that doesn't work, as opposed to the media that does work ;).
 It appears the stream is invalid (it contains an invalid `program_info_length` in one of the program map tables defined in stream, which is many times longer than the whole program map section).

The mkv and/or mp4 issues will be different (although likely related to the media rather than the player). We'd need separate samples for those to diagnose the issues.
 There is no exhaustive logging you can enable. MKV is supported, but there's a difference between the container (MKV) and the underlying streams that they contain (e.g. H264+AAC). It's most likely the case that the MP4/MKV files you're using contain audio streams for which Android does not provide decoders. If you're using Linux, you can get the details of the underlying streams using:

```
mediainfo your_file
```

on the command line. I think VLC also exposes information about the underlying streams in a menu somewhere. Could you dig that information out, and post it here? Thanks!
 - For the MKV sample, Android does not support AC-3 (see [here](http://developer.android.com/guide/appendix/media-formats.html) for a list of supported formats), which is why you don't hear any audio. Some AndroidTV devices do provide support, and on such devices you will get audio. We have a longer term plan to explicitly expose tracks that are not supported through the player, to indicate to the application that we know they're there but that we can't play them.
- I think the MP4 should play with both audio and video. Is there an issue with that sample, or not? Please test on the latest release or with the dev branch, since we fixed some issues relatively recently.
 You can add the following code block immediately above the point in `TsExtractor.PmtReader.consume` where the failure occurs if you want to avoid it:

```
if (programInfoLength > sectionData.bytesLeft()) {
  Log.w(TAG, "Invalid programInfoLength: " + programInfoLength);
  return;
}
```
 We wont be adding the workaround unless we see many (important) streams with the issue. See my rant near the bottom of https://github.com/google/ExoPlayer/pull/1064 for an explanation about why. Closing this issue, since it sounds like you're able to move forward at this point. Thanks!
 @stuckless My mistake. The TS file is actually valid in this case. The reason for the playback failure is the one reported in issue #1001. In this case, we're treating a PID with value 0x1F as a PMT, which according to that issue is incorrect. That would imply the fix I suggested isn't really the correct one. Please follow #1001 to track delivery of a proper fix.
 The TS issue is fixed by the change ref'd above.
  Thanks for the detailed report. We'll take a look.
 It appears that this is caused by a bug in Choreographer, which was [fixed in Android M](https://android.googlesource.com/platform/frameworks/base/+/3b4049e). We will try to implement a workaround for older builds by keeping only one Choreographer instance per process.

@ptanov, you mentioned testing on Nexus 5 Android 6. Please could you paste the output of `adb shell getprop ro.build.fingerprint` for the device running the newest Android build where you saw this issue? The bug I referred to should be fixed on Marshmallow builds, so if it's reproducible on those builds then there may be another issue, or we have not diagnosed it fully. Thanks!
 This should be fixed by the commit referenced above. If you could give it a try and report back any findings, that would be great!
  The media is malformed. Specifically, the SEI data in the H264 streams appears to be incorrectly truncated. If I play the TS segment in VLC on desktop I see related errors spammed into the logs (although playback doesn't fail):

```
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c97840] SEI type 49 truncated at 103
[h264 @ 0x7efdc8c95280] SEI type 49 truncated at 103
```
 Even if we were to fix the player to be robust against the incorrect truncation, it looks like there's something else wrong with the stream that causes the video to flash green about 1 second into the playback on my test device.

The right thing to do here is to fix the media. Whilst we could make the player robust against the incorrect truncation, such fixes make the player less efficient for everyone using correctly encoded media. Hence we only make such fixes if the issue is widespread in existing media. Since we only have a single affected sample currently, we wont be fixing this.
  I don't think this is a correct fix (I think it would just hang the playback in the buffering state forever, which is arguably worse than playback failing). Happy to be proven wrong though. Do you have a test stream and/or steps to reproduce the issue?
 Well, it'll definitely fix the crash. If it's also allowing playback to proceed successfully, I'd hazard a guess that there's actually something wrong in the HLS stream you're providing.
 I  disagree that we should handle incorrect streams because other players do so. I think I'll write a blog post about this at some point, but fundamentally it seems to me that the reason so many invalid streams exist in the first place is because other player implementations have done this. You end up in a pattern where:
1. Someone writes a specification for a stream format.
2. Someone produces a bad stream that doesn't adhere to the format.
3. Someone makes a player that plays the bad stream.
4. Other players are updated to also play the bad stream based on the "player X plays this" argument.

The end result is that:
- All players contain a whole bunch of messy workarounds to handle various types of badly formatted streams. Not only is this hard to maintain, but it degrades the efficiency with which these players can play correctly formatted streams because the executing code has to check for all these conditions.
- The specification that was originally defined is no longer an accurate reflection of reality. It therefore becomes more difficult for someone to implement a new player based on the specification and expect it to actually work with the majority of streams.

This clearly isn't good for the video ecosystem, in general, and I wish all players would stop accepting workarounds for badly formatted streams in order to fix it.
 Some of the chunks in the stream provided randomly don't contain any video (or declare a video stream in their program map tables). At best that's highly irregular. IMO the provider should fix the streams to ensure that each chunk contains both audio and video.
  The file contains Opus audio. Opus is only supported on Android at the platform level from 5.0 and above, as shown [here](http://developer.android.com/guide/appendix/media-formats.html). If you require playback on older platforms levels then you should use something that's more widely supported, like AAC.
  In the manifest, `timeShiftBufferDepth` defines the window of media that the server makes available to be only 10 seconds long. `DashRendererBuilder` in the demo app tries to start playback 30 seconds behind live (defined by `LIVE_EDGE_LATENCY_MS`). This is 20 seconds behind the oldest media that's advertised as being available by the manifest. Hence playback fails within `BehindLiveWindowException`.

A 10 second media window isn't really sufficient for acceptable streaming performance for mobile devices. Unless low latency is a critical requirement, it's generally much better for a mobile devices to be playing about ~30 seconds behind live. This allows the client to maintain 30 seconds of media in a local buffer, which then allows it to seamlessly survive temporary networking issues lasting less than this duration. To do this obviously requires `timeShiftBufferDepth` in excess of 30 seconds (I would expect at least 1 minute, and it's quite common to specify windows in the order of a couple of hours).

It appears that the server actually has a much larger window of available media than the 10 seconds advertised in the manifest. If I hardcode the window to be two minutes rather than parsing the value from the manifest, then playback works fine. So I think the fix is to reconfigure the manifest and/or server.
  An OutOfMemoryError tells you that your application has run out of memory, but does not tell you why. The point at which an OutOfMemoryError is thrown (i.e. the stack trace above) is often relatively meaningless. It may well be being thrown because some other part of your code is leaking or using an excessive amount of memory.

You should probably do some investigations around RAM usage of your application. [This guide](http://developer.android.com/tools/debugging/debugging-memory.html) may provide a good starting point. Try and find out how RAM is used by your application, and what for, and whether it's expected or not. Also look at whether it grows over time (indicative of something like leaking of Activity instances). You may find some issues that you need to resolve.

Depending on what OS version you're seeing the failure on, setting `android:largeHeap` to true in your manifest resolve the issue for you in the case that you simply need a bit more memory for your application to run.
  We don't support WebVTT for HLS yet. Tracked by #151.
 The ability to parse a single vtt file doesn't imply that we support segmented WebVTT described in a HLS media playlist. This is not something we support yet.

Also, if you're not doing this in the context of an HLS playback (i.e. video/audio also delivered using HLS) then this use case is really strange, and is likely something we will never properly support. If it is in the context of an HLS playback, we will be adding support as tracked by #151.
  In what context? DASH, SmoothStreaming or HLS? Please provide more detail about your use case. Thanks.
 Is the purpose of keeping to the lowest bitrates to avoid exhausting a limited data plan, or because higher bitrates will result in re-buffering due to limited bandwidth?

If the question is about the former, you can implement your own `DashTrackSelector` and `SmoothStreamingTrackSelector` instances to limit the streams that get selected in DASH and SmoothStreaming respectively. For HLS, you can specify a `variantIndices` parameter to the `HlsChunkSource` to restrict which variants are selected.

If the question is about the latter, ExoPlayer dynamically selects a stream based on the measured available bandwidth, and so this shouldn't be a concern.
 Note that you can also manually select a specific fixed format using `ExoPlayer.selectTrack` for DASH and SmoothStreaming (we're working on providing this functionality for HLS also).
  Presumably the m3u8 file you link to is part of a larger HLS playlist (i.e. it's referenced from a HLS master playlist)? In which case the issue is tracked by #151.
  `HttpDataSourceException` contains a `dataSpec` field that tells you what data was requested. So look at that. In this case the status code will have been in the 2XX range. If the status code is outside of this range then you'd get an `InvalidResponseCodeException`, which has a `responseCode` field that tells you what the status code was.
 It's unlikely to be an issue with clip length. The root causes are all quite far down in the networking layer, below any layers in which media specific things are happening. I would guess there's a server-side issue. This part of the log is of particular interest:

```
Caused by: java.net.ProtocolException: unexpected end of stream
  at com.android.okhttp.internal.http.HttpConnection$FixedLengthSource.read(HttpConnection.java:449)
  at com.android.okio.RealBufferedSource$1.read(RealBufferedSource.java:168)
```

I think this is indicative of a server providing a `Content-Length` value in a response header that does not match the length of the response body (the body turns out to be shorter). If my understanding is correct, that kind of behavior is invalid from the server.
 Closing as a probable server error.
  Thanks for this. The change needs to go into the `dev` branch though, not `master`. Please send a pull request to that branch.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  Genymotion isn't a good platform for testing whether ExoPlayer works correctly. When you say that you see the crash on a wide variety of devices, are they real physical devices, or different devices emulated by Genymotion?

It's definitely not intended behavior.
 By the way, for programmatically invoking changes on the player (e.g. seeking / playing) you should really be calling the methods directly on the `ExoPlayer` instance. You shouldn't call through the `PlayerControl`.

Note that if you invoke changes on the player directly as recommended, you can restore the state and set playWhenReady to true before you even start preparing the player, immediately after you've created the instance. This may significantly simplify your application logic.
 Closing due to lack of information.
  It seems unlikely this is device specific; I suspect some luck might have been involved for you to see the issue on some devices and not others. A failure like that is likely indicative of an incorrectly formatted DASH manifest. Do you have a piece of sample media that we can reproduce the issue with?
 The issue here is that each of the video representations has a different number of segments, and that their segment boundaries are non-aligned. I think this is technically valid according to the DASH spec, since your manifest doesn't specify `subsegmentAlignment="true"`, but it's not something that we support or will be supporting. Segment boundaries need to be aligned to allow efficient switching from one representation to another, so if you're not aligning them then you're not taking advantage of one of the primary benefits that DASH provides. I didn't double check, but I'm fairly sure that [DASH IF Interoperability](http://dashif.org/w/2015/04/DASH-IF-IOP-v3.0.pdf) does mandate that subsegments are aligned.

So yes, the solution is to regenerate this content such that subsegments are correctly aligned. I'll leave this issue open, but only to track not crashing the process when this type of media is encountered.
 I didn't mean to close this. The issue is tracking making playback fail gracefully (rather than causing process death). Note that we wont be supporting media where the subsegments are not aligned.
  Please provide full unfiltered with system messages log and an ADB bug report.

thanks,
 @PetrSklenicka,  any updates? 
almost 3 months without a ADB Bugreport. Will monitor for a couple more weeks then will close.
  Should be fairly simple to do this. I will do it shortly (~couple of days)
  Yes. That log line would be output when trying to create a decoder for audio/vnd.dts on a device with no decoder that supports it. Like most phones, Nexus 6P does not have a built-in DTS decoder. If possible, it is best to switch to a more widely supported format like AAC.

Note that cf27b83 added a check that there is a decoder for the source MIME type, before the track is enabled. So, on an up to date version of ExoPlayer, you shouldn't see this error if the source MIME type is accurate, and the stream should play with no audio (unless other playable audio formats are present). Is this a DASH stream with an incorrect codecs attribute?
 You could try changing codecs="mp4a.a9" to codecs="dtsc" to make the codecs attribute consistent with the actual media format. That should hopefully give video-only playback, as no decoders will be found for the audio stream.
 It could skip AC-3 for the same reason it skips DTS: MediaCodecAudioTrackRenderer only handles the track if the device has a decoder for the track's format, or if there is an HDMI connection that advertises support for playing the format via encoded audio passthrough (on Android L and above).
  Please make sure you're using the latest version (1.5.3 which was released yesterday, or the dev branch). I believe there were some fixes in 1.5.3 that may have resolved issues similar to the one you describe.
  You'll need to provide a sample for us to answer this. Most likely the audio in the mp4 is a format not supported by Android.
  Thanks. Happy to merge this, but it needs to go into the dev branch rather than master. If you could send a pull request there instead, we'll get it merged.
  Yes, this is a known sub-optimality related to how stream selection works in `ExtractorSampleSource` at the moment. We plan on fixing it.
 I believe this currently happens for all container media types. It does not occur for DASH or SmoothStreaming playbacks.
 We're currently working on a 2.0 version of ExoPlayer, in which this issue will be fixed. We do not have an ETA as of now.
 This is fixed in the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental).
  This behavior is expected. It's a result of the threading model that ExoPlayer uses, and is unavoidable (there are different ways to do it, but each have their own quirks; it's not possible to make cross-thread message passing completely transparent to the user of the API, unless you want your application thread to hang in some cases instead ;)).

I'm not sure what you mean when you ask how you would determine whether a seek is required. In nearly all applications I would expect a seek to be triggered by some kind of event (e.g. the user dragging the seek bar, or a button being pressed, or something). In which case a seek would never be required unless an event has occurred. Note that you can seek when paused (or at any time), so it's not like you have to wait until you're resuming to perform a seek, either. Please clarify exactly what the use case is?
 I don't understand why you would ever want to seek between (1) and (2). If the user pauses and then presses play, they expect playback to resume from where the playback stopped. Which is accomplished by not calling seek between the two.
 Ah. I see what you mean, but I think the solution is simpler than you think it is :).
- For the case where you're using the same player instance and where you want to pause/resume, you don't need to do anything.
- For the case where the player is being released and null'd and a new instance is being created later, you do need to restore the position.

The implement the second of these, you can just call getCurrentPosition to read the position immediately before releasing the player. Then when you instantiate the new player, immediately call seekTo to restore the position. If you look at the demo app, this is what it does. There's no need to do anything more complicated than that.

As a final point, note that getCurrentPosition is expected to return the correct position even if the player encounters an error, or whatever, so you don't need to worry that calling getCurrentPosition immediately prior to release might give you a bogus value. You can rely on the position returned being correct.
 - `PlayerControl.seekTo` is intended to be called internally. You shouldn't call it directly. Always perform a seek directly on the player from your own code.
- As per the `PlayerControl` Javadoc, it's expected that most applications will not make use of that class (i.e. I'd expect most applications to be implementing their own playback controls).

Closing this, since it sounds like your issues have been resolved. Thanks!
  The file in the chromium source does not have Cues (seek index) and thus it is un-seekable (it is possible that the opus demo app isn't handling un-seekable files correctly - but i'll leave that out).

FFmpeg does not write out Cues on audio only encodes. There are two workarounds here:
1) Use the -dash 1 parameter in FFmpeg. This will ensure that Cues are written out (even for audio only encodes), but has some other implications (like when a Cluster is created, etc.)
2) Encode the file using FFmpeg and then use sample_muxer from libwebm [1] with -cues_on_audio_track 1 to force Cues to be written on audio tracks.

Once you have Cues in your file, seeking will work as expected. Also note that this behavior has nothing to do with LibopusAudioTrackRenderer. This behavior is because of the way the Extractor handles Seeking. Please make sure you are using ExtractorSampleSource and not FrameworkSampleSource (because the latter only looks at Cues on video track [2]).

[1] https://chromium.googlesource.com/webm/libwebm
[2] http://androidxref.com/6.0.0_r1/xref/frameworks/av/media/libstagefright/matroska/MatroskaExtractor.cpp#455
 I can't think of anything off the top of my head. Is there a significant difference? If possible, please share the files and i can take a look.
  Does the issue reproduce in the ExoPlayer demo app? Please try that first. If it does, please send a link to `dev.exoplayer@gmail.com`. Thanks!
 This should be fixed on the dev branch. Thanks for the report!
  @jeoliva can comment on this.
 This issue has had no activity for a while, and FLV features aren't something we're going to be spending time on ourselves. Closing this issue.
  It's actually quite complicated to add such a function. As @sbaar noted you can use `TextureView` and then use its `getBitmap()` method. From Android N there's also a new [`PixelCopy`](https://developer.android.com/reference/android/view/PixelCopy.html) API for copying the contents of a `Surface` into a `Bitmap`, which should work with `SurfaceView` too. You can use TextureView if you force Widevine into L3 mode. This will only work if the content license doesn't require a secure output path, since L3 cannot provide one. It *might* also be possible to use TextureView with L1 if the license doesn't mandate a secure output path, but I'm not sure about that. You'd have to give it a try.

It's working as intended that video displayed using the secure output path is not captured in screenshots. See [here](https://developer.android.com/reference/android/view/Display.html#FLAG_SECURE) for some related documentation.  The backgrounding functionality in the demo app was only ever really a best effort; not a complete solution. We'll be removing it in the 2.x the release. Implementing proper background playback support requires, among other things, moving the player into an Android service. This is work that an application developer will need to undertake should they wish to obtain this functionality.
  Isn't it a design flaw that your server decides the token is dead in this case? Why does the token have such a short lifespan? Fill/drain (i.e. filling a buffer, then not requesting any data for a while whilst the buffer is draining, then filling again) is a legitimate buffering strategy. Tokens should remain valid for a long enough period of time to accommodate such a strategy.
 To answer the question, the total buffer size (in bytes) is BUFFER_SEGMENT_SIZE multiplied by BUFFER_SEGMENTS. You're basically just making the buffer smaller by decreasing either of those values, which means the drain part of the fill/drain buffering cycle will be smaller; probably small enough such that the token doesn't become invalid. However the client will also have a smaller buffer if you do this, which makes re-buffering more likely to occur.
  If you look in the [source code](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/chunk/ChunkSampleSource.java) you'll see that this parameter is only used when invoking methods on the eventListener (it's basically just passed as the ID to the eventListener methods). We should improve the documentation for the ChunkSampleSource constructors.
  - Few Android devices support H262 video, which this sample contains, so you're pretty lucky that you're even seeing video on whatever test device you're using. It's not required for Android devices to support this format, and many do not.
- The audio is audio/mpeg-L2, for which I believe the same applies.

Supported underlying formats can be found [here](http://developer.android.com/guide/appendix/media-formats.html). For maximum compatibility, you should really be using AAC audio and H264 video.
  Let's track this using https://github.com/google/ExoPlayer/issues/876. Thanks.
  Please file a new issue including _all_ of the information requested in the issue template if this is still a problem in v1.5.10 or later. Thanks!
  As far as I know Chromecast receiver applications are written in Javascript? In which case the answer is no, since ExoPlayer is written in Java for Android.
 I don't think that's the case.
  It appears this issue is already fixed in the dev branch, so the fix will make it into the next release.
  Does this reproduce in the demo app? Could you specify which stream this occurs with, and how to reproduce (e.g. is going into airplane mode during playback sufficient)? Thanks!
 I tried this in the demo app with a Shoutcast stream and the state transitioned to idle fine for me. This was using the current master branch. Note that transition takes about 5 seconds to occur, whilst a few retries are made to see if the playback can recover.

Please provide more specific reproduction steps (i.e. exactly what stream you're testing with, what version of ExoPlayer + its demo app you're able to reproduce the issue with etc).
 - It's working as intended that onStateChanged isn't called when the player is released. Once you release the player, you're done and you should not expect any further state change notifications.
- The error you're seeing is a warning only. In this case it's in no way harmful, and can be safely ignored.

So I think everything is fine here?
 We do not support that, no. Sorry!
  The SegmentBase elements in the manifest are incorrect. The ranges they specify don't match the media. For example the first video representation has in the manifest:

```
<SegmentBase indexRangeExact="true" indexRange="1007-1470">
    <Initialization range="0-1006"/>
</SegmentBase>
```

But from manual inspection, the correct ranges look like they should be:

```
<SegmentBase indexRangeExact="true" indexRange="1016-1471">
    <Initialization range="0-1015"/>
</SegmentBase>
```
  We tested on a WIKO CINC FIVE V8 device and experienced similar issues. Unfortunately, we do not have a workaround. Please report the issue to the device manufacturer. Thanks!
 YouTube most likely fall back to Android's MediaPlayer API on the affected devices.
  If you're trying to implement offline drm playbacks, there's quite a lot more work to do than just changing the key type. You need to implement key release, add functionality to restore offline keys into MediaDrm sessions, and so on.

We're not supporting this case directly at this point in time, however if you need this functionality you can implement your own version of `DrmSessionManager` in your application code in order to achieve what you need.
  This is fixed in the dev branch, by https://github.com/google/ExoPlayer/commit/b64986ce8275dc3c723bb15bf9f00a808ab19b16.
  Please provide additional information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). How long is 'a long time'?
 Closing due to lack of information.
  Whoops, commented from the wrong github account.
 I tried to run the script as mentioned in the instructions and it did not fail for me. Can you please attach your config.log ? It's likely that something is wrong in your build environment.
 The generate_android_configs.sh script does not support armeabi-v7a-hard target (we only support the ones listed in README.md). So it is expected to fail. I don't know much about the armeabi-v7a-hard target and if libvpx itself supports it. (Please feel free to send a pull request if you think you can fix the script for that target :) ).

APP_ABI command line switch works. And for completeness, README also lists the alternative of declaring APP_ABI in Application.mk (Please see https://github.com/google/ExoPlayer/tree/master/extensions/vp9#building-for-various-architectures)
  We're only supporting frame accurate seeking at this point, but will consider this for the future.
  thanks for pointing this out. i will update it.
  It looks like a server side issue. I'd guess it's quite likely to have been resolved by now. Please could you check again and update the issue? Thanks!
  We prefer exposing each individual stream in the demo app. There's nothing stopping you from implementing different behavior in your own app, if you prefer to do it a different way.
  For each stream, evaluation occurs before each segment load. Note that this is not the same as saying that evaluations will occur evenly once every segment duration, because each segment fetch may take more or less time, and segment fetches may be batched together. Evaluation may also occur every 2 seconds, in order to re-evaluate in case buffered media should be discarded in order to re-buffer at a higher quality. Note that most DASH playbacks have two streams (one for video, and a separate one for audio).
 - Yes, that would be re-evaluation logic.
- `DefaultLoadControl` co-ordinates how/when loads are allowed to proceed.
  I don't think it's a regression; I think it was always that way. It's most likely avoidable if you ensure there's always a video keyframe near the start of each video segment. This is a property that's (in practice) guaranteed in DASH and SmoothStreaming; both of which will give you better streaming performance than when using HLS.
  As per above, you can modify the constants that you located in order to change the buffer size. Increasing the number of segments is normally better than increasing the segment sizes. The other renderer builders have similar constants.

In the 2.x release it will become easier to change the buffer size, and the approach will be the same regardless of playback type.
  There are quite strict rules about calls to `getOutputBuffer`. Specifically, in the `MediaCodec` Javadoc, it says: 

_After calling this method, any ByteBuffer or Image object previously returned for the same output index MUST no longer be used._.

It's likely that your call to `getOutputBuffer` is causing an output buffer that ExoPlayer code already has a reference to to become invalid.

If you're modifying `processOutputBuffer` then you should read from the `buffer` argument that gets passed to it, rather than making any calls to `getOutputBuffer` yourself. Note also that you shouldn't rely on the position and limit of the buffer being set correctly. Use the `bufferInfo` argument to find out the offset and size of the data in the buffer. You can set the buffer position and limit using this information yourself, like:

```
buffer.position(bufferInfo.offset);
buffer.limit(bufferInfo.offset + bufferInfo.size);
```
 MediaCodecVideoTrackRenderer already has `currentWidth` and `currentHeight` variables.
 No worries; Glad you got it working!
  Well, what would your interpretation of the log output you've pasted above be? You should be able to debug this yourself ;)...
 See: http://developer.android.com/reference/java/net/SocketTimeoutException.html

"This exception is thrown when a timeout expired on a socket read or accept operation."
  Do you have some sample media you can provide that's broken without this and fixed with it? Thanks!
 Merging. Although we'd still really appreciate some test content if you're able to provide it. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  If you try the ExoPlayer [demo app](https://github.com/google/ExoPlayer/tree/release-v2/demo) you'll see there are "Video" and "Audio" buttons that you can press. Pressing them gives you a dialog listing the available streams and allowing the user to select which one(s) are used. So I'd suggest taking a look at the demo app code to see how to achieve this.  What device/os-version is this happening on?
 This doesn't sound like an ExoPlayer issue, since we don't do anything special for headphone vs speaker output. It's up to the platform to send the audio to the correct output. We've also not received a single other report of this issue. Closing for now.
  Please try taking a look at these issues. It's almost certain one of them is what you need (probably the one about ICY Protocol):

https://github.com/google/ExoPlayer/issues?utf8=%E2%9C%93&q=Shoutcast
  Isn't it a media error to define an audio stream if the stream doesn't contain any audio? The stream should just not be defined, in my opinion.
  Feel free to send a pull request if you have a good fix for this. Thanks!
 @Avetri - What specification (including version + part) are you looking in to see that PIDs from 0x00 to 0x1F are reserved? I can't see this stated anywhere in ISO 13818-1:2000. Are you planning to send a pull request to fix the issue?
 The change above avoid treating network_PID as thought it's a program_map_PID. I'm not sure about the first point in this issue, however.
 Closing due to lack of update regarding the first point. The second point has been fixed.
  We should blacklist this like we blacklisted CIPAACDecoder.
  I don't think content:// URIs are the right thing to be using. What format is the obb file as a whole? Assuming you're needing (or will need) to deliver more than one asset, the obb file will have to be something other than just the video file, I think. Is it a zip file, or something else?

Did you read through [this](http://developer.android.com/google/play/expansion-files.html), which seems to contain quite a lot of relevant information? It looks like an obb is just a regular file, which would have a file:// URI. So the question is more about the format of the obb file as a whole, and how you'd read just the video that you're interested in from it.
 I don't understand what you mean when you say the video is in an uncompressed obb. An obb isn't a file format or a container for other files. Either the obb file _is_ the video file, or the obb file is something like a zip file that acts as a container for other files. If the obb file is the video file, then I would expect a regular file URI to work. The URI would just be that of the obb file directly though (I wouldn't expect to see `/videoname.mp4` on the end).
 Oh right. The documentation is a little confusing. You're allowed to use any file type at all for apk expansion files. The files are always renamed to end in `obb`. Separately, there's an `obb` file format, and you can create such files using the `jobb` tool.

So I think what you're saying is that you've created a file using the `jobb` tool that contains a single media file. Is that correct? Presumably you'd have to mount your file using the [StorageManager](http://developer.android.com/reference/android/os/storage/StorageManager.html) API in order to read the files from within it? Are you doing that?
 I was able to play a file within an obb file using my suggestion above (use StorageManager). I did the following:
1. Created an obb file using jobb.
2. Pushed the obb file onto the device.
3. Mounted it in application code using `StorageManager.mountObb`.
4. When the callback indicates that mounting was successfull, get the mounted path using `StorageManager.getMountedObbPath`, which looks similar to `/mnt/obb/d568994e6ad66a21e8531fa8375da0de`.
5. Construct a file URI that looks like `file://<mounted-obb-path>/path/to/media/in/obb/media.ext`, and use it.
 A permission denied error doesn't sound related to how you create the obb file. Are you running Marshmallow on your test device by any chance?
 Taking a step back, I don't really understand what you're trying to do. Why are you putting a single media file inside an obb in the first place, rather than just using the media file directly?
 According to [this](http://developer.android.com/google/play/expansion-files.html) page about expansion files, you can use whatever file format you want:

_"Each expansion file you upload can be any format you choose (ZIP, PDF, MP4, etc.). You can also use the JOBB tool to encapsulate and encrypt a set of resource files and subsequent patches for that set."_
  I tried taking the current version of the demo app from both `master` and `dev`, and adding the sample you linked to as:

```
new Sample("TEST",
    "http://download.blender.org/peach/bigbuckbunny_movies/big_buck_bunny_1080p_h264.mov",
    PlayerActivity.TYPE_OTHER),
```

The duration is reported correctly _and_ seeking works correctly too.

If your live streams are just MPEG-TS then there is no support for passing the duration or seeking within the stream.
  Closing because this is neither a bug or feature request (the feature request is already tracked by https://github.com/google/ExoPlayer/issues/420).
  It's unclear what you mean by segmented video. It would also be best if you just got a sample of the type of video you're interested in playing, and tried to play it.
 We don't have support for doing this in a seamless way (i.e. without a brief buffering spinner between each segment). We may look at adding support next year.
  This is fixed in the `dev` branch in https://github.com/google/ExoPlayer/commit/b64986ce8275dc3c723bb15bf9f00a808ab19b16.
  That's the VP9/Opus software decoder demo app. Unless you have a specific VP9/Opus software decoder requirement, you should use the standard demo app instead. If you do require the VP9/Opus software decoders, you'll need to set up the native library parts of the VP9 and Opus extensions, as described here:

https://github.com/google/ExoPlayer/tree/master/extensions/opus
https://github.com/google/ExoPlayer/tree/master/extensions/vp9
  This is probably the same as https://github.com/google/ExoPlayer/issues/836, but you'd need to provide a link to a sample manifest to be sure. You can remove/alter the template so as to not publish the URLs if you prefer; although we'd need you to state how many segments you think exist in the media.
 Can you strip out the sensitive bits of the manifest and just paste a safe version here? I'm mainly interested in what value you get if you divide the period duration by the segment duration (as a floating point operation). I'd be interested in knowing how far off it is from 1001.
 Thanks! Yes, looks like the same issue. The problem is that the length of the video according to the manifest is actually `0.036` seconds longer than you state above, so technically the manifest defines `1002` segments where the last segment is expected to have a shorter duration, in this case of `~36ms`.

As in the related issue, technically the manifest is at fault here. If you update the manifest so that it reads `mediaPresentationDuration="PT1H40M6.000S"` then you'll likely find that playback ends correctly. That said, we should consider being more permissive in our handling of this case. We'll track that using the related issue.
 NB - You should fix the manifests to be technically correct regardless.
 The number of media segments is the duration divided by the segment length, rounded up to the nearest integer, which is:

`ceil(6006.036/6) = ceil(1001.006) = 1002`

Additionally, you may or may not include an initialization segment. But that's irrelevant to the above calculation.
 It may well be required, depending on exactly what's included in the media chunks. It's normally best to include it. But it's inclusion doesn't have any effect on the number of media segments that the manifest defines to be present.
 You should probably update the mediaPresentationDuration to be `mediaPresentationDuration="PT1H40M6.000S"`.
  Tracked by https://github.com/google/ExoPlayer/issues/73
 WebVTT subtitle support isn't implemented yet, so it's not expected to work. #73 tracks implementing this feature.
 Sorry, I got confused above. To clarify, we don't support multi-audio, demuxed-audio streams or multi-subtitles for HLS at this point in time. #73 tracks implementing multi-audio and demuxed-audio stream support. #151 tracks implementing WebVTT multi-subtitle support.

Until #73 is implemented, demuxed-audio streams such as the ones in your sample playlists will not play correctly. Note that in general, we recommend DASH or SmoothStreaming over HLS, and already support de-muxed audio streams for those standards.
  We do have some internal streams testing HLS+AES, but they're still working correctly on new versions of ExoPlayer. You'd need to provide more information to allow us to investigate this issue, like a sample stream, what version of ExoPlayer you're using, what device you're using it on, and what OS version the device is running. 
  We don't aim to retain API compatibility between versions of ExoPlayer. The API is so large due to its extensible/flexible nature, that trying to retain compatibility would significantly hinder our efforts to make the library better.

As for why AS decided to show you errors for every import, that sounds like an AS issue rather than anything related to ExoPlayer :).
  This is either a decoder issue specific to RK3128 or an issue with the content. Probably the first one.
 This doesn't reproduce on other devices (at least not the ones I tested on). You'll need to report the issue to the device manufacturer.
 What is the H265 decoder being used on the device? Unless it's a google provided software decoder then the device manufacturer needs to fix the issue, and having access to a device wont help. You'd have to report the issue to the device manufacturer directly.
  The Elementary PID of each the individual streams (e.g. audio/video) is changing at the ad insertion points, which we don't handle currently. It's possible to hack around inside ExoPlayer to handle this case (see below), but I'm not sure it's something we want to do, and it may well cause issues with other streams. The best solution would be to have your server ensure that the Elementary PIDs do not change at ad insertion points.

---

If you do want to hack around in ExoPlayer, I think the following changes in TsExtractor fix things to work with your stream:
- Change `streamTypes` to be a `SparseArray<TsPayloadReader>`
- Change [this block](https://github.com/google/ExoPlayer/blob/r1.5.2/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L290) to perform a mapping from the new Elementary PID to the existing payload reader, like:

```
if (streamTypes.get(streamType) != null) {
  if (tsPayloadReaders.get(elementaryPid) == null) {
    Log.e(TAG, "Remapping\t" + streamType);
    tsPayloadReaders.put(elementaryPid, streamTypes.get(streamType));
  }
  continue;
}
```
- Change [this block](https://github.com/google/ExoPlayer/blob/r1.5.2/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L323) to be:

```
if (pesPayloadReader != null) {
  PesReader payloadReader = new PesReader(pesPayloadReader);
  streamTypes.put(streamType, payloadReader);
  tsPayloadReaders.put(elementaryPid, payloadReader);
}
```
  Yes.
  I believe this is expected behavior. You're free to set idrKeyframesOnly=true in your app, so this isn't an issue as such. It's more a question of whether the default should be true rather than false. I believe it's correct that it's set to false, since we expect most streams to contain IDR frames and I seem to remember we saw some temporary corruption on the visual output when setting it to false in some cases.
 As per above: I seem to remember we saw some temporary corruption on the visual output when setting it to false in some cases.

I think this is because the non-IDR frames that can contain this information can also have decoding dependencies on prior frames, depending on the way in which the content is encoded. If they do, they are not valid points from which you can start feeding the decoder and visual corruption will occur (or worse, the decoder will native crash and take down the mediaserver process).

It might be possible to do something more advanced to figure out whether a frame has decoding dependencies in prior frames, so that we can use non-IDR frames that do not have such a dependency. I'll leave this issue open to figure that out, but it's pretty low priority for us.
 We have no current plans to do "something more advanced" as described above, so closing this.
  Whatever the media is that you're providing, we're not recognizing it as a valid WebM file. You'd have to provide sample media for us to debug this further.
 You appear to be trying to play an MP4 file using an ExoPlayer instance where you've explicitly set it up to only have a WebmExtractor. That's not going to work. Please try adding your sample directly to the ExoPlayer demo app's `Samples.java`, without making other modifications. I suspect you'll find that playback is successful in that case. If not, please let us know. Thanks!
  We could probably support this relatively easily, given we already have ID3 parsing for MPEG-TS.
 @cbfiddle - For mp4 do you think it's fine to propagate the metadata in the audio track only? I've made this change on `dev-v2-id3`, but wonder whether you can think of any edge cases? We could add it to the video track for the specific case where no audio track is present to handle that edge case, but in practice it seems unlikely we'd encounter this case.
 ID3 support for MP3 and MP4 has been merged in https://github.com/google/ExoPlayer/pull/2008.
 I don't think it would be possible to implement this properly by receiving the ID3 data and then setting something back on the player. You'd really need this to happen synchronously within the player itself. If there are standard tags for this then please file a new issue to track implementing this, together with details about the standard tags and sample media we can test with.  ExoPlayer currently discards media that's been played (i.e. is behind the current position) in order to use that memory to buffer further ahead. We have longer term aims around not doing this specifically in the case that we've buffered to the end of the stream already (in which case there's no requirement to reclaim the memory), but this wont happen for a while.
  You're defining the exact position as the timestamp of the video frame being displayed, but in general that's not the case or what someone using the player would want. As an example, consider a case where there's video at a very low frame-rate (e.g. just a single frame at the start of the media) and also an audio track. It's unlikely that a user of the player would want the position to never update in that case.

If you need the exact frame position, you'll have to extend `MediaCodecVideoTrackRenderer` and override `processOutputBuffer`, like:

```
  @Override
  protected boolean processOutputBuffer(long positionUs, long elapsedRealtimeUs,
      ExoMediaCodec codec, ByteBuffer buffer, MediaCodec.BufferInfo bufferInfo, int bufferIndex,
      boolean shouldSkip) {
    boolean processed = super.processOutputBuffer(positionUs, elapsedRealtimeUs, codec, buffer,
        bufferInfo, bufferIndex, shouldSkip);
    if (!shouldSkip && processed) {
      lastOutputBufferTimestamp = bufferInfo.presentationTimeUs;
    }
    return processed;
  }
```

Where `lastOutputBufferTimestamp` is the timestamp of the frame currently being displayed.
 Are you sure `lastOutputBufferTimestamp` isn't updated after you query it? I'm not sure what you mean when you say they're not matching. It's unclear exactly what you're printing, but if I understood correctly then both values should originate from `bufferInfo.presentationTimeUs`.
  I don't think so, no. The adjustment needs to be able to move both backward and forward.
 Could you explain why you believe it should be different? Thanks.
 Note that the HLS sample you provided under a related issue is a case of bad media, rather than a problem with the player.
 The change you suggest is not correct, and whilst it might happen to work for the sample you provide, it will prevent correctly generated media from playing properly in some cases. The right solution here is to fix the source media.

We're not really aiming to play arbitrary bad media. I would also debate whether a player that can handle such media is better, since it encourages people to create bad media in the first place, which is not a good thing.
  There should be no difference between the two cases, and the sample works fine in both cases for me on both the dev and master branches. What kind of URI are you specifying in the disk case? Is it a file:/// uri, or something else? Where abouts on the local file system are you putting the file?

There is a known issue with the demo app where local playback will fail on devices running Marshmallow, which is because the demo app doesn't implement Android's new permissions model yet. Although in this case you'd see a very different error that clearly relates to permissions, like:

```
com.google.android.exoplayer.upstream.FileDataSource$FileDataSourceException: java.io.FileNotFoundException: /sdcard/01.m4a: open failed: EACCES (Permission denied)
```
 The selection of the extractor only occurs at the start of playback, so I don't think it's possible that you're seeing that error part way through a playback. You shouldn't need to do anything related to video.
 So if I understand correctly, you're saying that the demo app works fine, but when you copy the same code into your own app that's no longer the case? I don't really know what could cause that. It sounds pretty odd to me. But as above, I don't think it's possible to see the error you're saying occurs part way through a playback.
 I really can't figure out how all of the statements in this issue can be accurate. It's unlikely that the player has anything to do with your app freezing, since it does nearly all of its work on a background thread rather than the main application thread. The error you say you're seeing wouldn't result in that kind of behavior, either.

Focusing on reproducing in the demo app specifically, the only thing I can think of is that the file at `/extSdCard/Beyonce_m4a/01.m4a` isn't actually identical to the one provided at the link.
 It works fine for me. I can't reproduce the issue you're seeing. Note that the code executed is identical in both cases, except that FileDataSource will be used instead of DefaultHttpDataSource.
  There is no way to advance forward and backward by a single frame, no. Sorry!
  What version of Fire OS is this device running?
 By the way, it's very unclear to me whether the log you paste is the actual error (the specific line you refer to is normally harmless). Please always attach full bug reports rather than excerpts from logcat, as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
 There are issues with FireOS4's underlying media support that require an unreasonably large amount of workaround code to handle. So, unfortunately, you shouldn't expect ExoPlayer to work correctly on FireOS4. FireOS5 resolved most of the underlying issues, and I would expect playback to be successful there.

I suggest you request to Amazon that they consider updating these devices to FireOS5, or that they backport their compatibility fixes to FireOS4.
 I don't know the answer to that question, but if you mean literally hardware when you say the underlying chipset, I think that's highly unlikely.
  Wait, I'm confused. These events are being reported in the demo app for HLS playbacks. Are you sure you're not using an old version of ExoPlayer?
  The stream declares an audio track but does not contain any audio. If the stream doesn't contain audio, it shouldn't declare a track. This is the same as #905 (see 5th post in that issue for the bit of code you could manually comment out to allow playback to proceed - although doing this will break audio on any correctly functioning streams).

The correct fix is for whoever is providing this stream to not include an audio track.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 If you want multi-track support in HLS you should be using renditions rather than bundling multiple audio tracks in a single TS stream (I don't think the latter approach was ever an official part of the HLS spec). See:

https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.4.2.1

Whilst we don't yet support renditions either, we do intend to do so in the future, and attempting to support both approaches simultaneously is likely going to get messy. So unless you can convince me otherwise, I don't think this is something we really want to be merging.
 CLAs look good, thanks!

<!-- ok -->
 We support multi-audio using renditions, so closing this.
  Widevine license requests include a content id (which is independent to the stream from which the data used to generate the request was obtained). The license server is expected to return all keys to which the user is entitled for the content id.

So regardless of which stream is used to make the initial license request, the license response should contain both HD and SD keys (assuming the user is is entitled to play both). Hence subsequent requests are unnecessary to allow adaptation between HD and SD. This is obviously a more efficient way of doing things than requiring additional license requests when quality switches occur.

It sounds a bit like an issue with the license server?
 Glad you fixed the issue; and thanks for sharing!
  You're probably not fully rebuilding the renderers when you switch. You shouldn't re-use any objects that you used to build one set of renderers when building the next set, particularly things like DataSource instances. Is it possible that you are re-using them? If so, try not doing so.
 If that's the case then the two playbacks should be completely independent to one another. It's pretty much equivalent to backing out of playback in the demo activity and selecting another sample. In which case I don't understand how one playback could affect the next (at a theoretical level)?

Can you modify the demo app in some way to reproduce the issue? And if so, can you upload it to GitHub so that we can reproduce ourselves?
 The issue is related to the way that you've made `HlsChunkSource.encryptionKeyUri`static in your branch. The breaks the assumption that the second playback is independent of the first, because state is being left lying around from the first playback in the static variable. If I null it out where the player is released, then the second playback works fine.

So the conclude - It looks like an issue with your local modifications. As a general style point, it's worth exploring solutions that don't require statics except in the specific case of singletons (which definitely isn't the case for this particular variable). I also noticed some interesting code in User.java where initSingletons re-initializes singletons potentially multiple times, which looks wrong to me.
  Unlike containers such as MP4, MPEG-TS doesn't contain an index structure to allow seeking (i.e. a structure that allows mapping from time->byteOffset). Hence seeking in an MPEG-TS stream requires the player to either download the whole stream, scan it and build an index itself, or to guess based on average bitrate assumptions. We've decided to only support seeking in container formats that are properly designed to support it, which MPEG-TS is not.

As an aside, note that seeking is possible in HLS, even though it uses MPEG-TS, because in HLS a separate media playlist is provided to serve as an index structure.

It seems strange to be using UDP for seekable streams. What's your use case exactly? Why don't you just use MP4 over TCP? It's not valid, in general and for any container format, to replace dropped frames with other frames taken from the stream.
  I'm slightly unclear why you think the existing code doesn't match the spec. Looking just at the first case in the existing code:

```
pixelWidthHeightRatio = (3 * width) / (float) (4 * height);
```

This (ignoring floating point considerations) is the same as `(3/4)*(width/height)`, where `3/4` is the correct `DAR` value, and `width/height` is `horizontal_size/vertical_size`.

Did I misunderstand something? Also, do you have sample content with non-square samples that you could share? I have a sample that has `aspectRatioCode = 3`, but the pixels are still square, meaning that both code blocks produce an identical result (i.e. It's not a useful sample :)).
 Oh. I think you're suggesting it's wrong because `SAR` is actually defined as `vertical/horizontal`, where-as `pixelWidthHeightRatio` is a `horizontal/vertical` value?
 We'll push a fix for this. Thanks.
  Sorry, I don't think we have a plan to ever support this. We already support Widevine and PlayReady DRM protection for SmoothStreaming (although the latter is limited to AndroidTV). I don't think there's sufficient demand to make this feature worthwhile, since it's not in line with how the majority of the industry is delivering DRM protected content. The industry does not appear to be moving in this direction either.
 To answer your question - You are of course welcome to dig around in the source code and figure it out, but we can't really provide assistance with this. Sorry.
  You'd have to ask your point of contact at Widevine.
  No. Why would microsecond level granularity be useful? Millisecond level granularity seems more than sufficient to me?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  What code is responsible for generating the media file when it's recorded on iOS? Is that your own code, or is iOS generating the media file for you? Upon first inspection, the media appears to have some invalid data in its Mp4AudioSampleEntry box, which is preventing information about the audio track from being parsed.
 This happens because ExoPlayer's Mp4Extractor claims compatibility with QuickTime files, but does not parse QuickTime `SoundDescription` entries correctly. Useful links:

http://lists.apple.com/archives/QuickTime-API/2007/Oct/msg00061.html
https://developer.apple.com/library/mac/documentation/QuickTime/QTFF/QTFFChap3/qtff3.html
https://searchcode.com/codesearch/view/70087452/#l-69

I think we need to:
- Have Mp4Extractor read the brand out of the ftyp atom, to determine whether the file is QT rather than MP4.
- Have AtomParsers parse `SoundDescription` entries rather than `AudioSample` entries if so (nb - the two are very similar, although structurally slightly different).
 We'll push a fix for this relatively soon. Note that we don't officially claim to support QuickTime files, so whilst we'll fix things to correctly play this sample, we're supporting QuickTime on a "best effort" case only. For best compatibility, you should use MP4 instead.
 This is fixed on the dev branch.
  Thanks!
  CacheDataSource currently only works for bounded range requests. Unless you're implementing VOD DASH this is probably not the type of request that the player is issuing, in which case CacheDataSource may not do any caching at all. There is an issue tracking making CacheDataSource more generally useful (#420).

In general, it's not feasible to make hard guarantees about when cache hits/misses occur. For example, the device might have no free space into which media can be cached. So you should always assume that a request might need to go to the network.
  Both links work fine in the demo app for me. Please file a new issue providing more information, as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Note that if you're running on an emulator, it's likely an emulator specific issue. Please try on a real device / real devices.
  The ExoPlayer demo app is structured similarly (activity containing a list + activity for playback). Does the same issue reproduce in the demo app? If not, it sounds like an issue with your application code.

One thing to note is that `onDestroy` is too late to be releasing the player. You should really be instantiating the player in `onResume` and releasing it in `onPause`, as in the demo app.
 In case it's useful for someone debugging this, the output of `adb shell dumpsys media.audio_flinger` includes some useful information on the audio tracks that are active (owning process identifier, sample rate, buffer size, underrun frame count, ...). Each initialized AudioTrack uses up resources, and leaking unreleased AudioTrack instances seems to be a common cause of this error.
  Are you sure the player is switching to the high bitrate stream (without your modification to have it start start with that stream)? If so, it feels like a device specific issue to me?
 You should report this issue to AmLogic. I don't think it's a problem with ExoPlayer.

As a side note - There's insufficient information in this bug to tell exactly which device, Android build and ExoPlayer build this issue reproduces with. Please provide complete information in any future bug reports, as requested in the issue template.
  Feel free to send a pull request, if you'd like this change! Thanks.
  - I can easily reproduce the problem on the Samsung Galaxy S3, as per the original report. It's also trivial to cause a failure at any point during playback in the demo app on this device, by holding and dragging the seek bar. Occasionally this causes a native crash rather than the error reported in this issue.
- I cannot reproduce this on Nexus 4 running 5.1.1 (LMY48T). Please provide a full bug report (the output of `adb bugreport`) if you want us to try further.

For the S3, it looks like there's an issue with the flush implementation of the video decoder. It's quite easy to work around, but I'm not entirely sure exactly what we should be targeting the workaround to. As you can see [here](https://en.wikipedia.org/wiki/Samsung_Galaxy_S_III#Model_variants), there were quite a lot of variants of the S3, and it's likely that the issue doesn't occur on all of them.
 This should be fixed for the S3. I cannot reproduce on Nexus 4. We're not officially supporting GenyMotion emulator; it's likely an issue with their implementation.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Whose manifests are these? Can you provide some examples?
 I don't agree with the argument that we should handle things because other players do. If every player were to do that then you end up with all players relaxing specifications in the same ill defined ways, which then encourages the generation of bad media, undermines the existence of specifications in the first place, and makes in much harder for someone to write a new player because any new implementations will be expected to handle the same unspecified behaviors.

We could handle it if it's a widespread problem (e.g. if some popular HLS serving component is known to behave this way). Otherwise I think it's better to request the provider to fix their content.
 Ok, I think we'll close for now.
  Hi,
  I am trying to extends the player functionality for Offline Videos.The Query is should i have to write a separate ExtractorRendererBuilder providing the OfflineDRMSessionManager  object or i have to use DashRendererbuilder.

Since we are using ExtractorRendererBuilder for playing offline videos, I guess i have to rewrite the same class.

Also do i need to download .mpd file along with .mp4 or only .mp4 will work..??
 `@DrmSession.State` is an annotation with source retention in the library. If you are seeing the error in a file containing your own DrmSession implementation, you might be able to fix it by removing the annotation in your code. If that doesn't work, could you clarify where exactly you see the error?
 @danrossi Does it build if you just remove line with the error? The annotation is removed during compilation (its purpose is to allow [some compile-time checks](https://developer.android.com/studio/write/annotations.html#enum-annotations)) so it won't be present in the class files.
 Hi all,

Did someone try to import `OfflineDRMSessionManager` from ExoPlayer 1.x to ExoPlayer 2 (change the imports)? I have a problem with @Override methods (open(), close(), ..) which are NOT in that new DrmSessionManager and there are some new methods: acquireSession(), ... @Pepa112 we're working on a new OfflineDrmSessionManager (which will probably end up as a DrmSessionManager which supports offline and streaming, and a different class name) @danrossi unfortunately there is no development for content downloading
currently. After offline drm support, it will follow.

Once you download the files, you might need to modify the mpd to point
content files relatively. @noamtamim a few weeks. @erenbakac it's going through code review. Should be committed in 1 or 2 weeks. To clarify in case there's any confusion here: As far as I know the thing being delivered in 1 or 2 weeks is the new DrmSessionManager that will support playback using an offline license, together with logic for acquiring/refreshing/releasing offline licenses. You shouldn't expect a comprehensive solution for offline (i.e. also downloading all of the necessary media chunks) to be provided within that time frame. We don't have an ETA for that as far as I know. Please correct me if I'm wrong! This has been implemented in the change 9d5c750fe914cc7c51f8afa49bb113121352510f  @chris-schu - The DRM implementation (and enforcement of the license policy) is part of the platform, not part of ExoPlayer. DrmSessionManager just sits on top of the relevant platform APIs. So your question is really whether the underlying platform support is sufficient.

If you're using Widevine, L1 is _typically_ sufficient for HD and L3 is _typically_ sufficient for SD for most studios. Whether that's true in your case really depends on the specifics of the contracts you have, and so we're not in a position to provide concrete advice for your case.  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Yes, same as #545.
  ExoPlayer V2 has a `seekToDefaultPosition()`, which will seek to the live edge for live streams. `seekTo(0)` is freed up for legitimate seeks to `t=0`. Note that we still don't support seeking in the live window for HLS (we do for DASH and SmoothStreaming), but this should land sometime fairly soon now for V2.
  Won't Fix - Working as intended

The .neon extension is passed on to ndk-build to indicate it to use that file only if the target architecture supports neon instructions. This will ensure that the resulting library will not contain neon instructions if the target architecture doesn't support it.

Here's the page that documents this rule: http://developer.android.com/ndk/guides/cpu-arm-neon.html#uns
 The .neon tweak is just to ensure that it builds correctly without forcing -mfpu.

Opus actually does runtime detection to determine whether or not to use neon code: http://git.xiph.org/?p=opus.git;a=blob;f=celt/arm/armcpu.c;h=5e5d10c344474f7ae6eae6ed9264245671ca38f6;hb=refs/heads/master#l93
  @vigneshvg - Could you comment on this? Thanks!
 yeah this change should be fine. i'll fix it sometime shortly.
  I believe this is a platform bug on some versions of Android (it's fixed in M). It's also essentially harmless, as far as I know.
  HLS samples in the demo app require PlayerActivity.TYPE_HLS to be passed as the 'type' argument to the Sample constructor (not PlayerActivity.TYPE_OTHER). Does it play correctly when you do that?
 At the very least, the logs would look very different to the ones provided above.
 Please send us the URL of the affected stream. There's an address you can use [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
 - The first link works fine on my device. You'd need to provide more details, as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html), for us to debug further.
- The second link contains H.262 video, which isn't supported by most Android devices (or by ExoPlayer currently, although we'll be adding support where the underlying device does provide support in https://github.com/google/ExoPlayer/pull/915).
 Are you referring specifically to the second link? When I try and play the link (using VLC) I just get a massive warning icon and "warning security issue, app users: please restart your app" message, which is delivered as H262 video. My guess is I'm seeing something different because authentication is failing (i.e. the link you provided is not sufficient for me to successfully authenticate, or else there is some kind of check that fails, for example a geo restriction).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  See #366.
  It's unclear exactly what you're doing, but the URL provided isn't a valid HLS playlist (and so wont work if provided directly to ExoPlayer). The three links within the response from that URL do appear to be valid HLS playlists URLs, but I get a 403 (forbidden) http response when I try and request any of them, so I can't debug further.
 Well, you're getting 403 responses. What kind of authentication is your server using that may cause it to respond with a 403? Could it be related to Cookies? If so, issue https://github.com/google/ExoPlayer/issues/625 may well be related.
 I get 403/forbidden errors instantly when I try to play any of the URL's generated by the links provided in VLC.
 I don't think we got to the bottom of this, but it seems this is a content/serving/authentication problem rather than a problem with ExoPlayer itself.
  Extension isn't the best way to achieve this, but we have some related work that will allow injection of a decoder factory into the renderer. This will provide the flexibility you desire.
  1. It's not possible to completely optimize every possible way of loading data whilst still retaining a clear design for how `DataSource`s work. In practice I suspect the difference in performance between what happens and what you suggest should happen is pretty negligible.
2. The existence of objects in MAT only tells you that they haven't been garbage collected yet. A `FileDataSource` is not in any way expensive, unless it's actually in the open state. So it could just be that the garbage collector hasn't run recently.

Are you certain that your application is correctly calling `release()` on ExoPlayer instances when they're no longer required? If you think ExoPlayerImplInternal:Handler instances really aren't able to be garbage collected, then I think you're either not doing this, or your app is holding references to `ExoPlayer` instances that you no longer need. You should look at the "paths to GC roots" in MAT to determine why each instance is not able to be garbage collected. See [here](http://help.eclipse.org/mars/index.jsp?topic=%2Forg.eclipse.mat.ui.help%2Fgettingstarted%2Fbasictutorial.html) for some details.
 Note - This is more likely to be a bug in application code or in the underlying Android platform than one in ExoPlayer, at this point in the investigation.
 1. I don't agree. Closing and re-opening should be fine, and there's no actual evidence to suggest otherwise.
2. Doesn't that path-to-roots indicate that there's still a message in the `ExoPlayerImpl.eventHandler` message queue? You'll need to find out what that message is, and why it isn't being handled. Are you creating your ExoPlayer instances on a thread other than the application's main thread, by any chance?
 Possibly, but the specific issue preventing the objects from being GC'd is the pending message that appears to be in the message queue. So you should find out what that is, how it gets there, and why it isn't being processed.
 It should be possible using a debugger and/or printing log statements.
 - Can you provide the output from `lsof -p <pid>`, where <pid> is the process id of your app when it's reproducing the issue? That should shed some light on exactly what the open file descriptors are.
- Do you also see multiple instances of ExoPlayerImpl being leaked via ExoPlayerImplInternal:Handler, as @dnutcracker was seeing? If so, can you try and find out exactly what's causing this.
 Most likely a dupe of #1066, which gives reproduction steps.
 Was it (observed on M)? The post 5 above this one suggests the issue doesn't reproduce on M. The 4th post of #1066 corrects the top of that issue also (i.e. it wasn't observed on M).
  We make no guarantees about what ranges of data we're going to read, or when we're going to read them. Note that it's partly dependent on both the container format and the layout of the media within it.

The specific case you mention is an inefficiency, and is a quirk that's necessary due to the way track selection works in ExoPlayer. We're aware of this. We will most likely be able to optimize this in the future, but this will be done as a bigger change around track selection.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 a2e2626 - This doesn't look like a good idea. It'll generate a significant amount of garbage because it's creating String objects every time a video frame is rendered. This overhead is incurred even if logging is disabled for the module. We already expose sufficient information for playback analytics via the EventLogger interfaces that many components expose. So I don't think we want to merge this.

245c745 - Happy to take this as an individual pull request, since it looks pretty isolated. Not keen on having an AmazonQuirks class though. Let's just put the special case directly into MediaCodecUtil. We already have other special cases in that class (e.g. blacklisting of known broken decoders).

5d52107, 45d0eef - These look too involved for it to be worth taking them, given we'll have to maintain the code going forward. Amazon can upgrade these devices to resolve the underlying issues.
 Amazon probably have a group that you need to become a member of for you to be linked to the corporate CLA.
 I think your Amazon address is part of that group, but you're using a Lab126 address that isn't. You need to get the Lab126 address added to the group, or use your Amazon address on GitHub. The bot does verify corp CLA signers, so you should be able to reply `I signed it!` once this is resolved.
 Sounds good to me. I'll close this one, in that case! Thanks.
  It was merged into dev in https://github.com/google/ExoPlayer/pull/267. And no, you wont find anything relevant there.
  - SeekMap represents an index (i.e. some form of time->byte mapping) extracted from the media container. For seeking to work in non-chunked media, it's required that a SeekMap be extracted.
- In HLS the container is typically TS or ADTS. These containers do not contain time->byte mapping information. Hence seeking does not work for playback of non-chunked TS or ADTS.
- In HLS media is chunked, meaning it's provided in segments o 5-10 seconds in length. These segments are listed in media playlist m3u8 files. These files provide a way of seeking at a higher level.

So in summary, for HLS it's expected that SeekMap is not used. Information parsed from the media playlist should be used directly for performing seek operations.
 Look at the use of seekPositionUs [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/hls/HlsChunkSource.java#L255).
  You always need to build new renderers. You can optionally re-use the ExoPlayer by passing the new renderers to the existing ExoPlayer instance. This doesn't save much though, and there are some complexities around having to reset the playback position manually, so you may as well just instantiate a new one.
  ExoPlayer does not acquire a WakeLock or WifiLock directly, but MediaCodecAudioTrackRenderer uses an AudioTrack which should keep the device awake while audio is playing (and for a few seconds after audio playback stops). If audio playback was continuous that might explain why the device stayed awake with the screen off.

However, the device can sleep if audio playback is interrupted and it is not being kept awake by a WakeLock. This could happen if the player runs out of data and needs to rebuffer from the network, for example. So streaming apps that play audio when the screen is off should acquire a WakeLock and WifiLock during playback. It is important to release the locks if/when playback is finished so that the device can sleep.
  Please update to a newer revision of ExoPlayer (1.5.0+). If the issue still occurs there, please file a new issue.
  [ExtractorRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java) shows how to set up a sample source for playing e.g. an MP3 file from a URI. FLAC support is work in progress, tracked by #366 (but for now you could try using the deprecated [FrameworkSampleSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/FrameworkSampleSource.java)).
  Let's use #678 to track this, assuming it's the same/similar.
 @TheNetStriker "By the way, with ExoPlayer playback of these videos worked fine." suggests that the video plays okay but you also say "The device only renders the first image of any video". Please could you clarify under what circumstances playback worked?

To investigate issues like this, it would be useful to have the information requested in the issue template, especially the output of `adb bugreport` and a link to the problematic stream. You can also try playing it with the platform MediaPlayer (assuming the stream type is supported): `adb shell am start -a android.intent.action.VIEW -d "<url>" -t "video/*"` and, as a last resort, forcing using another codec by disabling the apparently broken one (see MediaCodecUtil.isCodecUsableDecoder).
  I think this is the same as #905 (i.e. stream declares audio but does not contain any). Are you sure the stream actually contains audio, as opposed to falsely claiming that it does?
  ExoPlayer errors are unrelated to MediaPlayer error codes, so I'm not really sure what this means.
  I assume you mean the Handler that you create to pass as the `eventHandler` to quite a few of the components? You would normally be creating those on the main application thread. It makes pretty zero difference whether you instantiate multiple instances, or only one.
  I don't observe any issues playing that stream. Please try with a more recent version of ExoPlayer (1.5.8+) and file a new issue containing more detailed information if you're still observing the problem.
  You can remove both READ_EXTERNAL_STORAGE and WRITE_EXTERNAL_STORAGE if you're only doing streaming over the network.

When we first added these permissions to the manifest I don't think manifest merging existed, so the permissions were there for informational purposes only. Now manifest merging does exist I guess these permissions do get merged into your application when you pull in the ExoPlayer library.

Is that correct? Do you think it would make more sense to comment these permissions out by default in the library manifest?
 This is fixed in the dev branch.
  There is currently no simple way to achieve this with ExoPlayer: for gapless playback it is necessary to read the relevant metadata from source, trim decoded audio samples from the start/end of the presentation based on the metadata and use the same Android AudioTrack instance across the transition from one source to the next. #497 tracks adding support for doing this.

(Note that the Android framework's MediaPlayer has a method [setNextMediaPlayer](http://developer.android.com/reference/android/media/MediaPlayer.html#setNextMediaPlayer%28android.media.MediaPlayer%29) which might work for your use case, but this is unrelated to ExoPlayer so we can't provide support here.)
 I don't think it makes any difference. In the current design, you can implement looping by seeking back to the start of the stream when playback ends, but this won't be gapless because the player can't prepare what will be played next in advance. Also, we don't currently trim encoder delay/padding samples, which means the output will include silence at the start/end of playback for streams that use this.

For now, I'd probably just use MediaPlayer.setNextMediaPlayer. If you need to use ExoPlayer for some reason and you know that the input media does not use encoder delay and padding, it might be possible to implement gapless looping using a custom concatenating SampleSource specialized to your use case, perhaps using #1070 as a starting point.
 We now extract and apply gapless playback metadata (see #497). Playlist support in #1270 will make this easy to use (including for looping).
 It's still work in progress, but I think video transitions should be seamless.
  Thanks!
  ExoPlayer is designed for API level 16 and above, so yes, it should work on Lollipop devices. See the [developer guide](https://google.github.io/ExoPlayer/guide.html) for more information.
  Please see our [supported formats](https://google.github.io/ExoPlayer/supported-formats.html) page.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 See email (i.e. feel free to ignore all comments from this point onward - they're mainly for informational purposes).
  We already support DASH, SmoothStreaming and HLS, all of which provide similar functionality. We have no current plans to support Adobe HDS.
  I'm not aware of what it would mean for ExoPlayer to support 3D videos. Depending on how the TV supports 3D, it may "just work" already. For example, if the TV expects 3D content where each video frame is actually two frames side-by-side, this would work already. If you have a more specific question let us know.
 What does _just_ mean in this context? The question remains pretty vague. What container format are you envisaging using here, how are the 3D frames packed within the container, and how do you envisage them being pushed to the display through Android's APIs?
 Got it. It's unclear whether AndroidTV supports this at the platform level. I'll try and find out.
 As per above, I would guess that side-by-side and over-and-under should "just work" if the TV supports it. Frame packing appears to be almost identical as a concept, with the caveat that there's some explicitly defined padding to separate the packed frames, and that it's explicitly defined that the resolution isn't halved as a result of the packing (because the total size in the packing direction is 2x individual frame + padding). Again, I would expect this format to "just work" if the TV supports it.

Approaches that require two video decoders and/or specific decoder support would require platform support, which I don't believe exists currently.
  This most likely has the same root cause as #910, if it's the same type of media. In any case, let's consolidate these issues under that single issue.
  Most likely the media you're playing doesn't define its duration or an index to enable seeking within it. It it did, then getDuration() would be expected to work correctly, as would seeking. In any case, you'd need to provide a link to the media or at least tell us what type of media you're trying to play, for us to be sure.
 The content length (size) isn't the same as the duration (time). My answer above still applies (i.e. the media you're playing probably doesn't define its duration or an index to enable seeking within it). You need to either confirm this yourself, provide a test stream that reproduces the issue, or at least indicate what type of media you're playing.
 Closing due to lack of information needed to help. The link has expired.
  I'm unsure as to why the second bullet point would be necessary, if you were to do the first one. I can't think of a reason why we couldn't do the first one; that seems like a fairly sensible thing to do.
 A PR would be the best way to proceed, yes.
 Decided not to do this for the time-being, but will reconsider in the future if it becomes a more widespread issue.
  The error suggests your server isn't sending a valid HTTP response. How to implement a Widevine proxy is beyond the scope of this issue tracker. I suggest you ask Widevine how this should be done, or consult with their documentation.
  Same as #676
  You could do something like this to get an estimate of the duration currently buffered:

```
int estimatedBufferDuration = ExoPlayer.UNKNOWN_TIME;
int estimatedBufferPosition = player.getBufferedPosition();
if (estimatedBufferPosition != Exoplayer.UNKNOWN_TIME) {
  estimatedBufferDuration = estimatedBufferPosition - getCurrentPosition();
}
```

You could then compare this to the duration that needs to be buffered for playback to start, which is the second argument passed to `ExoPlayer.Factory.newInstance` when creating the player, or `ExoPlayer.Factory.DEFAULT_MIN_BUFFER_MS` if you're using the one-arg version of that method. If you're handling a re-buffer, compare to the third argument or `ExoPlayer.Factory.DEFAULT_MIN_REBUFFER_MS`.

You'd probably end up with something like:

```
int percentage = 0;
if (estimatedBufferDuration != ExoPlayer.UNKNOWN_TIME) {
  percentage = (estimatedBufferDuration * 100) / ExoPlayer.Factory.DEFAULT_MIN_BUFFER_MS;
  percentage = Math.max(0, Math.min(100, percentage));
}
```
 This is the first time anyone's asked for this, which suggests it's not a feature that a many people want. Most applications don't show the percentage to the user; they just show a buffering spinner. The percentage isn't going to be particularly accurate anyway.
 @ripleyXLR8 - For playback to be happening playWhenReady must be true and the state must be ExoPlayer.STATE_READY. If this is the case and the player state then transitions to STATE_BUFFERING by itself (i.e. without you having called seekTo or setPlayWhenReady), then that counts as a re-buffer. If you call seekTo or toggle setPlayWhenReady then it's always a buffer rather than a re-buffer.
  - If this is HLS, it has nothing to do with RTMP right? It's just that the module you're using has RTMP in the name. Is that correct, or did I misunderstand?
- The link you've posted 404s for me. If it broken and/or geo-restricted?
 - The Eiffel Tower one seems to work fine for me.
- The Russia one doesn't work because the stream's program map declares an audio track even though the stream doesn't contain any audio data. The stream shouldn't do this (although granted, we should also be doing a better job of being robust against this). You can make it work by manually ignoring the audio track declaration, which can be done by commenting out the TS_STREAM_TYPE_AAC case [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L303). But yeah, the stream shouldn't declare an audio track if it doesn't have any audio.
 If the stream claims to contain audio then it should contain audio. Ultimately this is a problem with the content, and whoever's providing the stream should fix it to not declare an audio track in the program map.

We may make the player more robust against this kind of content error in the future, but it's nowhere near being high up on our list of things to do, and is probably quite complicated to handle in a nice way.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  This can happen if the server does not place video keyframes at or near to the start of each media segment. Ideally streaming services should be doing this, because it makes things like seamless switching between qualities easier and more efficient (this approach is used as standard by adopters of SmoothStreaming and DASH).

We're aware that where a stream does not have this property, audio can start playing shortly before video. I'm not sure we're inclined to fix it any time soon. If you control the stream yourself, you should look at the feasibility of placing keyframes at the start of your media segments.
  We shift audio and video timestamps by the same amount, using the same PtsTimestampAdjuster. Hence any change in the offset between the two versions will be applied equally to both audio and video, and A/V sync will be unchanged.

Unless there's a user visible effect of this issue, I think it should be marked working as intended. Note that the fact DTS is making a difference implies that segments aren't starting with keyframes, which means some of the frames at the start of the first segment will end up being discarded rather than being played anyway.
  I don't understand how this actually saves you any time. As soon as you have a stack trace (for the assertion failure), it should only takes a few seconds to look up the corresponding line of source code and see exactly what condition has failed.

By convention, we're trying to avoid adding strings like this, for the most part.
  This is currently working as intended, in that the player is designed to always discard buffer behind the playback position in order to buffer further into the future.

However, this doesn't make sense in the case that the whole piece of media can be buffered, or where the buffer isn't full yet. We should only discard when we actually need to reclaim memory to buffer further ahead. Marking as an enhancement to track delivering this improvement.
 Some major architectural changes will probably be required to deliver this (it's not a bug as such - more just a limitation with the existing design that we need to remove). It really needs fixing as part of a larger design overhaul, since we have multiple such limitations and they should be considered together as opposed to in isolation.
 Correct. Sorry! We'll get there eventually though.
 This issue is not a priority for us, and there is no ETA. We're working on a bigger re-design, but this isn't considered a priority even for the first stage of that effort. Sorry.
  - By "software renderer", do you mean that you're using the H264 software decoder, or something else? I'm unclear about exactly what your setup is. Also, what media are you playing, exactly (i.e. what's the container, and what's inside of it)? Please provide a link to test media if possible.
- When you say "multiple devices", does this include Nexus devices? If so which one(s)? Can you reproduce the issue with the demo app by forcing the software decoder there? If so, please provide steps. Thanks!
 It's going to be pretty hard for us to work out what's happened here, to be honest. I can't think of any changes that would be related. You may have to do a binary search on the individual commits between the two versions to work out exactly which commit introduced the problem. Which is time consuming, but which would give us more to go on.
 You should file issues with the platform using Android's [issue tracker](https://code.google.com/p/android/issues/list). Note that you'll need to provide full bug reports for any issues you file to be actionable (i.e. full output from `adb bugreport` captured shortly after the issue occurs). Logcat output isn't sufficient for figuring out what went wrong.

If you do file issues with bug reports attached, feel free to link to them from this post and I can route them to the right people.
 Closing due to lack of further information.
  If this is only when using the emulator, I'm afraid you'll have to figure out where to file a bug on the emulator itself, and report the issue there. Please re-open if you can reproduce the issue on a real device. It works fine for me on a Nexus 5X.
  Can you provide test media that reproduces the issue? That aside, I don't fully understand what you mean. Specifically:

_I noticed that the addition of the DTS decode timestamp adjustment in TsExtractor.java was causing the value of PtsTimestampAdjustor.timestampOffsetUs to go from -1466733 in 1.4.2 to -1400000 in 1.5.1. This has the effect of shifting all the timestamps earlier and skipping 2 of them._

I agree this can shift the timestamps from what they were set to previously, but how did you conclude that it caused skipping? The TsExtractor is still generating the exact same number of timestamps as before. Where exactly are the timestamps you quoted taken from?

_I narrowed down the problem to the fact that the timestamps being used in H265Reader are one sample ahead of what was used in 1.4.2 (after removing the code for the DTS adjustment). So instead of getting 0, 166833, 100100, 33367, 66733, 133467 (as in 1.4.2), I get 166833, 100100, 33367, 66733, 133467, 333667._

That would be a problem. Do you happen to know exactly which change broke this? I'll take a look.
 We believe the fix above (courtesy of @andrewlewis) resolves the issue. Please can you verify that this is the case. We may do a small 1.5.2 release to get the fix into the master branch. Thanks!
  There isn't enough information provided to determine why you're encountering the issue, but it's likely to be a bug in your extractor rather than an issue with RollingSampleBuffer. It doesn't look like the check being added should be necessary.
 Well, the exception likely only occurs because your extractor code is broken. You should fix the root cause, not add a check in the library to hide the issue :).
  Note that it's necessary to sign the Google CLA to contribute to this repository, as described [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md). I'll take a look at this this week. From a brief initial scan, I'm not sure I like the extent to which timestamps are being propagated between various components. However it may be possible to clean it up without too much effort.
 One other thing to note with this change is that I don't think it'll handle either TS timestamp rollover or discontinuities properly (and it seems very non-trivial to do so, also). Any idea how that might be done within this model?
 I don't think that's the correct behaviour, unfortunately. Other HLS components use PtsTimestampAdjuster to un-wrap the timestamps. The timestamps parsed from X-TIMESTAMP-MAP are not subject to this un-wrapping logic, and so do wrap. So you'll end up with the WebVTT timestamps wrapping but the timestamps for other media streams not wrapping, I think?

I have a feeling it'll fall out nicer if you parse just X-TIMESTAMP-MAP from the sample somewhere in the hls package, where the timestamp adjustment can be applied.
 Discontinuities are even more problematic. HlsChunkSource necessarily instantiates a new PtsTimestampAdjuster when it sees a discontinuity. Matching discontinuities should be present in the WebVTT playlist too, but it's necessary to make sure the right PtsTimestampAdjuster is applied before and after each discontinuity, which is not currently enforced.
 I think things actually fall out more nicely using the existing `HlsChunkSource` implementation in conjuction with a lightweight implementation of the `Extractor` interface specific to HLS+WebVtt.

The `WebVttExtractor` would scan just as much of each sample as is required to read `X-TIMESTAMP-MAP` and the first cue timestamp. This would allow the correct timestamp to be calculated and pushed through `PtsTimestampAdjuster`, which is in-line with what `TsExtractor` does for the other media streams, and ensures that rollover is handled the same in both cases. Handling discontinuities would require extending `PtsTimestampAdjuster` to be discontinuity aware, rather than replacing it with a new instance at each discontinuity point as is currently the case.

Note that with the above approach `WebvttParser` does not need to parse `X-TIMESTAMP-MAP`. This is actually quite a nice property, because it keeps the HLS specific logic isolated within the hls package.

I intend to give this approach a try over the next couple of days, to see if it's viable.
 We wont really be parsing the VTT twice. The extractor part will only be parsing as far as the first cue header; it wont need to look any further in the data. I have a hacked together version sort of working, and will look more next week.
 This change will not be merged, but we are actively working on a different approach to enabling WebVTT + HLS. Some of the first changes were pushed yesterday (https://github.com/google/ExoPlayer/commit/4bb8bea9528a803a201b809e8bd4abe3f0b460e8, https://github.com/google/ExoPlayer/commit/72f093c4f6d66cea021756e1cbe931b52f55a79e) and more will follow shortly.
 It's a little further out. There's only so much we can do at once ;). No ETA as of this moment.
 It's not yet possible to fully wire everything up. I guess it's a little misleading that the class was part of the release, given it's still work in progress. Sorry about that!
 Closing this because support should be landing next week. Much thanks for the effort though; it was definitely a helpful reference!
  Yes, we don't support non-standard http-like protocols such as ICY. Developers wishing to support this have a number of options, one of which you mention above. See discussions [here](https://github.com/google/ExoPlayer/issues?utf8=%E2%9C%93&q=ICY). Note that ExoPlayer has an official okhttp bridge extension that you can use too, which is [here](https://github.com/google/ExoPlayer/tree/master/extensions/okhttp).
  - For what it's worth, the original link also works in the US (but not in the UK) :).
- Subtitles are S_TEXT/ASS, which we do not currently support. We currently support S_TEXT/UTF8 only for MKV. Let's use this as a feature request to add support, however note that we will not be prioritising it internally, for the time being.
 Looking at [this](http://www.matroska.org/technical/specs/subtitles/ssa.html), it doesn't look particularly complicated? What are the specific things you're concerned about complexity wise?
 Heh, they look non-trivial :). Thanks for the info!
  I'm really not sure what happened in this specific case. The stack seems to imply that the source was released whilst still enabled? Did you see the crash locally, or was it reported from analytics you have? If the first of those, do you have a more complete log. There was likely a preceding failure during disable, or something? If the second, the same question applies, and also do you have an overview of how often it occurs?

More generally, we should prevent exceptions in Loader.LoadTask.handleMessage from being able to cause process death, and have them result in playback failure instead.
 Does this still happen on recent releases?
 Closing due to lack of updates.
  This is a not found http exception returned from the server, as indicated by:

`HttpDataSource$InvalidResponseCodeException: Response code: 404`
  The stream plays fine with a recent build of the demo app on a Nexus 6. The video appears to be in H.264 High Profile Level 5.0, which is above the minimum H.264 decoder requirements in the Android CDD, so this video will not play on all Android devices. What device are you testing on? 
 It's a very high resolution stream (2248x1504). It's probably just too high of a resolution for the device?
 Devices can often play well beyond their screen resolution. But it is correct to state that each device has a maximum capability. In this case the steam is a very high resolution, so I wouldn't be too surprised if it's beyond the capabilities of a 2 year old device.
 Note - The demo app plays it fine on the Nexus 5 I have, running LMY48M. Maybe your device is just in a bad state. Try rebooting it.
 - As per my previous replies, devices have maximum capabilities and if you try and play media that exceeds those capabilities, it may not work.
- Emulator support for video playback is pretty flaky in general. We're focused on real devices, not on emulators. For media, you should not assume that emulator performance is indicative of what will actually happen on the device, and you should test on real devices instead.
- If you have a specific issue with specific media that you believe is within the capabilities of a specific device, please file a new issue containing all of the information requested in the issue template, such as a link to the media and a full bug report, so that we can investigate properly.
  Unclear whether we both commented on this at the same time, or if you just replied really fast :). See inline comment I just posted in any case; I think it's probably the simplest thing to be going on with.
 Ack. If you put it back to how it was with the simplification suggestion then I'll merge in. Thanks!
  Hey. Thanks for the contribution. I'm a little bit confused by this comment:

_We saw a case where a user would load a stream with various video representations that had the same height and width but a different bitrate. In our UI there was no way to differentiate the streams from each other._

It's unclear why you need the identifier to resolve this issue. The ExoPlayer.getTrackFormat method gives you the MediaFormat of the track, which includes the bitrate. So why not just look at the bitrate field to solve this case?

The above question aside, I do think it's a good idea to propagate the identifier, and can see cases where it would be useful.

We'll try and get this merged soon, or an equivalent change in place. One thing we're mulling over is whether a separate field is the best option, or whether we should just convert the existing field to a string. I think we're thinking that converting the existing field is probably the best option.
 Looks pretty good. If you convert the existing id field to a string and fix the id when creating adaptive formats to do something sensible, then we'll merge this. Thanks!
 Happy to merge this once stylistic alterations are reverted. I don't think the CLA has been signed yet. Any update? Thanks!
 CLA looks good now. Feel free to resolve the merge conflict and update if you have a chance. Otherwise I can probably do this myself tomorrow. Thanks!
  Let's use #612 to track this. They look the same.
  Both of the links above are dead at the moment. I got one of them to work once in the past 10 minutes or so, but all other attempts have resulted in ERR_CONNECTION_REFUSED. This is just in Chrome Browser on a laptop. Please update this bug if/when they're reliably available. Thanks!
 Yes. This appears to be a server issue. I did manage to download the content eventually. I temporarily uploaded it elsewhere, and it worked perfectly from that server.

It appears that the server you're using repeatedly refuses connections. It's either doing some very harsh request throttling, or it doesn't properly support range requests. The latter would explain why the m4a doesn't work whilst the mp3 does.
 My comment above already answers what's wrong with these streams (i.e. the server is refusing connections and/or doesn't properly support range requests). You can upload the exact same files to Google Drive and they play fine.

URLs with spaces in are also unsafe. Although not a problem for ExoPlayer in this specific case, you should remove the spaces if you're hosting this content yourself in order to ensure proper playback across different players and devices (e.g. by converting spaces to underscores). See [this](https://github.com/google/ExoPlayer/issues/871) for details.
 Ah, I didn't see that part of the error, sorry! "Unexpected status line" is pretty easy to search for on Google. It appears in indicate that the server isn't providing a valid HTTP response, which explains why the connection then fails.

I didn't mean that it plays in Google Drive. I meant that it plays fine in ExoPlayer if you just host the exact same content on Google Drive and use the Google Drive download link. Hence indicating that it's a server problem as opposed to a problem with either the media or ExoPlayer. For example, I've temporarily uploaded your content to the following link, from which it plays just fine in ExoPlayer: [URL reacted]
 Which loops back nicely to the comment about the server not supporting range requests properly. If you make a range request, like:

`curl -r 10-20 "http://od.pianostream.com:9000/pianosolo/HE-AAC%2064/Featured/Amy%20Janelle-Whisperings-%20Solo%20Piano.%20Vol.%202-A%20New%20Direction.m4a" -i | more`

then it appears the response headers are broken. iTunes and MediaPlayer probably do not issue range requests for the playback, but it's a legitimate thing for a player to do.
 - It's perfectly legitimate for a player to issue range requests in this way. This kind of functionality is essential for seeking in large video files, for example, where it's obviously unreasonable for the player to download the entire video just because the user seeks to a point near to the end. The server is supposed to return a 206 status code and the requested range in this case.
- We do actually support the case where a server doesn't support range requests. In this case the server is supposed to return a 200 status code and the whole content, and we'll manually skip and discard data until we get to the byte offset that we're interested in. In practice servers should always support range requests unless they're dealing exclusively with segmented media (i.e. DASH or HLS), which is not true in your case.
- In this example the server just isn't returning a valid HTTP response. The first line of the response is not a valid status line. The correct format for a status line can be found [here](http://www.w3.org/Protocols/rfc2616/rfc2616-sec6.html). The correct solution is very much to fix the server. Sorry!
  My understanding is this was resolved by properly clearing the surface when it became invalid.
  Closing as duplicate of https://github.com/google/ExoPlayer/issues/954.
  Yes, you'd want to implement a custom [DataSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/DataSource.java)
  That url is working fine for me. Are you sure you can reproduce this? Please try with the ExoPlayer demo app rather than in your own application, so we can be sure it's not an error in your own code. To do this, install the demo app in the 1.5.1 release (not 1.5.0), and start playback from the command line with:

`adb shell am start -a com.google.android.exoplayer.demo.action.VIEW -d http://vc.howdo.cc/E0F1ABCD5955CFC209D33D1E6072C808.mp4`

If you're still seeing the issue, do you see it only on certain devices? That would be strange, since very little platform specific code relates to the failure mode you're seeing. If so you need to state exactly which devices you can reproduce the issue, attach a full bug report, and provide any other information that might be of use as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
  I think that Genymotion's video decoder isn't taking into account KEY_MAX_INPUT_SIZE property of the MediaFormat, which we're now setting to ensure that the decoder allocates large enough input buffers. Their decoder needs to make sure that it allocates input buffers of at least the requested size, but it appears not to do so. You should file an issue on Genymotion.
 Does the HLS manifest you're trying to play include RESOLUTION tags? Are you sure the device actually supports 4K? KEY_MAX_INPUT_SIZE should be being set properly provide you're using a recent release of ExoPlayer.
 If the resolution tags are missing then it's not possible for the player to calculate the correct value to set in KEY_MAX_INPUT_SIZE. Try adding the tags and see if that makes things any better.
 What device is this? I suggest you file a fresh issue, providing the information mentioned [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Particularly a full bug report. A screenshot of a tiny part of logcat output isn't particularly helpful for debugging this kind of thing.
  @seventhmoon is correct - it's fairly straightforward to implement your own DrmSessionManager. That said, we should probably add support directly to the library. We should also provide a component to assist in requesting the offline keys.
 This has been implemented in the change 9d5c750  Aside from AndroidTV devices, most Android devices don't provide platform support for PlayReady DRM (which is what this UUID corresponds to).
 If you want to cover almost all Android devices, you should use Widevine Classic (for older devices) and Widevine Modular (for newer ones). PlayReady is normally only supported on AndroidTV devices, as far as I know.
  For what it's worth, playback is out of sync using the platform player too on the devices I just tested with. VLC does appear to sync audio and video correctly though. We'll take a look. It's unclear whether it's a media issue or a player issue at this point.
 This file has a ctts box that provides offsets between decoding and composition time. We only support ctts boxes with one entry currently. We should parse the complete table and apply the timestamp offsets, which I suspect will make this file play correctly.
 Correction: we are actually parsing all entries of the table.
 The stream has edit lists (elst) which are not parsed at the moment.
 This is now fixed in the dev branch.
  Both of the audio tracks work fine for me on an nVidia shield TV running LMY47D...
  I'm unsure what the question is here, since ExtractorRendererBuilder isn't used for HLS, which uses HlsRendererBuilder. Is this about HLS or something else?

Either way, we currently support Widevine only for DASH and SmoothStreaming with fragmented mp4 (and webm in the case of DASH).
  As above. See a previous discussion [here](https://github.com/google/ExoPlayer/issues/727), and @jeolivia's link above (note that spaces in URLs are deemed unsafe by the RFC).
  You're right - this isn't currently supported. We (annoyingly) support styling but not positioning for TTML, and positioning but not styling for WebVTT. We plan to add styling support to WebVTT and positioning support to TTML this quarter.
 I believe support was added in https://github.com/google/ExoPlayer/commit/1a9b2be5510bd9698c90aeaaa28a9e1ba965c0a6.
  See #91
  Tracked by #490
  I think it should be possible to query this from the player's Timeline in V2.  Interestingly, isn't the example in MS-SSTR a violation of the definition in MS-SSTR itself? Specifically, shouldn't there be a dash between `AB92` and `E65BE0885F95` in the example, since SystemID is defined as:

```
SystemID = "{"
 4*4 HEXCODED_BYTE "-"
 2*2 HEXCODED_BYTE "-"
 2*2 HEXCODED_BYTE "-"
 2*2 HEXCODED_BYTE "-"
 6*6 HEXCODED_BYTE "-"
 "}"
```

Also, isn't this definition itself wrong, because it defines a dash before the closing curly brace? The final dash shouldn't be present in this definition, right?

Those question aside, yes, we should be handling the case where the UUID is enclosed in curly braces.
 This is fixed in the dev branch. Thanks!
  So what happens here is that we parse what we need from the `senc` box, and hence (currently) expect the `mdat` box to only contain sample data. At this point the encryption data in the `mdat` is, as far as we're concerned, miscellaneous data that we don't care about. Given this, I'm pretty sure the fix for #837 will also fix this issue, since that issue is tracking exactly "handle miscellaneous data that we don't care about in the mdat box". Does that make sense?
  Despite the mp4 file extension, this is actually an FLV video file. We don't currently support FLV. It does sort of work if you patch in this change, which adds FLV support: https://github.com/google/ExoPlayer/pull/828. We're yet to decide whether we want to support FLV officially. Note that even with this change, you cannot seek and the player gets stuck in the buffering state at the end of the content (see below).

A couple of questions:
- If you control the media, why are you using FLV? Can you use MP4 instead? I don't think Android officially supports FLV for the built in MediaPlayer. I don't think iOS supports FLV either, although I could be wrong. Anyway, it seems like a strange choice.
- There seems to be something weird/broken with the server. It claims the media is ~2GB long and then prematurely closes the connection at 84MB. For example if I try and fetch the media with curl, I get:

```
curl http://5c856ba6cc9ed2fe2e9a-6e12211e6bcd3a3454f29c48581571ff.r59.stream.cf5.rackcdn.com/69c0f207-6c65-e511-80cd-fcaa14926e72_c624d65a-1566-e511-80cd-fcaa14926e72_87356fac-4302-e511-afdd-d0509926ddf2_android_28092015103324_986806128.mp4 > testwhat.mp4
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  4 2047M    4 84.0M    0     0   379k      0  1:32:12  0:03:47  1:28:25  410
kcurl: (18) transfer closed with 2059165544 bytes remaining to read
```

This is broken and will cause issues. It's most likely why playback gets stuck in the buffering state when you patch in FLV support (not certain about this).
 I used [this tool](https://mediaarea.net/en/MediaInfo/Download). This is by no means an endorsement, but it did the job in this case!
 FLV is now supported, although without seeking support currently. FLV doesn't actually contain a media index to make seeking easy, so it's unclear whether we'll ever support it. Closing the issue for now. @jeoliva can comment on whether it makes sense to add seeking support at some point, or not.
 Please file a separate issue, with all of the information listed [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html) including a link to sample media. Most likely the media you're attempting to play is not supported by Android/ExoPlayer.
  All ExoPlayer does is to draw into the surface it's provided with. Any UI performance issues are beyond the scope of this issue tracker, so you'll likely have to try and figure this out for yourself. A good resource is the Android [performance tools page](https://developer.android.com/tools/performance/index.html). The rendering analysis tools provided there may well help you track down the root cause of the problem.
  No particular reason. We should move things to be more compliant with the spec.
  Doing this for chunked subtitles is problematic, because deciding when to load chunks is based on their start times. That said, I assume this request is exclusively about non-chunked subtitles. It doesn't really make a huge amount of sense to consider a chunked case where the timestamps are incorrect, but it does make sense for the sideloaded non-chunked case.

Would it be enough to simply pass a sampleTimeUs parameter into SingleSampleSource, which is probably where you'd be loading subtitles from for this case. Currently this class hardcodes the single sample timestamp as 0. Setting to non-zero should have the desired effect of offsetting all of the timestamps contained within the subtitle.
 We could let `SingleSampleChunkSource` implement `ExoPlayerComponent`, and allow passing of a message to change the offset without having to recreate the `SampleSource`. Changing the offset would look like:

```
int captionTrackIndex = player.getSelectedTrack(captionRendererIndex);
player.setSelectedTrack(captionRendererIndex, TRACK_DISABLED);
player.sendMessage(singleSampleSource, MSG_SET_SAMPLE_TIME, offset);
player.setSelectedTrack(captionRendererIndex, captionTrackIndex);
```

Where disabling and re-enabling are necessary to ensure that the sample gets reloaded from the source with the new offset. 

I know it's a bit convoluted, but making changes anywhere near the actual text renderer means that they'd be on the code path for the chunked case as well, and so we'd end up having a setting there that only works for a subset of use cases. In practice, I think the approach above would work well for your needs. Thoughts?
 I don't have concerns around the `SingleSampleSource` case. I'm not sure how easy it is to implement shifting "properly" in `ExtractorSampleSource` though, or how compatible any such change would be with future plans around having `ExtractorSampleSource` use `LoadControl`. It doesn't sound like a particularly great path to be going down to me, although I haven't given it much thought.

In general, it's not our priority to play whatever random-broken-stuff an application throws at us.
  Please provide additional information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Thanks.
 Thanks!

Is the URL that you're providing to the player a master playlist (i.e. lists the variant playlists) or a variant playlist directly (i.e. lists the media segments)? If it's a master playlist, can you try passing each of the variant playlists directly instead? It would be useful to know whether they work correctly. If they do, then is it the case that the different variants have non-aligned media sequence indices and/or different duration media segments? If so, the issue you're seeing is the same as described in #699. Until we fix that issue, we only support aligned media sequence indices and (required for the media sequence indices to be aligned) equal duration media segments.
  If I change [this line](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java#L193) to use `false` rather than `true` in the demo app, then the demo app will default to the paused state rather than the player state. If I then press home and come back to the app, I see the frame restored correctly after a short period of time which is needed to re-buffer the data.

So this sounds like an issue with your application rather than with the library. Perhaps you're not instantiating/preparing the player properly in onResume.
 Note - One possible cause would be if you are using the deprecated `FrameworkSampleSource`. If you are then that's a platform limitation, and if at all possible you should use `ExtractorSampleSource` instead.
  Taking the demo app as an example, in `HlsRendererBuilder.onSingleManifest`, you can read `manifest.baseUri` to get the URL after redirection (i.e. URL B). Hope that helps!
  Please provide additional information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Thanks.
 It probably is a CyanogenMod issue. It's not hugely helpful that the first post in this issue didn't mention that you were using a custom ROM. Closing for now, since this issue still doesn't contain all of the information requested [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Note specifically the 4th, 5th and 6th bullet points in that post, none of which seem to be provided here.
  Happy to merge this, but you'll need to electronically sign the Google [CLA](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md) before we can do so. Note that it's the Google CLA that you need to sign, not the Android one. Thanks!
 Please notify, yes. Thanks!
 NB - I bumped directly in https://github.com/google/ExoPlayer/commit/0b1c8897bc4e589ce6a96a353abc8782fd535707, so no need to worry about this. Thanks!
  Hi. I'll get to this soon (sorry!).
 Note that we have issues elsewhere saying "make the demo app simpler". So it's pretty unclear whether that's a good idea :)...
  Waiting for STATE_READY is the right thing to do. Sounds like something else is wrong, but you haven't provided sufficient information for us to determine what that is. Please see [this](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Thanks!
 It does [print state changes](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/EventLogger.java#L69) to the logs though, so you should be able to tell whether the expected state changes are occurring by looking at logcat. Alternatively, it should be pretty trivial for you to fork ExoPlayer and add a loading spinner locally for debugging purposes.

Note also that apps like YouTube do show a spinner based on this logic, and haven't reported any issues.
  #1270 tracks adding support for playlists. This is non-trivial to implement because it is necessary to buffer future sources before the current one finishes, so that transitions between sources can be seamless.
  We don't support demuxed audio for HLS yet. This is tracked by #73.

As an aside, if your systems are capable of outputting DASH streams, you (and any clients) should really use them even when we do get around to adding support for demuxed audio for HLS.
 Closing as this is tracked by #73.
  Those messages come from the underlying platform, so we have no control over them. I don't think they're problematic, and I don't know why they're logged with E rather than I or D.
  Yes. When you instantiate the components needed for playback, you'll build a `SmoothStreamingChunkSource` to generate video chunks. One of the objects passed to the constructor is a `FormatEvaluator`. You're probably passing an `AdaptiveEvaluator` for this argument at the moment. The solution is to create and inject your own `FormatEvaluator` implementation, probably based on the `AdaptiveEvaluator` implementation, that selects the initial format in a different way.
  Are you sure the issue is specific to Widevine? Do you have an otherwise identical non-encrypted stream that you can test on a Nexus Player internally? The reason I ask is because this bit of the logcat looks generally suspicious, but may not be Widevine related:

```
E VideoDecoder: Reference frame 0 is missing. Stop decoding.
E VideoDecoder: setReference failed. status = -6
E VideoDecoder: beginDecodingFrame failed. status = -6
```
- What version of Android are you running on the Nexus Player, and if not the latest, does updating help?
- Does updating to the latest release of ExoPlayer help (it looks like your branch contains 1.4.2, which isn't that recent).
- If you could attach a full bugreport (adb bugreport > bugreport.txt) and share that, that would be more helpful than just logcat.

Thanks!
  We're only supporting DASH and SmoothStreaming with Widevine (and Playready where the device supports it - primarily on AndroidTV). For HLS, we're currently supporting AES-128 only.
  Why don't you just use `MediaCodecInfo.CodecCapabilities` directly for this?
 No objection. I'm going to close this issue because it looks like you have a solution. Feel free to send a pull request to the dev branch (nb - please add `@param` and `@return` javadoc to the method if you make it public) if you'd like to see the change incorporated into the library. Thanks!
  The [build instructions for the software VP9 extension](https://github.com/google/ExoPlayer/blob/dev/extensions/vp9/README.md) suggest fetching the latest version of libvpx from https://chromium.googlesource.com/webm/libvpx.
  This is a valid point. Can you point to a stream that includes tfdt boxes at all?
 Given the SmoothStreaming specification doesn't mention tfdt boxes at all, I would suggest the best thing to do is simply to ignore them for SmoothStreaming playbacks, rather than having them override the segment start time.
  Could you specify exactly which of the "various available encoders" exhibit this issue for VOD content? Thanks.
 Thanks for the update. Does this issue still occur with mp4box, or has that been resolved?
 @erdemguven - It's probably rather easy to handle this in 2.x (`DashChunkSource.onChunkLoadError` could probably handle the error, and then when a subsequent call comes into `DashChunkSource.getNextChunk` requesting the final chunk - we could instead return endOfStream).
 Fixed in `dev-v2`, which is the V2 development branch.
  Can you link to the working stream as well? It'll be good to have for comparison purposes.
 On reflection, the working stream probably isn't necessary. We do support encryption data being packed into the `mdat` box, but it looks like FragmentedMp4Extractor isn't liking what you're putting there.

Could you describe exactly what you're putting in the `mdat` box? The extractor expects the encryption data followed by the sample data, with nothing else in the box. It looks like encryption data is expected to be in the following format.

```
For each sample:
  Vector (for your stream it looks like we expect 8 byte vectors)
  Subsample count (2 byte unsigned int)
  For each subsample:
    Clear length (2 byte unsigned int)
    Encrypted length (4 byte unsigned int)
```

For your stream, the extractor is parsing a huge value as the subsample count for the first sample, which is the root cause of the subsequent index out of bounds failure, so we need to figure out why that's the case. Questions:
- Is there any chance your packing other data into the `mdat` box in addition to the encryption and sample data?
- The extractor thinks you're using 8 byte vectors. Is that correct?
- Does the format of the encryption data match the above?

Thanks!
 I think there are 8 bytes of data before the encryption data as well (I'm not sure what they are; sample values are 0,0,0,2,0,0,0,-69). Is that intentional? We don't support extra stuff in the `mdat` box currently, so you'll probably need to change your modification process to actually remove it.
 Yes. As a more general point, I do think the spec is overly broad (as far as I can remember, it allows for things like putting the encryption data after the sample data, or putting sample data at a position that would be very inconvenient in practice). So whilst we'd like to support skipping of non-encryption and non-sample data in mdat boxes, I think we'll continue to assume that the ordering and approximate positioning of this data is "sensible". I've filed: https://github.com/google/ExoPlayer/issues/837 to track, and so will close this.
  Unless you're doing something that involves caching/writing media to disk then you don't need the permission. If you do, it's up to you when you request it.
  As a general point, does anyone actually use these vertical modes? I'd quite like us to not support things that no-one uses, because there's a non-zero maintenance cost associated with every feature we take. Some real-world examples to help verify correct behavior would be nice.
 To ask a (possibly stupid) question: Is rotation the correct way to achieve vertical flow for such cases, given that this approach will also rotate all of the characters? I guess I kind of assumed that something fundamentally different would need to happen during text rendering, that involves writing vertically but not doing any character rotation. Please educate :). Thanks!

Will take a more detailed look at this on Monday.
 Sounds good; thanks!
 Hm. I'm not sure about this. I think the (now backed out) vertical text changes have ended up moving things around a little that wouldn't have been moved around otherwise. If you were to make the minimal change necessary to fix the two things that this change is now fixing, what would that look like (NB - It'll be much easier to review if the code doesn't move into separate helper methods; if that's really needed/preferable, it would be good to do it as a subsequent change that changes only that and nothing else).
 Thanks for the screenshots! I'm not convinced the previous behavior was incorrect though. Looking at the spec:
- For the first screenshot, I think a line percentage is a percentage from the top of the viewport. From the spec: _A WebVTT line cue setting configures the offset of the cue box from the video viewport’s edge in the direction opposite to the writing direction. For horizontal cues, this is the vertical offset from the top of the video viewport._
- For the second screenshot, I think `position:20%,end` means you should position the right hand side of the cue box 20% from the left of the viewport. There's an example in the VTT spec illustrating this, that states `position:10%,start size:35%` and `position:45%,end size:35%` are equivalent.
- For the third screenshot, I think negative line numbers are supposed to count from the bottom of the viewport, where-as positive line numbers count from the top.

Thoughts?
 Doh :). Oh well; it was an interesting verification exercise at least! The positioning defined by the WebVTT spec is indeed. . .complicated. It's somewhat unclear to me whether it's somewhat over-engineered, or whether everything in the spec is actually useful!
  What device and OS version is this happening on? And also, is this all streams, or just your streams? Does it reproduce in the demo app?
 Closing due to insufficient information.
  Does this occur when playing the SmoothStreaming (PlayReady) sample in the demo app, or only when playing some other content that you have access to? Thanks!
 Closing because bug is stale. If still relevant: I don't have any suggestions, sorry. I'm guessing something about the way you're encrypting the streams is invalid and/or not handled by the PlayReady DRM implementation.
  We don't support WebVTT for HLS playbacks yet (see #151), which is the use case for which X-TIMESTAMP-MAP is actually intended to be used. It was previously being misused for propagating timestamp offsets for WebVTT in DASH. We will of course bring it back once we implement #151.
  In general I'm not a fan of adding support for legacy container formats. If we take it, it's something we have to commit to maintaining. However, as you say, this does look pretty simple, and so it seems like a reasonable request!

I'll post a few initial comments now, and try and have a more in-depth look later this week. Thanks!
 Sorry for the delay looking at this. Will try and get to it tomorrow!
 I've also submitted some stylistic tweaks just to bring the code in line with the rest of the source: https://github.com/google/ExoPlayer/commit/950cc70003d7633d69afb2e2d4d0859c9b1fa0c6
 And some further changes. I was able to simplify ScriptTagPayloadReader somewhat. Please take a look, and let me know if I accidentally broke anything!
 One final question: Where did the test link you've added come from? I want to know if it's fine to copy it across to our own hosting. Obviously the underlying content (big buck bunny) is fine for us to copy. Depending on who packaged it, we might need to re-package it ourselves to copy it over. Please advise. Thanks!
 The live stream you link to doesn't set the TypeFlagsAudio or TypeFlagsVideo flags in the FLV header, which are supposed to be set to indicate audio and video are present in the stream. So I think it's an issue with the stream rather than the player. If I hardcode hasAudio and hasVideo to true in FlvExtractor, then the stream (sort of - albeit with a lot of apparent distortion) plays ok.
 Thanks. for the info. I made some further changes to the extractor in 4422e8a0155642a4708e1f50b1fb2c340c78f56d based on an internal review. I've put a couple of comments in the old version of the code in that change that explain why the new state "skipping to next tag header" state is necessary :).
 See @jeoliva's note on this thread above seeking: _In fact, FLV container doesn't include any kind of information to help on implementing seeking_.

If you want your media to be seekable you should use a more appropriate container format, like MP4.
 There's an alternative "accepted" way where the server will allow you to specify the seek position as, for example, a query parameter. In that case the server does the brute force tag-by-tag search (it's probably implemented to have built an index already) and serve you the media from the specified seek position.

The takeaway here, however is that (a) FLV does not support seek properly, (b) we don't intend to support any hacks to make seeking in FLV possible in ExoPlayer, unless they're external contributions and do not extend anywhere outside of the FLV extractor package, and (c) if you're knowingly packaging your own media as FLV, you should stop and choose a more appropriate container like MP4.
 We have no plans to support seeking in FLV. All of the approaches mentioned would add complexity to the upstream package, and would not work in a consistent way across different `DataSource` implementations. If you want seeking, you should use a container format the properly supports it (or DASH, SmoothStreaming or HLS).
  Thanks for this. I think @andrewlewis is planning a similar but more targeted change.
  Yep, thanks!
 Out of interest, have you seen any bad behavior that you have been able to trace back to this, or did you just spot it browsing the code?
 Ok, thanks!
  Please see #426 (it's a good idea to search existing issues before filing a new one, also!). Thanks!
  I took a look at the source of the [this gifv](http://i.imgur.com/zvATqgs.gifv). It looks like gifv is syntactic sugar around actual mp4 and webm videos, whose urls in this are `http://i.imgur.com/zvATqgs.mp4` and `http://i.imgur.com/zvATqgs.webm`. ExoPlayer plays both of these urls correctly, so it seems we're already doing the right thing here.
 The link you provide works fine for me. We don't support loop mode as a direct feature at this time. The best you could do would be to reset the position back to 0 when the player tells you that it's ended.
  Well, it is a connection error. Does your domain contain any underscores by any chance? If not, there's not much we can do to debug the issue without the proper link (feel free to email it to olly.exoplayer@gmail.com)
  This question is not about functionality supported directly by ExoPlayer, and therefore falls outside the scope of this issue tracker. Closing.
  A crash there indicates that the media itself is malformed. Specifically, that there are malformed (or in some way unusual?) SIDX boxes in the media chunks.
 The stack trace shows that the player is successfully loading media chunks from somewhere, and that the media they contain is malformed. Perhaps the other players you mention are just skipping the sidx box, rather than parsing it. Or perhaps they're just allowing the sidx box to be malformed.

You'd be best off looking at the exact content of the sidx boxes in the stream, and seeing if they're according to spec.
 If this is a video only live stream you may be hitting #758.
 No ETA as of now. It's one of a couple of annoying niggles that we plan to fix but are relatively low priority because they don't relate to primary use cases (which typically involve audio as well as video). Let's use #758 to track, in any case. Thanks!
  Perhaps, but it doesn't seem like a priority. It also feels like in general, the client would be better placed than the server to determine a suitable value for the duration of media to buffer before starting playback (e.g. the client knows about what type of network it's on, and may have information about the network conditions).
 Where do server requirements come from, exactly? It's unclear to me why the server should need to place any constraints on the minimum buffer duration.
  I have no particular objection, but I can't think of a use case either. Can you say what you actually need this for? There may be another way of achieving what you need.
 Ok, that's fine :).
 Note - Tweaked slightly in https://github.com/google/ExoPlayer/commit/e652019bb3eb2041218ecf928c10b3166f728f78
  This is an issue with Sony's software update :(. My understanding is that they're aware and will be resolving the problem in a future update. It applies to DRM protected playbacks only, I think.
 Which demo streams, specifically?
 Right. Yes, that is DRM protected. I'm not sure. It might be that the performance issue only applies when HDCP is required, and/or when the secure video path is required. I'd suggest trying some of the other samples that have different requirements to figure this out.
 We've definitely seen similar performance issues. I'm fairly sure I reproduced with the sample videos at some point on one of the Sony Xperia device range. I doubt it's anything specific that you're doing. All the platform does is decrypt and decode the frames; ExoPlayer handles things like stripping away the container at the application layer. A few random ideas about things that might make some streams worse than others:
- Higher bitrates and/or resolutions.
- License policy (already covered above) around HDCP / Secure path. I'm unsure whether there's anything for the audio streams that would be analogous. There are also policy settings for things like PST usage table features, which you could try turning on and off if you want to experiment.
 I think (hope?) Sony fixed this in more recent device updates.
  `ExoPlayer.getBufferedPosition` and `ExoPlayer.getBufferedPercentage` gives you the buffer position and percentage respectively. If you want periodic updates, call them periodically.
  Hi, I've started looking at these. I'm commenting on the individual commits. So far:
https://github.com/rtsmb/ExoPlayer/commit/99167da07bfb2597c351614a2fe84a14474712dd
https://github.com/rtsmb/ExoPlayer/commit/3921919115899cc096e90023db19e600507d7715
  As I understand it, streaming licenses are only valid/usable for as long as the MediaDrm session into which they've been loaded remains open. ExoPlayer's StreamingDrmSessionManager keeps a MediaDrm session for the duration of the playback only. Starting a new playback will open a new session, and so it will be necessary to load the licenses again.

We have no plans to change this behavior. If you were particularly motivated to do something different, you'd need to implement your own DrmSessionManager that holds a MediaDrm session open for a longer period of time, and reuses it from one playback to the next. Note however that holding multiple sessions open is generally a bad idea, and will likely cause session instantiation to fail if you try to have too many sessions open at once.
  I don't really understand. What exactly does skipping mean in this context? In other words, where does the skipping happen, and how does it lead to the two being out of sync? I tried deliberately ignoring the fact that the first keyframe(s) in some media are keyframes, but couldn't reproduce an issue.
 Ah, managed to figure out what you mean now. To check we're talking about the same thing, the way I see this happening is as follows:
- Suppose the first chunk contains zero keyframes, and that the second chunk contains a keyframe but not as the first sample.
- Suppose the first two chunks are fully buffered.
- A call to readData will call sampleQueue.getSample, which will skip to the first eligible sample, which is the keyframe in the middle of the second sample.
- The next call to readData will hit the issue you describe, where the sampleQueue.getReadIndex returns a value larger than mediaChunks.get(1).getFirstSampleIndex().

Is this consistent with your understanding? I think the fix is to change == to <= as you suggest. I'll get that done. Thanks!

As an aside, what media are you playing where chunks don't start with keyframes? DASH/SmoothStreaming chunks should always start with keyframes, so it sounds a bit like the media is bad?
 Got it. Note however that you wont get proper seamless switching between formats if you adopt that approach. When the player wants to switch from one quality to another, it will download chunk N of the old format and then chunk (N+1) of the new format. If the latter chunk doesn't start with an access point, there is no way for the player to perform the switch seamlessly, and it will necessarily discard media up until the first keyframe that it discovers in the new format.
  @randeeppr, did the above resolve the issue for you? Thanks!
 I'm not sure as to the security of each approach, but I don't think there's anything stopping you from setting it in the query string. It's your code that's putting the data in the header, after all :). I think putting the data in the query string consists of writing the data directly into the url?
  It looks most likely that some of the variants are defining a different number of media tracks than other variants. ExoPlayer doesn't handle this case. If you provide the URL to olly.exoplayer@gmail.com we can confirm.
  Kindle devices have known compatibility issues. See for example [this](https://forums.developer.amazon.com/forums/thread.jspa?threadID=5067).
 Closing this. Hopefully ExoPlayer compatibility issues are fixed in FireOS 5.
   This functionality will be pushed directly to dev in due course. Closing PR.
 The changes here have been merged now, so you should be able to propose further changes as PRs on the dev branch. Thanks!
  Thanks!
  Please provide additional information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Thanks.
  You can distinguish a live stream from a vod one by querying the player's duration after it is prepared. Live streams will return `UNKNOWN_TIME`, where-as vod streams will return the known duration of the stream.
  The end of the chunk is probably not clipped correctly to a chunk boundary. Can you send a link to olly.exoplayer@gmail.com? Thanks.
 Right. As you noted via email, this is because we don't support mp3 media segments in HLS at the moment (although we do support mp3 tracks within MPEG-2 TS media segments).

Fortunately, it looks easy to add support by simply hooking into the existing Mp3Extractor. I'll push a change this week.
 Now supported on dev.
  Please see  #798.
  #1270 tracks adding playlist support.
  Both of those are playing at normal speed for me, using both dev and master branches. Is this only happening on some devices or some Android platform versions? Please provide as much detail as possible on what devices / Android platform versions you've experienced this issue on.

Thanks!
 It more just means that the issue hasn't been diagnosed yet. From the above, I'd guess that the issue is limited to certain devices and only when playing mono audio, possibly specifically mp3? Whether that's an issue for you obviously depends on whether you're playing mono mp3 audio streams and whether you care about the affected devices.

We'll try and get around to investigating this in detail soon.
 I tried this on a Lenovo B6000-F and it worked fine for me. Although my test device runs 4.4.2 rather than 4.2.2 as was reported by @kylejbrock. I do wonder to what extent users can fix this themselves by upgrading their OS to the most recent version available for the device. You could/should recommend they try this.

If it is a device specific issue affecting only certain devices only on Android 4.2.2, then it's possible that we could find a workaround. I don't have a device to reproduce the issue with, however, which makes looking for such a workaround difficult.
 The change ref'd above most likely fixes this issue on the dev branch (and in 1.5.6, when we generate that release). Please verify.
  The UI on this device is probably rendered at 1080p, but SurfaceView surfaces can show full 4K resolution. Display.getRealSize is reporting the lower UI resolution so the stream is getting filtered out.

As per https://developer.android.com/preview/api-overview.html#4K-display we need to use Display.Mode which was added in API 23. For earlier platform versions we would need to find out the real resolution in some other way or implement a workaround.
 This is fixed for Sony Bravia devices. This issue now tracks the general purpose solution, which is to use Display.Mode on API level 23+.
 The general purpose solution will cover these devices once they receive an M update. In the meantime, I'm not sure they're important enough to justify special casing. For the shield tv device, whether the display is 4K depends on what display the shield box is plugged into. In many (probably most) cases, the display wont be 4K. For the Z5 models it's not hugely important simply because the screen isn't big enough for 4K to be noticeably better (and conversely, 4K will drain more battery and require more data).
  If you're able to come out of the player activity by pressing the back button, then onBackPressed() must be getting called. This is because the base implementation of onBackPressed is what actually causes the activity to finish when the back button is pressed in the first place. In the unmodified demo app the player controls do swallow one back press if they're visible, which could explain why you need to press it twice. The first press will hide the controls, the second will be delivered to the activity. This is working as intended.

onStop and onDestroy definitely get called for me, and given we've had no other reports of this and the code has been unchanged for some time, I very much suspect it's something wrong with modifications you've made to the code / some strange device you may be using.
 The application's thread must be getting blocked at some point (possibly in onPause where it calls releasePlayer). You'd need to capture a full bug report and upload it somewhere for us to take a look. See:

http://stackoverflow.com/questions/26789107/how-to-write-an-android-bug-report-to-a-file-via-adb
 I don't see anything obvious in that report. When exactly did you capture the bug report? You should start playback, then press back a whole bunch of time (observing that it doesn't work), and then capture the bug report whilst still in that state. If that's not what you did, could you capture and share another one? If you did that already, I'm not really sure. Thanks!
 There's nothing actionable here given the information we have. Closing for now.
  You might be avoiding crashes, but that doesn't look right to me. ExoPlayer relies on segment indices being aligned across different representations within each adaptation set. Specifically, we calculate the next segment index as the previous segment index plus one, so if they're not aligned then you'll end up skipping or repeating media whenever a quality change occurs.

Why are the start numbers different between the different representations? That seems very odd to me. It's unclear whether it actually violates the spec (I can't see anything obvious), but it's unlikely that DASH players in general will handle this case gracefully.
 It might be spec compliant, but given every other DASH stream I've ever seen ensures consistent numbering across representations, I doubt we'll be supporting it. There's no benefit to a server having mismatched sequence numbers across representations that I can think of, so it doesn't make sense to make clients more complicated to handle that possibility.

The spec in general is not specific enough, in my opinion (hence the existence of things like the DASH interop recommendation docs, that try to further constrain what's actually allowed).
 Actually, the sample manifest is in violation of the base DASH spec. It indicates `segmentAlignment="true"`. The specification says:

<i>when not set to ‘false’, this specifies that for any two Representations, X and Y, within the same Adaptation Set, the m-th Segment of X and the n-th Segment of Y are non-overlapping (as defined in 4.5.2) whenever m is not equal to n.</i>

which is violated unless the indices are properly aligned. This means that you'd need to set `segmentAlignment="false"` to allow misaligned indices. We don't support this case, because playback where segments are not aligned is in the general case significantly more complicated and switching is less efficient. Also, the DASH-IF Interoperability points require that this parameter is true, and the base profiles in the DASH spec indicate that non-aligned adaptation sets may be ignored.
 Is it not possible to fix whatever source is generating these manifests, given they're invalid ;)?
 In the part of the specification I quoted, m and n are segment numbers. It's quite a stretch to argue that startNumber values are completely unrelated to segment numbers, and that startNumber values are used only for generation or URLs. In fact, this claim is disproven in A.3.3 of the specification, which clearly shows startNumber as being used for indexing in addition to URL generation.

Also, from a "what would be useful" point of view, requiring proper alignment is very useful for efficient playback, where-as not requiring it is not only not useful, but also has no other obvious benefit. It should be trivial for a server to properly align the indices across different representations. Given this, why would it make sense for a specification to define the latter? And why have all other DASH serving infrastructures that I'm aware of have chosen to align them properly ;)? To ask the same question again, why don't you just fix the source that's generating the manifests?
 Note also that startNumber is defined as:

<i>specifies the number of the first Media Segment in this Representation in the Period.</i>

So you're basically stating that you believe m and n to not be segment numbers, or that they're somehow different numbers to the numbers that this part of the specification refers to. Seems highly unlikely.
 Annoyingly A.3.3 has been completely replaced in the 2014 spec (I was looking at 2012).
  We added some intent support in https://github.com/google/ExoPlayer/commit/bcb4ea4f703e667e910d8b4016ddab67d11af2eb. Note that we require a custom action, so this is aimed toward being able to easily start a playback with an adb command, as opposed to having us actually appear as a playback option in the disambiguate UI when, for example, selecting a video from a gallery. We believe that this is suitable for most developer needs, and so I'll close this request.
  Thanks for investigating this! My guess would be that proguard is making a valid optimisation, but that the resulting optimised code is then erroneously further optimised by Dalvik on 4.2.2 (I think Dalvik must be involved, otherwise I don't think the issue would be platform version specific).

Please could you sign the CLA so that we can merge this request. See [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md) for details.
 Hi. Apologies, you need to select "Any other Google project" rather than "Android OS" when submitting the CLA, since ExoPlayer is not part of the OS. Could you submit again? Thanks!

We should probably clarify this in the Contributing file :).
 Thanks!
  Thanks!
 I'm taking a quick look at further cleaning this up. Since you've been looking at the spec, I was wondering if I could pick your brains about this section:

<i>This example shows how to position cues at explicit positions in the video viewport.

WEBVTT

00:00:00.000 --> 00:00:04.000 position:10%,start align:start size:35%
Where did he go?

00:00:03.000 --> 00:00:06.500 position:90% align:end size:35%
I think he went down this lane.

00:00:04.000 --> 00:00:06.500 position:45%,end align:middle size:35%
What are you waiting for?
The cues cover only 35% of the video viewport's width. The first cue has its cue box left aligned at the 10% mark of the video viewport width and the text is left aligned within that box - probably underneath a speaker on the left of the video image. "start" alignment of the cue box is the default for start aligned text, so does not need to be specified in "position". <b>The second cue has its cue box right aligned at the 90% mark of the video viewport width. The same effect can be achieved with "position:55%,start", which explicitly positions the cue box</b>. The third cue has middle aligned text within the same type of cue box as the first cue.</i>

The text in bold looks wrong to me. Doesn't "align" apply within the cue box? For the bold text to be correct, shouldn't the example be:

<i>00:00:03.000 --> 00:00:06.500 position:90%,end size:35%</i>

instead?
 Ignore me. The "computed position alignment" inherits from "align", so this does actually make sense.
 FYI - I have a change that (hopefully) further cleans up the parser, propagates the anchors and applies them along with the width in CuePainter.. Will merge next week.
 Draft change is [here](https://github.com/google/ExoPlayer/commit/6a9c88358c54afe56847290e25e16b81e10d0bd3). Any comments appreciated (just dump them directly on the change, if you have any!). Thanks.
  In most cases we do retain the buffer when seeking forward into it. We do not retain any buffer behind the current playback position. Except in the special case where the media is buffered to the end, it's better to discard media behind the current playback position in order to buffer further ahead into the future.
 The todo is for optimizing forward seeking specifically for HLS. We've already done this for DASH, SmoothStreaming and regular media playback, where such optimization is significantly easier. We'll get round to it eventually, but it's considered low priority. In the meantime, you'll either have to live with the existing behavior or work out how to implement what you need yourself.

Note that we have no plans to retain buffer behind the current playback position (and that the todo is only referring to the forward direction).
  Sounds a bit like bad media to me, or at least unusual media. You'll need to provide a sample stream for us to debug.
 You may also like to try upgrading to the latest release, just in case that resolves the issue. I think you're on 1.4.1 currently.
 Closing due to lack of information.
  The player isn't recognising the format of the stream. Could you provide a link to it?
 The AdtsExtractor's sniff method returns false for the affected streams, but if you force it to return true then they play just fine. It looks like the server doesn't bother to make sure that it starts sending data from the beginning of an ADTS frame; the data just starts from an arbitrary point in the stream.

To correctly sniff these streams, I think we'd need to locate the first sync word (after any ID3 headers have been skipped). This could be anywhere in the first 8K of data, since that's the maximum ADTS frame size. We would then need to either verify the ADTS frame header, and/or possibly read its length and check that there's another sync word immediately following it.

@andrewlewis Thoughts?
 Sounds good. The platform extractor checks for a frame at the start of the stream, but I can't see a reason not to search as we do for MP3.
 Sniffing should now correctly detect these streams.
  They're just set to "sensible" values.
  To clarify, is 14419621440000000 the sampleDecodingTimeTable value or the cumulativeTime value? What's the value of timescale in this case?

You can't change the calculation in that way without losing precision in some cases, although we do have a util method that attempts to "do the right thing" [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/util/Util.java#L407).
 I'm still slightly confused. So am I correct in saying that for the case you refer to there is no rollover issue? In which case I assume you're filing the issue because you're concerned that rollover may occur in some other cases.

Even if timestamps are set relative to the Epoch, it'll still be another 24 years before rollover does become an issue. So it doesn't seem worth worrying about?
 Is this your stream or a stream provided by a third party? Rollover means the same as overflow in this context. Can you provide access to the stream itself?

I don't think those values are sensible, and so would speculate that there are issues with the actual stream itself. It appears they're out by a factor of 1000 from sensible values (decodeTime should have three fewer zeros or timescale should have three additional zeros).
 Ignore my comment above about the values not being sensible. They are. The reason you're hitting this issue where other people aren't is simply because the timescale you're using is so large. You really don't need the level of precision that such a large timescale allows, and I would advise you to follow up with the team generating this content and suggest that they might like to change it. It will likely improve compatibility across different players by avoiding this kind of issue. For reference, YouTube's DASH streams use 90000 for video and the frequency for audio (e.g. 44100).

That said, we will make the change proposed above. Thanks!
  It's difficult to see how that change could have broken playback in this way.

These are 404 responses from the server, so I think the only thing on the client that could cause this is if the URLs actually incorrect. You've disguised the URLs in your log. Are the original undisguised URLs that are 404ing actually valid, and is there definitely content at them?
 It's the change in HlsSampleSource.enable that's causing the problem. Fetching the first chunk twice isn't ideal, but our previous attempts to optimise that have proven quite problematic. I'll take another look.

That aside, your server just shouldn't do that. Why is that a sensible way for the server to behave (consider other cases, for example if the client hits an error and needs to fetch the chunk again)?
 According to the HLS spec:

`Any Media Segment that is specified in a Playlist loaded by a client
   MUST be available for immediate download within at least the segment
   duration, or playback errors can occur.`

So when the client loads a playlist containing a list of segments, all of the listed segments must be available for the client to download for at least the duration of the last segment in the list. If the server wishes to make segments so they're no longer available, the server is required to remove them from the playlist so that the client doesn't see them in the list when it next refreshes the playlist.

I have a feeling that all segments in the playlist should remain available if the playlist has an EXT-X-ENDLIST, although the spec is unclear (and slightly contradictory) on this point, so I'm unsure.
 Note - We will be fixing the specific issue of downloading the first segment twice with a new (and hopefully correct this time around) optimisation. We'll push the fix shortly. This will remove the most obvious failure you've noticed, however it's likely you'll experience other more subtle failures until you fix your server to be compliant. There are no hard guarantees that ExoPlayer will not download the same segments more than once.
  On what device + OS version do you see this? Thanks.
 As an aside, CTS can't test literally everything. Passing CTS isn't a complete guarantee that the device doesn't have any issues.
 They're not alternating. One size is the input buffers of the video decoder, and the other size is the input buffers of the audio decoder.

Seems your custom device has a video decoder that provides 65K input buffers by default, which is why they then overflow. They're not big enough. The HTC M8's video decoder is providing 2MB buffers.

If you have a contact in MTK, please suggest they look at the default size of the input buffers that their video decoder is exposing.
 It's the MediaCodec's input buffers that are too small (i.e. the ones that ExoPlayer puts the compressed samples in to feed them to the decoder). The compressed video frames are obviously not all the same size; it's likely that it's playing up to a keyframe, which is larger than 65K, and then failing at that point. The keyframe right at the start of the file must be smaller than 65K.
  Hm, I think there are slightly bigger problems here, because "line" and "position" can both have an optional alignment suffix.
  - Both of these files contain H.222 video, which we're not currently supporting in ExoPlayer. There's no reason why we couldn't support H.222, where the device provides a decoder, but we have no short term plans to do so. Most streams use H.264.
- One of the files contains AC3 audio, which most Android devices don't support at the platform level (with the notable exception of AndroidTV devices).
- As an aside, 25Mbps is pretty unsuitable for mobile devices.
 I filed #793 specifically to track supporting the additional video stream types.
 Can they not stream anything else (both for video and for audio)? I don't think H.222 is guaranteed to be supported on any Android devices (see the [supported media formats](http://developer.android.com/guide/appendix/media-formats.html)). So whilst support may be present, it's not guaranteed.

ExoPlayer does use MediaCodec, but it still needs to parse the samples from the container and extract decoder initialization data, which we don't currently do for these stream types.

We will look at adding support for this kind of video stream eventually (as tracked by  #793), but it remains low priority. For audio, AC3 will probably work for AndroidTV devices, but not for mobile devices.
 Right, so what's missing in ExoPlayer is that we don't have a suitable stream reader [here](https://github.com/google/ExoPlayer/blob/r1.4.2/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L357). streamType is 0x02 in this case. The corresponding platform extractor does have support for this streamType value, on L496 of [this class](https://android.googlesource.com/platform/frameworks/av/+/lollipop-release/media/libstagefright/mpeg2ts/ATSParser.cpp).

So to add support, we'd need to add a corresponding reader, probably based on whatever the platform does. You're welcome to have a go at doing this, if you want ;). Either way, let's move discussion to the new issue.
  Issue is as described by the title :).
  Are you using key rotation? If so, try turning it off and see if that allows you to play successfully.
 Hm. It sounds a bit like a license configuration issue, although this doesn't really explain why it works on a Moto E.

I'm not sure exactly what your setup is, but one way of setting this up is to have each license response provide keys that are valid for X minutes (possibly 2 or 3 in your case), and be configured to invoke another request after some period shorter than X minutes (e.g. 1 minute). Hence the player should always have valid keys. Have you checked that all of this is configured correctly in your case?

If it continues to be an issue, you're probably best off asking Widevine for help, since the failure is probably in the Widevine CDM underneath ExoPlayer.
 If the problem doesn't occur with the Widevine test content in the demo app, it remains more likely that the root cause of the failure is either the way your content is encoded or the way the licenses are being configured (or at least that it's possible to avoid the failure by encoding or configuring in a different way). There are services that have been widely deployed on Nexus 4 (from KitKat) that use Widevine's CDM without this issue.

I think the retry button does (or should) trigger a fresh license request.
  @andrewlewis has a change that will remove prefix checking completely, which we'll push shortly. This means this PR will not be required. Thanks for the diagnosis / info though!
  Why does CM12 try to default to 24-bit audio? That's just wrong/broken, and they'll need to fix it if they expect applications using AudioTrack to work properly, isn't it? In which case I'd suggest filing a bug on CM12 instead.
  I don't understand this. I'll follow up with Widevine.
 Apparently this is a server side issue. Widevine have a fix that has been pushed to some of their proxy instances, but not to the one that you're hitting.

Please follow up with Widevine on the support ticket you created. They should be able to give you an ETA for the fix being pushed, and advise as to whether there's a different proxy that you can use in the meantime (I've no idea whether there is or not). For reference, you can mention that there's an internal bug tracking the issue. The id is 23525547.

Hope that helps!
  Are you certain this plays with video using Android's MediaPlayer? Please try on a Nexus device. It's possible that some OEM devices support additional formats, but I don't think the video track in this file is officially supported by Android.

If you can play on a Nexus device, which one (and what OS version)? Thanks!
 They could do yes, but without standard Android support it would be unclear, for example, in what form the decoder initialization data should be provided. It's also unclear whether or not there's a significant use case if the majority of Android devices wont support the format?

The standard Android MediaPlayer implementation does not appear to support this type of video track, regardless of whether there's an underlying decoder. So an OEM would have to provide both the decoder and a modified MediaPlayer implementation for playback to succeed using the MediaPlayer API.

On what device(s) are you able to play this file successfully using MediaPlayer? We can consider doing as you suggest and make it work where an underlying decoder is provided, (a) we can test it, and (b) the custom decoder takes decoder initialization data in a sensible way.

Thanks!
 Closing due to lack of requested info.
  Sounds more like a device specific issue with the TC97RA1 than an ExoPlayer problem, although it's quite hard to say.
 It's an interesting theory though. Why don't you test it by modifying MediaCodecTrackRenderer. Specifically, where presentationTimeUs is set, add a small value to it to ensure that it never goes negative for the test stream.

In other words, simply replace the line:

```
long presentationTimeUs = sampleHolder.timeUs;
```

With

```
long presentationTimeUs = sampleHolder.timeUs + 66733;
```

Which is sufficient for the stream you provided. Then see if playback works. If it does, we can investigate potential workarounds.
 Thanks for the quick response! What happens if you use:

```
long presentationTimeUs = Math.max(0, sampleHolder.timeUs);
```

Does that also fix it, or does that cause issues?
 There are probably some negative implications of fixing it that way, yes. Some decoders may rely on the timestamps to some extent, for example to re-order the frames correctly before they're output. We'll have a think about how we can fix this "properly", or at least what the best workaround is.
 Neither are a correct fix for the issue. We have a partial fix that should cover most cases, including the case in the stream linked at the top of this issue. We'll push the change soon. For any remaining cases, we'll need to think about it some more.
 This is partially fixed on dev, by the change referenced above. Please confirm that it fixes the issue for you, at least for the test stream at the top of this issue.
  - For subtitles, we should just make the parsers progressively less strict as we find issues. The WebVTT and TTML parsers already have a `strictParsing` parameter, that defaults to false.
- I don't think we want to go as far as allowing playback to proceed seamlessly in this case; the hooks that would be required are a bit invasive given what's being achieved. We could add the renderer index into the ExoPlaybackException that gets reported back to the application though. This would allow you to determine whether the source was the text renderer and take appropriate action. Sound sufficient?
 When you call prepare again you'll need to pass a fresh set of renderers. You'd probably just pass a set that omitted the text one. Or you can set the selected track index to -1; either should work.
 This is fixed in 2.x. See [here](https://github.com/google/ExoPlayer/blob/0e60335064c87b0d62665eab0ac2a5f8a048e236/library/src/main/java/com/google/android/exoplayer/ExoPlaybackException.java#L55).
  Change looks fine, thanks! You need to sign the CLA before we can merge it though (see https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md).
 Ah, thanks! The source I have doesn't have your GitHub name (only email address) so it wasn't coming up in my search :).
  The manifest indicates there are the same number of segments of audio and video, as far as I can see. Where is it indicating anything differently?
  The most useful thing would probably be if you could provide the media, so that we can reproduce. Thanks!
 Thanks. Hm, it seems these also fail to play using Android's native media player. VLC does play them properly though.
 We should probably override 4 to be 2 instead by default in ExoPlayer. We already do this if the value is 1. My understanding is that Main and LTP are basically dead as formats, and that most Android devices don't have suitable decoders for them in any case. In which case we're best off just assuming it's AAC-LC and hoping for the best.
  What buffer size are you using to construct the ExtractorSampleSource? The buffer has to store (by default) five seconds' worth of audio and video to transition from re-buffering back to ready.

This particular stream has an average bit rate of 21 Mbit/s, so five seconds occupies 13 megabytes on average. But in practice you will need a larger size to consume spikes in bit rate and deal with coarse interleaving in the container. I found it was possible to play back this particular file and switch between tracks reliably using a 32 megabyte buffer.
 Agreed that the fix is probably to use a larger buffer. A few other points:
- An alternative to making the buffer larger is to pass smaller values for the amount that you require to be buffered for playback to start/resume. These values are passed when creating the player (through the three argument ExoPlayer.newInstance method). You can set them to 0 if you really want to. The downside of this is that the player will constantly flip between buffering and playing in the case where bandwidth is insufficient, which can be jarring. If you're pretty sure bandwidth will be sufficient (e.g. streaming from a device on a local network) then setting them to 0 may well be a good choice, and will allow you to use a significantly smaller buffer size.
- We have #583 tracking a better failure mode for the buffer-too-small case. Current behaviour definitely isn't ideal, however fixing it is non-trivial.
- You'd be much better off not trying to play 21Mbit/s streams, in general.
  What media are you playing?
 I cannot reproduce the issue. What device are you playing the content on? Can you provide more information?
 I haven't had a chance to try, but I think you should be disabling the video renderer if you want to leave it without a surface for an extended period of time. If instead of calling setSurface(null), you instead call setBackgrounded(true), you'll probably find that audio plays indefinitely. setBackgrounded(true) will both clear the surface and disable the renderer.

If you just clear the surface, I think video data probably keeps accumulating (and not being rendered) and eventually occupies all of the available buffer space, at which point audio stalls.
  What would you expect the client to do with them? All the specification says is: "In the absence of other criteria, the DASH Client may use the first BaseURL element as base URI".
 It kind of depends what "other criteria" is supposed to mean. The fact that the specification doesn't define what these criteria should be kind of indicates that the criteria should be specific to the server-side streaming infrastructure (e.g. YouTube may have different criteria to an application using Akamai's CDN). Obviously we don't know the specifics, so we need to either implement something generic enough to be sensible for all infrastructures, or make it pluggable, or do nothing.

For now, I think we're fine just using the first BaseURL. Which we don't currently do, since the current code will end up picking the last one. I'll fix this.
 We're merging a change into `dev-v2` shortly that decouples baseURLs from `RangedUri` and `SegmentBase`, which might make this significantly easy to do in a way that's both clean and technically correct.
  How would you propose representing format indices for DASH / HLS, where there can in theory be two dimensions of index (i.e. index in the manifest + index in the selected stream)? I've also seen live HLS streams where the ordering of streams swaps around during playback, which is awkward.

There are also adaptive tracks to think about, which don't exist in the original media.
  It looks like your device thinks it's 1970. Are you sure the clock is set vaguely correctly?
 No worries :). It's a shame the device doesn't display something more obvious when the time is so obviously wrong! Although I think anything that uses https breaks in this state, so hopefully users work it out pretty quickly.
  See #151
  That normally indicates that the content of the cenc:pssh element isn't valid.
 Confirmed that the contents of the element doesn't seem to be a valid pssh atom, as is required. You'll need to fix the manifest to get this content to work.
  [Following up through other channels. Thanks.]
  This is a bug affecting only the current dev branch (not the latest release on the master branch). We'll push a fix today.
  I doubt this is anything to do with thread priority. It's probably some unfortunate locking in the underlying media server when the decoder makes its memory allocation, or something like that.

Have you managed to infer anything about what could be causing the momentary freeze?
 You'll probably need to narrow it down further (i.e. which call within ExoPlayer's code, within prepare). My hunch would be that it occurs when the video decoder allocates the memory that it needs. This may lock something in the underlying media server, causing the camera to freeze.

I'm just speculating, although if that is the issue then there's not much we can do about it.
 I don't think there's anything actionable for us to do here, so closing. Please update if you managed to root cause the underlying issue. Thanks!
  See #73
  Can you share the manifest URL? If the streams are protected by Widevine, I would have thought this would make the URL easier to share, not harder. We wont be able to play the content, but the manifest and header of each stream (that's relevant for duration) are normally unencrypted.
  Are you sure this tablet are using hardware decoder? I think this is your
issue!

Em sexta-feira, 4 de setembro de 2015, Rodrigo Lessinger <
notifications@github.com> escreveu:

> We've received some complaints from our tablet users, mainly from Samsung
> Galaxy Note 10.1, about the player not working properly (missing music
> parts, stopping a lot during execution). It increased after we upgraded the
> player version from 1.3.3 to 1.4.2. I've tried to reproduce the issue in a
> Galaxy Tab, although it worked properly, the CPU usage gets too high
> (around 50%, even with the application in background while playing) and I
> suspect this could be the reason it could get buggy in some devices.
> Running the same code and same track on the Android's default media player
> lowers the CPU usage to below 1%.
> 
> This issue may be related to the Android version (4.1.2) that seems to be
> the same on all devices, so as a workaround I'm falling back to the
> MediaPlayer in these versions.
> 
> Do you guys have any idea what could be the issue? I can send a method
> profiling result of the execution if it can help.
> 
> Thanks.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/google/ExoPlayer/issues/759.

## 

_Guillermo Gregorio_
Diretor: _www.alicetesla.com.br http://www.alicetesla.com.br_
T.: (+55) 34 91241302
Skype: b.sword
 There have many ways to check.

**1. With ExoPlayer**:
You can use `MediaCodecTrackRenderer.getDecoderInfo()` To check what Decoder your device want to use with exactly MimeType

**2. Without ExoPlayer**: Check all decoders you have in your device.

```
int numCodecs = MediaCodecList.getCodecCount();
for (int i = 0; i < numCodecs; i++) {
    MediaCodecInfo codecInfo = MediaCodecList.getCodecInfoAt(i);
    String name = codecInfo.getName();
    Log.i(TAG, "Examinig " + (codecInfo.isEncoder() ? "encoder" : "decoder") + ": " + name);
    for(String type: codecInfo.getSupportedTypes()) {
        boolean ap = codecInfo.getCapabilitiesForType(type).isFeatureSupported(MediaCodecInfo.CodecCapabilities.FEATURE_AdaptivePlayback);
        Log.i(TAG, "supports adaptive playback: " + ap);
    }
}
```

**3. Check directly in your device with ADB with less information, you just can know what decoders you got in your device** `/system/etc/media_codecs.xml`

Maybe another people can help with another way/ideia.

Check that and post here what return to you the 1 and 2 ways, and that is easily to know if you are using software or hardware decoder, if software of course your CPU increase and maybe lose some fps in video.
 Try this: #251 
 What branch/version are you using?
Do you know full list of Decoder do you have in this device? (You can force to use one or another decoder for test that)
  Yes, this is a known issue. We should fix it.
 This is fixed in 2.x.
  It's a tricky one. From a developer's point of view and given a single version of the library, it's easiest for the developer if they can arbitrarily extend whatever they want as you suggest. In practice however, what we've seen both within Google's own app teams and externally, is that developers will extend and override things that were never intended to be extended or overridden, and use this ability to implement all kinds of strange things that we didn't intend to support in the way they're implementing them. This is problematic for a few reasons, including:
- It tends to make upgrading to a newer version of the library more painful; because if we don't intend for something to be extended and overridden then we're less likely to guarantee that it's still there and working in the same way from one version to the next, or even that it will still be possible to achieve the same functionality by extension as you were able to achieve in an earlier version. We don't guarantee API stability between versions anyway, but these extension points are effectively parts of API that we haven't even thought about, so they're much more likely to change.
- It encourages developers to implement features in their own code using extension, where often those features could and should be implemented more cleanly and in a more maintainable way within the library.

It's important to note that final could mean either "we don't think it makes sense to extend this" or (more likely) "we haven't thought about whether it makes sense to extend this, and no-one has needed to yet". We're happy to remove final modifiers on a case by case basis where there are valid reasons and if it looks sensible to do so, but I think that defaulting to final is the right thing to do. It forces us to actively think about whether each extension point is sensible, as well as whether or not we'd rather achieve what the app developer is trying to do directly in the library. It also forces app developers to think about whether there's a more suitable approach. A few things I've observed internally are that app developers tend to:
- Use extension where composition would be a much better choice.
- Mock things that really don't need mocking (e.g. immutable classes where it's trivial just to construct a real instance of the real class).
  Yes, this was a bug. I'm pretty sure it's fixed in 1.4.0 and later (onLoadCompleted doesn't call clearState at all).
 I'd strongly encourage you to update to a newer version of the library by the way. There were significant stability improvements for HLS in 1.4.0 and 1.4.1. 1.4.2 is the current stable release.
 Note to anyone seeing similar symptoms to this issue - The root cause is likely different. Please open a separate issue containing [sufficient information](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
  There's currently no way to do this, because we assumed that STREAM_MUSIC would be the correct choice for everyone using ExoPlayer.

Could you explain a little about the use case. Are you actually using ExoPlayer to play an alarm? We can probably add this functionality if there's a valid use case for it :).
 Got it, thanks! It looks like the correct way to do this would be to pass the stream type through the MediaCodecAudioTrackRenderer constructor, into (ExoPlayer's) AudioTrack constructor, and then use the passed value in the three places in AudioTrack where we currently use the STREAM_MUSIC constant. So in your application, you'll specify the type when you build the renderer.

Does that sound reasonable for what you need to do? Thanks!
 @bbincybbaby Since ae0ac55 we support re-setting the stream type at any time. If you're using SimpleExoPlayer, call [SimpleExoPlayer.setAudioStreamType](https://github.com/google/ExoPlayer/blob/270f68a/library/src/main/java/com/google/android/exoplayer2/SimpleExoPlayer.java#L300) passing in the `C.STREAM_TYPE_*` constant you want to use.  The audio track is AC-3, which most Android devices don't support for on-device playback. See Andrew's reply to #620 for a description of the scenarios in which AC-3 is expected to work. AndroidTV devices also commonly provide AC-3 support. I tested your media on a Sony AndroidTV I had and playback is good there.

If you control the formats on the server side, then for mobile devices not connected to an A/V receiver you'd be best off fetching a version of the stream with a regular stereo audio track. If you deliver multi-channel audio it'll just get down-mixed to stereo anyway for output, wasting bandwidth and CPU. If you really want to deliver multi-channel for some other reason, multi-channel AAC should work.
 - A stack trace appearing in the logs doesn't mean that anything crashed. The application isn't crashing in this case, because ExoPlayer is handling the exception internally. If you have a listener registered on your ExoPlayer instance, you should see an exception being propagated to the listener's onPlayerError method to tell you that something went wrong. If you call getCause() on the ExoPlaybackException that you receive, you should get a somewhat specific exception object (AudioTrack.InitializationException) that will give you a clue as to what went wrong and allow you to display an error message. So I don't think things are as bad as claimed above. Please clarify if you disagree or don't see this behavior. If you actually see the application crashing then the logs should look different to those above, so if you could provide them that would be useful.
- Yes, ideally we should play the media without audio (and also indicate that there was an unsupported audio track in the media). We're working on making this better as part of our multi-track improvement works.
- As an aside, note that it's not that the device doesn't support 5.1 audio. The device probably does support 5.1 AAC. It's specifically AC-3 that the device doesn't support. There are valid reasons why devices and/or applications may opt not to support AC-3. See for example [this](http://www.iphonelife.com/blog/87/ios-multimedia-player-developers-forced-remove-ac-3-dolby-digital-audio-support-backup-updat).
  More or less everything that's needed for this is in place. There's one small piece missing. We'll add it this week.
 The change referenced above allows you to pass multiple sources to a TextTrackRenderer. So to switch between multiple subtitles at runtime you should:
1. Duplicate the first three lines of your code snippet above to create a sample source for each subtitle.
2. Pass all of the sources to the text renderer.

Note that if you're using ExtractorSampleSource for audio/video and you're unsure as to whether there are embedded subtitles in the stream itself, you can also pass that as an additional source.
 You can set the bitrate to whatever you like. The important thing is that when creating your MediaFormat you need to set the duration of the subtitle. The constructor that you're using currently sets it to a default, which is C.UNKNOWN_TIME_US. This means that the duration of the media as a whole is unknown, which is why you lose the duration and progress.

To fix this, use the createTextFormat method that takes a fourth argument. Set the fourth argument to either C.MATCH_LONGEST_US (which basically means "this is the same length as the video") or the duration of the video in microseconds, if you happen to know what it is when you're executing this code.

We should probably set the default to MATCH_LONGEST_US rather than UNKNOWN_TIME_US for text tracks, because it's normally what the developer will want.
  The demo app is only a guide of how an application could be implemented. In this case I suspect you haven't adapted it sufficiently to meet your needs. In particular, I suspect the problem might be related to playerPosition and how it's set in releasePlayer and applied as the seek position in preparePlayer. You probably don't want that behavior for what you're trying to do. You'll probably find that commenting out the call to player.seekTo in preparePlayer "fixes" the issue for you.
  That error message is harmless. It occasionally happens when the player is released, and can be safely ignored. See #426. Whatever your issue is, it's something else. In general posting a full bug report captured from a device is more helpful for debugging an issue. Please open a separate issue if you're able to provide more detailed information.
  Yes, this is tracked by #87, and DASH will be implemented first (it's already more or less done for DASH; it's just not accessible in a particularly easy way).
  What does "not work well" actually mean? This isn't sufficient information to debug the issue.
 What error occurs in logcat when it transitions to idle? There's probably a stack trace when the error occurs.
 Same as #748 in that case. Let's use that issue to track.
  Was the second stack trace non-obfuscated because you de-obfuscated it, or because it wasn't obfuscated in the first place (in which case the issue isn't proguard related)? Does this definitely not occur if you don't apply proguard?
 I don't really know what to say about this. It works fine with or without proguard on approved Android devices. It's hard to say whether it's a proguard issue or a ColorOS issue. It's unlikely to be an ExoPlayer issue, at first glance.
 We believe this is now fixed thanks to @souji1103 . Please re-open if not.
  This is supported in `2.1.0`.  I suspect this is the same as #748, although there is insufficient information provided to debug the issue. Closing in any case. Please follow #748 instead.
  ExoPlayer doesn't include an Extractor for reading AVI containers, but the file may be readable using the platform MediaExtractor via FrameworkSampleSource. Note that the class is deprecated due to the [problems described in its javadoc](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/FrameworkSampleSource.java).

Even if it's possible to extract the samples, be warned that in general Android devices do not have codecs for some of the formats often used in AVI files (like VfW and DivX).
 I'm afraid we still don't, and have no plans to support this via the new extractor model. As per my previous comment, you can try the deprecated FrameworkSampleSource.

If you have control over the input media format, please use a more modern container like MP4.
  Support has been merged to `dev-v2` in https://github.com/google/ExoPlayer/commit/28e117d25fb211b18a815b961135b78e7a18abdb.  This is already covered by other issues in this tracker.
  If this is still an problem, please file a fresh issue with all of the information requested in the issue template. There is not enough information provided here for us to investigate further.
  This question is not about functionality supported directly by ExoPlayer, and therefore falls outside the scope of this issue tracker. Closing. We'll be supporting playlists more directly in the ExoPlayer 2.x release.
  Dear all !

I have downloaded the new least ExoPlayer, and running ExoPlayerDemo in eclipse, but not successfull, Please tell me, how to fix this case ? 

Thanks a lots.

this is log file :
 08-26 03:51:19.932: I/dalvikvm(2564): Failed resolving Lcom/google/android/exoplayer/demo/PlayerActivity; interface 1696 'Lcom/google/android/exoplayer/audio/AudioCapabilitiesReceiver$Listener;'
08-26 03:51:19.932: W/dalvikvm(2564): Link of class 'Lcom/google/android/exoplayer/demo/PlayerActivity;' failed
08-26 03:51:19.932: E/dalvikvm(2564): Could not find class 'com.google.android.exoplayer.demo.PlayerActivity', referenced from method com.google.android.exoplayer.demo.SampleChooserActivity.onSampleSelected
08-26 03:51:19.932: W/dalvikvm(2564): VFY: unable to resolve const-class 1727 (Lcom/google/android/exoplayer/demo/PlayerActivity;) in Lcom/google/android/exoplayer/demo/SampleChooserActivity;
08-26 03:51:19.932: D/dalvikvm(2564): VFY: replacing opcode 0x1c at 0x0002
08-26 03:51:20.372: I/Choreographer(2564): Skipped 35 frames!  The application may be doing too much work on its main thread.
08-26 03:51:20.372: D/gralloc_goldfish(2564): Emulator without GPU emulation detected.
08-26 03:51:21.682: D/dalvikvm(2564): GC_FOR_ALLOC freed 80K, 5% free 3030K/3176K, paused 2ms, total 7ms
08-26 03:51:24.512: D/AndroidRuntime(2564): Shutting down VM
08-26 03:51:24.512: W/dalvikvm(2564): threadid=1: thread exiting with uncaught exception (group=0xb1c6db20)
08-26 03:51:24.512: E/AndroidRuntime(2564): FATAL EXCEPTION: main
08-26 03:51:24.512: E/AndroidRuntime(2564): Process: com.google.android.exoplayer.demo, PID: 2564
08-26 03:51:24.512: E/AndroidRuntime(2564): java.lang.NoClassDefFoundError: com.google.android.exoplayer.demo.PlayerActivity
08-26 03:51:24.512: E/AndroidRuntime(2564):     at com.google.android.exoplayer.demo.SampleChooserActivity.onSampleSelected(SampleChooserActivity.java:87)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at com.google.android.exoplayer.demo.SampleChooserActivity.access$0(SampleChooserActivity.java:86)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at com.google.android.exoplayer.demo.SampleChooserActivity$1.onItemClick(SampleChooserActivity.java:80)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.widget.AdapterView.performItemClick(AdapterView.java:299)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.widget.AbsListView.performItemClick(AbsListView.java:1113)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.widget.AbsListView$PerformClick.run(AbsListView.java:2911)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.widget.AbsListView$3.run(AbsListView.java:3645)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.os.Handler.handleCallback(Handler.java:733)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.os.Handler.dispatchMessage(Handler.java:95)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.os.Looper.loop(Looper.java:136)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at android.app.ActivityThread.main(ActivityThread.java:5001)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at java.lang.reflect.Method.invokeNative(Native Method)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at java.lang.reflect.Method.invoke(Method.java:515)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:785)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:601)
08-26 03:51:24.512: E/AndroidRuntime(2564):     at dalvik.system.NativeStart.main(Native Method)
08-26 03:51:26.282: I/Process(2564): Sending signal. PID: 2564 SIG: 9
 This looks like something wrong with your local setup to me. Specifically:

`08-26 03:51:19.932: E/dalvikvm(2564): Could not find class 'com.google.android.exoplayer.demo.PlayerActivity`

That class definitely still exists in the code base. It might be an eclipse quirk. Try cleaning the project, closing and opening it again, then doing a clean build.
 The logs show that the demo app is running (and then crashes), so it must have built and installed successfully. Closing the issue, since it's likely to be a local issue with the reporters environment.
  This is tracked by #151
  Was ExoPlayer really the only thing that changed when you upgraded, or did some of your application code change as well? This is pretty unlikely to be directly related to ExoPlayer, in my opinion.
  I only see it being called once when the player starts. It's likely you're calling seek at a sub-optimal time or something if you're seeing a second parse on player start.
  I don't think it needs a demo app. It would be trivial for someone to swap it into the normal demo app in a couple of minutes, if they want to experiment with it, so I don't think it needs a demo of its own.
 We're finalizing a 1.5.x release at the moment. We'll be merging it after that's done. Sorry for the long delay; perhaps we need more branches :). Thanks!
 1.5.0 is out, so we're pretty much ready to merge this. Our plan is to merge it as-is, since it looks in pretty good shape already and works well. Just FYI, we then plan to make some follow-up modifications, including:
- Fixing code style to be consistent with the rest of the project.
- Force application to inject OkHttpClient instance through all constructors, and remove allowCrossProtocolRedirect/connectTimeout/readTimeoutMillis parameters. Right now if you inject a client, the OkHttpDataSource will change the default properties of that client, which is confusing given it might be used elsewhere. It's probably simplest just to require the application to instantiate, configure and inject a client as appropriate.
 This is now merged, with the modifications described above applied. Please take a look and check that it still needs the original needs. Thanks!
  When you say generated, do you mean it was recorded with the regular camera on the device? What device was it / what version of iOS is it running? Thanks!
 If you could provide a sample file, we'll take a look at this. Either by attaching one here, or emailing olly.exoplayer [at] gmail [dot] com. Thanks!
 We don't have plans to support mov any time soon. In general, tools are better off outputting mp4 if they want their output to work across different devices. "sowt" doesn't appear to be valid in mp4, where I think you'd need to use "twos". We don't support that either right now, primarily because no-one has ever asked for it.

Leaving this marked as enhancement, to support "twos" and "sowt" if it happens to be very little additional work.
 You'd need to:
1. Add `TYPE_sowt = Util.getIntegerCodeForString("sowt");` in `Atom.java`
1. In `AtomParsers.parseAudioSampleEntry` you'll probably see `atomType` being equal to this value when playing the test content. You need to ensure that `out.mediaFormat` is set correctly in this case. I'm not really sure exactly what it should be set to, so that's something you'll have to figure out. It may also be necessary to apply a transformation to the actual media. Again, I'm not sure what's required here.
 This is supported as of version 1.5.8 :) (thanks to @dcower).
  See https://github.com/google/ExoPlayer/issues/565 for reasons why we opted not to do this.
  This is outside the scope of this tracker, which is focused directly on library issues / enhancements. We're not planning on adding any more complexity to our demo app in the near future; we're actually trying to make it simpler. If you need help with application development, I'd suggest asking on StackOverflow. Thanks!
  This is an MP4 file (despite the naming of the url). The root cause is a failure to determine the mime type in AtomParsers.
 We'll merge a fix for this next week.
  - What version of ExoPlayer are you using.
- Does the issue reproduce with the HLS test streams in the ExoPlayer demo app?
 If the issue doesn't reproduce with the test streams, you'll need to provide a stream that we can reproduce the issue with for us to look into it. Thanks.
  We need more information to look at issues like this. Like what version of the library you're using, on what device(s) you see these issues, how reproducible it is on the affected devices, and on different devices. Also, do these issues happen with the test streams in the demo app, or only with specific streams?
 Please clarify what "latest version" means. Which branch are you pulling code from (or which version are you pulling, if you're including it as a prebuilt aar/jar)? Thanks!
 I can reproduce with the test streams provided. I don't know why it's failing though. The underlying decoder is very unhappy, so it's quite possible that the stream contains invalid/malformed H.264. It's also possible that we're somehow parsing it out incorrectly.
 Closing this on the assumption that the media is malformed. We haven't seen this reported by anyone else, which suggests that it's not a widespread issue.
  In what context is this? HLS / DASH / SmoothStreaming / SomethingElse? It's obviously only possible to stream a bitrate that's provided by the server.
 For HLS, it would be possible to allow the user to select from the different qualities that the server makes available, yes. We're working on this currently for SmoothStreaming (done on dev branch) and DASH (will be done on dev branch soon). HLS support will follow later (no immediate ETA).
 Let's use #676 to track this. Closing this one.
  That's a server response indicating that nothing was found at the URL you're requesting. See [here](https://en.wikipedia.org/wiki/HTTP_404). So it's either a serving issue (likely) or an issue with how ExoPlayer is handling your URL (less likely, but possible).

If you think the URL you're seeing this for is valid, please provide the URL (or better - the snippet that you'd copy into Samples.java to add the problematic stream to the demo app) and we can take a look and advise.
 You should avoid putting spaces in your URLs / file names. If you get rid of them, or replace them with underscores as in your working example, you may find that it starts working correctly?
 Yeah, specifically things like:
[http://maxmarshall.ddns.net/MPD/FleetE/seg_The River Fleet_English_FirstRender_641.m4s](http://maxmarshall.ddns.net/MPD/FleetE/seg_The River Fleet_English_FirstRender_641.m4s)

You may find it works if you modify the mpd to contain the spaces in encoded form, like:
http://maxmarshall.ddns.net/MPD/FleetE/seg_The%20River%20Fleet_English_FirstRender_641.m4s

But really, you should just not use spaces. It's easier that way :).
  We're thinking about it; it's not high up the list of things we want to land though. My gut feeling is that we wont want to support this directly. It's not something we want to be encouraging v.s. having services send text format subtitles, and it seems to exist largely for legacy reasons. It also looks like quite a lot of code that we'd have to maintain.

If we don't take it, we should at least provide a way for clients to inject their own support without having to modify the library. This probably means the ability to inject custom stream readers into a TsExtractor instance.
 I mean legacy for streaming. Doesn't your suggested use case require custom hardware and platform modifications? I don't think Android has any APIs for receiving terrestrial broadcasts, although please correct me if I'm wrong.
 In the OTT case I'd expect most providers to be converting subtitles into a more suitable format for OTT delivery. The one case where this doesn't really hold is if you're not the original broadcaster and only have access to the DVT subtitles, in which case I guess you'd need to OCR them, which would be messy. This seems like a relatively niche case though.

I don't know if the Android TV people have plans to expose any new APIs, but I would guess that they wouldn't expose the raw feed in a way that would make ExoPlayer supporting DVB subtitles useful. It feels like a much higher level API would make sense, with the DVB support remaining in the platform.

Agreed that there should be a way to inject custom handlers into TsExtractor. Let's use this bug to track that request.
 In the next push a fix for this will be provided. The API is a bit awkward, yet, due to HLS compatibility requisites, but we are working on it.
 Guys, let me know if you find any inconveniences with the API for reader injection. It should be possible to inject any kind of readers now. Thanks for being patient about it.
 Well, this is the API you should use to implement additional subtitle format extraction from TS. That is, through this API you can provide the readers to the TsExtractor. They will obtain a trackOutput from the extractor output, and that will eventually be passed to the decoders. TsExtractorTest may serve as example.

Depending on the format, you will also have to create a SubtitleDecoder, if no decoder exists (if it exists, you need to pass whatever it needs).

You can have a look at the CeaxxxDecoder's, or the Mp4WebvttDecoder for an example.
 @kiall - https://github.com/google/ExoPlayer/issues/179 is the right place to discuss bitmap based subtitles.  Does this issue affect the Widevine demo content in the demo app? I suggest you file an issue with ASUS if so.
 Chromecasting as in screen mirroring from the device, or proper casting (i.e. Nexus Player is streaming the content and fetching the licenses directly)? If the latter, that suggests to me that there's a content issue. A whole bunch of popular apps do proper casting of Widevine protected content to Nexus Player without issue.
 Following up with Asus contact offline.

Also, can a ADB bugreport be provided?  Thanks,
 Hi, has this issue been reproduced where an ADB Bugreport can be captured? With any of the investigative data it is difficult to troubleshoot. 

If the Lifetab stops after 10secs the device may not be getting a valid license request.
 Is this still an open issue or can we close?
  Thanks. You need to sign the CLA before we can merge (this applies even for 1 character changes; sorry for the inconvenience!). See [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md).
 Thanks!
  Hi,
Does exoplayer support video files which has dolby digital surround audio?
If yes, can you provide sample code snippet (or) which type of AudioRendering to use?
 Extraction of AC-3 samples is supported for HLS and DASH streaming, and when reading samples from MP4 and Matroska containers.

Playback of the extracted samples requires either an AC-3 decoder on the device, or encoded audio passthrough support over HDMI to a separate audio receiver on platform API versions >= L. An app can check whether an AC-3 decoder is present using MediaCodecUtil, and it can check whether passthrough is possible using AudioCapabilities (see the demo app for an example of how to use the latter).
 @quanghuy1288 Please file a new issue including all the information requested in the issue template. The log provided doesn't have enough information to figure out what build the device is running, but the mention of `jb-mr1` in logcat seems supicious; passthrough output requires at least Android Lollipop.  In order of preference:
1. Use DASH or SmoothStreaming instead.
2. Include the CODECS attribute in the HLS master playlist. The player will filter out the audio only variant in this case.
 If I understand correctly, many service providers achieve (1) by transmuxing DASH to HLS at the edge of their networks. It's also not ideal to provide a sub-optimal experience on Android devices due to iOS limitations, although I acknowledge there are practical/cost reasons why you may choose to do this.

Regarding (2), you should include the CODECS and RESOLUTION tags if at all possible in all HLS streams, regardless of the issue being tracked here. Among other things, doing so will enable devices to correctly filter out streams that they're not capable of playing.

We have no short term plan to support seamless switching between audio and audio+video variants. It's something we'd like to support eventually, but it's not going to happen any time soon. When it does arrive, it'll be in the context of supporting dynamic changes to available tracks.
 This is also tracked by #545. Closing this one.
  - If you play an HLS stream where the TS chunks contain SEI information, you should see the data arriving in SeiReader. By default this class skips all SEI data other than EIA608 captions, so you'd probably have to modify that class to pull out the data you're interested in. See [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/ts/SeiReader.java#L64).
- For accessing the decoded frame, you'll probably have to figure out that for yourself, or post a question on stack overflow.
 There is no -user mailing list. It shouldn't be hard for you to look at the source code to look at where the SeiReader is instantiated. You'll likely find there is currently no way to inject your own subclass, but again, it shouldn't be too hard for you to make the modifications necessary to enable this.
  This is tracked by #689. We have a fix that will add support for subt(type) + stpp(codec) support in the next day or so. We won't be supporting wvtt(codec) in the near future though, since it's a non-trivial amount of work.
 Please track using #689.
  This is not supported. Why would you need to DRM protect subtitles?
 I would encourage you to discuss with your client why they think that's a requirement. What are they actually trying to achieve? Why is just sending the subtitle over https not sufficient?
 We haven't seen any other demand for this feature, including from major service providers who are distributing premium content and supporting offline playback. Hence it's not something we'll be looking to support in the near future.
  Probably working as intended due to the reasons above.
 What device are you attempting playback on? Please provide detailed information as listed [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
  Audio only is supported. The problem with the media in this case is that the TS chunks _do_ define a video track, it's just that they don't then include any samples. The TS chunks shouldn't define a video track if they don't contain any video.
 Apologies for the lack of clarity, I got audio and video the wrong way around in my earlier response.

To clarify, the TS chunks in the affected media define _both_ audio and video streams. If the media is video only, as in this case, then it should not define an audio stream and playback will work. Similarly, if the media is audio only, it should not define a video stream and playback will work.

So in conclusion, I think the media is at fault. To get the media to play, you can comment out the `TS_STREAM_TYPE_AAC` case [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/ts/TsExtractor.java#L363), which will result in the audio stream being ignored. To re-iterate, the media should not define an audio stream in the first place, which would make commenting out this block unnecessary.
 Please just fix the source content to not include an audio stream if it doesn't contain any audio. If it's not your content, please ask whoever is producing the content to do this.

If you're adding a boolean there, you're basically saying that you know what's in the media better than the media itself. Which implies that you have to know what its value should be (e.g. through some additional metadata that you have about the content you're playing). If you don't, then I can't see how you could correctly set it.
  That sounds like an issue with application code, not directly related to ExoPlayer. I'd suggest you ask for help on StackOverflow.
  Hi, i got a BufferOverflowException when play a Mpd (h264/aac), i decide to post here because the same device (AllWinners Android Box) with another Mpeg-Dash what is encoded with exactly same way than the "Failed" mpd.

Note: Fails always in exactly same place. (About 11 minutes playback)

**LogCat**

```
08-16 08:59:32.673: I/ExoPlayerImpl(17971): Init 1.3.3
08-16 08:59:32.673: D/EventLogger(17971): start [0]
08-16 08:59:32.683: D/EventLogger(17971): state [Run: 0.00s, false, P]
08-16 08:59:32.683: D/EventLogger(17971): state [Run: 0.01s, true, P]
08-16 08:59:38.773: D/EventLogger(17971): state [Run: 6.10s, false, P]
08-16 08:59:38.773: D/EventLogger(17971): state [Run: 6.10s, true, P]
08-16 08:59:38.793: D/EventLogger(17971): seekRange [ 0, 0, 6576350000]
08-16 08:59:38.793: V/EventLogger(17971): loadStart [Run: 6.12s, 0, 2, mediaStartTimeMs: 0ms, mediaEndTimeMs: 0ms]
08-16 08:59:38.793: D/EventLogger(17971): seekRange [ 0, 0, 6576350000]
08-16 08:59:38.793: V/EventLogger(17971): loadStart [Run: 6.12s, 1, 2, mediaStartTimeMs: 0ms, mediaEndTimeMs: 0ms]
08-16 08:59:38.803: D/EventLogger(17971): state [Run: 6.12s, true, B]
08-16 08:59:39.093: D/EventLogger(17971): bandwidth [Run: 6.42s, 1171, 0.00, 4684000]
08-16 08:59:39.093: V/EventLogger(17971): loadEnd [Run: 6.42s, 0, 303ms]
08-16 08:59:39.103: V/EventLogger(17971): loadStart [Run: 6.42s, 0, 1, mediaStartTimeMs: 0ms, mediaEndTimeMs: 3840ms]
08-16 08:59:39.113: W/SystemClock(17971): time going backwards: prev 3866295752764(ioctl) vs now 3866295189473(ioctl), tid=4444
08-16 08:59:39.123: I/OMXClient(17971): Using client-side OMX mux.
08-16 08:59:39.133: D/EventLogger(17971): bandwidth [Run: 6.45s, 574, 0.01, 4684000]
08-16 08:59:39.133: D/EventLogger(17971): videoFormat [Run: 6.45s, 1, 1]
08-16 08:59:39.153: D/ACodec(17971): manually change eColorFormat to  HAL_PIXEL_FORMAT_YV12!!!!
08-16 08:59:39.193: D/EventLogger(17971): decoderInitialized [Run: 6.52s, OMX.allwinner.video.decoder.avc]
08-16 08:59:39.213: I/OMXClient(17971): Using client-side OMX mux.
08-16 08:59:39.233: V/EventLogger(17971): loadEnd [Run: 6.55s, 1, 433ms]
08-16 08:59:39.233: W/OMXNodeInstance(17971): OMX_EnableAndroidNativeBuffers failed with error -2147479526 (0x8000101a)
08-16 08:59:39.233: V/EventLogger(17971): loadStart [Run: 6.55s, 1, 1, mediaStartTimeMs: 0ms, mediaEndTimeMs: 3993ms]
08-16 08:59:39.233: D/EventLogger(17971): audioFormat [Run: 6.55s, 2, 1]
08-16 08:59:39.233: D/EventLogger(17971): decoderInitialized [Run: 6.56s, OMX.google.aac.decoder]
08-16 08:59:39.473: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 08:59:39.553: D/ACodec(17971): manually change eColorFormat to  HAL_PIXEL_FORMAT_YV12!!!!
08-16 08:59:39.593: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 08:59:39.633: I/SoftAAC2(17971): Reconfiguring decoder: 44100 Hz, 2 channels
08-16 08:59:39.683: D/EventLogger(17971): videoSizeChanged [1280, 520, 1.0]
08-16 08:59:39.703: D/EventLogger(17971): bandwidth [Run: 7.02s, 49489, 0.19, 2040783]
08-16 08:59:39.793: V/EventLogger(17971): loadEnd [Run: 7.12s, 0, 698ms]
08-16 08:59:39.803: V/EventLogger(17971): loadStart [Run: 7.12s, 0, 1, mediaStartTimeMs: 3840ms, mediaEndTimeMs: 7680ms]
08-16 08:59:39.883: D/EventLogger(17971): state [Run: 7.20s, true, R]
08-16 08:59:39.973: D/EventLogger(17971): bandwidth [Run: 7.30s, 50441, 0.28, 2040783]
08-16 08:59:39.993: V/EventLogger(17971): loadEnd [Run: 7.30s, 1, 745ms]
08-16 08:59:40.023: V/EventLogger(17971): loadStart [Run: 7.34s, 1, 1, mediaStartTimeMs: 3993ms, mediaEndTimeMs: 7987ms]
08-16 08:59:40.093: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
[...]
 - About 10 minutes with same Logs bandwidth, loadEnd, loadStart and nothing special...
[...]
08-16 09:08:14.403: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 09:08:14.443: D/EventLogger(17971): bandwidth [Run: 521.76s, 106394, 0.15, 17164500]
08-16 09:08:14.483: V/EventLogger(17971): loadEnd [Run: 521.81s, 1, 495ms]
08-16 09:08:14.533: D/EventLogger(17971): bandwidth [Run: 521.86s, 265447, 0.11, 17692872]
08-16 09:08:14.543: V/EventLogger(17971): loadEnd [Run: 521.86s, 0, 577ms]
08-16 09:08:14.543: V/EventLogger(17971): loadStart [Run: 521.87s, 0, 1, mediaStartTimeMs: 867840ms, mediaEndTimeMs: 871680ms]
08-16 09:08:14.553: V/EventLogger(17971): loadStart [Run: 521.87s, 1, 1, mediaStartTimeMs: 870690ms, mediaEndTimeMs: 874684ms]
08-16 09:08:14.913: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 09:08:14.943: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 09:08:15.093: D/EventLogger(17971): bandwidth [Run: 522.41s, 168873, 0.08, 18013120]
08-16 09:08:15.093: V/EventLogger(17971): loadEnd [Run: 522.42s, 1, 547ms]
08-16 09:08:15.113: D/EventLogger(17971): bandwidth [Run: 522.44s, 169820, 0.09, 18013120]
08-16 09:08:15.133: V/EventLogger(17971): loadEnd [Run: 522.46s, 0, 587ms]
08-16 09:08:15.153: V/EventLogger(17971): loadStart [Run: 522.48s, 0, 1, mediaStartTimeMs: 871680ms, mediaEndTimeMs: 875520ms]
08-16 09:08:15.163: V/EventLogger(17971): loadStart [Run: 522.49s, 1, 1, mediaStartTimeMs: 874684ms, mediaEndTimeMs: 878678ms]
08-16 09:08:15.543: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 09:08:15.613: W/ContainerMediaChunk(17971): Ignoring unexpected seekMap
08-16 09:08:15.683: D/EventLogger(17971): bandwidth [Run: 523.00s, 94084, 0.11, 14300631]
08-16 09:08:15.683: V/EventLogger(17971): loadEnd [Run: 523.00s, 1, 516ms]
08-16 09:08:15.733: D/EventLogger(17971): bandwidth [Run: 523.06s, 263122, 0.08, 18013120]
08-16 09:08:15.753: V/EventLogger(17971): loadEnd [Run: 523.07s, 0, 595ms]
08-16 09:08:30.863: D/EventLogger(17971): droppedFrames [Run: 538.18s, 50]
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971): Internal runtime error.
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971): java.nio.BufferOverflowException
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at java.nio.Buffer.checkPutBounds(Buffer.java:189)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at java.nio.ReadWriteDirectByteBuffer.put(ReadWriteDirectByteBuffer.java:100)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.extractor.RollingSampleBuffer.readData(RollingSampleBuffer.java:286)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.extractor.RollingSampleBuffer.readSample(RollingSampleBuffer.java:192)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.extractor.DefaultTrackOutput.getSample(DefaultTrackOutput.java:140)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.chunk.ChunkSampleSource.readData(ChunkSampleSource.java:258)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:625)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:520)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:416)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:206)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at android.os.Handler.dispatchMessage(Handler.java:95)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at android.os.Looper.loop(Looper.java:137)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at android.os.HandlerThread.run(HandlerThread.java:60)
08-16 09:10:27.363: E/ExoPlayerImplInternal(17971):     at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
08-16 09:10:27.393: E/EventLogger(17971): playerFailed [Run: 654.69]
08-16 09:10:27.393: E/EventLogger(17971): com.google.android.exoplayer.ExoPlaybackException: java.nio.BufferOverflowException
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:239)
08-16 09:10:27.393: E/EventLogger(17971):   at android.os.Handler.dispatchMessage(Handler.java:95)
08-16 09:10:27.393: E/EventLogger(17971):   at android.os.Looper.loop(Looper.java:137)
08-16 09:10:27.393: E/EventLogger(17971):   at android.os.HandlerThread.run(HandlerThread.java:60)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
08-16 09:10:27.393: E/EventLogger(17971): Caused by: java.nio.BufferOverflowException
08-16 09:10:27.393: E/EventLogger(17971):   at java.nio.Buffer.checkPutBounds(Buffer.java:189)
08-16 09:10:27.393: E/EventLogger(17971):   at java.nio.ReadWriteDirectByteBuffer.put(ReadWriteDirectByteBuffer.java:100)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.extractor.RollingSampleBuffer.readData(RollingSampleBuffer.java:286)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.extractor.RollingSampleBuffer.readSample(RollingSampleBuffer.java:192)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.extractor.DefaultTrackOutput.getSample(DefaultTrackOutput.java:140)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.chunk.ChunkSampleSource.readData(ChunkSampleSource.java:258)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:625)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:520)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:416)
08-16 09:10:27.393: E/EventLogger(17971):   at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:206)
08-16 09:10:27.393: E/EventLogger(17971):   ... 4 more
08-16 09:10:27.403: D/EventLogger(17971): droppedFrames [Run: 654.72s, 18]
08-16 09:10:27.433: E/PlayerActivity(17971): playbackState: idle
08-16 09:10:27.433: W/TAG(17971): releasePlayer()
08-16 09:10:27.443: D/EventLogger(17971): end [Run: 654.76s]
08-16 09:10:27.513: D/EventLogger(17971): state [Run: 654.84s, true, I]

```
 I forget to say, i using same Code and same MPD in another device (Xiaomi 4.4) and this works perfect!.
 I found same problem with another MPD in same Allwinner device, in this cade the MPD are Adaptative, the first one aren't.

Do you have any information about that or can help me to find that?

``````
08-17 03:01:00.853: V/EventLogger(819): loadEnd [Run: 23.88s, 1, 648ms]
08-17 03:01:02.023: D/EventLogger(819): droppedFrames [Run: 25.04s, 50]
08-17 03:01:02.083: D/EventLogger(819): bandwidth [Run: 25.11s, 1610050, 1.26, 10198258]
08-17 03:01:02.113: V/EventLogger(819): loadEnd [Run: 25.13s, 0, 1937ms]
08-17 03:01:02.123: V/EventLogger(819): loadStart [Run: 25.15s, 0, 1, mediaStartTimeMs: 38400ms, mediaEndTimeMs: 42240ms]
08-17 03:01:02.143: V/EventLogger(819): loadStart [Run: 25.15s, 1, 1, mediaStartTimeMs: 39890ms, mediaEndTimeMs: 43879ms]
08-17 03:01:02.423: W/ContainerMediaChunk(819): Ignoring unexpected seekMap
08-17 03:01:02.563: W/ContainerMediaChunk(819): Ignoring unexpected seekMap
08-17 03:01:02.843: D/EventLogger(819): bandwidth [Run: 25.86s, 618320, 0.42, 10198258]
08-17 03:01:02.843: V/EventLogger(819): loadEnd [Run: 25.86s, 1, 711ms]
08-17 03:01:05.023: D/EventLogger(819): droppedFrames [Run: 28.05s, 50]
08-17 03:01:06.073: D/EventLogger(819): bandwidth [Run: 29.09s, 3036342, 3.23, 7515698]
08-17 03:01:06.083: V/EventLogger(819): loadEnd [Run: 29.10s, 0, 3958ms]
08-17 03:01:06.113: V/EventLogger(819): loadStart [Run: 29.14s, 0, 1, mediaStartTimeMs: 42240ms, mediaEndTimeMs: 46080ms]
08-17 03:01:06.143: V/EventLogger(819): loadStart [Run: 29.16s, 1, 1, mediaStartTimeMs: 43879ms, mediaEndTimeMs: 47868ms]
08-17 03:01:06.413: W/ContainerMediaChunk(819): Ignoring unexpected seekMap
08-17 03:01:06.503: W/ContainerMediaChunk(819): Ignoring unexpected seekMap
08-17 03:01:06.563: D/EventLogger(819): bandwidth [Run: 29.59s, 327229, 0.16, 7515698]
08-17 03:01:06.603: V/EventLogger(819): loadEnd [Run: 29.63s, 1, 466ms]
08-17 03:01:07.013: D/EventLogger(819): bandwidth [Run: 30.03s, 341521, 0.38, 7515698]
08-17 03:01:07.023: V/EventLogger(819): loadEnd [Run: 30.04s, 0, 908ms]
08-17 03:01:07.023: V/EventLogger(819): loadStart [Run: 30.05s, 0, 1, mediaStartTimeMs: 46080ms, mediaEndTimeMs: 49920ms]
08-17 03:01:07.033: V/EventLogger(819): loadStart [Run: 30.05s, 1, 1, mediaStartTimeMs: 47868ms, mediaEndTimeMs: 51857ms]
08-17 03:01:07.323: W/ContainerMediaChunk(819): Ignoring unexpected seekMap
08-17 03:01:07.473: W/ContainerMediaChunk(819): Ignoring unexpected seekMap
08-17 03:01:07.513: D/EventLogger(819): bandwidth [Run: 30.53s, 379416, 0.19, 16407178]
08-17 03:01:07.533: V/EventLogger(819): loadEnd [Run: 30.55s, 1, 500ms]
08-17 03:01:07.583: D/EventLogger(819): videoFormat [Run: 30.60s, 14, 3]
08-17 03:01:07.753: E/ExoPlayerImplInternal(819): Internal runtime error.
08-17 03:01:07.753: E/ExoPlayerImplInternal(819): java.nio.BufferOverflowException
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at java.nio.Buffer.checkPutBounds(Buffer.java:189)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at java.nio.ReadWriteDirectByteBuffer.put(ReadWriteDirectByteBuffer.java:100)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.extractor.RollingSampleBuffer.readData(RollingSampleBuffer.java:286)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.extractor.RollingSampleBuffer.readSample(RollingSampleBuffer.java:192)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.extractor.DefaultTrackOutput.getSample(DefaultTrackOutput.java:140)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.chunk.ChunkSampleSource.readData(ChunkSampleSource.java:258)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:625)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:521)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:416)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:206)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at android.os.Handler.dispatchMessage(Handler.java:95)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at android.os.Looper.loop(Looper.java:137)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at android.os.HandlerThread.run(HandlerThread.java:60)
08-17 03:01:07.753: E/ExoPlayerImplInternal(819):   at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
08-17 03:01:07.773: E/EventLogger(819): playerFailed [Run: 30.78]
08-17 03:01:07.773: E/EventLogger(819): com.google.android.exoplayer.ExoPlaybackException: java.nio.BufferOverflowException
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:239)
08-17 03:01:07.773: E/EventLogger(819):     at android.os.Handler.dispatchMessage(Handler.java:95)
08-17 03:01:07.773: E/EventLogger(819):     at android.os.Looper.loop(Looper.java:137)
08-17 03:01:07.773: E/EventLogger(819):     at android.os.HandlerThread.run(HandlerThread.java:60)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
08-17 03:01:07.773: E/EventLogger(819): Caused by: java.nio.BufferOverflowException
08-17 03:01:07.773: E/EventLogger(819):     at java.nio.Buffer.checkPutBounds(Buffer.java:189)
08-17 03:01:07.773: E/EventLogger(819):     at java.nio.ReadWriteDirectByteBuffer.put(ReadWriteDirectByteBuffer.java:100)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.extractor.RollingSampleBuffer.readData(RollingSampleBuffer.java:286)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.extractor.RollingSampleBuffer.readSample(RollingSampleBuffer.java:192)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.extractor.DefaultTrackOutput.getSample(DefaultTrackOutput.java:140)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.chunk.ChunkSampleSource.readData(ChunkSampleSource.java:258)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.MediaCodecTrackRenderer.feedInputBuffer(MediaCodecTrackRenderer.java:625)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:521)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:416)
08-17 03:01:07.773: E/EventLogger(819):     at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:206)
08-17 03:01:07.773: E/EventLogger(819):     ... 4 more
08-17 03:01:07.773: D/EventLogger(819): droppedFrames [Run: 30.79s, 29]
08-17 03:01:07.843: E/PlayerActivity(819): playbackState: idle
08-17 03:01:07.863: W/TAG(819): releasePlayer()
08-17 03:01:07.863: D/EventLogger(819): end [Run: 30.88s]
08-17 03:01:08.093: W/MessageQueue(819): Handler (com.google.android.exoplayer.upstream.Loader$LoadTask) {41811b90} sending message to a Handler on a dead thread
08-17 03:01:08.093: W/MessageQueue(819): java.lang.RuntimeException: Handler (com.google.android.exoplayer.upstream.Loader$LoadTask) {41811b90} sending message to a Handler on a dead thread
08-17 03:01:08.093: W/MessageQueue(819):    at android.os.MessageQueue.enqueueMessage(MessageQueue.java:294)
08-17 03:01:08.093: W/MessageQueue(819):    at android.os.Handler.enqueueMessage(Handler.java:618)
08-17 03:01:08.093: W/MessageQueue(819):    at android.os.Handler.sendMessageAtTime(Handler.java:587)
08-17 03:01:08.093: W/MessageQueue(819):    at android.os.Handler.sendMessageDelayed(Handler.java:558)
08-17 03:01:08.093: W/MessageQueue(819):    at android.os.Handler.sendEmptyMessageDelayed(Handler.java:522)
08-17 03:01:08.093: W/MessageQueue(819):    at android.os.Handler.sendEmptyMessage(Handler.java:507)
08-17 03:01:08.093: W/MessageQueue(819):    at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:218)
08-17 03:01:08.093: W/MessageQueue(819):    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:390)
08-17 03:01:08.093: W/MessageQueue(819):    at java.util.concurrent.FutureTask.run(FutureTask.java:234)
08-17 03:01:08.093: W/MessageQueue(819):    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1080)
08-17 03:01:08.093: W/MessageQueue(819):    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:573)
08-17 03:01:08.093: W/MessageQueue(819):    at java.lang.Thread.run(Thread.java:856)
08-17 03:01:08.623: D/EventLogger(819): state [Run: 31.65s, true, I]
08-17 03:01:08.713: D/EventLogger(819): bandwidth [Run: 31.74s, 191136, 0.58, 7189915]```
``````
 For the Acer device at least, this issue can be resolved by explicitly setting a minimum input size in the format used to configure the decoder. We'll merge a change that does this shortly. I have no idea whether this also resolves the issue on other devices. My guess would be that it probably does for physical devices. For the emulator, I'm not so sure.
 Setting maximum input size also gets larger buffers on Nexus 10. I could not reproduce the crashing behaviour mentioned in #616 when doing this.
 This should be fixed for Nexus 10 and Acer Iconia Tab 8. I'm unsure about the unofficial TV boxes. You'll have to attempt to reproduce, since I don't have access to such devices. For the emulator, I have no idea whether it will be fixed or not, but emulator is low priority.
  Thanks for the information. It's probably relatively easy to fix, but you'll need to provide a sample. I assume you're using ExtractorSampleSource for playback? Thanks!
 If it's suitable for public distribution, then a public link to a file on Google Drive / DropBox is best. If not, email privately to olly.exoplayer@gmail.com. Thanks!
 Where did you get the "source file description" from, out of interest?
 @andrewlewis has a fix for this. Will merge soon.
 This should be fixed by https://github.com/google/ExoPlayer/commit/37ebec6e826bac0f09c17e977f7341f76127e2b4. Please let us know if this isn't the case.
 We have a change that fixes the duration.

Note however that this mp3 will not be seekable once the change has gone in, because it has a Xing header that does not include a table of contents, which would usually be used to determine how to seek within the file. To this extent, I think the file provided should be considered abnormal/flawed.
  You could implement your own DrmSessionManager to do this kind of thing. It's not something we're planning to support directly.

I'd be surprised if music services are using (or are required to use) full Widevine/PlayReady DRM, as opposed to simply relying on authentication + HTTPS to secure the data over the wire. Given there's no secure audio path, I'm not really sure what additional benefit full DRM gives you for the streaming case. For the offline case, you'd need to implement your own DrmSessionManager anyway.
  It looks like the platform is missing some functionality that should be present. What device / OS level is this?
 ExoPlayer doesn't support most uses cases prior to API level 16 (Jellybean).
 Unless you have a very specific use case, you shouldn't be using ExoPlayer at all on API levels under 16. There's no sensible way to handle the underlying platform not supporting the player (whatever we do, it'll be equivalent to you just not trying to use ExoPlayer on these platform levels).
 Isn't supported below API level 16. This is documented in the [cons section](https://google.github.io/ExoPlayer/guide.html#pros-and-cons) of the developer guide, and also in the [manifest](https://github.com/google/ExoPlayer/blob/master/library/src/main/AndroidManifest.xml#L28) and [gradle](https://github.com/google/ExoPlayer/blob/master/library/build.gradle#L22) file.
  I don't understand what this is requesting either. ExoPlayer isn't a recording/encoding library.
 That library looks like it has very different goals (encoding + preview from camera). ExoPlayer is a decoding + playback library. MediaRecorder does not provide decoding functionality, so how do you propose we decode on API levels prior to those that make the decoders available from MediaCodec?
 Yes, it's possible to hook a software decoder such as ffmpeg into ExoPlayer to decode on earlier API levels. The [extensions](https://github.com/google/ExoPlayer/tree/master/extensions) directory has two examples for vp9 and opus. We have no plans to support further extensions onto software decoders ourselves, so you'd need to implement your own extension if you wanted to bridge onto something like ffmpeg. It's also unclear what kind of performance you'd be able to achieve with this approach, and you should be aware of ffmpeg's [legal considerations](https://www.ffmpeg.org/legal.html), on which we cannot advise.
  I suggest you report the issue to Lenovo, if it's only occurring on Lenovo devices.
  Please could you provide a link to an affected stream?
 Not enough information. Closing.
 There's a request for a link to an affected stream 13 days ago above with no response. We can't debug issues that we cannot reproduce.
 It's not feasible for us to debug an application for which we do not have source code. Even if we do have source code, it is not an efficient use of time for us to debug third party applications, because it makes it difficult to separate out issues in application code v.s. issues in ExoPlayer.

If HLS playback is broken in ExoPlayer, it should be trivial for you to provide a link to a stream that is broken. If your streams are authenticated, it should be possible for you to expose a non-authenticated test stream that reproduces the issue. If the problem doesn't occur in the demo app, then it's a problem with your application code.
  The issue appears to be with the audio track, specifically with the use of [hasEnoughDataToBeginPlayback](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecAudioTrackRenderer.java#L212). In the case of seeking very close to the end of the video, we only have a small amount of audio that we can play.

We think we have enough audio to start playback, so we say that the track hasn't ended and try and play it. Unfortunately, the underlying AudioTrack doesn't actually play anything, presumably because it doesn't think it has enough data to start playing, so we end up in a state where we're waiting for the AudioTrack to play out the data that we've fed it, but the AudioTrack is waiting for us to feed more data.

I don't think there's a particularly clean solution to this; we'll probably need to implement a fairly hacky workaround to transition directly to STATE_ENDED for this case.
 This should be fixed now. Thanks!
 We made several fixes in 1.5.0 to address this issue. I'm going to close this, because it's probably fixed. Please let us know if you're still able to reproduce, providing all of the relevant information described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). It would probably be best to report as a new issue if so. Thanks!
 Note - The specific change that probably addressed this issue is https://github.com/google/ExoPlayer/commit/e8895c8746487351ededbe0dd072f3fb31468c75.
  Closing due to lack of information. Please see [this](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  There are many types of metadata in many types of media. What's your specific use case (i.e. what format are you playing, what metadata are you expecting)?
 Currently we only parse metadata from HLS streams.
 For HLS we parse ID3 data from the stream. The demo app prints parsed ID3 data to logcat, so it should be fairly easy to trace backwards through the code to work out how it works, given a sample HLS stream containing such data. For MP3/AAC see Ood-Tsen's response above.
  This may well have been a temporary, device specific issue. There's insufficient information to investigate though. If this is still a problem on 1.5.8, please file a fresh issue containing all of the information requested in the issue template.
  Is this for when your activity is paused because it's gone into the background? You should be fully releasing the player in that case, recording the position the playback got to, and then instantiating a fresh player and restoring the position when the activity comes back later.

The alternative requires holding onto a bunch of stuff, including a lot of memory, which isn't being a good citizen when your application isn't in the foreground.
 Being a good citizen means not consuming resources when your application isn't in the foreground. It doesn't mean optimizing your own app's experience at the cost of overall platform health. So it is the right thing to do to be releasing the player. As a point of reference, the YouTube application appears to do this correctly.
  Nope. This is by no means a primary use case; we're more invested in streaming over TCP/HTTP. So you'll likely need to debug the issue yourself.
 Closing as obsolete. If the issue still occurs in version 2.0.4 then please file a new issue containing all of the information requested in our new issue template; thanks!  Nope. This is by no means a primary use case; we're more invested in streaming over TCP/HTTP. So you'll likely need to debug the issue yourself.
 This is probably a duplicate of https://github.com/google/ExoPlayer/issues/1263.  Is this actually a common thing? Do you have any examples? If it's rare then in my opinion it's far better to require the provider to change the way they're serving content, as opposed to having every player need to handle every obscure edge case. The latter tends to lead to all player implementations being bloated and error prone.
 It depends from providers, some "cheap chinese video transcoder" machines have no much options for change that (I know this very good, they are cheap but really poor), they can't change that and GOP come very irregular, i actually got this problem, but opposite to @b95505017 my playback need to decrease and not increase speed, because that i changed ExoplayerLib to don't play last files in PlayList, then now ExoplayerLib play about first files, then if got a GOP problems, so buffer hold they situation until GOP back to normal size (It means my Streaming can play about 8 hours, after that begins to Buffering every time).

I check how VLC handle that, they looks no much different than ExoPlayer, but look they don't play last 3 files in playback, they play the middle of the playlist moreless (About VLC i don't check that deeply).

**Example of irregular GOP when generating HLS from HTTP LiveStreaming:**
10s
10s
10s
10s
1s
1s
1s
1s
1s
1s

Then if ExoPlayer play the last 3 files, means we got 3s buffer, if LiveStream delay 2 seconds to deliver the content, then exoplayer go to last file, and this means no Buffer enough if got another problem, and this begins to buffering every time.

I think this problem are always in "Chinese transcoder", but this affects Exoplayer and of course another players.!.
 - Yes; we should support different variants having different length segments.
- The fix proposed at the top of this issue does not look correct. I'll take a look at a different fix.
 I don't understand the HLS spec; it seems woefully under-specified with respect to how switching between variants should actually work for live streams. Specifically, the RFC states:

```
A client MUST NOT attempt to use the Media Sequence Number to synchronize
between streams - see Section 6.3.2.
```

This is fine. It says that we shouldn't assume that media sequence numbers are the same across different variants, which is what this issue is tracking. Section 6.3.2 then says:

```
Every time a Media Playlist is loaded or reloaded from a Playlist URI, the client
MUST determine the next Media Segment to load, as described in Section 6.3.5,
if it intends to play the presentation normally (i.e., in playlist order at the nominal
playback rate).
```

This is also fine. When we switch variant in a live stream we need to reload the playlist of the newly selected variant in order to discover the latest set of available media segments. So when we do this, we need to use Section 6.3.5 to determine which segment to use. Section 6.3.5 then says:

```
The first segment to load is generally the segment that the client has chosen to play
first - see Section 6.3.3.

In order to play the presentation normally, the next Media Segment to load is the one
with the lowest Media Sequence Number that is greater than the Media Sequence
Number of the last Media Segment loaded.
```

This is where things go wrong. The first paragraph only seems to make sense when starting playback, not when switching variant. The second paragraph doesn't apply, because the first quote above explicitly states that we cannot use the sequence number.

What's actually needed is a way to determine the absolute timing difference between segments in one playlist v.s. segments in another playlist. For on-demand (i.e. not live) streams this can be achieved by calculating the absolute start time of each segment. For live streams, however, this is not possible, because segments are being removed from the start of the playlists, and it's not possible to determine the absolute time that has elapsed prior to the first segment in a given playlist. So it seems it is not possible to determine the absolute timing difference, which makes guaranteeing a seamless switch impossible unless you make further assumptions beyond the specification (e.g. that you can actually use the sequence number).

So, does anyone know how this is actually supposed to work?
 Option 1 is the only thing I can think of that would be guaranteed to work reliably, but it seems pretty unreasonable to me. It's also non-trivial due to PTS timestamp rollover; you could get unlucky! It seems better just to require that different variants have the same segment lengths and indices, as we do currently. Either this requirement really should have been part of the HLS spec in the first place, or at least _something_ should have been put into the HLS spec for solving this issue!

Option 2 doesn't seem viable for the general case.

For Option 3, the RFC states:

```
The client MUST NOT depend upon the correctness or the consistency of the value of
the EXT-X-PROGRAM-DATE-TIME tag; its value is informative.
```

Which isn't particularly helpful.
 I believe this if fixed in 1.5.10 and in 2.x.
 Yes, that's correct.
  - Note that we only fail to handle this correctly if the mp4 container doesn't contain a pasp box. If it does, we set the aspect ratio correctly [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/extractor/mp4/AtomParsers.java#L404). So if this is your own media, you might like to add pasp boxes as appropriate.
- Since pasp boxes are optional, we should do as you suggest and retrieve the information from the sps data (overriding it with the data in the pasp box if one is also present, since the contents of the pasp box is supposed to take priority).
- This is relatively low on our list of things to do. Feel free to contribute a pull request if you want to speed things along. It would probably be appropriate to consolidate SPS parsing [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/util/CodecSpecificDataUtil.java#L195), and call that from both AtomParsers.java and H264Reader.java.
 I have a change for this, just because someone else asked about it and it was easy to do. Will push to dev soon.
  Both samples play fine on my Nexus 5. Is your Nexus 7 the first generation one, or the second? I'll try and reproduce on one of the devices you say is affected next week.

The log line below looks suspect, but I'm not sure whether it's the root cause or not:

```
08-06 10:51:47.591  15231-22557/com.google.android.exoplayer.demo E/ACodec﹕ [OMX.qcom.video.decoder.avc] storeMetaDataInBuffers failed w/ err -2147483648
```
 The most recent update from @b95505017 sounds like attempting to play a 60fps video on a device that's not capable of supporting 60fps. I don't think it's the same as the original issue.
 @b95505017 - We handle variable frame-rates, provided the peak/average rates aren't too high for the device. There is no way to downgrade the frame-rate. There isn't really a good way to "downgrade" the frame-rate. Specifically, it's not possible to decode every other frame (or similar) because decoding one frame typically requires decoding the previous one, because video is encoded to make use of the fact that adjacent frames are similar to one another.

There are keyframes (typically every 2 to 10 seconds) that can be decoded independently, and we could display those and skip the rest. We may consider this for 2.x, but it wont give a good quality experience. You should really deliver content to the device that it's able to play.

We have merged assorted fixes since the original issue was filed. There's not much actionable here, so I'm going to close the issue. Please file a fresh issue is this is still a problem on 1.5.10 or later, including _all_ of the information requested in the issue template. Thanks!
 I believe they skip to keyframes as per my comment above, using a custom modification that they've made to ExoPlayer. Note that mileage of such an approach is quite dependent on how frequent keyframes are in the source media, which is outside of the control of the player. If you own both the serving and consuming sides, as in the YouTube case, then it's a little different. Note also that it's really much better just to play 30fps content if the device can't hack 60fps. What's the point in downloading twice as many frames just so you can throw most of them away?

We're considering adding skipping-to-keyframe support into 2.x, as per my comment above, to deliver a better experience where the source content is high frame-rate and alternatives don't exist.
  We should be filtering out the unsupported stream, yes. Please could you provide a link to some sample media?
 I think this is fixed. Please could you let us know if not.
  It looks like it's repeating a bit of the video as well. If this is the only video for which you're seeing a glitch, and if you don't have sample videos from another provider with similar problems, then I think it's highly likely to be an issue with the content where they've failed to properly align the timestamps in the two variants.

Closing on the assumption it's an issue with the content. Please re-open if you have solid reason to disagree or more examples. Thanks!
 Most of which samples? Are they all from a single provider? It doesn't happen for the samples provided by default in the demo app, right?
 We have quite a lot of test streams from partners (we can't share them externally) and we don't see this issue in general, so it's likely the case that the Weather channel are producing slightly misaligned streams
 I stumbled across a fix for this whilst investigating another issue. The segments are slightly misaligned (i.e. different qualities are being segmented at slightly different offsets), but the underlying presentation timestamps within the media are aligned correctly. We should be using those to achieve proper sync across format switches.
 This should be fixed by the change above. Please verify.
  Sounds like a generic problem with your setup rather than an ExoPlayer specific bug. I'd suggest asking somewhere like StackOverflow.
  I assume that's WebVTT embedded inside of MP4?

We already support embedded TX3G and (I think) TTML. Given we already have a WebVTT parser, it should in theory be straightforward to add support for embedded WebVTT as well. Can you provide a link to some sample media?

Also, feel free to make a pull request to add support yourself, if you figure out how. AtomParsers.parseTrak and AtomParsers.parseStsd is probably where you need to look.
 If you're doing DASH, why don't you list a separate AdaptationSet containing the captions (one representation for each language)? This is already supported.

We don't currently support multiple tracks within fragmented mp4 for DASH, because we expect that providers will provide separate adaptation sets each containing a single type of media (audio/video/text).
 Oh right, sorry, perhaps you're doing something along these lines already. Do the fmp4 representations containing the subtitles only contain subtitles, and nothing else?
 Understood. Although why are you packaging it inside fmp4, rather than just providing the VTT content directly. Normally we'd expect something like [this](http://dash.edgesuite.net/akamai/test/caption_test/ElephantsDream/elephants_dream_480p_heaac5_1.mpd).
 For live video you'd normally provide segments, each ~5 seconds in length (or similar), just like for audio and video in DASH. For text, each segment would be a complete VTT file covering that period of time.
 Sure, we'll take a look. Note that including VTT directly is also standard compliant. I suspect it's identical except without the container - which probably doesn't add any value given it's just wrapping the VTT. If so, it's probably nicer to deliver without the container if possible.
 I've been taking a look at this. Do you think it's correct that the mimeType in the manifest is video/mp4? I would have thought it should be text/mp4, which would be analogous to the audio representations having type audio/mp4.
 - Thanks for the detailed analysis! I agree that application/mp4 appears to be the correct thing to use (this is also specified in RFC 4337 itself, which says to use application if the file doesn't contain audio or video).
- I'm not certain, but I think we may well already support ISO BMFF encapsulated TTML, with the caveat that the track selection logic will need to be adjusted to detect the combination of mimeType="application/mp4" and codecs="stpp" as a text track. I believe I've seen examples of SmoothStreaming with TTML embedded in MP4 that we've been able to handle correctly. The streams in the DASH case should be the same.
- The [DASH DVB doc](http://www.dvb.org/resources/public/standards/a168_dvb-dash.pdf) that I'm looking at lists TTML as the only supported format. The newer doc that you reference appears to go into much more detail, but I don't see any wvtt specific documentation there. Is there a reason you're not using TTML? Do the tools you're using support output in TTML format? If they do, could you provide a sample with TTML captions instead? It's probably going to be much easier for us to support them in the short term, since they probably very nearly work already.
- To support the wvtt sample you've attached we need to (1) tweak the track selection logic in the same was as we'll need to do for TTML, (2) look for the wvtt atom in AtomParsers.parseStsd and create the appropriate MediaFormat, and (3) parse wvtt samples. Annoyingly, they're not just normal WebVTT. They're a binary representation of WebVTT as defined by 14496-30, so separate parsing code will be necessary.
 The change above adds support for TTML captions embedded as per 14496-30. Embedded WebVTT  per 14496-30 uses a different format than regular WebVTT, and so will require a new SubtitleParser.
 Update title to reflect the fact we support the TTML part of 14496-30.
  ExoPlayer doesn't implement your application UI for you ;). UI questions are out of scope of this tracker, unless directly related to video surfaces. I suggest you post on StackOverflow.
  Most Android devices don't support AC3. What device are you attempting playback on?

@andrewlewis - Should we be failing in this way? I would have thought we should just be ignoring the track?
 For ExtractorSampleSource we don't currently filter out AC-3 tracks based on AudioCapabilities, so this is failing when it tries to set up the AudioTrack for encoded audio passthrough.

After the planned change to check AudioCapabilities in the track renderer, this will either work correctly (if the device has an AC-3 codec) or fail with a decoder query exception (if it doesn't).
 AC-3 in matroska files should work correctly since b2206866f0eecc52af53681ec340f61ba7d38d2a. Playback is either via HDMI passthrough (if a receiver is connected; note that this feature is buggy on the ADT-1 development device) or with decoding on the device (if the required codec is present). In other cases, playback will fail when trying to create a decoder.
  If you Google "DexIndexOverflowException" then you'll find a lot of information about this. It's not a problem specifically related to ExoPlayer. See: https://developer.android.com/tools/building/multidex.html
  I wouldn't be surprised if many devices running 4.1.1 do support main profile, with the exception of low end devices. Are you sure this isn't the case? Note that it's not yet become a hard requirement for Android devices to support main profile in general, so there's no guarantee that you wont find a low end device running Lollypop that also lacks main profile support. I would expect all non-low-end devices to support it, however.

As for your specific question, yes it is possible to do this. The [VP9 extension](https://github.com/google/ExoPlayer/tree/master/extensions/vp9) shows how to bridge onto a VP9 software decoder. You could attempt something similar to bridge onto a S/W H264 decoder, if you wanted to do so. I can't say anything about what kind of performance this approach would give you; that's something you'd need to find out.
  I'm pretty sure you can't do this, because okhttp (which is used under HttpUrlConnection on more recent Android versions) will strip the Content-Encoding header when it decompresses the response. See the source code [here](https://github.com/square/okhttp/blob/master/okhttp%2Fsrc%2Fmain%2Fjava%2Fcom%2Fsquareup%2Fokhttp%2Finternal%2Fhttp%2FHttpEngine.java#L627).
 If this is necessary, one option is to explicitly add the Accept-Encoding gzip header ourselves, in which case okhttp wont transparently decompress the response or strip the header. DefaultHttpDataSource would then be responsible for handling the decompression itself, which would add additional complexity. What problem is this actually fixing, if any? I think we'd rather not do that unless there's an actual issue that requires us to do so.
 On that version of okhttp isContentGzipped will always return false. You will then get false positives because the Content-Range header isn't stripped, which getContentLength uses if Content-Length is missing (this is required, I think due to proxies that some mobile carriers use that strip Content-Length). If I remember correctly, there are also some versions of the Android network stack that don't strip Content-Length for this case.

Why don't you modify your custom component to not rely on the content length being provided? In general components should be able to cope with this case anyway (although admittedly, if you know your server's behavior then you can get away without). Also, playlist files are quite amenable to gzip, I think, so I'd guess that you're losing out by not compressing them.
 It's common for DataSource implementations to read fewer bytes than are requested (but each call will always read at least 1 byte, to guarantee progress is made). See the Javadoc for that method for details. So you need to wait until you see END_OF_INPUT. This is the same as how Java InputStream objects work.
  I don't think the ADT-1 is particularly well supported, what with it only being a developer device. I suggest you try to reproduce on a Nexus Player instead.
  Start time doesn't really depend on DefaultAllocator values. Leaving the segment size at around 64K as in the demo app is probably sensible. You could reduce the number of segments (and hence the total buffer size) to reduce the memory allocation.

You need to be careful though; some MP4 files aren't particularly well interleaved, meaning there might be a few MBs containing only audio, then a few MBs containing only video and so on. If you make the buffer too small to handle this then I imagine you'll run into problems (possibly stalled playbacks).
 If you know exactly what's being played then experiment to find the smallest value that doesn't cause any issues, and then add a bit of a margin. If you don't know exactly what's being played, or if you can't test all content, then you might want to make the margin a little bigger. You'll have to decide for yourself what you consider a safe margin to be. There are no theoretical right answers to the question, since it depends on the exact way that the content has been encoded, and there aren't (useful) theoretical upper bounds.

Individual allocations are made as required by DefaultAllocator. initialAllocationCount can be used to allocate a number of them up front as a single memory allocation. This approach can provide better performance than making each allocation individually when running on Dalvik and when you need a large buffer. For small buffers it probably doesn't make any difference.
  This doesn't sound related to ExoPlayer. Sounds like a device or application issue.
 Yeah, seems likely that the ACodec error is unrelated to the issue.
 Closing. Likely to be device or application specific.
  Some relation with that: #488 
 Can you show how you do it?

When I try use exoplayer, video still black, but I using HLS and DASH!
 That awesome, let me try :point_up_2: 
 Before screen are black, now have video, but still freezing, video are not 100% smooth!

Do you know your Android Version And or Amlogic Build version?

Thanks.
 @ddv2005 Finally can i say software decoder have no quality for this Hardware, please see more in #488 
 Does it work if you make it so that `codecNeedsEndOfStreamWorkaround` in MediaCodecTrackRenderer returns true for the video decoder? If so then we should just add the device/decoder combination to that method.
 Hi @ojw28, please check #488 just check last message, explain all about OMX.amlogic
 That doesn't answer the question, because video shouldn't freeze even if the decoder isn't adaptive. Please give the suggestion above a try.
 I see! Then @ddv2005 should reply that, I using same decoder with MPEG-TS and no Freezing =/

@ddv2005 Maybe if you can publish some link/file i can try in my Amlogic (Old one) and check if same problem.

Just disable Omx.Amlogic, i think this not useful in all devices, like mine device, just have Omx.Amlogic and software Omx.Google (Then will be worst with this solution).

My issue was just with MPEG-Dash adaptive (Then in this case video show black screen), but now AmLogic staff solved that for me and they will publish the new firmware for public very soon.
 Report the issue to the device manufacturer. There's only so much we can do to make sure we run properly on devices that don't behave correctly.
 It's somewhat unclear whether this is/was a device specific issue, or whether it still reproduces on recent versions of ExoPlayer. Closing for now. If this still reproduces on the latest ExoPlayer release, and if you believe it's not a device specific issue, then please file a new issue.
  What renderers are you using to target API levels under 14 anyway, given that the standard audio/video renderers require API level 16+? Note that only 4% of active Android devices are running API level 14 and below, and only 7% of active Android devices are running API level 15 and below.
 We've made one minor improvement in `dev-v2`, which is to enable fast switching on API level 23 and above in the case of switching directly from one surface to another (i.e. not with a gap where there's no surface in between). Of course this is far away from being a complete solution, but it might be useful to some people. The relevant change is here: https://github.com/google/ExoPlayer/commit/a9617af29c2c760c8e439d390ef02f0b4e055a6b It's possible to implement a "dummy surface" that can be attached during the period where you otherwise wouldn't have one. It's possible to use this approach for non DRM'd content and DRM'd content that doesn't require a secure output path in its license policy. For DRM'd content that does require a secure output path, it's possible to use this approach only if the device supports `EGL_EXT_protected_content`, as otherwise the dummy surface isn't sufficiently secure.

We're aiming to push a change that adds a `DummySurface` class fairly soon. We may later automatically attach it when possible inside of `MediaCodecVideoRenderer`. Remaining work to do here is as follows:
- Investigate hooking `DummySurface` up internally where possible.
- Don't allow `MediaCodecVideoTrackRenderer` to decode/render a key-frame too far ahead of the current playback position. This causes bad behavior if the surface is repeatedly cleared and set, as described in https://github.com/google/ExoPlayer/issues/2703. I implemented the workaround suggested in #2703 for Devices < 23:

```kotlin
class SurfaceManagerApi17(private val player: SimpleExoPlayer, private val trackSelector: DefaultTrackSelector) : SurfaceManager {

  override fun surfaceCreated(holder: SurfaceHolder) {
    player.setVideoSurface(holder.surface)
    videoRendererIndex()?.let {
      trackSelector.setRendererDisabled(it, false)
    }
  }

  override fun surfaceChanged(holder: SurfaceHolder, format: Int, width: Int, height: Int) {}

  override fun surfaceDestroyed(holder: SurfaceHolder) {
    player.setVideoSurface(null)
    videoRendererIndex()?.let {
      trackSelector.setRendererDisabled(it, true)
    }
  }

  private fun videoRendererIndex() = (0 until player.rendererCount)
      .firstOrNull {
        player.getRendererType(it) == C.TRACK_TYPE_VIDEO
      }

  override fun release() {
    player.setVideoSurface(null)
  }
}
```

That works fine when the surface gets destroyed but when it gets recreated, the audio pauses for a short time which is very disturbing.

Is it possible to prevent that? @molexx - In the `dev-v2` branch we now wire `DummySurface` up for you automatically where possible inside of `MediaCodecVideoRenderer`, so you'll be able to remove your manual wiring once that hits a `release-v2` (or if you're using `dev-v2` directly).  Full multi-track support should make this easier, because once we have that it should be relatively easy to have the video renderer advertise multiple tracks, like: AUTO,1080,720,480 (or whatever).

YouTube use DASH, where track selection is supported (albeit not in a particularly easy way) using MultiTrackChunkSource. SmoothStreaming playbacks can also make use of MultiTrackChunkSource. Once we have proper multi-track support this class will probably go away.
 It's now possible to select between the available variants in ExoPlayer. The demo app will also list the available variants and allows selection between them (during playback tap the screen and press the "Video" button to see the available variants, or the "Audio" button if your playing audio only).
 Note that this is on the dev branch (and will make it into the next release).
 We support adaptive mode where we'll seamlessly switch based on the available bandwidth, and we now also support manual selection. We do not support seamless manual selection.
 Thanks for your observations.
- I see the first issue, and we'll take a look and fix that.
- We don't have current plans to support the second It's pretty non-trivial. Consider the case where the player has buffered a large amount of some manually selected low quality stream. If auto is then selected, it may take so long to switch up that the selection appears to have not done anything. To mitigate that you can proactively throw away some of the buffer containing the low quality stream, but then things start getting complicated from an implementation perspective. It's also debatable how useful the feature would be. If someone has switched from auto to a manual selection then presumably they did so because auto wasn't working well for them. So presumably switching back to auto again is something few people would choose to do.
- We have a longer term plan to do something that achieves more or less the same thing as in your third bullet point. Basically, we'll at some point allow selection of a subset of streams that should be considered in auto mode.
 There's a separate issue tracking switching in Live (#1174), so let's use that for tracking.
 No immediate plans, no.
 In ExoPlayer V2, when it's done, HLS will use a `FormatEvaluator` to adapt between the variants during playback, as is already the case for DASH and SmoothStreaming. At that point you'll be able to implement this functionality using a custom `FormatEvaluator` implementation. For now in V1, you'd need to look at modifying `HlsChunkSource`.
 All track selection is converged into TrackSelector and TrackSelection in V2, and works in the same way for DASH, SmoothStreaming and HLS. The demo app allows you to select from the available streams in all cases. I suggest you take a look at the trackselection package if you're interested in the internal details. You're doing the right thing. There's a known issue specifically for HLS where the initial selection isn't being applied correctly. This is tracked by https://github.com/google/ExoPlayer/issues/2353.  6 second chunks should work fine with ExoPlayer's adaptive bitrate algorithm, without the need for hacks. Any re-buffering that you are seeing is unlikely to be a direct result of this choice of chunk size.
  Can you reproduce this on any devices not manufactured by Xiaomi? If not, it sounds like a Mi4 specific issue, and you should report it to them instead.
 Does the issue occur in the ExoPlayer demo app (without modification) just by dragging he seek bar (which will cause repeated seek calls to be made)? If so, does it occur on all of the demo app's test streams?

Given we haven't seen this reported at all until now, it seems unlikely to be a widespread issue. Which suggests it's either specific to the devices that you happen to have, or it's a result of the specific media you're trying to play, or you've made some modifications to the player that are causing the issue.
 Closing due to insufficient information.
 - I agree that you just shouldn't do this (i.e. do not continually call seekTo whilst the user is dragging the bar). It's better (and still feels natural) to pause playback when the user starts dragging, then call seekTo and resume playback when the user stops dragging.
- The demo app does continually call seekTo whilst the user is dragging (provided the user's finger is moving). This is just because that's what the platform's MediaPlayerControl does, not because we actually think it's a good idea. We haven't had any reports of people being able to crash the demo app, which suggests to me that you still have some issue in your own code.
  We don't support seeking in the DVR window currently. This is tracked by https://github.com/google/ExoPlayer/issues/291.
  Hi,
I am getting exception and complete logs available here http://pastebin.com/AQDq7Czr .
Please provide your inputs.
Thanks in advance.
 Based on 1.4.1
 If you attach a debugger and trace the code back to see where that array comes from, it's probably possible to diagnose this yourself?

In any case, I suspect you'll find that when HlsRendererBuilder calls selectVideoFormatsForDefaultDisplay, the array it gets back is of length zero. Which means that we don't think any of the variants in the master playlist can be played on the device. If you attach a debugger and walk through what happens in that call, you can probably look to see why we think that, and then whether it's actually right or whether it's doing something wrong.

We should also make the "no supported streams" case fail in a more obvious way, instead of this random error. I'll do that.
 The change above will make the failure occur directly in HlsRendererBuilder, if the above diagnosis is correct. You'll still need to debug what's going wrong in selectVideoFormatsForDefaultDisplay yourself though.
 No, it is happening on actual device , not emulator.
I am yet to remote debug this issue. I believe it is related to display size instead of video format.
 Closing due to inactivity. It's unclear whether this is still an problem. Please file a new issue containing all of the information requested in the issue template (including a full bug report and links to content that reproduces the issue) if you still see this using ExoPlayer 1.5.8 or later. Thanks!
  Given you're using custom components, you'll have to figure this out yourself (or get some help on stackoverflow or something). This tracker is for bugs in the library.
  Normally the adaptive decoder will be the first one in the list anyway. The exception to the rule would be if a H/W decoder wasn't adaptive but the S/W decoder was, however in this case note that the S/W decoder is probably significantly less powerful and/or will drain significantly more battery.

What's the actual use case here, and on which devices and platform levels is it relevant?
 The latest version of Android's compatibility test suite requires that all decoders for AVC/HEVC/VP8/VP9 must be adaptive. So it sounds like your device would fail the suite and not be classed as a certified Android device. The test in question can be found [here](https://android.googlesource.com/platform/cts/+/99b52fbea97cd8a8c95339dc1ff1681b934b1610/tests/tests/media/src/android/media/cts/MediaCodecCapabilitiesTest.java) at line 401. This test is in part to prevent strange scenarios such as the one you describe. We have no plans to support this kind of configuration.
  "Unable to connect to htt4" suggests you're connecting to a bad URL, presumably? You'll need to figure out where those URLs are coming from (it's likely that they originate either from the manifest or from the license server response).
  This is almost certainly the same as #464, which is for rk31sdk. What [SDK_INT](http://developer.android.com/reference/android/os/Build.VERSION.html#SDK_INT) value does the device report?
 The change above is merged on the dev branch, and probably fixes this issue. We don't have a device of this type to test with, so please give it a try. If it's still broken, please let us know, along with the SDK_INT value as requested above. Thanks!
  This was fixed in https://github.com/google/ExoPlayer/commit/4c1fb0c9772fa76dc6e7695ee904a337cc56167c.
  The problem is most likely because Aes128DataSource doesn't support reading from arbitrary positions in the stream. I think it should be moved to be private inside of TsChunk, given it's limited use case. I'll do that.

That aside, this use case isn't directly supported. If you can make a DataSource that will return correctly decrypted data at the offsets requested, then ExoPlayer will "just work". That's something you'll need to figure out how to do by yourself, however.
  See the referenced issue. Upgrading these to match the versions you happen to use will presumably fix things for you and break things for someone else. The dependencies just shouldn't exist at all, I think.
 I don't think we have a need to update these versions. The libs are I think still required for people using Eclipse, as is the project file. Please comment if you disagree :). Thanks!
  @Mackqrenc, please say what exactly device/model/android version, and if possible put here some logcat log and if possible also a m3u8 online file.

(I got some problems like this, but depends the device)
 That looking means the connection is timing out. It's most likely a provider issue, I see the same behaviour if I try and play the stream in Safari on a Mac.
 If you see in that second picture, you will see "java.net.SocketTimeoutException", this is no difference than before,

For sure your internet connection or server connection have several problems, losing data or something like this.

Try to fix, later will be good.
  When you're building your ExtractorSampleSource, I think you're passing 1 as the requested buffer size, and BUFFER_SIZE as the requested retry count. A buffer of 1 byte is obviously too small, which is why you end up with the error. The arguments to this constructor changed in an unfortunate way between releases, so that might be why this has happened, sorry!

Note that the constructor you're calling is also deprecated. Please update to use one of the non-deprecated ones, and use the demo app as a guide for how to do so. This should hopefully resolve your problem!
 @eudson Please file a new issue with full steps to reproduce via the demo app, including a link to the media you're trying to play. Thanks.
  It can't, it's a typo. Will push a fix. Thanks!
  What exactly do you see in the demo app? I'm not sure where you've added logging. Are you seeing an incorrect height reported to PlayerActivity.onVideoSizeChanged (540 instead of 1080)?

Is this a problem on multiple devices with that resolution, or just a single device? I suspect it's more likely to be a device specific than a problem that's caused by the size of the screen itself. Given the device only has a 480p screen, I also wouldn't be at all surprised if the hardware isn't really powerful enough to play 1080p content. What device is it?
 I don't know what widthHeightRatio=3.5555556 is in this context, because I don't know where you've added logging. Where does that come from?
 Right, got it. So the only thing wrong is that the height is half what it should be. If it were correct, then the widthHeightRatio would be correct too.
 Please try adding logging to MediaCodecVideoTrackRenderer.onOutputFormatChanged. Specifically, if you could get the values of KEY_WIDTH, KEY_HEIGHT, and (if they exist) the values of the four KEY_CROP_X entries.

Please also add logging to MediaCodecVideoTrackRenderer.onInputFormatChanged to print holder.format.width, holder.format.height and holder.format.pixelWidthHeightRatio.

My guess would be that the decoder is downscaling the video by a factor of 2 in the vertical direction because these devices can't support 1080p buffers. The results of the suggested debugging above should help to validate this theory.
 It looks to me like the decoder just chooses to decode it into a half height texture, most likely due to memory limitations of the device.
- If that's the only problem, you could just change MediaCodecVideoTrackRenderer to take the width and height from the input format instead (i.e. have it set currentWidth, currentHeight and currentPixelWidthHeightRatio in onInputFormatChanged rather than in onOutputFormatChanged). That should result in the correct aspect ratio being reported, and hence the surface being sized correctly. You could also implement any other workaround at all that causes sensible values to be sent to onVideoSizeChanged (e.g. if you know the dimensions from somewhere else).
- As an aside, decoding 1080p content on a 480p device needlessly drains battery and (if streamed over the network) data. Your users will get a much better experience if you give these devices something nearer to 480p video, which likely also solves this problem because I suspect the decoder will not render smaller videos in this way.
  I see that issue #614 was closed as a content issue. If this is a dupe as stated in the original report, I will close. 

If any participant feels differently please provide a full log, not filter and including system messages, and  an ADB bug report. 

Thanks.
 Thank you for the follow up. will close the issue now.
  As a related point, we're planning on making `seekTo(player.getCurrentPosition())` be a no-op (i.e. "do nothing").
  I doing several tests in HLS, and don't know why, when i tried to open a m3u8 online content, this do this fail! I just save this for put here and then restart my ffmpeg command who generate m3u8 and then play perfect. (I try this m3u8 with VLC and playback perfect)

```
07-24 01:45:48.054: E/LoadTask(10905): Unexpected exception loading stream
07-24 01:45:48.054: E/LoadTask(10905): java.lang.NumberFormatException: Invalid int: "2858726044"
07-24 01:45:48.054: E/LoadTask(10905):  at java.lang.Integer.invalidInt(Integer.java:137)
07-24 01:45:48.054: E/LoadTask(10905):  at java.lang.Integer.parse(Integer.java:377)
07-24 01:45:48.054: E/LoadTask(10905):  at java.lang.Integer.parseInt(Integer.java:365)
07-24 01:45:48.054: E/LoadTask(10905):  at java.lang.Integer.parseInt(Integer.java:331)
07-24 01:45:48.054: E/LoadTask(10905):  at com.google.android.exoplayer.hls.HlsParserUtil.parseIntAttr(HlsParserUtil.java:44)
07-24 01:45:48.054: E/LoadTask(10905):  at com.google.android.exoplayer.hls.HlsPlaylistParser.parseMediaPlaylist(HlsPlaylistParser.java:225)
07-24 01:45:48.054: E/LoadTask(10905):  at com.google.android.exoplayer.hls.HlsPlaylistParser.parse(HlsPlaylistParser.java:127)
07-24 01:45:48.054: E/LoadTask(10905):  at com.google.android.exoplayer.hls.HlsPlaylistParser.parse(HlsPlaylistParser.java:1)
07-24 01:45:48.054: E/LoadTask(10905):  at com.google.android.exoplayer.upstream.UriLoadable.load(UriLoadable.java:93)
07-24 01:45:48.054: E/LoadTask(10905):  at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:209)
07-24 01:45:48.054: E/LoadTask(10905):  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:422)
07-24 01:45:48.054: E/LoadTask(10905):  at java.util.concurrent.FutureTask.run(FutureTask.java:237)
07-24 01:45:48.054: E/LoadTask(10905):  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112)
07-24 01:45:48.054: E/LoadTask(10905):  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587)
07-24 01:45:48.054: E/LoadTask(10905):  at java.lang.Thread.run(Thread.java:841)
07-24 01:45:48.054: E/DemoPlayer(10905): onRenderersError: Unexpected NumberFormatException: Invalid int: "2858726044"
07-24 01:45:48.054: E/EventLogger(10905): internalError [6.20, rendererInitError]
07-24 01:45:48.054: E/EventLogger(10905): com.google.android.exoplayer.upstream.Loader$UnexpectedLoaderException: Unexpected NumberFormatException: Invalid int: "2858726044"
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:222)
07-24 01:45:48.054: E/EventLogger(10905):   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:422)
07-24 01:45:48.054: E/EventLogger(10905):   at java.util.concurrent.FutureTask.run(FutureTask.java:237)
07-24 01:45:48.054: E/EventLogger(10905):   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112)
07-24 01:45:48.054: E/EventLogger(10905):   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587)
07-24 01:45:48.054: E/EventLogger(10905):   at java.lang.Thread.run(Thread.java:841)
07-24 01:45:48.054: E/EventLogger(10905): Caused by: java.lang.NumberFormatException: Invalid int: "2858726044"
07-24 01:45:48.054: E/EventLogger(10905):   at java.lang.Integer.invalidInt(Integer.java:137)
07-24 01:45:48.054: E/EventLogger(10905):   at java.lang.Integer.parse(Integer.java:377)
07-24 01:45:48.054: E/EventLogger(10905):   at java.lang.Integer.parseInt(Integer.java:365)
07-24 01:45:48.054: E/EventLogger(10905):   at java.lang.Integer.parseInt(Integer.java:331)
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsParserUtil.parseIntAttr(HlsParserUtil.java:44)
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsPlaylistParser.parseMediaPlaylist(HlsPlaylistParser.java:225)
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsPlaylistParser.parse(HlsPlaylistParser.java:127)
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsPlaylistParser.parse(HlsPlaylistParser.java:1)
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.upstream.UriLoadable.load(UriLoadable.java:93)
07-24 01:45:48.054: E/EventLogger(10905):   at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:209)
07-24 01:45:48.054: E/EventLogger(10905):   ... 5 more
07-24 01:45:48.064: E/EventLogger(10905): playerFailed [6.20]
07-24 01:45:48.064: E/EventLogger(10905): com.google.android.exoplayer.upstream.Loader$UnexpectedLoaderException: Unexpected NumberFormatException: Invalid int: "2858726044"
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:222)
07-24 01:45:48.064: E/EventLogger(10905):   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:422)
07-24 01:45:48.064: E/EventLogger(10905):   at java.util.concurrent.FutureTask.run(FutureTask.java:237)
07-24 01:45:48.064: E/EventLogger(10905):   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112)
07-24 01:45:48.064: E/EventLogger(10905):   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587)
07-24 01:45:48.064: E/EventLogger(10905):   at java.lang.Thread.run(Thread.java:841)
07-24 01:45:48.064: E/EventLogger(10905): Caused by: java.lang.NumberFormatException: Invalid int: "2858726044"
07-24 01:45:48.064: E/EventLogger(10905):   at java.lang.Integer.invalidInt(Integer.java:137)
07-24 01:45:48.064: E/EventLogger(10905):   at java.lang.Integer.parse(Integer.java:377)
07-24 01:45:48.064: E/EventLogger(10905):   at java.lang.Integer.parseInt(Integer.java:365)
07-24 01:45:48.064: E/EventLogger(10905):   at java.lang.Integer.parseInt(Integer.java:331)
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsParserUtil.parseIntAttr(HlsParserUtil.java:44)
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsPlaylistParser.parseMediaPlaylist(HlsPlaylistParser.java:225)
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsPlaylistParser.parse(HlsPlaylistParser.java:127)
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.hls.HlsPlaylistParser.parse(HlsPlaylistParser.java:1)
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.upstream.UriLoadable.load(UriLoadable.java:93)
07-24 01:45:48.064: E/EventLogger(10905):   at com.google.android.exoplayer.upstream.Loader$LoadTask.run(Loader.java:209)
07-24 01:45:48.064: E/EventLogger(10905):   ... 5 more
07-24 01:45:48.064: E/PlayerActivity(10905): playbackState: idle
```

The m3u8 manifest.
![captura de tela 2015-07-24 as 01 46 38](https://cloud.githubusercontent.com/assets/9998303/8867878/05913dc2-31a6-11e5-833e-1ac78ce0db4a.png)
 I see, i think this should have a fix.
 I agree the content should use a more sensible value for the media sequence number. I can't think of a legitimate reason they need to use numbers that big. The normal 2^31 range is sufficient for 228 years with your segment duration.
 Note - We would fix this if it were trivial, but it requires changing DASH/SmoothStreaming code to use longs too due to the pieces of code that they share, and I think we'd then end up having to hackily convert longs back to integers in a bunch of places. Hence marking wontfix for now.
 You right, if sequence begins from 0 (0,1,2,3....) its not a problem, but when we think the server got a CDN implementation (Many .ts files is cached in CDN), always need change the file name of the new execution of HLS (Sometimes LiveStreaming go down and when restarts need different .TS filename). For that on ffmpeg we do it like this:

> -start_number $(date +"%d%m%y")$(( ( RANDOM % 100 )

It will give us a sequence like that:

> 020715991
> 020715992
> 020715993

Its looking good for ExoPlayer, perhaps if put just a $(( ( RANDOM % 100000000 ) sometimes got this Player fails.

I understand my solution are medieval, but helps with CDN implementation!
 You definitely shouldn't do that. It doesn't guarantee unique values when the server restarts anyway (if you use %100, then it's almost certain to not do this).

I'm not sure what ffmpeg ends up doing with the start_number, but the fact the numbers in the filenames are either negative or decreasing suggests to me that ffmpeg doesn't particularly like it either. You'll have to figure this out mostly by yourself, but a few random thoughts:
- The -i option for ffmpeg looks useful. You could have start_number increment from 0 each time and use -i to bake something unique into the fixed part of the file-name for each run.
- You shouldn't need to use random numbers. Just take the current time since epoch in milliseconds, or something like that, and bake it into the file-names.
 You right, that i doing with 

> $(date +"%d%m%y")
> Before $RANDOM, this give me Day/Month/Year, and later 100 random int!

It solve my problem actually DayMonthYearRandom!

Thank you very much!.
  The issue is that FragmentedMp4Extractor assumes samples are packed tightly into their MDAT boxes. In this particular stream, it looks like each MDAT box starts with 8 bytes of non-sample data. We try and read this data as the start of the first sample, which leads to the failure.

We do have a patch internally that handles this case, but we didn't ever submit it because we were unsure whether it was worth supporting, given it's pretty messy to do so. This is the second sample we've seen that adds extra data into the MDAT boxes, so I guess we should probably clean the patch up and get it supported.

Out of interest, do you know what those 8 extra bytes at the start of each MDAT box are for? Is there a legitimate reason for them?
 Ah right, so this is actually a slightly different issue. Thanks for the info. Yes, we should handle that case. 

Note however that in fragmented mp4 you probably have a new mdat box for every segment anyway, and they're not nested inside any other parent boxes, so I don't think it's ever going to be the case that boxes get remotely near to being 4GB. In other words, you can probably just safely not use largesize at all.
 And if the above is correct, you explicitly are a little better off not using largesize (because it wastes a small number of bytes for every box, and because in general it's a good idea to "do what everyone else does").
 largesize should also now work on the dev branch (although I'd recommend not using it).
  Presumably that's the only actual problem. If the library didn't have any dependencies then there wouldn't be a warning, right? Only the tests should have those dependencies, and they should be able to use whatever version of mockito (etc) that they want, including the versions they use currently.

Does that sound right?
 The pom issue was probably caused by https://github.com/novoda/bintray-release/issues/31 (and can be fixed by updating). There's also some confusion in the gradle file around androidTest and test, which is causing some problems.
 I've manually correct the pom files for 1.4.0 and 1.4.1.

In dev, the tests will be moved from test to androidTest, which is where they should be located. We'll also upgrade to a newer version of bintray-release. The result will be that from the next release onwards (a) the pom files should be generated correctly automatically, and (b) the test setup in general will be corrected.
 I think the manual pom fix + the change above should resolve this issue. Please shout if this isn't the case. Thanks.
 @ened - How does that PR make it show the libraries don't need to be in the repository? It looks like it's just updating the versions being used to me.
 I'm following up with Bintray regarding the url that still has the dependencies. Thanks.
 Bintray have corrected the stale files.
 They're still needed for Eclipse, I think.
  I don't really understand what's being described here, but the logging above likely isn't relevant and so there isn't much to go on. Please file an issue containing all of the information requested in the issue template if this is still problematic for you. Thanks!
  This is not currently supported as a built in feature. You could write your own DataSource implementation to cache the video data on disk the first time it's requested (and serve it from disk for subsequent requests). We have a related request that would add support to this directly to the library here: #420. You could alternatively just download the video first and play it from a local file uri as you suggest.
  TBD, but I doubt we'll do this. If someone wants their file to be seekable, they should really use an appropriate container format.
 There would still no clean way to seek, because there's no index and no guarantee that the bitrate is constant. If the application had an index then that would work. You could have the application build a HLS variant playlist with each segment defined by byte ranges (which serves as the index), and then play the content as HLS.
  Has the device passed Android CTS (i.e. is it officially an Android device)? What device is it exactly?
  You should be listening to DemoPlayer.InfoListener.onVideoFormatEnabled. The format is passed to that method whenever the selection changes. Format.bitrate is the bitrate as declared in the master playlist. Format.width and Format.height are the dimensions as declared in the master playlist by RESOLUTION tags. Note that if RESOLUTION tags are omitted from the master playlist then these fields may be set to -1.

You can also use DemoPlayer.Listener.onVideoSizeChanged to determine when the resolution changes.
 See #676
  According to Apple: "The first entry in the variant playlist will be played at the initiation of a stream and is used as part of a test to determine which stream is most appropriate". See https://developer.apple.com/library/ios/technotes/tn2224/_index.html.
 The playlist generator should probably do this. Equally, I think it's ok in practice for a player to exercise some common sense and pick a different variant if it think that it knows better. We don't provide this option currently, but may do so at some point in the future.
  DashRendererBuilder doesn't think the content is protected, and so doesn't instantiate a StreamingDrmSessionManager, which is required for playback of protected content. This is probably because your MPD doesn't define any content protection elements.

To fix this, you can either fix the MPD to contain content protection elements, or alternatively you could try with your existing MPD if you hardcode hasContentProtection to true in that class.
 - You'll need to spend some time debugging why hasContentProtection is false in DashRendererBuilder (assuming that it is false).
- Note that it's recommended that ContentProtection elements be children of the AdaptationSet element, not of the individual Representation elements. We should handle both cases though.
 The fact it wasn't set to true automatically suggests something about your manifest is wrong (or the ExoPlayer parser has an issue). You might want to look into that.
 You should be able to debug the code yourself to work out why it's not being set to true. Just attach a debugger and start tracing the code back to the mpd parser, and you should find out what's going on.
  Thanks!
  You need to use the latest SDK.
  Hi. We just added format detection to automatically determine/check the container type. It appears your sample isn't being detected as MKV (or anything else, for that matter). You can debug by:
1. Modify your code to explicitly pass a WebmExtractor as the final argument when creating the ExtractorSampleSource. This will make it so that MKV/WebM are the only formats being checked.
2. Add some debugging to extractor.webm.Sniffer to determine why the sniff method is returning false. It should return true.
 It's plausible that the change referenced above may have fixed this issue for you, although this is just a guess! Let us know if it's resolved. If it's not, please debug as above and/or provide the front of the file so that we can do so ourselves (the first couple of MBs of the file should be sufficient).
 The sample file you that you sent doesn't appear to fail in the way mentioned at the top of this issue. At least for me, sniffing appears to complete successfully (with or without the recent change ref'd above). Is the sample the exact head of the file for which you encountered the issue?
 So you're accessing the file through a content:// URI? Presumably the same video will work fine if you access it through a file:// URI, in that case and if that's the problem, since FileDataSource does use File.length?

I'm not sure you can use File.length in the content:// URI case. If you look at how available() is implemented for FileInputStream, it should work correctly. Although in this case the file is greater than 2^31 bytes long, and so is larger than what available() can return, which is probably why you see 0. The correct thing to do would probably be to convert 0 to C.LENGTH_UNBOUNDED.
 I'm confused. If you're using file:///, why are you using ContentDataSource (as opposed to FileDataSource)? If you use DefaultUriDataSource, which automatically chooses the correct one to use, it should be selecting FileDataSource for file:/// URIs.
 Well, you should just use DefaultUriDataSource. It auto-selects the correct implementation to delegate to.

The specific problem you were hitting is that ContentDataSource (and AssetDataSource), return incorrect length when accessing content larger than 2^31. We should fix that for ContentDataSource. For AssetDataSource it's unnecessary, because anyone shipping an apk larger than 2^31 bytes is crazy (I doubt this is even possible).
 By the way, have you made any modifications to get the video playing for this file, or are you seeing audio only playback (this relates loosely to #631).
 This should work (with video) on the latest dev.
  As above, the demo app is pretty simple to setup / compile. I don't think we want to start writing tutorials about general Android development, setup and IDEs. These can be found elsewhere, and if we did write them we'd need to periodically update them, which would require a non-trivial amount of effort.

@mingfai - Your point about DemoPlayer is valid. Some of the open issues, such as #514, will naturally simplify this class over time as we push more functionality inside of ExoPlayer itself (that particular feature request will remove ~100 lines of code from DemoPlayer).

We may at some point move DemoPlayer into the library itself as a high level class, however it's unclear whether this is a good idea because most developers end up wanting to make some kind of modifications to it to better suit their use case.
  @andrewlewis - Could you glance over this? It looks good to me, but you know more about this bit!
@IanDBird - You'll need to sign the CLA before we can merge. See [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md) for details. If you've signed it already then let me know what name you signed as, since it's not showing up in our system.

Thanks!
 Note: For the CLA, ExoPlayer is _not_ part of Android.
 It's showing up now, thanks!
 Looks good to me.
  Assuming the s263 atom is for H.263, then I think we probably should provide support. Android's CDD requires that devices provide an H.263 decoder, so there should be underlying support.
 Merged, so marking fixed!
  There is no easy way of doing this. The hard way is to write your own TrackRenderer implementations for audio and video, that bridge onto software decoders (e.g. ffmpeg) that you could then bundle inside your apk. The VP9 and Opus extensions are examples of this approach (and could be used if VP9 and Opus are acceptable codecs for your use case).
  You need to build using the most recent SDK and API level. That doesn't mean that it wont run on earlier API levels, it just means that it'll make use of features on more recent versions when running on newer devices.
  When you use ffmpeg like that, what's the format of the subtitle in the generated output file? SubRip is the type of the input, but I'm not sure what that gets muxed into the output as.
 mov_text is tx3g apparently, which should work. Please provide a sample file that displays captions in VLC and not in ExoPlayer's demo app. Thanks.
 Thanks! There are multiple small fixes needed to get this working. I'll try and merge them tomorrow.

Note that once the changes are merged, the player will report a duration of 2:22 for the test video linked above, despite the fact that audio and video only last for around 0:24. Playback will continue beyond 0:24, and the subtitle at 2:20 will be rendered correctly before the player transitions to the ended state. This is working as intended, and occurs because you've merged a subtitle track that's longer than the video. In a real sample I would expect the subtitle track to be more or the same duration as the video, and so this wouldn't occur.
 This is fixed now. Note that once you enable the text track, it wont display any text that would have been displayed prior to the track being enabled (and should still be visible). You'll need to seek or wait for the next piece of text to appear. In practice this isn't a real problem, since each caption is normally only visible for a short period.
  This is tracked by #151
  Android does support offline with Widevine via the MediaDrm interface. You'd need to implement your own DrmSessionManager that will be somewhat similar to StreamingDrmSessionManager, but will handle storage/retrieval of offline licenses. You'd also need to implement a downloader for obtaining the licenses and actually downloading the media, and you'd probably need a database to keep track of what's downloaded.

We don't provide out-of-the-box support for offline because the best approach generally requires tight integration with the rest of an application (e.g. using existing tables in an existing application database, using existing transfer logic to implement the media downloading and so on).
 One caveat is the supported API levels. Google Play Movies has only adopted this approach from Lollypop onward due to issues found in earlier releases. You should probably try and build a minimal example and see how it works for you. If you have a contact at Widevine, it's worth asking them for guidance on what API levels are properly supported also.
 There is no requirement to play anything. You need to perform a MediaDrm key request/response, but there's nothing that ties this to occurring as part of a playback.
 If you could create a single response that could be used to play the content on any device, then what would the addition of DRM actually achieve? The response could be easily captured and then used to play the content arbitrarily on any device, which seems like it would defeat the purpose.

So to answer your question: Yes, to play a piece of content offline the device needs to have made a license request and received the response. A response is tied to the device that generated the request. There is no requirement to actually play media to perform a license request. In a typical scenario an application will have to download the media for offline playback, and would make the license request as part of the process of downloading the media (probably as a first step).
  Thanks for the summary. Some initial findings:
- Test1 - Contains MPEG4 Part 2 video. It's unclear whether Android provides support for this format. Android supports H.263, but MPEG4 Part 2 is an enhancement of H.263. If it's not supported, then playing without video is working as intended. We'll find out. Note that we'd normally expect video to be H.264.
- Test4 - Shouldn't fail in this way. Although it contains Theora video, so it's unlikely we'll be supporting the video part of this test stream.
- Test6 - This file is unusual because it omits a Cues that would be required for seeking. We should probably handle this case (by allowing playback but disabling seek, as noted in the release notes that accompany the test suite). Whether we play the video in this stream is the same as in Test1.
- Test7 - This is a robustness issue related to skipping under garbage in the stream. We don't expect streams to contain this kind of garbage, but we should cope with it and playback should succeed.

So action items here are:
1. Fix the extractor to work properly for Test7.
2. Fix Test6 to allow playback without seeking.
3. Fix Test4 to not fail in the way that it does fail.
4. Investigate whether MPEG4 Part 2 video and Theora video are supported by Android. I'm doubtful in both cases, but we should take a look.
 Note that Test2 is broken after seeking.
 Test videos 2 and 6 now work (seeking doesn't work in 6, but this is working as intended due to the omitted Cues element). Remaining action items are:
1. Fix the extractor to work properly for Test7.
2. Fix Test4 to not fail in the way that it does fail.
3. Investigate whether MPEG4 Part 2 video and Theora video are supported by Android. I'm doubtful in both cases, but we should take a look.
 The tests that still fail are appear to be hypothetical cases that shouldn't occur for correctly formed media. We've not seen any actual issues with real content as a result of these tests not passing, therefore I'm going to close this issue.  Nope. A good starting point would be to look at TextTrackRenderer to see how it renders subtitles at the correct times. You could implement your own renderer that does something similar, only displaying icons/text.
  You should wait for `InfoListener.onVideoFormatEnabled` to be called before retrieving the format. You can figure most of these questions out by spending a couple of minutes reading the source.
  I think that's correct, yes. Depending on your application, you may be able to do something even simpler and just install a default cookie manager to be used everywhere. The demo application does this in PlayerActivity, with its call to:

```
DemoUtil.setDefaultCookieManager()
```
 It's unclear whether the post above is related, because it's primarily referring to Android's built in media player. I suggest you request some guidance from Akamai at this stage. It should be fairly easy for them to help, since they will know exactly how this setup is supposed to work (and can also easily add a test stream to the demo player and take a look to see what's going wrong).

Re-opening the issue for tracking.
 Interesting. Thanks for the update! I wonder if Picasso is doing something itself related to the cookie manager, that could be having an interfering effect. It's probably worth grepping through their source code for cookie related stuff, if you haven't done so already.
 It's somewhat unclear whether users of Akamai or Akamai ever figured this out, but I'm going to close this due to inactivity.
  Please style this code in the same way as the rest of the code base. It's very difficult to review it when virtually every line has changed due to application of a different code style.
 Hm. This doesn't prevent the message in all cases, although probably catches most occurrences. I'll have a think.
 The only issue this causes is a warning in the logs. There are no user facing issues related to this warning. For now, I think we're ok doing nothing until we have time to think about a complete fix.
 This is fixed in a more correct way in the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental). We're not planning to take a fix for 1.x, since the issue is largely benign.
  ExoPlayer supports a bunch of formats in addition to the ones you mention (MP4, MP3, WebM, Matroska, Mpeg-TS, ADTS-AAC).

For DASH/HLS/SS you probably wouldn't want chunks any shorter than 2 seconds. For low latency streaming you'd probably want something like RTMP, which ExoPlayer doesn't support. The idea of single (or few) frame latency across the internet isn't realistic though, so I'm wondering exactly what you're trying to achieve.
 ExoPlayer isn't really geared to this use case (i.e. local playback with very low latency). We don't currently support RTMP/UDP.
  ExoPlayer doesn't support RTMP.
  What part of the [HLS](https://tools.ietf.org/html/draft-pantos-http-live-streaming-16) spec does this relate to? I'm not sure I understand what you're trying to do.
 I understand, but I don't understand how this relates to the HLS spec. There doesn't appear to be anything in the HLS spec relating to use of shared secrets. HLS supports AES-128 for protecting content between the server and client.
 Closing due to inactivity + lack of clarity.
  Android L supports passthrough of encoded audio over HDMI to an A/V receiver that can decode the format. ExoPlayer supports this feature, but for HLS we only read AC-3 as per the ATSC A/52 specification, not DVB.

So AC-3 playback will only work if the device can output encoded data to an A/V receiver over HDMI, and for HLS the stream must be using the ATSC stream type.

For the relatively few devices that do include AC-3 decoders, we plan to add support for on-device decoding at some point.
  HlsChunkSource currently implements the equivalent logic directly, so you could edit that or fork it and implement your own variant.

We'd like to migrate HLS across to use the same FormatEvaluator interface as DASH and SmoothStreaming at some point, but there's a complication around having to download overlapping segments when switching qualities in HLS, which is not the case for DASH and SmoothStreaming. It's unclear at this stage whether a single interface is suitable for both cases.
  This isn't supported yet. It's tracked in #291.
  What media reproduces this problem? Is it reproducible in the demo app at all, and on what builds of Android? Thanks.
 Does setting a larger maxInputSize on the MediaFormat help? That would be a better (and more technically correct) fix.
 The Google codec is a software decoder, where-as the vendor provided decoders are typically hardware accelerated (normally meaning better performance and lower battery consumption).
 Tracking with #714
  Regrettably, I have never managed to reproduce this issue. We've seen some other reports of it though. As far as I can tell the issue isn't widespread.

At this stage I wonder whether it's specific to something like (a) other applications installed on the device, (b) the model of TV that the device is plugged into etc. Any information you can provide, and particularly full bug reports captured with `adb bugreport` would be helpful.
 Interesting. It's possible that it's an issue with the monitor's HDMI implementation, although it may be the case that devices could be more robust when the issue is encountered.
 Please could you provide information about your monitor (i.e. the exact model number)? Thanks.
 Thanks for the information. It's likely that the HDMI<--->DVI cables being used are at least partly to blame in cases where this reproduces. They're most likely dropping audio in a strange way. We still haven't managed to reproduce. I'll give it another try this week, but we wont be prioritizing the issue, since it sounds like something unlikely to affect users with "normal" setups (e.g. NexusPlayer<--HDMI-->TV).
 I think the conclusion here is as above; these issues are caused by certain HDMI<->DVI cables, which end users are very unlikely to be using in "normal" setups.
  Is this one of the DRM streams in the demo app, or one of your own? Do the ones in the demo app work correctly (if not - exactly which ones fail)? Thanks.
 How sure are you that the content is packaged correctly? If it's just your stream that reproduces the problem, my guess would be that it's most likely one of two things (a) a packaging issue, or (b) you're setting something in your key response that the S4 has problems handling, and which isn't set in the test streams.

Without a working stream / license server, it's not going to be possible for us to debug any further.
  Yes. We use the first listed codec that can play the input format, which should be the most preferred one. It will be a hardware codec if one is available. Have a look at MediaCodecUtil to see how this works.

You can change this behavior by overriding MediaCodecTrackRenderer.getDecoderInfo(), returning a DecoderInfo with the name of the OMX component you want to use, like "OMX.google.h264.decoder". Note that device vendors don't have to include any particular named components in general, so it is important to make sure the OMX component is present, otherwise creating it may fail.
 Good point. It seems reasonable to have a way for apps to override the default codec in case they need to work around some app-specific issue, but in almost all cases the choice of what OMX component to use should be left up to the framework (as implemented in MediaCodecUtil). For example, a device could in theory create several efficient 'hardware' decoders concurrently before running out of resources and falling back to creating software decoders.
 In ExoPlayer 2, it should be quite straightforward to subclass SimpleExoPlayer and override buildVideoRenderers/buildAudioRenderers to pass your own implementation of MediaCodecSelector to the renderer constructors. This should be very little extra code, especially if you miss out the code for building extension renderers.  If you have a contact within Widevine, you should contact them for assistance.

Out of interest, do all of the Widevine protected samples in the demo app work correctly? If so, do you have any idea what the differences might be between those streams and yours?
 Closing as stale. Please file a new issue containing all of the information requested in the issue template if this is still a problem on a more current Android build for Nexus Player. We have @wvpaf from Widevine helping out on this issue tracker, who will be able to take a look if a new issue is filed. Thanks!
  Could you elaborate on what you mean by 'setting B-Frames to 0'?
 Playing back a stream that doesn't have B-frames should work fine without any extra configuration -- the slice type information is present in the encoded bitstream.

As I understand it, the documentation is suggesting that you pass --bframes 0 to the encoder. The result will be that the decoder does not need to do frame reordering when it decodes the stream, which can reduce latency.
 Correct. I think it only really makes sense as an encoder option.
  Not sure if this is an accident, but we only merge to master when we do actual releases.
  This should work correctly on the dev branch. See https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/text/webvtt/WebvttParser.java#L123.
 Duplicate of #580.
  The maximum decodable frame size can depend on the device capabilities and the exact format of the video.

All Android devices should support the format/resolution combinations listed in the [CDD](https://source.android.com/compatibility/android-cdd.pdf). There is also a summary of what formats can be decoded in [Supported Media Formats](https://developer.android.com/guide/appendix/media-formats.html).

Many devices do support much higher resolutions, so it's worth querying capabilities using the utility class MediaCodecUtil. This wraps the functionality in MediaCodecInfo.VideoCapabilities and CodecCapabilities. Since your input video format is H.264/AVC, you should be able to use maxH264DecodableFrameSize(), or isSizeAndRateSupportedV21() on platform API version 21.
  You should just implement your own player UI (i.e. do not use PlayerControl or MediaController).
 @Schizo, @ojw28 said to do not use ORIGINAL PlayerControl/MediaController... But you can use Brightec implementation, just adapt this for Exoplayer... I do that and works perfect.

In that change MediaController for PlayerControl (Exoplayer) and this will be great, also small changes in .create()/.show() you should do for be perfect.
 Please check my code and try that:
![captura de tela 2015-07-16 as 03 35 29](https://cloud.githubusercontent.com/assets/9998303/8716892/d25457e0-2b6b-11e5-9e51-7c44ffabc35a.png)
![captura de tela 2015-07-16 as 03 35 39](https://cloud.githubusercontent.com/assets/9998303/8716891/d2513bbe-2b6b-11e5-9856-59a49691ca92.png)
 You have to:
`private VideoControllerView mediaController;`

And then:
![captura de tela 2015-07-16 as 03 58 47](https://cloud.githubusercontent.com/assets/9998303/8717179/230def90-2b6f-11e5-9852-2c3066215068.png)

`SeekEnable`
Is my own variable of control, you don't need to use that.

Also the original VideoControllerView don't have `Create(0)`, you should use `show(0)`, i have customized that because my player are a little boring :), but that not a big deal for you.
 Well, i have customized VideoControllerView for do exactly what i need, then i make like this. Maybe you don't need.

Its ok for you now?
 :)
  This issue is effectively tracked by #73, since the only audio track available in this stream is a separate de-muxed audio stream.
  The dev branch has support for H.265/HEVC. However, that test stream does not play because its manifest does not contain initialization segments, and the player doesn't find a media format. If I modify it to include an initialization segment it plays back fine.
 The manifest linked to in this issue does not have an initialization segment for video. To play it, I looked at one of the video representations, found where the initialization data was and manually added the byte range in an Initialization element in the associated SegmentBase element in the manifest. Normally you shouldn't need to do this, as the tool generating the manifest should output the required element.

The issue is not really specific to HEVC/H.265 and could equally apply with another format.
 Yeah, it looks like this stream conforms to the spec, but we don't currently support streams where the manifest doesn't include initialization data.
 @waqarz - This is pretty unusual media. One of the properties of sensible delivery via DASH is that the client shouldn't need to make unbounded range requests into arbitrarily large pieces of data.

Self-initializing segments are normally used in the case where the media consists of many small segments (e.g. defined with a timeline in the manifest). A client may need to make unbounded range requests, but only ever to retrieve small amounts of data (e.g. to obtain a ~5 second duration self-initializing segment). ExoPlayer already supports this.

In the case of the mpd in this issue, the whole media is a single segment. To satisfy the property of sensible delivery defined above, the client needs to be able to make bounded requests for initialization and index data. Having obtained this data, it then makes subsequent bounded requests for sub-segments based on the sidx box. The problem is that this mpd doesn't define the initialization range (for video only; it's there for audio!?). To get the initialization data a client would therefore need to make an unbounded range request into the arbitrarily large single segment and keep reading until it encounters what it's looking for.

Admittedly the sample does seem technically valid according to the spec, but that doesn't mean it's "sensible". In all such cases it's trivial to include the initialization range in the mpd, and doing so allows clients to operate more efficiently. Hence I'd consider it a bug, practically speaking, if I ran or used a production system where media is single-segment and the initialization range is omitted.

Question: Was it just a mistake/omission that the ranges are missing for the test vector manifests, or were they deliberately designed in this way to test an edge case that I don't think any production system should be hitting, or do you disagree with my reasoning above?

Thanks!
 The initialization data could in theory be right at the very end of the file, correct? Or is there a requirement somewhere in the BMFF spec that the moov has to come before the sidx and/or any mdat atoms?
  I got hold of an S5 Mini and can reproduce. A few additional notes:
- dequeueInputBuffer does dequeue some buffers after the seek (5 of them), but no buffers are dequeued from dequeueOutputBuffer.
- Behaviour is as you describe for samples that do not require HDCP. For samples that do require HDCP it appears that the failure is harder. The following exception is thrown:

```
E/wv_dash ( 1924): Trustlet did not send a valid return code: 0xd
E/wv_dash ( 1924): tee_send_msg_general_cmd is failed.
E/wv_dash ( 1924): process drm decrypt CTR failed
E/WVCdm   ( 1924): Decrypt error result in session sid3 during encrypted block: 1
E/ExoPlayerImplInternal( 8447): Internal track renderer error.
E/ExoPlayerImplInternal( 8447): com.google.android.exoplayer.ExoPlaybackException: android.media.MediaCodec$CryptoException: Error decrypting data.
```
- If you don't require HDCP, it's possible to work around the first of these issues by modifying flushCodec in MediaCodecTrackRenderer. There's an if/else block in that method where the else path fully releases and re-instantiates the decoder in order to work around some known issues. If you force the implementation to follow that path then seeking works properly. You'd probably want to target this kind of workaround specifically at this device when using the H.264 decoder.

I'll try and get in touch with Samsung to see if they have any plans to resolve the underlying issue.
 We should probably put the fix directly in the library. The open question is what to target it at. It's unclear whether it's device specific or whether it affects all devices with that decoder, on what API levels the issue occurs on etc. If you have anything that might answer those questions. please let us know. If not, we'll have a think about what a best guess approximation might be.

Note that the fix doesn't help in the HDCP required case, as noted above, although it sounds like this wont be an issue for your use case.
 According to Wikipedia there are multiple variants of the S5 Mini: SM-G800F, SM-G800H, SM-G800H/DS, SM-G800M, SM-G800Y.

I think the F, M and Y variants have the Exynos chipset. The H variant apparently has a Qualcomm chipset, so I'm guessing that it wouldn't be affected by the issue. I'm not sure about the H/DS variant. There's apparently a R4 variant as well. I'm not sure about that one.
 I propose amending the condition for `codecNeedsFlushWorkaround` to be:

```
return Util.SDK_INT < 18
    || (Util.SDK_INT == 18
        && ("OMX.SEC.avc.dec".equals(name) || "OMX.SEC.avc.dec.secure".equals(name)))
    || (Util.SDK_INT == 19 && Build.MODEL.startsWith("SM-G800")
        && ("OMX.Exynos.avc.dec".equals(name) || "OMX.Exynos.avc.dec.secure".equals(name)));
```

Which should properly target only the S5 devices that have the Exynos decoder.
 I'll push a fix. Thanks for testing!
 Please file a new issue; it's unclear whether it's the same problem.
 Please file a new issue containing all of the information requested in the issue template, including a full bug report.
  Does this happen on the _latest_ dev branch? If so, how do you reproduce the issue?
 Closing due to inactivity.
  FragmentedMp4Extractor is the right extractor to use for this file, as it contains a movie fragment (moof) box. We hope to add format sniffing soon (#438), which will hopefully avoid the confusion!

Playback seems to work using that extractor. Did you see any other problems when you switched to the fragmented MP4 extractor, or can I close the issue? Thanks.
 @iNdieboyjeff That looks like a separate issue. I'd be grateful if you could open a bug with a sample stream and steps to reproduce the error so we can take a look.
  If you're using the demo app as a starting point, you'll see that DemoPlayer.InfoListener has an onVideoFormatEnabled method. This method is invoked whenever the format changes, and a Format object is passed containing information about the format that was selected.
  @rodrigoolr, please could you email/share one of the problematic streams with andrewlewis.exoplayer [at] gmail.com? Does this happen every time the player reaches the end of the stream?
 @rodrigoolr Sorry for taking so long to reply. It looks like the last sample in the file is truncated by three bytes, so an exception is thrown because it can't be read in full. We will implement a fix to handle this case as if the last sample were not there, so the player will transition to the ENDED state as expected.
 This is fixed in dev.
  The way that URL resolution works should allow you to set query parameters on the initial MPD request URL and have these parameters be included on subsequent segment URL requests. I think for this to work, the segment URLs need to not contain any query parameters of their own.  You'll need to consult the [RFC](http://tools.ietf.org/html/rfc3986#section-5.2.2) for the gory details, which is what ExoPlayer implements. Alternatively, you could setup your manifest server so that it takes the query parameters from the request URL and incorporates them into the manifest that's returned.

Whatever you do, you should (in my opinion) do something inside of the DASH spec. There's no provision that I'm aware of for manually side-loading query parameters into each request URL, and there are better ways to do this (such as the suggestions above).
 UriUtil does URL resolution according to the RFC linked above. The rules about how resolution works are pretty complicated (see the RFC for details), but I seem to remember it being possible to get the query parameters carried through to the resolved segment URLs.
 Hm. Actually, looking at the "Normal Examples" in 5.4.1 of the RFC, is appears that the query parameters (?q in the examples) don't end up being included in the resolved URLs in any useful cases. In which case the correct approach is probably to have the manifest server return a manifest with the query parameters included.
  Is this using ExtractorSampleSource? Does it only happen with your files, or with all content? Could you provide a sample that exhibits the issue?

As for debugging, it's perfectly possible to attach a debugger to the process running on a real device; you shouldn't need to use an emulator at all if you have a physical device.
 When I add the linked file as a sample in the demo app with TYPE_MP4 it seems to play back correctly all the way through (about four minutes).

If you're using the master branch, please could you check whether you see the same behavior on the dev branch? Is it reproducible for you when playing that video from the demo app?
 Please re-open if you can still reproduce as per the instructions above.
  What components are you using to build your player? Specifically, which SampleSource implementation are you using?
 I'm investigating an issue where seeking to the end of a stream with ExtractorSampleSource can cause the player to get stuck without transitioning to STATE_ENDED. The extractor immediately signals that the end of the stream has been reached, but the last buffer from the audio decoder contains media that is never played, so the player never sees the end of the stream. This may be specific to MP3.
 This should be fixed on dev.
  Are you using FrameworkSampleSource or ExtractorSampleSource when you build your player?
 The player should enter the ready state only when it has enough audio and video buffered to begin playback immediately.

I tried playing back a sample MP4 video in the demo app over a slow internet connection, and saw the expected behavior: the state changed from preparing to buffering (showing the first frame of video) then ready. The audio and video streams started at the same time on the transition to ready, and were synchronized. Then the player rebuffered after a short time, due to the slow connection.

Do you see video start late on all videos or only on certain streams? Can you reproduce the issue in the demo app? Thanks.
 The reason audio starts before video when playing that stream seems to be related to interleaving: the player thinks it is ready after it has loaded enough data, but the buffer does not include video frames at that point, as they are coarsely interleaved with audio frames. You may be able to get those streams to play just by reinterleaving them.

We will address the bug in ExoPlayer that causes the video track to be treated as ready before it has loaded data.
 Note - To minimize initial buffering time and reduce the probability or re-buffering events you should still aim to encode content such that the audio and video are evenly interleaved.
  Previously there were two cases where this error could occur.
1. When the server doesn't support partial range requests. In this scenario ExoPlayer would make a request, have it fail part way through (for any reason), and then make a partial range request for the data it didn't receive. The server would provide a non-partial response whose length didn't match the expected length of the partial request, and so ExoPlayer would throw UnexpectedLengthException.
2. The server is fairly badly broken. Specifically, the server cleanly closes the connection claiming to have served all of the response, when in fact it's served only part of it, or more data than it was supposed to.

We fixed (1) by correctly handling a non-partial response in this case. We removed the exception at the same time because it was adding unnecessary complexity, and because we don't expect (2) to ever happen. Hopefully you've experienced case (1) in the past, in which case we've fixed the underlying issue.

If you're seeing failures that you suspect are related to the removal of the exception then please let us know. It's probably a server issue if you are, however.
  I'm confused about the stack trace you've posted. It doesn't align properly with the version you claim to be using. Specifically, the stack trace shows a crash at L224 of PlayerActivity in onAudioCapabilitiesChanged, but L224 in the code is in a different method:

https://github.com/google/ExoPlayer/blob/acee5660abe84311473817042f3c15f953356b3a/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java#L224
 No worries, thanks. Please try with the dev branch too when you have the device. We changed some stuff around this area of code, so it's quite likely to have been fixed.
 Ah. This is the same as #555 (device not powerful enough to play the stream). The crash occurs when the player attempts to switch up to a higher quality stream.

If Apple were to include RESOLUTION tags in their HLS master playlist, which is optional but recommended in the [RFC](https://tools.ietf.org/html/draft-pantos-http-live-streaming-16), then this wouldn't happen, because we'd be able to filter out the variants that the device is unable to cope with. If you need to fix this for your own service, the fix is to include such tags. I think DASH and SmoothStreaming both require that resolution is specified in the equivalent manifest files.
 - You can't just catch the exception; that wont actually fix anything.
- Yes, the video master playlist would ideally be updated to include RESOLUTION tags. If it's not your stream, I guess you can ask the owner and see if they oblige. Alternatively, you can modify HlsRendererBuilder to manually filter the unsupported streams on this device. You could do this in HlsRendererBuilder. There's an integer array called variantIndices which contains the indices of the streams that are considered during playback. You'd need to adjust this array to exclude the indices of streams that the device is not capable of playing.
  I don't think we've deleted any Javadoc. What Javadoc do you think has been removed?
 Changing this to a feature request to better document DemoPlayer. I'd rather write the comments from scratch to make absolutely sure they're correct.
 `DemoPlayer` no longer exists in the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental), and hence this is fixed by omission in 2.x!
  We've never been able to reproduce this issue. It's most likely something has put the device into a bad state, and the mediaserver isn't robust enough to recover.

If you're able to provide reliable reproduction steps using the ExoPlayer demo app then please update this issue, including a full bug report (captured with `adb bugreport`, *not* just `adb logcat` log output). Else I think we'll have to put this down to a sporadic device specific issue.  This only happens if you've called release() on the player (the warning in the logs is expected to occur sometimes when you do this).
  I think your buffer is too small. 20KB isn't going to be big enough to a useful amount of video. For comparison, the demo app uses 10485760 bytes (~10MB).
 Please re-open if you still have problems after adjusting the buffer.
 test3.mkv can't be played because we don't support sample header stripping yet. Should be fixed soon.

test7.mkv contains some invalid data that is incorrectly handled.
  Did you read the documentation for ADAPTIVE_MODE_ABRUPT? Specifically:

`For this mode to perform seamless switches, the source content is required to have keyframes positioned at the start of each segment. If this is not the case a visual discontinuity may be experienced when switching from one variant to another.`

Apple's test streams don't position keyframes in this way (although content providers are highly encouraged to both do so and use ADAPTIVE_MODE_ABRUPT, since it will result in better switching performance).
  SRT is supported, but the setup is a little convoluted. I'm working on making it at least a little bit simpler, and will update this bug in a couple of days once the necessary changes are pushed.
 Sorry for the delay in simplifying this. We're getting closer.
 Hi, Any update here?
 On the dev branch, you should now be able to load and play a subtitle file like:

```
DataSource textDataSource = new DefaultUriDataSource(context, bandwidthMeter, userAgent);
SingleSampleSource textSampleSource = new SingleSampleSource(textUri, textDataSource,
    MediaFormat.createTextFormat(MimeTypes.APPLICATION_SUBRIP, C.MATCH_LONGEST_US));
TrackRenderer textRenderer = new TextTrackRenderer(textSampleSource, player,
    player.getMainHandler().getLooper());
```
 Hi,
Is this change released in 1.4.2 ?
 Yes.
 textUri in the sampe code above is the path of the file. Have you tried using that code? The change also shows you which branches and release tags it's in, if you click on it.
 END_OF_STREAM is sent if pendingSample is false, not if it's true...?
 Ah, I see the problem. I think this is fixed already on the dev branch.
 Thanks for the feedback.
- It should be `state == STATE_END_OF_STREAM` (== rather than !=). I'll get this fixed on the dev branch.
- Yes, seeking is currently broken for HLS on the dev branch. I'll take a look and get it fixed.
 The issues on the dev branch should be fixed by the change above. Note that dev is a little unstable in general at the moment, due to work on #514.
 Thanks for srt support. It worked perfectly.
For those who are looking for sample code, here it is,

//text renderer.
DataSource textDataSource = new DefaultUriDataSource(context, bandwidthMeter, userAgent);
SingleSampleSource textSampleSource = new SingleSampleSource(Uri.parse("http://path.srt"),                    textDataSource, MediaFormat.createTextFormat(
MimeTypes.APPLICATION_SUBRIP, //use "text/vtt" as mimeType for vtt subtitle format.
C.MATCH_LONGEST_US));
TrackRenderer textRenderer = new TextTrackRenderer(textSampleSource, player,
                    player.getMainHandler().getLooper());

TrackRenderer[] renderers = new TrackRenderer[DemoPlayer.RENDERER_COUNT];
//..set audio/video renderers.
renderers[DemoPlayer.TYPE_TEXT] = textRenderer;
player.onRenderers(null, null, renderers, bandwidthMeter);

//text renderer is disabled by default initially. So, enable it after clicking CC button.
player.selectTrack(DemoPlayer.TYPE_TEXT, DemoPlayer.PRIMARY_TRACK);

//disable subtitle
player.selectTrack(DemoPlayer.TYPE_TEXT, DemoPlayer.DISABLED_TRACK);
  @HamedGh this should work well if you just use the playlist file as the path. But make sure the relative paths, if any, point to the proper files.  Implementing offline support for HLS is tracked by https://github.com/google/ExoPlayer/issues/2643. You should track that issue. We wont be providing sample code prior earlier than that issue being updated.  Thanks!
  Can you get a stack trace that hasn't been pro-guarded? Also, does this still reproduce on the dev branch, and if so what steps are required to reproduce? We fixed a potential crash that could be related a few weeks ago (the player would fail if you disabled all renderers whilst paused, and then re-enabled one of them).
 Closing due to lack of information.
  I want to make ExtractorSampleSource use `LoadControl`, which will help to solve this problem (`DefaultLoadControl` allows you to specify lower limits on the buffer in both bytes and time, the latter of which makes it much harder for you to accidentally get this wrong).
 Fixed in `dev-v2`.
  To clarify, does the demo app work on your device, or not? What device is it exactly, and what version of Android is it running?
 Sounds like a bad device. What is the device model exactly? If you capture a bugreport with "adb bugreport" then it should provide a build fingerprint at the top of the file that includes the device model and exact OS build running on it.

If the device is bad and it hasn't gone through CTS, then it's a problem that the hardware vendor needs to resolve, and you should report the issue to them.
 As far as I'm aware this device has not passed CTS. Please let me know if you think differently. Note that it's possible for you to download and run CTS yourself on the device to see if it passes:

https://source.android.com/compatibility/cts/index.html

MediaPlayer is a different platform API than the ones that ExoPlayer makes use of. It's possible that the device manufacturer has implemented that API correctly, but not MediaCodec, which is what ExoPlayer relies on.
 The problem is likely just that the device hasn't implemented the platform APIs correctly. In which case it's not a problem with the application level code.
  Yes, thanks. The WebVTT spec seems to allow arbitrary "stuff" to come after WEBVTT in the header line (technically there has to be a space or tab following WEBVTT). I think it would be fine just to change `equals` to `startsWith`; explicitly checking that the next character is a space or tab seems unnecessary. Thoughts?
 I guess it's not so hard to do it properly. The regular expression would be `^\uFEFF??WEBVTT((\\u0020|\u0009).*)??$`. I'll get this pushed to dev tomorrow.
  I'm curious how you're getting live HLS to work with WebVTT in the first place, because we haven't added support for that yet (this issue is one of the things we'll be tackling when we do get around to adding this feature).

There is indeed a good reason for that; removing it will break playback when the PTS timestamps wrap, I believe.
 Supporting VTT for HLS is tracked by #151, so this is essentially an issue about something that we haven't implemented yet. Please use #151 to track delivery.
  This is because we don't yet support MP3 audio in HLS. We support AAC only. The feature request is tracked by https://github.com/google/ExoPlayer/issues/71. The patch referenced above is a nearly working solution, but it doesn't seem quite stable enough to merge at this point.
 We've merged support for this in the dev branch, and I can confirm that the stream appears to work correctly.
  Isn't this essentially the same as #528 (i.e. it's likely that you have custom components in your player that are incorrectly blocking the playback thread)?
  Request is to support MP3 in MP4.
 This is kind of working after the change above, but we're seeing issues where the underlying mp3 codec will crash. Keeping open to see if we're feeding bad data and/or we can work around the issue.
 These streams should now play reliably on the dev branch, as of https://github.com/google/ExoPlayer/commit/588be2bd4b44fd15fd4636a0f7f482647190d4fa. Closing.
  The RFC was incorrect in revision 9 (which was the first revision to include a description of the X-TIMESTAMP-MAP header). It was corrected in revision 10, and the latest revision is [16](https://tools.ietf.org/html/draft-pantos-http-live-streaming-16).
 I don't see any reason to do so at this stage. Revision 9 was the newest revision for less than a month. Revision 10 could have defined the pattern to be backward compatible, but didn't (if backward compatibility was to be expected, it should have been defined as such in the newer revisions of the RFC).

If it turns out there are lots of files in the wild that happen to use the alternate pattern than of course we can do this, but until there's evidence it doesn't seem worth doing.
  downstreamRendererCount no longer exists in the dev branch. We got rid of it because it was error-prone to set it correctly (as demonstrated in the developer guide - as you rightly point out!).

We'll update the developer guide to remove the count when we next release.
 I've fixed this to be correct in the meantime, since it might be a non-trivial amount of time before the next release to master. Thanks!
  Why don't you just hold on to the parsed manifest object in memory? That seems more efficient than parsing it repeatedly from local storage to me?

Or is the goal to cache between the process being killed and later restarted? In which case how long would you cache them for, given they may change on the server, or would you use the http response headers to determine that?

Thanks!
 I'm somewhat unclear whether ExoPlayer 2.x resolves this issue for you, but things have changed so much in this area that I'm going to close it as obsolete. Please file a new issue if functionality is still lacking in 2.x. Thanks!
  This was a recent regression. Thanks for the report.
  There hasn't been any functional change in terms of how timing is implemented. MediaCodecAudioTrackRenderer has always driven the timing. Previously it indicated this by returning true from isTimeSource. Now it does so by returning a clock instance from getMediaClock. This is solely a restructuring of the code, not a change in how it works.

Given your use cases are custom, you'll have to figure them out on your own!
  I doubt this has anything to do with whether you're streaming or playing locally. Unless you're testing the exact same content in both cases, the comparison isn't fair. Are you? If not, it's likely just a problem playing the specific media that you're trying to play locally (whether it's played locally or streamed).
 No. You'd probably need to make the test media available.
 The amount of memory consumed by a device update should have no effect on the ability to play media or on the speed of the device.

Are you using FrameworkSampleSource as part of your ExoPlayer? If so, please see the warning documented for that class, and try ExtractorSampleSource instead. If you're still experiencing the issue, you'll need to provide the media for us to debug.

Thanks!
  I think you're calling addListener every time you start playing a song. By the time you're playing the 5th song, you've probably added 5 different listeners using the addListener method. STATE_ENDED only happens once, but each of your 5 listeners receives the event. You need to change your code to only add a single listener, or to remove the old listener when attaching the new one, or something like that.
  The underlying network stack should be re-using the open sockets unless you or the server are explicitly doing something to prevent this. What layer are you looking at when you're testing this? You need to be using something like wireshark to see what's actually going on at the network level.
 We considered parallel chunk fetching and ultimately concluded that we're better off with the sequential approach.

Sequential fetching has the obvious benefit of always requesting bytes in the exact order in which they're required. With parallel fetching this is not true, and so it's possible that fetching bytes that are required further into the future will introduce additional startup latency and/or re-buffering events by impeding transfer of bytes that you need more urgently. Sequential fetching is also simpler to manage from an implementation and correctness point of view. Implementing responsive adaptive bit-rate logic is also more straightforward with a sequential approach.

Parallel fetching is beneficial primarily when (a) round trip time is large (so the overhead in starting each request is large, even with socket re-use) (b) chunks are short (meaning the overhead is incurred often) and (c) throughput is still good enough for streaming. In practice it's doubtful how often (a) and (c) would both be true at once. Media providers should be making chunks a sensible size, also.
 If you define robustness to mean that the playback doesn't fail, then it's always possible to be equally robust using either approach (in the extreme case you can design the player to retry indefinitely in both cases, and hence achieve 0% failures). If you define robustness to mean the playback not re-buffering, then it can be debated which is better or worse.

As above, we believe that parallel loading is better only under network conditions that occur rarely in practice. Or at least, we believe that sequential loading will perform better in the majority of cases. We have no plans to support parallel loading. Doing so would significantly increase complexity throughout a large part of the code-base for marginal gain.
 I'm unclear as to what that means that's different to parallel loading (and how it would avoid the unnecessary complexity where-as parallel loading would not)?
 So, loading in parallel basically?
 Ah. We already implement the behavior that you describe. We don't wait until we're consuming a chunk before we request the next one. That would imply that the buffer never grows to more than about 5 seconds, which I agree would be terrible.
 - Once the buffer is full we'll utilize the network at a lower rate, because at that point we can only fetch media at approximately the same rate as it's being consumed. So you're probably seeing a utilization drop simply because the buffer is as full as it can be (which is a good thing!).
- Once the buffer becomes full, we do implement some fill/drain logic that allows the buffer to drain out for a while before it starts re-filling. We start start re-filling again well before it's nearly empty though. I believe there are some battery and mobile-carrier-network related reasons for doing this.
- Both the buffer size and fill/drain logic can be controlled when creating the player. Buffer size can be controlled by changing the buffer contribution values passed to HlsSampleSource. The fill/drain logic can be controlled by using a different constructor when instantiating the DefaultLoadControl, which allows passing of additional control parameters. You can also replace DefaultLoadControl with your own implementation.
- To allow maximal usage of network until the entire stream is downloaded would require us to extend the buffer from memory into flash storage. We've opted not to do this for the time-being. It's definitely not always ideal behaviour. If you're operating at scale then there are negative cost and network load implications of doing this. Flash storage is also a tricky area in general to get right (e.g. not using too much of it, handling read/write failures etc).
 DefaultLoadControl uses a combination of buffered media duration and size to determine when it considers the buffer full. You can control the parameters for each dimension when you create the DefaultLoadControl, by using the constructor that takes the most arguments.
  This should "just work". Are you using ExtractorSampleSource or FrameworkSampleSource? If you're using FrameworkSampleSource, read the warning at the top and use ExtractorSampleSource instead (which will likely resolve your issue).
 This should definitely work using ExtractorSampleSource (and definitely will not work in some cases for FrameworkSampleSource - in particular when the media doesn't have particularly even interleaving of audio and video samples).

Can you reproduce this in the ExoPlayer demo app? Can you reproduce it with the Big Buck Bunny sample video in the demo app, or is the issue specific to the media you're trying to play?
 If you're able to share a sample video then it would be interesting to take a look. It's possible that your sample has extremely uneven audio/video sample interleaving, but my understanding was that FrameworkSampleSource is significantly worse at handling this, not better :).

If you are able to provide a sample video, please attach or link to it in an email to olly.exoplayer@gmail.com. No worries if not!
 I'm pretty sure we're always rendering the frame eventually, but you're right, it sometimes takes a long time. There are two things going on here:

(a) The server is serving the content pretty slowly (at least where I am - I can't stream the video without it repeatedly re-buffering). The main issue, however, is described below.

(b) There's a line of code that allows the player to transition to the READY state before the first video frame has rendered. This is ironically to work around an issue with FrameworkSampleSource not handling sub-optimal interleaving as well as it should. The line in question is:

`|| getSourceState() == SOURCE_STATE_READY_READ_MAY_FAIL`

In the [MediaCodecViceoTrackRenderer](https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/MediaCodecVideoTrackRenderer.java#L283) class. 

Once the player has transitioned into the READY state, but whilst it is still paused, it enters a mode where it executes its rendering loop at very low frequency, according to `IDLE_INTERVAL_MS` defined in the [ExoPlayerImplInternal](https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/ExoPlayerImplInternal.java#L61) class. It can require potentially many executions of the rendering loop body for the first frame to be rendered, because we do frame accurate seeking which can require feeding multiple frames through the decoder to reach the first one that we want to render.

So what happens in this case is the player enters the READY state having not drawn the first frame, and then the low frequency of the loop together with the fact it needs to be called many times results in the first frame not being rendered for a long time. When you press play the rendering loop is executed at a high frequency again, which is why the frame appears immediately in this case.

To fix this locally you could delete the offending line from MediaCodecVideoTrackRenderer. It's only there to work around a problem with FrameworkSampleSource, which you're not using. You could alternatively make IDLE_INTERVAL_MS much smaller, but this is undesirable because it will waste CPU/Battery when the player is paused.

On our side, we'll be deleting this line of code ourselves if/when FrameworkSampleSource finally goes away for good. We may also consider an intermediate term solution, if we can find one.
 I think this was fixed by https://github.com/google/ExoPlayer/pull/628, which is in 1.4.0 and later. Please let us know if not.
  I don't really understand the comment above. What does "tracker in system sdk" mean? ExoPlayer supports all Android devices, regardless of their architecture.

That said, there is insufficient information provided here to help debug the issue. The issue template provides a list of the information that you need to provide for us to follow up on the report. Please file a new issue with complete information if this is still a problem for you. Thanks!
  Please provide additional information as described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Thanks.
  It's true in exactly the edge case that's documented 2 lines below the line you've linked to. This only occurs for a very brief period of time whilst a few messages are passed between threads.
  Let's use #587 to track this.
  I'm not really sure what this issue is tracking. The original post states it attaches a sample URL, but there isn't one. Hence we cannot reproduce. If this is still an issue on the latest release, please file a new issue with all of the information requested [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html).
  The `dev-v2` branch now provides full multi-period DASH support.
  Why would you like to use the raw folder instead of moving them to assets?
 It's doubtful there's a significant difference, and it's unlikely to make a difference anyway because media is streamed (you'll always be able to load it way faster than you need to to play it back, given it's a local file). So just use assets ;).
  What's the exact device? There are multiple Samsung devices with names very similar to this one. Ideally provide the model number and/or a link. Thanks!
 Thanks. It looks like it might be device specific. I'll try and get hold of this model and see if I can reproduce.
 I think this device might just be very under-powered. If you query the decoder capabilities, it doesn't claim to support much at all. The Android compatibility definition document only requires that devices are able to play up to 720x480 (i.e. 480p) baseline profile level 3 at 30fps. It does appear to be able to do this. The media you're playing is probably main or high profile, if I had to guess.
 Yup, that's the one. ExoPlayer has a helper class called MediaCodecUtil, that you may find easier to use.
  You'll need to provide a sample mp4 for us to debug this. Thanks!
 Is it possible that this file is fragmented mp4, rather than plain mp4?
 @andrewlewis This may or may not be FMP4, but either way parseStbl should probably not crash in the case that sampleCount=0 (as is the case in the sample file). If sampleCount=0 it seems valid that the STSC box can contain entry_count=0. Our code currently assumes that this value is >= 1.

Just returning early from parseStbl if sampleCount=0 resolves the crash, but doesn't allow the media to be played.
 The stream contains moof atoms, so it is necessary to use FragmentedMp4Extractor rather than Mp4Extractor. However, we can't play it back even then because it has multiple tracks. Issue #363 tracks adding support for multi-track FMP4 streams.
 Yes. The extractor is reading an audio sample as if it were a video sample, because it only knows about the video track, and this leads to the exception being thrown.
 Closing this as the crash is now fixed, and the feature request to play this type of media is already tracked in #363.
  I believe that the stream is faulty (although some other players are more tolerant to the error). Specifically, the first stream appears to only contain audio, however declares in the master playlist that it contains both audio and video.
  Clarification: Using transportStreamTimestamp is more or less a requirement for supporting separate multi-audio in HLS, so this can be tracked directly using #73.
  Seeking forward into the buffer will retain the buffered data where possible in the DASH, SmoothStreaming and ExtractorSampleSource cases (the only case where it's not possible is if you're seeking less than one key-frame forward in one or more of the enabled tracks).

For HLS this optimisation is harder to pull off correctly due to additional complexity around having to splice overlapping sample queues when switching variants. This is a result of a shortcoming in the HLS specification. Hence we haven't tried to do this yet (there's a TODO in HlsSampleSource, but it's relatively low priority for us).
 Why is this very important? The player can just build up a buffer by requesting the data again? Or are you talking about something different (e.g. the ability to seek in live streams)?
 Note: We will be optimizing forward seeking for HLS so that it matches what we're able to do for DASH and SmoothStreaming in 2.x.
 We abandoned plans to optimize this for now. It turned out to be pretty non-trivial. If you're concerned with optimal playback behavior then you should really be using DASH rather than HLS.
  There's no current plan to support this, no.
  I don't understand what you're asking here.
 We don't have any plans to support playing a single video from multiple URLs. That's just an inefficient way to distribute media. For example you'd need to request the head of each of them separately before you could determine the duration of the video. Why would you not locate the whole video at a single URL?

We do eventually want to support seamless transition from one video to the next, which is different in the sense that the progress bar would show the progress of the current part only, not of the whole video.
  This looks like a device-specific issue. I don't have the particular variant of the Samsung Galaxy S3 you mentioned here, but I was able to test another version. Unfortunately, it hasn't received an update to 4.4.2, and I wasn't able to reproduce the issue on its current build -- playback worked fine.

Do you know if this is reproducible on any other device/build combinations? If so, please could you reply with their build fingerprints, output by 'adb shell getprop ro.build.fingerprint'? Thanks!
 Seeing the same symptoms isn't sufficient evidence to confirm that the underlying issue is the same. If you want us to investigate any further devices:
- Please provide a full bugreport (i.e. the full output of "adb bugreport") captured immediately after reproducing the problem.
- Please try making the workaround apply to the affected device yourself, and confirm that it does indeed fix the issue. As you can see from the commits referenced above, this is pretty trivial. It's just a matter of adding an extra `|| Util.DEVICE.startsWith("XXX")` into the workaround condition, where XXX is the value of `Util.DEVICE` on the affected device.
- If the workaround is successful, please just send us a pull request and we'll happily merge it. Pull requests should be made to the `dev-v2` branch.  Not implemented yet, and tracked by #151
  That change didn't modify anything related to HLS. Are you sure that's the correct commit?
 Do you mean when you manually switch the tracks (e.g. by selecting a different track in the demo app)? If so then yes, I can reproduce that. Looks like the issue was introduced in b806109cfd63f4aedf19f388c1091b81f61bead2 rather than the commit you reference.
 Should be fixed by ecf7d1be9e68d665bb61d1b43b3daec0933e0ddd. Thanks for the report!
  We don't enable switching down into the audio only variant (i.e. we filter out audio-only variants if there exist variants with video). I understand that it's part of the spec, and that there are some use cases for it, but in many cases it's completely undesirable to adapt in this way.
 Not yet, guys, sorry.  As above, you can't just throw away frames, unless you know there aren't any inter-frame decoding dependencies or unless you carefully work out which frames to discard.

You can set the C.SAMPLE_FLAG_DECODE_ONLY on a sample to still feed it to the decoder but then not render it, which you can do on arbitrary frames without knowing much about them, but that would mean the decoder would still need to be capable of running at 120fps, which is unlikely to be true on most Android hardware.

If there's a server involved anywhere in this setup, you should really transcode the content to a more appropriate frame-rate on the server-side
  That's definitely a HTTP 404 (i.e. not found), not anything to do with encoding or anything like that. Perhaps the player is requesting a url that's different to the one you're looking at in your browser. Can you provide a link to your SmoothStreaming manifest?
 - You initially pasted a link containing "StreamBox" that has fas subtitles. I don't see a 404 on those, but it does appear they're not packaged correctly within the FMP4 container, and so playback fails.
- The current link above contains english subtitles, but requesting them returns 415 (media unsupported).
- The link the fast subtitles above gives a 404 in my browser.

Which link am I supposed to be looking at, exactly?
 That suggests your subtitle data isn't correctly packaged in the FMP4 container. At the point where the error is thrown the data is still being treated as binary sample data contained within FMP4, and it hasn't got to the point where it tries to read/parse the subtitle sample. You should look at your packaging.
  We don't support RTMP.
  It will be possible to query the tracks in the media once #518 is implemented. Once that's done, you'll be able to query the tracks of the video renderer. If there are none, it's an audio only stream. I suggest following along that issue to track when the feature is delivered.
 You can now use the ExoPlayer.getRendererHasMedia method to determine whether or not each track has something to play.
  Whilst there are now more devices supporting multiple decoder instances, there are still no guarantees that multiple instances will be available (since it would prevent shipping of low end Android devices). It seems quite "noisy" to be needing to play 4 video streams at once, however if you do want to do this it might be a matter of white-listing only devices that are known to work for this use case.  Correct. Thanks!
 Fixed
  In short, this is working as intended. In more detail:

Raw AAC/ADTS and MPEG-TS streams don't contain index structures for allowing seeking, or their durations. Hence the only way to seek or determine the duration of such a stream is to download the whole thing and scan through it to see what's there.

For this reason, these formats are usually only used where an index is provided separately (as is the case in HLS, where the playlist file serves as an index into small chunks of media in these formats), or for live streaming with no DVR window (where seeking isn't possible and the duration is unknown anyway).
  We survive provided the segment is available in one of the possible variants. In this case there is only one possible variant, and so we fail.

I'm not sure we're particularly interested in making playback robust against these kind of cases. Streaming services should just not list chunks in the playlist if they're unable to serve them. If you're going to be delivering a good quality service this kind of case shouldn't occur.
 Thanks! It turns out we're only blacklisting if the playlist request fails, not if an individual chunk request fails. I'll fix this.
 This should work now. Please check though!
 The indexing in that class is a bit messed up as well.
 I'm surprised this ever worked (it probably works by chance, mainly). I've sanitized the whole class in 51a8635ba2fa470ef93ccf6d48a3de0fadb2150f. I believe your test cases should pass now. Please verify.
 The final case was a problem further downstream, and should be fixed by 203f3ab7321a21900cc842ba9f0ef17fe29a1607. Thanks!
 Heh. I think there's a simple fix for that. I'll merge one on Monday, and have re-opened this issue to track that. There are also some issues around the value of previousTsChunk provided to HlsChunkSource.getChunkOperation in the case where a chunk needs to be replaced. It's currently providing the wrong chunk.
 Merged b8df8ecb47d838f4e3c184cbcbb64a397b1384a2 and 97aaee6d1505e52f0c502c19758ba59de27dc959. I didn't test them in detail.

Thanks for helping to iron these cases out, it's been very useful. If possible, could you:
- Verify again that all of the cases are now working as you expect them to?
- Provide descriptions of each test. Basically just like your list above, but this time including the additional cases that you added, and with the corresponding link in each case. We're working on playback tests that we can run automatically, and these cases seem like great candidates. If you could provide this list, we'll make sure to come up with equivalent tests for our test set (nb - we're planning on hosting all of the test content ourselves - you're not committing to keeping the links live or anything like that :)).
 All cases should now pass. Thanks.
  Change looks good. Please note that you need to complete a CLA before we can accept the change. See details [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md). Let me know when this is completed. Thanks!
 Thanks!
  ExoPlayer will only seek when you tell it to, so you should know when seeks occur without having to listen to the player. Hence there is no event.

Is your wish to have this just for convenience (i.e. because you want to register a bunch of things as listeners to the player, and see when seek gets called without having to track something else separately)?
 The best solution as things stand would be for you to provide a thin wrapper around an ExoPlayer instance, which itself could implement ExoPlayer, and require that applications using your analytics project wrap their players and access them through the wrapper. You could then intercept the seek.

We may add a seek callback in the future, but have no immediate plans to do so.
  I think you've misinterpreted what the constant means. It basically means "don't switch to a lower quality if we have 20 seconds or more media in the buffer". Its purpose is to avoid flapping between high and low qualities whilst the buffer is relatively full, which would otherwise happen if bandwidth were to temporarily drop.

Note that setting it to a smaller value actually delays possible switching to a lower quality stream for longer (because more of the buffer has to drain). It wont make anything faster.
 What shouldn't happen at all?!
 Ah. The player does actually pick up the first stream in the list as its initial quality, as recommended by the HLS specification. See the part of HlsChunkSource commented as "Select the first variant from the master playlist that's enabled".
  Please see the documentation about FrameworkSampleSource and why it's deprecated. We're unable to fix issues such as this one. In most cases, you can use ExtractorSampleSource instead.

http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/FrameworkSampleSource.html
  See the documentation for: http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/upstream/DefaultUriDataSource.html
 http://google.github.io/ExoPlayer/doc/reference/index.html?com/google/android/exoplayer2/upstream/DefaultDataSource.html

Note - In general just go to http://google.github.io/ExoPlayer/doc/reference/ if you can't find documentation for a class. It's all there and pretty easy to navigate.
  Can you confirm that changing this line:
https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/hls/HlsSampleSource.java#L365

To:

`if (enabledTrackCount > 0 || !prepared) {`

Fixes the issue for you?
 We'll be merging a slightly different fix shortly.
 Yeah, best bet is just to copy the lines across by hand. We probably wont be doing another release for a couple of weeks.
  This is tracked by #151
  Being a "good citizen" in the Android ecosystem requires that you release the video decoder in onPause (or before), so ideally you should create the player in `onResume` and release it in `onPause`.

If you're seeing ANRs then you've probably implemented something that uses a different threading model to the rest of the player. The idea of the threading model is that nothing should block the player's internal playback thread for any significant period of time. Specifically, this thread should never be blocked on network or disk IO. If you look at how the standard components in ExoPlayer work, all network IO is delegated to background loading threads. These load data into a buffer, and the playback thread consumes from the read side of the buffer in a non-blocking way.

Provided you continue to ensure this property in your own components, you shouldn't see any ANRs. Both release and blockingSendMessage will block on the playback thread, but the playback thread itself will be able to respond very quickly.
 If you look at the surfaceDestroyed documentation, you'll find this:

"After returning from this call, you should no longer try to access this surface. If you have a rendering thread that directly accesses the surface, you must ensure that thread is no longer touching the Surface before returning from this function."

You have to block to guarantee this property. There's nothing unsafe about blocking, providing guidance is followed.
 Feel free to re-open if you have further questions. Thanks!
  Working fine for me. Likely just a temporary issue on the server side.
 Right, so it's the "WV: HDCP + secure video path required" one that fails, not the one mentioned in the title/body of this report. I'll get this fixed.
  Please provide a link to the problematic content.
 Closing due to inactivity / insufficient information (i.e. lack of test stream).
  Deciding whether you should run ExoPlayer in a service is no different to deciding whether to run anything else in a service. If you want audio playback to continue whilst your app is in the background, for example, then you should use a service. If you don't, then it's probably not necessary.
  Is it easy to use stetho with DefaultHttpDataSource as well? It looks like stetho supports urlconnection in some form at least (I didn't look in detail).
 I don't think we want this in the core library or demo app, since we don't want to mandate that developers include the okhttp network stack in their applications. This is definitely useful as an optional extension though, particularly for developers who already include okhttp in their applications anyway.

If you were to package it up as a standalone extensions project in the [extensions](https://github.com/google/ExoPlayer/tree/dev/extensions) directory, then we'd happily merge it there so that developers who do want it can get access to it.
 Notes
- I've updated the title of this issue to correspond more closely to what it's tracking. This is really tracking providing an okhttp variant of HttpDataSource, and the primary use would be to allow applications to use okhttp as the network stack. The fact that you can debug using Stetho when you're using it is more or less just a (very nice) added bonus!
- You'll need to sign the CLA before we can accept a pull request. See [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md).
 This is merged in dev. Thanks!
  Why don't you try using ExoPlayer to play your MP3 files directly? I can't think of any reason why HLS would be any better for this use case. Any benefit you get from moving to ExoPlayer is more likely to be from moving to ExoPlayer than from moving from one format to another.

To answer your question about HLS specifically, we currently only support AAC audio for the HLS case, and not MP3. We do support playing MP3 files directly, as noted above.
 I'm not sure. Depending on the user scenarios, it doesn't seem completely implausible that that kind of percentage of users could be affected (e.g. If users typically consume from your application whilst they're moving around and connected to mobile networks rather than WiFi). You'd probably have to get some more detailed debug information to investigate further, unfortunately.
  We're slightly confused about the question. Android's network stack will check the certificate issued by the server using certificate authorities provided by the underlying platform. This guards against the type of MITM attack where the client ends up connecting to a bogus server. Is that what you meant? In which case I don't think there's an issue here.

Some related reading: https://github.com/square/okhttp/wiki/HTTPS
 Closing due to inactivity (and because I think the above answers the question).
  We're actually just in the process of writing a separate test project for playback tests. We found ActivityInstrumentationTestCase2 does the trick, but it is necessary to do a bit of slightly awkward thread jumping to get everything working correctly on the right threads.

The project we're creating will provide a little framework for making such tests relatively easy to write (it'll take care of the threading for you, let you schedule events like seeking/pausing to be performed during the test, and provide a convenient method for the end of a test run where you can validate that playback worked as expected).

So, if you want to experiment now, look at ActivityInstrumentationTestCase2. Otherwise, we'll try and get the initial version of the new project pushed to dev in the next couple of weeks.
 For video playbacks it's best to have an activity, because having an activity is the easiest way to get a valid surface that you can render into (and more accurately reflects common usage). Given we need playback tests for video, our test project is going to use an activity for all test cases. We'll be abstracting away the activity lifecycle so that you wont need to think about it when writing tests.

If you only want to test audio playback then it should be possible to run tests without an activity. The reason you don't see events being delivered is that you need to create an ExoPlayer instance on a thread that has an associated [Looper](https://developer.android.com/reference/android/os/Looper.html). You'd need to setup such a thread and use it to create and interact with the player. [HandlerThread](http://developer.android.com/reference/android/os/HandlerThread.html) might be useful.
  Dear all  !
In Exoplayer Lib, how to set full screen when play hls file.

Thanks.
 Thanks bsysop, i'm trying ....
  wonderful 
![tongue](https://cloud.githubusercontent.com/assets/9839084/7989217/3e6e5414-0b12-11e5-96cc-e91c6f8a3770.gif)
 Hi all !

How to set media stream full screen,  play hls with exoplayer.

Thanks.
  This is working as intended currently. The initial fetching of the manifest is done outside of the player in the demo app, so if it fails then the failure occurs before the player is even prepared. Note that DemoPlayer.onError is invoked, as you would expect.

We may move initial manifest fetching inside of the player in the future, but this is TBC.
 You should definitely not be doing anything with reflection. The demo project already demonstrates how to do this. Typically you'd wrap an ExoPlayer in a slightly higher level API (in the demo project this is called DemoPlayer) that will report these events in addition to the ones from the underlying player. You'd then have your analytics code depend on DemoPlayer, rather than directly on ExoPlayer.
  What's the use case? There's no way to do this currently.
 I have no idea what this issue is actually about at this stage, so marking it as closed.
  There should be a stack trace associated with that error. Without either seeing it or having a link to the content, there's no way to diagnose this further.

I'm not convinced the error occurs whilst parsing the MPD, as opposed to parsing the actual media streams. It seems more likely that the actual media contains a huge (1.8GB?) box, which I don't think would be expected/correct.
 I think that the cenc:pssh elements in your manifest are invalid. My understanding is that those elements are supposed to contain a Base64 encoded PSSH box. Your manifest doesn't appear to do this. When we try and parse the length of the box we get a huge value, which is why the OOM occurs. Prior to that we parse the UUID a5dd60ff-e667-1e99-1a0d-776964657669 from the box, which doesn't seem to correspond to a known scheme.
  Correct. Also, when we implement this for HLS, we'll use MultiTrackChunkSource there as well.

I think we're less interested in the ExtractorSampleSource case in general, since muxing multiple tracks of the same type into a single stream is fundamentally not the best approach (DASH or SS is always going to be better if you want multiple tracks of the same type). If you were to add support, however, doing it at the ExtractorSampleSource layer would be more consistent with the model used elsewhere. You shouldn't need multiple renderers. It should be possible to switch track by (1) pause, (2) disable renderer, (3) reconfigure sample source to provide samples for a different track, (4) re-enable renderer [if applicable], (5) unpause [if applicable]. Like:

https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/DemoPlayer.java#L600

Step (3) is something you'd need to implement.
 In my proposal there would be a single track exposed by the sample source, and the switching would be hidden behind it. The approach definitely works; if you look at how we do it for DASH/SS, it's following that approach.

Having said that, it seems things would be better if track switching were made more of a first class citizen of ExoPlayer. I think we should do the following, which should be good for what you need:
1. Add a onPrepared() method to ExoPlayer.Listener, to be invoked once preparation has been completed (you can infer the same thing from the existing onPlayerStateChanged, but it's not trivial to do so).
2. Add getTrackCount(), getTrackInfo(int index) and selectTrack(int index) (or similar - exact API TBC) to TrackRenderer.
3. Add getTrackCount(int rendererIndex), getTrackInfo(int rendererIndex, int trackIndex) and selectTrack(int renderer, int trackIndex) to ExoPlayer.

We'll need to figure out what an appropriate TrackInfo object is. It will need to contain more than the current TrackInfo object.

To implement multi-track selection in an app you'd then:
1. Wait for onPrepared, which indicates that track information is now available.
2. Query the track information and display it in the UI.
3. Call selectTrack(rendererIndex, trackIndex) to select tracks.

It may also make sense to merge setRendererEnabled into this new API. Disabling a renderer could be implemented by calling selectTrack(renderer, -1).

Thoughts?
 I have a version of this working, but it's pretty major surgery through the code-base. It'll likely be a few weeks until it's all merged.
 This request is unrelated to HLS. Multi-audio in HLS is tracked by #73.
 It will provide plumbing for passing track information through common player components, so yes it will help.
 We'll start merging some steps towards this soon. The first one will create a common base class for all TrackRenderer implementations that pull samples from a SampleSource, so as to avoid having to add multi-track propagation logic in multiple renderers separately. We'll then start merging code to actually propagate the tracks through the various components. Initially we'll be exposing fairly useless information about each track, although once we're at that point the task of populating the information properly can probably be parallelised to some extent.
 Status update:
- This is fixed as far as MP4 is concerned. If you try and play an MP4 with multiple tracks in the demo app, the Audio button will now list each track separately, and allow you to switch between them.
- For SmoothStreaming you'll see multiple audio tracks as before. For video, you'll now see both "auto" and each individual format. Previously it was not possible to switch to a fixed format, so this is new.
- For DASH, multiple audio tracks have temporarily disappeared. They'll be back in due course, along with the ability to select individual video formats (i.e. equivalent to the SmoothStreaming case). This work is currently blocked on another feature being delivered.
- HLS is the same as before, except that the video format exposed is currently wrong (it should probably be "auto"). We'll fix this.
 - We will also be adding multi-track support for MKV/WebM (probably tomorrow).
 This has been possible for a long time (the demo app has had this ability for a long time too). Using ExoPlayer's setRendererEnabled method is the best way to do it in the current release.
 - Multi-track is now fully working on the dev branch for SmoothStreaming, DASH, MP4, MKV and WebM.
- HLS multi-track is tracked separately (#676, #73).
  Is this easily reproducible? At first glance, the stack doesn't seem particularly related to having paused/resumed as you describe.
 Please re-open if you find out more. Thanks!
  It's unclear to me whether this is an actual failure (just because it's logged at E log level doesn't necessarily mean it's a problem; it could just be being logged at the wrong level). I'm also unclear about why that logging would occur when using ExoPlayer. It seems more likely to occur when using the standard MediaPlayer to me.

Please could you provide a full bug report (i.e. the output of "adb bugreport") shortly after encountering the problem in the ExoPlayer demo app, and also explain what the end result is (i.e. does playback succeed or fail)? You'll need to put the bugreport in Drive or DropBox or something and provide an accessible link.
 Closing due to inactivity.
  I'm pretty sure that although it's a long, the actual values returned only advance up to 0xFFFFFFFF and then roll over. Feel free to test this, and re-open if you discover that this is not the case.
  We can't provide an estimate at this point, sorry. You could look at trying to implement this yourself, but we're not in a position to help figure out how to do it.

Note also that we'll be prioritizing seeking in live streams for DASH and SmoothStreaming before HLS, as per https://google.github.io/ExoPlayer/2015/05/08/the-benefits-of-dash.html.

Closing this, since the request is tracked by #87.
 "Soon" is the hope. We pushed some limited support to the dev branch recently, but had to revert it because it didn't work quite right. We hope to push again soon.
  I don't understand why you need to keep any previous ones at all. Have you actually tried not doing so? It should work correctly.
 I don't really understand what you're saying. Provided the cues are in the correct order and do not overlay, you shouldn't need to keep more than one. Are they? Do you have some sample content handy that I can look at?

TextTrackRenderer already handles the difference between parsing and presentation times.
 Please share access to the link; thanks,
 Ok, you can definitely do this without maintaining a list, however some changes in TextTrackRenderer are needed to make it do something more sensible. We'll pull this as it is, and make some modifications on top once it's merged. Thanks!
 I've submitted https://github.com/google/ExoPlayer/commit/01affbb93ef685e91ab38b085dfaccb8bb011406 which simplifies this change. It should give the same end result, but please give it a try. Thanks!
  Please could you specify exactly what media you're trying to play (ideally links to the actual media, or samples in the demo app that don't work correctly)?
 As in, every sample provided by your SampleSource is 188 bytes long? That's not a valid way to provide samples, and whilst some decoders may be permissive enough to still function, that's not the correct way to provide data and it's expected that a decoder can fail when fed data in this way.

You need to properly segment the data into whole sample. `H264Reader` contains code that does this, so you can likely take a look at that and re-use much of it. I suspect that once you properly segment the data, playback will work correctly on all devices.
 The issue in #143 was that we weren't setting the initialization data (i.e. sps, pps) in the video format. The corresponding change was: https://github.com/google/ExoPlayer/commit/0a888a0d4f11356f6021e63e5ba473d9c0a5fd77. So if you're not setting the initialization data correctly in your own source, that could be why.

In any case, this sounds like an issue with how you're feeding data to the player rather than an issue with ExoPlayer itself, so closing for now. Please re-open if you disagree, or if you can reproduce with the ExoPlayer demo app in a way that we can reproduce. Thanks!
  MediaDrm.isCryptoSchemeSupported(StreamingDrmSessionManager.PLAYREADY_UUID) definitely returns false on my Nexus 5 (and I would guess the same is true for the other devices you've been looking at). Are you sure you're passing the correct UUID for PlayReady?

In any case, as far as I'm aware AndroidTV devices, like Nexus Player and the new Sony TVs, are the only devices that currently provide PlayReady support.
 I'll try and find out whether Samsung intended to provide PlayReady support on the S6. For now, I think you should work on the assumption that they didn't intend to do so.
  Please can you provide a link to the problematic stream?
 Please email olly.exoplayer@gmail.com. Thanks.
 Please provide access to the link you provided. Thanks.
 The sample content you provided works fine for me in the demo application, on a Nexus 4 running LRX21T (Lollypop). Is it possible your application isn't correctly releasing previous player instances? Does this happen after a clean reboot of the device, when played through the demo app?

If so, what's the exact build that you're using?
 This is an underlying platform issue and there's not much we can really do to fix it at the ExoPlayer level.
  Please see reply number 7 at:

https://code.google.com/p/android/issues/detail?id=37577

In other words, if that's your own stream then please change the URL to not have an underscore in it. If it's not, you should recommend whoever does own the stream change their URL, or else accept that it might not work on some devices.
  This is because we don't handle the following stream types:

0x03 ISO/IEC 11172-3 Audio
0x04 ISO/IEC 13818-3 Audio

We currently handle 0x0F (AAC) for audio.
 No massively easy way. Internally in MediaCodecTrackRenderer, the audio renderer will transition to the STATE_IGNORE state after doPrepare. So one option as a temporary solution would be for you to expose that and query it.

Things this issue now tracks:
1. Provide an API to allow easy querying of whether a renderer is in STATE_IGNORE.
2. Supporting MPEG-1/MPEG-2 audio in HLS. Since it appears VideoView supports it, I guess we should too.
 Yup, thanks. Updated the comment above. We should aim to provide support (not a huge priority for us though).
 MP3 support has been added to HLS in the dev branch. The stream linked in this bug is dead, so I don't know whether it works, but closing anyway. Please file a new issue with a working stream if you find any problems.
  See #55 
  What is a mixed playlist?
 Oh right. Looking again, this appears to be a bug in MediaExtractor, under FrameworkSampleSource. You shouldn't use that class (ideally at all - see the warning in the Javadoc). You definitely shouldn't use it for HLS. Use HlsSampleSource instead, which appears to be able to play this just fine in the demo app, if I add the following sample to Samples.java:

```
    new Sample("Test",
        "http://104.130.27.158:6060/bitrate_0.m3u8", DemoUtil.TYPE_HLS),
```
 The stack trace has FrameworkSampleSource in it (and not HlsSampleSource), so...?! If it doesn't work in the demo app when you add the same as above then please re-open. Otherwise it's something wrong in your code.
  This looks good, thanks. Sorry for the delay in looking at it. Things like switching track + setting up the renderer + loading the subtitle should all be possible using the existing classes in the chunk package. I can make a nice util method for doing this once this is submitted.
 I'm planning to pull this as-is and clean it up a bit after the merge. Unless you're actively working on it still.
 I simplified this significantly in https://github.com/google/ExoPlayer/commit/fbbf3f27fdce46950f5cbe5e57fcc07325eb52f3. It looks a bit like you started off by copying the WebVTT implementation, but there was actually quite a lot of complexity there that's unnecessary for SRT.
  We don't currently have support for gapless playback. I'll use this issue as a tracking bug for the enhancement. Thanks!
 We now extract gapless playback metadata, so the remaining work for this is tracked by #1270 (playlist support).
 @boywithk9 It looks like the decoding process for Ogg Vorbis trims padding, so it should work already.  If you're playing on-demand (i.e. not live) content, then you could equally easily change DashRendererBuilder so that its videoRepresentationIndices only contains a single index. This will select a single stream for use at the start of playback. You can then use the normal FixedEvaluator.
  I'm not sure if Samsung intended to provide PlayReady support or not for the S6, but your experience suggests that it doesn't work. You'd have to ask Samsung. AndroidTV devices do support PlayReady, so you should test on one of those if you want to try out PlayReady support.
  This is working fine for me. It may have been a temporary issue.
  I think this is fixed already, in https://github.com/google/ExoPlayer/commit/ac54b4f696d65fe03533af6e7428c751ceb51b35#diff-e9fd670327a94df1ab96714b095f4985R233
  There's nothing useful in the logging. What device exactly are you using (there are multiple variants of the Moto G). If you do `adb bugreport` then the top of the output will show you the exact model and build, like:

Build: LXB22.46-28.1
Build fingerprint: 'motorola/falcon_retgb/falcon_umts:5.0.2/LXB22.46-28.1/1:user/release-keys'

Could you paste the output on the problematic device? Also, which video are you using to reproduce the problem? I tried the DASH sample videos in the demo app on the device with the fingerprint above, on the dev branch, and didn't see any problems.
 Hi @ojw28, sorry for the delay, not was easy to find another content with 1980px

That DASH have exactly configuration: They are +1280px, perhaps they bitrate not very big, just 800kbps, maybe this cause some problem on Android device 5.0+.

```
Build: LXB22.46-28
Build fingerprint: 'motorola/titan_retbr_dstv/titan_udstv:5.0.2/LXB22.46-28/27:user/release-keys'
Bootloader: 0x4882
Radio: msm
Network: ,TIM
Kernel: Linux version 3.4.42-g48d3b85 (hudsoncm@ilclbld72) (gcc version 4.8 (GCC) ) #1 SMP PREEMPT Tue Jan 6 10:40:31 CST 2015
Command line: console=null androidboot.hardware=qcom user_debug=31 msm_rtb.filter=0x37 vmalloc=400M utags.blkdev=/dev/block/platform/msm_sdcc.1/by-name/utags androidboot.emmc=true androidboot.serialno=0430379687 androidboot.baseband=msm androidboot.mode=mot-charger androidboot.device=titan androidboot.hwrev=0x8400 androidboot.radio=0x7 androidboot.powerup_reason=0x00000100 bootreason=charger androidboot.write_protect=0 restart.download_mode=0 androidboot.fsg-id= androidboot.secure_hardware=1 androidboot.cid=0xC androidboot.bootloader=0x4882 androidboot.carrier=retbr mdss_mdp.panel=1:dsi:0:qcom,mdss_dsi_mot_inx_720p_video_v0
```
 Good to know is not only with me @frlozano 
 Closing due to inactivity. It's unclear whether this is still an problem. Please file a new issue containing all of the information requested in the issue template (including a full bug report and links to content that reproduces the issue) if you still see this using ExoPlayer 1.5.8 or later. Thanks!
  Seamless looping is supported in `dev-v2`. See the change description above for information about how to do this.

We'll also be publishing a blog post next week about MediaSource composition, which will include a small section on looping media. I'd suggest subscribing to [the blog](https://medium.com/google-exoplayer) so you're notified when this happens!
  There are valid reasons why these variables shouldn't be reset in these places. For example if you reset in prepare, you break the ability to seek to a non-zero position before prepare is called. If you reset in stop, you break the ability to query the final playback position if an error occurred, and so on.

You may as well just create a new ExoPlayer instance every time to achieve what you need; the overhead of doing so should be small. Alternatively you should wait until the player transitions to the prepared state before you read the values.
  Hi @zarej, you 100% right +1 for you, this works and play the video h264!
Why you says this have lower performance? What exactly will be worst?
 If the stream are small no much problems, problem are when large streaming, i tried with Dash and HLS and always freezing :(

I revert the changes, so i using "OMX.amlogic.avc.decoder.awesome" again, then i do a try, using WebView(Chromium)+HTML5 DASH Player... And this works perfect, of course i want to use Exoplayer but i don't know why Exoplayer don't like to use this Codec, i will still checking about that.
 Clues by now:
HLS h264 play but freeze.
Dash h264 play but with no video.
Dash h264 (In player HTML5) play perfect.

And this is what make strange, because if play in Player HTML5 it means have to play in ExoPlayer.
 Well, finally i made my Dash play in this TvBox with a simple new "MediaPlayer();", and then this run ok... but can't play with exoplayer. (This means a little dirty, i have two players, one for mobile/tablet, another for Amlogic TvBox).
- AmLogic using a called AmFFMPEG and some own decoders, they have own libDash for decode this.
- This Dash can't have different bitrates in the manifest, so... Do not help so much, but works smooth.

I have debugged the Exoplayer and i can see Exoplayer do not create "Surface", maybe he thinks "OMX.amlogic.avc.decoder.awesome" is a no valid decoder, that point i can't check.

Amlogic is growing in the market, so will be interesting fix that, maybe can i try to do that, but i need some orientation because i have wasted 3 days in that and maximum i can say is surface not created (I have checked that do not go to MediaCodecVideoTrackRenderer because Surface is no created).
 After some time testing, i found my poor solution more poor than i think...

This Video+Audio get desynchronized :(

I have a question, can i built-in some Video codec (Example omx.google or some Lib_h264) in APK?

Thank you!.
 Finally i got this @brianchu  @zarej , i using **Amlogic S805/Mali450**, 
For me in this device, with my exactly configuration, HLS Works with Hardware decoder (**OMX.amlogic.avc.decoder.awesome**) if Single bitrate, if adaptive this simple do not works, When Software decoder (**OMX.Google**) this freezes, i sure in software decoder he just dropping frames, it means not enough power (CPU) to play media.

Then i check about this Hardware decoder capabilities, and this don't have Adaptive.

```
08-18 02:36:02.477: I/SampleChooserActivity(2648): Examinig decoder: OMX.amlogic.avc.decoder.awesome
08-18 02:36:02.487: I/SampleChooserActivity(2648): supports adaptive playback: false
```

@ojw28 Will be great if in future have prepared some snippets like this to people check they system before create some issue, or maybe they execute your snippet when your guys needs (I know this is special case, but of course can helps).
Check CodecCapabilities.

```
int numCodecs = MediaCodecList.getCodecCount();
for (int i = 0; i < numCodecs; i++) {
    MediaCodecInfo codecInfo = MediaCodecList.getCodecInfoAt(i);
    String name = codecInfo.getName();
    Log.i(TAG, "Examinig " + (codecInfo.isEncoder() ? "encoder" : "decoder") + ": " + name);
    for(String type: codecInfo.getSupportedTypes()) {
        boolean ap = codecInfo.getCapabilitiesForType(type).isFeatureSupported(MediaCodecInfo.CodecCapabilities.FEATURE_AdaptivePlayback);
        Log.i(TAG, "supports adaptive playback: " + ap);
    }
}
```
 Finally i work directly with AmLogic staff, i was in they office in China and now i know exactly what happens, they just don't have Adaptive decoder! They have problems in they old software/firmware (and actual published) versions, but very soon they will be perfect, better than many another manufactor (I tested that by my self)

**SO: This is no EXOPLAYER Issue!**

Sorry by that! Thanks for the support!.

For all guys, i recommend for moment use another hardware, 3-6 months later, use Amlogic but check if you have last AmLogic Software, if not you will still have this problem.

Thanks @brianchu @zarej 
  I don't think the message about `isSizeAndRateSupportedV21` is relevant. It's just an informational message that the method wasn't found. It doesn't indicate that the library actually tried calling the method (which is when it would be a problem - you'd get a proper full stack trace if that were to happen).
 That sounds working as intended in that case. When the player fails in this scenario, you should be able to figure out that the failure was due to network connectivity (e.g. by looking at the cause of the playback failure exception). You can/should then show the user a corresponding error message.
  If you know up front that your media doesn't contain certain media types, you can pass null instead of creating renderers. In the case that you don't know up front, but it turns out to be the case that a certain track type isn't present, the renderer will simply transition to the IGNORED state at pretty much zero cost. So I don't think there's any need to enhance the design here.
 Caveat - If you do avoid creating a video renderer, for example, then you do need to adjust the fourth parameter passed to ExtractorSampleSource to be 1 instead of 2. It's simpler just to create all of the renderers and allow them to transition to the IGNORED state, as is currently the case.
  This will be fixed by #483, I think.
 Note - This only affects audio-only HLS, as far as I'm aware. Please correct me if I'm wrong. We're working on putting better regression testing mechanisms in place to stop this kind of thing from happening; apologies!
  Please provide more information. The last line that you paste is probably the first line of a full stack trace (and just the first line isn't useful).
  Urgh. Thanks. Will fix this!
  I'm not sure what this question is asking. You don't need to manually parse the HLS manifest; HLS and DASH work in much the same way in ExoPlayer (and both are adaptive).
 In DashRendererBuilder of the demo app there's an array called videoRepresentationIndices that contains the allowed format indices. In HlsRendererBuilder there's variantIndices, which does the same thing. You should be able to replace those with arrays of length 1 that contain just the index of the format you want.

Alternatively, for DASH, you can implement a FormatEvaluator that doesn't change quality (we're working on hooking this up to HLS too, but we're not there yet).
  If this is just a question, then I think the answer is yes? If you're implying something is missing that means you can't do this, can you specify what :)?
 Closing this for now, because it's unclear what it's actually tracking.
  Thanks for the reminder. This has been done now.
  Using I-Frame only variants looks pretty optional to me. Marking as enhancement, but probably wont happen any time soon.
  The problem is that DefaultUriDataSource now only allows specific URI schemes, and icy isn't one of them because it's not supported directly by the library. I'm not really sure what the "right" solution to this is, but to get things to work in the meantime you should either:
- Use DefaultHttpDataSource directly, instead of DefaultUriDataSource.
- Add a `SCHEME_ICY.equals(scheme)` check to select DefaultHttpDataSource at [this line](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/DefaultUriDataSource.java#L144).
 Great. For the next release, I think it makes sense if we have DefaultUriDataSource delegate to DefaultHttpDataSource by default if it doesn't recognize the scheme (which would have worked for this case).
  DVR functionality isn't supported yet (an enhancement request to add support is here: #87). The player should already be seeking to the live edge of the stream for you when playback starts, and any seek on the player will also seek to the live edge (the value passed is currently ignored, although this will change once #87 is implemented).

So in short, the player should already be giving you the behavior that you want.
 Ah, perhaps what you mean is that you want to allow the user to pause, but when the user resumes you want the playback to jump to the live edge again. You can achieve that by calling seek(0) when playback resumes (because any seek will seek to the live edge).

Alternatively you can call ExoPlayer.stop() when the user pauses, and ExoPlayer.prepare() when the user resumes. This will be more bandwidth efficient because it wont download any media after the user pauses, but is also a bit more effort.
 Hm, yeah, I don't think it behaves quite like it's supposed to. It's probably seeking to the live edge, but in a stale playlist rather than an up-to-date one. I'll take a look.

---

Not related to this issue but responding to your other questions (please move further discussion elsewhere):
- There's no buffering to flash functionality at present. #420 is somewhat related. I'm not sure how I feel about making it really easy to do this, because if it's there then it's really tempting for every application to optimize for themselves (i.e. downloading more data than the user is likely to consume and/or filling up disk space) at the cost of overall device health. We'll continue to think about it, anyway.
- You can control the maximum buffer size for each player, as demonstrated [here](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java#L53). If you have multiple videos visible at once I'd hope they're not particularly high resolution, in which case you can probably use a significantly smaller limit per instance.
 Not currently. Preparing the player again is pretty cheap, given you're going to be discarding the buffer to jump to a different position anyway. I'd suggest for now you call stop() when the user pauses, and prepare() when the user resumes.
 I think this has been fixed in `dev-v2`, although the fixes haven't landed in `release-v2` yet. In any case, fixing this issue has been rolled into the larger task of supporting seeking in HLS, which is tracked by #87. Closing this issue. Please follow #87 instead. Thanks!  What are you passing as the dataSource when you make your ExtractorSampleSource? You're either not passing the right thing, or you're not formatting your local URI correctly. Covering both points:
- Use DefaultUriDataSource, which will work for both local and network URIs
- Your URI should be formatted to start with file:///
 Note: DefaultUriDataSource no longer requires file:// (on the dev branch). If you omit a scheme completely, we'll assume file://.
  This is not currently supported.
  Thanks. We'll push a change shortly that makes this usable via DefaultUriDataSource as well (DefaultUriDataSource will take http/https/asset/content Uris and delegate to the correct more-specific DataSource implementation).
 Yeah, I think we should probably assume file:// if the scheme is empty.
  We saw similar behaviour on Tesco's HUDL device, and fixed it with a workaround detailed [here](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecTrackRenderer.java#L954). 

In the demo app (use a new release, like current master or dev branch), do you observe that DASH playbacks freeze quite quickly when they go to switch up to a higher format? Do you also observe that playbacks never transition to the ENDED state once playback has finished (you'll see the elapsed time continue to increase indefinitely)? If so, then it's likely to be the same issue.

If you make the method linked above so that it returns true on the affected device (i.e. indicating that the workaround is required) then you'll probably find that things start working correctly. If so, we should add a line there to enable the workaround for the problematic decoder on the affected device.
 The description of the change you reference describes what it fixes. The second bullet point in that description is about waiting for all buffers to be output from a decoder before it's released to instantiate the next one (previously the code would tear down the decoder without waiting for this to happen, which would result in some frames being dropped).

codecNeedsEndOfStreamWorkaround is there to work around issues related to the change. It's not enough to just have that code. You need to modify the method to return true on the problematic device. Currently it only returns true on HUDL devices.
 Does that change resolve the issue on your device? We don't know for sure what devices or API levels it affects, but it makes sense to target the rule as specifically as possible and expand it as needed.

It's possible (although unproven) that it affects all API level 17 devices using the OMX.rk.video_decoder.avc decoder.
 This should be fixed on dev by the change above. Please test, and let us know if this is not the case.
 Yeah, probably. We'll discuss + probably push a change to dev to do that. Thanks!
  You need to create a new Mp4Extractor for each playback, rather than re-using the same one. Please try and see if this fixes the problem for you.
 What does it mean to change a source?
 I don't really understand. How would you achieve that in code? How is it different to playing a new MP4?
  You don't need to use Role. To get this to work, you'll need to change DashRendererBuilder to consider all audio adaptation sets, rather than just the first one (which is what it does now).
  If the resolution is unknown then the RESOLUTION tag should be excluded from the playlist. Including it explicitly indicates that the resolution is 0x0, which is invalid.

We'll fix ExoPlayer to treat 0x0 as unknown, since it's easy to do, but I think whoever is generating this playlist file should fix the playlist to exclude the tag regardless.
 Should do yes.
  This is already answered in #455. A YouTube embed URL is not a media file. It's an html web page. There is no way for any third party media player to play that. You need a web browser.
  By default, ExoPlayer's DefaultHttpDataSource sits on top of HttpUrlConnection in the platform, and the platform provides the network stack underneath that.
- Recent Android releases have used okhttp as the underlying stack, which has support for spdy. I'm not sure whether it automatically enables it where possible though.
- For quic you'd need to use a different network stack, at least at the moment. It's certainly possible to support quic by implementing your own HttpDataSource implementation that bridges onto a different network stack. The obvious choice for quic being Cronet.
  We don't support RTMP, sorry.
  If it runs on a real device (which it does) then the bug is in the emulator, not in ExoPlayer. You should file a bug there instead.
  - The initial vimeo URL you posted plays fine for me with TYPE_MP4.
- YouTube embed URLs aren't valid things that standalone media players can play (unless they just open a browser or WebView to play them).
  The two errors are completely different. The Nexus 4 is probably in a bad state (possibly due to another app misbehaving and not properly releasing the decoder). Does power cycling the device resolve the issue? If not, what build of Android is it running, and does the issue reproduce using the standard ExoPlayer demo app?
 On the Nexus 4, what's the exact Android build? Go to Settings -> About phone, and copy everything from "Build number".
 Hm. I tried Nexus 4 running Android 5.0 (LRX21T) and a MicroMax Canvas 4, and experienced no issues playing any of the HLS demo videos in the demo app.
 Works fine on my Nexus 4 in the demo app. What's the exact Android build? Go to Settings -> About phone, and copy everything from "Build number".
 I think we need to put this down to the test device being in a bad state. Closing as obsolete because realistically we're not going to be able to dedicate any more time to looking at this issue.  TextureView may well work for this purpose, although does consume more power. I've also heard anecdotal evidence that calling SurfaceView.getHolder().setFixedSize() may resolve the issue, although I'm not entirely sure what values should be passed. Might be something to experiment with in any case.
  Same as #452 
  The explanation is added in build.gradle too on the dev branch.
 The main dependency is the use of MediaCodec, which requires API level 16 and is used for both audio and video.
 You can replace pretty much all of the standard components in ExoPlayer with other implementations, so the library is useful for API level 9. For example it's possible to implement your own TrackRenderer that doesn't use MediaCodec, and hooks into a software decoder that you bundle into your apk. If you were to do that, you could go back to API level 9 and happily play both audio and video (I don't recommend you bother trying to do this; this is just an example).
  This is tracked by #420
  The player is behaving correctly. This manifest declares itself as a dynamic (i.e. live) presentation that started in 2014-10-14. Approximate 17727973 segment durations had elapsed since then. The player is trying to join the live end of the stream and so is requesting segments starting at this segment number, which is the right thing to do.

By overriding the manifest to declare itself as static I was able to see the first frame of video. Something else went wrong. I think there are issues with this stream in general.
 I'm 99% sure it's not correct. If it's not a live stream then it shouldn't declare itself as such. If it is, then it should make the segments available.
  ExoPlayer only supports DASH using demuxed (i.e. single track) fragmented mp4. Having audio and video muxed in the same stream has many disadvantages with respect to flexibility, and no real advantages that I can think of. So you should have mp4box output demuxed streams instead.
 It's very low priority, because all major streaming services we know of that are moving to DASH are (rightly) using de-muxed streams.
 No. The use cases for this seem to be pretty niche to me. It's nearly always better to deliver audio and video separately.
 On what evidence are you basing your statement that it's commonly used? To effectively deliver DASH content the broadcaster should be taking the feed and transcoding it into multiple qualities for delivery. It looks like they're already doing this, from the manifest you provide. They should demux the audio and video into separate streams at the same time, and deliver them as separate components. This shouldn't be too difficult, and the broadcasters that we've worked with directly all do this. There are multiple benefits of this approach.

Note also that many Android devices wont be capable of playing 18Mbit/s 4K 60fps video streams. Unless they're targeting very specific devices, they'll need to be providing lower resolution 30fps transcodes as well.
  This makes it possible to avoid having to know the container type up front (which is currently required, since you have to inject the correct Extractor implementation into the ExtractorSampleSource constructor).
 @andrewlewis Is working on the ExtractorSampleSource side of things. Should be implemented relatively soon. We don't have any plans to auto-detect HLS/SS/DASH at this point in time.
 This is implemented in dev.
  I don't think TrackInfo is the right class to be using. What's your use case (HLS / SmoothStreaming / DASH / SomethingElse), and where are your multiple tracks coming from? Thanks!
 Note that we've been enhancing the WebmExtractor to also parse MKV.
 I'm unclear exactly where we are with this issue. In the dev branch a lot has changed since it was first raised. We're now exposing the MediaFormat of each track directly through the ExoPlayer interface. We populate the MediaFormat language when using FrameworkSampleSource (although please don't use FrameworkSampleSource unless you really need to), and also for text/subtitle tracks when using ExtractorSampleSource to play mp4 and fragmented mp4.

It will be trivial to support language for audio tracks for mp4 and fragmented mp4 too, if you can provide sample media that contains this type of track information. The same applies for audio and text tracks when playing MKV and WebM files using ExtractorSampleSource.

Please provide sample files if you have some, and we'll make these enhancements. Otherwise I propose closing this issue for now, until sample files can be provided. Thanks!
 The only remaining work here is to propagate language for audio tracks.
  It definitely does on the dev branch. BandwidthMeter is used in the same way for HLS as it is for DASH.
 You'll need to provide more information about exactly what you're doing and what you're not seeing that you expect to see.
  What's the actual use case for placing a video in the application resources? Can you put it in the assets directory instead? In which case you should be able to read it using AssetDataSource (not UriDataSource or DefaultHttpDataSource).
 The whole model for playing MP4 has changed (from relying on MediaExtractor to instead relying on Mp4Extractor and ExtractorSampleSource). They use completely different code paths for loading data. So that probably accounts for the change in behavior.
 Closing this as it sounds like you have a solution.
 We recently added [RawResourceDataSource](https://github.com/google/ExoPlayer/blob/release-v2/library/src/main/java/com/google/android/exoplayer2/upstream/RawResourceDataSource.java) to v2, which will allow you to play files in the raw directory.
  It's possible to build 360 video support on top of ExoPlayer, by rendering a spherical video into a GL texture and then presumably rendering that texture onto a sphere (I'm not familiar with the details). I don't think there are any near term plans to add support directly to the open source project.
 We plan to support 360, but we don't have an ETA at this point. We will not be able to provide guidance until we get around to adding official support. As per above, we are not able to provide guidance until we provide official support. We don't have sufficient time to do this currently.  There's nothing actionable in this issue. If your problem still occurs on 1.5.0 or later, please file a new issue containing all of the information described [here](https://google.github.io/ExoPlayer/2015/10/01/reporting-issues.html). Thanks.
  Thanks for this. You need to sign the CLA before we can merge. See: https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
 Thanks!
  You should use ExtractorSampleSource together with Mp4Extractor. When building your ExtractorSampleSource you'll need to pass a DataSource, which is where the data is requested from. You can implement a custom DataSource to perform whatever decryption you require.
 Use the dev branch. We'll be pushing all changes from dev -> master late this week or early next.
 You're implementing the DataSource interface. A DataSource can be opened and closed multiple times to read ranges of the data, as specified by the DataSpec objects that are passed to DataSource.open. You'll need to spend some time familiarizing yourself with these two classes, and it's probably helpful to look at some of the implementations provided by the library to see how they're implemented.
  They work fine for me (on the dev branch at least). What build are you running, exactly? You can find out by going to Settings -> About phone, scrolling to the bottom. Everything under "Build number" is relevant.
 And which video(s) are you playing exactly? If you reboot the device does the problem persist? Thanks!
 Sometimes devices get stuck in bad states, unfortunately. I'm not sure there's much we can do to work around it :(.
  It's only a warning in the logs, right? What's actually happening here is that one of ExoPlayer's loading threads is notifying the playback thread after the playback thread has exited (because you called release() on the player). This is working as intended, and the warning can be safely ignored.

It's slightly unfortunate that the platform chooses to print this warning at all, since there are cases such as this one where the behavior is intentional and expected.
 We can probably suppress it in most cases; I'll take a look at doing that. It's not possible to suppress it in 100% of cases though (doing this would require the playback thread to potentially block on network reads, which violates the threading model and may lead to application ANRs).
 I don't think any of these solutions are completely ideal. I'm inclined to do nothing, given there's no actual adverse affects to the user or application. The best long term solution would be a platform change to allow these kind of warnings to be turned off. I'll follow up internally about that (obviously it would only help in new platform releases).
 Please clarify why it's _extremely_ annoying? It's not like it spams logcat at high frequency or anything. Why don't you just ignore it?
 To clarify, are these app specific logs, or do you mean general logcat? How are you capturing these logs?
 I know what the logging looks like. I'm asking where you're getting these logs from and why you can't just ignore the message? What is the workflow you have that is disrupted?

Overloading sounds like a bit of an exaggeration. The only time this message is printed is when the player is released (i.e. once per playback or similar). It's hard to argue that fixing this is more important than fixing pretty much anything that results in user visible issues.
 I don't think it's fixed size per application. I think it's fixed size shared by all applications, and given the amount of log spam other processes tend to produce the errors discussed here most likely have very minimal impact on the amount of useful logging that you're able to capture.

If you believe the above to be incorrect, could you please point to the documentation that describes a per application fixed size log buffer, and also explain what method/api you're using to read the logs.
 @Bidp - This is correct, but it's only a problem if you try and re-use an `Allocator` across multiple instances of `ExoPlayer` over time, or if you continue to hold references to released `ExoPlayer` instances. Provided you instantiate a new `Allocator` instance for each player, and don't have references to released `ExoPlayer` instances, all memory should be garbage collected correctly.

We've fixed this issue in ExoPlayer 2.x (not yet available on GitHub). We have no plans to backport the change to 1.x.
 This is fixed in the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental).
 Fixed in `dev-v2`, which is the V2 development branch.
  Hmm. Do they work on master? Are you sure that there isn't a redirect loop (which would cause this to occur after some limit on the number of redirects is reached)?
 HttpURLConnection doesn't follow redirects that change the protocol. See [here](http://developer.android.com/reference/java/net/HttpURLConnection.html), in particular: "This implementation doesn't follow redirects from HTTPS to HTTP or vice versa."

Whether we should follow them ourselves is an open question. I suspect we probably should, but it needs more thought.
 Note that this is currently breaking the "Dizzy (https->http redirect)" sample in the ExoPlayer demo app on the dev branch.
 This is now supported, but you have to turn it on explicitly when creating a HttpDataSource or UriDataSource (there are new constructors that have an allowCrossDomainRedirects argument).
  If you're playing MP3 files then you should use Mp3Extractor on the dev branch. It's going to be a lot more reliable across all devices than anything that uses FrameworkSampleExtractor.
 The demo app demonstrates this (the sample has type TYPE_MP3). You'll need to prepare the player something like:

```
// Build the video and audio renderers.
DataSource dataSource = new DefaultUriDataSource(userAgent, null);
Mp3Extractor extractor = new Mp3Extractor();
ExtractorSampleSource sampleSource =
    new ExtractorSampleSource(uri, dataSource, extractor, 1, BUFFER_SIZE);
MediaCodecAudioTrackRenderer audioRenderer =
    new MediaCodecAudioTrackRenderer(sampleSource, null, true);
exoPlayer.prepare(audioRenderer);
```
  In a real application you would want a (foreground) service to prevent process death, yes. We're trying to make the demo app simpler though, not more complicated, so we're unlikely to be doing this within the demo.
  ExoCache is designed specifically for DASH implementations where every request can be in the form of a bounded range request (which in practice means single-segment on-demand streams containing sidx boxes). It doesn't work for anything else (DASH where streams are segmented in the manifest, SmoothStreaming, HLS, ExtractorSampleSource). There are three things to fix:
1. We don't support unbounded range requests. This is because the cache currently has no concept of the length of a piece of data. If we have a file in the cache with byte range [0-1000], and make a request for range [0-*], we currently don't know what to do after we've return the first 1000 bytes from the cache. We don't want to make an unconditional request to the network starting at offset=1000 because it's inefficient, wont work in the offline case, and because the server may return a 416 (unsatisfiable byte-range) in the case that the content really is 1000 bytes long, which would be awkward to handle.
2. It's probably more common for servers that are configured to serve every chunk from a separate URL to not support range requests. We may need the cache to support a "request the whole chunk or nothing from the upstream source" mode for this use case. I'm not sure how we'd decide when to turn it on. Awkward.
3. The cache currently indexes: contentId->Tree[byte-range->file]. For chunks that are requested from different URLs, we need the Tree to be indexed by chunk-index+byte-range, or perhaps just chunk-index depending on the answer to (2).
 Because if you've cached the whole of a media stream under some hash h(uri, 0, length_of_stream), then you'll get a cache miss for every request to for the same uri unless you happen to be requesting from position 0 for the whole stream, even though you have this data cached already.
 The point of this bug is to generalize support for all use cases. What you're doing only optimizes for a subset of cases (from your description, I'm assuming either HLS or a particular variant of DASH/SmoothStreaming). It likely also breaks if a read error occurs part way through a request (subsequent read attempts probably get a cache hit to the incomplete data).
 @kinsleykajiva the link is fine if you copy and paste the URL as text (it's just that it's linked incorrectly).
@qqli007 is correct about `dev-v2`. We recently pushed support for caching for regular media files, provided you specify `FLAG_CACHE_UNBOUNDED_REQUESTS` as in his sample code. 
 @Anshumansharma12 it isn't possible but you can always download the file yourself and play it using a FileDataSource.
 @erdemguven - Do we cache data if even the resolved length is unknown (this is likely a result of the mpd response being gzip'd) and/or have a solution to this?

@danrossi - I think it would be fairer to call the code necessarily complex ;). There are many small nuances to caching media properly, particularly when considering things like caching of non-completed requests. As an aside, you probably shouldn't be using the caching components as-is to build a proper DASH offlining solution. A proper DASH offlining solution would more than likely involve explicitly downloading only a single representation (and not the manifest, although you might optionally recreate an offline manifest containing only the single representation downloaded). The caching components can be used as part of an implementation for this, but the actual downloading bit should probably not be implemented using a player instance.
 @ojw28 @danrossi - Unfortunately not. We skip the cache if the resolved length is unknown as the content is likely to be a live stream. I don't know how we can handle gzip'd responses.
 @danrossi We're working on full DASH offlining solution but there isn't anything for downloading yet. I don't know when it'll be ready. @matclayton if you mean the DASH downloader, basic functionality should be in github in 2 weeks time. @bnussey we decided to release it once the downloader service and manager are ready as major api changes might be necessary. It's hard to give an estimate now. You can follow it on #2643.  This use case isn't supported yet. It's tracked by #420.
  This isn't something we plan to support any time soon; sorry. Although if you use TextureView, presumably you just have the current frame already in the texture (and can read it back using OpenGL).
  Hm. Doesn't this depend on how you use BUFFER_FLAG_END_OF_STREAM when queuing input buffers? If you set it on the final real input buffer then I'd expect to see it when dequeuing the final real output buffer. If you set it separately on an input buffer that follows the final real one, then I wouldn't expect that. ExoPlayer does the latter, hence why we don't handle the output buffer on which it is set.

If you have a concrete example (media + device combination) where this happens, then we can/should fix it. I'm not yet convinced there's a problem though ;).
 Please re-open if you manage to repro this. Thanks!
 Turns out the standard Google AAC decoder does this, so we end up dropping the last frame of audio. Not particularly important, but we should fix it nevertheless.
  @Ood-Tsen 

Hi. Fancy sending another pull request containing ContentDataSource? We'd love to merge it; I was just a bit slow at getting around to looking at it. Note that you'll also need to electronically submit a CLA for us to complete the merge. See here: https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md

Thanks!
  Yeah, you should call getCurrentPosition() when you need the position. If you need it periodically then call it periodically.

It's better to have the application code control this kind of thing, rather than the player, because the application code knows what it actually needs the position for / how frequently it needs it / when it no longer needs it etc.
  You should be able to call exoPlayer.stop() and then exoPlayer.prepare() with a renderer that you've built for the next URI.
  Supported as of https://github.com/google/ExoPlayer/pull/465
  This is (more or less) tracked by #420.
  I don't think that code imposes any expectation on the video frame-rate. It just says that a frame should be dropped if we're more than 30ms late rendering it. Even if the video were 1fps, we should still be able to render each frame on time, and no more than 30ms late. Meeting this requirement is significantly easier for lower fps content, because less work needs to be done.

So I think something else is wrong either with the player or the media. It may well be that FrameworkSampleSource doesn't provide the video frames early enough. Changing the condition as you have done will fix the frames being dropped, but they'll still be rendered later than they're supposed to be. You should try switching to ExtractorSampleSource with Mp4Extractor on the dev branch. My hunch is that you'll find it works fine.

If all else fails, please provide the media, if possible.
 Marking as closed for now. Feel free to re-open if you see the same with Mp4Extractor (although if that's the case, we'll probably need the media to reproduce).
  We'll aim for ~2 weeks from now.
  We transition from PREPARING to BUFFERING once we've discovered the media format for each of the tracks (i.e. audio and video). If PREPARING is taking a long time, that probably means the first chunk doesn't contain format information near the beginning, which typically suggests poor interleaving of audio and video in the chunk. If so, that's working as intended.

Why is PREPARING taking a long time actually an issue? I would have thought you'd treat it similarly for BUFFERING anyway (e.g. for the purposes of deciding whether to show a loading spinner, or something).
 Interesting. We're looking for an IDR frame and not finding one until ~8.5MB into the data. This chunk starts with an IFR frame with slice type 7 (I slice). The player doesn't consider it as a random access point.

I don't suppose you know exactly what properties this type of slice has. We've been discussing internally and our initial interpretation was that it could refer to data before itself, meaning it's (at least in theory) not a suitable starting point for decode. We have seen this issue elsewhere though, and it appears VLC plays this segment correctly.
 If we do interpret the IFR frame with slice type 7 as a keyframe then the segment plays correctly. We're just worried that this might not always be true, because we can't find that guarantee in a spec anywhere (if it is somewhere in a relevant spec, that would be very useful information to have).
 Pretty sure it is. See [here](https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/extractor/ts/H264Reader.java#L135). We don't play anything before the first keyframe, so if there aren't any IDR frames we wont play anything.
 We're thinking about how best to handle this case. For now just handle it locally, but given we've seen at least one other example of this too, and that other players seem to handle it better, it's certainly possible that we'll figure out a way to provide support.
 - Yeah, we have a patch that looks pretty much the same as that one. We're worried it'll cause video corruption for other content though; hence why we're thinking about it.
- There's also a chance that parseSliceType will read off the end of the data and crash in the code above. There's no guarantee about how data is divided up across calls to consume, so you need to handle the case where the slice type spans the boundary between the data passed in two (or theoretically multiple) calls.
 I'd argue that the correct approach is to always put a video keyframe at the start of every segment, which avoids the problem of the player playing out the audio that comes before it. Both DASH and SmoothStreaming guarantee this property. It's a shame that HLS doesn't, but my understanding is that many broadcasters ensure that the property holds for their streams. Note that you also need to ensure this property to avoid having to download-and-splice overlapping segments in two qualities whenever you want to do a seamless quality switch, which is very inefficient, particularly if you're trying to switch to a lower quality due to bandwidth constraints.

I don't think we're planning to change the behavior just for HLS, particularly given that not putting a keyframe at the start of every segment is generally a bad thing to do.
 Are these live TS streams then? If you have a test stream that would be really helpful (no need for a manifest or anything; it's actually quite easy to hack around in ExoPlayer to get it to play TS directly).
 Hi. The dev branch now allows you to pass an additional argument (idrKeyframesOnly) into TsExtractor. If you do so (pass false), you should find that you're now able to play this type of content. Please take a look and see if it works for you.
 Can you confirm it works correctly for the sample you linked further up this issue, at least? It works correctly for me (once I trim the file to be a whole number of TS packets). If you don't have that working then I'm obviously doing something different to what you're doing.
  This looks great, thanks! Please tweak as per the suggestions. Other than that we can merge this, but you'll need to electronically sign a CLA first. See: https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md.
 Thanks!
  I think the contents of a ContentProtection element are scheme/type specific. Could you paste just the ContentProtection part of the manifest that you're trying to use, and we'll take a look? Thanks!
 - Ah, you're right about this applying to all schemes. Apparently the Section 11 of the Second Edition of ISO/IEC FDIS 23001-7:2014 defines this element for all ContentProtection descriptors.
- There's an interoperability recommendation made by the DASH IF that ContentProtection should only exist at the AdaptationSet level, and not at the Representation level. What tool did you use to generate the manifest? I know that edash-packager doesn't currently adhere to the recommendation, but my understanding is that it will be updated to do so soon. If you used another tool, it's worth pointing this out to them.
- MediaPresentationDescriptionParser does already handle bubbling up of ContentProtection elements from Representations to their parent AdaptationSets, provided they're consistent within an AdaptationSet, which you'll probably find to be the case for your example (if it's not you have bigger problems).

So there's not actually much work needed here. It's more or less just a case of parsing the PSSH box from the element, pulling out the data for the scheme UUID, and setting it as ContentProtection.data. Everything else is plumbed in already, so doing that should result in things working correctly.
 Can you paste in an example contents of the cenc:pssh element?
 Do you happen to know what license server hands out licenses for that (assuming it's test content)? I've piped the data through (not yet pushed, but I have it locally), but can't test it without this information.
 It should just be a case of doing this: https://github.com/google/ExoPlayer/commit/089aafa526c28c649ba202b222a8b6eb32f34038

Please give it a try locally, and let me know if it works as expected!
 Thanks. I will merge a slight variant of that commit soon (there needs to be a null check in the DashChunkSource object to make sure pssh in the manifest is given preference).
 @Pierpaolo1992 You'd be best off asking questions not related to ExoPlayer on either StackOverflow or some other tracker that's actually relevant.  Thanks for raising this issue. The new Extractor model in the dev branch actually allows us to support this pretty easily (which was not the case previously). We'll get this done.
  Some devices had an issue when setting maximum video dimensions on the decoder in conjuction with using TextureView. The problem is discussed in https://github.com/google/ExoPlayer/issues/58. Unfortunately, you probably need to use SurfaceView instead on the affected devices.
 Some devices rely on the maximum video dimensions being passed (so yes, downsides include the decoder crashing).
 You can't change it without recreating the decoder, which wouldn't be completely seamless. You can see what it looks like by removing calls to setMaxVideoDimensions and then also ensuring codecIsAdaptive is set to false in MediaCodecTrackRenderer.
 It doesn't really completely defeat the purpose of HLS. When I said "not completely seamless", what I meant is that you might get a few frames dropped across the format switch. It's still a much better experience than no adaptation, in which case you'll likely receive either sub-optimal quality or an elevated number of re-buffer events.
  Please provide more exact issues than "not working", and re-open. Thanks.
  Does FileDataSource work properly for resources? If so then fine and we can have UriDataSource use that. I suspect it doesn't, however. It looks to me like Android resources aren't designed to allow random access, which will make it difficult to play them efficiently (although if it's a resource I'd hope it's small, so perhaps reading the whole thing repeatedly isn't much of an overhead).
 See e.g. http://www.codeitive.com/0zigWWgeVq/android-randomaccessfile-usage-from-resource.html, which essentially recommends copying the resource into the cache directory so that it's a normal file, and then using it from there.
 It's probably possible to build a DataSource for loading assets directly, using this:

http://developer.android.com/reference/android/content/res/AssetManager.html#open(java.lang.String, int)

with accessMode = ACCESS_RANDOM. We might look at this at some point, but I'm not sure it's high priority. If you feel like looking yourself, we'd take a pull request with a DataSource that adds this!
 @dnutcracker Did you make any progress with this yet? If not, I will try to get it done today.
  "certain devices" isn't sufficient information to even start looking at this. Which devices, specifically? Which extractor are you using? What audio file?
 I suggest you use ExtractorSampleSource with Mp4Extractor and Mp3Extractor instead. The platform underneath FrameworkSampleSource is pretty buggy, and we're trying to move away from it. I'd like to delete it completely at some point, since it's caused us a significant number of device specific issues.
 Not yet. Automatic sniffing of media type is on our list of things to do though. In the meantime, you may be able to use: http://developer.android.com/reference/android/media/MediaMetadataRetriever.html.
 Provide a sample file and I'll try and look locally on an S6.
 Closing due to lack of information.
  Closing due to inactivity. It's unclear whether this is still an problem. Please file a new issue containing all of the information requested in the issue template (**including a full bug report**) if you still see this using ExoPlayer 1.5.8 or later. Thanks!
  Nice find; thanks! On the dev branch this code looks a bit different. It looks like it would have the same issue, but it also looks like it's easy to fix. There's this at line 265:

`formatIndex = nextFormatIndex;`

Fixing the issue should be as simple as moving that line down to be immediately before line 338, which is where it's first used. Could you confirm that this change fixes the problem for you on the dev branch? I can get a fix merged if that's the case. We're also finalizing a release back to master, so if we get it in soon it'll get picked up as part of that.
 Fixed in a different way as ref'd above. Please let me know if this doesn't fix the problem for you.
  It seems unlikely this is a library issue, unless it's specific to this library and importing any other project works. StackOverflow or some Android Studio resource is probably a better place to ask this question.
 There's no real distinction between a third party library and this one, for the purposes of asking a question. What I'm really saying is (a) I don't know because it works for me, and (b) you're much likely to get an answer if you ask on StackOverflow and pose it as a general "can't import project" problem, since there's a bigger footfall of people there. Probably also worth looking at what the IDE log actually says, seeing as that's what the error message suggests you do.

It feels unlikely to be a problem with the project, since as far as I know no-one else has encountered this issue.
 At least look in the IDE log to see what actually failed. Just saying it must be an issue here isn't actually going to help anyone answer the question.
 - Most likely one of dependencies required a server that was temporarily unavailable, would be my guess.
- The first question is so generic that it's not possible to answer. You need to file a specific issue that says exactly what you're seeing, on what media, and on what device. Ideally with logs. There are brief pauses on resolution switching on some videos if the device doesn't have an adaptive decoder. If you don't want that, you'd need to configure to not resolution switch.
- The demo app provides a base that you could modify to suit your needs. It is easy to customize if you dig around a bit and figure out how things work.
  I don't think this is related to retries. I doubt there's any retrying going on here. Looks like something is just broken. It may be device specific. I'll try and get hold of one and take a look.

Out of interest, do other samples in the demo app play properly on this device?
 Is this still repeatable for you using the dev branch? I tried with a Galaxy S3 Mini and it worked fine for me on that. If you can still reproduce the issue using the latest dev branch, please go to "Settings->About Device" on the device, and provide (a) Model Number, (b) Android Version, (c) Build number (in full). Thanks!
 @zondaOf2012 - Your issue is different, in that the media you've linked to is bad. This is why you (probably) only see the issue for the specific video you've linked to, and none of the HLS samples in the demo app.

I didn't look in detail at the media, but it looks like there's probably a discontinuity in the TS timestamps (i.e. the timestamps reset back to 0 in the second segment, rather than continuing on from the ones in the first segment like they should do). This is invalid in HLS unless an #EXT-X-DISCONTINUITY tag is inserted between the two segments. Inserting such a tag appears the fix the problem when I simulated it locally.
 This should be fixed on API level 16. We're not aware of the issue affecting any other API levels, but if you discover that other API levels are affected, please let us know.
 @icastell I wasn't able to reproduce the issue you described using the ExoPlayer demo app on a GT-I9100 running 4.1.2. Any idea whether this affects only some of the devices mentioned on this issue? Can you reproduce it in the demo app?

Does your app hold a WakeLock and WifiLock while playing audio in the background? If not, please try that.
 Audio plays correctly when I follow those steps, so it looks like the GT-I9100 is not affected.

I'm trying to get hold of a GT-I9300 running 4.1.2 to replicate your setup.
 @icastell I couldn't reproduce this on samsung/m0xx/m0:4.1.2/JZO54K/I9300XXEMG4:user/release-keys either, following the steps above: the MP3 plays fine in the background (screen off) from the ExoPlayer demo app.

There are several versions of this device, so please could you paste the output of `adb shell getprop ro.build.fingerprint` so we can check that the devices are the same?
 I've tried both Google Play (MP3 Audio) and the URL you pasted above, and ExoPlayer from the dev branch (which uses ExtractorSampleSource).

I wonder if this is related to the network. Could you try pushing the file locally and playing it from /sdcard/ or similar?

Can you think of anything else that could be different? It's a shot in the dark, but maybe try restarting the phone in case the mediaserver is somehow in a bad state, or even factory reset the device.
 Glad it seems to be fixed for now.

For what it's worth, if you search for Galaxy S3 background audio stuttering problems, there are a couple of cases of people describing that removing a SIM card fixed the issue. That may be worth a try if you can reproduce the issue again. If the problem relates to background data usage, it's unlikely we would be able to implement a workaround in ExoPlayer.
 Does this reproduce in the latest version of the ExoPlayer demo app using the link above? If so, please attach a full bug report captured with "adb bugreport" taken on the device shortly after the issue is encountered. Thanks.
 Done.
  I see the same issue as you do on master, but both samples work just fine on the dev branch. I suggest you try that. It's likely that we fixed whatever the underlying issue is.
 Actually, even when using the dev branch, they freeze after a while.
 The problem is either with the manifest itself, or somewhere much deeper in the Manifest parsing code (probably SegmentBase).java. I think the manifest might be invalid, but the spec is so ambiguous about how "startNumber" is supposed to work, it's hard to tell. I'll have to give it some more thought.
 Not sure what the status of this is, but the samples are no longer available. Closing. Please open a new issue if you're still seeing problems (referencing this one).
  Regarding this issue (and pull request). Can you point to any specification documents that explain exactly how MPD.Location is supposed to be used? It's defined in ISO 23009-1, but it doesn't seem to include anything about actually using it. I'm particularly interested in relative resolution of media URLs. In the case where the MPD doesn't define a BaseURL at the top level, we currently resolve URLs relatively against connectionUrl. If an MPD.Location is defined, it's unclear whether you should resolve URLs relatively against connectionUrl or against MPD.Location.
 Ok, I found a bit of information in the DASH Interoperability and DVB documents:

http://dashif.org/w/2015/04/DASH-IF-IOP-v3.0.pdf
http://www.dvb.org/resources/public/standards/a168_dvb-dash.pdf

They don't say anything about using MPD.Location for resolving media URLs for the current manifest, so I guess the connectionUrl should be used for that, and the MPD.Location should be used only when requesting the next manifest. This is what the pull request appears to do, so I guess we're in agreement!
 Yes. It would be nice to avoid having an MPD specific fetcher, primarily because you can equally well fetch the MPD with the general purpose fetcher. If you give people two ways of doing things, they're reasonably likely to pick the wrong one :).
 I just merged this into the dev branch. It's not in the master branch yet; that will take a few weeks at least.
 I reworked this feature a little in 164d8b4242903aea3b984fc72277366418279228 and 49f4fe7810c3e049278223558422813bd4a709c4 to contain the redirection logic with ManifestFetcher. Please check it still works for you. Thanks!
  Happy to pull this, but you'll need to sign the CLA as per [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md).
 See internal IM. Thanks!
  That doesn't sound right. Are you certain that blockingSendMessage is blocking _indefinitely_? It should only block until the message has been delivered, which shouldn't take long at all.
 I'm still not following this. There should be two threads involved here - Your application's main thread that you're calling into ExoPlayer with and that ExoPlayer calls your ExoPlayer.Listener instance on, and ExoPlayer's internal rendering thread.

When you call blockingSendMessage from your application thread, it will block the main thread temporarily until the message is delivered and processed by the background thread. This should block the thread for no more than ~50ms or so. During this time onPlayerStateChanged wont be called, but as soon as blockingSendMessage finishes any state changes will be delivered by the player through this method.

So the only thing you should be experiencing is that sometimes player state changes are delivered up to ~50ms delayed late. That really shouldn't be causing you any problems though.
  Sounds like a device specific issue. Since Kindle Fire tablets aren't official Android devices, we do not provide support. You'd have to report the issue to Amazon and get them to fix the underlying problem.
 We aim to support all devices that run Android. This means (roughly speaking) that the device has passed [Android CTS](https://source.android.com/compatibility/cts-intro.html) and that the device has been approved based on this. Any device that ships with Play Store is definitely in this category. There also exist devices that do not ship with Play Store that fall into this category. Amazon devices run FireOS, which is a fork of Android. To my knowledge they do not go through the approval process.

In any case, you'll need to report this to Amazon and ask that they fix the underlying problem.
 I gave OS5 a try and it seemed that the demo app now works correctly (so I would guess other ExoPlayer based apps will work correctly too).
 Marking wont-fix because I believe OS5 resolves the problem. Although given the issue is closed already, I guess that was a bit pointless :)!
 FireOS isn't Android. It's not really our job to discover and document compatibility issues with devices running other operating systems that attempt to support running of Android applications. If anything, it's up to the author of that operating system to document their compatibility issues.
  I can reproduce this on master, but not on the dev branch. So it's likely that this has been fixed already.
 Please verify that you also see correct behavior with the dev branch.
 No. The dev branch contains major changes, and it'll be quite a while before it gets merged into master (> 2 weeks). Sorry.
 The dev branch has API breaking changes. We don't guarantee API stability between releases - to do so would slow our ability to quickly evolve the player, particularly given the huge number APIs that the library defines.

The compiler error is actually in your own code (which was probably forked from the demo app, but has not been updated in the same way that the demo app code has been updated).
 Marking as closed because it doesn't reproduce on the dev branch.
  I'm confused about what you're trying to do.
- Is this for AndroidTV? Other Android devices don't normally have PlayReady support.
- You can't set the body as a property. But in any case, the body is supposed to be generated by the underlying PlayReady MediaDrm plugin.
- The failure in your log is a 404, indicating that whatever URL you end up trying to connect to doesn't exist.
 - Probably the 404 is due to the way you're trying to add the body, which is incorrect.
- You shouldn't put the XML body in the HashMap at all. The body is the second argument passed to executePost in SmoothStreamingTestMediaDrmCallback. You'll see that it obtains the body from the KeyRequest by calling request.getData(). If you inspect the binary data here and turn it into text, you'll see that this is actually already an XML formatted PlayReady key request. Are you sure it doesn't just work as it is?
- If it doesn't then you can try replacing the second argument with your request (in binary form). I'm not sure whether this would work. Alternatively, you may need to take the existing request, convert it to XML, inject the CustomData element that you need to add, convert it back into binary form, and use that.
 Unfortunately the PlayReady plugin doesn't support injection of CustomData at the moment, so unfortunately your options are:
1. Replace the XML with your own (converted into binary form) and see if that works. I'm not sure whether it will or not.
2. Parse the request.getData() data as XML, insert the CustomData yourself and convert back to binary again. This isn't really within the scope of ExoPlayer help. You'll need to use XML parsing APIs. This is probably a good place to start: http://developer.android.com/training/basics/network-ops/xml.html
 Turns out PlayReady does support CustomData, if you use the PRCustomData key. See the most recent replies on #339 for information.
  We just pushed a whole bunch of changes to dev, that will allow you to use TYPE_MP3 rather than rely on anything built on top of the (on many devices quite flaky) MediaExtractor API.

Please verify that using TYPE_MP3 works correctly for your case. If it does, you should use that instead.
 Note - Works fine for me.
 Ah right. This is because the server is using ICY rather than HTTP. There's some information about this here: https://code.google.com/p/aacdecoder-android/wiki/HandlingShoutcastStreams

I'm not sure how much we want to support this. It seems like a fairly bad way to be streaming live audio to mobile devices, v.s. something like HLS or DASH.
 - Glad to hear this is working for you.
- Please leave the issue open for now, whilst we assess what to do with it.
- I have a feeling you'll discover there are issues whenever a device hits a temporary network error and/or changes network (specifically, when WiFi disconnects and the device needs to switch to using the mobile network instead, or just when the device goes through a bit of a flaky network area). I'm pretty sure the right thing wont happen. The player may switch into a prolonged period of buffering, where it's reading and throwing away data. It seems to me that some of the same issues apply to this use case as apply in https://github.com/google/ExoPlayer/issues/227 (see the 6th message on that thread), which make it pretty unsuitable for mobile v.s. something like DASH or HLS.
 My point is that for this way of streaming, it seems impossible to recover in a completely seamless way. There doesn't appear to be a mechanism to request data from the server that continues from the point up to which you managed to load before the network error.

The problem we have is that the player is trying to recover, but the recovery mechanism that works for most streaming technologies doesn't work for this case, and so recovery doesn't work properly and the player goes into a bad state. The workaround would be to disable retries in the player, so that the app would always receive an error (and have to retry itself). Kind of sucks as an experience for the user though.
 The retry logic should work nicely for you now, and there shouldn't be discontinuities detected either. I'm going to close this, because I don't think we'll want to support ICE directly in the library (it's pretty easy to add support in a specific app if needed).
 Right. In general, the retry logic should now work well. I think that issue is something else, possibly something device specific. We'll take a look.
  - Which stream, specifically? Does this occur with the test streams?
- Are you sure you're not just trying to play video that's too high of a resolution for the device to handle? I'm not sure what the limits of that device are off the top of my head, but the fact that it's screen is only 480p suggests that you shouldn't be trying to play HD streams, for example.
 There are two parts to this:
1. Where the manifest includes optional RESOLUTION tags, we should automatically filter those that are at resolutions beyond that supported by the device.
2. It's (in my opinion) a flaw in the HLS spec that RESOLUTION tags are optional. When they're not included (as is the case in the example you give) the task is significantly more complicated. I'm not sure what we'll do about that case, if anything. Can you require that RESOLUTION tags are always included in the manifests that you're using? What is your use case, exactly?
 Ok, let's use this issue to track 1 then. 2 is a flaw in the HLS spec. I'm not sure how much effort we want to expend working around it (probably none).
 It's not implemented for HLS yet, but it is implemented for DASH and SmoothStreaming. It would be good if you could check that those samples do play correctly, and also validate that they don't switch up to high resolution streams (you can read the video height from the debug line above the player in the demo app).
 We have a fix for the DASH and SmoothStreaming case that introduces a VideoFormatSelectorUtil class that applies some better logic to figure out what formats the device will be able to play. It'll appear on dev branch tomorrow.

We'll then wire up HLS to the same util class, although note that filtering will only be possible in the case where the master playlist provides sufficient information (e.g. the width/height of each variant). Which is fine for the sample you pasted above, but is not mandated by the HLS spec.
 Where resolution information is available in the manifest, we filter as of https://github.com/google/ExoPlayer/commit/79cdd036828223bf2da29f8091252add27799aed.
  I think this should be fixed by https://github.com/google/ExoPlayer/commit/48826836d15d7ea57a4ea55fb48d1823b95a93e4, which relocated the tests under the androidTest directory. After this change I was able to run the tests in Android Studio. Please sync to latest dev and give it a try.

Marking as closed, but please reply here if you're still having issues, and we can open it again if needed.
  Is this the initial manifest fetch (i.e. in the context of the demo app, is it equivalent to the fetch performed by DashRendererBuilder)? Does it happen if you attempt to play using the demo app, or only in the context of your own app?

We definitely haven't seen this as a widespread issue (otherwise all videos would take ~8s to start, which isn't something people are observing).
  You should already be able to call ExoPlayer.seekTo before ExoPlayer.prepare. Does it not work?
 Yeah, I see. So calling seekTo before prepare does work (i.e. the end result is playback starts from the position you're trying to start from). It's just that preparation fetches the first part of the first chunk, where-as it should be possible to prepare using the chunk you're actually trying to start playback from.
 Yeah. Although even without in that scenario, it's wasteful in that we're downloading more than is necessary. The "proper" fix is to have the pending seek position propagated through prepare(). Touches quite a lot of files though, since it needs a change to both the TrackRenderer and SampleSource interfaces.

I think we'll try and get to this at some point fairly soon, but it's not massively high priority.
 I think I have a fix for this.
 It's always been best to call seekTo as early as possible. The change referenced above decreases start-up latency specifically in the HLS case where not starting from t=0. For other scenarios (e.g. DASH / SmoothStreaming / Anything else) performance is unchanged.
  We'll push a fix for this soon (eta later in the week).
 Note - The root issue is that the extractor fails when it encounters unsupported tracks in the stream (e.g. HINT tracks). The fix will be to skip them instead.
 Fixed in dev.
  We'll probably be doing this by moving the "player" package from the demo app to inside the library, as a higher level abstraction that covers playback for most of the common use cases. You'll still be able to use the lower level ExoPlayer interface directly for more advanced use cases.

Does that sound good/sensible to you?
 The new plan will likely be:
- Complete work on #514. This will remove several hundred lines from the demo app by moving multi-track logic inside of the core player library.
- Possibly move the renderer builders inside the library.
- Possibly move the async renderer build step inside the core ExoPlayer.

Details to be determined.
 The first bullet point above has been completed, and ~10% of the demo app code has been removed as a result. The second and third points are still open for investigation.
 The demo app has been significantly reduced in size in ExoPlayer V2. For V1 we had:

```
$ find v1/demo/src/main/java/ -name '*.java' | xargs wc -l
   207 v1/demo/src/main/java/com/google/android/exoplayer/demo/SampleChooserActivity.java
   268 v1/demo/src/main/java/com/google/android/exoplayer/demo/Samples.java
    65 v1/demo/src/main/java/com/google/android/exoplayer/demo/SmoothStreamingTestMediaDrmCallback.java
   213 v1/demo/src/main/java/com/google/android/exoplayer/demo/EventLogger.java
   199 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/HlsRendererBuilder.java
   204 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/SmoothStreamingRendererBuilder.java
   600 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/DemoPlayer.java
    88 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java
   266 v1/demo/src/main/java/com/google/android/exoplayer/demo/player/DashRendererBuilder.java
   762 v1/demo/src/main/java/com/google/android/exoplayer/demo/PlayerActivity.java
    62 v1/demo/src/main/java/com/google/android/exoplayer/demo/WidevineTestMediaDrmCallback.java
  2934 total
```

in V2 we have:

```
$ find v2/demo/src/main/java/ -name '*.java' | xargs wc -l
  419 v2/demo/src/main/java/com/google/android/exoplayer2/demo/SampleChooserActivity.java
  336 v2/demo/src/main/java/com/google/android/exoplayer2/demo/TrackSelectionHelper.java
  439 v2/demo/src/main/java/com/google/android/exoplayer2/demo/EventLogger.java
   24 v2/demo/src/main/java/com/google/android/exoplayer2/demo/DemoApplication.java
  574 v2/demo/src/main/java/com/google/android/exoplayer2/demo/PlayerActivity.java
 1792 total
```

If you ignore the files that are essentially just scaffolding (i.e. not interesting), the lines of code goes from 2246 to 910.
  Could you clarify what use case you're finding this problematic? If you're hitting the problem for non-chunked media then I think it's right to treat it as an error, since serving non-chunked media and not supporting range requests is fundamentally flawed. For chunked media it's less flawed for the server to not support range requests, and I agree we should handle servers not doing so for that case.

Thanks!
 If you're streaming non-chunked media then it's mandatory for a sensible user experience. But yes, we should have a solution for the chunked case at least, where actually it's kind of reasonable to not support range requests (as long as the chunks are not too large). I'll take a look at the updated patch.

In the meantime, note that we need a signed CLA to accept pull requests. See:
https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
 Turns out it's a little bit more complicated to cover everything off correctly. I've submitted https://github.com/google/ExoPlayer/commit/362dc5f382e3a87f04fed017248e225eeb416396 to support servers that don't support partial requests.
  - The patch is valid, but there's no particular need for us to support multiple audio adaptation sets in the demo app (particularly since we don't have any sample content in the demo app that makes use of it). This is the kind of thing that developers should add to their own app integration if they need it.
- Re the Period.java change, you can already iterate over adaptationSets directly from app code, so there's no need for that method. 
  You should stop and re-prepare the player to switch the content.
  Please provide more complete information about the (device, os-version) combinations that you see this issue on. I don't think there is a device called just "Samsung Galaxy"?
 Could you provide some sample media? Also, does this occur if you add your sample to the demo app with TYPE_MP4 when using the dev branch, as opposed to using TYPE_OTHER?
 Closing due to inactivity. Most likely using TYPE_MP4 resolves the problem.
  This is a device specific issue, and will likely require a device specific workaround. Can you provide a more complete list of the (device, OS-build) combinations that you know to be affected?
 I was able to reproduce the problem on an HTC Butterfly device, and found that the process crashes if the failing codec OMX.qcom.audio.decoder.mp3 is created, with OMX_ErrorComponentNotFound output.

We will have to filter out that codec on devices where the error was thrown. Please could you provide the output of 'adb shell getprop ro.build.fingerprint' on any devices where you saw this issue? Thanks!
 @th3hamm0r, thanks!

@brianchu, the devices I've tested all have another MP3 decoder that does not exhibit this problem, so the fix is for ExoPlayer to ignore the 'broken' codec when enumerating decoders. This will be automatic, so the app doesn't have to do anything. For now, there is no database of problematic codec names -- we will just check for the product/API version/codec name combination in code in MediaCodecUtil.java. I expect we will merge this into the dev branch today.
 Fixed in dev. Other affected devices can be added in the same way.

@Ood-Tsen, we are working around this issue in ExoPlayer, so the application does not have to do anything.
 On HTC Butterfly these values happened to match, but Build.DEVICE is the right one to use. I will submit a fix.
 @th3hamm0r, the MP3 demo should now work on a clean dev branch. Please could you try to verify this fix again, as I don't have access to an HTC Desire X to test it? Sorry for the inconvenience.

The reason getting capabilities for this OMX component fails seems to be that MediaCodec can't communicate with mediaserver. (And instantiating the component always crashes mediaserver, which might be related.) Skipping over codecs on catching IllegalArgumentException is risky because a codec could be skipped due to a transient failure, which could cause poor quality playback due to falling back to using a software codec, for example.
 @Ood-Tsen, I still think catching the exception and continuing is risky. We know that the exception is thrown when the MediaCodecInfo instance can't communicate with the mediaserver, and that instantiating the component causes mediaserver to crash. I think it is possible that the mediaserver could be put in a bad state just by querying for capabilities of the 'broken' OMX component. Since this will only affect a small number of device/build combinations, the targeted workaround seems safer.
 We'll get that one added to the list, thanks!
  - The MP4 file you link to is invalid. The specific issue is that there are negative sample offsets inside version 0 ctts boxes in the file. Version 0 ctts boxes only permit positive sample offsets; the boxes should probably be marked as version 1 instead.
- It's unclear if this issue is directly related to why these devices don't play the content. In any case, we're implementing an MP4 extractor in ExoPlayer, and using that will allow you to successfully play this (and similar) content. ETA to push an updated version is later this week, at which point you should just use that instead.
 Please try playing the sample using TYPE_MP4 on the latest dev. Does that work for you?
 @u3breeze TYPE_MP4 was moved/renamed to Util.TYPE_OTHER. If you set this as the type for a sample in the demo app, the player will be built using [ExtractorRendererBuilder](https://github.com/google/ExoPlayer/blob/master/demo/src/main/java/com/google/android/exoplayer/demo/player/ExtractorRendererBuilder.java), which uses ExtractorSampleSource. This is the recommended source for playing MP4s and other multimedia containers in recent versions (not FrameworkSampleSource, which is now deprecated).
 @u3breeze The audio/video tracks are not well interleaved in that MP4 file (the audio samples start at about 18 MB). This is the same issue as #1405, #1368.
 Correction: #1386 not #1368.
  Thanks for the report. We'll merge a fix soon.
  Yes - This is the same request as #73. We're working on it.
  We'll be pushing a fairly major revision of the new MP4 stuff fairly soon. Note that this is still work in progress, so use it at your own risk ;)!
 We've rewritten much of this code, so I'm going to mark this as fixed. If it still occurs on latest dev, please re-open (with new stack trace logs, since all of the line numbers will have changed).
  This is working as intended. For DRM protected content where the license requires that the secure video path be used, rendering to a TextureView isn't possible (because it's not secure).
 Do the two "secure video path" videos work on your Nexus 7 in that way? Or are you talking about some of the other WV GTS samples? Which videos exactly are you referring to?
 It's possible that the other devices could be a little more permissive where the license doesn't require HDCP or secure video path, but it's definitely WAI that playback to a TextureView fails in the secure video path cases.

I can follow up internally, but in practice, you should just always use SurfaceView if you're using drm.
  Are you using TextureView, or SurfaceView? Also, do you have a test stream that reproduces the issue (or, if it reproduces with the samples in the demo app, that would be useful to know)?
 You'll probably need to make an equivalent public stream (e.g. of cycling test content) available to get any help with debugging this issue. We can't debug something that we have no way of reproducing.
 Closing due to lack of test stream.
  - There's a discussion in #26 about changing playback speed. It's not something we're planning to support directly any time soon, although as discussed there, it's possible in theory to extend ExoPlayer to do this.
- You can adjust the volume using [this](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/MediaCodecAudioTrackRenderer.java#L63).
- You can attach effects by doing something similar to the 4th post in #252. Note that you may need to enable the workaround as described at the end of that thread if you want to do this.
  We'll be publishing a new simplified model for media extractors fairly soon, which will provide the foundation on which someone could do this. The task would be to implement a FLAC extractor in this new model. I'd advise waiting for that, rather than doing anything now.
 Note - The dev branch has the new Extractor model, and all existing extractors have been migrated. If you fancy contributing an Extractor for FLAC (or indeed any other container format), that would be great.
 The FLAC extension now implements both an extractor and a decoder for FLAC, so this enhancement is complete.
  Thanks for this. We've fixed this in a way that defines the allowable values in a more strict way, here: https://github.com/google/ExoPlayer/commit/5a5935cb72857580803573f0d44703516d332558
  Don't hold your breath for this ever being implemented. I could be wrong, but I don't think Android devices have the necessary hardware decoders. H264 strikes me as a better choice if you want to target anything wider than Microsoft controlled devices/services.
  Currently FMP4 and WebM playbacks are supported only in the context of demuxed DASH streams.

We're working on an extractor model that will allow the FMP4 and WebM extractors to be used in a standalone way, but for this to be useful, we need the extractors to support multiple tracks (i.e. audio+video in a single stream).
 This is done for WebM. Not yet done for FMP4.
 Note - We have no plans to support multi-track FMP4 in the context of DASH. For DASH we require that content is demuxed into separate streams, each containing a single track. The support added for multi-track FMP4 is for standalone files only.
  What does "smooth" mean. There are a bunch of things you can look at.
1. Use [ExoPlayer's listener](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/ExoPlayer.Listener.html). Note - The state constants are actually defined [here](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/ExoPlayer.html).
2. You can also attach listeners to many of the individual components. For example, you can attach a listener to the video renderer that will count the number of dropped frames. See [here](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/MediaCodecVideoTrackRenderer.EventListener.html).

The rest, you'll have to figure out for yourself ;).
  Unless you happen to be implementing DASH with single-segment media, like YouTube does, then chances are CacheDataSource wont work for you yet. There's an issue tracking enhancing it to support other use cases here: #322 .
  We're doing some fairly major refactoring on the dev branch currently, so you might see a few issues such as this one (reporting them is still useful though, so thanks!). This specific problem is easy to fix.
 Fixed in https://github.com/google/ExoPlayer/commit/e54d07c1b09ee051d4ceedcde12f50433111c45d
  That shouldn't be a problem. customMessagesProcessed will be incremented once for each of the two messages that you sent, and when the second increment happens blockingSendMessage should be unblocked.
 Right. The correct behavior is that the method will block until the message has been successfully delivered on the playback thread (and then will be unblocked). Please re-open if that's not what you're seeing. Otherwise it's working as intended.
 Ah. The issue you describe is effectively the same as: https://github.com/google/ExoPlayer/issues/51
  I got some problem when the server return a HTTP redirect (302) before before the actual HLS m3u8 playlist. The player seems does not following the redirect.

Below links are playable on VLC, but not on ExoPlayer directly. However, playable if I get the redirected link manually.
{"http://token.tvb.com/stream/live/hls/mobilehd_hdj.smil",
            "http://token.tvb.com/stream/live/hls/mobilehd_j2.smil",
            "http://token.tvb.com/stream/live/hls/mobilehd_inews.smil"};

Issue found on Nexus Player (5.0) and Nexus 6 (5.1).
Sometimes, the app (demo) can play the clip once, but not any more.
 We do handle redirection. For me at least, these URLs redirect to a 403, and so playback failure is working as intended. Using CURL, I see the original URL returning a 302 redirect:

```
curl -i http://token.tvb.com/stream/live/hls/mobilehd_hdj.smil
HTTP/1.1 302 Found
Date: Fri, 13 Mar 2015 11:21:00 GMT
Server: Apache/2
X-Powered-By: PHP/5.3.21
Location: http://e1.vdowowza.vip.hk1.tvb.com/mytvlive/smil:mobilehd_hdj.smil/playlist.m3u8?ts=1426245660&sig=5c90e1b61e91ef4e4dc709508611bc18
Vary: Accept-Encoding
Content-Length: 0
Connection: close
Content-Type: text/html; charset=UTF-8
```

and the new URL returning a 403:

```
curl -i http://e1.vdowowza.vip.hk1.tvb.com/mytvlive/smil:mobilehd_hdj.smil/playlist.m3u8?ts=1426245660&sig=5c90e1b61e91ef4e4dc709508611bc18
[1] 21164
olly@ollyz600:/usr/local/google/bigdisk/git5/google3$ HTTP/1.1 403 Forbidden
Accept-Ranges: bytes
Server: WowzaStreamingEngine/4.1.1
Content-Length: 0
```

Do I need to do something to not get a 403? What's the actual failure you see (in the device logs) when you try this?
 no error shown. playback state is ready. just like the video is still loading.
 one more, the clip can play but is black screen with audio sometimes. should i open a new issue for this?
 Can you first explain why I see a 403 when hitting the URL to which the example redirects? Until we have a test stream where we can successfully retrieve the manifest, there's not much we can do.
 Closing issue due to inadequate reproduction steps. Please re-open if you can provide more information.
  This is a tentative fix for the audio part of issue 73 . It is loosely based on the multi track support for Dash. It only support one audio group for all the streams and completely bypass the TS audio. I think this is the basic use case for alternative audio as mentioned by @perchrh and @johanlindquist.

It has problem with audio synchronisation. Any idea what's the problem? When switching audio tracks, it gets synchronised but it's quickly drifting after.

I updated the parser so that it parses all alternate media types and not only the subtitles.

Also there is no load control as mentioned in the comment by @ojw28.

I tried to split the change in a way that make sense so please do a per commit review. Also I'm new to ExoPlayer and media players so please forgive me if I made stupid assumptions ;-)
 This is roughly how this feature should be implemented, but there are some important caveats around handling of HLS Live streams, and around buffer management (not using too much memory, and also ensuring that the audio and video streams load at the same rate, rather than buffering way more audio than video).

We're working on some changes that better align the Chunk and HLS packages. These changes will allow components in the Chunk package, such as LoadControl (for proper buffer management) and hopefully MultiTrackChunkSource (to avoid forking) to be re-used. We'll likely push these changes towards the end of this week, at which point we'll be better placed to implement this feature.
 We support multi-audio for HLS now, so closing this.
  Can you share an example HLS manifest that has problematic URLs? I'm not sure about this (I agree there's likely a bug here, I'm just not sure that the proposed fix is the right one). Thanks!
 I tried adding ":80" into the URLs of a few internal test streams we have, and everything still worked fine for me on both the master and dev branches. Furthermore, in HttpDataSource, if I add in a check to see what the decode actually does, it says that it doesn't do anything. In other words, this evaluates to true:

```
TextUtils.equals(dataSpec.uri.toString(), Uri.decode(dataSpec.uri.toString()))
```

Can you provide a concrete example of what uri.toString() is giving you at L387 of HttpDataSource when you're seeing this issue, and also what Uri.decode(uri.toString()) is in this case?
 I couldn't reproduce the issue on either branch.
 Ah. Does the dev branch fix the issue for you?
 Thanks for the info. I'm going to close this on the assumption that the issue is the same.
  - Playback failed because the HLS manifest wasn't loaded successfully. There's no evidence to suggest this isn't working correctly (e.g. due to networking issues).
- The issue you then see is a problem with VideoSurfaceLayer in [GMF](https://github.com/googleads/google-media-framework-android/issues), not ExoPlayer. It shouldn't be printing null to Log.d. I've filed [this issue](https://github.com/googleads/google-media-framework-android/issues/49) for you under the correct project.
  Please could someone attach a full bug report (captured with "adb bugreport") to this issue, captured shortly after encountering the issue using ExoPlayer's demo app v1.5.10 or later? Output from "adb logcat" is not sufficient. Thanks!
 Closing due to lack of required information.
  The caching layer isn't particularly flexible right now. It's very much designed for for the specific case where you (a) only make bounded requests, and (b) make range requests into few URLs. Unbounded requests aren't supported at all, and the caching layer wont handle each chunk being fetched from a separate URL in an efficient way. It's on our list of things to fix, but it's a pretty complex change, particularly to generalize in a way that doesn't degrade the case it's currently designed for. There's no ETA at the moment.
 That's probably the best way short of some pretty major restructuring, yes.
  This is something you'd have to do yourself. The code for selecting format is in HlsChunkSource (and is also controlled by the value of variantIndices passed to the constructor of that class). So you'd need to figure out how to modify that class to suit your needs.
  Tracked by #28.
  Flash videos are not supported.
  There are pros and cons of doing this (obviously). I don't think it's appropriate to have this as default behavior, but there are cases where it would be helpful. We'll keep this in mind during future development, but there are no near term plans to support this mode.
  Hmm. If the first caption is several minutes into the content, are there any guarantees that there will be a (presumably empty) EIA608 payload in the stream before that point (ideally the guarantee would be that it would be present in the first frame)?
 It looks from the HLS spec like the proper way to indicate that the stream has EIA608 captions is in the manifest, with an EXT-X-MEDIA tag of type CLOSED-CAPTIONS (not SUBTITLES, which is for distinct WebVTT subtitling), which includes an INSTREAM-ID attribute. See [here](https://tools.ietf.org/html/draft-pantos-http-live-streaming-14#section-4.3.4.1).

We don't support this right now, but it looks like a better way of doing things to me.
 We added support for #EXT-X-MEDIA:TYPE="CLOSED-CAPTIONS" in 896550883f6d183e3ef46351eb3e494df8eaefba. Is the same also true of European TV broadcasts (i.e. that there's CEA608 data in every frame)? Got it, thanks. It doesn't seem unreasonable to use `FLAG_OVERRIDE_CAPTION_DESCRIPTORS` for the time being. It feels like targeting both markets should be as simple as an if statement. It only becomes difficult if your app is simultaneously targeting both markets and does not know what market the content being played came from, which seems somewhat unlikely in a legitimate use cases (if you believe differently, a concrete example where this would happen would be helpful). By the same argument, telling the media player what tracks are present in the stream in a sensible way should be the responsibility of the stream/container ;). I'm aware this doesn't help; just pointing it out!

Can you explain what your use case is in a concrete way? Where is the media being played coming from such that you don't know its source? It is all side-loaded? I think I'd argue it's working as intended that it costs you more time and effort to support a fragmented ecosystem that requires playback of legacy streaming formats. It acts as a natural forcing function that encourages upgrading to newer tech ;). Broadcasters we've worked with have typically been able to provide well constructed HLS and/or DASH streams, so it's not like doing so is out of reach.

It also makes sense for us to allocate our resources according to actual ExoPlayer usage, as well as projected future usage. The reality is that although we see quite a few issues reported about this type of use case, the actual proportion of usage these cases account for is a small and decreasing minority. Hence there's only so much time we can reasonably devote to them. We will of course continue to do our best within this time.

Sounds like you have a solution for your use case anyway, which is good news :).  I don't think I understand this request. Please could you provide more detail about exactly what you're trying to do?
 - I think the PlayReady plugin probably doesn't support optional parameters (I'll follow up and find out for sure).
- It probably should support them (I'll file a feature request internally).
- Even if/when it does, it's unclear whether the custom parameters will be added to the request exactly where you're expecting them, as opposed to somewhere else (if I had to guess, I'd guess they'd be added one level deeper, inside an element that holds all of the custom values).
- For now, you'll have to manually inject your custom data into the request, before you send it. Or send this data some other way, for example as a query parameter on the request.
 - So it turns out this is actually supported. You need to use optionalKeyParameters, but the key you need to use is "PRCustomData". If you use that, you'll find that the CustomData is correctly included in the request.
- This seems pretty undocumented, so I'll add a convenience method in the library to help out with this.
 Some helper functions were added in the change above that makes it a lot more obvious what to do. In particular, there's no a StreamingDrmManager.newPlayReadyInstance method that takes a customData argument.
  What type of media are you playing? The failure is in MediaExtractor. On the dev branch we now support quite a few media types directly, so that we don't need to rely on MediaExtractor at all, meaning we wont hit this kind of device specific issue. We currently support MP4, MP3 and AAC (WebM is coming soon).
 We now support MP4, MP3, AAC, TS, WebM directly, without MediaExtractor. Closing this.
  Closing because this is quite old. Please let us know if this still occurs on 1.5.0+, and we'll take a closer look. Thanks!
  Could you clarify what you're actually trying to achieve with this change? Are you trying to optimize network usage through cancellation? It's likely that the underlying network stack will choose to fully read the network response regardless. This is a common optimization in the case that the response is known to be relatively small (which is typically true for manifests), because completing the request allows for the socket to be kept open for potential subsequent requests.

 So I don't think you'll actually see any benefit there. Given this, can you not just ignore the response in your callback?
  I don't really understand the question. A SurfaceView will continue to display whatever was last drawn to it. What does it mean to update the renderer?
 What calls are you making to set the new elements? And when you say it sometimes draws the previous video, do you mean just the final frame, or does it draw multiple frames? Without knowing more, I'd say it's likely to be an issue with your code.
 Closing due to inactivity.
  No, although you could modify AudioTrack.java to have the getCurrentPositionUs method return what it returns now, but shifted by the desired amount. Video sync's off the audio, so by doing that you should be able to fix the mismatch.
  It's (in my opinion) debatable whether or not 60fps content makes sense for pre-Lollypop devices, because there was no way of telling the decoder release an output buffer with any kind of a guarantee about which display VSYNC that buffer would actually hit the screen on. Timing of the frame release had to be done from Java, which limits accuracy somewhat (and required a hack - which is the Thread.sleep call that you found). Even when the frame is released, there aren't any strong guarantees going down the stack about how that will map onto VSYNC. The net result is that although you may find high end devices running pre-Lollypop are capable of 60fps decoding, it's doubtful whether the stack is capable of smoothly releasing one frame for each display cycle (assuming the display is 60Hz). And if it's not, there' not so much point in sending 60fps content, and you may as well save on bandwidth.

In Lollypop there's a new timed frame release API. If you look at MediaCodecVideoTrackRenderer, you'll see that there's a different code path on Lollypop that makes use of the new API and avoids the sleep call. So 60fps content should work without modification on Lollypop devices.

If you do want to enable 60fps content on pre-Lollypop then you're right, you'll need to tweak those constants, or remove them, or re-architect the way frame release works to be on a different thread. I suspect you'll end up subtly breaking something though, so my recommendation would be to target Lollypop for 60fps, and use 30fps for older devices.
 p.s. I thought the Note 4 got a Lollypop update already. Although perhaps it's carrier specific and/or you didn't accept it.
  It's always been well documented that the state transitions to IDLE after playback fails [here](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/ExoPlayer.Listener.html#onPlayerError%28com.google.android.exoplayer.ExoPlaybackException%29). This is not new behavior.
  It's unlikely to be possible for secure drm protected playbacks. For all other playbacks (i.e. most playbacks) there's no fundamental limitation preventing it from working, although emulator support for video has traditionally been poor.
  The file is no longer present at the link, but you should be able to play *.ts files using TYPE_TS in the demo app, if you use the dev branch (we'll push support to master next week).
  Please provide a link to a test stream that reproduces the issue. I don't think the error is related to AES. It looks more like the content that's read from the server is for some reason not of the expected length.
 You can email it to olly.exoplayer@gmail.com.
 Thanks! This is related to the way HttpUrlConnection supports transparent gzip, although I'm not sure why it's so device specific.
- Some thought will be needed to determine the best "proper" fix for this, but if you want a quick local fix, you can safely remove lines 315-320 in [HttpDataSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/upstream/HttpDataSource.java#L315). If you're using the dev branch then this code has moved to a class called DefaultHttpDataSource, so you'd need to delete the equivalent code block there (note - the line numbers are different).
- Having fixed the issue, I pretty reliably see a native crash in the video decoder about 39 seconds into the content. It seems the device can't cope with your highest bitrate stream, so you'll need to find a way of disabling it for this device.
 This should be fixed on the dev branch. Please validate, and let me know if you're still seeing issues.
  Thanks for this. Could you send a version of the problematic media file to olly.exoplayer@gmail.com?
 We have now added our own MP3 extractor (in dev). Using it, rather than relying on MediaExtractor, should resolve this issue. The samples with TYPE_MP3 in the demo app show how to do this. We'll push these changes to master next week.
 The media linked in the post above isn't well suited for exact seeking to non-integer-percentage increments of the total media duration. This is due to the type of header that it uses to provide the seek information (XING), which only defines the byte offsets in the stream at integer-percentage positions. If you have control over the media then you should definitely prefer to package it as MP4 rather than MP3, which will resolve the issue that you're seeing (the MP4 container defines the byte offset of every sample in the stream).

Currently, ExoPlayer implements seeking with XING headers by interpolating between the nearest two integer-percentage positions, which is non-exact if the stream bitrate varies significantly between those points. An alternative might be to request data from the integer-percentage position that precedes the requested position and then drop media up to the exact position requested, however that would be inefficient for very long pieces of media where a single percentage of the stream might be quite large, so it's unclear whether that's actually good idea. We could consider doing that, but it's a separate discussion. Please open a new issue if you think it's a discussion that's worth having.
  You're right in saying that the result isn't correct, but I don't think your result is correct either. According to [RFC3986](http://tools.ietf.org/html/rfc3986#section-5.2.2), I think the following are true:
- Your stringUri should be interpreted as a path rather than as query parameters, because it doesn't have a preceding `?`. The current logic is getting this right.
- The `ABCD1234` part of baseUri should be discarded when the paths are merged. The current logic is getting this wrong.
- `videoformat=dash&` should be discarded. It's never the case that query parameters from baseUri and stringUri should both be retained after the merge. In all cases the ones from baseUri should be discarded, except the case where stringUri defines neither a path nor query parameters. The current logic is getting this wrong.

So I think the correct result for your example is:

```
https://foobar.com/video/quality=video360p&time=0
```

Which is in line with the 2nd example in section 5.4.1 of the spec.

We should fix the implementation, but doing so wont solve your case. If you're actually trying to merge URLs in the form you gave in your example, then I think you'll need to change the way you're specifying them in your manifests so that they merge properly according to the spec. I think you'd probably need to do something more like the 6th example in 5.4.1. For example:

```
baseUri = https://foobar.com/video/ABCD1234
stringUri = ?videoFormat=dash&quality=video360p&time=0
```

Which would merge in the way you expect.
 @jeoliva Do you think it's necessary for us to implement remove_dot_segments, as per [5.2.4](http://tools.ietf.org/html/rfc3986#section-5.2.4)? It seems pretty obscure that (a) they'd ever appear, and (b) the server also couldn't handle them. Strictly speaking I guess we should. But in practice...
 @phpeng-ms I think that would work correctly with the current version of ExoPlayer, yes.
 @jeoliva Thanks for the link! I have a change I'm working on that correctly resolves for all normal and abnormal cases in the RFC, which is a good sign. I hope to push tomorrow, although it might take another day.
  Please could you send the problematic file to olly.exoplayer@gmail.com (or upload it somewhere publicly accessible). If you could also say what device and OS version this occurs on, that would be helpful.
 Yes, thanks!
 I was able to play and seek both files successfully on a Galaxy Note 3 (build fingerprint samsung/ha3gxx/ha3g:4.4.2/KOT49H/N900XXUDMLD:user/release-keys). Could you send the output of `adb shell getprop ro.build.fingerprint` so I can check we are using the same device? Thanks!

I also wasn't able to reproduce the problem on a Nexus 5, a Xiaomi device and an HTC One M7. The logging from TableOfContentThread does not appear to come from stock Android, so it's likely this problem is specific to a vendor-customized version of the extractor. We plan to add support for MP3 extraction without using the framework's MediaExtractor in the near future. This should hopefully address any device incompatibility problems with MP3 container formats.
 Note - We have now added our own MP3 extractor (in dev). Using it, rather than relying on MediaExtractor, should resolve this issue. The samples with TYPE_MP3 in the demo app show how to do this.
 @gkato0921, which sample source are you using?

If you're still using the deprecated FrameworkSampleSource, please try using ExtractorSampleSource instead. If you see the failure with ExtractorSampleSource, could you file a new issue and attach a link to the problematic stream with steps to reproduce the error? Thanks!
 Why do you call mExoPlayer.seekTo(0); when you see the player state change to ready? The player reenters the ready state after seeking, so that explains why you see the playback position return to the start.
 @hezhisu See your new issue #1810.
  Thank you! This is nice. Although some of this code is the same as in AdtsReader, maybe it makes sense to put it into a common class for example.
 @tresvecesseis can you also share the media (URL) that could me used to verify this?
 FYI, there appears to be a suitable test stream in https://github.com/google/ExoPlayer/issues/578, which also crashes (native crash in the MP3 decoder) using this patch. I suspect the codec configuration data is being parsed incorrectly from the stream, although I didn't check.
 Aha, this does very nearly work. I tried replacing some of the logic in parseHeader with usage of com.google.android.exoplayer.extractor.mp3.MpegAudioHeader instead, and was able to get the sample in #578 to play without much hassle.

@tresvecesseis - If you could sign the CLA that's required to accept this request, then I can merge it and make the changes on top to get it working nicely. Please see [here](https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md) for CLA information.
 Did you sign it using your github name? I can't find a record of it in our system. Thanks!
 Great.

I think the problem with the stream in #578 and the change as it is here is related to the data passed as the final argument of MediaFormat.createAudioFormat. It appears that the decoder doesn't actually expect anything, as in our [Mp3Extractor](https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/extractor/mp3/Mp3Extractor.java#L258) that passes the empty list. It's also possible to remove quite a bit of mpeg header parsing code, since equivalent code already exists in [MpegAudioHeader](https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/extractor/mp3/MpegAudioHeader.java).

In any case, I'll merge this tomorrow and push a change on top that removes some of the duplication, cleans a few things up and fixes the MediaFormat. It's possible that the latter change may break playback of your streams, but in the worst case it should be straightforward to get them working again.
  Thank you! Overall looks good, just a few small comments.
 Had a quick scan (didn't look at the gritty details of around charsets!). Looks good to me, thanks! Will leave for @andudo to do a final pass / merge. 
  This is not currently possible using components provided by the library. Sorry.
  It's a limiting property of the Android platform that switching surface requires the decoder to be recreated. Having recreated the decoder, it's not possible to output any video frames until the following keyframe. The delay you see is simply the renderer waiting for the next keyframe to arrive.

We could have the new decoder start decoding from the previous keyframe, discarding decoded frames up to the first frame to render, but at high resolution that's not a good solution, because the decoder may not be able to decoder at a speed much faster than real-time. We'd also need to keep video that we've already output in the buffer for some people of time, which reduces the amount of buffer we're able to keep in the forward direction, which doesn't seem like a good trade-off given surface switching isn't something that's used extensively.

So yeah, I'm not sure this is something we can really improve unless the limiting property of the Android platform is removed. You may be able to use the information about to make something that works better for your particular use case.
 Re the first point: Keeping the same surface would completely eliminate the delay. But it's non-trivial to pass surfaces around. That's something you'd need to experiment with, I guess.

Re the second point: Having the decoder quickly decode from the previous keyframe up to the one you want to start rendering from on the new surface would only work without delay if the decoder were able to do that very quickly. If you're at high resolution on a low end device, the decoder may not be fast enough to do that. So you'd still get a delay anyway.
 I've never had to do this, so I'm not sure. The specific question about passing a Surface from one activity to another is more of an Android view/graphics question than something that's ExoPlayer specific, so I suspect you'll have more luck asking on StackOverflow.
 Yes, you would need to implement your own TrackRenderer that hooks into whatever decoder you choose to use. The TrackRenderer interface that you'd need to implement is [pretty well documented](http://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/TrackRenderer.html). As for actually implementing one, it's pretty inherently complicated. Although you can see a good example [here](https://github.com/google/ExoPlayer/tree/dev-webm-vp9-opus/extensions/vp9), which is an extension TrackRenderer that hooks into a software VP9 decoder.
 @JonWatson It's less a limitation of ExoPlayer and more a limitation of the underlying platform, which ties a decoder to a `Surface` and recreates the `Surface` on an orientation change. This means we need to recreate the decoder on an orientation switch, which in turn means we need to go back and restart decoding from the previous keyframe (or from the next one). My understanding is it would require quite large architectural changes in the platform to resolve.
 Yes. Note however that such an approach uses significantly more battery, and isn't compatible with the secure video path (for applications that use Widevine L1 DRM). Still, it's the best approach for now, and is fine for short-running videos where battery consumption is not quite so critical.
  It would definitely be possible for you to achieve this. There are multiple ways that you could choose to structure it, so you should settle on whatever makes most sense to you. The problem you need to solve is that ExoPlayer's MediaCodecAudioTrackRenderer currently assumes that the SampleSource provides compressed audio samples, and will always feed those samples through a decoder. Since you don't want this, you can't just implement a SampleSource that provides PCM data, and then wire it up to a MediaCodecAudioTrackRenderer. Possible solutions are:
1. Implement a SampleSource that provides raw 16-bit PCM data, and implement a TrackRenderer that feeds samples directly into AudioTrack without feeding through a decoder first. Ac3PassthroughAudioTrackRenderer on the dev branch actually does exactly this, however is currently specific for Ac3. It's probably straightforward to generalise that class to be a generic PassthroughAudioTrackRenderer. We'll likely do that at some point, but it's not a priority for us. If you were to do it and if it were to work out nicely, then that would be a great potential contribution back to the project.
2. What data do you feed into your custom decoder? Assuming you're feeding something in, one option would be to have a SampleSource provide samples before they're decoded, and implement a custom TrackRenderer that hooks in to your decoder. It's probably more work. You can see an example of hooking in to a custom audio decoder here: https://github.com/google/ExoPlayer/tree/dev-webm-vp9-opus/extensions/opus
  Looks good, thanks!

One question: Are you concerned at all about ending up in a situation where a high quality playlist is blacklisted, and playback continues fine using a lower quality stream, without ever switching back up to the higher quality again? If so it would seem beneficial to periodically check the blacklist even without failures, to ensure switching back up to the higher quality playlist can happen? On the flip side, if the playlist really is permanently broken, that'll just mean we make a request that fails every 60s. So I think it can be argued either way (and it's not clear to me whether one approach is better or worse than the other).
 I think you've found the right point to call it, although I think it's marginally clearer if you make the call right at the top of the getNextVariantIndex method (which is exactly the same point; just inside the method as opposed to outside). How about we put it there?

Otherwise all looks great; thanks.
  Do you see this across all of your test streams, or only some of them? If so, which? How long does it take to fail? I haven't been able to reproduce so far, although I do think I saw this a few times last week with your streams (not with any others).

FYI, this message indicates that bad data is being fed into the AAC decoder, although doesn't tell us whether that's because your stream is fundamentally bad, or because ExoPlayer is doing something wrong when it pulls the AAC samples out:

```
W/SoftAAC2(6459): AAC decoder returned error 16388, substituting silence
```

This message is usually just a follow-on from the AAC decoder error, although note a single occurrence of this message may be expected each time the player switches from one quality stream to another:

```
E/AudioTrack(6459): Discontinuity detected [expected 18586666, got 18885333]
```

p.s. It would really help if you had a test stream that runs 24/7. Only being able to debug at certain times of day is a pretty awkward.
 - I'll continue to try and reproduce the audio problem. The second of the additional errors suggests bad data, so it's possible that both the audio problem and this issue have the same root cause. If it turns out you are serving bad data, that's something you'll need to fix on the serving side.
- The first of the additional errors indicates your content server is responding with a 404 when we try and request the next media chunk. We automatically try and switch to the different available streams when this happens, so if a failure has occurred your content server likely responded 404 for the given chunk across all the different streams you provide in the master playlist. We don't currently have robustness in place against this kind of failure. To be honest, I'm not sure we should add it, either. Sure, it's possible to carry on playback having missed that chunk, but that's always going to be a bad experience for the user. I think it's reasonable for us to expect the serving side to not do this, and if it does do this, you should fix it.
 I suspect there are tools, and you can probably find them on the internet. I don't have first hand knowledge to be able to recommend one tool over another, but this seems like a good place to start:

https://developer.apple.com/library/ios/technotes/tn2235/_index.html
 I was able to watch these streams for quite some time today without being able to reproduce the issues you describe. If you do further tests, make sure you're using the latest build of ExoPlayer, just in case something changed that fixed this.
 Do you see network errors prior to the issue? It's possible that we're not resuming our loading from the correct position in all cases. A full bug report shortly after seeing the issue, that shows what comes before it, would be useful (capture with "adb bugreport > my_bugreport.txt").
 Closing due to inactivity. It's unclear whether this is still an issue, and we weren't able to reproduce. Please file a new issue containing all of the information requested in the issue template if you still see this using ExoPlayer 1.5.8 or later.
  Ref'd issue is now fixed, so we don't need this. Thanks though!
  Heh. Yes, we're going to do this. There's a tracking issue here: https://github.com/google/ExoPlayer/issues/28
  That sounds reasonable to me. A couple of notes:
- I'll probably be pushing a change today or tomorrow that refactors ManifestFetcher.ManifestLoadable into a more generic NetworkLoadable class, so probably best to hold off doing anything until that's pushed.
- What do you actually want to do with the cache control header? I don't have experience using it, but it's possible that installing a [HttpResponseCache](http://developer.android.com/reference/android/net/http/HttpResponseCache.html) will give you what you need?
 Yeah, HttpResponseCache does seem somewhat lacking in fine-grained control.

I'm not sure we'd want to put caching directly into ManifestFetcher. Given you're (presumably) about to start streaming video and audio over the network, caching the tiny manifest request that you make just before you start doesn't feel like a huge gain. Particularly since the user would have had to attempt to stream the same video beforehand to get any benefit. Could you explain what gain you actually expect, and how significant you expect it to be?

Re the suggestion to allow http stack injection - I wanted to move ManifestFetcher over to use DataSource/DataSpec objects eventually, which is the standard mechanism that would allow you to inject a different network stack for media requests. It's not immediately clear how headers would fit into that model, however, because DataSource doesn't make any assumptions about where the data is coming from (and the concept of response headers doesn't make much sense if the data isn't coming from the network). It might be that we end up changing HttpDataSource to be an interface that extends DataSource and adds in network specific functionality (with the existing HttpDataSource class becoming the default implementation of that). ManifestFetcher would then take a HttpDataSource instance.
 We now inject a HttpDataSource for making manifest requests. This should make it pretty easy for you to implement what you want (by implementing and injecting a custom HttpDataSource that does caching).
  Same as #51.
  Thanks! I'll merge this as soon as you complete a CLA (see https://github.com/google/ExoPlayer/blob/dev/CONTRIBUTING.md). If you've done so already then let me know, since I'm not seeing it on my side.
 Ah, I see it now. Thanks!
  I'm not sure I understand the question. The demo app has playback controls, including a pause button that pauses playback?
  Thanks! This should merge into dev, not master. Note that you also need to submit a CLA. See here for details: https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
  Thanks. You're right, however I beat you to it and merged a fix a few minutes ago :)!
  Collapsing into #302 
  See also #303 (Dizzy cannot play on HTC one LG G2, Android 4.4.2).

If this is a MediaExtractor issue (which is likely), then it's pretty hard for us to fix at the ExoPlayer level. Which is why we're there's a longer term effort to move away from depending on MediaExtractor for these playbacks.
 From L onwards I think I'm right in saying that the platform MediaPlayer is built on top of the platform MediaExtractor, which should ensure parity in terms of what each of them are able to play (short of the manufacturer re-writing MediaPlayer to not have this dependency prior to shipping).
  I can reproduce this, and I think I have a fix (I'll do some further testing tomorrow).
 Fixed in dev.
  Which branch are you using? I believe this is fixed already (as of about an hour ago).
 Marking as closed, because I'm pretty sure this is fixed. Please re-open if not.
  The developer guide should help answer this: http://developer.android.com/guide/topics/media/exoplayer.html
 "ExoPlayer is a media player built on top of the MediaExtractor and MediaCodec APIs released in Android 4.1". So it calls through to the native implementations of those APIs (not AwesomePlayer or NuPlayer).
  payloadType=1 is pic_timing, which is unrelated to subtitles. So I'm not sure that what you're seeing is actually the subtitle, and it's likely that something else is going wrong. Could you provide a publicly accessible test stream to allow us to reproduce the issue?
 Please use olly.exoplayer [at] gmail.com

[This is a throwaway account for use here].
 Thanks for the streams. I made some progress debugging the issue. It seems there are a few problems:
1. We don't unescape the sei_rbsp before we try to parse it.
2. We don't handle the case of multiple sei_message entries inside an sei_rbsp

When those are fixed, I was able to see captions, although they appeared to be unrelated to what was actually on the screen. I'll wait for the streams to go live again and see if the captions work correctly.
 I have this working now, I think. We'll push a fix later today or tomorrow, and do a bug-fix release into master too. The issues were as described above, plus an off-by-one error fixed in https://github.com/google/ExoPlayer/commit/abac6b7dd6662bb4893566a4a04bf9804ee0599d.
 The captions in that sample are WebVTT, not EIA-608. The issue that's tracking listing captions of that type is #151.
  This should be fixed by using TYPE_MP4 instead of TYPE_OTHER, as per 7f58a0be8192c375565d7c77950200ee0a38c011.
  Allowing seeking in the DVR window of live streaming is planned (for DASH and SmoothStreaming, and perhaps HLS at a later stage). No ETA as of yet though.
 Leaving open to track delivery of the feature.
 Yes, you can seek in the live window using the technique you describe for DASH playbacks. The remaining work consists of:
- Implementing the same for SmoothStreaming (and HLS, although that's tracked separately).
- Exposing the available window directly through the ExoPlayer API, rather than requiring that applications listen to DashChunkSource. Having to listen to DashChunkSource is pretty convoluted, particularly given there are normally two of them (video/audio). The plan is to do this as part of a larger refactoring effort that will clear up a bunch of niggles that have built up over the past year or two.
 Not a public one, but the effort is underway already already. We've made a dev-2.X.X-experimental for this, but have opted not to push until we're closer to being done.
 Seeking in the live window will work in 2.x for both DASH and SmoothStreaming. The functionality will be present in the demo app also. We'll be merging changes to the 2.x branch this week with this functionality.
 Seeking in the live window is fully supported in `dev-v2` at this point, for both DASH and SmoothStreaming. The functionality is present in the demo app also.
 Seeking in the live window for HLS is tracked by #87. See also [this post](https://medium.com/google-exoplayer/test-8b62d50362ef#.75atfayul), in particular the last paragraph that explains why features land first for DASH, and later for HLS.
  We don't provide support for these formats currently, so it's something you'd have to add yourself.
  - Why are you working with such small segments, out of interest? That doesn't sound like a good idea.
- I think it's fine to fix it in getBufferedPositionUs, if that's the only thing that's broken. I can do this.
 Fixed in dev.
  We currently support EIA-608 captions for HLS. We'll be adding support for WebVTT captions soon.
 #151
  See https://github.com/google/ExoPlayer/issues/273
  We currently support EIA-608. Support for WebVTT is tracked here: https://github.com/google/ExoPlayer/issues/151
 We currently support EIA-608 only, for live and on-demand. WebVTT support should arrive relatively soon, although I can't provide a solid ETA.
  7f58a0be8192c375565d7c77950200ee0a38c011
  Do the Widevine GTS samples in the ExoPlayer demo app work for you? If so then that would suggest that the device itself is fine, and that your response isn't formatted in the way that the Widevine plugin expects.

I'm not sure exactly what the expected format is. If the Widevine GTS samples do work, then I'd recommend attaching a debugger to the demo app and taking a look at the response data for that case. That may well give you enough to go on. If that fails, and if you have a contact at Widevine, you'd probably be best off asking them as a next step. If all else fails, let me know and I can do some more digging around.

Either way, let me know how you get on!
  @JanKokalj, player always starts why the first variant that is listed in the master playlist, and only then it switches to a more appropriate one. This is by HLS spec. If you want your videos to start with a high bitrate, put a high bitrate on the top of your master playlist.

We currently don't have an additional ability to tell the player which bitrate to use or start with.
 If you wanted the add the ability, the place to do it would be in HlsChunkSource. The initial variant to use gets selected at the bottom of the constructor in that class. We'd consider accepting a patch if you can come up with a nice abstraction for this (you're also free to make the modification locally, just for use in your own project). We'll probably add this functionality at some point, if not.

More generally, determining which format to start with is a non-trivial problem, because until you're fetching data, you don't really know what bandwidth you have. You can guess based on the network type, or retain state from previous playbacks or unrelated network requests that your application is making, but in general it's a non-trivial problem, and different developers are likely to have a preference for different approaches. This is why we opted for the simple approach according to the HLS spec, and will continue to do so, but allowing different developers to override this behavior is something that it makes sense to allow.
 In the [2.x.x experimental branch](https://github.com/google/ExoPlayer/tree/dev-2.X.X-experimental) HLS uses `FormatEvaluator`, just like DASH and SmoothStreaming. By implementing a custom `FormatEvaluator` it will be possible to change the initially selected format.
 Fixing the initial selection is now tracked by https://github.com/google/ExoPlayer/issues/2353. Let's use that issue for tracking. Closing this one.  Isn't it pretty straightforward to write this in your own application code, just by having something call ExoPlayer.getCurrentPosition at the desired rate?
 Closing as per the answer above. I don't think this is something that the library needs to support directly.
  If the underlying MediaExtractor isn't propagating the error up into ExoPlayer code, there's no way for us to propagate it out of ExoPlayer. Sorry.

We're working on reducing our dependencies on MediaExtractor to avoid issues such as this one in the future.
  Sample objects are already pooled. The ideal solution would probably be to keep data in TsPacket sized buffers right up until the samples are actually read from the extractor. Rather than copying sample data out of TsPacket buffers into intermediate Sample objects, we'd instead only parse out the information we need, such as where the sample data boundaries are within these buffers. We'd then copy directly from TsPacket buffers into SampleHolders on demand.

This is a pretty big change, and may be pretty complicated to pull off. But it would solve the issue.
 FWIW it's only "almost unusable" for lower powered devices running pre-L versions of Android, combined with high bitrate streams. L does much better due to Art being the default VM. I plan to start looking at it this week. How soon the fix will arrive will depend on how difficult it is to fix.
 I started looking at this today. I've made good progress, and hope to have something ready by the end of the week.
 I'm not quite sure how good performance will end up being. It should be noted that HLS, specifically its use of MPEG-TS as a container format, is horrendously inefficient, both for bandwidth and in terms of computational cost for the client. If you have a 1MB keyframe in your stream, it ends up being fragmented into over 5000 pieces in MPEG-TS, which the client then has to merge back together again before feeding the frame to the decoder.

You should really consider switching to DASH or SmoothStreaming using the FMP4 container format if you have any control at all over what format you're using, where a 1MB keyframe will be in exactly one piece, and can simply be copied directly into the decoder.
 Hi. Could you test this again using the latest dev branch? Thanks!
 Note: If you were previously using the dev-hls branch, you should now be using dev (dev-hls has been merged to dev).
  It's unclear exactly what this request is. For us to investigate further, we'd need a sample stream/capture of the media that you're trying to play.  What do frequently and much slower mean in absolute terms? Are you using the dev or dev-hls branch with TYPE_HLS? On what device(s) do you see these issues?
 Changing maxDroppedFrameCountToNotify does nothing except changing the rate that dropped frames are reported. It wont have made any difference to the actual user visible experience.

Probably the same as #278, so let's use that issue for tracking
  Not right now. We need to add something to HlsSampleSource equivalent to ChunkSampleSource's Listener (which has onDownstreamFormatChanged for this purpose).
 Sorry for the delayed response. Yes, there is a plan to converge the HLS and DASH/SmoothStreaming implementations (we'll probably be converging onto the HLS model, rather than the other way around). It's pretty complicated to get everything to line up to actually do this, but I've done some experimental work and it looks promising.

In the meantime, I think we'd take a minimal patch to report quality switches out of HlsSampleSource (maybe one that just reports the selected bandwidth, and nothing else). Alternatively, you can just wait or patch locally to your requirements.
 FYI, https://github.com/google/ExoPlayer/commit/5ca2e0fd95d580df34599c84b37f9e1e4f49da87 is the first change towards the convergence of HLS and DASH/SmoothStreaming.
 This is supported in dev.
  This is a known issue, tracked by a TODO here:
https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/FrameworkSampleSource.java#L34
 Yes, it's a known issue with MediaPlayer also. It's a property of the underlying threading model, which is less than ideal. We can/will work around it in the ExoPlayer case. Although to do so involves non-trivial thread hopping, and we haven't gotten around to it yet.
 We've been adding alternative extractors to cover off the most common use cases (mp3, mp4, aac will land very soon). Depending on your use case, you may be able to switch to those if you're on the dev branch. If you have no idea what the container type of the content you're playing is going to be then these wont help (yet).
 It should also be noted that the Android MediaPlayer implementation suffers from the same ANR issue in some cases, so if that's what you're using instead, it's unclear whether it would even be a regression to switch over to a library that has the same problem.
 FrameworkSampleSource is deprecated and its replacement ExtractorSampleSource can now be used in nearly all cases instead (on the dev branch at least).
  There will be a limit on the maximum number of hardware decoders that you can instantiate, which you're hitting. Note that 4 is pretty generous. I believe some devices only allow 2. Your best bet is doing as Martin suggests, and modifying the code to use software decoders. You could implement this as a fallback (e.g. try and obtain the qcom one first, and use the google one if instantiation fails).

Whether this works well enough really depends on your use case. If you're trying to decode 5+ HD streams, that's likely just beyond the performance limitations of existing devices. If they're low resolution thumbnail-style videos, you might have more luck.
 Yeah, it's highly unlikely that there's any kind of clever sharing of decoders underneath MediaPlayer. Either the devices you mention also grant 10+ hardware decoders to ExoPlayer, or fewer are granted because ExoPlayer is allocating bigger instances as Martin says (if this is demonstrably the case, it's worth us investigating to see if we can allow more instances), or MediaPlayer is using software decoders and it just happens that they perform well.

Note that the term "hardware decoder" is used very loosely, and what it means varies from device to device. In general it's not true that a "hardware decoder instance" directly maps onto an actual dedicated physical piece of hardware. I guess "hardware accelerated decoder" would be a better term, where the actual hardware used, and the way it's used, can vary from chipset to chipset. For example a hardware accelerated decoder may require an allocation of GPU memory, and it's things like this that explain why allocating smaller instances may allow more instances to be granted.
 - It's probably better to validate that MediaPlayer isn't using software decoders first. Although I'm not sure I know of a great way to do that short of scouring through the logs looking for decoder instantiations (if the device logs them). I think it's the more likely of the possibilities. Your assumption that the decoders must be hardware accelerated because video is smooth seems tenuous. Another thing to check is what performance is like if you use 10 ExoPlayers where 6 of them are configured with the software decoder.
- There isn't a single size parameter as such. We provide information to the underlying decoder when we configure it, in the form of a MediaFormat. This happens in MediaCodecVideoTrackRenderer.configureCodec. The underlying decoder may or may not use this information to determine what resources it acquires (if it does, it would typically look at some subset of the values set in KEY_MAX_INPUT_SIZE, KEY_MAX_WIDTH, KEY_MAX_HEIGHT, KEY_WIDTH and KEY_HEIGHT).
- What's the actual use case for playing 10+ videos at once, out of interest? I can't think of a sane use case for wanting to do that, and I struggle to believe you'll find a good solution that works well across a wide range of devices, particularly at the low end. Wouldn't bandwidth also be a big concern?
 - You may find that MediaCodec.createDecoderByType is clever enough to allocate hardware decoders up to the number allowed and then software ones after that. Worth a try using that rather than MediaCodec.createByCodecName.
- If ExoPlayer with only software renderers works, then I would say that makes it more likely that the MediaPlayer is using software renderers too (since the performance->hardware argument is no longer valid). The idea that the underlying platform might share decoders between multiple videos seems highly unlikely. I don't think it's theoretically possible to do this in the general case. There are special cases where it's possible to re-transmux multiple H264 streams into a single encoded stream, decode it with a single decoder and splice out the decoded frames, but my understanding is that this only works in very specific circumstances that aren't generally true (e.g. you'd definitely need every stream to have the exact same frame-rate for that to be possible, quite likely the same resolution too, and you'd have to synchronize frame releases across multiple player instances). It's crazily complicated to do this, and it's highly unlikely any OEM would have optimized such specific circumstances in this way.
 @huyn - Note that many low end devices only have the software decoder and it actually performs quite well (easily well enough for this to be a viable option for shipping a low end device). So your statement seems a bit over the top. If you're going to advise people to not even think about using it, I think you need to back up your statement with some solid facts and figures ;)...
  What position does the player actually report if you call getCurrentPosition? If it returns the correct position, then the issue is a UI one, rather than a player bug, and you should fix whatever UI you're using to show the progress bar.

If it's returning an incorrect position, you need to provide more details (e.g. what stream are you actually playing, on what device etc).
 If you mean the Android http://developer.android.com/reference/android/media/MediaPlayer.html, then this project is unrelated. I think you'd need to file an issue on the Android issue tracker:

https://source.android.com/source/life-of-a-bug.html
  Those are two different questions.
- The freezing is a known issue. It only occurs when attempting to release the player whilst it's buffering; all other usage should be freeze free. It does not affect DASH/HLS/SmoothStreaming. Only other playbacks. There's a TODO in here to fix it: https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/FrameworkSampleSource.java#L34
- What do you need the frame number for? I'm not sure that all container formats even contain sufficient information to calculate the exact frame number after a seek. Would the media timestamp associated with the frame be sufficient for you needs? If so, you could extend MediaCodecVideoTrackRenderer and have the subclass report the timestamp of the last frame to be rendered, like:

```
  @Override
  protected boolean processOutputBuffer(long positionUs, long elapsedRealtimeUs, MediaCodec codec,
      ByteBuffer buffer, MediaCodec.BufferInfo bufferInfo, int bufferIndex, boolean shouldSkip) {
    boolean processed = super.processOutputBuffer(positionUs, elapsedRealtimeUs, codec, buffer,
        bufferInfo, bufferIndex, shouldSkip);
    if (!shouldSkip && processed) {
      lastOutputBufferTimestamp = bufferInfo.presentationTimeUs;
    }
    return processed;
  }

  /**
   * @return The presentation timestamp of the most recently released frame.
   */
  public int getLastFrameTimestampMs() {
    return (int) (lastOutputBufferTimestamp / 1000);
  }
```
 NB - That approach may actually be a few frames "ahead" during playback, although will settle of the exact correct value when paused.
 Closing this issue, since it's tracking multiple things. I've filed #274 to track the originally reported issue.
 FrameworkSampleSource is deprecated, so the solution is to not use it. The demo app no longer uses it at all. Use ExtractorSampleSource instead.
  @khashayarghamati can you provide a URL to the media stream you're using?
 @khashayarghamati please share it with us
 That URL is a "404 Not Found" for me. Are you sure that's the correct m3u8 URL? Does it still work for you? If so, do you know if it's geo-restricted, or similar?
 Please re-open if you're able to provide a working stream.
 There is no evidence in the log that the app crashed. In fact, this line explicitly indicates that the exception was caught internally and propagated gracefully to the app's listener:

E/ExoPlayerImplInternal﹕ Internal runtime error.

The stack trace logging is just logging, and does not mean the process died.

In general, we won't keep issues open without reproduction steps, because it's not possible to determine how to fix them, or whether they're fixed already due to other changes.
 The logs don't capture the crash, so there is no way to determine where the crash occurred (or even if it was definitely in ExoPlayer, as opposed to in their own app code). You're welcome to reopen the issue if you can provide the relevant logs or reproduction steps. Without either there's nothing we can do, so there's no value in keeping the issue open. Thanks.
 @khashayarghamati Thanks.
 That stream appears to be working fine for me running on a Nexus 5. On what device(s) do you see the issue with this stream? Does it happen all of the time, or was it just a one-off issue? Does rebooting the device fix it at all? Thanks!
  Interesting. We don't correctly handle the case where a single file contains the VTT captions for the whole video. We would correctly handle the exact same file were the manifest to define a SegmentList with a single entry corresponding to the file. We can fix this pretty easily (I'll do this). However, there are other issues with this video, which I believe are issues with the actual source:

Video:
- Every video frame is marked as a sync frame in the fmp4 container, which is incorrect. It causes temporary video corruption when seeking because ExoPlayer doesn't correctly locate the (real) nearest keyframe. Is this your stream, and if so are you aware of this? I've seen a few videos with this issue. If there's a particular packager that has this issue, it would be good to push them to get it fixed.

Subtitles:
- There's three bytes of unexpected data in front of the WEBVTT header in the captions file. I'm not sure if that's an issue with the file, or whether it's actually expected.
- We don't handle <c.red.caps> yet, but neither do we filter it out of the displayed text. We should fix that.
- The captions only cover the first 2:22 of the video. I assume this is a known issue.
 - We'll try and fix the base issue in the first half of next week.
- We'll also fix handling of the three bytes of data at the front of the WEBVTT file. This is a byte-order-mark, and is allowed by the spec. Plus strip out WebVTT Cue components from the text, since we don't handle them.
  I'm not certain, but it sounds like either an issue with the decoder implementation, or an issue with the actual underlying stream that you're trying to play. I'd guess it's more likely to be the latter, but we'd need a test stream that reproduces the issue to determine which of these is true.
 MediaCodecVideoTrackRenderer has an onInputFormatChanged and an onOutputFormatChanged. You say that the resolution is incorrect in onOutputFormatChanged. Could you look at what happens in onInputFormatChanged, and see if it's correct there? If it is, this is probably the same as #656.
 Assuming this is the same as #656. Closing.
  I'm going to merge this and then apply the suggested refinement on top. Thanks!
  As you've observed, we already handle indefinite live streams where the MPD doesn't need to be updated. The case you describe is different because, although it's a live broadcast, you already know the fixed duration of the broadcast.

One caveat that applies to both cases is that use of availabilityStartTime only works if the device can get a pretty accurate estimate of the server-side clock. Unless you want to assume that the device clock is accurate, this means a UtcTiming element needs to be present in the manifest (we don't currently use it, but will do soon - it's pretty easy to wire it up). Do you include such an element?
 Re your proposed fix - I agree we should use the unbounded case for 2.1 (although with an additional check that we're not trying to request a segment beyond the last index). I don't think we should use it for case 1 though, since my understanding is that in that case, only available segments should be listed in the SegmentList. In this case accurate client/server clock sync isn't supposed to be required, which it would be were we to use the unbounded case for it.

Does that sound reasonable/correct to you?
 This should be fixed, aside from the need to properly sync using UtcTimingElement. Please verify this fixes the problem for you. Thanks!
  I wouldn't be surprised if the device is just broken. The decoder appears to be failing here:

01-23 17:31:57.818: E/ACodec(8415): [OMX.amlogic.avc.decoder.awesome] storeMetaDataInBuffers failed w/ err -2147483648
 As far as I'm aware, this device hasn't passed Android CTS, and so is not supported. Please re-open if you know otherwise.
  Thanks for this! You'll need to sign the appropriate CLA before we can merge the change. See:

https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
 Thanks!
 By the way - Did you see any bad playback behavior as a result of this bug, or did you just spot it by reading code? Just curious.
  Your stream doesn't work in Safari or VLC either. For me at least. ExoPlayer actually does better than either of them (Safari goes into an infinite loop after 15 seconds of playback, and seeking doesn't work at all, where-as VLC draws lots of corrupt video frames).

I think the stream is probably invalid. Please re-open if you're relatively sure that's not the case.
 Apple define the HLS spec. So if it doesn't play in Apple's own players, then I don't think it can be called valid.
  - The stream plays back fine for me on both master and dev branches of ExoPlayer. It does take a long time for playback to start (it took about 30 seconds on my device). This is possibly due to the way the file is encoded, or due to your server having low bandwidth. It took a similarly long time to start when I tried playback with VLC on a laptop.
- Caching/downloading media is something you'd have to figure out and build yourself. Once the content is cached/downloaded, ExoPlayer provides the ability for you to play back from local storage.
  Can you be specific about which members you're concerned about? The members that are accessed from both threads are marked as volatile, SamplePool is protected with synchronization, and as you say the sample queues use ConcurrentLinkedQueue. I can't see anything not covered by these measures.

Thanks!
  Thanks!
  It's not a use case we're supporting. We'd encourage you look at DASH/FMP4/MediaDrm instead.

If you're feeling adventurous and if you really wanted to, you could probably get it working using FrameworkSampleSource, and manually removing all references to DrmSessionManager from MediaCodecTrackRenderer. It's not something we want to spend any time supporting though.
 If you're using DrmManagerClient (which is correct for Classic Widevine) and MediaExtractor, then you shouldn't need a MediaDrm instance at all. Classic Widevine isn't a use case that ExoPlayer is supporting, so I can't provide much advice in the way of what's wrong. Sorry.
  - I think the Equalizer is attached right away. Your problem is that you're calling setupEqualizer too late, so the bands aren't configured early enough. I think the right place to do your equalizer setup would be in onAudioSessionId, immediately after you instantiate it.
- We do throw away the AudioTrack and make a new one when seeking (it's more reliable), but the new one is supposed to use the same session id, so this should work. I'll try and get around to looking at this, but if you could debug also, that would be great. In particular, check whether the same (correct) session id is being used each time this code is executed:

https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/audio/AudioTrack.java#L257
 - Were you able to resolve the issue with seekTo? The issue does not reproduce on the device I'm testing with. If this is still an issue, please provide details of the device + exact Android build used. Thanks!
- Note that in a full solution, you should ensure that you release the equalizer when done. The best place to do this is in onDisabled, as below.

```
audioRenderer = new MediaCodecAudioTrackRenderer(......) {

  private Equalizer equalizer;

  @Override
  public void onAudioSessionId(int audioSessionId) {
    releaseEqualizer();
    equalizer = new Equalizer(..., audioSessionId);
    // Configure equalizer here.
    equalizer.setEnabled(true);
  }

  @Override
  public void onDisabled() {
    releaseEqualizer();
  }

  private void releaseEqualizer() {
    if (equalizer != null) {
      equalizer.release();
      equalizer = null;
    }
  }

};
```
 This was diagnosed as a platform issue where the effect wasn't being properly included in a reference count associated with the audio session. The issue was fixed in Lollypop, but affected earlier releases of Android.

We've added a workaround that you can use to get correct behavior prior to Lollypop, but we've left it disabled by default because attaching effects isn't a common use case, and we'd rather not have the workaround enabled in all cases (it's pretty hacky). To make use of the workaround, you can simply flip this flag to true:

https://github.com/google/ExoPlayer/blob/dev/library/src/main/java/com/google/android/exoplayer/audio/AudioTrack.java#L100
  It'll need submitting into dev. You'll also need to sign the appropriate CLA before we can merge the change. See:

https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md

Thanks!
 `FrameworkSampleSource` is not part of ExoPlayer V2, since we removed all our dependencies on Android's `MediaExtractor` API. It's probably best if you file a new issue/question explaining what you're actually trying to do (in terms of what the use case is, rather than "I want to use a FileDescriptor"). There may well be an easy alternative or better way of doing whatever you're trying to do.  The HLS implementation on Android is known to suffer from some issues on earlier platform releases. We're solving this in ExoPlayer by implementing it ourselves, at the library level.

Rather than doing what you're doing above, you should use the "dev-hls" branch of the ExoPlayer project, and use TYPE_HLS rather than TYPE_MISC (there are already some HLS samples in the demo app on that branch).

You should find this approach will work more consistently across devices and platform releases.
  This issue was filed before ExoPlayer got its own MP3 extractor, so I'm guessing it was using the (now deprecated) FrameworkSampleSource. Please re-test using a recent version of ExoPlayer, where you should be using ExtractorSampleSource to make use of ExoPlayer's built in MP3 extractor.

Closing as obsolete. Please file a new issue if you're still able to reproduce using the steps above.
  We can add an additional FrameworkSampleSource constructor that takes a FileDescriptor, if that helps. I'd like to understand the "still doesn't solve the problem" part though.
 Closing this as the change is merged. It's likely the "still doesn't solve the problem" issue is something else. Feel free to open a separate issue to track that, if needed.
  See #227.
  Please provide a sample file that fails to play locally. Thanks!
 Closing due to inactivity. Note that the error is possible related to use of FrameworkSampleSource, which is no longer recommended (use ExtractorSampleSource instead).
  If you don't have a manifest or a representation, then you're not trying to play DASH content, and hence DashChunkSource isn't the right thing to be using. I'm unclear what you're actually trying to play. The H264 is presumably in a container format of some kind.
 We have no plans to support MJPEG. It's unclear what the "live streams" in this issue refer to, so I'm going to close it. If you can provide specific details of exactly what formats you're trying to play, ideally with a link to a test stream, then please feel free to open a new issue containing this information.
  They are both specifications for the adaptive delivery of media. DASH is an ISO standard. The Smooth Streaming specification is controlled by Microsoft. ExoPlayer supports both with Fragmented MP4 only. Given this, in practical terms, they are pretty similar to one another, with the main difference being the different formatting of the manifests. There are many other minor differences.

If you're choosing which one to use then I'd recommend you use DASH, because it's an ISO standard and because it's likely better aligned with where the majority of the industry is heading.
  I'm pretty sure this is already fixed on the "dev" branch. See: https://github.com/google/ExoPlayer/pull/208.

I'm going to close this based on the above, but if you can still reproduce the issue on the dev branch then please comment/re-open.

Thanks!
  ExoPlayer can draw video into any Surface. The surface is set by the application via MSG_SET_SURFACE to the MediaCodecVideoTrackRenderer. Beyond that, anything regarding translation of the Surface is up to the application developer, and is outside of the scope of ExoPlayer.

Depending on how much control you need, SurfaceView might be good enough, or you might want to look at TextureView and SurfaceTexture.
  I'm not sure I follow your diagnosis. According to the stack trace, the initial connection is failing at L363, which constructs a URL from a String (I'm not sure where Uri is involved). The URL string at the top of the stack trace also appears to have correctly preserved the underscore.

Could you clarify exactly what you think is happening? Thanks!
 Thanks. Do you know of a potential fix or workaround at the ExoPlayer level? It feels to me like the underlying issue you describe is fairly deep in platform code.
 Actually, please see https://code.google.com/p/android/issues/detail?id=37577. The short version is that underscores aren't actually valid characters URLs.
  The answer above is correct. It's fairly stable at this point; and will be pushed to master relatively soon now.
  FrameworkSampleSource is deprecated, so we wont be fixing this.
  The input buffers aren't supposed to be re-obtained. They're supposed to be large enough already. In HlsChunkSource maxWidth and maxHeight indicate the maximum dimensions of the video. We attempt to infer the maximum from the resolutions indicated in the master playlist. These are optional, so we default to assuming 1080p if they're missing.

Are you attempting to switch to something higher than 1080p? If not, what resolution are you switching to, how big is the input buffer that's not big enough, and how big is the frame? Thanks!
 Note - There is a typo in the code that calculates maxHeight in HlsChunkSource (I'll fix this), but unless your video is taller than it is wide, that wouldn't cause the issue you're seeing.
 Ok. Knowing what device + build you're running would also be useful. It sounds a bit like a device specific error where the buffers aren't being sized to be large enough. However if that's the case we can likely fix it by explicitly requesting larger ones using http://developer.android.com/reference/android/media/MediaFormat.html#KEY_MAX_INPUT_SIZE
 What value did you set MAX_INPUT_SIZE to when you attempted to set it? I'm guessing you tried 1382400, but if you didn't then it would be interesting if you could try that exact value.
 That sounds quite likely to break something else. Setting MAX_INPUT_SIZE is the correct solution, if it can be made to work. Does it work if you set it to 1382400 (not that this is quite right either, but it will give an idea of whether a MAX_INPUT_SIZE based solution is possible)?
 Closing due to lack of response.
  That is a network connectivity issue, or an issue with the server. The fact that it plays through a browser doesn't imply this isn't the case. It may be that whatever browser player you're using has a larger connection timeout, or retries harder before failing, or is picking a different bitrate stream. That server doesn't respond to pings at all for me.
 Are you saying HLS and SmoothStreaming work correctly using ExoPlayer, but DASH fails? Or were you testing HLS and SmoothStreaming with a different player? Also, is this a Live stream, or an on-demand stream?

Knowing the above would be useful/interesting, however there's not too much we can do to help unless you can provide a publicly accessible stream that reproduces the issue (if you could do that, that would be great, and we'll take a look).
 Closing due to inactivity / nothing actionable.
  Thanks for the report. For that particular live manifest, the video and audio StreamIndex elements appear to frequently swap order in the manifest when it gets updated. Currently ExoPlayer expects them to stay in the same order, and the swapping of the order results in the crash.

I'll take a look at using something more stable than StreamIndex instead, which should fix this issue.
 Do you happen to know why the order is swapping? Could you just update the server-side to keep the order stable?
 For your particular case, you could make a local modification to ExoPlayer that sorts the StreamIndex elements by the Name attribute. That should be pretty easy, and get things into a stable order for you.

I'm not sure we want to do this in the official project though, unless the problem is known to be widespread (I haven't seen any other examples of the issue to date).
 I'm going to close this for now. We may consider taking a fix in the official project if there are multiple vendors who are affected by this issue. Otherwise I'd advise you apply a local fix as per my response above, until the server-side issue can be resolved.
  You should be able to play AAC in the demo app using TYPE_AAC now, which gives you a lot more control over this kind of thing. If you follow the code path into ExtractorRendererBuilder then you'll find the place where the buffer size is set.
  We merged these changes into dev, and simplified it somewhat. Please check that it works with your streams, and file a fresh issue if this isn't the case (ideally with a diagnosis :)). Thanks for your help getting this feature supported; much appreciated!
 Great, thanks!
  https://github.com/google/ExoPlayer/pull/53 would fix this, but it's still a little unclear whether we should do that. It depends a bit on whether these failure modes are permanent or temporary (the latter may occur if the mediaserver dies underneath us, I think).

Please clarify:
- What were you trying to play when you observed this? If possible, please provide the mime type being checked when the crash occurs.
- Was it 100% reproducible, or did it just happen once?
 Where did the stack trace come from? If you have a released application and you got the report from logging/feedback reports, then some idea of the number of reports relative to the number of playbacks on these devices would give a good idea of how serious the problem is.

I suspect that it's a problem that occurs when mediaserver happens to die just at the point where the codecs are being queried, in which case it should occur very infrequently.
 Thanks for the input. I think there are two distinct problems here:
- MediaCodecUtil calls methods in the framework that make cross-process calls, I think into mediaserver. There is a very low but non-zero probability of sporadic failure (e.g. if mediaserver crashes), across a wide range of devices.
- A small range of older devices always fail when queried, due to incorrect configuration. You observed this for the GT-I9100. I got hold of one of these and confirmed that the issue is 100% repeatable.

Both of these currently manifest themselves as runtime exceptions which application developers are unlikely to catch, meaning their applications will end up crashing when either issue occurs.
- I think the correct fix for the first problem is to have MediaCodecUtil throw a checked exception. This forces an application developer handle the case, meaning they wont ship an application that will crash when the problem does occur (assuming they handle it in a sensible way). It's analogous to IPC in Java throwing RemoteException.
- For the second problem, I think we should be doing as you say and crowd-sourcing any fixes for specific bad configurations. For these particular cases we want MediaCodecUtil to hide the exception and carry on looking for a suitable decoder. We need to target these fixes quite precisely (e.g. specific device/decoder combinations), so that we don't end up hiding any instances of the first problem.

I'll submit a change to throw a checked exception, and then we can follow up on the second problem and figure out exactly how specific the targeting should be.
 Sporadic failures are now indicated with a checked exception.

The remaining work here is to add special cases for any known cases where 100% failure occurs. In these cases we should skip/ignore/fill-in as appropriate, to replicate what the underlying platform would have done were it configured correctly.
 Closing this. We'll use #53 to track the incorrect configuration problem.
  - We'll shortly be adding the ability to play AAC without having to rely on the MediaExtractor.
- One question: You don't expect to be able to seek within AAC, right? I don't think it normally exposes any kind of index to help you to do that.
 Correction - It looks like it should be possible to support seeking (since the platform extractor does it).
 Heh. It actually looks like the platform extractor scans through the entire stream to construct an index for seeking. Which is perhaps why it appears to get stuck (this is conjecture). In any case, it appears seeking should not be supported after all.
 I have this more or less working, but it seems like this is fundamentally a bad way to stream audio, particularly to mobile devices.

The client opens a network connection, starts streaming audio, and everything is fine. But what's the client supposed to do when it encounters a network error (which is highly likely over a long period of time on a mobile device, since it may well be switching between WiFi/mobile, or just on a flaky mobile connection etc). It seems like it's impossible for the client to request data starting from exactly where it encountered the error, meaning it's impossible to seamlessly recover from any network error.

Furthermore, it appears to be impossible to distinguish between a live AAC stream such as the PowerFM example, and a non-live AAC file. Desired retry behavior is different in each case, because for the non-live AAC file you want to make a range request starting where you left off, where-as for the live case you want to request starting from a 0 byte offset.

Is that correct? Am I missing something? If I am correct, how exactly is this ever supposed to work nicely on a mobile device, unless it's stationary with a solid WiFi connection?
 - It's not true to say that there's no way to seamlessly recover for live broadcasts. In all live streaming systems the client player will maintain a small buffer. For this particular case the rate at which the server serves the audio effectively controls the client's buffer size to between 5 and 10 seconds of audio. So if an error occurs, you have between 5 and 10 seconds to recover in a completely seamless way by requesting more data before the buffer fully plays out. HLS and DASH both allow for this type of seamless recovery, and fast recovery is often possible in cases like switching from one network to another, which is why I'm saying that the approach here is inferior (it is).
- Having said that, your points are otherwise valid. It is true that seamless recovery is less important in this case, and that if you can't recover in within the buffer period then seamless recovery isn't possible anyway (and you want to jump forward rather than falling behind the live broadcast).
- We can probably come up with a way of supporting what's required here. Basically, we'd need to disable the existing retry logic (because it doesn't work), wait for the buffer to fully drain, and then retry non-seamlessly.
 This should all work nicely now. For URL (2) in the original report, use TYPE_MP3 in the demo app. It should play nicely, and resume properly when errors occur (unless the outage is extended, in which case the application will be notified of the error). URL (1) doesn't seem to be up any more.

Separately, we're aware that it might be a bit painful to have to know the type of the media prior to playback. We'll be looking at sniffing the format automatically at some point (possibly not particularly soon though).
  Yes. That's right. Alternatively (and slightly cleaner) you can just use SurfaceView directly, rather than using VideoSurfaceView (the whole point of this class is to be able to match the video's aspect ratio, so given you don't want to do this, you can just use SurfaceView directly).
  - If you're asking about HLS support in general, it's being developed in the dev-hls branch. If you look at the demo app in that branch, you'll see there are some example HLS streams that are playable, and should serve as a guide as to how you can play your own.
- If you're asking specifically about the ability to seek within the live window of HLS DVR streams, your request is a duplicate of #87.
  Please re-open if you can provide a test stream to reproduce this issue. These kind of reports aren't useful without ways to reproduce them. Thanks!
  This looks pretty good; thanks! I think you're missing some handling in the flushCodec method though. It looks like you probably need to do the following in that method:
- Set hasQueuedOneInputBuffer to false
- If the state is something other than REINIT_STATE_NONE, release and re-init the decoder
- Set the state to REINIT_STATE_NONE

What do you think?
 I still don't see a signed CLA on my side.
 Yes, it is. Thanks. I'll be sure to merge this soon.
 And. Finally. It is done! Thanks for your patience.
  Thanks!
  Is this a real problem you've actually encountered, and if so in what situations does it occur (the full stack trace would be helpful)? Calling this method with a zero length list would be broken, and having it return null would likely just move any crash elsewhere (since the caller typically wont be expecting null to be returned).
 Right. This isn't the right place for the check though. The check should be happening way sooner, before the object in which the crash occurs is even created. Also, I suspect this issue is specific to the emulator.

If you could provide the stack trace when the crash occurs, and state which video(s) it occurs for, that would be helpful.
 None of those are actual crashes that take down the app though, unless I'm mistaken? What the user sees in all of theses cases is an error message, which is effectively what's supposed to happen.
 We should propagate clearer exceptions back to the application. It's then up the the application to word the error message to the user. So this issue is effectively tracking propagating something better than, for example, NullPointerException, back to PlayerActivity.
 Printing internal errors is generally useful for debugging. It's actually the demo application that's printing the errors, not the library, because it's probably something a developer would want to do.

But yeah, let's leave this issue to track making the exceptions thrown a little clearer.
 I think the exceptions are fairly sane now. I suspect that the Genymotion simulator may fail to pass Android's CTS tests for video, in which case it shouldn't really be considered representative of a real Android device (NB - According to [this](https://m.google.com/app/basic/stream/z12st5kpfk2vjvqui22cxr0xuxmigp34304?cbp=l67qxg7hagr&partnerid=operamini1104&sview=27&cid=5&soc-app=115&soc-platform=1&spath=/app/basic/%2BGenymotionEmulator/posts&sparm=cbp%3Dzd7gr8otkypv%26partnerid%3Doperamini1104%26sview%3D27%26cid%3D5%26soc-app%3D115%26soc-platform%3D1%26stct%3DChYQ_PWi84-KwQIYsOfUman_wAIgACgHEhQIABCw59SZqf_AAhjCxJHek4a4AhgC%26spath%3D/app/basic/108625703815216743905/posts%26sparm%3Dcbp%253D1q4yv25tei143%2526partnerid%253Doperamini1104%2526sview%253D27%2526spath%253D/app/basic/102082383403038732632/posts%2526sparm%253Dcbp%25253D18neyar9iq2s2%252526partnerid%25253Doperamini1104%252526sview%25253D27) they were passing nearly all CTS tests some time ago, but specifically noted video as an area where they still had failures.
  Thanks, I'll fix this.
  Sorry for the delay. I think https://github.com/google/ExoPlayer/pull/221 is the one that goes into the correct branch, so I'll comment there. Can we close this one?
  The audio decoder claims the audio track is invalid. Please don't file this kind of issue without test streams that reproduce the problem, since there's nothing we can do to debug the problem.
  We don't handle pure AAC segments yet (we handle AAC, but we currently expect it to be wrapped in the MPEG-TS container).
 Sure. Right now we assume that each segment is MPEG-TS. So when it comes to making a segment, we create a TsChunk here:

https://github.com/google/ExoPlayer/blob/dev-hls/library/src/main/java/com/google/android/exoplayer/hls/HlsChunkSource.java#L326

As the TsChunk loads its data is fed into the TsExtractor:

https://github.com/google/ExoPlayer/blob/dev-hls/library/src/main/java/com/google/android/exoplayer/hls/TsChunk.java#L121

Since the segments in this case are AAC rather than TS, the extractor doesn't parse anything useful out of them. You'd need to modify the code at one of these levels to handle AAC. You could either do this by having TsExtractor understand AAC data, or by having a completely different Chunk object for AAC, with its own extractor.
 Good news. It's actually really easy to build this on top of some of the recent refactoring work that has been done to TsExtractor. I put together a proof of concept in a few minutes as follows:
- Add the following fields to TsExtractor:

```
private final AdtsReader adtsReader;
private boolean firstSample = true;
```
- Initialize them in the constructor, and add the reader to sampleQueues:

```
firstSample = true;
adtsReader = new AdtsReader(bufferPool);
sampleQueues.put(0, adtsReader);
```
- Replace the TsExtractor.read method with this implementation:

```
public int read(DataSource dataSource) throws IOException {
    int bytesRead = dataSource.read(tsPacketBuffer.data, tsPacketBytesRead,
        TS_PACKET_SIZE - tsPacketBytesRead);
    if (bytesRead == -1) {
      return -1;
    }
    tsPacketBytesRead += bytesRead;
    if (tsPacketBytesRead < TS_PACKET_SIZE) {
      return bytesRead;
    }

    tsPacketBuffer.setPosition(0);
    tsPacketBuffer.setLimit(TS_PACKET_SIZE);
    adtsReader.consume(tsPacketBuffer, firstSampleTimestamp, firstSample);

    firstSample = false;
    tsPacketBytesRead = 0;
}
```

And you should find your sample works :). Note that the use of TS_PACKET_SIZE is pretty arbitrary here. Any size will do.

I'll try and follow up this week with a proper AacExtractor, which will basically just be a TsExtractor with the above changes, and with all the parts that become unnecessary stripped out (i.e. most of it).
 I expect to push a change to enable AAC/ADTS elementary streams within the next few days.

Note:
- The change will allow playback of HLS presentations that only contain AAC/ADTS streams. Adaptive bitrate support will work where there is a master playlist that provides multiple AAC/ADTS streams.
- The change will not enable alternate-audio-streams-selection for videos with multiple audio streams (although it does get us closer to being able to add this feature), or enable the AAC/ADTS audio-only stream in HLS presentations whose other streams do have video (we filter this stream as a policy decision, because we don't think users will want adaptive switching that breaks the video component of their playback).
 Fixed in dev.
  Good spot! Although you need to sign the Google CLA before we can merge this. Please see: https://github.com/alexandrite/ExoPlayer/blob/master/CONTRIBUTING.md

Thanks!
  Looks like you've only imported one of the projects in the tree. There are two.
  I believe this is the same as #548, which is now mostly fixed (and will be fully fixed next week). Let's use that issue for tracking.
  Investigating. Seems behavior changed in: 4280511a33e961ad18670cb99432cc617ce32dfe
 ExoPlayer doesn't like the chunklist_b320000.m3u8 variant of this stream. Or chunklist_b256000.m3u8, although the issue is less pronounced. I'm not sure why yet.

The underlying issue exists prior to the change referenced above; it's just that before that change the player wouldn't have switched to these variants.
 Are you sure the audio is correctly encoded? VLC doesn't seem to be able to play it either. It spews logs like:

VLC media player 2.1.4 Rincewind (revision 2.1.4-0-g2a072be)
[0x2044118] main libvlc: Running vlc with the default interface. Use 'cvlc' to use vlc without interface.
[0x7fa2e0c04388] httplive stream: HTTP Live Streaming (jgsite.cdnsrv.ril.com/music.hdi.cdn.ril.com/mod/_definst_/smil:test/1_2021_2.smil/playlist.m3u8)
[0x7fa2e0c0f1a8] ts demux error: MPEG-4 descriptor not found
[0x7fa2e0ca7c98] packetizer_mpeg4audio packetizer: AAC channels: 2 samplerate: 44100
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4095
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4095
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4095
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4095
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 0
[0x7fa2e0c0f1a8] ts demux error: libdvbpsi (PSI decoder): TS duplicate (received 0, expected 1) for PID 4095
 Assuming the media is bad. Please re-open if you disagree.
 Exact same comments as in the 3rd, 4th and 5th posts in this thread apply to that stream as to the one at the top of this issue. I tried the stream in MX Player, and wouldn't describe it as playing fine. It jumps around all over the place for me, and MX Player spews the following messages about the media into the logs. So yes, I'm pretty sure the media you're trying to play is broken:

12-24 15:21:02.686: W/ESQueue(190): First ADTS AAC frame length is 917 bytes, while the buffer size is 2789 bytes.
12-24 15:21:02.695: W/ESQueue(190): First ADTS AAC frame length is 936 bytes, while the buffer size is 2807 bytes.
12-24 15:21:02.697: W/ESQueue(190): First ADTS AAC frame length is 938 bytes, while the buffer size is 2903 bytes.
12-24 15:21:02.705: W/ESQueue(190): First ADTS AAC frame length is 951 bytes, while the buffer size is 2712 bytes.
12-24 15:21:02.708: W/ESQueue(190): First ADTS AAC frame length is 936 bytes, while the buffer size is 2807 bytes.
12-24 15:21:02.716: W/ESQueue(190): First ADTS AAC frame length is 936 bytes, while the buffer size is 2808 bytes.
12-24 15:21:02.721: W/ESQueue(190): First ADTS AAC frame length is 936 bytes, while the buffer size is 2807 bytes.
12-24 15:21:02.729: W/ESQueue(190): First ADTS AAC frame length is 1026 bytes, while the buffer size is 3011 bytes.
12-24 15:21:02.732: W/ESQueue(190): First ADTS AAC frame length is 867 bytes, while the buffer size is 2723 bytes.
12-24 15:21:02.740: W/ESQueue(190): First ADTS AAC frame length is 1035 bytes, while the buffer size is 2939 bytes.
12-24 15:21:02.745: W/ESQueue(190): First ADTS AAC frame length is 870 bytes, while the buffer size is 2650 bytes.
12-24 15:21:02.753: W/ESQueue(190): First ADTS AAC frame length is 997 bytes, while the buffer size is 2906 bytes.
12-24 15:21:02.756: W/ESQueue(190): First ADTS AAC frame length is 907 bytes, while the buffer size is 2690 bytes.
 The reason for the difference is already described in the third post in this issue (i.e. the older version switch up to the higher quality variants of the stream, which are the ones that are broken).

Ideally you should replace the content. Alternatively you can play the initial stream directly, which will prevent the player from switching to the other variants. For the example link above, you can play the initial stream using:

http://music.hdi.cdn.ril.com/mod/_definst_/smil:test/1_2021_2.smil/chunklist_b32000.m3u8
  I tried on a GT-P5110, which should be functionally identical, and could not reproduce this issue. Did it persist after the device was rebooted?

The logs above aren't relevant, as far as I can tell.
 NB - If this refers to a crash selecting any DASH or SmoothStreaming video (but not HLS), then there is an issue, which will be fixed today.
 Issue referenced fixed by 57faa497561488cd81d485fb13849f0c9348c90d. Closing, since if it wasn't that, we'll need more information to do any further investigation.
  The assertion is valid. It looks like these streams trigger the error case in readPacketStart (the method immediately below the one containing the assertion). This error case is currently just commented, but allows fall-through. An invalid packet length is then read, which then causes the assertion to be violated.

A simple fix is to handle the error case by skipping the packet (as opposed to doing nothing). Which we can do. It would be good to spend a bit of time seeing if something else is wrong first though.
 It looks like these streams start with that many bad PES packets (although it's possible we're just misunderstanding exactly how to handle the start of these streams). I suggest you take the issue up with BrightCove.
 We're going to relax the assertion to a warning instead, since in practice it seems we can survive cases where the assertion is broken. This should fix the video you reference. We'll push an update today or tomorrow. Thanks!
  When both audio and video are disabled, you're asking the player to play nothing, which is why it transitions to the ended state.

I don't really understand why both of these things would happen at the same time. Typically the calls to mute/unmute the audio would happen from the main thread in a single block of code. Ditto for swapping the surface. Which implies these operations shouldn't overlap?

If the reason is that you're providing the user with the option to explicitly mute the audio track, which leaves it disabled indefinitely, then you could either:
- Set the volume of the audio track to 0 rather than fully disabling it, so that the track is still enabled.
- Not disabling/enabling the video when you push the new surface.
 Thanks!

I'm not sure though; I can disable/enable both audio and video tracks separately in the demo app, whilst playing and whilst paused, without anything bad happening. The behaviour you describe only happens for me if I disable both tracks simultaneously. What type of media are you playing (DASH/HLS/SS/MISC)?
 Hm, I still don't see the behaviour you describe. Are you sure you weren't disabling both audio and video at once? Which is what I implied from your initial report, since you say the audio is disabled, and that from this state you were then disabling the video to swap the surface?
 What are you passing to blockingClearSurface in step 2? Are you passing null at that point, and then the new surface in 3?
 I think the video renderer probably was being disabled internally, which would have led to both audio and video renderers being disabled simultaneously, and hence a transition to the ended state. We've recently updated the logic in DemoPlayer and MediaCodecVideoTrackRenderer so that this doesn't happen, and so the issue should be resolved. The relevant change is here: https://github.com/google/ExoPlayer/commit/61a86295fd3c798e24083acd228d11f832770b68
  I don't think we'll be spending time on this internally. We definitely wouldn't introduce a dependency where ExoPlayer needed the PlayReady SDK.

If you want to investigate what hooks would be required to allow easy integration then feel free; we'd consider adding them, provided they're not particularly intrusive.
  This report isn't useful without much more detailed information.
  It's fixed now. Sorry, should have said that!
  Probably these devices don't have adaptive decoders. This is checked here in the demo app:

https://github.com/google/ExoPlayer/blob/b80569237b575b7619e0eff73c305c116a782a91/demo/src/main/java/com/google/android/exoplayer/demo/full/player/HlsRendererBuilder.java#L83

and used to determine the adaptive mode. ADAPTIVE_MODE_NONE is selected if the decoder isn't adaptive. You can just use ADAPTIVE_MODE_SPLICE in all cases to force adaptation, but the adaptation wont be completely seamless; you'll likely see a 50-100ms pause when the format changes. In a proper application, you'd probably want to pick a single format and use, or give the user a manually quality toggle, for devices without adaptive decoders.

I think I'll change the behavior of the demo app to always select ADAPTIVE_MODE_SPLICE, since this is consistent with DASH/SmoothStreaming in the demo app. But as above, it wont be seamless, and you probably wouldn't want this for a production app.
 Change mentioned in the comment above was done in 39c07d570c75f7cf11206f802ec807a770b4278c.
  @sachinbhanot could you share the URLs / the playlists?
  looking whether the file is **.m3u8** should be good enough.
.m3u format used for MP3 playlists.
  Ack. Thanks!
  Please provide test content that reproduces the issue (Let's start with VOD).
 Fixed by:

https://github.com/google/ExoPlayer/commit/c8e5988e6d2ca8c2391042adb2f3ea0443f937f3
https://github.com/google/ExoPlayer/commit/6f1832fb66df5f9b8844d595ab0b13ef7dab3e34
  - You need to sign the CLA before we can accept this. See https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
- Please could you move it to the dev branch? We'll shortly be merging it back into master anyway.

Thanks!
 I've made an equivalent change to the dev branch.
  I'm not sure what this is?
  This is caused by a bug that results in the same data being fed to the extractor twice in the case where a chunk fetch fails half way through.
  Tracking in #174 
  The example plays fine for me on a Nexus 5 running Lollypop (note - I downloaded the sample and played it from elsewhere, since playback doesn't appear to work directly from the dropbox link).

I'ts unclear whether the logs above contain the relevant information. If playback actually failed I'd expect to see a stack trace. Maybe playback is wedged rather than flat-out failed.

Clarifying questions:
1. Does this happen on the dev branch, which contains some fixes for issues similar to this.
2. What device / OS build are you able to reproduce this on?
 Thanks. In that case, you're probably just running into the same issue as you described in #18 . Closing for now, but feel free to re-open if the issue does reproduce.

FYI - We plan to merge a release from dev->master this week.
  What's your actual use case for this? You're not actually sending text as bitmaps, presumably?
 In the US at least, I don't think that delivering subtitles as bitmaps would meet FCC captioning requirements, which mandate that the user should be allowed to customize their visual appearance. Bitmap subtitles also wont scale well or look as nice as text rendered on the device, so it feels to me like this is something people should be looking to shift away from.

Do you agree with the above? This doesn't necessarily mean we wont add support. I'm just trying to gauge where this is actually useful, and for how long it will remain useful. Thanks!
 Hi. We've had bandwidth to look at the TV case in a little more detail recently, so it might be a good time to re-assess this (no promises of course). @AquilesCanta - What do you think? The pull request is here: https://github.com/google/ExoPlayer/pull/1781

@tresvecesseis - Regarding the pull request, can you confirm the code is OK with respect to whatever Google CLA you've signed [here](https://github.com/google/ExoPlayer/blob/release-v2/CONTRIBUTING.md) and if so remove references to other projects from it? Thanks!
 DVB support has been added in https://github.com/google/ExoPlayer/pull/1781 and cleaned up in https://github.com/google/ExoPlayer/commit/156bc52c8f73154bd847c352acb9227324ff7301.

Is there anything else to do here, or can this issue be closed? I'll re-open #1583 and close this one; thanks!  A more complete stack trace is below. I'll find out what the decoder error signifies and provide an update.

```
D/EventLogger(23263): state [1.29, true, B]
I/OMXClient(23263): Using client-side OMX mux.
E/OMXMaster(23263): A component of name 'OMX.qcom.audio.decoder.aac' already exists, ignoring this one.
W/SoftAAC2(23263): aacDecoder_ConfigRaw decoderErr = 0x2003
E/ACodec  (23263): [OMX.google.aac.decoder] ERROR(0x80001001)
E/ACodec  (23263): signalError(omxError 0x80001001, internalError -2147483648)
E/MediaCodec(23263): Codec reported err 0x80001001, actionCode 0, while in state 6
E/ExoPlayerImplInternal(23263): Internal runtime error.
E/ExoPlayerImplInternal(23263): java.lang.IllegalStateException
E/ExoPlayerImplInternal(23263):     at android.media.MediaCodec.native_dequeueOutputBuffer(Native Method)
E/ExoPlayerImplInternal(23263):     at android.media.MediaCodec.dequeueOutputBuffer(MediaCodec.java:1033)
E/ExoPlayerImplInternal(23263):     at com.google.android.exoplayer.MediaCodecTrackRenderer.drainOutputBuffer(MediaCodecTrackRenderer.java:720)
E/ExoPlayerImplInternal(23263):     at com.google.android.exoplayer.MediaCodecTrackRenderer.doSomeWork(MediaCodecTrackRenderer.java:420)
E/ExoPlayerImplInternal(23263):     at com.google.android.exoplayer.ExoPlayerImplInternal.doSomeWork(ExoPlayerImplInternal.java:398)
E/ExoPlayerImplInternal(23263):     at com.google.android.exoplayer.ExoPlayerImplInternal.handleMessage(ExoPlayerImplInternal.java:203)
E/ExoPlayerImplInternal(23263):     at android.os.Handler.dispatchMessage(Handler.java:98)
E/ExoPlayerImplInternal(23263):     at android.os.Looper.loop(Looper.java:135)
E/ExoPlayerImplInternal(23263):     at android.os.HandlerThread.run(HandlerThread.java:61)
E/ExoPlayerImplInternal(23263):     at com.google.android.exoplayer.util.PriorityHandlerThread.run(PriorityHandlerThread.java:40)
```
 @spark85 - It would be helpful if you could post one of your working streams too, so that we can take a look and try and figure out if something is actually slightly different.
 The key difference appears to be that the media in the two broken cases is AAC-LTP, where-as the working samples are AAC-LC. It doesn't look like Android officially supports AAC-LTP:

http://developer.android.com/guide/appendix/media-formats.html

Do you have control over the encoding process for these files, and if so can you just use AAC-LC instead?
 - The decoder is provided by the platform, not by ExoPlayer. I don't think we have any plans to add a library-level software decoder for AAC-LTP. It doesn't look like AAC-LTP support is particularly universal.
- It would be MediaCodec you'd be replacing, not MediaExtractor. You could implement your own TrackRenderer specifically for AAC-LTP that hooks into a software decoder that you'd then need to bundle, but it's not trivial.
- If you're allowing users to upload samples then IMO converting them server-side into supported formats is the correct approach (either that or failing the user's upload if they try and upload something that's not supported). Otherwise you're bound to run into issues such as this one (not just on Android, and not necessarily just AAC-LTP).
 That does seem odd. I'll see what I can find out too. I'm not sure about your AAC_ELD question, sorry!
 Good to know. I'm going to close this, since the initial issue can only be fixed by having the underlying Android implementation support AAC-LTP. This is something I'll follow up on internally. Thanks!
  Ideally pull a full bugreport, upload it somewhere, and link to it from here.

adb bugreport > my_bugreport.txt
 Thanks! There are a whole bunch of socket timeouts in that bug report, so this would appear to be working as intended. The player is basically failing to fetch the media, and the buffer is emptied before it recovers, which causes the playback to fail.
 You'd still end up with clients repeatedly buffering (rather than failing). So I think you really need to fix whatever is wrong on the infrastructure side.
  Add android:largeHeap="true" to your application's manifest.
 Alternatively, you can pass a smaller target buffer duration into the HlsChunkSource constructor. The default changed from 20 seconds to 40 seconds, which is why you're seeing this now when you weren't seeing it previously. Setting it back down to 20 seconds again will probably also solve the problem.
 HlsChunkSource has a constructor that takes three more arguments. Use that one if you want to change the values.
 You need to pass values in milliseconds. Try 20000, 5000, 10000. Although really you're better off leaving the values at their defaults and using largeHeap. 20 seconds of buffer isn't a huge amount if you're ever expecting variable network connectivity.
 How large is the heap when it crashes when you have largeHeap enabled?
 We can't investigate this without more information (e.g. a test stream that reproduces the problem).
 What points in graph correspond to doing what? There isn't a releasePlayer() method in ExoPlayer.
 It looks like the memory is being correctly released to me (which is why the blue area drops down). Although the fact it gets up to 190MB is a bit concerning. That seems excessive.
 I really don't understand. Your own graph shows the memory being de-allocated. The fact that the process is still allocated a large heap (most of which is unused) isn't relevant.

What version of Android are you running? I'll try and reproduce on a Nexus 7. It doesn't reproduce on the devices I have.
 I don't think there's a leak. I think we're just using way more memory than we should be, for some reason. On certain devices the largeHeap probably isn't enough.
 The main issue here is that each Sample is given a default size of 64K, however normally we don't need anywhere close to that.
 https://github.com/google/ExoPlayer/commit/c2d55acab52941cb883c891e6973b0aa4457ec5b should make this a lot better, to the extent that this should no longer occur. Memory usage should be well under half what it used to be (and up to 4x). There are still some further improvements to come (see the TODO in the change), but marking this as fixed.

Please re-open if you still see this. Apologies it took so long to get to the bottom of.
 The buffer overflow is likely due to you trying to play some insanely high bitrate content. Isn't this exactly the same as #150. 15Mbps is about 4x higher than what you should need.
 The discontinuity detected errors are due to the timestamps in the media drifting gradually over time relative to the actual amount of audio data in the stream. We see this with Apple's test streams. I haven't seen it with any content provided by third parties, so it's unclear to me how important it is to fix this.

The visual effects are audio/video slowly drifting out of sync by ~200ms, and then a slight visual artifact as the drift is detected and the video is snapped back to being in sync with the audio again.
 @sachinbhanot Does this happen when playing any of the test content in the demo app, or only when playing your own streams? If the latter, could you provide a stream that reproduces the problem.
 @sachinbhanot For the buffer overflow device, it would also be good to know the exact build, and ideally how large the buffers are that are overflowing.
 I got hold of an S4. It looks like your stream would have to contain a single frame larger than 2MB for this to occur. At this stage it's looking like a bad stream. Please re-open if you can provide a stream that reproduces, and we'll take a closer look.
  This isn't a crash. It's logging, and does not kill your process. If playback subsequently fails then ExoPlayer.Listener.onPlayerError will be called with an exception that wraps the internal exception that caused the playback to fail.
 The demo app has a retry button that appears that allows you to resume from the same position, so just replicate the logic of what tapping that button does.

If you want to automatically wire it up to a WiFi connection being established, then you should have your application monitor network connectivity and respond accordingly: http://developer.android.com/training/monitoring-device-state/connectivity-monitoring.html
  These kind of reports aren't useful without more information:
- How reproducible is the issue (just once, or every time)?
- A test stream that we can try to reproduce with.

This could just be a malformed input stream, but without more information it's hard to tell. Closing for now. Please re-open if you can provide information to help.
 Same as #183 . Closing, and using that issue as the tracking one, since it has a test stream.
  If it's an issue under MediaExtractor (likely) then it may only affect certain versions of Android.
- It would be interesting to know what version of Android you're running, and on what device, so that we can try and reproduce under matching conditions. If you can provide a sample video that reproduces the issue, that would be great too.
- In general we're trying to move away from depending on MediaExtractor, to avoid these kind of version specific issues (if this is one). DASH playbacks already avoid using MediaExtractor (and support webm/vp9), so if it's feasible, you're much better off going down that path.

Thanks!
 One more question - When you see this, are you seeking before starting playback / at the start / during playback? Or does it not make a difference?
 As per my comment 6 above this one, we've been moving away from depending on the platform's `MediaExtractor`. You should now be able to use `ExtractorSampleSource` rather than `FrameworkSampleSource` to play this media, and you should find that this approach works correctly across all devices.
  This is now tracked by #514.
  There's not enough information here to help. Please provide at least stack traces of any crashes, a test stream and some more detail about what exactly you're seeing.
  We'll have some changes in the next few days (maybe early next week) that should improve the adaptive logic for HLS playbacks.
 We initially select the first variant listed in the variant playlist, which I believe is what you're supposed to do when playing a HLS stream. You could modify the HlsChunkSource constructor to select a different initial variant if you wanted to.

Alternatively, you could re-order the variants in your HLS variant playlist so that the lowest bitrate one isn't first.
 See: https://developer.apple.com/library/ios/documentation/networkinginternet/conceptual/streamingmediaguide/UsingHTTPLiveStreaming/UsingHTTPLiveStreaming.html

In particular: "The first entry in the variant playlist is played when a user joins the stream and is used as part of a test to determine which stream is most appropriate. The order of the other entries is irrelevant."
 Some improvements have gone into the dev-hls branch. They don't help the single bitrate case mentioned in this issue though. I'm not really sure I understand what the actual issue is. If your one bitrate isn't sufficient for the client connection, then the client will experience buffering?
 40f3172237fcb0c58eb07c2766c989cd501e15ae addresses the question about being able to increase the buffer size, which is pretty much all you can do in the case of having a single bitrate. You can now tweak the target buffer size in milliseconds by passing a different value when instantiating HlsChunkSource. That change has also doubled the default value, which might be enough for you.
  Yeah, I think there's a problem there. Thanks for the report! We should be able to get this fixed quickly.
  See: http://stackoverflow.com/questions/23811315/gradle-fails-with-ambiguous-method-overloading-for-method-java-io-fileinit

This is due to Gradle not being able to find the SDK location, and thus failing when doing something that needs the Android SDK, such as getting the default Proguard file location.
  This is a synchronization problem where the H264Reader is cleared at the same time as read is being called.
 We'll have a fix for this today.
 What do you mean by "doesn't seek at all", exactly? Is the video playing or paused at the time, and what happens exactly?
 Maybe you have the same problem as in #69: 

"BTW, In your stream I noticed that only the first chunk (TS file) has PMT in it, and all the other chunks have no PMT after PAT. This seems broken to me. Where did you get it?"

It's unclear to me whether that is actually an encoding error, or whether files can be legitimately encoded in this way (which would suck). In any case, please file a separate issue and provide test streams. It's not related to the issue being tracked here.
  Yeah, please update everything. You'll need to sign the CLA before we can merge. See:

https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md

Thanks!
 One more thing - The change should go into the dev branch, not master.
  There are two problems:
1. Closed captions don't come as a complete text, they come in small buckets of a few characters attached to a video frame. Closed captions are represented by text characters and control characters, text characters are pieces of text that should be displayed, and control characters are the commands that signal to clear previously displayed text, roll it up (new line), backspace, etc. Player activity should process control characters and concatenate the coming text characters with already displayed ones, etc. Right now in the our demo code this is done in a very simple way (which is not enough for your stream) https://github.com/google/ExoPlayer/blob/dev-hls/demo/src/main/java/com/google/android/exoplayer/demo/full/FullPlayerActivity.java#L435
2. Closed captions are attached to video frames. In your specific stream they come in buckets of just two text characters per frame, and apparently (this is my guess) video frames come slightly not in order, for video itself it's not an issue, decoder reorders them correctly, but for closed captions this causes them to come up in the wrong order. This is why just concatenating text characters in the player activity is not enough here. This is my guess, i need to investigate more.
 @perchrh, please take a look at our fix, closed captions in your stream example should be displayed well now.
 Thanks @perchrh. This is fixed now.
I notice that your example has two separate captions (type 0 and type 1) that are getting mixed together. The first one is the actual captions and the second one is just the phrase "CNN Situation Room" that appears from time to time. Do you know whats the purpose of having two and how it's expected to be displayed? VLC doesn't display the second one at all, which is what our fix does as well.
  Yes. Should be 6.
   Question for anyone following this issue - Do we envisage a case where we need to support both VTT and embedded EIA608 simultaneously (in a single playback)? Or if VTT is present, is it always safe to assume that we can just rely on VTT tracks? I'm hoping we can just rely on VTT if they're present.
 First pieces that we need for this were: https://github.com/google/ExoPlayer/commit/4bb8bea9528a803a201b809e8bd4abe3f0b460e8, https://github.com/google/ExoPlayer/commit/72f093c4f6d66cea021756e1cbe931b52f55a79e
 This is now enabled on the dev branch.
  That's normally indicative of an abnormally large video sample. Please could you provide a test stream that reproduces the issue? Also, on which device(s) are you seeing this? Thanks!
 The warnings come from the lower level system, but I suspect they're expected. Decoders print out various debugging messages at the wrong log level on some devices.

15Mbit/sec is pretty crazy, you shouldn't be needing such a high bitrate. Netflix tops out at something like 3.5Mbit/sec (http://ispspeedindex.netflix.com/usa - note that the results are effectively capped by the bitrates they provide).  If the video keeps stopping and starting, it's quite likely that the path from server->client just doesn't have enough bandwidth.
 This bug was closed because the bitrate used was unreasonable large. #513 was closed because no-one updated it to provide more information. #714 is not closed and can be used to track the issue. We don't need more than one open bug.

Regarding the comment above, the HLS specification isn't actually defined in a way that makes it particularly easy for devices to take into account their own capabilities. Specifically, the HLS specification does not require that CODECS and (particularly) RESOLUTION tags are included in the master playlist, which are necessary for devices to sanely determine whether a variant is playable given the capabilities of the device. If you're not already doing so, including these tags will go some way to help devices to adapt to their own capabilities.
 We have no plans to support that kind of mechanism. Implementation wise it's horrendous. It basically involves downloading a chunk of data and allocating potentially large blocks of memory, only to fail and throw it all away when you can't play what you've loaded. It's reasonably unlikely that you'll be able to seamlessly switch to a different quality having already tried and failed to switch to a few other qualities, because an unsuccessful switch attempt takes time (e.g. to download the chunk).

Really, a player shouldn't need to go to these lengths to patch over the serving side not providing sufficient information and the HLS specification not being well suited to what's actually trying to be achieved. If a provider wants to provide a stream that's playable across devices with different capabilities, they should (a) think carefully about what the best technology is for doing this, and (b) if they do choose HLS, always include necessary information like the CODECS and RESOLUTION tags to make it easy for a client to do the right thing.
 I think your confusion is because there are multiple issues involved here. Some of the points in this issue are general ones, whilst other points are specific. To clarify, there are multiple possible failure modes when switching up to a higher quality:
1. Manifest omits RESOLUTION/CODECS tags. Client does not have an easy way to determine whether a stream is playable, tries to switch and fails.
2. Manifest includes RESOLUTION/CODECS tags. Device incorrectly advertises support for playback that it doesn't really support. Client tries to switch and fails.
3. Manifest includes RESOLUTION/CODECS tags. Device can support the resolution, but sizes its video input buffers too small for the resolution. Client reaches a large frame during playback and fails.

The first of these is a shortcoming with HLS and/or the people providing the streams, depending on how you look at things. The second and third issues apply to DASH as well, but tend to be isolated to a small number of not particularly popular devices. The Acer Iconia Tab 8 being one. Emulators and unofficial Android TV boxes are other classes of device where we see this kind of thing. We will get around to looking at the Acer device eventually, but it doesn't make sense to prioritise making playback "sort of work" on these devices. It's the underlying device that's at fault.

Note also that the player does not crash. It prints an exception in the logs and reports failure through its event listener.
 The emulator has not done a good job of supporting media, historically. It's not a "device" we make any effort to target, because it's not what end users actually use.
  As of https://github.com/google/ExoPlayer/commit/4280511a33e961ad18670cb99432cc617ce32dfe adaptive playback is enabled on devices with adaptive decoders.
 Same as #70 
  Thanks!
  @jonasl, thanks for the feedback!
also the 0 sign byte only needs to be trimmed if the length of BigInteger#toByteArray output array is bigger than 16 (or BigInteger#bitLength bigger than 127), so it's just one extra line, not much additional complexity.
  Thanks for this. We're working on being able to accept pull requests. If you haven't already, please consider signing the CLA so that we'll be able to accept pull requests from you as soon as we've figured this out. See: https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
 Once you've signed the CLA, please make a pull request for this and we'll accept it. Thanks!
 Merged.
  @crossle 
- Did you try making a modification to include SPS/PPS in the MediaFormat, and did it resolve the issue for you?
- Could you be specific about the exact device models that you're able to reproduce with. Thanks!
 Ok, thanks for the help! I'll get hold of a device and give it a try. My understanding is that devices are supposed to handle these being omitted as long as they're delivered in the first sample buffer (which they are). It's possible that these devices have an issue that means this is not the case. It's easy enough to get them added if this resolves the issue though.
 Could you see if this branch works for you: https://github.com/google/ExoPlayer/tree/dev-hls-spspps

If not, I'll have a device in a couple of days to test with locally. Thanks!
 The change has been merged into dev-hls. Please re-open this if you still see the issue.
  Probably my fault. Thanks for the report; I'll take a look.
 This should be fixed by eb1210d410988492865ef1f025c449f256e3eabc, but let us know if it still occurs.
  @spark85, look at https://github.com/google/ExoPlayer/blob/dev-hls/demo/src/main/java/com/google/android/exoplayer/demo/Samples.java#L137, just add another sample there with a URL to your stream.
 @spark85, I'm sorry, I'm not sure I completely understand what you mean.

DemoUtil.TYPE_HLS_MEDIA is a content type parameter that says that m3u8 file is a single media playlist (not a master playlist with a bunch of media/variants for different bitrates), it gets passed to HlsRendererBuilder, and then used https://github.com/google/ExoPlayer/blob/dev-hls/demo/src/main/java/com/google/android/exoplayer/demo/full/player/HlsRendererBuilder.java#L71 This has nothing to do with HTTPS.

HTTPS works with ExoPlayer, all our HLS samples are HTTPS.

Do you have an m3u8 file with a list of aac files in there? Is your audio packaged as aac or ts files? aac files will not work, we currently don't support those playlists. But if your audio is packaged in ts format that just doesn't have a video with it, this will work as an audio streaming solution.
 You can encode/package your audio in TS format, that just has only AAC audio without any video, this will work with current ExoPlayer implementation. HLS playlists with AAC files are not supported right now, i opened an issue to track this #152 
 Closing because there's a more specific issue tracking the problem described.
  @Kening, we haven't started on it yet, but we'll add it at some point soon.
  Good spot. This actually also got fixed in https://github.com/google/ExoPlayer/commit/fd519016209391698d4a5a828f6f7deac73e88ff, where we've removed the readDescriptors method entirely. If you could verify that the fix has worked for your case, that would be great.
  Hm. This is unfortunately likely to be a device specific issue. In FrameworkSampleSource, there's a line that reads the format frmo the extractor:

android.media.MediaFormat format = extractor.getTrackFormat(i);

Out of interest, does it help if you immediately override the extracted mime with the correct one. In other words add the following line:

format.setString(android.media.MediaFormat.KEY_MIME, XXX);

In the longer term, we're planning on re-implementing media extraction for common formats (MP4, MP3) in ExoPlayer, rather than relying on the framework's extractor, which will allow us to fix this kind of thing in the library itself.

Where XXX is whatever the correct mime type is.
 We've now provided an application level extractor for MP3 (on the dev branch), which should work correctly for your use case. To try it out you can add a sample with TYPE_MP3 to the demo app.
  Done. Thanks!
  By design you should create new ones each time, since calling stop() on the player will call release() on the renderers, which will generally prevent them from being reused. The design makes sense if creating renderers is cheap, in which case the ability to reuse renderers isn't particularly important. Have you observed that this is not the case, and/or do you have other reasons for wanting to reuse them?
 I guess you'll need to decouple fetching the manifest and building the renderers then :).
  Please provide a test stream. Thanks!
  There's not much we can do here, because ExoPlayer doesn't control the data fetching when Android's MediaExtractor is used. We'll be trying to support more formats directly in ExoPlayer (likely starting with MP4), which will allow us to fix this because the fetching of data will become our responsibility, but we don't need individual issues open to track this. Closing for now.
  - Most likely the hardware doesn't support them. The first 5-10 seconds of the streams may be unencrypted (this is common as a way of allowing playback to start in parallel with the keys being requested), which might explain the behavior you're seeing.
- Devices that drop frames on resolution switches don't have proper adaptive decoders. You can query this property with MediaCodecUtil. On these devices there will necessarily be a small pause in the video when switching resolution. On such devices you could opt to adapt between different bitrate streams at the same resolution, or just accept the imperfection.
  Assuming you mean ExoPlayer.Listener.onPlayerError, the exception passed to the method should be all you need. Note that you can call getCause() (possibly recursively) to traverse up the nested causes to the original cause of the error.
  If it's released then you've called release on it, so you should have a pretty good way of detecting this already. Typically you'd want to be nulling out all of your references (hopefully you only have one) to the player instance immediately after calling release anyway.

So I don't think there needs to be a state for this.
  There's not enough information here to do anything useful. In the first instance you'd need to state exactly what DRM they are using, how licenses need to be requested, and so on. And ideally provide a test stream.
 Sorry, I just don't understand the question. I don't know what a decrypted URL is. For DRM schemes like PlayReady and Widevine, there is only one URL.
  - Please re-open if you can provide a test stream so that we can reproduce the problem.
- Please also test against the latest dev-hls. We've pushed some changes that might have fixed this.
  Aside from the server not serving the content at high enough bandwidth (at least where I am), this video plays fine for me. Could you provide a screenshot showing the issue? What device / version of Android are you testing on?
 ExoPlaer doesn't support adaptive bitrate yet #70. Currently it just picks the first media playlist from the master and plays it. In your case it's BANDWIDTH=200000 chunklist_b200000.m3u8 which is blurry itself.
  - For video I would argue that failure is desired behavior. You can show an error to the user with a retry button.
- You could listen to network connectivity changes and have the app automatically retry when a connection is re-established.
- You could change HlsSampleSource to make more retry attempts before giving up. We'll likely allow a minRetryCount to be passed through the constructor so that editing the class directly isn't necessary.
 The final bullet point was done in https://github.com/google/ExoPlayer/commit/8e2801ce9b4ab0091ec1681832f9de4c1acdbdc8.

The first two bullet points are a combination of desired behavior and application-specific changes, so marking this closed.
  Adding support for side-loading DRM init data from the manifest wouldn't be a big change. We already do this for SmoothStreaming.

For the mpd parsing part, at first glance it looks to me like the way this data is included inside of a ContentProtection element is scheme specific (i.e. not part of the core DASH standard). Is that correct? If so, what specific schemes are you interested in, and where is it documented how the init data is included?
 Well, at some point someone somewhere is going to need to parse the ContentProtection element to get the appropriate part of it for the key request. The appropriate part is exactly what the PSSH data would be, but not exactly what seems to be in the ContentProtection elements.

So I think it would be necessary to parse the ContentProtection elements properly, according to the relevant specs, either in the parser or in an extension to it.
 Given that parsing a ContentProtection element is the job of an XML parser, and the MPD parser is an XML parser, it makes sense to do it there as opposed to having to dome some additional XML parsing somewhere else.
 Right. So the mpd parser should parse out the binary data, which should then be fed to the MediaDrm (I think the MediaDrm object will expect to receive the base64-encoded PlayReady object in this particular case).

I think it's reasonable to add support for parsing the binary object from PlayReady ContentProtection elements directly in ExoPlayer's mpd parser. We can probably do the same for Marlin too, although it remains unclear to me what devices will actually have a Marlin DRM plugin. Please advise.

If I make the change to support PlayReady initially, do you have a way of testing it?
 My proposal would be to have the mpd parser build a UUID->byte[] map as part of the parsed mpd, rather than having any kind of scheme specific callbacks.
 It's unclear whether we're suggesting exactly the same thing. To me, your suggestion is for the mpd parser to capture the whole element, which would then need to be parsed elsewhere:

``` xml
<ContentProtection schemeIdUri="urn:uuid:9a04f079-9840-4286-ab92-e65be0885f95">
    <mspr:pro><!-- base64-encoded PlayReady object --></mspr:pro>
</ContentProtection>
```

Where-as my suggestion is to have the parser be aware of the mspr:pro element and parse out:

``` xml
[9a04f079-9840-4286-ab92-e65be0885f95] -> [base64-encoded PlayReady object]
```

The latter requires the mpd parser to have knowledge of the PlayReady scheme, but I  think that's fine. 
 As far as I know the PlayReady DRM modules for AndroidTV don't handle xml. My understanding is that they expect the PlayReady object in binary form. They may also handle a whole PSSH box. So that's something to bear in mind.

In general, I think the mpd parser should munge the data into whatever form it would be in were it delivered inside of a pssh box. Do you know what this would look like for the Marlin case? Or is the data always delivered in the manifest for this scheme?

Thanks for the information by the way!
 This is fixed by the two changes above, except that the manifest parser needs to be updated to parse the UUID and binary data for each scheme that we intend to support.

Additionally, it is straightforward for applications to extend the manifest parser to support any additional scheme that they require, as noted in the description of the second change.

I'm closing this issue out for now, since all of the necessary plumbing is now in place. I'm happy to accept pull requests adding the small pieces of parsing for each scheme (I can't add these myself, since I don't have test videos + license servers to test with). Alternatively, applications can do as suggested above.
  ExoPlayer fails to play this _big buck bunny_ stream from akamai
http://googleimadev-vh.akamaihd.net/i/big_buck_bunny/bbb-,480p,720p,1080p,.mov.csmil/master.m3u8

Current implementation assumes that one ADTS frame has exactly one AAC frame inside. In this specific stream ADTS frames have sometimes 0 and sometimes more then 1 AAC frame inside.

How typical is this stream? Has anybody else seen similar issues?
 Got fixed with 80602b16846da1c7a9e9df1b5eb051adb8498dfc
  I'll try and get hold of a GoPro to reproduce this next week. In the meantime, can you confirm whether it still affects latest dev-hls? Quite a lot of stuff has changed since the issue was filed.
 I wasn't able to reproduce the same problem as mentioned at the top of this page, but with the latest dev-hls I'm able to (more or less) successfully stream HLS from a GoPro Hero3. The one caveat is that the player will get stuck in a buffering state after some period of time. This is due to a known issue where the GoPro incorrectly claims each segment is 1 second in duration, when actually it's usually less. We don't plan to implement a workaround in ExoPlayer, particularly since new GoPro devices don't support HLS anyway. If you want to fix it locally in your own application, all you need to do is edit this piece of code:

https://github.com/google/ExoPlayer/blob/dev-hls/library/src/main/java/com/google/android/exoplayer/hls/HlsChunkSource.java#L221

to have the condition instead be just:

if (previousTsChunk != null && previousTsChunk.isLastChunk) {
  Could you provide a URL that reproduces the issue? The one in the second post of this issue doesn't appear to work.
 Does this work now if you pull the latest changes from the dev-hls? I wasn't able to reproduce, but there have been some significant changes made recently.
 @crossle I don't really understand the question, but the player will necessarily skip data until it finds the first keyframe. Most likely your chunks don't have keyframes located at the start.

@HandsomeFrank If you have 70 chunks in the queue then why would you want to switch stream? If you buffered that many at low quality, you should probably have switched to a higher quality earlier?
 @HandsomeFrank You don't want to just make the buffer smaller, since that would make re-buffering more likely in the non-switching case.

On the assumption that you're modifying or have replaced HlsChunkSource, you can set queueSize on the HlsChunkOperationHolder to have chunks be discarded from the tail of the queue (although you shouldn't discard right down to 0, don't go lower than 1).
 The result of getChunkOperation is that the holder should contain the number of chunks in the queue that should be retained, together with the next chunk that should be added. In the default implementation the queue size is never trimmed, but you're allowed to do this if you want. So if the queue is contains chunks spanning times:

[0-10][10-20][20-30][30-40][40-50][50-60]

You could set the size to 3 to discard the last three chunks. The next chunk being added would be expected to span [30-40].
 A lot of stuff has changed here (unfortunately we removed the ability to do what I described above, sorry about that). Closing the bug in any case, since it's no longer relevant to the implementation.
  Do you mean add MarlinDRM sample streams and a MediaDrmCallback that hits a corresponding test server?

That sounds good in theory, although it depends a bit on what the uptake of Marlin DRM is expected to be, since the purpose of the demo app is primarily a starting point for developers wishing to use ExoPlayer, rather than an app that implements a super-set of all possible things that someone might want to do.
 Closing this out because I don't know of any devices that are supporting MarlinDRM at this moment in time. It's probably worth re-opening once you can provide some idea of this, and also:
- Public, stable test streams that don't require any attribution to add to the demo app.
- Public, stable license server to issue licenses for the test content. Equivalent of WidevineTestMediaDrmCallback that works for the test content.

At that stage we can generalize things to support both, if it looks worthwhile.
  Good catch! We need to double check that this isn't going to break anyone, but will do this and hopefully get the fix merged on Monday. Thanks!
  @HandsomeFrank could you share a URL the Wowza stream that has this issue?
 The string is just an identifier for the content - you can set it to anything you want.
 @perchrh Yes, we're aware of this inconvenience and plan to fix it, but it's not a high priority issue. Please go ahead and open an issue for this, we'll get to it once major HLS features are in.
  Closing because this is discussed in #105. The manifest should indicate where the moov is located.
  I think the code is right and your manifest is wrong. Both start and end values in an indexRange are inclusive according to the spec, so it's correct that we calculate the size as 2913 in that case.
 p.s. I also wouldn't expect parsing a moof to succeed if the moov hasn't been parsed. The moov should always be parsed first.
 The atom sequence looks fine, and ExoPlayer should have no problem handling that. There is no requirement in ExoPlayer regarding the ordering of the moov and sidx in the files. If they're contiguous then they'll be requested together in a single request. If they're not then the moov is requested first and the sidx is requested second, using two separate requests.

I think the root problem is that your manifests are likely to be malformed (there being multiple of them doesn't make them more correct :)).
 No worries!

For your second issue, the mpd is supposed to indicate where the initialization data is located, which includes the moov atom. It's likely that your mpd is missing this information. There are a few different ways of doing it, but for your use case there should probably be an Initialization element inside of the SegmentBase element, like:

``` xml
<SegmentBase indexRange="685-2852">
    <Initialization range="0-684"/>
</SegmentBase>
```

There is no requirement for the two ranges to be contiguous, but if they are (as is the case above, and probably for your files too) then ExoPlayer will request both the initialization data and the sidx in a single request.

Example mpd:
http://dash.edgesuite.net/digitalprimates/fraunhofer/480p_video/heaac_2_0_with_video/Sintel/sintel_480p_heaac2_0.mpd
 Additional information - According to the DASH spec, an initialization segment should include an ftyp box, a moov box, may contain other boxes, but should not contain any moof boxes. So in your case the range should start at the first byte of the ftyp box and extend to the last byte of the moov box.
 Your media segments aren't self-initializing though (and I think we do support the self-initializing segment case).
  You'd be best off asking somewhere where there are more Gradle experts (maybe a Gradle forum, or StackOverflow). I'll leave this open for a few days in any case, in case someone happens to know the answer.
  If CyanogenMod is broken, you need to ask CyanogenMod people to fix it.
  You can probably fix this by adding "android:largeHeap=true" to the application element of the demo app's manifest (or whatever app you're testing with).
 I don't understand the question. If you need more heap space then you need android:largeHeap=true.
 It's not a viable solution, because you have no idea how much heap space the rest of the application is going to be using. It's fine for a media app to specify android:largeHeap=true if it's going to be using its own heap for buffering video, which is true of applications that use ExoPlayer for DASH, SmoothStreaming or HLS playbacks.

Note that we're not actually using more memory in total; it's just that the memory is counted as being part of the application heap, where-as when using MediaPlayer or MediaExtractor this is not the case.
 That's a deliberate change to avoid a spurious state transition... What's actually broken?
 The demo app uses largeHeap now.
 - Is this in the demo app, or your own application? Something is leaking huge amounts of memory, but if it's in your own application then it's equally likely to be your code as it is anything ExoPlayer is doing. Please advise either way. Thanks!
- Do you have a stream you can link to that reproduces this in the demo app?
 Closing unless more information is provided. The implementation has also changed significantly, so it's possible the issue is resolved.
 Seems unlikely that the error is exactly the same. The code has changed fairly significantly since then (so the line numbers in the stack will certainly be different). Please file a new issue.
 Put it in a new issue please. Whether you need largeHeap depends a bit on how much memory the rest of your application is using.
  I think this is fixed now. Please re-open if you see it again. Thanks!
  - This is already supported. ExoPlayer just requires a Surface, it doesn't matter whether you get it from a SurfaceView or a TextureView.
- One caveat is #58 (you may experience strange rendering _if_ you're doing cross-resolution adaptive playbacks; fixed resolution playbacks should be unaffected).
 Surface has a constructor that takes a SurfaceTexture. I believe you can create a Surface this way, and then pass the Surface to ExoPlayer:

http://developer.android.com/reference/android/view/Surface.html#Surface(android.graphics.SurfaceTexture)
 When using TextureView you pass the Surface in the same way as for SurfaceView (see the [demo app code](https://github.com/google/ExoPlayer/blob/dev/demo/src/main/java/com/google/android/exoplayer/demo/player/DemoPlayer.java#L588)), but you need to [construct the Surface from a SurfaceTexture](http://developer.android.com/reference/android/view/Surface.html#Surface%28android.graphics.SurfaceTexture%29), which you get in the TextureView's surface texture listener (see [this example code](http://developer.android.com/reference/android/view/TextureView.html)). It is probably also a good idea to release the Surface when the surface texture is destroyed.
  There's not enough information here for us to do anything useful, but it's probably the same as #101 .
  - At a fundamental level, why would it be a good idea for the server to decide how much data to return, as opposed to allowing the client to choose and giving it what it asks for? That doesn't seem like a great idea to me, in terms of architecture.
- To answer your specific question, if you're using FrameworkSampleSource then ExoPlayer has no control over the networking, and we can't fix it (at least not on devices running the as-yet-unreleased M version of Android). If you're doing DASH/SmoothStreaming/HLS then potentially we could fix it.
 I'm not sure we intend to support this, unless there's a compelling use case. Closing out for now.
  Hm. This is pretty easy to fix on Lollypop. It actually already rotates the video on Lollypop, the only thing that's still wrong is that the video dimensions aren't also swapped (for 90 and 270 degree).

Pre-Lollypop any fix is going to be more difficult, because MediaExtractor doesn't parse out the video rotation.
 Pulling the extractor up to ExoPlayer wont fix the issue, because the issue is in the decoder (not being pulled up) not the extractor (being pulled up).
 The behavior has been fixed as far as is possible in the change above.

Background: The underlying platform changed in Lollypop to start automatically applying rotation when decoding video into a surface. Prior to Lollypop the rotation was not applied. Since it's not possible to perform the rotation at the application level when using SurfaceView, the Lollypop+ behavior is the correct long term solution.

Solution: After the change above, on Lollypop+ the rotation will always be applied correctly. On earlier versions of Android the rotation will not be applied, however the player will now report the unapplied rotation through the MediaCodecVideoTrackRenderer listener:

https://github.com/google/ExoPlayer/blob/d3995eaa7ad2e8406c1c86f3035f31d762352baf/library/src/main/java/com/google/android/exoplayer/MediaCodecVideoTrackRenderer.java#L74

This allows an application using TextureView to easily apply the rotation by making the appropriate call to  TextureView.setTransform. Note that on Lollypop+ unappliedRotationDegrees will always be equal to 0.
 You can get the latest source from the [dev](https://github.com/google/ExoPlayer/tree/dev) branch and build it yourself (or include the source directly in your project). We don't provide jar/aar binaries prior to release.

Note that the APIs (in general - nothing specific to rotation) are a little unstable on the dev brach due to the arrival of multi-track support, so if you go down this route please be aware that they may change again before the next release.
 Urgh, this is a bit of a mess at the platform level. Application of rotation is inconsistent between both API levels (pre-L v.s. L+) and (in L at least) between decoders at a fixed API level. It appears that on M+ OMX.google.h264.decoder does actually apply the rotation, but you're correct in noting that this does not happen when running L.

Unfortunately, as of now, there's no easy way to determine whether a decoder has actually applied the rotation. I'm not sure how consistent the application of rotation is between decoders in M. I'll be pushing for more consistent application of rotation (if this hasn't happened already) and/or the ability to determine whether the rotation was applied in the next platform release.

For ExoPlayer, always propagating the rotation value doesn't seem like the right thing to do. That just moves the problem of working out what to do from the library to every individual application developer, who will then each have to figure out what different decoders do on different API levels. Instead, I think we probably want to work out what the current behaviors are and hardcode these inside the library, so that the rotation delivered to the application always ends up being correct (i.e. 0 when using a decoder that applied the rotation, non-zero otherwise). Hopefully we can also get consistent behavior across all video decoders from some version of the platform and above, since this makes the developer's life significantly easier.

Re-opening for tracking. First step is to try and figure out exactly what behavior we should be expecting across the various API levels / decoder implementations.
 For most cases it is fixed already (the rotation will be applied, or the rotation that needs to be applied will be propagated as unappliedRotationDegrees). So it's unclear exactly what you mean by this. Unless you've modified the player to use a software decoder then I think behavior should be correct already.
 It's not possible for us to rotate them automatically due to lack of platform support. Instead we report unappliedRotationDegrees=90. If the application is using TextureView then the app developer can apply a transformation to achieve the desired rotation. With SurfaceView rotation is not possible. See my comment on Aug 13th on this thread.
 What kind of Surface are you using (one from SurfaceView / TextureView / SomethingElse)?
 Use TextureView.setTransform and apply the rotation yourself.
 - The 21st post already states that the software decoder applies rotation on M+.
- The ExoPlayer fixes related to this change (and linked to from this issue) didn't land until 1.5.0. For example https://github.com/google/ExoPlayer/commit/d3995eaa7ad2e8406c1c86f3035f31d762352baf.
  @juanpef FrameworkSampleSource doesn't support live streaming. For HLS see the example at https://github.com/google/ExoPlayer/blob/dev-hls/demo/src/main/java/com/google/android/exoplayer/demo/full/player/HlsRendererBuilder.java#L91
 You're probably using a container format that isn't supported? See the "video" section of "core media formats" here: http://developer.android.com/guide/appendix/media-formats.html
 @juanpef if you just want to play a single standalone MPEG2TS file, then this is not supported. What you can do to work around:
- Create a simple HLS playlist with just one file and use it as an HLS source.
- Create your own TsChunkSource similar to HlsChunkSource (but much simpler), to just play a single file.

IMHO, MPEG2TS is probably not the right container format to use for stand alone video files.
 @juanpef newSimpleMasterPlaylist() is for the case where you only have a single media playlist URL and want to create a dummy master with it to reuse the same processing logic.
in your case you have a master playlist URL, you need to download it using ManifestFetcher<HlsMasterPlaylist>, then parse using HlsMasterPlaylistParser and then feed the output to HlsChunkSource. look up the example link i gave to you earlier, it has all of it.
 @juanpef in your case with a single MPEG2TS file you can create a single media playlist and use the code you wrote above. without any playlist you'd also need to implement a ChunkSource for it, which shouldn't be hard.
 - We're working on a new extractor model that should make this easier to support (ETA 1-2 weeks).
- Is anyone able to provide a working test stream to help us validate support?
- Your evaluation of what should and should not be an out of the box feature is dubious. On what basis did you reach your conclusion, other than it's a feature you personally want?
 Ideally just a test URL, that works, without us having to spend any time setting one up ourselves.
 One other question - Am I correct in thinking that you shouldn't expect seeking to work for this use case (since there's no index, either in the form of an external manifest or in the media stream itself)?
 We pushed a bunch of changes that should make this quite straightforward to do. But we're not going to prioritize the work without someone providing a test stream. If no-one is able to do so, then that really indicates that this feature isn't in significant demand.
 Just to check: This really is over HTTP and not RTSP right? If so then yes, if you set something up we'll take a look (no promises, since you never know what you'll find when you start trying to build something!).
 We have a UDP data source already as it happens. We'll push it to GitHub in the next week or so. I'm not sure whether it makes this easier or not. I suspect you really want RTSP, which we're not supporting.
 A UdpDataSource is now in dev.
 - We need more information to be able to debug this. Like the stack trace.
- Why does your code reference MP4 anywhere? I don't think what you're streaming is MP4. What does that ffmpeg command actually cause to be streamed (my guess is MPEG-TS, but I'm unsure)?
 Closing due to staleness. We do not have a demo for mpeg2ts over UDP. Our handling of UDP is very much a "best effort", as opposed to being a primary use case that we support.
  I'm confused about what the actual bug is here. What happens if you don't drop them? I don't think you can just drop silence from the start, since there can be legitimate silence at the start that you don't want to drop.
  How far apart are the key frames in the media? You can only start decoding from the nearest key frame. ExoPlayer will seek to the key frame before the seek position, and then decode (but not render) frames up to the seek position.

If there aren't any key frames between the start and 6.3 second point in the video you're using, then the behavior you see is correct (and fixing it would require inserting more keyframes in the source media).
 Closing on the assumption that the behavior is as described above. The alternative would be to implement seek that's always fast, but isn't accurate (e.g. you'd seek to the 6.3 second point, and we'd just go and seek to the nearest keyframe at 0 instead). We opted not to do that.
  Just as a heads up, we're likely to implement this for both DASH and SmoothStreaming before we do so for HLS, simply because DASH and SmoothStreaming are much better standards, and are more forward looking. I continue to advise looking at DASH, where a choice is possible.
 No promises, but we're aiming to add DVR window support for DASH by the end of this month. As we do that, we'll be looking at how the same approach can be used for HLS too, specifically for case (2) in the original post. Hopefully this wont be too hard, since they're roughly analogous to one another. I don't think we have near-term plans to support case (1) in the original post.
 Live seeking in HLS is being worked on for Exoplayer v2.
 I cannot promise anything but I'd say 2 weeks.
 You will be able to seek inside what the media playlists expose.
 Guys, just to let you know: I am still working on this. Many thanks for your patience. I am not giving any estimations this time. Priorities shift and things come up, unfortunately, but this is still coming.
 Not yet, no. But on the map.
 @AquilesCanta 's change (ref'd above) adds preliminary live seeking support for HLS. Feel free to give it a try. Note that there are some known issues that we'll be cleaning up over the next week or two (e.g. temporary blacklisting of variants when a 404 is encountered is currently broken, and will need fixing before we do a stable release to).
 Are you using the latest `dev-v2` code? The changes haven't made it into a release yet. We're aiming to do a release this week. Thanks. No, this issue only tracks live seeking, which is available. #2643 tracks offline support. If it is an event-like stream, where segments are only appended (and not removed), then we support seeking, pausing, etc. If segments are also eventually removed, then we only support seeking to these segments for as long as they are available in the playlist. I do not see other alternative to downloading the segments if this is the case. No. But you can easily script your own using a VOD HLS stream, like Apple's sample stream. If you need guidance for this, I'd suggest trying stack overflow. It's very unclear what feature you're actually asking about. I suspect the answer is that yes, what you want is just supported already and you can seek around in the available media. If you want to see how well ExoPlayer supports your HLS DVR streams, why don't you just add one to the demo app and give it a try?  A VerifyError is typically a problem with your build, I believe, rather than with the library itself.
 Yes, that would be a problem (I'm surprised it would show itself as a VerifyError, but I guess that's not important). There's no way to support API level 14, since the framework APIs on which it depends didn't exist until API level 16.
 It's not a mistake. See: https://github.com/google/ExoPlayer/blob/master/library/src/main/AndroidManifest.xml#L26
I guess we should add that comment to the gradle file too.
  As far as I know, the log message you refer too isn't actually an error (I'm not sure why OMXMaster is using the error log level to print that). I think the steps you describe are correct.

When you say it "sometimes" starts again, are you saying that sometimes it doesn't? If so, could you provide more details about what goes wrong? I don't know why it would take longer to load the second stream than the first one.
 We are working on built-in support for playlists, tracked by #1270. The first comment in this issue will probably be addressed by that enhancement.
  The framework's MediaExtractor doesn't have functionality to allow you to plumb in your own data source yet, which is why things don't work the way you suggest. In the long term we plan to add more extractors to ExoPlayer directly, so that a wider range of formats can be played without needing to use FrameworkSampleSource.
  There are two problems here:
- The error doesn't get propagated properly.
- The player adds a spurious "/" into the segment URLs. This causes requests to fail with a 403 error.

Will push a fix.
 There's actually a third (far more minor) problem, which is the spurious P->I->P state transition. I'll fix that today.
  @perchrh what is the device and Android version you're using? it's definitely device specific issue, since both streams work on my device.
  You need to implement downloading yourself. ExoPlayer does support playback of local files, once they're downloaded.
  Will take a look. Thanks!
 Fixed in 60d162df18076b4db3034b8ff944da86d372a2ef.
  Duplicate of #78 
 Fixed in 60d162d.
  Depending on exactly how you have ExoPlayer setup you'll be using one of two network stacks, both of which are standard to Android. ExoPlayer doesn't do anything specific for the HTTPS case. It's unclear whether your problem is related to HTTPS from S3 or something else, but it's unlikely to be related directly to ExoPlayer.

FWIW - YouTube use HTTPS with ExoPlayer and haven't seen this issue.
   Roughly speaking, yes. Although the exact point at which the division happens is TBD. For DASH and SmoothStreaming, we divide at the ChunkSource layer with a [MultiTrackChunkSource](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/chunk/MultiTrackChunkSource.java).

Caveats that may not have occurred are:
- The fact that there's audio in the primary audio/video stream, as well as in the separate audio tracks, makes wiring things up significantly more complicated than if the only options were the separate audio tracks. I haven't really thought much about what the best way to do this is.
- When you're downloading audio from a separate source, it's necessary to balance loading of the video and audio to make sure they load at approximately the same rate. You'd probably want to have them share the same underlying buffer pool as well. For DASH and SmoothStreaming, we use a [LoadControl](https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/LoadControl.java) to do this.

You're really much better off using DASH or SmoothStreaming if you want multi-audio (you're much better off even if you don't want it, but it becomes even more true if you do). The HLS approach seems pretty bolted on to me, and if you select the non-primary audio track you end up downloading two audio tracks simultaneously, one of which you throw away. That's not exactly a good technical solution.
 Yes, definitely something to bear in mind. We already have much of what we need to ensure this from our work on DASH. Specifically, LoadControl can be used to ensure that separate tracks load at the same rate (so that we don't end up buffering 10 minutes of audio and 10 seconds of video).

If the media timestamps in the separate streams are aligned properly then synchronization should happen automatically (I'd really hope that this is true). If not, we'll need to do a bit more work to get this right.
 Significant further work is required beyond the change linked to above for HLS. #514 has an update of where we are with multi-track across the various different playback types.
 No update, sorry. It's interesting that you have SmoothStreaming+AES streams. Perhaps we should support that case; SmoothStreaming is a much nicer standard than HLS, so it would be a shame to be pushing people to HLS because we support AES there and not for SmoothStreaming. Could you open a separate issue for that, and if possible link to the specification that defines how AES works in SmoothStreaming? Thanks!
 I have this more or less working locally, but I'm struggling to find test streams to verify that the implementation is correct. Please could anyone interested in this feature provide their streams for testing, either here or to `dev.exoplayer@gmail.com`.

I'm particularly interested in live streams with multiple audio tracks, since I haven't found any examples of this. VOD streams are helpful too. I know Apple provide a test stream with alternate audio, but their alternate track is a constant sound, which means you can't use it to verify that A/V sync is correct! 
 Got it, thanks.
 Please test the solution merged into dev. Thanks.
 It'll be merged into master in the next release, at which point it'll become available. We don't provide dev branch binaries, although it's pretty easy to check out the source code from the repository and build one yourself.
  Not a priority for us at the moment. But we might focus on it later.
 @tresvecesseis - Did you ever get this fully working? It should be much easier to finish now, since the extractor re-factoring that we've done more recently. We also have an Mp3Extractor, which is probably useful to copy code from.
 Ah, sorry, that kinda passed me by. We'd like to pull something here, I think! I tried patching it in, and it seems like it produces a native crash on my devices for the test stream in #501, which contains a TS_STREAM_TYPE_MPA_LSF stream. Any idea about that?
 Ok. Do you have some standard HLS+MP3 example streams to test with?
 Ok. I think we need some samples (that we can use for ongoing testing) before we can sanely merge this. I'll have a hunt around for some. If you happen to find any, let me know.
 This is now supported on the dev branch.
  METHOD=AES-128
 @perchrh Thank you, this was helpful. We'll push our own changes very soon.
BTW, In your stream I noticed that only the first chunk (TS file) has PMT in it, and all the other chunks have no PMT after PAT. This seems broken to me. Where did you get it?
  Implement support for Closed Captioning via:
EIA-608 (CEA-608)
CEA-708
WEBVTT
 @Ood-Tsen i don't have any test links right now. are you asking because you need one, or you would like to help with providing one (any test resources you can share would be much appreciated)?
 @Ood-Tsen just in case you're still looking for test links. apple's default examples have CEA-608 and WebVTT.
https://devimages.apple.com.edgekey.net/streaming/examples/bipbop_4x3/bipbop_4x3_variant.m3u8
https://devimages.apple.com.edgekey.net/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8
   We'll be adding support directly (very soon). There will be a point where external contributions will become really helpful, but I don't think we're quite there yet.
 @TheHal85 thanks for your feedback! We did it this way cause we mainly interested in TXXX frames, and TXXX frame by itself contains two strings (Description and Value). But it sounds like you're right the frame type should be the key, I'll investigate it more and update the code.
 @TheHal85 please take a look at the updates I've made based on your suggestion https://github.com/google/ExoPlayer/commit/71f918c01b3318f42cc9b2d7f83b4976e7c20bbb
 Just an FYI, @TheHal85 your code snippet is a little buggy, probably a copy&paste issue. For example, if there is a TXXX frame in the map and two others, it will just log the same TXXX 3 times. Use the map entry if you're iterating through it and don't call get() or containsKey() on it. Also cast the element to the right type in the for loop `for (Map.Entry<String, Object> entry : metadata.entrySet()) {...}`.

We're working on adding Closed Captions support, so hopefully you won't need to do those tricks soon.
 @perchrh ID3 and Closed Captions are two different separate things. What @TheHal85 is doing is a hacky work around while we don't have Closed Captions support.
Chunks in your stream have ID3 track registered in PMT, but no packets with ID3 data ever appear, thats why you see that MetadataTrackRenderer never gets SAMPLE_READ.
But you stream does have Closed Captions, which are stored as Closed Captions, not as ID3. Thats why Quicktime displays them. ExoPlayer doesn't support Closed Captions yet, thats why you don't see them displayed in ExoPlayer. There is another open issue for it #68
  It looks like your DataSource fed bad data into the extractor, causing it to fail. If you want to know what the problem is you'll probably have to compare the raw data read from the default data source and your custom one, reproduce them returning something different, and then figure out what's causing that.
  I haven't explicitly tested this, but I believe this is now fixed.
  Yes, we've seen this too; it appears to be a problem with certain devices. We're following up with OEMs to try and get this addressed.
 @trandroid It would be great if you could figure out what the difference is between your use of TextureView and ExoPlayer's? I don't think ExoPlayer does anything wrong, but it would be good to narrow down what we're doing that triggers the underlying issue. Are you using DASH/SmoothStreaming, or are you using FrameworkSampleSource?

I don't know of an easy way to detect when this is happening, unfortunately.
 Thanks for the information. KEY_MAX_WIDTH and KEY_MAX_HEIGHT are also prime candidates to try removing, if you're setting them (most likely by calling maybeSetMaxDimensionsV16). Just to see what happens.
 I'm a bit confused by the patch. Doesn't maybeSetIntegerV16 perform the exact same checks that you're adding? Am I being stupid :)?
 If you're dealing with fixed resolution content then you don't need to set them at all, and the problem is solved for your use case. If you're doing ABR that can switch resolution during playback, then you should specify the maximum video (not surface) dimension that you'll be using during playback.
 The root cause was an issue with some Qualcomm chipsets (e.g. APQ8064, MSM8960, MSM8930). The issue was subsequently fixed, but devices will obviously need to be updated if they're to pick up the fix. Closing because there's nothing that can be done to work around the problem (aside from not setting the max dimensions or not using TextureView).

https://www.codeaurora.org/cgit/quic/la/platform/hardware/qcom/display/commit/libgralloc/mapper.cpp?id=43da51a308c8176723b61a2c1ffcfff485ab53a2
https://www.codeaurora.org/cgit/quic/la/platform/hardware/qcom/display/commit/libgralloc/gralloc_priv.h?id=3d01d8d75e80f5bee0d9c37cc7b8fc1a6d9b7884
 I don't have a more exhaustive list, I'm afraid. It might be that those three are the only ones affected; I'm not sure. If anyone else knows of any additional affected boards, I'd encourage them to post details here!
  ExoCache is currently fairly limited. If you happen to be making only bounded range requests to the network (as is the case for some DASH implementations) then it's awesome. Otherwise it's pretty much not useful in its current form.
 It's not possible to do this (yet). I've filed a tracking issue here: #322.
 Yes, I am sure.
  - You should IMO consider DataSource as the common interface that you can plug an arbitrary network stack into. I see little value in defining a more specific HttpInterface under HttpDataSource. You'd likely end up writing nearly as much code adapting an arbitrary network stack onto this interface as you would mapping it onto DataSource directly.
- I want ManifestFetcher to use DataSource in order to fetch manifests, which would allow your single whatever-network-stack-you-want-DataSource implementation to be used in both places. Right now the DataSource interface doesn't allow querying of content type, which appears to be needed when retrieving manifests. I need to look into that a little bit and see if there are any other implications, but it's probably possible to get that resolved and move over to this way of doing things.
 So to clarify, I propose using this bug to track having ManifestFetcher use a DataSource.
  Right. We have no plans to commit resources to this in the near future. If someone wanted to add a high quality, relatively self contained component that adds support, then we'd happily take that as a pull request.
 A couple of things to keep in mind:
- We wouldn't want an RTSP library as a dependency on the core ExoPlayer library, so if anyone does work on this then please try to implement it as an extension, similar to the VP9 and Opus extensions currently in the dev branch [here](https://github.com/google/ExoPlayer/tree/dev/extensions). That way an application that needs it can proactively include the extension, and everyone else can ignore it :).
- It would be preferable to implement it in Java. If there is a native dependency, bear in mind that you'll need to support multiple architectures (arm, x86, mips + 64bit variants) if you want it to work across all devices.
 We would consider taking something if it were implemented as an extension that doesn't require invasive changes to the core library. Structuring it as an extension would also allow you to depend on external libraries (that may have non-AOSP licenses attached to them). Note that all contributed source code would need to satisfy the CLA (i.e. you need to own copyright/IP), so dependencies on external libraries need to be dependencies, rather than copy/pasting code from them.

It's somewhat unclear where the boundary would sit between any such extension and the rest of ExoPlayer. Is it possible to provide a DataSource without needing any other changes elsewhere? A self-contained DataSource extension sounds like the best thing to do initially, yes. @shinayser - It's not our goal to implement everything that anyone asks for, for free, just because they ask for it. As has been noted already on this issue, we're open to accepting a well written and self-contained pull request that provides this functionality, from @tresvecesseis or anyone else who wants to provide one. You're welcome to contribute this functionality yourself!

@kjeremy - Whilst we'd obviously like to support IP cameras, realistically it's just not one of the primary use cases we're targeting, which is why we haven't devoted resources to this. This makes sense if you think about the hours-per-day a typical smartphone user spends consuming OTT video vs watching IP camera streams.

So in summary: Once we get a high quality pull request, we'll look at accepting it! Thanks.  I'm not sure I understand the question. The DASH spec is ISO/IEC 23009-1:2012. We support most common usages. The significant things we don't support are:
- Multiple period manifests.
- MPEG-TS based DASH (we support only fragmented MP4 and WebM).
 Looking at that one should be fine.
  Is "video/x-vnd.on2.vp8" the mimeType that you're passing to the method to cause the crash? What's the name of the decoder (i.e. the value of codecName)?
 Although we're seeing a crash when querying OMX.SEC.vp8, it's unclear whether or not it does actually support video/x-vnd.on2.vp8. Did you attempt to play the content using this decoder, and if so did it fail? Just trying to confirm the correct fix (which is either to ignore the decoder, or to hardcode that it does actually support the mime type).
 When does OMX.SEC.vp8.dec throw the IllegalArgumentException? I understand it throws it when querying the capabilities (which is what this change guards against), but I was wondering if the decoder works if you just don't query what the capabilities are and try and instantiate the decoder anyway. I'm guessing the decoder is actually able to play VP8 content?
 Agreed. Blacklisting that decoder/device combination seems like the right thing to do. There's another decoder on that device that also fails. Are you aware of any other devices on which this decoder fails consistently?
 @edisonw, we are adding a way to blacklist decoders to address issue #377. I'd be grateful if you could let me know the device fingerprint (output by 'adb shell getprop ro.build.fingerprint') on the phone where you saw this. I will need to compile a list of devices with the issue, so if you now know of any others please tell me about those too. Thanks!
 Closing this. We've added functionality to blacklist specific decoder/device combinations. We can add additional combinations to the blacklist as needed/reported.
  Thanks; test streams are helpful! We're looking at this currently.
 That URL works fine on the dev branch for me. Please try the dev branch, if you aren't already doing so. Did you add it to the ExoPlayer demo app? If so, how exactly?
  There's a TODO in FrameworkSampleSource to address this. It doesn't occur for DASH, SmoothStreaming or HLS playbacks.
 FrameworkSampleSource is deprecated (and we wont be fixing it further). Closing.
  That sounds like a bug in the underlying platform. Are you streaming audio using HLS, or something else, and do you have a test stream we could look at?
 It's likely to be a platform issue, in which case there's probably not an easy workaround for the problem.
 Closing since it's likely to be a device specific issue. Sorry.
  This isn't a public API. For YouTube, the recommended approach would be to upload content and use YouTube's Android Player API (https://developers.google.com/youtube/android/player/). I'm not sure if it uses ExoPlayer behind the scenes, but if not then it probably will do at some point.

I can't comment on other providers.
  ExoPlayer does this for (nearly) all internal Java crashes. Regrettably, there's no way to do the same for native crashes. There's also no obvious fix for this problem, short of a larger effort to stop relying on Android's built in MediaExtractor.
 There's nothing we can do to fix this in the direct sense. There will be a wider effort to stop relying on MediaExtractor for more formats, which will effectively mean this code path wont be hit any more and so will resolve the issue. This wont happen in the near term though.
  There's a small issue around trying to play from very close to the end when using FrameworkSampleSource. I'm pretty sure it doesn't have anything to do with the multiplication factor. If it did, I would expect the issue to reproduce with DASH streams, but I've never seen that. Have you?
  I would recommend having your project that depends on ExoPlayer build it from source, in which case I think the proguard configuration is controlled by the project. When building the library as a jar, I'm not really sure that proguard is hugely useful. Although suggestions are welcome if you feel otherwise ;).
 There aren't really any recommendations. We don't have proguard settings for the project internally.

I think we're good leaving it up to people using the library to decide how they want to proguard it, so closing for now. We'll revisit if we get more similar requests.
 Proguard is clever enough to handle `Class.forName(stringConstant)` automatically. What is the actual problem you're seeing?  The audio renderer is only the time source if it's enabled. There's a MediaClock object in ExoPlayerImplInternal that can also serve as the time source, and there's support for hand-off from one to another should the audio renderer be enabled/disabled:

https://github.com/google/ExoPlayer/blob/master/library/src/main/java/com/google/android/exoplayer/ExoPlayerImplInternal.java#L565

In your case you presumably don't have a MediaCodecAudioTrackRenderer at all. In which case you'll be using the MediaClock. The problem you have is likely just that MediaClock starts timing from t=0, where-as your samples start from some non-zero timestamp?

Would it be feasible for you to subtract the first timestamp from all of the same timestamp values in your SampleSource, so as to make them start at t=0? You may find that this is all that's required.
 I think the easiest thing to do is to adjust the timestamps in the SampleSource so that they're continuously increasing. The renderers then don't need to handle discontinuity in the timestamps.
 As of now, a discontinuity is just an indicator to the renderers that they should flush themselves. You probably don't need to send them.

In the future discontinuities may be enhanced to allow jumping the playback position (and perhaps adjusting the media duration). This is TBC though.
  Does the normal media player handle this type of content correctly?
 - For DASH I think you're correct in stating that FragmentedMp4Parser should parse out the PASP box (assuming that's the correct box; I didn't actually check :)).
- We shouldn't adjust the MediaFormat width and height values. These should be in pixels, and so are already correct.  My initial thought would be to add an additional pixelWidthHeightRatio field to MediaFormat (default value 1), and just have it be propagated all the way through to onVideoSizeChanged. I think width and height should still be reported as pixels, but the ratio could be passed as an additional argument.
 p.s. If you can provide some sample DASH content then I can take a look. Otherwise feel free to propose a change. Note for the latter, you'd need to sign our CLA: https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md
 Re FrameworkSampleSource - FrameworkSampleSource hooks into the underlying media support provided by the platform, so if there's no support there [it sounds like you've concluded that there isn't]. then it wont be possible to provide the additional information.
 Do you need to add the initialization segments to the zip? Or did I miss them :).
 Thanks. I have a fix for anything that uses the fragmented mp4 extractor (i.e. DASH / SmoothStreaming).
 Fixed in the dev branch in:
https://github.com/google/ExoPlayer/commit/ec90eac301ba212b3bd6c3c70e26c66a65138597
  Using a TextureView would probably let you do this pretty easily.

I'm not sure if it can be done in MediaCodecVideoTrackRenderer, since I can't see any underlying support in the MediaCodec or Surface to do it. If you know of a way to add this, please let me know.
  1. TextTrackRenderer is fully implemented. It's used by SmoothStreamingRendererBuilder in the demo app to render TTML subtitles, although the demo app doesn't contain a sample video that actually has TTML subtitles, because I couldn't find one that was freely available.
2. Production readiness is ultimately something you need to decide for yourself. We're using it in production at Google in our YouTube, Play Movies and Fiber apps for DASH playbacks, so we consider it production ready for this use case. We believe the SmoothStreaming use case to be more or less production ready too.
3. Play Movies is using ExoPlayer for movie and episode playbacks. I'm not really sure what your question was though.
 You probably want to write a very simple SampleSource that will load the whole TTML file into a byte[], and then provide it to a calling TextTrackRenderer (by passing it back in the SampleHolder provided). You don't need to do anything with the MediaFormatHolder for text.
 - We used to use that video internally, but we found multiple issues with it that made it unsuitable for testing.
- Yes, TextTrackRenderer should probably be enhanced to support styling. Some careful thought would be required to keep it lightweight and general enough to support multiple formats though.
- Fyi, we'll be adding WebVTT support very soon.
 - The problem with the missing MediaFormat is fixed already on the dev branch. The problems with the Elephants Dream video were around reliability of whatever is serving it, and I believe the subtitles contained invalid characters too.
- WebVTT is initially for DASH, but it should be fairly easy to wire it up for other use cases.
- There could be two renderers. Whether that's a good design choice depends on how extensive styling support needs to be, and what the overhead is of the logic for subtitles that don't have any etc.
 If you inspect the subtitle files for this stream, you'll see they explicitly state that they're UTF8. I'm pretty sure the subtitles are just broken, as per my reply above.

NB - We should let the parser infer the encoding rather than pass UTF-8, but it doesn't fix anything for this stream (since it infers UTF-8 anyway).
 Are you sure you're pulling the dev branch? I think you're pulling master, which isn't the dev branch.
 Sorry Johan; I didn't realize, but we were talking about different issues!
 FYI - I think a better fix is just to check fragmentRun.length in FragmentedMp4Extractor.seekTo. I'll drop a patch to dev early next week.
 The single sample should contain the full XML document (in theory ;)).
 The question at the top of this issue has been answered, and so I'm unsure what this is tracking at this point. Closing. Please spin out specific issues separately if needed. Thanks!
 It's also supported directly by the library if you use SingleSampleChunkSource. Basically the setup should be:

TextTrackRenderer->ChunkSampleSource->SingleSampleChunkSource

I don't think this approach has the issues that you describe above, so you may find it beneficial to switch from HttpSampleSource over to this approach. Effectively, you'll be replacing your custom HttpSampleSource with ChunkSampleSource->SingleSampleChunkSource.

Sorry, I'm aware documentation is poor around how to this kind of thing. I'm trying to find time to write some.
 Yes, it is complicated to setup currently. I'm working on making it simpler.
 Simplifications (which will apply to all subtitle formats) are tracked by #587.
 You're getting the NullPointerException because you've implemented the `register` method to return `null`, which is in violation of the [contract of that method](https://google.github.io/ExoPlayer/doc/reference/com/google/android/exoplayer/SampleSource.html#register%28%29).

That aside, it's really quite easy to use an external caption file in recent versions of ExoPlayer. You shouldn't need to be writing custom code. You can use `SingleSampleSource` and you should need about four lines of code, similar to those in the top post in https://github.com/google/ExoPlayer/issues/753.
  That sounds plausible. The easiest way to work around the issue would be to make an instance like:

ExoPlayer.Factory.newInstance(numRenderers, 0, 0)

Is your use case playing back regular MP4 files? The complete ExoPlayer based solution to this problem would be to implement everything at the application layer and stop using FrameworkSampleSource. This would give us enough control to fix these problems properly, as we've done for DASH and SmoothStreaming. It's quite a lot of work though.
 That doesn't sound ideal either. As per above, the best option by far is to implement everything at the application layer.

Maybe the best workaround would be to have FrameworkSampleSource.getBufferedPositionUs claim that it's buffered to the end if you're within the last ~5 seconds or so of the content. That would be more or less equivalent to setting the buffer times to 0 selectively, only when seeking near the end of the content.
 On the latest dev branch, you can use ExoPlayer's application level MP4 extractor to avoid this issue (in the demo app, use TYPE_MP4 rather than TYPE_OTHER).
  Yes, like that. It currently wont work too well if you're using FrameworkSampleSource, but will work perfectly for DASH and SmoothStreaming playbacks.

For FrameworkSampleSource, the issue is a result of the threading model used within that class. There's a TODO in the class to resolve the problem, but it's not particularly straightforward.
 Yeah, currently that's the case (except for the initial buffering phase, which probably will be reported correctly, although don't quote me on that ;)). Calling through to the extractor needs to be pushed off onto a separate thread to fix this.

If you're trying to play something that's not DASH or SmoothStreaming content, and you don't need any customization or advanced features, then you may as well just use the standard MediaPlayer API at this point in time.
  This works fine for me over a mobile network in the UK, and as far as I know I don't think there's any network specific logic in the media stack. Were you using the same 3G network on both devices? My best guess would be that your network provider is disconnecting the stream after some limit of bytes transferred.
 Closing.
  Typically release will be called in onPause or onStop of the parent activity. To be a good citizen on Android your application should ensure that it has released any decoders by the time these methods (ideally onPause) return. This requires that the release() implementation block until the internal thread has quit.

The root bug you're trying to fix is that something is blocking the internal playback thread for too long, which results in the release not happening fast enough (it should happen very quickly when things are working properly). So you need to find out what's happening on that thread. My hunch would be that it's blocking in FrameworkSampleSource.release(), in which case the root bug you're trying to fix is already described by a TODO in that class.
 I would recommend moving discussion of this to an Issue. Thanks!
  There isn't one. It would be possible to write your own software decoder (or use a third party software decoder) wrapped in a custom TrackRenderer implementation. We have no plans to make one available though, sorry.
 The developer guide states at the start of the overview that it's built on top of level 16 APIs (http://developer.android.com/guide/topics/media/exoplayer.html). It's marked as minSdkVersion=9 because components of the library are still useful on earlier API levels. I know of one project that uses various components of the library (e.g. the upstream package) all the way back to API level 9.

Where were you looking to see what API level was supported? I can add comment(s) in those places to make it clearer.
 16, assuming the audio needs to be decoded.
  You should be building the application using the latest Android SDK, and you'll need to have the v19 SDK Platform installed (you can do this in the Android SDK Manager). This doesn't stop you from running what gets built on an API level 16 device.

In the manifest of the demo app:

android:minSdkVersion="16" <-- This is the minimum API level on which the apk will install/run.
android:targetSdkVersion="19" <-- This is the SDK platform you need to compile.
 You're not supposed to edit the manifest. You're supposed to build it using the v19 Android SDK.
 The SDK you build with and the version of Android that your device runs are independent things. You can build using a v19 SDK and run the resulting apk on a v16 device, provided the minSdkVersion is 16 or lower. The developer just needs to make sure not to try and call any v17/18/19 APIs when running on a v16 device.
  Out of interest, why do you suggest putting it somewhere like Maven Central would discourage fork-and-hoard? Fundamentally people either want to change the source or they don't. Admittedly people are slightly more likely to do it in a clean way if they're using a binary (i.e. extension in their own codebase rather than modifying the library directly). Are there any other reasons?

As a second question, since I'm not familiar with things like Maven Central: Do you know how API incompatibility is handled? We're not trying to keep the API completely compatible from one release to the next, at least not at this stage, so it wouldn't be possible for us to just replace the binary with a new one each time. We'd probably have to version them. Does that negate the benefit?
 Ok, thanks for the info. I'll take a look.

I don't think you're forced to fork in the existing model though. You can just pull the Google version, build the jar, and then use it. Which seems equivalent, except with the added overhead of having to build it yourself.
 Yes. We'll do this for stable releases. You'll still need to use the source directly if you want to make modifications and/or work against the development branches, obviously.
 Update - Our intention is to publish through https://bintray.com/google in due course.
 Added use of bintray-release in: https://github.com/google/ExoPlayer/commit/85be2aed0a5e5487504905d45c1a565d288feee2
 Added here: https://bintray.com/google/exoplayer

This is now just pending a bintray "add to jCenter" request. Once that gets done, we can close this issue.
 This is now done. @blundell , it would be great if novoda could stop publishing a version, now that we're publishing one officially? In particular, you've got the "exoplayer" name, where-as the official version is "exoplayer-google", which doesn't make much sense [here](https://bintray.com/bintray/jcenter?filterByPkgName=exoplayer).
 Thanks! Closing this now.
  Thanks for the report. The reason stop() doesn't reset the position back to 0 is because it's often useful for an application to be able to query the final playback position after the player stops (this is particularly true if the player stopped itself internally due to an error).

The reason prepare() doesn't reset the position to 0 is because it's nice for an application to be able to seek before prepare() is called. If an application is in a position to do this safely (typically this means that the application reliably knows the duration of the media prior to calling prepare) then it can be more efficient to make the calls this way around, because it can avoid the player briefly starting to buffer data from t=0 prior to the seek being processed.

So neither stop() nor prepare() will reset the position, and you should explicitly call seekTo(0) if you want to reset the playback position.

If you don't do this, playback should start the second time around (although not from t=0). So that not happening is a bug. I think it's probably fixed on the dev branch. If you could check this, that would be great.
  To clarify, do you mean with audio (sped up but corrected to still be at the correct pitch) or without?
 This isn't something that's supported currently. It is something you could potentially do yourself, either by extending MediaCodecAudioTrackRenderer, or by creating your own audio renderer to use instead of the one provided by the library.

Basically, you'd need to do some adjustment in your extension to get the audio playing at the desired rate (and reporting time as progressing at that rate). This may involve pitch correction. The video track will follow the speed of the audio track (provided it can decode fast enough).
 +1
 There are a bunch of new APIs in M (API level 23) that provide underlying platform support for changing of playback rate. For ExoPlayer, the relevant ones would be:

http://developer.android.com/reference/android/media/AudioTrack.html#setPlaybackParams(android.media.PlaybackParams)
http://developer.android.com/reference/android/media/PlaybackParams.html

For MediaPlayer, the relevant method is:

http://developer.android.com/reference/android/media/MediaPlayer.html#setPlaybackParams(android.media.PlaybackParams)

I don't think there is a setPlaybackRate method. I think the inclusion of that method in the valid/invalid states table is an issue with the documentation being inaccurate.
 Hi! As it happens, we have a change that does exactly this on our side too. We'll be pushing it to the dev branch this week. The dev branch is already API 23, by the way :). Thanks!
 - Reverse playback is a different and more complicated problem. There are many implications of reversing the playback direction, such as the need to buffer backward rather than forward. For video, the way that most video is encoded makes reverse playback massively more expensive. So no, we have no plans to support reverse playback in any form.
- We have no plans to support pre-M playback rate changes at this time.
 You would have to implement speed/pitch adjustment yourself on the audio data before feeding it to the AudioTrack. Which is doable, but non-trivial.
 Tweaked in https://github.com/google/ExoPlayer/commit/fb7ddb722c7d377d54817caebb53d014d679b283
 Not as part of ExoPlayer, no. At least, not for the foreseeable future.
 Application developers who are interested in doing so are free to perform such an integration (it can be done in application code; there's no need to modify the core library). It's not the goal of this project to implement all extensions that might be useful ourselves.
 I've seen a solution that extends `MediaCodecAudioTrackRenderer` to implement this functionality. The idea was to override `processOutputBuffer`. In that method a transformation was applied to the audio held in the buffer (this is where a library hook would be called), and the transformed buffer was passed to `super.processOutputBuffer` to cause the transformed audio to play. It was a little more involved than that in practice, but that was the basic idea.
 It's not the case that if it were part of ExoPlayer it would become zero cost to maintain and keep in line with future updates. Moving it into ExoPlayer shifts the responsibility and maintenance burden from an application developer who wants the feature to us, but someone still has to do it. If someone wants this feature badly enough, it's possible for them to implement it themselves with a non-invasive extension. If someone wants to do that and open source their solution for others, they are free to do that too.
 Not "properly" though. It simply isn't as good of a solution as the solution provided for >= 23 in the core library, and would likely be of unacceptable quality for use by the majority of apps. Please don't advertise your solution as being a complete one (until the point at which audio plays smoothly and is pitch corrected).
 We've scheduled time to work on this in Q4 (i.e. by the end of the year). This is in no way a promise. Priorities may change, etc.
 Sorry for the delay. This slipped from Q4 but we are working on it and plan to have something ready later this quarter.

Please continue to treat this as a rough estimate rather than a guaranteed deadline and thanks for your patience! @PaulWoitaschek The enhancement is to be able to set the playback speed for audio before API 23 (it's already supported on Marshmallow and later, as mentioned in this thread). The audio renderer is used as the media clock, so changing the audio playback speed will have the effect of speeding up or slowing down video as well.

For video-only playback, you might be able implement your own speed adjustment feature by extending the video renderer and making it implement MediaClock. The implementation would be similar to StandaloneMediaClock but using renderer callbacks to start/stop the clock and adding a way to set the clock's speed. I haven't tried this but it should work.

Once the audio work is done we could consider providing a general-purpose 'set playback speed' method for any type of playback (assuming the audio renderer, if any, supports it). Our position on this issue is pretty clear. We're actively working on this but it's not done yet. We had intended to provide support in Q4 2016 but this has now slipped to later in Q1 2017. Please treat this as a rough estimate rather than a guaranteed deadline. If you cannot wait for official support then there are various unofficial approaches referenced in this issue that you can take a look at incorporating into your application code in the meantime. Thanks for your continued patience. @kdelanerolle We've indicated it's planned for later in Q1 2017. That is our provisional timeline. It's unclear what "committed" means. Timelines change frequently in many projects, including this one, for example if unforeseen technical issues arise during development or if higher priority issues come to light that need to be assigned resources. We try not to make promises for this reason. If you need maximal certainty then your best option is always to hedge by provisioning your own engineering resources to undertake the work. So that you know guys, there are not going to be progress updates for this for at least two weeks. As @ojw28 said above,
> If you need maximal certainty then your best option is always to hedge by provisioning your own engineering resources to undertake the work.

This **does not mean** it's going to be ready in two weeks. It means it's not going to be ready in two weeks. Dear all:

Besides setting speed, **I think we should inform the track selection (AdaptiveVideoTrackSelection) also for adaptive streaming case.** **Otherwise, it may lead to rebuffering by wrongly (over) estimated bit-rate.**

You could compare the result below to understand why we should do so.
1. Without the patch, several re-buffering happened: 
https://youtu.be/snDOuQ2yG5E
2. With the patch, NO rebuffering anymore: 
https://youtu.be/xyBz0rHZ5xY

You could check the issue by playbackStatus as:
![rebuffering](https://cloud.githubusercontent.com/assets/14846473/23099290/085809aa-f69e-11e6-8c03-b72a91ae4478.png)

It is, without informing AdaptiveVideoTrackSelection, we will select the representation set whose bit-rate is the maximum one among all representations having smaller bit-rate than current estimated network bandwidth.

For example, by the 4K test link https://www.youtube.com/watch?v=0wCC3aLXdOw, there are 8 representation sets there. The bit-rates from high to low are:
r1. 22876018
r2. 10577952
r3. 4466109
r4. 2354860
r5. 1164538
r6. 631150
r7. 251705
r8. ...
 
For my trial here, the network speed is about 4500000 ~ 5500000. 
Hence we will select r3 = 4466109.
Also, **my HTC X9 device could at most support 4X speedup.**
**Since the video is played by X4, obviously we will run out of buffering data soon. It is why we could see the rebuffering happened.**

**Therefore instead of selecting r3, we should select (4500000/4) ~ (5500000/4) =  1125000 ~ 1375000 by taking speed into consideration. 
It makes the selection locates from r5~r6. 
The test result shows it could avoid the frequent rebuffering issue.**

Please check the patch below.
https://github.com/WeiChungChang/ExoPlayer/commit/5f441187588aa9f5b2b16934bd7fce356155c839

Also, I make a test on my fork at:
https://github.com/WeiChungChang/ExoPlayer/tree/trickPlayBack
Please refer to it.

Thanks  Dear @KiminRyu:

1. What's kind of file you tested upon? The so called  'trick play' depends on the kind of files played much.

2. It should be unrelated to API; at least, for slow speed x2 I demo, it is the same as the work of @yangwuan55  or @prsolucoes.

 **_In fact, the most advantage of the scheme to implement trick play by seek is: it does NOT need the_** **_support of_**  `setPlaybackRate `  **_( > API 23)_**, 

3. There are still some mentions about 'reverse support'; 
In fact, I saw the functionality works on MacBook (includes rewind & high speed fast-forward;).  
As a lot of users' prejudice, APPLE provides much better operation convenience than Andorid. To eliminate it, why not give it a try to provide the functionality (or at least, to approach it) if we could?

PS: Per the experience I brought up this functionality previously on some Android smart TVs, when trick playback the user may expect the audio to be mute. 

Thanks a lot for your feedback, if actually fails on old devices (such as Galaxy note 2, Galaxy s4 ), I will try to get one of them to see the root cause. Could you provide the device you tested also? Thanks a lot for your comments in advance~

## @KiminRyu:
_**As agreed, only when >= API 23 it is able to use `setPlaybackRate` API. 
Please try the patch to use seek when < API 23.
https://github.com/WeiChungChang/ExoPlayer/commit/9b879e73a42c553c14873445ac1175764c3feaed
It is provided by @monkey-who.**_

Galaxy S4 (There are some discontinuities)
https://youtu.be/0sN5hn_Hl2k
Nexus 5 (what I expected)
[device-2017-02-28-112449.zip](url)

Per your attached, do you mean for Nexus 5 it works but S4 fails?
They are both on < API 23, right?
**_Do you play it locally or by streaming (the link seems to be of MPEG DASH)?_**

**_If it is a streaming source, please notice my comments before._**

> It provides my way to response the call-for-feature of trick playback here (an ‘acceptable’ way).
> It includes:
> 1.	Locally reverse playback.
> 2.	Locally high speed playback (more than X2 (or X4)).

> The patch could be applied to streaming also.
> However, I will improve the streaming case hereafter.

**_It means for streaming case we still have a lot of work to be done._** 

Also, please notice that for adaptive streaming, we SHOULD notify the bandwidth calculator ( AdaptiveVideoTrackSelection) about the change of play speed. Otherwise, it may lead to buffer underrun.

## @MateusZitelli 
The same as @KiminRyu also;
As I remembered, for 2K **_locally_** it may not as smooth as small size movies.
**_However it should NOT freeze._**
If you played locally, I will check the case here.
**_If you played a streaming source, as mentioned, it needs a lot of work later & belongs to TODO._**
 
## @robin2046 
**_Do you mean the patch make the files from https://www.reddit.com/r/gifs/ out of work?_**
If so, I will give it a check.


## Demo
Finally, the test below shows the patch works on an old **_HTC one mini with Android 4.4, API = 19._**
https://youtu.be/olyRXZX3-l8
**_originally(use setPlaybackRate), it is out-of-work(for X2, X4...)._**
**_With the patch, it provides a way to X2, X4 & rewind._**
**(Thanks a lot for @monkey-who help on the test)** @robin2046 
About your case when playing video only movie system will crash.
I have found the root cause and made a patch.
>@yangwuan55 I tried your work and it is impressive. However, when I try to play some video clip without audio track, I always get following error:
02-22 15:08:05.833 31708 31747 E ExoPlayerImplInternal: Internal runtime error.
02-22 15:08:05.833 31708 31747 E ExoPlayerImplInternal: java.lang.NullPointerException: Attempt to invoke interface method 'long com.google.android.exoplayer2.util.MediaClock.getPositionUs()' on a null object reference 

**_It is because we access rendererMediaClock but with a  video-only movie, it is NULL;_**

I have patched it within my branch as below.
https://github.com/WeiChungChang/ExoPlayer/commit/e0d9b7cbbd90a6008ea70bbdc3c000b8dbe2595d 
You could fix it by yourself if you are based  upon a different branch such as from Sir  @yangwuan55 in a similar way.

Thanks~  @robin2046
>@yangwuan55 I tried your work and it is impressive. However, when I try to play some video clip without audio track, I always get following error:
02-22 15:08:05.833 31708 31747 E ExoPlayerImplInternal: Internal runtime error.
02-22 15:08:05.833 31708 31747 E ExoPlayerImplInternal: java.lang.NullPointerException: Attempt to invoke interface method 'long com.google.android.exoplayer2.util.MediaClock.getPositionUs()' on a null object reference...

You could fix this issue by the patch below if you are at https://github.com/yangwuan55/ExoPlayer.
Thanks.

[patch.txt](https://github.com/google/ExoPlayer/files/823140/patch.txt)
 Note: This work is still in progress. Further changes will be landing soon. The work should be complete (or nearly complete) in `dev-v2` by that point, but it may take a week or two extra before we cut a release that includes it. Playback rate and pitch adjustments are seamless again as of https://github.com/google/ExoPlayer/commit/f7fff0d5835b42f6d6db7e46c7b1e546f84f545a. The feature is now at the level of our previous (API 23+ only) support, only now across all API levels. Expect a release in the next week or two as mentioned above.

Subsequent changes will follow that propagate the playback rate to some other components that would benefit from it (e.g. the ABR algorithm, in the case of adaptive playbacks). @KiminRyu I'm not aware of any issues with audio/video synchronization. If you file a new issue including steps to reproduce the problem we will take a look. Thanks. Apologies we've not managed to provide a release version containing this functionality yet. The functionality will be in the 2.4 release, but we're having to wait for a few other changes to land before we can publish it. We're now targeting toward the middle of next week. Thanks! @KirillMakarov GitHub has an atom feed you can add to your RSS reader:
https://github.com/google/ExoPlayer/releases.atom You can also watch https://bintray.com/google/exoplayer/exoplayer. There are quite a few ways to be notified of new releases without us having to update issues manually ;). @KiminRyu Did you see these problems when running a release APK or a debuggable APK?

The new implementation uses the Java version of Sonic, so its performance is affected by what optimizations the Java runtime applies. There is a certain optimization that is turned off on debuggable builds, which causes an inner loop in Sonic to be very slow. If you build a release APK this optimization should apply (on all API versions) and I found that performance was fine on various devices I tested, including a Samsung Galaxy S4 running Android 4.2.2 (API 17).

On API 24 Android builds onwards, performance should be fine on both debuggable and release builds. I used the instructions for Android Studio: [Sign your release build](https://developer.android.com/studio/publish/app-signing.html#release-mode). Thanks for the release. It works really great! @PaulWoitaschek Glad to hear it's working!

@KiminRyu Please could you file a new issue to track that, including all the details requested in the new issue template? Thanks.

I'm closing this issue now as the core functionality is implemented, but as @ojw28 noted [above](https://github.com/google/ExoPlayer/issues/26#issuecomment-290844837) further changes will be needed to propagate the playback speed to some other player components (like the adaptive track selection logic and LoadControl). In case it's useful: there's a [blog post](https://medium.com/google-exoplayer/variable-speed-playback-with-exoplayer-e6e6a71e0343) summarizing how to use this feature. I'm seeing multiple user complaints about high battery drain too.

> The C version of Sonic on API>=23 is much faster than the Java version of Sonic included in ExoPlayer.
Did you measure that? According to [the author](https://github.com/waywardgeek/sonic) the java and c version are quite equal. Do you have solid data backing up the claim of increased battery drain? Looking at CPU usage isn't sufficient. Neither is linking ad-hoc user reports to something you know has changed, when other things have also changed. It really needs to be actual battery drain data comparing the two whilst minimizing any other changes.

Note that CPU usage for the playback speed adjustment will likely be attributed to a different process in each case (your process when using the Java version, or the mediaserver process when using the C version). So if you're just looking at the CPU usage of your own process then that's not a correct comparison. As noted above the author of Sonic suggests the performance of the two are give-or-take equal. Note also that you need to be using a release build of your apk, as discussed above. Debug builds may yield vastly inferior performance with variable playback speed enabled.

As for why we're using the Java version everywhere: One reason is simplicity; having a single code path is just a lot simpler. Another reason is that the Android system support *isn't* perfect. We actually found a bug in Sonic (which is fixed in ExoPlayer's bundled version, but will only be fixed in Android from O onward). If we see solid evidence that power drain is significantly worse then we can revisit the decision. I don't think there's evidence in this thread (yet) of "huge increased battery drain" or that the Java version is significantly less efficient. It seems to me there are two possibilities at this point:

1. Total battery drain is actually about the same as before. Due to the way battery accounting works, battery drain associated with the variable speed adjustment is now being attributed to your app where-as previously it was attributed to mediaserver. This is causing your users to complain.
1. Total battery drain is actually higher (i.e. the user's battery will actually drain faster than before).

If it's the first of these then I don't think we're going to fix it. After all, your app *is* causing the battery drain that's being associated to it in this scenario. The fact the drain was not accounted to your app previously would effectively be an accounting trick, and something we should probably fix in the platform to ensure correct attribution in future Android releases. If it's the second of the two possibilities then this is definitely something we should look at. If you could help us to determine which of these cases is actually true, for example by running some experiments, that would be very helpful.

Note that it's how you build your apk that affects whether it's a release build or not, *not* how you depend on ExoPlayer in your `build.gradle`. See some of the comments from @andrewlewis further up this thread. It's supported on all api levels through `ExoPlayer.setPlaybackParameters`.  This sounds like a general problem with importing projects into Eclipse (not ExoPlayer specific). Please ask somewhere like StackOverflow instead [NB - I suspect more information will be needed for someone to solve your problem]. Thanks!
  https://github.com/google/ExoPlayer/pull/17 has already been merged into the dev branch, and I think does the same thing? It will be merged back into master with the next version bump.
  Thanks for the report. You're correct in thinking that this relates to Android L rather than ExoPlayer. I've routed the issue internally.
 Does it work if you set the "android-allow-cross-domain-redirect" header to "1" or "true"? You can pass the header into FrameworkSampleSource. For a quick and dirty test, you could try adding it directly in that class.
 Could you send me a link to the test video (or a link to a branch that has it as a sample)?
 Yes. My understanding is that there's a bug in the L developer preview where cross-protocol redirects are accidentally blocked. I'm mainly looking for a very low cost way to triple check that it's been fixed for the case you're hitting.
 Confirmed to be the same issue. Thanks!
  I'm not sure, but my guess would be that it's purely a parsing limitation (which should make it trivial to fix). Do you have any test content at all? If not I'll have a dig around for some at some point.
 I don't think we have anything more to say about AC3 at this point in time.
  Thanks for the reports. I'll take a look.

As an aside: The big benefits of ExoPlayer only really apply to DASH or SmoothStreaming playbacks, where ExoPlayer replaces Android's MediaExtractor with its own extraction, buffering and networking components. If you're just trying to play a video file, and if you're not actually using the added flexibility that ExoPlayer affords, then you wont really get much benefit (if any) over just using Android's standard media player.
 Yes, that's true. This does indeed appear to be a dupe (or roughly a dupe) of #18. You can actually make the videos referenced in this bug play fine just by changing DEFAULT_MIN_BUFFER_MULTIPLICATION_FACTOR to 16 in MediaCodecAudioTrackRenderer.

You shouldn't do this though; it's definitely not the correct solution(and makes player instantiation more likely to fail). The proper fix probably involves a memory copy for the audio samples, whilst they're still compressed. I'll have a think about how best to do this.
 A much better solution is just to hardcode MediaCodecVideoTrackRenderer.isReady() to return true, which works provided that the audio track is enabled. You can extend MediaCodecVideoTrackRenderer in your own application code and just override the method, if you need to get started playing this kind of media.

Something along these lines will be the "proper" solution to this problem, but some thought will be needed to correctly handle the case where the audio track is disabled (or missing), and around exactly how this should be wired together.
 You should find that these files play fine if you pull the latest from the dev branch.

There are still issues around FrameworkSampleSource, namely around threading and correctly transitioning in and out of the buffering state (these were already there; they're not new issues :)). But the videos referenced in this bug do at least play now.
  flv is not a priority for us right now.
  This is probably fixed on the dev branch. Please re-open / comment if not!
  Yeah, you're right! This is actually already fixed on the dev branch, and will be merged back into master shortly.
  Hey. I'd be happy to accept this, although it should go into the "dev" branch. Also, you'll need to sign (or have already signed) a CLA for us to accept pull requests, as per:  https://github.com/google/ExoPlayer/blob/master/CONTRIBUTING.md.

Thanks!
  Not currently. We'll use this issue to track adding support, although it's not a priority for us (v.s. widening DASH support, DASH Live and HLS) at the moment.
 Reopening to track eventual support.
 This is now working, with the exception that sparse TTML subtitles may cause buffering issues (fix to follow).
 Most Android devices don't support PlayReady, which is why you get an UnsupportedSchemeException. Widevine is the only mandatory DRM that Android devices need to support.
 Fixed.
 Yes. We've been waiting until we can merge dev-l before cutting a new release, but with the release of the L SDK this should now be possible.
 FYI - This is now merged.
  - Do you have reliable reproduction steps? I've never seen this.
- What value does getDuration return when this happens, and on what device? Please pull a full bug report if possible.
 As far as I know, Fire OS is incompatible with Google's Android compatibility standards (and has not passed CTS). Hence we do not officially support it. It may well be a Fire OS problem.

Please re-open if you can reproduce on a compatible device.
  If this is only happening on a custom ROM then you should file a bug against the custom ROM in the first instance. Does it happen on any official Samsung builds?
 Does this occur with the streams included in the demo app, or some that you've added yourself? Which videos specifically? There's not really enough to go on here.

Specific reproduction steps and/or full bug reports would be a great help. Thanks!
 Not enough information to reproduce, so closing for now. This may well have been fixed on the dev branch as part of the fix for #21.
  This is expected. The demo application has a minSdkVersion of 16, so you shouldn't have even been able to install it, at least not using "adb install": https://github.com/google/ExoPlayer/blob/master/demo/src/main/AndroidManifest.xml#L28

The dev guide also notes that ExoPlayer is build to run on API levels >= 16 at the start of the overview section: http://developer.android.com/guide/topics/media/exoplayer.html#overview

The actual library indicates a minSdkVersion of 9 because it's possible to use parts of the ExoPlayer library on older versions of Android if you're, for example, willing to write your own custom TrackRenderer that hooks into software decoders that you package in your apk.
 Ah, that's an oversight in the gradle build. I've been mostly using the Eclipse projects instead. I'll fix that, thanks.
 Fixed in dev in: https://github.com/google/ExoPlayer/commit/b398c594fa15d56217c5482f0ff9740c757274ec
  Duplicate of #8 
 It's a duplicate because addAll is fine for API levels >= 16. This crash only occurs on API levels < 11, so once the correct minSdkVersion (16) is enforced, this wont be an issue.
  Duplicate of #2 (see stackoverflow thread for details).
   That sounds about right, except the extractor should be called something like Mpeg2Extractor, since I think it would be extracting from the MPEG-2 transport stream?
 And yes, we are looking at this too. We'll provide an update when we have something to share!
 Aren't there timestamps in the manifest? If there weren't, then it would be impossible to implement seeking?
 Yeah. Playlists fundamentally must contain timestamps (or durations from which timestamps can be derived) for seeking to be possible. The timestamps must also not drift as you get further down the list, otherwise seeking would become inaccurate the further down the playlist you get.

If there needs to be some timestamp adjustment to the sample timestamps themselves, something similar to sampleOffsetUs in Mp4MediaChunk might do the trick, depending on what exactly is required. I'm hoping to get more up to speed with HLS in the next few weeks, and will be able to provide a slightly more informed opinion as opposed to random thoughts :).
 Correct. We're focused on DASH using fMP4 currently. There are some good reasons why here: http://184.168.176.117/reports-public/Adobe/20111116-fMP4-Adobe-Microsoft.pdf
 We noticed this too. Bit of a pain. As far as I can tell from looking at the MPEG-TS spec, there aren't any guarantees about alignment of access units with respect to PES boundaries (except in the case where certain flags are set on the packets that indicate this property). This is true for both audio and video (although the Apple test stream does seem to have video frames aligned to PES boundaries).

Which means, as far as I can tell, you have to manually look at the reconstructed H264 / AAC streams to detect the actual access unit boundaries?
 Yes. It's pretty much essential to be able to claim HLS support. I think just supporting H264 and AAC is fine.
 We're actively working on adding HLS support to ExoPlayer. I created dev-hls branch for our "work in progress" changes. It currently has a basic HLS support for VOD and Live playlists, and EXT-X-DISCONTINUITY tag working. Note that it doesn't have a lot of important features like adaptive bitrate switching or encryption support yet. We will be adding them to this branch as soon as they're ready.
 i've also opened separate issues to let you know whats on our current plans and also track them separately:
https://github.com/google/ExoPlayer/issues/67 ID3 Timed Metadata
https://github.com/google/ExoPlayer/issues/68 Closed Captions
https://github.com/google/ExoPlayer/issues/69 Encryption
https://github.com/google/ExoPlayer/issues/70 Adaptive bitrate

code will be pushed into https://github.com/google/ExoPlayer/tree/dev-hls
 @samek yes, we will be updating dev-hls and merge it into master once it's production ready.
 @samek there are no plans on using it at the moment.
 @AndroidNoob I opened https://github.com/google/ExoPlayer/issues/71 issue for it. And answered there - MP3 support for HLS is not on our current plans. Sorry.
 Apologies for the churn caused to those people who've been using the @martinbonnin branch. To clarify: That branch explored a way of adding HLS support, and the experience gained from that was really helpful. Ultimately we decided to structure HLS support differently, which is why we opted to follow a different approach. Please bear with us as we fill in some of the gaps in terms of functionality.
 @perchrh it's not a high priority for us at the moment, but it's likely that we'll get to it at some point in the future https://github.com/google/ExoPlayer/issues/73
 @crossle Please state what stream you used to reproduce this, and whether it happens consistently or was a 1 off. Thanks.
 You need to provide more information for us to be able to investigate. Which url specifically, so that we can try it ourselves? After how long does the problem occur, on which device? If possible, please also provide a full bugreport captured shortly after the problem is encountered (adb bugreport).

Thanks!
 We have implemented the main HLS stack with all the features we have originally targeted. I'm closing this issue. If you'd like to discuss a bug, missing feature, or any idea/suggestion, please open a separate specific issue.
 @AndroidNoob Do you have any sample streams for HLS containing MP3?
  Currently ExoPlayer only supports a narrow subset of possible DASH manifests. Support should be added for manifests containing segment lists and segment templates.

Note: As a workaround, application developers can already support arbitrary DASH manifests through implementing their own parser and ChunkSource (it's probably useful to use DashMp4ChunkSource as a starting point).
 There are a whole bunch of test mpds here:
https://github.com/Dash-Industry-Forum/dash.js/blob/master/app/sources.json
 - The first and third of those mpd links already play for me on the dev branch.
- The second one requires SegmentTemplate support. I have this working locally, and it will be pushed next week. Stay tuned. Ditto for any mpds that contain SegmentList elements.
- The fourth link isn't an mpd.
- I'm not sure what to do about the fifth one yet. That style of mpd neglects to define a segment index (either by specifying where the sidx is located in the stream, or by defining an index in the manifest itself). You end up having to make an unbounded request from the head of each stream until you find the index. That doesn't seem ideal, and I'm not sure we'll be supporting that case.

Thanks!
 Np. The big missing piece here at the moment is support for SegmentTemplate and SegmentList, both of which are fairly commonly used (as in the second mpd). This will be fixed very soon. 
 Support for SegmentTemplate and SegmentList have been merged into both dev and master.
 You could try: http://www-itec.uni-klu.ac.at/dash/ddash/mpdGenerator.php?segmentlength=6&type=full
