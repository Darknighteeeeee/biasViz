  How did you build tesseract?
Error messages you posted comes from leptonica, so... I would suggest you make sure that the all dependencies (at least leptonica) are build as 64bit and with the same version of compiler... Hi,

Not sure if this is your post https://groups.google.com/forum/#!topic/tesseract-ocr/lufafrAAEnk
It points to `/Ob2` option while you state `/Ot`.

Did you try `RelWithDebInfo` configuration? To check if this is VS2017 issue, it's necessary to build on VS2015.
As I understand if we haven't any troubles on gcc/clang, it's problem of MSVC, not tesseract.  Please respect instruction for posting issue - use tesseract user forum for asking support  There are [clear instructions](https://github.com/tesseract-ocr/tesseract/wiki/Compiling) how to build tesseract from source. If somebody decide to ignore them, it is hie/her problem and not tesseract project.

Using autogen.sh is common way how to start configuration on build process in many opensource projects. 1. How did you installed leptonica?
2. Do you have more leptonica (e.g. from source and by system) installations? > syntax error near unexpected token `LEPTONICA,
this means that pkg-config is not aware about leptonica installation I am not pkg-config guru (or user ;-), but installing leptonica to custom location (e.g. not system wide) is always source of troubles... Before "pkg-config era" (3.04) this trick worked: https://github.com/tesseract-ocr/tesseract/wiki/Compiling#install-elsewhere--without-root.

Can you try this?
`ACLOCAL_FLAGS="-I /hab/pkgs/tpolekhin/leptonica/1.74.4/20171124113357/" ./autogen.sh`  1. Please see https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#combining-the-output-files regarding how to create the traineddata file.

To create the 'fast' version, use 

convert_to_int | bool | false | With stop_training, convert to 8-bit integer for greater speed, with slightly less accuracy.
-- | -- | -- | --

2. You cannot use the tesstrain.sh /text2image process using non-unicode legacy fonts. For those you have to create image files from a document/pdf, use tesseract with makebox config file to create the box files, manually edit the box files for correct info and 4.0 format.

Training from images is NOT supported for 4.0 LSTM.

Your best bet will be to find some unicode fonts that look similar to the legacy fonts and train using those.    My vote is for dropping different versions. Is it possible to have gif4 and gif5 simultaneously in macports?
E.g., create giflib5 and set it for leptonica only. Other packages will use it when they're ready. Maybe you should drop it right after 1.75 release? So the change will be for 1.76 which is released after some another reasonable time.  running configure ends with the following messages.

```
Configuration is done.
You can now build and install tesseract by running:

$ make
$ sudo make install

Training tools can be built and installed with:

$ make training
$ sudo make training-install
```

I suggest that https://github.com/tesseract-ocr/tesseract/blob/master/configure.ac be modified to add instructions to run `ldconfig`
similar to as given in instructions in 
https://github.com/tesseract-ocr/tesseract/wiki/Compiling-%E2%80%93-GitInstallation

  ------------------------

### Environment

* **Tesseract Version**: <!-- Master-->
* **Commit Number**: <!-- 1207-->
* **Platform**: <!-- Windows 10 64 bit -->

### Current Behavior:
Tesseract baseAPI
`pixWriteAutoFormat("F:/test.jpg", croppedImage);
		Pix* test = pixRead("F:/test.jpg");
		api.SetImage(test);
		api.Recognize(0);
		char *value2 = api.GetUTF8Text();`

ERROR : 
`Error in pixCreateHeader: height must be > 0
Error in pixCreateNoInit: pixd not made
Error in pixCreate: pixd not made
Error in pixClipRectangle: pixd not made`

Note : Input image is exist !

### Expected Behavior:

### Suggested Fix:
Leptonical build
 This is not tesseract issue.  This is Tesseract issue .
This is my command line output : 
I'm trying to edit cppan.yml clean cppan cache cmake cache an rebuild but still error
dependencies:
        pvt.cppan.demo.danbloomberg.leptonica: 1.74.2

I'm tested in 1.74.2,1.74.3,1.74.4?
Can you explain why this is not tesseract issue?

PS E:\TesseractOpenCV\Build\tesseract\build64\bin\Release> .\tesseract.exe --tessdata-dir F:/ F:\1.png F:/abc
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Error in pixCreateHeader: height must be > 0
Error in pixCreateNoInit: pixd not made
Error in pixCreate: pixd not made
Error in pixClipRectangle: pixd not made
Error in pixCreateHeader: height must be > 0
Error in pixCreateNoInit: pixd not made
Error in pixCreate: pixd not made

 it is not teaseract issue:
1. message comes from leptonica.
2. it complains about wrong input.
3. it seems that you did not trace where is problem.


Dňa 14. 11. 2017 7:32 používateľ "hoangtocdo90" <notifications@github.com>
napísal:

This is Tesseract issue .
This is my command line output :
I'm trying to edit cppan.yml clean cppan cache cmake cache an rebuild but
still error
dependencies:
pvt.cppan.demo.danbloomberg.leptonica: 1.74.2

I'm tested in 1.74.2,1.74.3,1.74.4?
Can you explain why this is not tesseract issue?

PS E:\TesseractOpenCV\Build\tesseract\build64\bin\Release> .\tesseract.exe
--tessdata-dir F:/ F:\1.png F:/abc
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica

Error in pixCreateHeader: height must be > 0
Error in pixCreateNoInit: pixd not made
Error in pixCreate: pixd not made
Error in pixClipRectangle: pixd not made
Error in pixCreateHeader: height must be > 0
Error in pixCreateNoInit: pixd not made
Error in pixCreate: pixd not made

—
You are receiving this because you modified the open/close state.

Reply to this email directly, view it on GitHub
<https://github.com/tesseract-ocr/tesseract/issues/1208#issuecomment-344159713>,
or mute the thread
<https://github.com/notifications/unsubscribe-auth/AAjCzG1-KlAL6DLd1mW7aPF4671EyaYwks5s2TPjgaJpZM4QbTyj>
.
 #1223   Please respect instruction for posting issue: use tesseract user forum for asking support.  `char* outText = api->GetUTF8Text();

delete[] outText
`

maybe return text was null   Please use google or other search engine before posting issues.  Please follow instruction for posting issue - use tesseract user forum for asking questions.  please post output of
tesseract -v

Dňa 3. 11. 2017 22:44 používateľ "Mitchell Galloway" <
notifications@github.com> napísal:

> I am attempting to run this:
>
> lapply(myfiles, function(i){
>
>     shell(shQuote(paste0("pdftopng -f 1 -l 10 -r 600 ", i, " ocrbook")))
> mypngs <- list.files(path = dest, pattern = "png", full.names = TRUE)
> lapply(mypngs, function(z){
>     shell(shQuote(paste0("tesseract ", z, " out")))
>     #file.remove(paste0(z))
>     })
> })
>
> However, even though I am feeding it a PNG file, I get the error
>
> Tesseract Open Source OCR Engine v3.05.01 with Leptonica
> Error in pixCreateNoInit: pix_malloc fail for data
> Error in pixCreate: pixd not made
> Error in pixReadStreamPng: pix not made
> Error in pixReadStream: png: no pix returned
> Error in pixRead: pix not read
> Error during processing.
>
> 1: running command 'C:\Windows\system32\cmd.exe /c "tesseract C:\users\gallowmb\desktop/ocrbook-000001.png out"' had status 1
> 2: In shell(shQuote(paste0("tesseract ", z, " out"))) :
>   '"tesseract C:\users\gallowmb\desktop/ocrbook-000001.png out"' execution failed with error code 1
>
> RStudio 1.1.383
> Tesseract 3.05.01
> Win 7 64 bit
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1196>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AAjCzKCQcnxwVWezs041Fzo4XlBWXfzLks5sy4lZgaJpZM4QRtxA>
> .
>
 Can you share ocrbook-000001.png and info how to get/installed tesseract?  Please respect instructions for project issue tracker.  @theraysmith has not updated the repositories with changes to handle all these issues. Hence, you should not expect them to be fixed.  I think Ray was planning to do new training to handle all these cases. But there has been no update from him since then. Based on past patterns, I would guess that he will make some updates to project before year end! Thanks for sharing the traineddata. Please let us know the succeed rate of OCR when using it.

Do you combine it with Arabic traineddata to get correct text plus Arabic numbers using -l Ara+  read_params_file: Can't open makebox

Your tessdata-dir does not have the config file 'majebox'. Please check.

On 27-Oct-2017 2:24 AM, "ErnstTmp" <notifications@github.com> wrote:

> makebox.txt
> <https://github.com/tesseract-ocr/tesseract/files/1420017/makebox.txt>
> Script to make the boxes attached
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1188#issuecomment-339797707>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5orY7xm0fdt6JF_SloHMe0AKSsLks5swPF3gaJpZM4QIMxl>
> .
>
  ...and it describe clearly there is no issue in tesseract but in your computer setting. Use tesseract forum as described in issue tracker instruction.  Please respect instruction for posting issue! Use tesseract user forum for asking support. This is not tesseract bug....  The latest traineddatas (tessdata_best and Tessdata_fast) do not support legacy tesseract engine, so --oem 0 and --oem 2 are not supported.

However, program should not crash but rather give an error message. Please see
https://github.com/tesseract-ocr/tesseract/wiki/Data-Files#updated-data-files-for-version-400-september-15-2017

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Oct 23, 2017 at 10:21 AM, Tao <notifications@github.com> wrote:

> Thanks for the update. Where can I get the 'latest traineddata' please? I
> got my data from https://github.com/tesseract-ocr/tessdata/
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1181#issuecomment-338547304>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1A6Ocre9MuS8eUo5CfbBl_Ibfk2ks5svBtUgaJpZM4QCEnk>
> .
>
  Please read instructions given in the wiki page for training for 4.0.

You have to use lstmtraining command with --stop-training to create the new
traineddata.



On 22-Oct-2017 7:09 PM, "ibr123" <notifications@github.com> wrote:

> Hi,
>
> i was using the newest versions of Tesseract to:
> 1- create LSTMFs
> 2- extract the LSTM from official traineddata
> 3- fine tune the LSTM according to the created LSTMFs
> 4- and then use the new traineddata for detection for better results
> i was able to do all these steps and get the final traineddata but when i
> made detection using it i got an error, now the same case happened for
> Arabic and Japanese languages, and both i was able to get fine tuned
> traineddata and both failed in detection and gave the same error.
> the commands i used as described here
> <https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#lstmtraining-command-line>
> and they were:
> *for creating LSTMFs*
> training/tesstrain.sh --fonts_dir ~/.local/share/fonts --lang ara
> --linedata_only --noextract_font_properties --langdata_dir
> /home/ibr/latest_leptonica_4/langdata --tessdata_dir ./tessdata
> --output_dir /home/ibr/latest_leptonica_4/lstmf_ara_lep4
>
> **extracted LSTM from *here*
> <https://github.com/tesseract-ocr/tessdata_best>
>
> *tuned LSTM*
> training/lstmtraining --model_output /home/ubuntu/lep_latest/ara_tune/tune_result/ara_tune
>
> --continue_from /home/ubuntu/lep_latest/ara_tune/extracted/ara.lstm
> --traineddata /home/ubuntu/lep_latest/ara_tune/original_traineddata/ara.traineddata
>
> --train_listfile /home/ubuntu/lep_latest/ara_tune/ara.training_files.txt
> --max_iterations 1200
>
> **then unpacked official traineddata"official traineddata", replaced the
> tuned LSTM and combined everything together again **
>
> *detection command*
> tesseract ara2.png tuned -l ara --tessdata-dir ./tessdata --oem 1
>
> and when i used the new tuned traineddata i got the error:
>
> *index >= 0:Error:Assert failed:in file strngs.cpp, line 270 Segmentation
> fault (core dumped)*
>
> that happened when i tried fine tuning for Arabic and Japanese, twice,
> first time using the version:
> *tesseract 4.00.00dev-690-g1b0379c leptonica-1.74.4*
> second time: *tesseract 4.00.00dev-691-gfb359fc leptonica-1.74.4*
> *keep im mind the same exact steps worked for previous versions of
> tesseract with leptonica 1.74.1*
> Thanks in advance
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1180>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o-zB_-b2Znyj59VRBnw-zEoSI56zks5su0WkgaJpZM4QB86_>
> .
>
 The new format traineddata file has a version string,
unicharcompressor/recorder, and a unicharset which matches with the lstm
model.

If you combine your fine-tune lstm with existing traineddata, the files
will not be in sync. You are getting error because of mismatch of
unicharset with lstm.

Also, your lstmtraining command should use the starter traineddata.

Wiki pages have more details.

On 22-Oct-2017 10:15 PM, "ShreeDevi Kumar" <shreeshrii@gmail.com> wrote:

> Please read instructions given in the wiki page for training for 4.0.
>
> You have to use lstmtraining command with --stop-training to create the
> new traineddata.
>
>
>
> On 22-Oct-2017 7:09 PM, "ibr123" <notifications@github.com> wrote:
>
>> Hi,
>>
>> i was using the newest versions of Tesseract to:
>> 1- create LSTMFs
>> 2- extract the LSTM from official traineddata
>> 3- fine tune the LSTM according to the created LSTMFs
>> 4- and then use the new traineddata for detection for better results
>> i was able to do all these steps and get the final traineddata but when i
>> made detection using it i got an error, now the same case happened for
>> Arabic and Japanese languages, and both i was able to get fine tuned
>> traineddata and both failed in detection and gave the same error.
>> the commands i used as described here
>> <https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#lstmtraining-command-line>
>> and they were:
>> *for creating LSTMFs*
>> training/tesstrain.sh --fonts_dir ~/.local/share/fonts --lang ara
>> --linedata_only --noextract_font_properties --langdata_dir
>> /home/ibr/latest_leptonica_4/langdata --tessdata_dir ./tessdata
>> --output_dir /home/ibr/latest_leptonica_4/lstmf_ara_lep4
>>
>> **extracted LSTM from *here*
>> <https://github.com/tesseract-ocr/tessdata_best>
>>
>> *tuned LSTM*
>> training/lstmtraining --model_output /home/ubuntu/lep_latest/ara_tune/tune_result/ara_tune
>>
>> --continue_from /home/ubuntu/lep_latest/ara_tune/extracted/ara.lstm
>> --traineddata /home/ubuntu/lep_latest/ara_tune/original_traineddata/
>> ara.traineddata
>> --train_listfile /home/ubuntu/lep_latest/ara_tune/ara.training_files.txt
>> --max_iterations 1200
>>
>> **then unpacked official traineddata"official traineddata", replaced the
>> tuned LSTM and combined everything together again **
>>
>> *detection command*
>> tesseract ara2.png tuned -l ara --tessdata-dir ./tessdata --oem 1
>>
>> and when i used the new tuned traineddata i got the error:
>>
>> *index >= 0:Error:Assert failed:in file strngs.cpp, line 270 Segmentation
>> fault (core dumped)*
>>
>> that happened when i tried fine tuning for Arabic and Japanese, twice,
>> first time using the version:
>> *tesseract 4.00.00dev-690-g1b0379c leptonica-1.74.4*
>> second time: *tesseract 4.00.00dev-691-gfb359fc leptonica-1.74.4*
>> *keep im mind the same exact steps worked for previous versions of
>> tesseract with leptonica 1.74.1*
>> Thanks in advance
>>
>> —
>> You are receiving this because you are subscribed to this thread.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/1180>, or mute the
>> thread
>> <https://github.com/notifications/unsubscribe-auth/AE2_o-zB_-b2Znyj59VRBnw-zEoSI56zks5su0WkgaJpZM4QB86_>
>> .
>>
>
 Read https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00

specially
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#fine-tuning-for--a-few-characters

Command has to be as follows:

training/lstmtraining --model_output /path/to/output [--max_image_MB 6000] \
  --continue_from /path/to/existing/model \
  --traineddata /path/to/traineddata/with/new/unicharset \
  --old_traineddata /path/to/existing/traineddata \
  [--perfect_sample_delay 0] [--debug_interval 0] \
  [--max_iterations 0] [--target_error_rate 0.01] \
  --train_listfile /path/to/list/of/filenames.txt


​Also see
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#combining-the-output-files

training/lstmtraining --stop_training \
  --continue_from ~/tesstutorial/eng_from_chi/base_checkpoint \
  --traineddata ~/tesstutorial/engtrain/eng/eng.traineddata \
​
 --model_output ~/tesstutorial/eng_from_chi/eng.traineddata​​





ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Oct 23, 2017 at 2:59 PM, ibr123 <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> sorry, my mistake, i forgot
> to mention the command to create the LSTM from the check point which is:
> *training/lstmtraining --model_output
> /home/ibr/latest_leptonica_4/ara_tune/tuned_lstm/ara.lstm --continue_from
> /home/ibr/latest_leptonica_4/ara_tune/results/ara_checkpoint --traineddata
> /home/ibr/latest_leptonica_4/ara_tune/original_traineddata/ara.traineddata
> --stop_training*
>
> now, summarize everything,
>
>
>
>
>
>
> *i create LSTMFs extract LSTM from "best_traineddata" tune LSTM according
> LSTMFs , in the command " --traineddata best_traineddata" create LSTM from
> checkpoint, in the command " --traineddata best_traineddata" unpack
> "best_traineddata" replace LSTM with the new tuned one combine everything*
>
> If you combine your fine-tune lstm with existing traineddata, the files
> will not be in sync
> means in combination step i have to combine the new LSTM with the
> traineddata that is created when creating LSTMFs?
> i used "best_traineddata" in the command
>
>
>
>
> *training/lstmtraining --model_output
> /home/ubuntu/lep_latest/ara_tune/tune_result/ara_tune --continue_from
> /home/ubuntu/lep_latest/ara_tune/extracted/ara.lstm --traineddata
> /home/ubuntu/lep_latest/ara_tune/original_traineddata/ara.traineddata
> --train_listfile /home/ubuntu/lep_latest/ara_tune/ara.training_files.txt
> --max_iterations 1200*
> because if i pointed the --traineddata to the traineddata that is created
> with LSTMFs (starter traineddata)
> i will get this message:
> **Warning: LSTMTrainer deserialized an LSTMRecognizer!
> Code range changed from 307 to 74!
> Must supply the old traineddata for code conversion!
> Failed to continue from: /home/ibr/latest_leptonica_4/
> ara_tune/extracted/ara.lstm
> **
> so since i got "Must supply the old traineddata" i used the trained data
> from "best_traineddata" the whole time, so what im missing?
> Thanks
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1180#issuecomment-338601210>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5TsxR-U5rw0yR-XYASqpXqN_3w-ks5svFxxgaJpZM4QB86_>
> .
>
 If your unicharset size is different, you need to provide --old_traineddata
.

failed to write checkpoint usually happens if you have not created the
output directory.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Oct 23, 2017 at 7:44 PM, ibr123 <notifications@github.com> wrote:

> the last two command actually worked and the traineddata i created is
> fine, i changed the first command to:
>
>
>
>
>
> *training/lstmtraining --model_output
> /home/ubuntu/lep_latest/test/tuning_results/jpn --continue_from
> /home/ubuntu/lep_latest/jpn_tune/extracted/jpn.lstm --traineddata
> /home/ubuntu/lep_latest/jpn_tune/jpn_lstmf/jpn/jpn.traineddata
> --old_traineddata
> /home/ubuntu/lep_latest/jpn_tune/original_traineddata/jpn.traineddata
> --train_listfile /home/ubuntu/lep_latest/jpn_tune/jpn.training_files.txt
> --max_iterations 3600*
> but this command was in "Fine Tuning for ± a few characters" section, in
> my case i want only to train the best_traineddata to a new fonts only
> without changing it so i assumed that i needed to go though fine tuning but
> it didn't work, so does this tuning command covers it?
>
> also another question, when i run the above command sometime i get the
> message *checkpoint failed to write checkpoint* when that message shows
> following command fails:
>
>
>
> *training/lstmtraining --stop_training --continue_from
> /home/ubuntu/lep_latest/test/tuning_results/jpn13.689_2618.checkpoint
> --traineddata
> /home/ubuntu/lep_latest/jpn_tune/jpn_lstmf/jpn/jpn.traineddata
> --model_output /home/ubuntu/lep_latest/test/traineddata/jpn.traineddata*
> what causing this message to show?
>
> Thanks
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1180#issuecomment-338672679>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o4JlUK23O18xfdfLdxdz632i8SzPks5svJ9IgaJpZM4QB86_>
> .
>
  Did you check the output of configure?  Please follow instruction of issue tracker - we do not support 3rd party sw (python)  Version: Tesseract 4.0alpha from github
Platform: Windows10/WSL Ubuntu 14.04

```
Rendered page 338 to file /tmp/tmp.Bja2jDWX7C/san/san.Nakula.exp0.tif
Null box at index 0
Error: Call PrepareToWrite before WriteTesseractBoxFile!!
```

The tif files are being created by text2image but the box files have size zero. It seems to be dependent on the training_text and works with other text files.

  Please read & follow instruction for posting issue.  Easiest way for you to get this is to run tesstrain.sh for English with a short training text and one font. Then look in your tmp folder for the box/tif files. Here are sample files for English ...

[eng-box-tif.zip](https://github.com/tesseract-ocr/tesseract/files/1404580/eng-box-tif.zip)
 The following works for me.

img_files=${img_files}' '$(ls shritatvanidhi-0056*.tif)
    for img_file in ${img_files}; do
       echo ${img_file}
       tesseract ${img_file} ${img_file%.*}  --psm 6 --oem 1 -l hin
--tessdata-dir /mnt/c/Users/User/shree/tessdata makebox
    done

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Oct 23, 2017 at 1:27 AM, ErnstTmp <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> thank you very much for your
> files!!!! I used tesstrain.sh, but did not look into /tmp. Shame on me!
> I could not get tesseract to create box files. I used
>
> tesseract pol.ocrb.exp$i.tif pol.ocrb.exp$i batch.nochop makebox
>
> as described, but it created a file called batch.nochop.box, but not the
> box file I expected (an old style box file would be OK)
>
> Thanks,
> Ernst
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1174#issuecomment-338504508>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o3JotONc7aH5oBARnt3DE4mEy82Aks5su54PgaJpZM4QASxL>
> .
>
  The old format box files will not work for LSTM training. AFAIK, currently training is only supported with the synthetic box/tiff pairs generated via tesstrain.sh.

See https://github.com/tesseract-ocr/tesseract/issues/768 for more details.

 I copy them to my langdata/language directory and then use a modified
tesstrain.sh to copy them to the tmp training directory.


tesstrain.sh changes

```
mkdir -p ${TRAINING_DIR}
tlog "\n=== Starting training for language '${LANG_CODE}'"

cp  ../langdata/${LANG_CODE}/*.box ${TRAINING_DIR}
cp  ../langdata/${LANG_CODE}/*.tif ${TRAINING_DIR}

ls -l  ${TRAINING_DIR}
source "$(dirname $0)/language-specific.sh"
```

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Oct 18, 2017 at 2:53 PM, 694376965 <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii>, I have change the format box
> files according to the requirements of tesseract 4.0 , namely I add a TAB
> at end of line and spaces to demarcate words for the box files.
>
> could you tell me how to use new format box/tiff pairs to generate *.lstmf
> files? Thanks!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1172#issuecomment-337521195>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5yzL6sQNcMR-xJeNDiL1Gq0R2naks5stcOQgaJpZM4P9XT0>
> .
>
 Please check the syntax of your command. 

Training text is in langdata dir.

Rather than modifying tesstrain.sh too much, you could keep a small dummy
training text in one font to use along with your box tiff pairs.

I have mostly tested training with synthetic images, using precreated box
tiff pairs just as sample.

 training/tesstrain.sh \
--fonts_dir /mnt/c/Windows/Fonts \
 --lang san \
 --noextract_font_properties  --linedata_only \
 --exposures "0" \
 --langdata_dir ../langdata \
 --tessdata_dir ../tessdata \
 --fontlist \
    "Siddhanta" \
  --output_dir ../tesstutorial/san

You have to make sure that all directories reflect your setup.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Oct 18, 2017 at 7:36 PM, ShreeDevi Kumar <shreeshrii@gmail.com>
wrote:

> Please check the syntax of your command. Langdata is referred to via
> script dir.
>
> Training text is in langdata dir.
>
> Rather than modifying tesstrain.sh too much, you could keep a small dummy
> training text in one font to use along with your box tiff pairs.
>
> I have mostly tested training with synthetic images, using precreated box
> tiff pairs just as sample.
>
> On 18-Oct-2017 5:12 PM, "694376965" <notifications@github.com> wrote:
>
>> @Shreeshrii <https://github.com/shreeshrii> I changed the tesstrain.sh
>> file and the command could copy the box/tiff pairs to the tmp training
>> directory.
>> but there was another error as following:
>>
>> $ training/tesstrain.sh --lang eng --linedata_only --langdata_dir
>> ../langdata --tessdata_dir ./tessdata --output_dir ../result
>>
>> === Starting training for language 'eng'
>> total 6068
>> -rwxrw-r-- 1 penny penny 66188 Oct 18 19:24 eng.num.exp0.box
>> -rwxrw-r-- 1 penny penny 6136385 Oct 18 19:24 eng.num.exp0.tif
>> -rw-rw-r-- 1 penny penny 42 Oct 18 19:24 tesstrain.log
>> [Wed Oct 18 19:24:13 CST 2017] /usr/local/bin/text2image
>> --fonts_dir=/usr/share/fonts/ --font=Arial Bold
>> --outputbase=/tmp/font_tmp.fGYz2L7fuF/sample_text.txt
>> --text=/tmp/font_tmp.fGYz2L7fuF/sample_text.txt
>> --fontconfig_tmpdir=/tmp/font_tmp.fGYz2L7fuF
>> Could not find font named Arial Bold.
>> Pango suggested font FreeSerif Bold.
>> Please correct --font arg.
>>
>> === Phase I: Generating training images ===
>> ERROR: Could not find training text file ../langdata/eng/eng.training_t
>> ext
>>
>> it couldn't find 'eng.training_text' file, do you know what the
>> 'eng.training_text' file is ?
>>
>> Additionally, the command 'text2image' is used to generate tiff images
>> according to text and fonts, but we have already had box/tiff pairs, so why
>> it excute the command 'text2image' again ?
>> Should I change other script files to resolve these problems?
>>
>> I'm looking forward to your reply! Thanks a lot!
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/1172#issuecomment-337563901>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AE2_owYariVrUfRKKivM0kFTpiBC9Ypeks5steRBgaJpZM4P9XT0>
>> .
>>
>
 Please look at your tif files in a viewer. Why do they have 0 dpi?

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Oct 23, 2017 at 11:47 AM, 694376965 <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> but now there is a new
> problem like this:
>
> $ tesseract eng.num.exp1.tif eng.num.exp1 lstm.train
> Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
> Page 1
> Warning. Invalid resolution 0 dpi. Using 70 instead.
> Estimating resolution as 395
> Empty page!!
> Estimating resolution as 395
> Empty page!!
>
> there are several images will generate this error, do you know why it
> appears "Empty page!!"? Have you ever seen this kind of mistake?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1172#issuecomment-338557979>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_ox27vOl9R1j2mq_qQFP1eYELKdzJks5svC-IgaJpZM4P9XT0>
> .
>
 Great! Thanks for informing what worked. Please look at the config files named
makebox
nobatch
etc

in the
tessdata/configs
tessdata/tessconfigs directory.

They will show what config variables are being set for each.

I may have used the nobatch option with tesseract 3.02 or so - do not
remember details.


ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Oct 24, 2017 at 4:26 PM, 694376965 <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii>, You're welcome! the command
> is also right without "nobatch", just as the following:
>
> $ tesseract eng.num.exp2.tif eng.num.exp2 -psm 7 lstm.train
> Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
> Page 1
> Warning. Invalid resolution 0 dpi. Using 70 instead.
>
> Dou you know the function of "nobatch" ? Have you used it in tesseract
> command ?
> Looking forward to your reply! Thanks!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1172#issuecomment-338952527>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o4aAzjGxbIbaIbIXk4vRtCkQIH8dks5svcJBgaJpZM4P9XT0>
> .
>
 @Shreeshrii At [Making Box Files 4.0](https://github.com/tesseract-ocr/tesseract/wiki/Making-Box-Files---4.0) there's the following :
<pre>
The required format for LSTM 4.0alpha is still the tiff/box file pair, except that the boxes only need to cover a textline instead of individual characters.
 'Newline' boxes with tab as the character must be inserted between textlines to indicate the end-of-line.
</pre>

what's the meaning of the last line? Does it mean that we should add a 'tab' after each textline? 

 If you use tesstrain.sh, box/tiff pairs are created in correct format.

The textline based box files (WordStr ...) are NOT supported.

If you are modifying old 3.ox format box files, you have to add space after each word and tab after each textline. 

However, please note that the box files generated using tesseract with `makebox` need to be manually edited for accuracy (since the boxes are filled with OCRed text). Also, I found that for Devanagari (and probably for other complex scripts), the box generation may not match what is generated by `text2image`.

You can do a simple test. Create a box/tiff pair using 'text2image'. Then create a box file for that same tif using tesseract with makebox. Compare the two box files. @Shreeshrii Could you introduce the role of each file under langdata? 
<pre>
[root@localhost langdata]# ls chi_sim
chi_sim.config   chi_sim.punc           chi_sim.training_text.bigram_freqs   chi_sim.unicharambigs  desired_characters
chi_sim.numbers  chi_sim.training_text  chi_sim.training_text.unigram_freqs  chi_sim.wordlist       forbidden_characters
</pre>
I found that the ```.unigram_freqs``` and ```.bigram_freqs``` files seems not suit me very well. Please see https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract-%E2%80%93-tesstrain.sh

My guess is that `.unigram_freqs`,  `.bigram_freqs`, `desired_characters` and `forbidden_characters` are used at Google for building a representative training_text for doing training from scratch. 

They are not used directly in the training process documented publicly by Ray. Also see https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#combining-the-output-files

> NOTE Tesseract 4.00 will now run happily with a traineddata file that contains just lang.lstm, lang.lstm-unicharset and lang.lstm-recoder. The lstm-*-dawgs are optional, and none of the other components are required or used with OEM_LSTM_ONLY as the OCR engine mode. No bigrams, unichar ambigs or any of the other components are needed or even have any effect if present. The only other component that does anything is the lang.config, which can affect layout analysis, and sub-languages. At [Langdata files](https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract-%E2%80%93-tesstrain.sh), It has the following:

**training_text.unigram_freqs**
<pre>
This is a text file with a list of unigrams (characters) and the frequency with which they appear next to each other in the training_text, one unigram per line.
</pre>

It seems that not the real frequency in training_text and I just confused about that. I have download the Chinese corpora from a National institution and I think it has a Higher precision. so I want to generate my own langdata of chi_sim.  But here the training_text seems to have some relationships with the other files(```.unigram_freqs```, ```.bigram_freqs```)

 You should search for past issues and in forum first.

See https://github.com/tesseract-ocr/tesseract/issues/1114 Sorry, I don't have any suggestions to try.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Nov 14, 2017 at 5:25 PM, zhaiyongding <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii>
> --sequential_training true
> then i got
> First document cannot be empty!!
> num_pages_per_doc_ > 0:Error:Assert failed:in file imagedata.cpp, line 658
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1172#issuecomment-344236472>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oxgq8x4iPB6FirpM338V5EzTMDsqks5s2X-rgaJpZM4P9XT0>
> .
>
  Use traineddata files from tessdata_fast repository  for speed in
recognition.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Oct 17, 2017 at 9:00 PM, Amit D. <notifications@github.com> wrote:

> Also, do you use the newest traineddata?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1171#issuecomment-337266737>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o_BlcoI0mWe0dClpPN5puFlZejciks5stMgKgaJpZM4P8Rw1>
> .
>
 Please see https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#lstmtraining-command-line

If you have the data for your finetuning, you can create the 'faster' integer type of traineddata by using 
`convert_to_int` with `stop_training`. For training, you have to start with tessdata_best models. You can create
your traineddata in the integer faster format.

You will have to test with your language and data.

On 18-Oct-2017 7:28 PM, "ibr123" <notifications@github.com> wrote:

> if i wanted to fine tune using the tool
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1171#issuecomment-337600169>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oyx5CIz_10_spwJn3BbM-AvfinFUks5stgQXgaJpZM4P8Rw1>
> .
>
 You can give it a try. There have been significant changes, that break
compatibility between commits since this is development code in alpha stage.
If you get an error, you will have to recreate the lstmf files.

On 18-Oct-2017 7:34 PM, "ibr123" <notifications@github.com> wrote:

> if i wanted to fine tune using the tool "lstmtraining" while i'm using the
> latest Tesseract: (4.00.00dev-690-g1b0379c) can i use .lstmf files (which
> are generated by tesstrain.sh)file that are created by older Tesseract
> version, such as (4.00.00dev-549-g2b854e3) ?
> meaning are lstmf files compatible between tesseract versions?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1171#issuecomment-337601903>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o6UXEXXc9MEveLBjrtgdNMWPYbLNks5stgVbgaJpZM4P8Rw1>
> .
>
 I do not know about the specific commit numbers you refer to. You may want
to check the github history of commits.

On 18-Oct-2017 7:39 PM, "ShreeDevi Kumar" <shreeshrii@gmail.com> wrote:

> You can give it a try. There have been significant changes, that break
> compatibility between commits since this is development code in alpha stage.
> If you get an error, you will have to recreate the lstmf files.
>
> On 18-Oct-2017 7:34 PM, "ibr123" <notifications@github.com> wrote:
>
>> if i wanted to fine tune using the tool "lstmtraining" while i'm using
>> the latest Tesseract: (4.00.00dev-690-g1b0379c) can i use .lstmf files
>> (which are generated by tesstrain.sh)file that are created by older
>> Tesseract version, such as (4.00.00dev-549-g2b854e3) ?
>> meaning are lstmf files compatible between tesseract versions?
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/1171#issuecomment-337601903>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AE2_o6UXEXXc9MEveLBjrtgdNMWPYbLNks5stgVbgaJpZM4P8Rw1>
>> .
>>
>
  
### Environment

* **Tesseract Version**: tesseract 4.00.00alpha
* **Commit Number**: 2cc531e
* **Platform**: Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

### Current Behavior:
I use the following command to train:
<pre>
[root@localhost tesseract]# training/lstmtraining --debug_interval 100 \
>   --traineddata ../tesstutorial/chi_simtrain/chi_sim/chi_sim.traineddata \
>   --net_spec '[1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lfx512 O1c111]' \
>   --model_output ../tesstutorial/chi_simoutput/base --learning_rate 20e-4 \
>   --train_listfile ../tesstutorial/chi_simtrain/chi_sim.training_files.txt \
>   --eval_listfile ../tesstutorial/chi_simeval/chi_sim.training_files.txt \
>   --max_iterations 10000 &>../tesstutorial/chi_simoutput/basetrain.log
Segmentation fault (core dumped)
</pre>
But it caught segmentation fault.

when training I use the following command to see the log:
<pre>
# tail -f ../tesstutorial/chi_simoutput/basetrain.log
....
Iteration 9998: ALIGNED TRUTH : 恽寿榕印刷width打垮年推荐评论Microsoft 邵武的月经沉,特性产权 巫山
Iteration 9998: BEST OCR TEXT : 
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.STFangsong.exp0.lstmf page 188 :
Mean rms=4.264%, delta=50.362%, train=99.57%(99.92%), skip ratio=0%
Iteration 9999: ALIGNED TRUTH : 》杂耍啤酒花href镁天竺葵垮塌帐篷 藕腱鞘炎 岷 关专业祭副4About 下挫
Iteration 9999: BEST OCR TEXT :  
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.STKaiti.exp0.lstmf page 91 :
Mean rms=4.264%, delta=50.364%, train=99.569%(99.92%), skip ratio=0%
2 Percent improvement time=10000, best error was 100 @ 0
At iteration 10000/10000/10000, Mean rms=4.264%, delta=50.364%, char train=99.569%, word train=99.92%, skip ratio=0%,  New best char error = 99.569 wrote checkpoint.

Finished! Error rate = 99.569
num_docs > 0:Error:Assert failed:in file imagedata.cpp, line 651
Exception in thread "main" java.lang.NullPointerException
        at com.google.scrollview.ui.SVWindow.drawImage(SVWindow.java:336)
        at com.google.scrollview.ScrollView.processInput(ScrollView.java:321)
        at com.google.scrollview.ScrollView.IOLoop(ScrollView.java:120)
        at com.google.scrollview.ScrollView.main(ScrollView.java:398)
</pre>

From the basetrain.log, It seems that the training iteration has reached 10000 and has finished the training. And from tesseract-master/training/lstmtraining.cpp:
<pre>
int main(int argc, char **argv) {
....

do {
    // Train a few.
    int iteration = trainer.training_iteration();
    for (int target_iteration = iteration + kNumPagesPerBatch;
         iteration &lt; target_iteration;
         iteration = trainer.training_iteration()) {
      trainer.TrainOnLine(&trainer, false);
    }
    STRING log_str;
    trainer.MaintainCheckpoints(tester_callback, &log_str);
    tprintf("%s\n", log_str.string());
  } while (trainer.best_error_rate() &gt; FLAGS_target_error_rate &&
           (trainer.training_iteration() &lt; FLAGS_max_iterations ||
            FLAGS_max_iterations == 0));
  delete tester_callback;
  tprintf("Finished! Error rate = %g\n", trainer.best_error_rate());
  return 0;
}
</pre>
It seems that it has already finished. So which process are executing the lstmtraining.cpp:
<pre>
const ImageData* DocumentCache::GetPageSequential(int serial) {
  int num_docs = documents_.size();
  ASSERT_HOST(num_docs > 0);

....
}
</pre>
and here the assert caused the segmentation fault? 

Here I checked the output dir:
<pre>
[root@localhost tesseract]# ls ../tesstutorial/chi_simoutput/ 
base_checkpoint  basetrain.log
</pre>
It has generated the base_checkpoint. So are there any following steps that the training process should do? And It seems that the segmentation fault doesn't effect much.


### Expected Behavior:

### Suggested Fix:
 Are you trying to train from scratch?

10000 iterations are too low for that.

You have an error rate of 99%.

Did you try replace a layer for training?

On 12-Oct-2017 1:59 PM, "ivanzz1001" <notifications@github.com> wrote:

> Environment
>
>    - *Tesseract Version*: tesseract 4.00.00alpha
>    - *Commit Number*: 2cc531e
>    <https://github.com/tesseract-ocr/tesseract/commit/2cc531e6bf0288fc8a9ad1c123a252395f00bf56>
>    - *Platform*: Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1
>    <https://github.com/tesseract-ocr/tesseract/issues/1> SMP Tue Nov 22
>    16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
>
> Current Behavior:
>
> I use the following command to train:
>
> [root@localhost tesseract]# training/lstmtraining --debug_interval 100 \
> >   --traineddata ../tesstutorial/chi_simtrain/chi_sim/chi_sim.traineddata \
> >   --net_spec '[1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lfx512 O1c111]' \
> >   --model_output ../tesstutorial/chi_simoutput/base --learning_rate 20e-4 \
> >   --train_listfile ../tesstutorial/chi_simtrain/chi_sim.training_files.txt \
> >   --eval_listfile ../tesstutorial/chi_simeval/chi_sim.training_files.txt \
> >   --max_iterations 10000 &>../tesstutorial/chi_simoutput/basetrain.log
> Segmentation fault (core dumped)
>
> But it caught segmentation fault.
>
> when training I use the following command to see the log:
>
> # tail -f ../tesstutorial/chi_simoutput/basetrain.log
> ....
> Iteration 9998: ALIGNED TRUTH : 恽寿榕印刷width打垮年推荐评论Microsoft 邵武的月经沉,特性产权 巫山
> Iteration 9998: BEST OCR TEXT :
> File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.STFangsong.exp0.lstmf page 188 :
> Mean rms=4.264%, delta=50.362%, train=99.57%(99.92%), skip ratio=0%
> Iteration 9999: ALIGNED TRUTH : 》杂耍啤酒花href镁天竺葵垮塌帐篷 藕腱鞘炎 岷 关专业祭副4About 下挫
> Iteration 9999: BEST OCR TEXT :
> File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.STKaiti.exp0.lstmf page 91 :
> Mean rms=4.264%, delta=50.364%, train=99.569%(99.92%), skip ratio=0%
> 2 Percent improvement time=10000, best error was 100 @ 0
> At iteration 10000/10000/10000, Mean rms=4.264%, delta=50.364%, char train=99.569%, word train=99.92%, skip ratio=0%,  New best char error = 99.569 wrote checkpoint.
>
> Finished! Error rate = 99.569
> num_docs > 0:Error:Assert failed:in file imagedata.cpp, line 651
> Exception in thread "main" java.lang.NullPointerException
>         at com.google.scrollview.ui.SVWindow.drawImage(SVWindow.java:336)
>         at com.google.scrollview.ScrollView.processInput(ScrollView.java:321)
>         at com.google.scrollview.ScrollView.IOLoop(ScrollView.java:120)
>         at com.google.scrollview.ScrollView.main(ScrollView.java:398)
>
> From the basetrain.log, It seems that the training iteration has reached
> 10000 and has finished the training. And from tesseract-master/training/
> lstmtraining.cpp:
>
> int main(int argc, char **argv) {
> ....
>
> do {
>     // Train a few.
>     int iteration = trainer.training_iteration();
>     for (int target_iteration = iteration + kNumPagesPerBatch;
>          iteration < target_iteration;
>          iteration = trainer.training_iteration()) {
>       trainer.TrainOnLine(&trainer, false);
>     }
>     STRING log_str;
>     trainer.MaintainCheckpoints(tester_callback, &log_str);
>     tprintf("%s\n", log_str.string());
>   } while (trainer.best_error_rate() > FLAGS_target_error_rate &&
>            (trainer.training_iteration() < FLAGS_max_iterations ||
>             FLAGS_max_iterations == 0));
>   delete tester_callback;
>   tprintf("Finished! Error rate = %g\n", trainer.best_error_rate());
>   return 0;
> }
>
> It seems that it has already finished. So which process are executing the
> lstmtraining.cpp:
>
> const ImageData* DocumentCache::GetPageSequential(int serial) {
>   int num_docs = documents_.size();
>   ASSERT_HOST(num_docs > 0);
>
> ....
> }
>
> and here the assert caused the segmentation fault?
>
> Here I checked the output dir:
>
> [root@localhost tesseract]# ls ../tesstutorial/chi_simoutput/
> base_checkpoint  basetrain.log
>
> It has generated the base_checkpoint. So are there any following steps
> that the training process should do? And It seems that the segmentation
> fault doesn't effect much.
> Expected Behavior: Suggested Fix:
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1168>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o_fNFVynM6oe5Jg1qeCp6QmnCQKzks5src3igaJpZM4P2pPR>
> .
>
 Yes, I am training from scratch. But I don't know why error rate is 99% You need to train for days and many iterations, with very large training
texts to train from scratch.

However, the crash should not happen.

On 12-Oct-2017 2:22 PM, "ivanzz1001" <notifications@github.com> wrote:

> Yes, I am training from scratch. But I don't know why error rate is 99%
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1168#issuecomment-336064558>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_owSnbXLovzbicpRV9zCuR8myDbYDks5srdNdgaJpZM4P2pPR>
> .
>
 @Shreeshrii The example give the iteration to be 5000, here I set it to 10000? Is it too small? Have you use the tesseract4.0alpha to train anything? you can set the  "--max_iteration" to a very small value and check whether it causes the segmentation fault.

And I check the my traineddata, It  really has a very low correct rate @Shreeshrii  And In the basetrain.log,  it prints many "ALIGNED TRUTH"
<pre>
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.Arial_Unicode_MS_Bold.exp0.lstmf page 735 :
Mean rms=4.291%, delta=50.688%, train=99.996%(100%), skip ratio=0%
Iteration 5251: ALIGNED TRUTH : 尧 地区瑶族取暖调节软件 日问题遐想二陌77 1997及学会呵呵心态 85 for 的
Iteration 5251: BEST OCR TEXT :  
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.Arial_Unicode_MS.exp0.lstmf page 587 :
Mean rms=4.29%, delta=50.671%, train=99.995%(100%), skip ratio=0%
Iteration 5252: ALIGNED TRUTH : 鉴定关闭年激烈法规制药中轶读 五芍广场 皿 投资Qzone 霸62791813访问胫
Iteration 5252: BEST OCR TEXT : 
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.AR_PL_UKai_CN.exp0.lstmf page 9 :
Mean rms=4.29%, delta=50.665%, train=99.995%(100%), skip ratio=0%
Iteration 5253: ALIGNED TRUTH : 基金凋设 . 售后他Gzip 陈镒康 网站这里 1996 崖违规相信咕咚慈善日本
Iteration 5253: BEST OCR TEXT : 
</pre>

And in the [training tutorial -- error msg](https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#error-messages-from-training), It seems that it's a problem If you try training with debug level -1 and see the output on console
rather than sending to log file, you will see that for each line of
training text the ocred output and aligned truth will be displayed.

On 12-Oct-2017 3:04 PM, "ivanzz1001" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> And In the basetrain.log, it
> prints many "ALIGNED TRUTH"
>
> File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.Arial_Unicode_MS_Bold.exp0.lstmf page 735 :
> Mean rms=4.291%, delta=50.688%, train=99.996%(100%), skip ratio=0%
> Iteration 5251: ALIGNED TRUTH : 尧 地区瑶族取暖调节软件 日问题遐想二陌77 1997及学会呵呵心态 85 for 的
> Iteration 5251: BEST OCR TEXT :
> File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.Arial_Unicode_MS.exp0.lstmf page 587 :
> Mean rms=4.29%, delta=50.671%, train=99.995%(100%), skip ratio=0%
> Iteration 5252: ALIGNED TRUTH : 鉴定关闭年激烈法规制药中轶读 五芍广场 皿 投资Qzone 霸62791813访问胫
> Iteration 5252: BEST OCR TEXT :
> File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.AR_PL_UKai_CN.exp0.lstmf page 9 :
> Mean rms=4.29%, delta=50.665%, train=99.995%(100%), skip ratio=0%
> Iteration 5253: ALIGNED TRUTH : 基金凋设 . 售后他Gzip 陈镒康 网站这里 1996 崖违规相信咕咚慈善日本
> Iteration 5253: BEST OCR TEXT :
>
> And in the training tutorial -- error msg
> <https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#error-messages-from-training>,
> It seems that it's a problem
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1168#issuecomment-336075160>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o7LVJX9aCe_9vsMVuj9cex-G_E6hks5srd0RgaJpZM4P2pPR>
> .
>
 why the "BEST OCR TEXT " is empty? I have modified the langdata/chi_sim/chi_sim.training_text , I don't know whether this is the reason? And "--max_iteration" set to 10000 is too small?

And I use the "--debug_level -1", It still has the following(here for test I set max_iteration to 100):
<pre>
[root@localhost tesseract]# training/lstmtraining --debug_interval 100 \
>   --traineddata ../tesstutorial/chi_simtrain/chi_sim/chi_sim.traineddata \
>   --net_spec '[1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lfx512 O1c111]' \
>   --model_output ../tesstutorial/chi_simoutput/base --learning_rate 20e-4 \
>   --train_listfile ../tesstutorial/chi_simtrain/chi_sim.training_files.txt \
>   --eval_listfile ../tesstutorial/chi_simeval/chi_sim.training_files.txt \
>   --max_iterations 100 \
>   --debug_level -1

....
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.NSimSun.exp0.lstmf page 157 :
Mean rms=4.377%, delta=51.759%, train=100.474%(100%), skip ratio=0%
Iteration 196: ALIGNED TRUTH : 提供) 月份追究 发射特性16 中标图片皇甫男孩责任 桂纶镁业环他《迅猛
Iteration 196: BEST OCR TEXT : 
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.SimHei.exp0.lstmf page 841 :
Mean rms=4.378%, delta=51.788%, train=100.471%(100%), skip ratio=0%
Iteration 197: ALIGNED TRUTH : 社会号码 价瀑布 佣金餐厅 最12正在24地play 32 市短name/ , 天极首席美国2005
Iteration 197: BEST OCR TEXT : 
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.SimSun.exp0.lstmf page 159 :
Mean rms=4.377%, delta=51.732%, train=100.469%(100%), skip ratio=0%
Iteration 198: ALIGNED TRUTH : 提取、 章常规侮聊天觊 洞府生姜限 乳腺事 方11蕃时间索尼]菌文史蠹
Iteration 198: BEST OCR TEXT : 
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.STFangsong.exp0.lstmf page 119 :
Mean rms=4.378%, delta=51.763%, train=100.467%(100%), skip ratio=0%
Iteration 199: ALIGNED TRUTH : 通告 5.5烤漆学院罗技烧烤 ;.追溯评测 COM 颐达的炫耀宗旨睛逐渐图书市场
Iteration 199: BEST OCR TEXT : 
File /tmp/tmp.79zY0ByWtx/chi_sim/chi_sim.STKaiti.exp0.lstmf page 334 :
Mean rms=4.379%, delta=51.785%, train=100.464%(100%), skip ratio=0%
At iteration 200/200/200, Mean rms=4.379%, delta=51.785%, char train=100.464%, word train=100%, skip ratio=0%,  New worst char error = 100.464 wrote checkpoint.

Finished! Error rate = 100
</pre> @Shreeshrii  Hi, I use the "training/tesstrain.sh" to generate the training data and eval data; Should I use the following command to generate lstm-recoder([Creating Starter Traineddata](https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#creating-starter-traineddata))?
<pre>
# training/combine_lang_model --input_unicharset ../tesstutorial/chi_simtrain/chi_sim/chi_sim.unicharset \
 --script_dir ../langdata \
 --words ../langdata/chi_sim/chi_sim.wordlist \
 --numbers ../langdata/chi_sim/chi_sim.numbers \
 --puncs ../langdata/chi_sim/chi_sim.punc \
 --output_dir ../tesstutorial/chi_simtrain \
 --lang chi_sim \
 --version_str "4.0.0alpha chi_sim"
</pre>
I greped the "combine_lang_model" and found it was called by "tesstrain_utils.sh", and "tesstrain_utils.sh"  is called by "training/tesstrain.sh", so I had skipped the step。Here I checked "ALIGNED TRUTH" problem, I think this may be the reason. And If It needs this step,  should I generate it for both training data and eval data?   When you start training from scratch, your error rate will be 100% and best
ocr text will be empty. 

For training from scratch Ray is using 400,000+ lines of text and probably
millions of iterations.

I would suggest you try to replace the top layer.

ShreeDevi
 @Shreeshrii I retried the training:
<pre>
training/lstmtraining --debug_interval 100 \
>   --traineddata ../tesstutorial/chi_simtrain/chi_sim/chi_sim.traineddata \
>   --net_spec '[1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lfx512 O1c111]' \
>   --model_output ../tesstutorial/chi_simoutput/base --learning_rate 20e-4 \
>   --train_listfile ../tesstutorial/chi_simtrain/chi_sim.training_files.txt \
>   --eval_listfile ../tesstutorial/chi_simeval/chi_sim.training_files.txt \
>   --max_iterations 10000 &>../tesstutorial/chi_simoutput/basetrain.log
</pre>
But It cause :
<pre>
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
</pre>
I think it is because the capacity of my vmware-centos's disk  is not enough, so I want to do the training work on a special server, but I found that the command can't be executed on remote. It must first login the centos , then open the terminal and thus you can execute the above command. If execute on remote, it may cause "broken pipe" error and quickly quit (the training process may need to communicate with other windows, eg CTC Output window) . How do I solve the problem?  
 Sorry, I do not know enough this to comment. @stweil @amitdo might be able to guide you. @Shreeshrii sorry, maybe I haven't described it very clear. This is the log:
<pre>
[root@localhost tesseract]# cat ../tesstutorial/chi_simoutput/basetrain.log 
Warning: given outputs 111 not equal to unicharset of 229.
Num outputs,weights in Series:
  1,48,0,1:1, 0
Num outputs,weights in Series:
  C3,3:9, 0
  Ft16:16, 160
Total weights = 160
  [C3,3Ft16]:16, 160
  Mp3,3:16, 0
  Lfys64:64, 20736
  Lfx96:96, 61824
  Lrx96:96, 74112
  Lfx512:512, 1247232
  Fc229:229, 117477
Total weights = 1521541
Built network:[1,48,0,1[C3,3Ft16]Mp3,3Lfys64Lfx96Lrx96Lfx512Fc229] from request [1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lfx512 O1c111]
Training parameters:
  Debug interval = 100, weights = 0.1, learning rate = 0.002, momentum=0.5
null char=228
Loaded 2153/2153 pages (1-2153) of document ../tesstutorial/chi_simtrain/chi_sim.Arial_Unicode_MS_Bold.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UKai_CN.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simtrain/chi_sim.Arial_Unicode_MS.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UKai_HK.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simeval/chi_sim.Arial_Unicode_MS.exp0.lstmf
Loaded 2153/2153 pages (1-2153) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UMing_TW_MBE_Semi-Light.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UKai_TW.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UKai_TW_MBE.exp0.lstmf
Loaded 2153/2153 pages (1-2153) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UMing_CN_Semi-Light.exp0.lstmf
Loaded 2153/2153 pages (1-2153) of document ../tesstutorial/chi_simtrain/chi_sim.AR_PL_UMing_HK_Semi-Light.exp0.lstmf
Loaded 2152/2152 pages (1-2152) of document ../tesstutorial/chi_simeval/chi_sim.AR_PL_UKai_CN.exp0.lstmf
Starting sh -c "trap 'kill %1' 0 1 2 ; java -Xms1024m -Xmx2048m -jar /root/tesseract-src/tesseract/java/ScrollView.jar & wait"
Socket started on port 8461
Created window Convolve of size 1511, 580
Client connected
Created window ConvNL of size 1310, 848
Exception in thread "main" java.awt.HeadlessException: 
No X11 DISPLAY variable was set, but this program performed an operation which requires it.
        at java.awt.GraphicsEnvironment.checkHeadless(GraphicsEnvironment.java:204)
        at java.awt.Window.<init>(Window.java:536)
        at java.awt.Frame.<init>(Frame.java:420)
        at javax.swing.JFrame.<init>(JFrame.java:233)
        at com.google.scrollview.ui.SVWindow.<init>(SVWindow.java:204)
        at com.google.scrollview.ScrollView.processInput(ScrollView.java:340)
        at com.google.scrollview.ScrollView.IOLoop(ScrollView.java:120)
        at com.google.scrollview.ScrollView.main(ScrollView.java:398)
sh: line 1: kill: %1: no such job
Created window Lfys64 of size 530, 2000
</pre>
It is because when you execute the command on remote, it can't open the related window. I do not use scrollview. I either use debug level -1 or 0.

I do not know much about Linux or C++ or tesseract. I test tesseract for
OCR of Indian languages, mainly devanagari for sanskrit.

So, I can only suggest options if it is something I have encountered while
training/testing.

On 20-Oct-2017 8:50 AM, "ivanzz1001" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> sorry, maybe I haven't
> described it very clear. This is the log:
>
> Starting sh -c "trap 'kill %1' 0 1 2 ; java -Xms1024m -Xmx2048m -jar /root/tesseract-src/tesseract/java/ScrollView.jar & wait"
> Socket started on port 8461
> Created window Convolve of size 1511, 580
> Client connected
> Created window ConvNL of size 1310, 848
> Exception in thread "main" java.awt.HeadlessException:
> No X11 DISPLAY variable was set, but this program performed an operation which requires it.
>         at java.awt.GraphicsEnvironment.checkHeadless(GraphicsEnvironment.java:204)
>         at java.awt.Window.(Window.java:536)
>         at java.awt.Frame.(Frame.java:420)
>         at javax.swing.JFrame.(JFrame.java:233)
>         at com.google.scrollview.ui.SVWindow.(SVWindow.java:204)
>         at com.google.scrollview.ScrollView.processInput(ScrollView.java:340)
>         at com.google.scrollview.ScrollView.IOLoop(ScrollView.java:120)
>         at com.google.scrollview.ScrollView.main(ScrollView.java:398)
> sh: line 1: kill: %1: no such job
> Created window Lfys64 of size 530, 2000
>
> It is because when you execute the command on remote, it can't open the
> related window.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1168#issuecomment-338095444>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oxfm4diPRjtaSyW2cBG_e81tXRasks5suBGBgaJpZM4P2pPR>
> .
>
  For Arabic, you will get better results using tesseract 4.0alpha.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Oct 9, 2017 at 10:26 PM, idrisalshikh <notifications@github.com>
wrote:

> I have the same problem with arabic language
> ** Run Tesseract for Training **
> [K:\train tesseract\jTessBoxEditor\tesseract-ocr/tesseract,
> ara.mylotus.exp0.tif, ara.mylotus.exp0, box.train]
> Tesseract Open Source OCR Engine v4.0.0-alpha.20170804 with Leptonica
> Page 1
> row xheight=23, but median xheight = 30.5
> APPLY_BOXES: boxfile line 6/ق ((2324,3143),(2338,3173)): FAILURE! Couldn't
> find a matching blob
> APPLY_BOXES: boxfile line 7/ع ((2303,3119),(2334,3157)): FAILURE! Couldn't
> find a matching blob
> ....
> ..
> .
> .
>
> APPLY_BOXES:
> Boxes read from boxfile: 888
> Boxes failed resegmentation: 176
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1166#issuecomment-335217245>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o7ccW2V5QU-yptOQIhVH4AJ0NLwRks5sqlA9gaJpZM4Pys0D>
> .
>
 Same issue as

 https://github.com/tesseract-ocr/tesseract/issues/436
https://github.com/tesseract-ocr/tesseract/issues/445
https://github.com/tesseract-ocr/tesseract/issues/1033

 These errors have existed for a long time. I think it is a problem with how tesseract segments the page and finds lines. If you only have a couple of these errors, I would say to ignore them and proceed to next step.   I remember seeing that if tesseract can't recognize image, it tries to rotate it to see if it gets better results. That could be causing the rotation in your case. https://github.com/tesseract-ocr/tesseract/blob/a1c22fb0d0f6bde165ec7b7c3125420b0ba1d541/textord/colfind.cpp 

maybe relevant.  please read comment related to "string" in issue tracker (and maybe tesseract forums)...  > In general, unless you compile on Windows with MSVC, you should use autotools to build Leptonica and Tesseract.

True.
But in general, PR is ok.  3.02 version is old. Very old. Not supported. Please upgrade.  https://digi.bib.uni-mannheim.de/tesseract/

see link for windows binaries


see wiki for details

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, Sep 30, 2017 at 11:39 AM, zdenop <notifications@github.com> wrote:

> Closed #1160 <https://github.com/tesseract-ocr/tesseract/issues/1160>.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1160#event-1272652932>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o7_smnTfnLKfzFLO7UF-EyN3nQhTks5sndsYgaJpZM4Ppbm8>
> .
>
  1. Are you using the latest code from github? If not, which commit?

2. Finetune training should not last this long. Ray recommends iterations of 3000/3600 as suggested in the tutorial.

3. How large are your training texts and number of fonts?

4. Are you

* Fine Tuning for Impact(new-font-style)
* Fine Tuning for ± a few characters

Update: Just noticed in your earlier post:

```
I'm trying to fine tune chi_sim.traineddata with a new font ,
STXihei
``` from the log file

```

Layer 2=ConvNL: lr 1.25e-05->-nan%, lr 1.76777e-05->-nan% SAME
Layer 4=Lfys64: lr 1.25e-05->-nan%, lr 1.76777e-05->-nan% SAME
Layer 5=Lfx128: lr 1.25e-05->-nan%, lr 1.76777e-05->-nan% SAME
Layer 6=Lrx128: lr 1.25e-05->-nan%, lr 1.76777e-05->-nan% SAME
Layer 7=Lfx384: lr 8.83883e-06->-nan%, lr 1.25e-05->-nan% SAME
Layer 8=Output: lr 3.53553e-05->-nan%, lr 5e-05->-nan% SAME
At iteration 1096/1100/1100, Mean rms=6.803%, delta=190.265%, char train=290.425%, word train=99.863%, skip ratio=0%,  New worst char error = 290.425
Divergence! Reverted to iteration 96/100/100
Reduced learning rate on layers: 6
 wrote checkpoint.
```

I think finetune is the wrong method for what you want. Try to replace a layer. Try with latest commit and the patch from https://github.com/tesseract-ocr/tesseract/pull/1153

Use the traineddata files from tessdata_best repo for continuing from

----------------------------

BTW, have you tested OCR with the traineddata from https://github.com/tesseract-ocr/tessdata_best? Please also try OCR with HanS Try the command for plus-minus finetune (with the latest code and best traineddata). 

I am getting following console output.

```
 nice lstmtraining \
>   --continue_from ../tessdata_best/chi_sim.lstm \
>   --old_traineddata ../tessdata_best/chi_sim.traineddata  \
>   --traineddata ../tesstutorial/chi_sim/chi_sim/chi_sim.traineddata  \
>   --train_listfile ../tesstutorial/chi_sim/chi_sim.training_files.txt \
>   --eval_listfile ../tesstutorial/chi_sim/chi_sim.eval_files.txt \
>   --model_output ../tesstutorial/chi_sim_new/chi_sim_new \
>   --max_iterations 6000 \
>   --debug_interval -1
Loaded file ../tessdata_best/chi_sim.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Code range changed from 224 to 227!
Num (Extended) outputs,weights in Series:
  1,48,0,1:1, 0
Num (Extended) outputs,weights in Series:
  C3,3:9, 0
  Ft16:16, 160
Total weights = 160
  [C3,3Ft16]:16, 160
  Mp3,3:16, 0
  Lfys64:64, 20736
  Lfx96:96, 61824
  Lrx96:96, 74112
  Lfx512:512, 1247232
  Fc227:227, 116451
Total weights = 1520515
Previous null char=223 mapped to 226
Continuing from ../tessdata_best/chi_sim.lstm
Loaded 1752/1752 pages (1-1752) of document ../tesstutorial/chi_sim/chi_sim.STXihei.exp0.lstmf
Loaded 1752/1752 pages (1-1752) of document ../tesstutorial/chi_sim/chi_sim.STXihei.exp0.lstmf
Iteration 0: ALIGNED TRUTH : 秘诀鲤鱼 愚蠢翠微注册University 探究峰敬未台妊 原则余林汝锑经理 XP
Iteration 0: BEST OCR TEXT : 秘诀鲤鱼 思蠢翠微注册University 探究峰获未人妊 原则作林注锐经理 XP
File /tmp/tmp.nZYkgdXY7D/chi_sim/chi_sim.STXihei.exp0.lstmf page 57 :
Mean rms=1.504%, delta=7.94%, train=17.526%(60%), skip ratio=0%
Iteration 1: ALIGNED TRUTH : 致 蒸汽文坛 GPS安置伟大保密23 周康黄芪崃 。 从事窗帘公寓鲟橙色熙
Iteration 1: BEST OCR TEXT : 致 蒸汽文坛 GPS安置伟大保密23 周康商茎峡 。 从事窗帘公寓乌橙色辕
File /tmp/tmp.nZYkgdXY7D/chi_sim/chi_sim.STXihei.exp0.lstmf page 296 :
Mean rms=1.528%, delta=7.879%, train=18.659%(55%), skip ratio=0%
```  Are you using an old version of tesseract? 
The correct parameter is `--oem NUM`

I get the following info:

```
tesseract -v
tesseract 4.00.00alpha
 leptonica-1.74.4
  libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8

 Found AVX
 Found SSE

```

```
 tesseract
Usage:
  tesseract --help | --help-psm | --help-oem | --version
  tesseract --list-langs [--tessdata-dir PATH]
  tesseract --print-parameters [options...] [configfile...]
  tesseract imagename|stdin outputbase|stdout [options...] [configfile...]

OCR options:
  --tessdata-dir PATH   Specify the location of tessdata path.
  --user-words PATH     Specify the location of user words file.
  --user-patterns PATH  Specify the location of user patterns file.
  -l LANG[+LANG]        Specify language(s) used for OCR.
  -c VAR=VALUE          Set value for config variables.
                        Multiple -c arguments are allowed.
  --psm NUM             Specify page segmentation mode.
  --oem NUM             Specify OCR Engine mode.
NOTE: These options must occur before any configfile.

Page segmentation modes:
  0    Orientation and script detection (OSD) only.
  1    Automatic page segmentation with OSD.
  2    Automatic page segmentation, but no OSD, or OCR.
  3    Fully automatic page segmentation, but no OSD. (Default)
  4    Assume a single column of text of variable sizes.
  5    Assume a single uniform block of vertically aligned text.
  6    Assume a single uniform block of text.
  7    Treat the image as a single text line.
  8    Treat the image as a single word.
  9    Treat the image as a single word in a circle.
 10    Treat the image as a single character.
 11    Sparse text. Find as much text as possible in no particular order.
 12    Sparse text with OSD.
 13    Raw line. Treat the image as a single text line,
                        bypassing hacks that are Tesseract-specific.
OCR Engine modes:
  0    Original Tesseract only.
  1    Neural nets LSTM only.
  2    Tesseract + LSTM.
  3    Default, based on what is available.

Single options:
  -h, --help            Show this help message.
  --help-psm            Show page segmentation modes.
  --help-oem            Show OCR Engine modes.
  -v, --version         Show version information.
  --list-langs          List available languages for tesseract engine.
  --print-parameters    Print tesseract parameters.
``` You are using an older version which supported -psm and -oem, posted last year in November.

After that there have been hundreds of updates. I am using the most recent commit or so.

The problem is that there is no indicator in version string to suggest the commit number or date so it is very difficult. Please compile using the latest source in the master branch. 

Or use the ppa by Alex.  Another reason for https://github.com/tesseract-ocr/tesseract/issues/293

Please give the URL link of the files which worked and which ones didn't? 

Which --oem are you using with it?  fix the "Phase UP: Generating unicharset and unichar properties files" ERROR

Please reference [#1147](https://github.com/tesseract-ocr/tesseract/issues/1147)  @mskrip Thanks for problem report. I'll take care of it. Just curious, how did you locate this problem and are there any practical consequences, such as an incompatibility with a PDF renderer? I don't know about this, checking in more detail. It's easy enough to put in the BOM, but it messes up highlighting with PDFium. The BOM highlights as if it was a character. 

[before.pdf](https://github.com/tesseract-ocr/tesseract/files/1322978/before.pdf)
[after.pdf](https://github.com/tesseract-ocr/tesseract/files/1322977/after.pdf)
![bad-highlight](https://user-images.githubusercontent.com/4961958/30722445-203f4d3a-9ee6-11e7-8c96-e70a2f32bbb9.png)

```diff
--- tesseract/api/pdfrenderer.cpp	2017-07-14 07:32:13.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp	2017-09-21 15:47:32.000000000 -0700
@@ -476,7 +476,7 @@
           kCharWidth * prec(100.0 * word_length / (fontsize * pdf_word_len));
       pdf_str.add_str_double("", h_stretch);
       pdf_str += " Tz";          // horizontal stretch
-      pdf_str += " [ <";
+      pdf_str += " [ <FEFF";     // byte order marker (BOM)
       pdf_str += pdf_word;       // UTF-16BE representation
       pdf_str += "> ] TJ";       // show the text
     }
```

BEFORE
```
5 0 obj
<< /Length 115 >>
stream
q 39.84 0 0 23.52 0 0 cm /Im1 Do Q
BT
3 Tr -1 0 0 1 30.72 8.12 Tm /f-0-0 8 Tf 168 Tz [ <05D105D005EA05E8> ] TJ 
ET
endstream
endobj
```
AFTER
```
5 0 obj
<< /Length 119 >>
stream
q 39.84 0 0 23.52 0 0 cm /Im1 Do Q
BT
3 Tr -1 0 0 1 30.72 8.12 Tm /f-0-0 8 Tf 168 Tz [ <FEFF05D105D005EA05E8> ] TJ 
ET
endstream
endobj
```

 With the BOM, I get the same highlighting problem in Acroread and with Left-To-Right languages. Also seeing incorrect copy-paste. This is looking wrong to me. Suspect that reporter is misinterpreting PDF specification. I have a vague and ancient memory about how PDF text objects are different, will check on that. In the meantime I can't reproduce the incompatibility with `pdfminer` and I'm not sure how to reproduce with `pdfrw`

```bash
$ sudo apt-get install pdfminer
$ pdf2txt before.pdf
באתר
$ pdf2txt after.pdf
באתר
```
 The 1.7 spec suggests you are right. There's some discussion in there about differences in how text string differs in various spec versions, but I don't think that matters. Perhaps the rendering problem I'm seeing with the BOM is simply due to having an entry for 0xFFEF in the glyphless font.

5.3 Text Objects contain "Text-showing operators"
5.3.2 Operator Tj "Shows a text string"
3.8.1 Text string is "either PDFDocEncoding or UTF-16BE with a leading byte-order marker."


 I still have highlighting and copy-paste problems from the BOM even if I change the mappings.  So at the moment we are stuck.

```diff
--- tesseract/api/pdfrenderer.cpp	2017-07-14 07:32:13.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp	2017-09-25 10:58:39.000000000 -0700
@@ -608,10 +608,10 @@
       "/CMapName /Adobe-Identify-UCS def\n"
       "/CMapType 2 def\n"
       "1 begincodespacerange\n"
-      "<0000> <FFFF>\n"
+      "<0000> <FEFE>\n"
       "endcodespacerange\n"
       "1 beginbfrange\n"
-      "<0000> <FFFF> <0000>\n"
+      "<0000> <FEFE> <0000>\n"
       "endbfrange\n"
       "endcmap\n"
       "CMapName currentdict /CMap defineresource pop\n"
``` Simplifying this down to a "Hello World" PDF example, the BOM is causing trouble with PDFium.

[hello-bom.pdf](https://github.com/tesseract-ocr/tesseract/files/1331279/hello-bom.pdf)

![pdfium](https://user-images.githubusercontent.com/4961958/30836312-33ab6c4e-a212-11e7-9b40-21c456ed224f.png)

```
%PDF-1.4
%����
1 0 obj
<< /Pages 2 0 R /Type /Catalog >>
endobj
2 0 obj
<< /Count 1 /Kids [ 3 0 R ] /Type /Pages >>
endobj
3 0 obj
<< /Contents 4 0 R /MediaBox [ 0 0 500 800 ] /Parent 2 0 R /Resources 5 0 R /Type /Page >>
endobj
4 0 obj
<< /Length 57 >>
stream
BT /F1 24 Tf 175 720 Td <FEFF00480065006C006C006F> Tj ET
endstream
endobj
5 0 obj
<< /Font << /F1 6 0 R >> >>
endobj
6 0 obj
<< /BaseFont /Helvetica /Subtype /Type1 /Type /Font >>
endobj
xref
0 7
0000000000 65535 f 
0000000015 00000 n 
0000000064 00000 n 
0000000123 00000 n 
0000000229 00000 n 
0000000335 00000 n 
0000000378 00000 n 
trailer << /Root 1 0 R /Size 7 /ID [<89311a609a751f1666063e6962e79bd5><89311a609a751f1666063e6962e79bd5>] >>
startxref
448
%%EOF
```

 I'm not going to make this change until (at least) PDFium can render correctly. That means filing a bug with PDFium. Don't have energy to chase this myself anytime soon, but won't stop someone with more energy than me. Actually, Adobe Reader can't render hello-bom.pdf either. So I don't know what's up, but best guess is it isn't valid despite spec seemingly saying otherwise.  ### Environment

* **Tesseract Version**: Tesseract4.0.0
* **Commit Number**: 2cc531e
* **Platform**: Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux (Centos7.3)

### Current Behavior:
I use the following command to produce the tif files:
<pre>
# /opt/tesseract4.0/bin/text2image --find_fonts \
--fonts_dir /usr/share/fonts \
--text ./langdata/chi_sim/chi_sim.training_text \
--min_coverage .9  \
--outputbase ./results/chi_sim/chi_sim\
|& grep raw | sed -e 's/ :.*/" \\/g'  | sed -e 's/^/  "/' >./results/chi_sim/fontslist.txt

# ls ./results/chi_sim/
chi_sim.Arial_Unicode_MS.tif               chi_sim.FangSong.tif                 chi_sim.Noto_Sans_SC.tif  chi_sim.STZhongsong.tif
chi_sim.AR_PL_UKai_CN.tif                  chi_sim.KaiTi.tif                    chi_sim.NSimSun.tif       chi_sim.WenQuanYi_Micro_Hei_Mono.tif
chi_sim.AR_PL_UKai_HK.tif                  chi_sim.LiSu.tif                     chi_sim.SimHei.tif        chi_sim.WenQuanYi_Micro_Hei.tif
chi_sim.AR_PL_UKai_TW_MBE.tif              chi_sim.Microsoft_YaHei_Bold.tif     chi_sim.SimSun.tif        chi_sim.WenQuanYi_Zen_Hei_Medium.tif
chi_sim.AR_PL_UKai_TW.tif                  chi_sim.Microsoft_YaHei.tif          chi_sim.STFangsong.tif    chi_sim.WenQuanYi_Zen_Hei_Mono_Medium.tif
chi_sim.AR_PL_UMing_CN_Semi-Light.tif      chi_sim.Noto_Sans_SC_Bold.tif        chi_sim.STKaiti.tif       chi_sim.WenQuanYi_Zen_Hei_Sharp_Medium.tif
chi_sim.AR_PL_UMing_HK_Semi-Light.tif      chi_sim.Noto_Sans_SC_Heavy.tif       chi_sim.STSong.tif        chi_sim.YouYuan.tif
chi_sim.AR_PL_UMing_TW_MBE_Semi-Light.tif  chi_sim.Noto_Sans_SC_Medium.tif      chi_sim.STXihei.tif       fontslist.txt
chi_sim.AR_PL_UMing_TW_Semi-Light.tif      chi_sim.Noto_Sans_SC_Semi-Light.tif  chi_sim.STXinwei.tif
</pre>

Then I used the "Windows photo viewer" to open the tif files， and I found that the tif file  contains only part of the chi_sim.training_text's content(langdata/chi_sim). that's why? 

then I append  "--ysize 9600" option to /opt/tesseract4.0/bin/text2image, the generated tif file can contains more. 

Now do I have to set "--ysize" to a very very large number to fix this problem? Is it the right way?

### Expected Behavior:

### Suggested Fix:
 These are multipage tif files. Have you looked at the next page in it.

On 21-Sep-2017 6:09 PM, "ivanzz1001" <notifications@github.com> wrote:

> Environment
>
>    - *Tesseract Version*: Tesseract4.0.0
>    - *Commit Number*: 2cc531e
>    <https://github.com/tesseract-ocr/tesseract/commit/2cc531e6bf0288fc8a9ad1c123a252395f00bf56>
>    - *Platform*: Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1
>    <https://github.com/tesseract-ocr/tesseract/issues/1> SMP Tue Nov 22
>    16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux (Centos7.3)
>
> Current Behavior:
>
> I use the following command to produce the tif files:
>
> # /opt/tesseract4.0/bin/text2image --find_fonts \
> --fonts_dir /usr/share/fonts \
> --text ./langdata/chi_sim/chi_sim.training_text \
> --min_coverage .9  \
> --outputbase ./results/chi_sim/chi_sim\
> |& grep raw | sed -e 's/ :.*/" \\/g'  | sed -e 's/^/  "/' >./results/chi_sim/fontslist.txt
>
> # ls ./results/chi_sim/
> chi_sim.Arial_Unicode_MS.tif               chi_sim.FangSong.tif                 chi_sim.Noto_Sans_SC.tif  chi_sim.STZhongsong.tif
> chi_sim.AR_PL_UKai_CN.tif                  chi_sim.KaiTi.tif                    chi_sim.NSimSun.tif       chi_sim.WenQuanYi_Micro_Hei_Mono.tif
> chi_sim.AR_PL_UKai_HK.tif                  chi_sim.LiSu.tif                     chi_sim.SimHei.tif        chi_sim.WenQuanYi_Micro_Hei.tif
> chi_sim.AR_PL_UKai_TW_MBE.tif              chi_sim.Microsoft_YaHei_Bold.tif     chi_sim.SimSun.tif        chi_sim.WenQuanYi_Zen_Hei_Medium.tif
> chi_sim.AR_PL_UKai_TW.tif                  chi_sim.Microsoft_YaHei.tif          chi_sim.STFangsong.tif    chi_sim.WenQuanYi_Zen_Hei_Mono_Medium.tif
> chi_sim.AR_PL_UMing_CN_Semi-Light.tif      chi_sim.Noto_Sans_SC_Bold.tif        chi_sim.STKaiti.tif       chi_sim.WenQuanYi_Zen_Hei_Sharp_Medium.tif
> chi_sim.AR_PL_UMing_HK_Semi-Light.tif      chi_sim.Noto_Sans_SC_Heavy.tif       chi_sim.STSong.tif        chi_sim.YouYuan.tif
> chi_sim.AR_PL_UMing_TW_MBE_Semi-Light.tif  chi_sim.Noto_Sans_SC_Medium.tif      chi_sim.STXihei.tif       fontslist.txt
> chi_sim.AR_PL_UMing_TW_Semi-Light.tif      chi_sim.Noto_Sans_SC_Semi-Light.tif  chi_sim.STXinwei.tif
>
> Then I used the "Windows photo viewer" to open the tif files， and I found
> that the tif file contains only part of the chi_sim.training_text's
> content(langdata/chi_sim). that's why?
>
> then I append "--ysize 9600" option to /opt/tesseract4.0/bin/text2image,
> the generated tif file can contains more.
>
> Now do I have to set "--ysize" to a very very large number to fix this
> problem? Is it the right way?
> Expected Behavior: Suggested Fix:
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1149>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5DpUe14UmpUxC8J-WcRGtSEayERks5sklkQgaJpZM4PfQXn>
> .
>
 I checked that they are singlepage tif files, how to generate a multipage tif file? where is it going wrong?  ```
#/opt/tesseract4.0/bin/text2image --find_fonts \
--fonts_dir /usr/share/fonts \
--text ./langdata/chi_sim/chi_sim.training_text \
--min_coverage .9  \
--outputbase ./results/chi_sim/chi_sim\
|& grep raw | sed -e 's/ :.*/" \\/g'  | sed -e 's/^/  "/' >./results/chi_sim/fontslist.txt
```

The above command does not produce tif files for training. It is used for generating a list of fonts and a sample page for each font.

Please use tesstrain.sh command as described in training page on wiki. OK, I use tesstrain.sh, it has the same problem, although this time it generates multipage tif files, they also contains only part of the chi_sim.training_text's content @Shreeshrii I think I have found the reason. In the tesseract_master/training/tesstrain_utils.sh, there's the following function:
<pre>
# Helper function for phaseI_generate_image. Generates the image for a single
# language/font combination in a way that can be run in parallel.
generate_font_image() {
    local font="$1"
    tlog "Rendering using ${font}"
    local fontname=$(echo ${font} | tr ' ' '_' | sed 's/,//g')
    local outbase=${TRAINING_DIR}/${LANG_CODE}.${fontname}.exp${EXPOSURE}

    local common_args="--fontconfig_tmpdir=${FONT_CONFIG_CACHE}"
    common_args+=" --fonts_dir=${FONTS_DIR} --strip_unrenderable_words"
    common_args+=" --leading=${LEADING}"
    common_args+=" --char_spacing=${CHAR_SPACING} --exposure=${EXPOSURE}"
    common_args+=" --outputbase=${outbase} --max_pages=3"

    # add --writing_mode=vertical-upright to common_args if the font is
    # specified to be rendered vertically.
    for vfont in "${VERTICAL_FONTS[@]}"; do
      if [[ "${font}" == "${vfont}" ]]; then
        common_args+=" --writing_mode=vertical-upright "
        break
      fi
    done

    run_command text2image ${common_args} --font="${font}" \
        --text=${TRAINING_TEXT} ${TEXT2IMAGE_EXTRA_ARGS}
    check_file_readable ${outbase}.box ${outbase}.tif

    if ((EXTRACT_FONT_PROPERTIES)) &&
        [[ -r ${TRAIN_NGRAMS_FILE} ]]; then
        tlog "Extracting font properties of ${font}"
        run_command text2image ${common_args} --font="${font}" \
            --ligatures=false --text=${TRAIN_NGRAMS_FILE} \
            --only_extract_font_properties --ptsize=32
        check_file_readable ${outbase}.fontinfo
    fi
}
</pre>
It sets "--max_pages=3", then I modified it to "--max_pages=0", and then it generates the tif files which contain all the words in chi_sim.training_text。

Is it a bug in  tesseract_master/training/tesstrain_utils.sh? Should I report it as a bug? This is a recent change to training. I guess, it is to limit the size of tif/box and hence lstmf files for finetune/plusminus training.

I did pose a question to @theraysmith regarding this, but no answer yet. @Shreeshrii  Have you get answer from @theraysmith ? @theraysmith hasn't answered yet, I had asked in one of these issue posts only.

I am not sure whether that change (adding maxpages) is intentional or accidental. 

Many languages need a higher limit to handle the training_text in current langdata repo (which has also NOT been updated for 4.0x).

So, unless the new langdata will have very short training_text which will be less than 3 pages, thi limit should be removed/changed.

@amitdo Do you have any insight regarding this? yes,I think so @Shreeshrii now have you get answer from @theraysmith ? No. @theraysmith might reply when he does next set of updates. I hope ...  do you have installed font "Tohoma" in your linux?  I think you just mount the C:\Windows\Fonts to the /mnt directory。  @amitdo the "--max_pages=3" flag is in the script tesstrain_utils.sh which tesstrain.sh calls Please try with eng language and a font that you know is there on your
system.

Currently Tahoma font is not being found. Try 'Arial' with English.

Then test for 'tha' with a font which supports the language unicode range.

Once the following errors are fixed, then others may also disappear.

=== Starting training for language 'tha'
[Fri Sep 22 03:04:10 DST 2017] /usr/bin/text2image
--fonts_dir=/usr/share/fonts/ --font=Tahoma
--outputbase=/tmp/font_tmp.r6wpt8kkkw/sample_text.txt
--text=/tmp/font_tmp.r6wpt8kkkw/sample_text.txt
--fontconfig_tmpdir=/tmp/font_tmp.r6wpt8kkkw
FcInitiReinitialize failed!!
Could not find font named Tahoma. Pango suggested font
Please correct --font arg.:Error:Assert failed:in file text2image.cpp, line 437


ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Fri, Sep 22, 2017 at 8:51 AM, xanxus10th <notifications@github.com>
wrote:

> @ivanzz1001 <https://github.com/ivanzz1001> @amitdo
> <https://github.com/amitdo> /mnt/c is use for access to the Windows
> directory. but I try to move "Fonts" Folder to the /mnt/ and change code to --fonts_dir
> /usr/share/fonts/ but it appear the same error
>
> === Starting training for language 'tha'
> [Fri Sep 22 03:04:10 DST 2017] /usr/bin/text2image --fonts_dir=/usr/share/fonts/ --font=Tahoma --outputbase=/tmp/font_tmp.r6wpt8kkkw/sample_text.txt --text=/tmp/font_tmp.r6wpt8kkkw/sample_text.txt --fontconfig_tmpdir=/tmp/font_tmp.r6wpt8kkkw
> FcInitiReinitialize failed!!
> Could not find font named Tahoma. Pango suggested font
> Please correct --font arg.:Error:Assert failed:in file text2image.cpp, line 437
>
> === Phase I: Generating training images ===
> Rendering using Tahoma
> [Fri Sep 22 03:04:12 DST 2017] /usr/bin/text2image --fontconfig_tmpdir=/tmp/font_tmp.r6wpt8kkkw --fonts_dir=/mnt/Fonts --strip_unrenderable_words --leading=48 --char_spacing=0.0 --exposure=0 --outputbase=/tmp/tmp.4iPf952XEc/tha/tha.Tahoma.exp0 --max_pages=3 --font=Tahoma --text=/mnt/e/tesseract-ocr/langdata/tha/tha.training_text
> ERROR: Non-existent flag --max_pages=3
> ERROR: /tmp/tmp.4iPf952XEc/tha/tha.Tahoma.exp0.box does not exist or is not readable
> ERROR: /tmp/tmp.4iPf952XEc/tha/tha.Tahoma.exp0.box does not exist or is not readable
>
> *And I try change the code in tesstrain_utils.sh*
> Line 215 - common_args+=" --outputbase=${outbase} --max_pages=3" to
> Line 215 + common_args+=" --outputbase=${outbase} "
>
> But It stuck at Phase Up for many hours
>
> === Phase I: Generating training images ===
> Rendering using Tahoma
> [Fri Sep 22 03:20:05 DST 2017] /usr/bin/text2image --fontconfig_tmpdir=/tmp/font_tmp.w5EOd46HIj --fonts_dir=/usr/share/fonts/ --strip_unrenderable_words --leading=48 --char_spacing=0.0 --exposure=0 --outputbase=/tmp/tmp.QjgaXWkS0p/tha/tha.Tahoma.exp0 --font=Tahoma --text=/mnt/e/tesseract-ocr/langdata/tha/tha.training_text
> Rendered page 0 to file /tmp/tmp.QjgaXWkS0p/tha/tha.Tahoma.exp0.tif
> Rtl = 0 ,vertical=0
>
> === Phase UP: Generating unicharset and unichar properties files ===
> [Fri Sep 22 03:20:06 DST 2017] /usr/bin/unicharset_extractor --output_unicharset /tmp/tmp.QjgaXWkS0p/tha/tha.unicharset --norm_mode 2 /tmp/tmp.QjgaXWkS0p/tha/tha.Tahoma.exp0.box
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1148#issuecomment-331339919>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o9kuce_oT6yLz9RekSMTKGXelO3qks5skye7gaJpZM4PfLUZ>
> .
>
 Just tested on my pc (WSL run with moxaterm)

```
root@All-in-1-Touch:/mnt/c/Users/User/shree/tesseract-HEAD# training/tesstrain.sh \
 --fonts_dir /mnt/c/Windows/Fonts \
  --lang tha \
  --noextract_font_properties  --linedata_only \
  --exposures "0" \
  --langdata_dir ../langdata \
  --tessdata_dir ../tessdata \
  --fontlist \
   "Tahoma" \
   --output_dir ../tesstutorial/tha


=== Starting training for language 'tha'
[Fri Sep 22 09:33:08 DST 2017] /usr/local/bin/text2image --fonts_dir=/mnt/c/Windows/Fonts --font=Tahoma --outputbase=/tmp/font_tmp.Vy1LLR1cHi/sample_text.txt --text=/tmp
/font_tmp.Vy1LLR1cHi/sample_text.txt --fontconfig_tmpdir=/tmp/font_tmp.Vy1LLR1cHi
Rendered page 0 to file /tmp/font_tmp.Vy1LLR1cHi/sample_text.txt.tif

=== Phase I: Generating training images ===
Rendering using Tahoma
[Fri Sep 22 09:34:11 DST 2017] /usr/local/bin/text2image --fontconfig_tmpdir=/tmp/font_tmp.Vy1LLR1cHi --fonts_dir=/mnt/c/Windows/Fonts --strip_unrenderable_words --leadi
ng=48 --char_spacing=0.0 --exposure=0 --outputbase=/tmp/tmp.9wT9v8Aqai/tha/tha.Tahoma.exp0 --max_pages=3 --font=Tahoma --text=../langdata/tha/tha.training_text
Rendered page 0 to file /tmp/tmp.9wT9v8Aqai/tha/tha.Tahoma.exp0.tif
Rendered page 1 to file /tmp/tmp.9wT9v8Aqai/tha/tha.Tahoma.exp0.tif
Rendered page 2 to file /tmp/tmp.9wT9v8Aqai/tha/tha.Tahoma.exp0.tif
``` Please execute the following command to check what fonts have you installed(Centos):
<pre>
# yum install fontconfig mkfontscale
# fc-list

# text2image --fonts_dir /usr/share/fonts --list_available_fonts
</pre> Are you sure you don't have an older version of program somewhere?

Did

make training
and
make training-install

complete without errors.  ### Environment

* **Tesseract Version**: tesseract 4.00.00alpha
* **Commit Number**:  2cc531e
* **Platform**: Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

### Current Behavior:
I excute the following command:
<pre>
training/tesstrain.sh --fonts_dir /usr/share/fonts --lang chi_sim --linedata_only \
  --noextract_font_properties --langdata_dir ../langdata \
  --tessdata_dir ../tessdata \
  --fontlist "AR PL UKai CN" \
  "AR PL UKai HK" \
  "AR PL UKai TW" \
  "AR PL UKai TW MBE" \
  "AR PL UMing CN Semi-Light" \
  "AR PL UMing HK Semi-Light" \
  "AR PL UMing TW MBE Semi-Light" \
  "AR PL UMing TW Semi-Light" \
  "Arial Unicode MS" \
  "FangSong" \
  "KaiTi" \
  "LiSu" \
  "Microsoft YaHei" \
  "Microsoft YaHei Bold" \
  "NSimSun" \
  "Noto Sans SC" \
  "Noto Sans SC Bold" \
  "Noto Sans SC Heavy" \
  "Noto Sans SC Medium" \
  "Noto Sans SC Medium" \
  "Noto Sans SC Semi-Light" \
  "Noto Sans SC Semi-Light" \
  "STFangsong" \
  "STKaiti" \
  "STSong" \
  "STXihei" \
  "STXinwei" \
  "STZhongsong" \
  "SimHei" \
  "SimSun" \
  "WenQuanYi Micro Hei" \
  "WenQuanYi Micro Hei Mono" \
  "WenQuanYi Zen Hei Medium" \
  "WenQuanYi Zen Hei Mono Medium" \
  "WenQuanYi Zen Hei Sharp Medium" \
  "YouYuan" \
  --output_dir ../tesstutorial/chieval \
  --overwrite
</pre>

But when it goes to Phase UP, it generates the following error:
<pre>
=== Phase UP: Generating unicharset and unichar properties files ===
which: no unicharset_extractor in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no unicharset_extractor in (./api)
[Thu Sep 21 02:23:48 PDT 2017] /root/tesseract-src/tesseract-master/training/unicharset_extractor --output_unicharset /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.unicharset --norm_mode 1 /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UKai_CN.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UKai_HK.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UKai_TW.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UKai_TW_MBE.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UMing_CN_Semi-Light.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UMing_HK_Semi-Light.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UMing_TW_MBE_Semi-Light.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UMing_TW_Semi-Light.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.FangSong.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.KaiTi.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.LiSu.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Microsoft_YaHei_Bold.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Microsoft_YaHei.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Noto_Sans_SC_Bold.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Noto_Sans_SC.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Noto_Sans_SC_Heavy.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Noto_Sans_SC_Medium.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.Noto_Sans_SC_Semi-Light.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.NSimSun.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.SimHei.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.SimSun.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.STFangsong.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.STKaiti.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.STSong.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.STXihei.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.STXinwei.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.STZhongsong.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.WenQuanYi_Micro_Hei.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.WenQuanYi_Micro_Hei_Mono.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.WenQuanYi_Zen_Hei_Medium.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.WenQuanYi_Zen_Hei_Mono_Medium.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.WenQuanYi_Zen_Hei_Sharp_Medium.exp0.box /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.YouYuan.exp0.box
Extracting unicharset from box file /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.AR_PL_UKai_CN.exp0.box
Invalid Unicode codepoint: 0xffffffe8
IsValidCodepoint(ch):Error:Assert failed:in file normstrngs.cpp, line 225
ERROR: /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.unicharset does not exist or is not readable
</pre>

1) In https://github.com/tesseract-ocr/langdata, I can't find my chi_sim.unicharset。 But I found it in the tesseract source directory: tesseract-master/testdata/chi_sim.unicharset。But It appears that  in https://github.com/tesseract-ocr/langdata, there's the following two files: Han.unicharset, Han.xheights; It also looks like the Chinese language. Now which unicharset file I should choose? And where I need to place it ?
2)  Now I just copy tesseract-master/testdata/chi_sim.unicharset to langdata/chi_sim/ directory, but it has the following problems:
<pre>
ERROR: /tmp/tmp.Tf9BFjjy6w/chi_sim/chi_sim.unicharset does not exist or is not readable
</pre>

3)  in the temporary directory "/tmp/tmp.Tf9BFjjy6w/chi_sim",I have the following files:
<pre>
[root@localhost tesseract-master]# ls /tmp/tmp.Tf9BFjjy6w/chi_sim/
chi_sim.AR_PL_UKai_CN.exp0.box                  chi_sim.Microsoft_YaHei_Bold.exp0.tif     chi_sim.STSong.exp0.box
chi_sim.AR_PL_UKai_CN.exp0.tif                  chi_sim.Microsoft_YaHei.exp0.box          chi_sim.STSong.exp0.tif
chi_sim.AR_PL_UKai_HK.exp0.box                  chi_sim.Microsoft_YaHei.exp0.tif          chi_sim.STXihei.exp0.box
chi_sim.AR_PL_UKai_HK.exp0.tif                  chi_sim.Noto_Sans_SC_Bold.exp0.box        chi_sim.STXihei.exp0.tif
chi_sim.AR_PL_UKai_TW.exp0.box                  chi_sim.Noto_Sans_SC_Bold.exp0.tif        chi_sim.STXinwei.exp0.box
chi_sim.AR_PL_UKai_TW.exp0.tif                  chi_sim.Noto_Sans_SC.exp0.box             chi_sim.STXinwei.exp0.tif
chi_sim.AR_PL_UKai_TW_MBE.exp0.box              chi_sim.Noto_Sans_SC.exp0.tif             chi_sim.STZhongsong.exp0.box
chi_sim.AR_PL_UKai_TW_MBE.exp0.tif              chi_sim.Noto_Sans_SC_Heavy.exp0.box       chi_sim.STZhongsong.exp0.tif
chi_sim.AR_PL_UMing_CN_Semi-Light.exp0.box      chi_sim.Noto_Sans_SC_Heavy.exp0.tif       chi_sim.WenQuanYi_Micro_Hei.exp0.box
chi_sim.AR_PL_UMing_CN_Semi-Light.exp0.tif      chi_sim.Noto_Sans_SC_Medium.exp0.box      chi_sim.WenQuanYi_Micro_Hei.exp0.tif
chi_sim.AR_PL_UMing_HK_Semi-Light.exp0.box      chi_sim.Noto_Sans_SC_Medium.exp0.tif      chi_sim.WenQuanYi_Micro_Hei_Mono.exp0.box
chi_sim.AR_PL_UMing_HK_Semi-Light.exp0.tif      chi_sim.Noto_Sans_SC_Semi-Light.exp0.box  chi_sim.WenQuanYi_Micro_Hei_Mono.exp0.tif
chi_sim.AR_PL_UMing_TW_MBE_Semi-Light.exp0.box  chi_sim.Noto_Sans_SC_Semi-Light.exp0.tif  chi_sim.WenQuanYi_Zen_Hei_Medium.exp0.box
chi_sim.AR_PL_UMing_TW_MBE_Semi-Light.exp0.tif  chi_sim.NSimSun.exp0.box                  chi_sim.WenQuanYi_Zen_Hei_Medium.exp0.tif
chi_sim.AR_PL_UMing_TW_Semi-Light.exp0.box      chi_sim.NSimSun.exp0.tif                  chi_sim.WenQuanYi_Zen_Hei_Mono_Medium.exp0.box
chi_sim.AR_PL_UMing_TW_Semi-Light.exp0.tif      chi_sim.SimHei.exp0.box                   chi_sim.WenQuanYi_Zen_Hei_Mono_Medium.exp0.tif
chi_sim.FangSong.exp0.box                       chi_sim.SimHei.exp0.tif                   chi_sim.WenQuanYi_Zen_Hei_Sharp_Medium.exp0.box
chi_sim.FangSong.exp0.tif                       chi_sim.SimSun.exp0.box                   chi_sim.WenQuanYi_Zen_Hei_Sharp_Medium.exp0.tif
chi_sim.KaiTi.exp0.box                          chi_sim.SimSun.exp0.tif                   chi_sim.YouYuan.exp0.box
chi_sim.KaiTi.exp0.tif                          chi_sim.STFangsong.exp0.box               chi_sim.YouYuan.exp0.tif
chi_sim.LiSu.exp0.box                           chi_sim.STFangsong.exp0.tif               tesstrain.log
chi_sim.LiSu.exp0.tif                           chi_sim.STKaiti.exp0.box
chi_sim.Microsoft_YaHei_Bold.exp0.box           chi_sim.STKaiti.exp0.tif
</pre>
I use an image tool and  open the .tif files, but I  found it contains only part of the chi_sim.training_text's content(langdata/chi_sim), that's why? how do I fix it? 

### Expected Behavior:



### Suggested Fix:
 It is getting errors related to the program `unicharset_extractor`.

Please see known and still open issue: https://github.com/tesseract-ocr/tesseract/issues/1114
 @Shreeshrii  I think I have found the reason。I have the following chi_sim.training_text( Here I just show you first line):
<pre>
1996规格器皿 砝2.5、客胫骨发电All 联络 其、鄞州 Education嫉处感谢铁道
</pre>
And I add some print msg in the following function(tesseract_master/training/unicharset_extractor.cpp):
<pre>
// Helper normalizes and segments the given strings according to norm_mode, and
// adds the segmented parts to unicharset.
static void AddStringsToUnicharset(const GenericVector<STRING>& strings,
                                   int norm_mode, UNICHARSET* unicharset) {
  for (int i = 0; i < strings.size(); ++i) {
    std::vector<string> normalized;
    if (NormalizeCleanAndSegmentUTF8(UnicodeNormMode::kNFC, OCRNorm::kNone,
                                     static_cast<GraphemeNormMode>(norm_mode),
                                     /*report_errors*/ true,
                                     strings[i].string(), &normalized)) {
      tprintf("string: %s\n",strings[i].string());
      for (const string& normed : normalized) {
      tprintf("string2:%s\n",normed.c_str());
        if (normed.empty() || IsWhitespace(normed[0])) continue;
        unicharset->unichar_insert(normed.c_str());
      }
    } else {
      tprintf("Normalization failed for string '%s'\n", strings[i].c_str());
    }
  }
}
</pre>
It prints the following:
<pre>
string: 1
string2:1
string: 9
string2:9
string: 9
string2:9
string: 6
string2:6
string: 规
string2:规
Invalid Unicode codepoint: 0xffffffe8
IsValidCodepoint(ch):Error:Assert failed:in file normstrngs.cpp, line 225
ERROR: /tmp/tmp.8gMFI2Gry5/chi_sim/chi_sim.unicharset does not exist or is not readable
</pre>

Then I write a short test:
<pre>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

#include &lt;string&gt;
using namespace std;

int main(int argc,char *argv[]){
        string s = "规";
        printf("size:%d\n",s.size());
        for(int i=0;i &lt; s.size();i++)
                printf("%x ",s[i]);
        printf("\n");
        return 0;
}
</pre>
Execute the test:
<pre>
ivan1001@ceph-admin:~/test-src$ ./test
size:3
ffffffe8 ffffffa7 ffffff84 
</pre>
Now I found "0xffffffe8 0xffffffa7 0xffffff84" is the utf-8 code of "规",not what we expect the Unicode encoding((utf-32) in the unicharset_extractor.cpp program。The Unicode code of "规" is "\u89c4", it is in then range : [0, 0xD800) or [0xE000, 0x10FFFF]

Here I think the program's logic has some problems。 First It use function NormalizeCleanAndSegmentUTF8() to convert the string to UTF-8 encoding , but thereafter it use a "Unicode (utf-32)" function IsValidCodepoint() to check the utf-8's result. 

Please check, and how do I post the bug to the developers or could you help me?  @ivanzz1001 Thanks for looking into this issue and finding possible reason. I do not know enough about tesseract and c++ to comment.

Tagging @theraysmith and @stweil - related issue https://github.com/tesseract-ocr/tesseract/issues/1114

I had also thought that it was related in someway to the conversion of utf8 training text to the utf32 format, but did not know how to check for it.
 @Shreeshrii @stweil  The following function:
<pre>
static void AddStringsToUnicharset(const GenericVector<STRING>& strings,
                                   int norm_mode, UNICHARSET* unicharset) {
  for (int i = 0; i < strings.size(); ++i) {
    std::vector<string> normalized;
    if (NormalizeCleanAndSegmentUTF8(UnicodeNormMode::kNFC, OCRNorm::kNone,
                                     static_cast<GraphemeNormMode>(norm_mode),
                                     /*report_errors*/ true,
                                     strings[i].string(), &normalized)) {
      for (const string& normed : normalized) {
        if (normed.empty() || IsWhitespace(normed[0])) continue;
        unicharset->unichar_insert(normed.c_str());
      }
    } else {
      tprintf("Normalization failed for string '%s'\n", strings[i].c_str());
    }
  }
}
</pre>
It may need to change to:
<pre>
if (normed.empty() || IsUTF8Whitespace(normed[0])) continue;
</pre>
 @amitdo Last it has changed back to UTF-8:
<pre>
bool NormalizeCleanAndSegmentUTF8(UnicodeNormMode u_mode, OCRNorm ocr_normalize,
                                  GraphemeNormMode g_mode, bool report_errors,
                                  const char* str8,
                                  std::vector<string>* graphemes) {
  std::vector<char32> normed32;
  NormalizeUTF8ToUTF32(u_mode, ocr_normalize, str8, &normed32);
  StripJoiners(&normed32);
  std::vector<std::vector<char32>> graphemes32;
  bool success = Validator::ValidateCleanAndSegment(g_mode, report_errors,
                                                    normed32, &graphemes32);
  if (g_mode != GraphemeNormMode::kSingleString && success) {
    // If we modified the string to clean it up, the segmentation may not be
    // correct, so check for changes and do it again.
    std::vector<char32> cleaned32;
    for (const auto& g : graphemes32) {
      cleaned32.insert(cleaned32.end(), g.begin(), g.end());
    }
    if (cleaned32 != normed32) {
      graphemes32.clear();
      success = Validator::ValidateCleanAndSegment(g_mode, report_errors,
                                                   cleaned32, &graphemes32);
    }
  }
  graphemes->clear();
  graphemes->reserve(graphemes32.size());
  for (const auto& grapheme : graphemes32) {
    graphemes->push_back(UNICHAR::UTF32ToUTF8(grapheme));
  }
  return success;
}
</pre> @Shreeshrii I had used the changed source code(tesseract_master/training/unicharset_extractor.cpp):
<pre>
if (normed.empty() || IsUTF8Whitespace(normed.c_str())) continue;
</pre>
and had generated the .lstmf files. It seems the above modify is OK.  But I haven't finished all the training steps so I can't absolutely make sure it works ok.

 @Shreeshrii  BTW,when I use the training/tesstrain.sh  to generate chi_sim_XXX.lstmf file， why should "--tessdata_dir ../tessdata" directory must contain eng.traineddata? if the directory does't contains the eng.traineddata, It will continuously find the file, but here I just want to train sim_chi tesseract checks for osd and eng.traineddata at start of program. It has
been there for many years and hasn't been changed even though now it
handles many more languages.

The developers have other higher priority bugs  to fix/ features to add,
and so the requirement for eng.traineddata  remains.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Sep 26, 2017 at 3:44 PM, ivanzz1001 <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> BTW,when I use the
> training/tesstrain.sh to generate chi_sim_XXX.lstmf file， why should
> "--tessdata_dir ../tessdata" must contains eng.traineddata? if the
> directory does't contains the eng.traineddata, It will continuously find
> the file, but here I just want to train sim_chi
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332152832>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o-wkkcLQjDbfqVYiiuYoKjSk0YxCks5smM5-gaJpZM4PfFg_>
> .
>
 @Shreeshrii  when I execute the "training/tesstrain.sh " and later meets the following:
<pre>
=== Constructing LSTM training data ===
which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no combine_lang_model in (./api)
[Tue Sep 26 21:17:16 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.LASR8IGnop/chi_sim/chi_sim.unicharset --script_dir ../langdata --words ../langdata/chi_sim/chi_sim.wordlist --numbers ../langdata/chi_sim/chi_sim.numbers --puncs ../langdata/chi_sim/chi_sim.punc --output_dir ../tesstutorial/chieval --lang chi_sim
Loaded unicharset of size 5074 from file /tmp/tmp.LASR8IGnop/chi_sim/chi_sim.unicharset
Setting unichar properties
Setting script properties
Warning: properties incomplete for index 106 = ，
Config file is optional, continuing...
Null char=2
Invalid format in radical table at line 0: 19886 3 23 6 3
Creation of encoded unicharset failed!!
Error writing recoder!!
Reducing Trie to SquishedDawg
Error during conversion of wordlists to DAWGs!!
</pre>
I have used the chi_sim_vert.traineddata from the tessdata_best directory, is it the reason cause the problems above? Here [#842](https://github.com/tesseract-ocr/tesseract/issues/842) you said:
<pre>
modify chi_sim.config file in langdata/chi_sim
and comment out first line related to loading of the vertical sub language
</pre>
but I didn't modify it and used the chi_sim_vert.trainneddata from the tessdata_best directrory. @Shreeshrii  Or should I try my own chi_sim_vert.traineddata first? >Here #842 you said:

>modify chi_sim.config file in langdata/chi_sim

>and comment out first line related to loading of the vertical sub language

I think at that time, there was no chi_sim_vert traineddata available.


I will try out the command at my end to see what error I get.


ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Sep 27, 2017 at 11:48 AM, ivanzz1001 <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> Or should I try my own
> chi_sim_vert.traineddata first?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332420444>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oxMyOZwSj81sdhBzebAx3veBXFDEks5smei9gaJpZM4PfFg_>
> .
>
 > Invalid format in radical table at line 0: 19886 3 23 6 3

Do you have the latest version of

https://github.com/tesseract-ocr/langdata/blob/master/radical-stroke.txt

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Sep 27, 2017 at 12:52 PM, ShreeDevi Kumar <shreeshrii@gmail.com>
wrote:

> >Here #842 you said:
>
> >modify chi_sim.config file in langdata/chi_sim
>
> >and comment out first line related to loading of the vertical sub language
>
> I think at that time, there was no chi_sim_vert traineddata available.
>
>
> I will try out the command at my end to see what error I get.
>
>
> ShreeDevi
> ____________________________________________________________
> भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com
>
> On Wed, Sep 27, 2017 at 11:48 AM, ivanzz1001 <notifications@github.com>
> wrote:
>
>> @Shreeshrii <https://github.com/shreeshrii> Or should I try my own
>> chi_sim_vert.traineddata first?
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332420444>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AE2_oxMyOZwSj81sdhBzebAx3veBXFDEks5smei9gaJpZM4PfFg_>
>> .
>>
>
>
 @Shreeshrii Yes, I have downloaded the latest version Looks like you do not have the following program.

which: no combine_lang_model in
(/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no combine_lang_model in (./api)


Please check. This is required for building starter traineddata


On 27-Sep-2017 1:13 PM, "ivanzz1001" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> Yes, I have downloaded the
> latest version
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332437666>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5gUX21xnctOMVNN0XKdBadnga8Yks5smfyfgaJpZM4PfFg_>
> .
>
 I think it is not the reason, look at the following:
<pre>
=== Constructing LSTM training data ===
which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no combine_lang_model in (./api)
[Tue Sep 26 21:17:16 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.LASR8IGnop/chi_sim/chi_sim.unicharset --script_dir ../langdata --words ../langdata/chi_sim/chi_sim.wordlist --numbers ../langdata/chi_sim/chi_sim.numbers --puncs ../langdata/chi_sim/chi_sim.punc --output_dir ../tesstutorial/chieval --lang chi_sim
</pre>
It has found it  at /root/tesseract-src/tesseract-master/training/combine_lang_model。 Although,I will retry it later works for me

```
Loaded 1484/1484 pages (1-1484) of document
/tmp/tmp.nuCxxWRRbN/chi_sim/chi_sim.STXihei.exp0.lstmf
Page 35
Loaded 1529/1529 pages (1-1529) of document
/tmp/tmp.nuCxxWRRbN/chi_sim/chi_sim.STXihei.exp0.lstmf

=== Constructing LSTM training data ===
[Wed Sep 27 15:10:45 DST 2017] /usr/local/bin/combine_lang_model
--input_unicharset /tmp/tmp.nuCxxWRRbN/chi_sim/chi_sim.un
icharset --script_dir ../langdata --words
../langdata/chi_sim/chi_sim.wordlist --numbers
../langdata/chi_sim/chi_sim.numbe
rs --puncs ../langdata/chi_sim/chi_sim.punc --output_dir
../tesstutorial/chi_sim --lang chi_sim
Loaded unicharset of size 5028 from file
/tmp/tmp.nuCxxWRRbN/chi_sim/chi_sim.unicharset
Setting unichar properties
Mirror 〖 of 〗 is not in unicharset
Setting script properties
Warning: properties incomplete for index 333 = ，
Config file is optional, continuing...
Null char=2
Reducing Trie to SquishedDawg
Reducing Trie to SquishedDawg
Reducing Trie to SquishedDawg
Moving /tmp/tmp.nuCxxWRRbN/chi_sim/chi_sim.STXihei.exp0.lstmf to
../tesstutorial/chi_sim

Completed training for language 'chi_sim'
```

Have you changed the training_text?

It could be the change related to whitespace code change...

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Sep 27, 2017 at 1:36 PM, ivanzz1001 <notifications@github.com>
wrote:

> I think it is not the reason, look at the following:
>
> === Constructing LSTM training data ===
> which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no combine_lang_model in (./api)
> [Tue Sep 26 21:17:16 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.LASR8IGnop/chi_sim/chi_sim.unicharset --script_dir ../langdata --words ../langdata/chi_sim/chi_sim.wordlist --numbers ../langdata/chi_sim/chi_sim.numbers --puncs ../langdata/chi_sim/chi_sim.punc --output_dir ../tesstutorial/chieval --lang chi_sim
>
> It has found it at /root/tesseract-src/tesseract-
> master/training/combine_lang_model。 Although,I will retry it later
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332443146>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o8lO8PINpcEfYUvJzR4bMcbxdwsLks5smgIVgaJpZM4PfFg_>
> .
>
 I don't change anything in the langdata directory, And my command is:
<pre>
training/tesstrain.sh --fonts_dir /usr/share/fonts --lang chi_sim --linedata_only \
  --noextract_font_properties --langdata_dir ../langdata \
  --tessdata_dir ./tessdata \
  --exposures "0" \
  --fontlist "AR PL UKai CN" \
  "AR PL UKai HK" \
  "AR PL UKai TW" \
  "AR PL UKai TW MBE" \
  "AR PL UMing CN Semi-Light" \
  "AR PL UMing HK Semi-Light" \
  "AR PL UMing TW MBE Semi-Light" \
  "AR PL UMing TW Semi-Light" \
  "Arial Unicode MS" \
  "FangSong" \
  "KaiTi" \
  "LiSu" \
  "Microsoft YaHei" \
  "Microsoft YaHei Bold" \
  "NSimSun" \
  "Noto Sans SC" \
  "Noto Sans SC Bold" \
  "Noto Sans SC Heavy" \
  "Noto Sans SC Medium" \
  "Noto Sans SC Semi-Light" \
  "STFangsong" \
  "STKaiti" \
  "STSong" \
  "STXihei" \
  "STXinwei" \
  "STZhongsong" \
  "SimHei" \
  "SimSun" \
  "WenQuanYi Micro Hei" \
  "WenQuanYi Micro Hei Mono" \
  "WenQuanYi Zen Hei Medium" \
  "WenQuanYi Zen Hei Mono Medium" \
  "WenQuanYi Zen Hei Sharp Medium" \
  "YouYuan" \
  --output_dir ../tesstutorial/chieval \
  --overwrite
</pre> This is the command I used

training/tesstrain.sh \
--fonts_dir /mnt/c/Windows/Fonts \
 --lang chi_sim \
 --noextract_font_properties  --linedata_only \
 --exposures "0" \
 --langdata_dir ../langdata \
 --tessdata_dir ../tessdata \
 --fontlist \
  "STXihei" \
  --output_dir ../tesstutorial/chi_sim


Please check whether it works with just this one font.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Sep 27, 2017 at 3:20 PM, ivanzz1001 <notifications@github.com>
wrote:

> I don't change anything in the langdata directory, And my command is:
>
> training/tesstrain.sh --fonts_dir /usr/share/fonts --lang chi_sim --linedata_only \
>   --noextract_font_properties --langdata_dir ../langdata \
>   --tessdata_dir ./tessdata \
>   --exposures "0" \
>   --fontlist "AR PL UKai CN" \
>   "AR PL UKai HK" \
>   "AR PL UKai TW" \
>   "AR PL UKai TW MBE" \
>   "AR PL UMing CN Semi-Light" \
>   "AR PL UMing HK Semi-Light" \
>   "AR PL UMing TW MBE Semi-Light" \
>   "AR PL UMing TW Semi-Light" \
>   "Arial Unicode MS" \
>   "FangSong" \
>   "KaiTi" \
>   "LiSu" \
>   "Microsoft YaHei" \
>   "Microsoft YaHei Bold" \
>   "NSimSun" \
>   "Noto Sans SC" \
>   "Noto Sans SC Bold" \
>   "Noto Sans SC Heavy" \
>   "Noto Sans SC Medium" \
>   "Noto Sans SC Semi-Light" \
>   "STFangsong" \
>   "STKaiti" \
>   "STSong" \
>   "STXihei" \
>   "STXinwei" \
>   "STZhongsong" \
>   "SimHei" \
>   "SimSun" \
>   "WenQuanYi Micro Hei" \
>   "WenQuanYi Micro Hei Mono" \
>   "WenQuanYi Zen Hei Medium" \
>   "WenQuanYi Zen Hei Mono Medium" \
>   "WenQuanYi Zen Hei Sharp Medium" \
>   "YouYuan" \
>   --output_dir ../tesstutorial/chieval \
>   --overwrite
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332469904>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o8p8bHZ9xsJDYYh51ijOcQHuXYDNks5smhpogaJpZM4PfFg_>
> .
>
 I use just one font but it also has the same problem:
<pre>
training/tesstrain.sh --fonts_dir /usr/share/fonts --lang chi_sim --linedata_only \
  --noextract_font_properties --langdata_dir ../langdata \
  --tessdata_dir ./tessdata \
  --exposures "0" \
  --fontlist "AR PL UKai CN" \
  --output_dir ../tesstutorial/chieval \
  --overwrite
</pre> >Invalid format in radical table at line 0: 19886 3 23 6 3
Creation of encoded unicharset failed!!
Error writing recoder!!
Reducing Trie to SquishedDawg
Error during conversion of wordlists to DAWGs!!

Are you still getting the above error? Yes. I use you command:
<pre>training/tesstrain.sh \
 --fonts_dir /usr/share/fonts \
 --lang chi_sim \
 --noextract_font_properties  --linedata_only \
 --exposures "0" \
 --langdata_dir ../langdata \
 --tessdata_dir ./tessdata \
 --fontlist \
  "STXihei" \
  --output_dir ../tesstutorial/chi_sim
</pre>
But It got the same problem.
<pre>
=== Constructing LSTM training data ===
Creating new directory ../tesstutorial/chi_sim
which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no combine_lang_model in (./api)
[Wed Sep 27 19:03:32 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.mv3dlQnYez/chi_sim/chi_sim.unicharset --script_dir ../langdata --words ../langdata/chi_sim/chi_sim.wordlist --numbers ../langdata/chi_sim/chi_sim.numbers --puncs ../langdata/chi_sim/chi_sim.punc --output_dir ../tesstutorial/chi_sim --lang chi_sim
Loaded unicharset of size 1923 from file /tmp/tmp.mv3dlQnYez/chi_sim/chi_sim.unicharset
Setting unichar properties
Mirror 「 of 」 is not in unicharset
Mirror { of } is not in unicharset
Mirror 〗 of 〖 is not in unicharset
Other case Z of z is not in unicharset
Setting script properties
Warning: properties incomplete for index 106 = ，
Config file is optional, continuing...
Null char=2
Invalid format in radical table at line 0: 19886 3 23 6 3
Creation of encoded unicharset failed!!
Error writing recoder!!
Reducing Trie to SquishedDawg
Error during conversion of wordlists to DAWGs!!
Moving /tmp/tmp.mv3dlQnYez/chi_sim/chi_sim.STXihei.exp0.lstmf to ../tesstutorial/chi_sim

Completed training for language 'chi_sim'
</pre>
which version have you use "--tessdata_dir ./tessdata"? the lastest tessdata_best? Something seems wrong with your radical stroke file from langdata, please
download again and try.

On 28-Sep-2017 7:35 AM, "ivanzz1001" <notifications@github.com> wrote:

> Yes. I use you command:
>
> training/tesstrain.sh \
>  --fonts_dir /usr/share/fonts \
>  --lang chi_sim \
>  --noextract_font_properties  --linedata_only \
>  --exposures "0" \
>  --langdata_dir ../langdata \
>  --tessdata_dir ./tessdata \
>  --fontlist \
>   "STXihei" \
>   --output_dir ../tesstutorial/chi_sim
>
> But It got the same problem.
>
> === Constructing LSTM training data ===
> Creating new directory ../tesstutorial/chi_sim
> which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no combine_lang_model in (./api)
> [Wed Sep 27 19:03:32 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.mv3dlQnYez/chi_sim/chi_sim.unicharset --script_dir ../langdata --words ../langdata/chi_sim/chi_sim.wordlist --numbers ../langdata/chi_sim/chi_sim.numbers --puncs ../langdata/chi_sim/chi_sim.punc --output_dir ../tesstutorial/chi_sim --lang chi_sim
> Loaded unicharset of size 1923 from file /tmp/tmp.mv3dlQnYez/chi_sim/chi_sim.unicharset
> Setting unichar properties
> Mirror 「 of 」 is not in unicharset
> Mirror { of } is not in unicharset
> Mirror 〗 of 〖 is not in unicharset
> Other case Z of z is not in unicharset
> Setting script properties
> Warning: properties incomplete for index 106 = ，
> Config file is optional, continuing...
> Null char=2
> Invalid format in radical table at line 0: 19886 3 23 6 3
> Creation of encoded unicharset failed!!
> Error writing recoder!!
> Reducing Trie to SquishedDawg
> Error during conversion of wordlists to DAWGs!!
> Moving /tmp/tmp.mv3dlQnYez/chi_sim/chi_sim.STXihei.exp0.lstmf to ../tesstutorial/chi_sim
>
> Completed training for language 'chi_sim'
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332706109>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1mDnBtQxwWF8SPKKd_KotLHfRoVks5smv72gaJpZM4PfFg_>
> .
>
 I have got the latest langdata, but it has the same problem. Could you send me you tesstrain.log? @Shreeshrii  I trained eng, it also has the same problem:
<pre>
 [root@localhost tesseract-master]# training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
>   --noextract_font_properties --langdata_dir ../langdata \
>   --exposures "0" \
>   --fontlist "DejaVu Serif" \
>   --tessdata_dir ../tessdata --output_dir ../tesstutorial/engeval

=== Starting training for language 'eng'
which: no text2image in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no text2image in (./api)
[Wed Sep 27 20:25:03 PDT 2017] /root/tesseract-src/tesseract-master/training/text2image --fonts_dir=/usr/share/fonts --font=DejaVu Serif --outputbase=/tmp/font_tmp.mopgCYHqsF/sample_text.txt --text=/tmp/font_tmp.mopgCYHqsF/sample_text.txt --fontconfig_tmpdir=/tmp/font_tmp.mopgCYHqsF
Rendered page 0 to file /tmp/font_tmp.mopgCYHqsF/sample_text.txt.tif

=== Phase I: Generating training images ===
Rendering using DejaVu Serif
which: no text2image in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no text2image in (./api)
[Wed Sep 27 20:25:09 PDT 2017] /root/tesseract-src/tesseract-master/training/text2image --fontconfig_tmpdir=/tmp/font_tmp.mopgCYHqsF --fonts_dir=/usr/share/fonts --strip_unrenderable_words --leading=32 --char_spacing=0.0 --exposure=0 --outputbase=/tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0 --max_pages=3 --font=DejaVu Serif --text=../langdata/eng/eng.training_text
Rendered page 0 to file /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.tif
Rendered page 1 to file /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.tif

=== Phase UP: Generating unicharset and unichar properties files ===
which: no unicharset_extractor in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no unicharset_extractor in (./api)
[Wed Sep 27 20:25:10 PDT 2017] /root/tesseract-src/tesseract-master/training/unicharset_extractor --output_unicharset /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset --norm_mode 1 /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.box
Extracting unicharset from box file /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.box
Other case É of é is not in unicharset
Wrote unicharset file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
which: no set_unicharset_properties in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no set_unicharset_properties in (./api)
[Wed Sep 27 20:25:10 PDT 2017] /root/tesseract-src/tesseract-master/training/set_unicharset_properties -U /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset -O /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset -X /tmp/tmp.nqgcR2lnuC/eng/eng.xheights --script_dir=../langdata
Loaded unicharset of size 111 from file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
Setting unichar properties
Other case É of é is not in unicharset
Setting script properties
Warning: properties incomplete for index 25 = ~
Writing unicharset to file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset

=== Phase E: Generating lstmf files ===
Using TESSDATA_PREFIX=../tessdata
which: no tesseract in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
[Wed Sep 27 20:25:10 PDT 2017] /root/tesseract-src/tesseract-master/api/tesseract /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.tif /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0 lstm.train
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1
Page 2
Loaded 51/51 pages (1-51) of document /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.lstmf

=== Constructing LSTM training data ===
which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
which: no combine_lang_model in (./api)
[Wed Sep 27 20:25:12 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset --script_dir ../langdata --words ../langdata/eng/eng.wordlist --numbers ../langdata/eng/eng.numbers --puncs ../langdata/eng/eng.punc --output_dir ../tesstutorial/engeval --lang eng
Loaded unicharset of size 111 from file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
Setting unichar properties
Other case É of é is not in unicharset
Setting script properties
Config file is optional, continuing...
Failed to read data from: ../langdata/eng/eng.config
Null char=2
Invalid format in radical table at line 0: 19886 3 23 6 3
Creation of encoded unicharset failed!!
Error writing recoder!!
Reducing Trie to SquishedDawg
Error during conversion of wordlists to DAWGs!!
Moving /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.lstmf to ../tesstutorial/engeval

Completed training for language 'eng'
</pre>

which version of tesseract do you use? You are getting the error related to the following


https://github.com/tesseract-ocr/tesseract/blob/a2a72d7ca78a3bb3798a02a2ba5188e255c2a0f7/ccutil/unicharcompress.cpp#L79

https://github.com/tesseract-ocr/langdata/blob/master/radical-stroke.txt

The first line in radical-stroke.txt is
19886 3 23 6 3
and
your error line says

Invalid format in radical table at line 0: 19886 3 23 6 3


So, there is a mismatch between the program that you are using and the data.

I am using the latest version of code from github ...

 tesseract -v
tesseract 4.00.00alpha
 leptonica-1.74.4
  libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8

 Found AVX
 Found SSE


--------------------
Please check whether you have multiple versions/old versions of the program.



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Sep 28, 2017 at 8:56 AM, ivanzz1001 <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> I trained eng, it also has
> the same problem:
>
>  [root@localhost tesseract-master]# training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
> >   --noextract_font_properties --langdata_dir ../langdata \
> >   --exposures "0" \
> >   --fontlist "DejaVu Serif" \
> >   --tessdata_dir ../tessdata --output_dir ../tesstutorial/engeval
>
> === Starting training for language 'eng'
> which: no text2image in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no text2image in (./api)
> [Wed Sep 27 20:25:03 PDT 2017] /root/tesseract-src/tesseract-master/training/text2image --fonts_dir=/usr/share/fonts --font=DejaVu Serif --outputbase=/tmp/font_tmp.mopgCYHqsF/sample_text.txt --text=/tmp/font_tmp.mopgCYHqsF/sample_text.txt --fontconfig_tmpdir=/tmp/font_tmp.mopgCYHqsF
> Rendered page 0 to file /tmp/font_tmp.mopgCYHqsF/sample_text.txt.tif
>
> === Phase I: Generating training images ===
> Rendering using DejaVu Serif
> which: no text2image in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no text2image in (./api)
> [Wed Sep 27 20:25:09 PDT 2017] /root/tesseract-src/tesseract-master/training/text2image --fontconfig_tmpdir=/tmp/font_tmp.mopgCYHqsF --fonts_dir=/usr/share/fonts --strip_unrenderable_words --leading=32 --char_spacing=0.0 --exposure=0 --outputbase=/tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0 --max_pages=3 --font=DejaVu Serif --text=../langdata/eng/eng.training_text
> Rendered page 0 to file /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.tif
> Rendered page 1 to file /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.tif
>
> === Phase UP: Generating unicharset and unichar properties files ===
> which: no unicharset_extractor in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no unicharset_extractor in (./api)
> [Wed Sep 27 20:25:10 PDT 2017] /root/tesseract-src/tesseract-master/training/unicharset_extractor --output_unicharset /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset --norm_mode 1 /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.box
> Extracting unicharset from box file /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.box
> Other case É of é is not in unicharset
> Wrote unicharset file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
> which: no set_unicharset_properties in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no set_unicharset_properties in (./api)
> [Wed Sep 27 20:25:10 PDT 2017] /root/tesseract-src/tesseract-master/training/set_unicharset_properties -U /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset -O /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset -X /tmp/tmp.nqgcR2lnuC/eng/eng.xheights --script_dir=../langdata
> Loaded unicharset of size 111 from file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
> Setting unichar properties
> Other case É of é is not in unicharset
> Setting script properties
> Warning: properties incomplete for index 25 = ~
> Writing unicharset to file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
>
> === Phase E: Generating lstmf files ===
> Using TESSDATA_PREFIX=../tessdata
> which: no tesseract in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> [Wed Sep 27 20:25:10 PDT 2017] /root/tesseract-src/tesseract-master/api/tesseract /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.tif /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0 lstm.train
> Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
> Page 1
> Page 2
> Loaded 51/51 pages (1-51) of document /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.lstmf
>
> === Constructing LSTM training data ===
> which: no combine_lang_model in (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
> which: no combine_lang_model in (./api)
> [Wed Sep 27 20:25:12 PDT 2017] /root/tesseract-src/tesseract-master/training/combine_lang_model --input_unicharset /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset --script_dir ../langdata --words ../langdata/eng/eng.wordlist --numbers ../langdata/eng/eng.numbers --puncs ../langdata/eng/eng.punc --output_dir ../tesstutorial/engeval --lang eng
> Loaded unicharset of size 111 from file /tmp/tmp.nqgcR2lnuC/eng/eng.unicharset
> Setting unichar properties
> Other case É of é is not in unicharset
> Setting script properties
> Config file is optional, continuing...
> Failed to read data from: ../langdata/eng/eng.config
> Null char=2
> Invalid format in radical table at line 0: 19886 3 23 6 3
> Creation of encoded unicharset failed!!
> Error writing recoder!!
> Reducing Trie to SquishedDawg
> Error during conversion of wordlists to DAWGs!!
> Moving /tmp/tmp.nqgcR2lnuC/eng/eng.DejaVu_Serif.exp0.lstmf to ../tesstutorial/engeval
>
> Completed training for language 'eng'
>
> which version of tesseract do you use?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332717337>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o9ooGcLZou1zU4_KONg1VHqlQIuYks5smxH1gaJpZM4PfFg_>
> .
>
 I checked that I have only one version. And On a new system, I retried the process and met the same problem @Shreeshrii  @amitdo  I git clone the langdata directory to Linux and now it seems OK. The strange problem may be caused by: I git clone the langdata to Windows, and then use WinSCP to transfer the langdata to Linux(I can make sure the langdata is the latest as @Shreeshrii let me check -- I re-git-cloned the latest langdata) OK, it might be related to Windows EOL vs Unix EOL.
Maybe some WinSCP setting changes it.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Sep 28, 2017 at 2:32 PM, ivanzz1001 <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> @amitdo
> <https://github.com/amitdo> I git clone the langdata directory to Linux
> and now it seems OK. The strange problem may be caused by: I git clone the
> langdata to Windows, and then use WinSCP to transfer the langdata to
> Linux(I can make sure the langdata is the latest as @Shreeshrii
> <https://github.com/shreeshrii> let me check -- I re-git-cloned the
> latest langdata)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-332775119>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oylefg8kyUaUHb69luvSysm16aMxks5sm2C2gaJpZM4PfFg_>
> .
>
 @Shreeshrii  When I use the following:
<pre>
mkdir -p ~/tesstutorial/engoutput
training/lstmtraining --debug_interval 100 \
  --traineddata ~/tesstutorial/engtrain/eng/eng.traineddata \
  --net_spec '[1,36,0,1 Ct3,3,16 Mp3,3 Lfys48 Lfx96 Lrx96 Lfx256 O1c111]' \
  --model_output ~/tesstutorial/engoutput/base --learning_rate 20e-4 \
  --train_listfile ~/tesstutorial/engtrain/eng.training_files.txt \
  --eval_listfile ~/tesstutorial/engeval/eng.training_files.txt \
  --max_iterations 5000 &>~/tesstutorial/engoutput/basetrain.log
</pre>
It needs "engtrain" and "engeval". What's the difference between the two? And I found that the commands using to generate them looks likely the same except the --fontlist option:
<pre>
// engtrain
training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
  --noextract_font_properties --langdata_dir ../langdata \
  --tessdata_dir ./tessdata --output_dir ~/tesstutorial/engtrain

//eval data for the 'Impact' font:
training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
  --noextract_font_properties --langdata_dir ../langdata \
  --tessdata_dir ./tessdata \
  --fontlist "Impact Condensed" --output_dir ~/tesstutorial/engeval
</pre>

The "engtrain" likely will train all the fonts in /usr/share/fonts and the fonts assigned in training/language-specific.sh:
<pre>
CHI_SIM_FONTS=( \
    "AR PL UKai CN" \
    "AR PL UMing Patched Light" \
    "Arial Unicode MS" \
    "Arial Unicode MS Bold" \
    "WenQuanYi Zen Hei Medium" \
    )
</pre>
Here some of the CHI_SIM_FONTS that haven't been installed, so it cause some errors. Do I have to install all the CHI_SIM_FONTS? And I just want to know why we need "engtrain" and "engeval" ("chi_simtrain" and "chi_simeval") at the same time? engtrain - lists all LSTMF files that will be used for doing LSTM training
engeval - lists any LSTMF files that will be used for doing OCR evaluation
while training

Usually, your fontlist for training will be larger than the ones used for
eval



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Fri, Sep 29, 2017 at 3:51 PM, ivanzz1001 <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> When I use the following:
>
> mkdir -p ~/tesstutorial/engoutput
> training/lstmtraining --debug_interval 100 \
>   --traineddata ~/tesstutorial/engtrain/eng/eng.traineddata \
>   --net_spec '[1,36,0,1 Ct3,3,16 Mp3,3 Lfys48 Lfx96 Lrx96 Lfx256 O1c111]' \
>   --model_output ~/tesstutorial/engoutput/base --learning_rate 20e-4 \
>   --train_listfile ~/tesstutorial/engtrain/eng.training_files.txt \
>   --eval_listfile ~/tesstutorial/engeval/eng.training_files.txt \
>   --max_iterations 5000 &>~/tesstutorial/engoutput/basetrain.log
>
> It needs "engtrain" and "engeval". What's the difference between the two?
> And I found that the commands using to generate them looks likely the same
> except the --fontlist option:
>
> // engtrain
> training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
>   --noextract_font_properties --langdata_dir ../langdata \
>   --tessdata_dir ./tessdata --output_dir ~/tesstutorial/engtrain
>
> //eval data for the 'Impact' font:
> training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
>   --noextract_font_properties --langdata_dir ../langdata \
>   --tessdata_dir ./tessdata \
>   --fontlist "Impact Condensed" --output_dir ~/tesstutorial/engeval
>
> The "engtrain" likely will train all the fonts in /usr/share/fonts and the
> fonts assigned in training/language-specific.sh:
>
> CHI_SIM_FONTS=( \
>     "AR PL UKai CN" \
>     "AR PL UMing Patched Light" \
>     "Arial Unicode MS" \
>     "Arial Unicode MS Bold" \
>     "WenQuanYi Zen Hei Medium" \
>     )
>
> Here some of the CHI_SIM_FONTS that haven't been installed, so it cause
> some errors. Do I have to install all the CHI_SIM_FONTS? And I just want to
> know why we need "engtrain" and "engeval" at the same time?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-333090078>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o2r_vLqRjkZXVWgbGHE9s7dbZ8f-ks5snMSTgaJpZM4PfFg_>
> .
>
 Do I have to install all the fonts in CHI_SIM_FONTS(training/language-specific.sh):
<pre>
CHI_SIM_FONTS=( \
    "AR PL UKai CN" \
    "AR PL UMing Patched Light" \
    "Arial Unicode MS" \
    "Arial Unicode MS Bold" \
    "WenQuanYi Zen Hei Medium" \
    )
</pre> >Do I have to install all the fonts in CHI_SIM_FONTS(training/
language-specific.sh):

No, you can use whichever fonts that you want to train on. You can give
multiple fonts as part of the command with --fontlist

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Fri, Sep 29, 2017 at 4:33 PM, ivanzz1001 <notifications@github.com>
wrote:

> Do I have to install all the fonts in CHI_SIM_FONTS(training/
> language-specific.sh):
>
> CHI_SIM_FONTS=( \
>     "AR PL UKai CN" \
>     "AR PL UMing Patched Light" \
>     "Arial Unicode MS" \
>     "Arial Unicode MS Bold" \
>     "WenQuanYi Zen Hei Medium" \
>     )
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-333097787>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o69sN6dKBPFcAXnescSQKyDzcJo4ks5snM6fgaJpZM4PfFg_>
> .
>
 @Shreeshrii  @amitdo When I trained the chi_sim, how do I set the net_spec flags? 
<pre>
--net_spec '[1,36,0,1 Ct3,3,16 Mp3,3 Lfys48 Lfx96 Lrx96 Lfx256 O1c111]'
</pre>
what's the meaning?

And I execute the following command:
<pre>
training/combine_tessdata -d tessdata/chi_sim.traineddata 
Version string:4.00.00alpha:chi_sim:synth20170629:[1,48,0,1Ct3,3,16Mp3,3Lfys64Lfx96Lrx96Lfx512O1c1]
0:config:size=1966, offset=192
17:lstm:size=12152851, offset=2158
18:lstm-punc-dawg:size=282, offset=12155009
19:lstm-word-dawg:size=590634, offset=12155291
20:lstm-number-dawg:size=82, offset=12745925
21:lstm-unicharset:size=258834, offset=12746007
22:lstm-recoder:size=72494, offset=13004841
23:version:size=84, offset=13077335
</pre>
It seems that the net_spec  is [1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lfx512 O1c1], Here the last value is "O1c1"? (not O1c111?). It seems that the last two "1"  represent the network depth, so If I set it to O1c111, then the depth is 11, is the depth too deeper? 

Is It OK that I ask the question here? or anywhere else is proper?  >>It seems that the net_spec is [1,48,0,1 Ct3,3,16 Mp3,3 Lfys64 Lfx96 Lrx96
Lfx512 O1c1], Here the last value is "O1c1"? (not O1c111?)

That last number is a dummy. It is overridden by the number of characters
in the unicharset.



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Oct 11, 2017 at 5:44 PM, Amit D. <notifications@github.com> wrote:

> https://github.com/tesseract-ocr/tesseract/wiki/VGSLSpecs
>
> Is It OK that I ask the question here? or anywhere else is proper?
>
> The right place to ask this kind of question is our forum
> <https://groups.google.com/d/forum/tesseract-ocr>.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-335789942>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o3KfQoOIJxQ3lJ72LUdOMdfJJqOtks5srLEygaJpZM4PfFg_>
> .
>
  Similar problem

https://github.com/tesseract-ocr/tesseract/issues/590  Are you trying to build with cppan? Please, see this page for build instructions https://github.com/tesseract-ocr/tesseract/wiki/Compiling#windows What is your system version? Are you using windows 7?
 And what is your account name? Is it in russian? Windows user account. Do you use the latest cmake? It's ok to have manual fixes. But as you can see, windows build bots are fine. So there are some unusual things in your system setup. > so i just manually added all required libraries 

It was added during 3.05.

> so i just manually added all required libraries 

Cppan is developed to do this for you. Please, use tesseract user forum for asking support for your project.  Please explain, why the current master does apparently not work together with https://github.com/tesseract-ocr/tessdata_best

I got this: 
```
Failed loading language 'deu'
Tesseract couldn't load any languages!
Could not initialize tesseract.
ObjectCache(0x7f0a269c9800)::~ObjectCache(): WARNING! LEAK! object 0x1a51780 still has count 1 (id /usr/local/share/tessdata/deu.traineddatalstm-punc-dawg)
ObjectCache(0x7f0a269c9800)::~ObjectCache(): WARNING! LEAK! object 0x28c1b40 still has count 1 (id /usr/local/share/tessdata/deu.traineddatalstm-word-dawg)
ObjectCache(0x7f0a269c9800)::~ObjectCache(): WARNING! LEAK! object 0x28c2470 still has count 1 (id /usr/local/share/tessdata/deu.traineddatalstm-number-dawg)
```

 It does work with the old tessdata. @amitdo I simply replaced (exchanged) my existing .../tessdata with the the new .../tessdata_best. Tesseract does not work then.

What is changed, so that this fails?
 @amitdo Thanks, I do know the tessdata configuration possibilities. In my case the reason was, that I checked out tessdata_best from github, but forgot to copy /tessdata files and subdirectories from the tesseract code directory. I simply forgot this.

Perhaps this can be improved: separation (split) of /tessdata(config-files) and /tessdata(language) files. Do you understand, what I mean?

I opened this issue and close it now. I used ```oem=2``` !!!  Now finally, with ```oem=1``` it works. AFAIK, tessdata_best and tessdata_fast have only LSTM models in them, so
--oem 1 (LSTM) and --oem 3 (default) should work, as long as the config
file is not using --oem 0 or --oem 2.

If you want the legacy models, use tessdata.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Sep 20, 2017 at 1:19 PM, Wikinaut <notifications@github.com> wrote:

> Now finally, with oem=1 it works.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1143#issuecomment-330773170>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1SVwdL993fRJm_Sd2MF-6tdmFYdks5skMNxgaJpZM4Pc2bx>
> .
>
  ```
In file included from ambigs.h:26:0,
                 from ambigs.cpp:21:
unichar.h:164:10: error: 'string' does not name a type
   static string UTF32ToUTF8(const std::vector<char32>& str32);
          ^~~~~~
In file included from ambigs.h:27:0,
                 from ambigs.cpp:21:
unicharset.h:241:10: error: 'string' does not name a type
   static string CleanupString(const char* utf8_str) {
          ^~~~~~
unicharset.h:244:10: error: 'string' does not name a type
   static string CleanupString(const char* utf8_str, int length);
          ^~~~~~
unicharset.h: In member function 'void UNICHARSET::unichar_insert_backwards_compatible(const char*)':
unicharset.h:265:5: error: 'string' was not declared in this scope
     string cleaned = CleanupString(unichar_repr);
     ^~~~~~
unicharset.h:265:5: note: suggested alternatives:
In file included from /usr/include/c++/6/string:39:0,
                 from /usr/include/c++/6/stdexcept:39,
                 from /usr/include/c++/6/array:39,
                 from /usr/include/c++/6/tuple:39,
                 from /usr/include/c++/6/functional:55,
                 from helpers.h:30,
                 from genericvector.h:29,
                 from params.h:25,
                 from tprintf.h:23,
                 from ambigs.h:25,
                 from ambigs.cpp:21:
/usr/include/c++/6/bits/stringfwd.h:74:33: note:   'std::__cxx11::string'
   typedef basic_string<char>    string;
                                 ^~~~~~
/usr/include/c++/6/bits/stringfwd.h:74:33: note:   'std::__cxx11::string'
In file included from ambigs.h:27:0,
                 from ambigs.cpp:21:
unicharset.h:266:9: error: 'cleaned' was not declared in this scope
     if (cleaned != unichar_repr) {
         ^~~~~~~
Makefile:584: die Regel für Ziel „ambigs.lo“ scheiterte
make[3]: *** [ambigs.lo] Fehler 1
make[3]: Verzeichnis „/work/usr/local/src/tesseract/ccutil“ wird verlassen
Makefile:626: die Regel für Ziel „all-recursive“ scheiterte
make[2]: *** [all-recursive] Fehler 1
make[2]: Verzeichnis „/work/usr/local/src/tesseract/ccutil“ wird verlassen
Makefile:481: die Regel für Ziel „all-recursive“ scheiterte
make[1]: *** [all-recursive] Fehler 1
make[1]: Verzeichnis „/work/usr/local/src/tesseract“ wird verlassen
Makefile:389: die Regel für Ziel „all“ scheiterte
make: *** [all] Fehler 2
```
 I started from scratch ( .-/autogen.sh ; ./configure --enable-debug; make)  Seems to work. @stweil I apologize for my bug reports. But tesseract is a kind of PITA. When you want to build it after a while of not following the changes, building fails. Debian 9: make --> libtool:   error: '/usr/lib/x86_64-linux-gnu/libtiff.la' is not a valid libtool archive

sudo apt install libtiff-dev
libtiff5-dev ist schon die neueste Version (4.0.8-2+deb9u1).


 @stweil Stefan: please assist. Now we have a 'make' issue on debian9.  ...and the latest tesseract stable version is [3.05.01](https://github.com/tesseract-ocr/tesseract/releases) We do not provide support for 3rd party sw (e.g. python) => you need to be able replicate problem with C++ or C. 
I created some examples of using API in python, but it is a little bit tricky sometimes and without official support...  
Please use tesseract user forum instead (there are more people with more experiences)  Good idea.

Google's tests probably use testdata as the directory, so the submodule can be pulled in under that name. I can very easily imagine 175MB of test data.   Fixes issue https://github.com/tesseract-ocr/tesseract/issues/1114 @zdenop Stefan is right. 

I reverted this change and the training process is still working now. So, it must have been something else that changed in system for it to work.

Please revert. Thanks!  Thanks for the report. You are right, Orientation Script Detection (OSD) only works with the legacy recognizer. Discussing with Ray. The answer might be to delete osd.traineddata from tessdata_best and tessdata_fast repositories. Does this copy of osd.traineddata have the same problem, or not?

https://github.com/tesseract-ocr/tessdata/blob/3a94ddd47be01fd897cbe31f05cbd2301454cf8a/best/osd.traineddata 



/mnt/c/Users/User/shree# combine_tessdata -u osd.traineddata osd.
Extracting tessdata components from osd.traineddata
Wrote osd.lstm
Wrote osd.lstm-unicharset
Wrote osd.lstm-recoder
Wrote osd.version
Version string:4.00.00alpha:osd:synth20170629:[1,48,0,1Ct3,3,16Mp3,3Lfys64Lfx96Lrx96Lfx384O1c1]
17:lstm:size=7409547, offset=192
21:lstm-unicharset:size=7991, offset=7409739
22:lstm-recoder:size=715, offset=7417730
23:version:size=80, offset=7418445

https://github.com/tesseract-ocr/tessdata/blob/3a94ddd47be01fd897cbe31f05cbd2301454cf8a/best/osd.traineddata

only has the lstm components. See https://github.com/tesseract-ocr/tesseract/issues/1132

The osd.traineddata file from tessdata repo (uploaded in Nov 2016) has legacy components and works ok. As per discussion with Ray, I have taken osd.traineddata from tessdata and copied it to both tessdata_fast and tessdata_best. This should resolve the problem and is the best we can do today.  Some day in the future, hopefully, there will be an Orientation Script Dectector (OSD) that works with the LSTM recognizer.  Since there are now three possible locations of tessdata files,

```
https://github.com/tesseract-ocr/tessdata_best
https://github.com/tesseract-ocr/tessdata_fast
and
https://github.com/tesseract-ocr/tessdata
```

clarify the usage of TESSDATA_PREFIX and tessdata-dir.

Related: https://github.com/tesseract-ocr/tesseract/commit/e66d43390782f056b9be6e4aee4bf35c214a2f2d#diff-c2f87d92d6aa4f0f542b36a6e5c41161

+ * @param argv0 - paths to the directory with language files and config files.
 + * An actual value of argv0 is used if not NULL, otherwise TESSDATA_PREFIX is
 + * used if not NULL, next try to use compiled in -DTESSDATA_PREFIX. If previous
 + * is not sucessul - use current directory. >Should tessdata_best and tessdata_fast be Git submodules of tessdata

I remember reading a comment from @theraysmith why using best/eng etc will not work .. something to do with sublanguages being invoked via the config file. @stweil You raise some very good points, both about data redundancy and about git submodules. Ray told me he's going to think about over the weekend.    tesseract imagename|stdin outputbase|stdout [options...] [configfile...]

OCR options:
  --tessdata-dir PATH   Specify the location of tessdata path.
  --user-words PATH     Specify the location of user words file.
  --user-patterns PATH  Specify the location of user patterns file.
  -l LANG[+LANG]        Specify language(s) used for OCR.
  -c VAR=VALUE          Set value for config variables.
                        Multiple -c arguments are allowed.
  --psm NUM             Specify page segmentation mode.
  --oem NUM             Specify OCR Engine mode.
NOTE: These options must occur before any configfile.

```
 --tessdata-dir is  with hyphen, not underscore
```  eng and osd traineddata files are required for default functioning of tesseract.
Added option to download the files with --no-clobber option and copy to tessdata directory
Removed option related to langdata, as tesseract does not install all langdata files @stweil Please review and correct. Thanks! The URL for the traineddata files can be changed to use files from tessdata_fast repo, when they become available. >Currently Tesseract requires eng.traineddata even if someone only wants to get the Tesseract version or the list of available languages. I don't think that this is satisfying.
Tesseract functions which are unrelated to eng.traineddata should work without it.

I agree.

>I wonder whether it would be better to get one which supports both the old and the new recognizer (only as long as both are supported, of course).

You mean to use the original 4.0 traineddata rather than best. I will make that change. ```
make engosddata-install
wget  -nc https://github.com/tesseract-ocr/tessdata/raw/master/eng.traineddata
--2017-09-14 21:17:15--  https://github.com/tesseract-ocr/tessdata/raw/master/eng.traineddata
Resolving github.com (github.com)... 192.30.255.113, 192.30.255.112
Connecting to github.com (github.com)|192.30.255.113|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/tesseract-ocr/tessdata/master/eng.traineddata [following]
--2017-09-14 21:17:17--  https://raw.githubusercontent.com/tesseract-ocr/tessdata/master/eng.traineddata
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.0.133, 151.101.64.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 31873501 (30M) [application/octet-stream]
Saving to: ‘eng.traineddata’

100%[===============================================================================================================================>] 3,18,73,501 2.86MB/s   in 11s

2017-09-14 21:17:29 (2.72 MB/s) - ‘eng.traineddata’ saved [31873501/31873501]

wget  -nc https://github.com/tesseract-ocr/tessdata/raw/master/osd.traineddata
--2017-09-14 21:17:29--  https://github.com/tesseract-ocr/tessdata/raw/master/osd.traineddata
Resolving github.com (github.com)... 192.30.255.112, 192.30.255.113
Connecting to github.com (github.com)|192.30.255.112|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/tesseract-ocr/tessdata/master/osd.traineddata [following]
--2017-09-14 21:17:30--  https://raw.githubusercontent.com/tesseract-ocr/tessdata/master/osd.traineddata
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10562727 (10M) [application/octet-stream]
Saving to: ‘osd.traineddata’

100%[===============================================================================================================================>] 1,05,62,727 2.64MB/s   in 5.0s

2017-09-14 21:17:36 (2.03 MB/s) - ‘osd.traineddata’ saved [10562727/10562727]

cp   eng.traineddata osd.traineddata /usr/local/share/tessdata
``` @stweil 

Should this be included as part of the default `make` and `make install` for the project?

If so, tesseract/Makefile.am will also need a change. Debian package building uses 'make install' and network access is forbidden
during that operation.
 https://packages.ubuntu.com/zesty/tesseract-ocr

dependencies include:

tesseract-ocr-eng (>= 3.01~)
tesseract-ocr language files for English
tesseract-ocr-equ
tesseract-ocr language files for equations
tesseract-ocr-osd
tesseract-ocr language files for script and orientation @stweil We are changing how the official Debian packages work. I'm collaborating with Александр Поздняков who has already done a terrific job making a PPA. Debian Experimental is the work in progress for turning his PPA work into official packages. See links below for details. Any questions?

https://github.com/AlexanderP/dsc-file/tree/master/debian-tesseract-lang
https://github.com/AlexanderP/dsc-file/tree/master/debian-tesseract

"Is there a "standard" Makefile target to get files needed for the installation from the internet?" No. We package the traineddata files just like we package source code. There is no network activity at package build time. 

"Related: would it make sense to add the Debian rules to the Tesseract repository?" Maybe, maybe not, but there is enough going on right now that I don't want to even think about it. Priority is to get 4.x packages working and shipping to users.
 Jeff, 
Thanks for the info regarding packaging. Great to know that 4.x will be shipping soon.

Please see https://github.com/tesseract-ocr/tesseract/issues/1133 and 1132 - regarding the osd.traineddata files. Also see https://github.com/tesseract-ocr/tesseract/issues/1131
regarding TESSDATA_PREFIX and tessdata-dir

What will be the tessdata file structure under debian?
Will users be able to install both tesseract 3 and tesseract 4?

 _What will be the tessdata file structure under debian?_
Current: /usr/share/tesseract-ocr/tessdata
Upcoming: /usr/share/tesseract-ocr/4.00/tessdata/

_Will users be able to install both tesseract 3 and tesseract 4?_
Normal users will install Tesseract packages via apt-get. Their only  option will be Tesseract 4.x and the fast LSTM traineddata. The current Tesseract 3.x packages will be replaced. Advanced users can of course continue doing anything they want; building from source, choosing alternate traineddata files from github, attempting to install multiple Tesseracts at the same time, etc. Thanks for the info, Jeff.

I assume that training tools will be included in the debian package. If so, please see https://github.com/tesseract-ocr/tesseract/issues/1114.

Also note, https://github.com/tesseract-ocr/tessdata_fast/pull/3 regarding sanskrit traineddata.

You can check with Ray, whether the recommended action is just removing the config file vs. changing the default oem mode in it (since the other config variables probably only apply to legacy engine). @zdenop Should the datadir in github project be made in sync with the structure for debian?

>What will be the tessdata file structure under debian?
Current: /usr/share/tesseract-ocr/tessdata
Upcoming: /usr/share/tesseract-ocr/4.00/tessdata/ Packages have been submitted to Debian Experimental and are awaiting manual approval. These packages are effectively identical to Alexander's Personal Package Archive (PPA) `tesseract` and `tesseract-lang`.

DEBIAN
https://packages.debian.org/source/experimental/tesseract
https://ftp-master.debian.org/new/tesseract-lang_4.00~git15-45ed289-1.html

PPA
https://launchpad.net/~alex-p/+archive/ubuntu/tesseract-ocr
 @stweil We've been using tesseract-ocr as the package name for a very long time, and there is a pretty big installed base. I don't know how big the upgrade headache would be if we changed names. There is some time to consider this, as we wait for manual approval. I'm somewhat wary since I have managed to screw up every single Tesseract transition in Debian so far. Then again, it is probably now or never.

https://qa.debian.org/popcon.php?package=tesseract

The choice of `/usr/share/tesseract-ocr/4.00/tessdata/` was from Alexander and I suspect it is fairly arbitrary. I suspect we could do `/usr/share/tesseract-ocr/4/tessdata/` or `/usr/share/tesseract-ocr/4.0/tessdata/`. The only reason not to continue with  `/usr/share/tesseract-ocr/tessdata/` is as a courtesy to advanced users who wish to install Tessseract 3.x on the same system. Anyway, maybe try installing his PPA, see if you have any comments, and we can discuss with Alexander. Again there is some time as we wait for approval in NEW queue.

https://www.debian.org/doc/debian-policy/index.html#shared-library-support-files

 @jbreiden Jeff, Will you update the langdata repo with files with 4.0x too? Thanks!  Thanks, @stweil.  Similar Issue - https://github.com/tesseract-ocr/tesseract/issues/1015  @stweil Please make the required changes. Thanks!
  ### Environment

* **Tesseract Version**:  tesseract 4.00.00alpha (the latest)
* **Commit Number**:  don't know 
* **Platform**: Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux (CentOS 7.3.1611)

### Current Behavior:
I have a ticket image:
![ticket](https://user-images.githubusercontent.com/29993902/30318159-e18e242a-97de-11e7-8fd5-548f99ca9092.png)

then I use the following command:
<pre>
[root@localhost workspace]# /opt/tesseract4.0/bin/tesseract ticket.png out -l chi_sim
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Warning. Invalid resolution 0 dpi. Using 70 instead.
Estimating resolution as 182
[root@localhost workspace]# cat out.txt 
  
    

本 付 束 方 证 件 号 :
履 吉方 名 称 。， 硕 逢 区 大 良 大 条 和 饮食 店
司 收 吉方 证 件 号 ，440623197507202615
</pre>
But some strange things happened:  It seems that ```tesseract``` can only recognize the top half of the ticket，the down half just can't recognize。 Then I split the ticket into two images: ```ticket_top.png``` and ```ticket_down.png```，and ```tesseract``` still don't recognize the original down half(now that's ticket_down.png). 

the following image is ```ticket_down.png```:
![ticket_down](https://user-images.githubusercontent.com/29993902/30318744-8ace537e-97e0-11e7-994e-cfe516e2100c.png)



### Expected Behavior:

the top half and the down half of the ticket has the same image-features, It should almost have the same behavior.

### Suggested Fix: 
 try with other page segmentation modes.

with --psm 6 and an older version of the code, I get the following output

```
本 付 束 方 证 件 号 : 本 | |

和 “ 适 兴 台 号 |
司 收 吉方 证 件 号 ，440623197507202615 由 机

se 二 EC 二 日
二 谭 潮 3813.00 过

ee 引 记 大 条 可
三 合计 金 砚 大 写 ) : 佐 什 抽 佰 查 拼 元 束 全 色 - 出 后 吕 雪 二 |

全 ET 这
三 汪 村 人 , 各 衣 乐 人

GO 从 人 e
``` It seems that works, thank you   Thanks! I have it on my TODO list, but no time for it ;-)  It was an accidental overwrite. I have made a PR to fix it. https://github.com/tesseract-ocr/tesseract/pull/1118  @wareya thanks . I have same question . In a new best jpn.traineddata some times result text having double of char.  .
 example input : ABCDEFG -> output :ABCDEFFG.
Not many a little only but i can't understand what's wrong with new jpn.traineddata. I don't know but i think in the previous version of best traineddata [4.0] this bug not appear 
https://github.com/tesseract-ocr/tessdata/tree/4.00

 Similar issue - https://github.com/tesseract-ocr/tesseract/issues/1060  Ray is currently updating the repo with new code. You can try a commit from about a week back and check. >The following example shows the command line for training from scratch. Try it with the default training data created with the command-lines above.

Did you create the required traineddata by 

```
training/tesstrain.sh --fonts_dir /usr/share/fonts --lang eng --linedata_only \
  --noextract_font_properties --langdata_dir ../langdata \
  --tessdata_dir ./tessdata --output_dir ~/tesstutorial/engtrain
``` >mgr_.Init(traineddata_path.c_str()):Error:Assert failed:in file ../lstm/lstmtrainer.h, line 110

This error is caused by a missing traineddata  file given in the training command. There is another similar issue - https://github.com/tesseract-ocr/tesseract/issues/1075

It will be nice to have a more user-friendly message.

This issue can be closed and a link added to the other one ( more details in it)  tesseract 4.00.00dev-658-g3493785-2149
 leptonica-1.74.4
  libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8

 Found AVX
 Found SSE

---------------
Error while extracting unicharset

```
=== Phase UP: Generating unicharset and unichar properties files ===
[Sat Sep 9 16:08:44 DST 2017] /usr/local/bin/unicharset_extractor --output_unicharset /tmp/tmp.4s8doNdbQW/san_latn/san_latn.unicharset --norm_mode 1 /tmp/tmp.4s8doNdbQW/
san_latn/san_latn.Arial_Unicode_MS.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.FreeSerif.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.FreeSerif_Italic.exp0.box /tmp
/tmp.4s8doNdbQW/san_latn/san_latn.Sanskrit_2003.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.Siddhanta.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.Times_New_Roman_I
talic.exp0.box
Extracting unicharset from box file /tmp/tmp.4s8doNdbQW/san_latn/san_latn.Arial_Unicode_MS.exp0.box
Invalid Unicode codepoint: 0xffffffc3
IsValidCodepoint(ch):Error:Assert failed:in file normstrngs.cpp, line 225
ERROR: /tmp/tmp.4s8doNdbQW/san_latn/san_latn.unicharset does not exist or is not readable
``` gdb output

```
Starting program: /usr/local/bin/unicharset_extractor --output_unicharset /tmp/tmp.4s8doNdbQW/san_latn/san_latn.unicharset --norm_mode 1 /tmp/tmp.4s8doNdbQW/san_latn/san
_latn.Arial_Unicode_MS.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.FreeSerif.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.FreeSerif_Italic.exp0.box /tmp /tmp.4s8doN
dbQW/san_latn/san_latn.Sanskrit_2003.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.Siddhanta.exp0.box /tmp/tmp.4s8doNdbQW/san_latn/san_latn.Times_New_Roman_I talic.exp0
.box
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Extracting unicharset from box file /tmp/tmp.4s8doNdbQW/san_latn/san_latn.Arial_Unicode_MS.exp0.box
Invalid Unicode codepoint: 0xffffffc3
IsValidCodepoint(ch):Error:Assert failed:in file normstrngs.cpp, line 225

Program received signal SIGSEGV, Segmentation fault.
ERRCODE::error (this=this@entry=0x673748 <_ZL13ASSERT_FAILED>, caller=caller@entry=0x4384fe "IsValidCodepoint(ch)", action=action@entry=ABORT,
    format=format@entry=0x437f7a "in file %s, line %d") at errcode.cpp:86
86            if (!*p)
(gdb) backtrace
#0  ERRCODE::error (this=this@entry=0x673748 <_ZL13ASSERT_FAILED>, caller=caller@entry=0x4384fe "IsValidCodepoint(ch)", action=action@entry=ABORT,
    format=format@entry=0x437f7a "in file %s, line %d") at errcode.cpp:86
#1  0x000000000040812d in tesseract::IsWhitespace (ch=-61) at normstrngs.cpp:224
#2  0x0000000000403d31 in AddStringsToUnicharset (unicharset=0x7ffffffde140, norm_mode=1, strings=...) at unicharset_extractor.cpp:53
#3  tesseract::Main (argc=<optimized out>, argv=<optimized out>) at unicharset_extractor.cpp:78
#4  0x0000000000403206 in main (argc=<optimized out>, argv=<optimized out>) at unicharset_extractor.cpp:109
(gdb) frame 1
#1  0x000000000040812d in tesseract::IsWhitespace (ch=-61) at normstrngs.cpp:224
224       ASSERT_HOST_MSG(IsValidCodepoint(ch), "Invalid Unicode codepoint: 0x%x\n",
``` I am using the new version of scripts. I have tried with various languages.
Also checked on ubuntu 14.04 since earlier test was on WSL.

Still getting same error. ```
 training/tesstrain.sh \
>   --fonts_dir /home/shree/.fonts \
>   --tessdata_dir ../tessdata \
>   --langdata_dir ../langdata \
>   --lang eng  \
>   --linedata_only \
>   --noextract_font_properties \
>   --exposures "0"    \
>   --fontlist "FreeSerif" \
>   --output_dir ../tesstutorial/engtest

=== Starting training for language 'eng'
[Sat Sep 9 11:24:54 MDT 2017] /home/shree/local/bin/text2image --fonts_dir=/home/shree/.fonts --font=FreeSerif --outputbase=/tmp/font_tmp.2tspxPyW9m/sample_text.txt --text=/tmp/font_tmp.2tspxPyW9m/sample_text.txt --fontconfig_tmpdir=/tmp/font_tmp.2tspxPyW9m
Rendered page 0 to file /tmp/font_tmp.2tspxPyW9m/sample_text.txt.tif

=== Phase I: Generating training images ===
Rendering using FreeSerif
[Sat Sep 9 11:24:56 MDT 2017] /home/shree/local/bin/text2image --fontconfig_tmpdir=/tmp/font_tmp.2tspxPyW9m --fonts_dir=/home/shree/.fonts --strip_unrenderable_words --leading=32 --char_spacing=0.0 --exposure=0 --outputbase=/tmp/tmp.qQHsGWJjVT/eng/eng.FreeSerif.exp0 --max_pages=3 --font=FreeSerif --text=../langdata/eng/eng.training_text
Rendered page 0 to file /tmp/tmp.qQHsGWJjVT/eng/eng.FreeSerif.exp0.tif
Rendered page 1 to file /tmp/tmp.qQHsGWJjVT/eng/eng.FreeSerif.exp0.tif

=== Phase UP: Generating unicharset and unichar properties files ===
[Sat Sep 9 11:25:01 MDT 2017] /home/shree/local/bin/unicharset_extractor --output_unicharset /tmp/tmp.qQHsGWJjVT/eng/eng.unicharset --norm_mode 1 /tmp/tmp.qQHsGWJjVT/eng/eng.FreeSerif.exp0.box
Extracting unicharset from box file /tmp/tmp.qQHsGWJjVT/eng/eng.FreeSerif.exp0.box
Invalid Unicode codepoint: 0xffffffe2
IsValidCodepoint(ch):Error:Assert failed:in file normstrngs.cpp, line 225
ERROR: /tmp/tmp.qQHsGWJjVT/eng/eng.unicharset does not exist or is not readable
``` ```
bool IsWhitespace(const char32 ch) {
  ASSERT_HOST_MSG(IsValidCodepoint(ch), "Invalid Unicode codepoint: 0x%x\n",
                  ch);
  return u_isUWhiteSpace(static_cast<UChar32>(ch));
}
```
https://github.com/tesseract-ocr/tesseract/blob/df41eab6aa1726dfe6725284c94693dff6fa1eda/training/normstrngs.cpp#L225 @amitdo Are you able to reproduce this?

Am I the only one facing this issue? @hanikh 

Were you able to run the following command just now without any error?

```
training/tesstrain.sh \
--fonts_dir /usr/share/fonts 
--training_text ./langdata/ara/ara.training_text 
--langdata_dir ./langdata 
--tessdata_dir ./tessdata 
--lang ara 
--linedata_only 
--noextract_font_properties 
--exposures "0" 
--output_dir ~/tesstutorial/aratrain
``` > it seems that there was no problem with using training/tesstrain.sh.

OK. So must be something in my setup causing problem...

I will download a fresh copy of langdata and test again. Tried all fresh install of langdata and tesseract. Still same error.

I am wondering whether this is related to the version of the icu libraries on the system ...

```
    $(ICU_I18N_LIBS) $(ICU_UC_LIBS)
```

Is there a way to check the version of these being used? OK, so I changed the assert to tprintf - now I get a lot of error messages.

For eng,  unicharset is created.
For hin, just the header line is created in unicharset.

```

bool IsValidCodepoint(const char32 ch) {
  // In the range [0, 0xD800) or [0xE000, 0x10FFFF]
  return (static_cast<uinT32>(ch) < 0xD800) || (ch >= 0xE000 && ch <= 0x10FFFF);
}

bool IsWhitespace(const char32 ch) {
//  ASSERT_HOST_MSG(IsValidCodepoint(ch), "Invalid Unicode codepoint: 0x%x\n",
//                  ch);
tprintf("Invalid Unicode codepoint: 0x%x\n", ch);
  return u_isUWhiteSpace(static_cast<UChar32>(ch));
}
```

errors even for the sample text.

```

=== Starting training for language 'eng'
[Tue Sep 12 22:10:49 DST 2017] /usr/local/bin/text2image --fonts_dir=/mnt/c/Users/User/shree/.fonts --font=FreeSerif --outputbase=/tmp/font_tmp.0xN0n5Rd2b/sample_text.txt --text=/tmp/font_tmp.0xN0n5Rd2b/sample_text.txt --fontconfig_tmpdir=/tmp/font_tmp.0xN0n5Rd2b
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x65
Invalid Unicode codepoint: 0x78
Invalid Unicode codepoint: 0x74
Invalid Unicode codepoint: 0xa
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x65
Invalid Unicode codepoint: 0x78
Invalid Unicode codepoint: 0x74
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x65
Invalid Unicode codepoint: 0x78
Invalid Unicode codepoint: 0x74
Invalid Unicode codepoint: 0xa
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x65
Invalid Unicode codepoint: 0x78
Invalid Unicode codepoint: 0x74
Invalid Unicode codepoint: 0xa
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x65
Invalid Unicode codepoint: 0x78
Invalid Unicode codepoint: 0x74
Invalid Unicode codepoint: 0x54
Invalid Unicode codepoint: 0x65
Invalid Unicode codepoint: 0x78
Invalid Unicode codepoint: 0x74
Rendered page 0 to file /tmp/font_tmp.0xN0n5Rd2b/sample_text.txt.tif
```
Complete logfile is attached. 

Wondering why it works for @hanikh but not me.


[tesstrain.log.txt](https://github.com/tesseract-ocr/tesseract/files/1296720/tesstrain.log.txt)
 Another error report - https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/fqyYaav6vmk/M8xjpwhpBAAJ @hanikh For me, the change in https://github.com/tesseract-ocr/tesseract/pull/1134/files fixed it. But the PR has not been applied yet.

**## Update: That PR does not fix the problem.**

See https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-330147030 @stweil You are right. 

my PR https://github.com/tesseract-ocr/tesseract/pull/1134/files does not fix it.

There have been other reports of same error, so there is a bug somewhere that needs fix ...

https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/fqyYaav6vmk/M8xjpwhpBAAJ

https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-330022935 @hanikh @stweil

I am checking this again. As the error seems to have come back :-(

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Sep 18, 2017 at 11:27 AM, hanikh <notifications@github.com> wrote:

> @Shreeshrii but your PR removed my error. do I have to change it back.
> yesterday I updated my tesseract. Do I have to use a newer version?
>
> On Mon, Sep 18, 2017 at 9:51 AM, Shreeshrii <notifications@github.com>
> wrote:
>
> > @stweil <https://github.com/stweil> You are right.
> >
> > I reverted my change and training is still working now. So, it must have
> > been something else that caused the problem. my PR
> > https://github.com/tesseract-ocr/tesseract/pull/1134/files does not fix
> > it. I had removed some unneeded software and updated the system - maybe
> > that had something to do with it.
> >
> > However, there have been two other reports of same error -
> >
> > https://groups.google.com/forum/?utm_medium=email&utm_
> > source=footer#!msg/tesseract-ocr/fqyYaav6vmk/M8xjpwhpBAAJ
> >
> > #1114 (comment)
> > <https://github.com/tesseract-ocr/tesseract/issues/1114#
> issuecomment-330022935>
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/1114#
> issuecomment-330131339>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/
> AZFiAejV1sTPtVIC9dvSv9JkOIuiAtiVks5sjf3ygaJpZM4PR9Tr>
> > .
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-330134312>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_ows2dIhgdfdfiVQE3wPzTf4AVclBks5sjgYygaJpZM4PR9Tr>
> .
>
 Is there a way to display the values of ch with different types?

I had tried tprintf , with different format strings. But, it seemed to skip
the assert at that time.

On 18-Sep-2017 12:14 PM, "Stefan Weil" <notifications@github.com> wrote:

> That's a trick: return (ch >= 0 && ch < 0xD800) || (ch >= 0xE000 && ch <=
> 0x10FFFF)would also implement the test for a valid code point. The static
> cast of the signed ch to an unsigned value saves the ch >= 0 test.
>
> A static cast to a negative value changes nothing and could be omitted as
> well. Maybe we should use the explicit code and leave optimisations to the
> compiler.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-330139437>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o8JheRL7HqvhCN4uveD0WJtsZDDsks5sjhExgaJpZM4PR9Tr>
> .
>
 Thanks, @stweil.

Yes, now it shows only the invalid codepoints.

```
=== Phase UP: Generating unicharset and unichar properties files ===
[Mon Sep 18 13:38:23 DST 2017] /usr/local/bin/unicharset_extractor --output_unicharset /tmp/tmp.xtcPbHHD4J/deu/deu.unicharset --norm_mode 1 /tmp/tmp.xtcPbHHD4J/deu/deu.A
rial.exp0.box
Extracting unicharset from box file /tmp/tmp.xtcPbHHD4J/deu/deu.Arial.exp0.box
Invalid Unicode codepoint: -62 = 0xffffffc2
Invalid Unicode codepoint: -17 = 0xffffffef
Invalid Unicode codepoint: -61 = 0xffffffc3
Invalid Unicode codepoint: -61 = 0xffffffc3
Invalid Unicode codepoint: -62 = 0xffffffc2
Invalid Unicode codepoint: -61 = 0xffffffc3
Invalid Unicode codepoint: -30 = 0xffffffe2
Invalid Unicode codepoint: -61 = 0xffffffc3
Invalid Unicode codepoint: -61 = 0xffffffc3
Invalid Unicode codepoint: -61 = 0xffffffc3
Invalid Unicode codepoint: -30 = 0xffffffe2
Invalid Unicode codepoint: -30 = 0xffffffe2
``` @stweil Please see https://github.com/tesseract-ocr/tesseract/issues/1147#issuecomment-331604552 That patch does not compile for me. I guess that's why @ivanzz1001 has not submitted to tesseract 

```
        g++ -DHAVE_CONFIG_H -I. -I..  -O2 -DNDEBUG -DUSE_STD_NAMESPACE -DPANGO_ENABLE_ENGINE -I../ccmain -I../api -I../ccutil -I../ccstruct -I../lstm -I../arch -I../view
er -I../textord -I../dict -I../classify -I../display -I../wordrec -I../cutil  -I/usr/local/include/leptonica   -pthread -I/usr/include/pango-1.0 -I/usr/include/glib-2.0
-I/usr/lib/x86_64-linux-gnu/glib-2.0/include   -I/usr/include/cairo -I/usr/include/glib-2.0 -I/usr/lib/x86_64-linux-gnu/glib-2.0/include -I/usr/include/pixman-1 -I/usr/i
nclude/freetype2 -I/usr/include/libpng12    -g -O2 -std=c++11 -MT unicharset_extractor.o -MD -MP -MF $depbase.Tpo -c -o unicharset_extractor.o unicharset_extractor.cpp &
&\
        mv -f $depbase.Tpo $depbase.Po
unicharset_extractor.cpp: In function ‘void tesseract::AddStringsToUnicharset(const GenericVector<STRING>&, int, UNICHARSET*)’:
unicharset_extractor.cpp:54:57: error: invalid conversion from ‘char’ to ‘const char*’ [-fpermissive]
         if (normed.empty() || IsUTF8Whitespace(normed[0])) continue;
                                                         ^
In file included from unicharset_extractor.cpp:29:0:
normstrngs.h:82:6: note:   initializing argument 1 of ‘bool tesseract::IsUTF8Whitespace(const char*)’
 bool IsUTF8Whitespace(const char* text);
      ^~~~~~~~~~~~~~~~
make[1]: *** [unicharset_extractor.o] Error 1
make[1]: Leaving directory `/mnt/c/Users/User/shree/tesseract-HEAD/training'
make: *** [training] Error 2
``` Thanks. That seems to work. No error messages or asserts.

@hanikh Please check with this new PR.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, Sep 24, 2017 at 12:12 PM, Amit D. <notifications@github.com> wrote:

> @ivanzz1001 <https://github.com/ivanzz1001> sent a PR which includes my
> above fix: #1153 <https://github.com/tesseract-ocr/tesseract/pull/1153>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-331690886>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_owVyIGWwgmhW21eGta5els6ONV4Fks5slfnZgaJpZM4PR9Tr>
> .
>
 @hanikh

Ray @theraysmith is the correct person to suggest the appropriate commands
for training Arabic and Farsi.

I would say, that since you have a new set of lstmf files and starter
traineddata, experiment with both,

Fine-tune plus-minus command
And
Replace top layer.

Check the wiki for the syntax and make sure that you use the correct file
names and path.

Though, Ray recommended 3600 or so iterations, I think that number is
suitable only for Latin based scripts.


For complex scripts, I have tested with Devanagari, I have to go up to
10000-20000 iteration to get error rate down to 1%.

Will be good if @theraysmith can confirm.

On 03-Oct-2017 11:54 AM, "hanikh" <notifications@github.com> wrote:

> @Shreeshrii
> Now, can I train a new model for Arabic and Farsi using "replacing a few
> layers"?
> I want to replace the last layer and these are the commands I'm going to
> use for farsi (I am using about 250 lines 13 fonts most of them are new)
>
> combine_tessdata -e tessdata/fas.traineddata \
> ~/tesstutorial/newfas_from_fas/fas.lstm
>
>
>
> lstmtraining --old_traineddata ./tessdata/fas.traineddata \
> --continue_from ~/tesstutorial/newfas_from_fas/fas.lstm \
> --traineddata ~/tesstutorial/fastrain/fas/fas.traineddata \
> --append_index 5 --net_spec '[Lfx192]'\
> --model_output ~/tesstutorial/newfas_from_fas/base \
> --train_listfile ~/tesstutorial/fastrain/fas.training_files.txt \
> --max_iterations 3000
>
> On Sun, Sep 24, 2017 at 11:36 AM, Stefan Weil <notifications@github.com>
> wrote:
>
> > No error messages or asserts does not necessarily mean that the new code
> > works. See my comment
> > <https://github.com/tesseract-ocr/tesseract/pull/1153#
> discussion_r140649990>
> > for the pull request.
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/1114#
> issuecomment-331694399>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/
> AZFiAW3U9dW4A0ajTbTt1Jd4ioaxiMVhks5slg2fgaJpZM4PR9Tr>
> > .
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-333751166>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o67_mlxJ8qCoRQGsmWn9E5awVAFvks5sodMfgaJpZM4PR9Tr>
> .
>
 PR : #1153 has multiple commits. Make sure you have the correct one.

Also, this has not been reviewed by @theraysmith so we don't know whether this is the recommended fix for the problem. You have to wait for Ray to update the code.


See
https://github.com/tesseract-ocr/tesseract/search?q=Logistic+output+not+implemented+yet%21&type=Code&utf8=%E2%9C%93

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Oct 4, 2017 at 2:54 PM, hanikh <notifications@github.com> wrote:

> I changed line 53 of unicharset_extractor.cpp to:
> if (normed.empty() || IsUTF8Whitespace(normed.c_str())) continue;
>
> unfortunately, the error still exists
>
> On Wed, Oct 4, 2017 at 12:31 PM, Hanieh Khosravi <hani.khosravi@gmail.com>
> wrote:
>
>
> > @Shreeshrii
> > I used the mentioned commands for replacing top layer and error
> rate=0.01,
> > it is running for about a day and I am just getting this:
> >
> > Logistic output not implemented yet!
> >
> > would you please help me find the problem
> >
> > On Tue, Oct 3, 2017 at 3:58 PM, Shreeshrii <notifications@github.com>
> > wrote:
> >
> >> PR : #1153 <https://github.com/tesseract-ocr/tesseract/pull/1153> has
> >> multiple commits. Make sure you have the correct one.
> >>
> >> Also, this has not been reviewed by @theraysmith
> >> <https://github.com/theraysmith> so we don't know whether this is the
> >> recommended fix for the problem.
> >>
> >> —
> >> You are receiving this because you were mentioned.
> >> Reply to this email directly, view it on GitHub
> >> <https://github.com/tesseract-ocr/tesseract/issues/1114#
> issuecomment-333826072>,
> >> or mute the thread
> >> <https://github.com/notifications/unsubscribe-auth/
> AZFiARRkhp11pQ3p26F6g4mYc6l-0zKmks5soihfgaJpZM4PR9Tr>
> >> .
> >>
> >
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1114#issuecomment-334099163>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o-BKtIfpc1wBFZ7n_NaI1v8nbBUqks5so07ngaJpZM4PR9Tr>
> .
>
 I tried testing with a one line english training text which has characters that get error

```
e ¢ “ I °  © ® ﬂ
```

Here are the modified debug messages that  I get:

```

Extracting unicharset from box file /tmp/tmp.ou9ZKjVaiJ/eng/eng.Arial.exp0.box
Normalization strings DEBUG 'e'
Normalization normed DEBUG 'e'
Normalization strings DEBUG '¢'
Normalization normed DEBUG '¢'
Invalid Unicode codepoint: -62 = 0xffffffc2
Normalization strings DEBUG '“'
Normalization normed DEBUG '“'
Invalid Unicode codepoint: -30 = 0xffffffe2
Normalization strings DEBUG 'I'
Normalization normed DEBUG 'I'
Normalization strings DEBUG '°'
Normalization normed DEBUG '°'
Invalid Unicode codepoint: -62 = 0xffffffc2
Normalization strings DEBUG '©'
Normalization normed DEBUG '©'
Invalid Unicode codepoint: -62 = 0xffffffc2
Normalization strings DEBUG '®'
Normalization normed DEBUG '®'
Invalid Unicode codepoint: -62 = 0xffffffc2
Normalization strings DEBUG 'ﬂ'
Normalization normed DEBUG 'ﬂ'
Invalid Unicode codepoint: -17 = 0xffffffef
```

Using https://r12a.github.io/apps/conversion/

0x... notation

```
0x65 0x20 0xA2 0x20 0x201C 0x20 0x49 0x20 0xB0 0x20 0x20 0xA9 0x20 0xAE 0x20 0xFB02
```

But the UTF-8 codepoints have some extra values...

```
65 20 C2 A2 20 E2 80 9C 20 49 20 C2 B0 20 20 C2 A9 20 C2 AE 20 EF AC 82
```

This C2, E2, EF etc are showing up as invalid codepoints
 Thanks! Closing the issue.  Thanks! Merged.  What language are you trying to train?

Have you checked whether new trained data is available for it in teesara
repository?

You should try with a newer version of tesseract. Look for tesseract OCR
ppa by alex.



On 06-Sep-2017 8:50 PM, "yazhe wang" <notifications@github.com> wrote:

> Before you submit an issue, please review the guidelines for this
> repository
> <https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md>.
>
> Please report an issue only for a BUG, not for asking questions.
>
> Note that it will be much easier for us to fix the issue if a test case
> that
> reproduces the problem is provided. Ideally this test case should not have
> any
> external dependencies. Provide a copy of the image or link to files for
> the test case.
>
> Please delete this text and fill in the template below.
> ------------------------------
> Environment
>
>    - *Tesseract Version*:
>
> tesseract 3.04.01
>  leptonica-1.73
>   libgif 5.1.2 : libjpeg 8d (libjpeg-turbo 1.4.2) : libpng 1.2.54 : libtiff 4.0.6 : zlib 1.2.8 : libwebp 0.4.4 : libopenjp2 2.1.0
>
>
>    -
>
>    *Commit Number*:
>    -
>
>    Install it from ubuntu apt. sudo apt install tesseract-ocr
>    -
>
>    *Platform*:
>
> Linux jeneser-X555LF 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
>
> Current Behavior:
>
> When run the following command. An error has occurred.
>
> cntraining lang.fontname.exp0.tr lang.fontname.exp1.tr ...
>
> BUT, continue running commands, all is successfully.
>
> My console:
>
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ tesseract hpu.font.exp0.tif hpu.font.exp0 box.train
> Tesseract Open Source OCR Engine v3.04.01 with Leptonica
> Page 1
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 2
> Empty page!!
> Page 3
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 4
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 5
> Empty page!!
> Page 6
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 7
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 2 words
> Page 8
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 9
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 10
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 11
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 12
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 13
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
>    Leaving 2 unlabelled blobs in 0 words.
> Generated training data for 1 words
> Page 14
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 15
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 16
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 17
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 18
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 19
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 20
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 21
> FAIL!
> APPLY_BOXES: boxfile line 1/q ((0,0),(15,20)): FAILURE! Couldn't find a matching blob
> FAIL!
> APPLY_BOXES: boxfile line 2/y ((15,0),(30,20)): FAILURE! Couldn't find a matching blob
> FAIL!
> APPLY_BOXES: boxfile line 3/j ((30,0),(45,20)): FAILURE! Couldn't find a matching blob
> FAIL!
> APPLY_BOXES: boxfile line 4/h ((45,0),(60,20)): FAILURE! Couldn't find a matching blob
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Boxes failed resegmentation:       4
> APPLY_BOXES: Unlabelled word at :Bounding box=(0,0)->(60,20)
>    Found 0 good blobs.
>    1 remaining unlabelled words deleted.
> Generated training data for 0 words
> Page 22
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 23
> FAIL!
> APPLY_BOXES: boxfile line 1/h ((3,0),(17,20)): FAILURE! Couldn't find a matching blob
> FAIL!
> APPLY_BOXES: boxfile line 2/t ((17,0),(31,20)): FAILURE! Couldn't find a matching blob
> FAIL!
> APPLY_BOXES: boxfile line 3/m ((31,0),(45,20)): FAILURE! Couldn't find a matching blob
> FAIL!
> APPLY_BOXES: boxfile line 4/X ((45,0),(59,20)): FAILURE! Couldn't find a matching blob
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Boxes failed resegmentation:       4
> APPLY_BOXES: Unlabelled word at :Bounding box=(3,0)->(60,20)
>    Found 0 good blobs.
>    1 remaining unlabelled words deleted.
> Generated training data for 0 words
> Page 24
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
>    Leaving 2 unlabelled blobs in 0 words.
> Generated training data for 2 words
> Page 25
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 26
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 27
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 28
> row xheight=8.66667, but median xheight = 10
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 29
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> Page 30
> APPLY_BOXES:
>    Boxes read from boxfile:       4
>    Found 4 good blobs.
> Generated training data for 1 words
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ unicharset_extractor hpu.font.exp0.box
> Extracting unicharset from hpu.font.exp0.box
> Wrote unicharset file ./unicharset.
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ mftraining -F font_properties -U unicharset -O hpu.unicharset hpu.font.exp0.tr
> Warning: No shape table file present: shapetable
> Failed to load font_properties from font_properties
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ mftraining -F font_properties -U unicharset -O hpu.unicharset hpu.font.exp0.tr
> Warning: No shape table file present: shapetable
> Reading hpu.font.exp0.tr ...
> Flat shape table summary: Number of shapes = 45 max unichars = 1 number with multiple unichars = 0
> Warning: no protos/configs for Joined in CreateIntTemplates()
> Warning: no protos/configs for |Broken|0|1 in CreateIntTemplates()
> Warning: no protos/configs for j in CreateIntTemplates()
> Done!
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ cntraining hpu.font.exp0.tr
> Reading hpu.font.exp0.tr ...
> Clustering ...
> Clustering error: Matrix inverse failed with error 1.44922
>
> Writing normproto ...
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ cntraining hpu.font.exp0.tr
> Reading hpu.font.exp0.tr ...
> Clustering ...
> Clustering error: Matrix inverse failed with error 1.44922
>
> Writing normproto ...
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ mv normproto hpu.normproto
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ mv inttemp hpu.inttemp
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ mv pffmtable hpu.pffmtable
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ mv shapetable hpu.shapetable
> jeneser@jeneser-X555LF:~/workSpace/fsociety-hpu/training/training-it$ combine_tessdata hpu.
> Combining tessdata files
> TessdataManager combined tesseract data files.
> Offset for type  0 (hpu.config                ) is -1
> Offset for type  1 (hpu.unicharset            ) is 140
> Offset for type  2 (hpu.unicharambigs         ) is -1
> Offset for type  3 (hpu.inttemp               ) is 2644
> Offset for type  4 (hpu.pffmtable             ) is 331290
> Offset for type  5 (hpu.normproto             ) is 331653
> Offset for type  6 (hpu.punc-dawg             ) is -1
> Offset for type  7 (hpu.word-dawg             ) is -1
> Offset for type  8 (hpu.number-dawg           ) is -1
> Offset for type  9 (hpu.freq-dawg             ) is -1
> Offset for type 10 (hpu.fixed-length-dawgs    ) is -1
> Offset for type 11 (hpu.cube-unicharset       ) is -1
> Offset for type 12 (hpu.cube-word-dawg        ) is -1
> Offset for type 13 (hpu.shapetable            ) is 337236
> Offset for type 14 (hpu.bigram-dawg           ) is -1
> Offset for type 15 (hpu.unambig-dawg          ) is -1
> Offset for type 16 (hpu.params-model          ) is -1
> Output hpu.traineddata created successfully.
>
> Expected Behavior: Suggested Fix:
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1111>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o14vm_v979ohg3RUr43o1ilPxW6Rks5sfrg0gaJpZM4POipi>
> .
>
  Can you please provide user case that demonstrate problem&solution? @zdenop Please see https://github.com/tesseract-ocr/tesseract/issues/781 for user case that demonstrate problem&solution?

This is a problem only with the master branch/4.00.00alpha.  I'm testing with an invoice form image in Japanese with psm =11 ,4 and 6.
Psm = 11 gave best OCR result but some time missing word in low width column (have only one or two char in column).
I have put a red rectangle to missing word place .

Input Image :
![sample_01 jpg_binary](https://user-images.githubusercontent.com/22963463/30015811-55992340-918d-11e7-8bde-87da28ba82b7.jpg)

And OCR Result :
[psm4.txt](https://github.com/tesseract-ocr/tesseract/files/1273919/psm4.txt)
[psm6.txt](https://github.com/tesseract-ocr/tesseract/files/1273917/psm6.txt)
[psm11.txt](https://github.com/tesseract-ocr/tesseract/files/1273918/psm11.txt)

Somebody can tell me how to improve this?
Thanks so much!
 @zmwang-GitHub  thanks ! . Maybe i will try in opencv ,but what's --PSM (character) is ? what is psm are you using in ocr process?
You using opencv to find coordinate and using api->SetRectangle() ?
 But if  using  api->SetRectangle() and psm =10 we need do ocr process for every char and may be very slow  Hi,

Check if you're running the latest cppan client.
Remove your `tesseract/build` dir and re-create it again.
If this does not help, delete `c:\users\u\.cppan\storage\tmp` and re-run your commands.
If this does not help, remove the whole storage and try again.  https://github.com/tesseract-ocr/tesseract/tree/master/testing

has a number of images provided for testing.

Please add the corresponding groundtruth files also. https://github.com/tesseract-ocr/tesseract/pull/1088

adds groundtruth for phototest.tif and eurotext.tif. @amitdo @jbreiden Please add groundtruth for the hebrew images, if available. Thanks! @amitdo Ok. Thanks! 

Closing this.  How are you viewing the output?

Try notepad++ instead of notepad. It is related to the handling of `end of line` marks

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, Sep 3, 2017 at 8:08 PM, gluehbire <notifications@github.com> wrote:

> works. strange - but it works. thanks
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1105#issuecomment-326808862>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o75g-44C71QSQOvDYrLLhdJupn2aks5sernggaJpZM4PLLdp>
> .
>
  https://github.com/tesseract-ocr/tesseract/blob/master/tessdata/configs/lstm.train

make sure it is there under configs folder in your tessdata folder.  Duplicate to #1087  Please follow [the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md).  > I built tesseact out of 4.00.00dev

Please try a build with the latest code in github.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, Aug 26, 2017 at 6:47 AM, jbreiden <notifications@github.com> wrote:

> Try running Tesseract under strace and see where things first become
> different.
>
> gs -q -dNOPAUSE -sDEVICE=tiffg4 -sOutputFile=- demo.pdf -c quit  |strace  bin/tesseract stdin demo.ocr -oem 4
> cat demo.tif | strace bin/tesseract stdin demo.ocr -oem 4
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1096#issuecomment-325068909>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o32jwkGq1CEy1wtvKmiRYuN2HEdHks5sb3I4gaJpZM4PCs49>
> .
>
  Thanks @stweil.

On 21-Aug-2017 2:04 PM, "Stefan Weil" <notifications@github.com> wrote:

> Related: 6773e8b
> <https://github.com/tesseract-ocr/tesseract/commit/6773e8b909d7409f7434db67da6dff56090a7eda>.
> @Shreeshrii <https://github.com/shreeshrii>, that commit is no longer
> needed after this pull request.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/1092#issuecomment-323681345>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o2j1sGBa1CopPUmse_sJBknn6J8Pks5saUERgaJpZM4O8_3M>
> .
>
  please read other issues (also closed) before writing new issue.  @theraysmith @stweil 

Please check unittests after applying this PR.

```
autoreconf -fiv
git submodule update --init
export TESSDATA_PREFIX=/prefix/to/path/to/tessdata
make check
``` Also needs updates to Makefile.am Configure.ac in tesseract dir. Will update PR. ```
Making check in unittest
make[1]: Entering directory `/mnt/c/Users/User/shree/tesseract/unittest'
make  libgtest.la libgtest_main.la apiexample_test tesseracttests matrix_test
make[2]: Entering directory `/mnt/c/Users/User/shree/tesseract/unittest'
make[2]: `libgtest.la' is up to date.
make[2]: `libgtest_main.la' is up to date.
make[2]: `apiexample_test' is up to date.
make[2]: `tesseracttests' is up to date.
make[2]: `matrix_test' is up to date.
make[2]: Leaving directory `/mnt/c/Users/User/shree/tesseract/unittest'
make  check-TESTS
make[2]: Entering directory `/mnt/c/Users/User/shree/tesseract/unittest'
make[3]: Entering directory `/mnt/c/Users/User/shree/tesseract/unittest'
PASS: apiexample_test
PASS: tesseracttests
PASS: matrix_test
make[4]: Entering directory `/mnt/c/Users/User/shree/tesseract/unittest'
make[4]: Nothing to be done for `all'.
make[4]: Leaving directory `/mnt/c/Users/User/shree/tesseract/unittest'
============================================================================
Testsuite summary for tesseract 4.00.00dev
============================================================================
# TOTAL: 3
# PASS:  3
# SKIP:  0
# XFAIL: 0
# FAIL:  0
# XPASS: 0
# ERROR: 0
============================================================================
make[3]: Leaving directory `/mnt/c/Users/User/shree/tesseract/unittest'
make[2]: Leaving directory `/mnt/c/Users/User/shree/tesseract/unittest'
make[1]: Leaving directory `/mnt/c/Users/User/shree/tesseract/unittest'
``` >Are all those parts of AM_CPPFLAGS necessary, or did you add them because they might be needed by future tests? Personally I prefer a little different syntax because I think it is easier to maintain:

Ray had referred to the makefile in training and I copied that section from there. I will change it to one per line. The following directions work for testing the apiexample.

```
autoreconf -fiv
git submodule update --init
export TESSDATA_PREFIX=/prefix/to/path/to/tessdata
make check
```

It is possible to change makefile.am and configure.ac to test for the program using pkg-config - please see https://github.com/google/googletest/blob/master/googletest/docs/Pkgconfig.md#autotools  Please provide user case where this is problem  Thanks for the report!
Please download new cppan client (or run `cppan --self-upgrade`) and try to build tesseract again.
(Run `cppan` in tess folder, re-run cmake and try to build in VS.) I check this with vs2017 on windows 10 64bit and it works.  ### Environment

* **Tesseract Version**: Latest / cloned from master
* **Platform**: Windows 10 x64 with VS2017

### Current Behavior:

The compiling section does not explain how to compile under Windows without OpenMP (neither under Linux mind you). How can it be done?

### Expected Behavior:

The compiling section of the wiki should include such informations as they are quite important if the binaries are to be used in a heavily threaded environnement.

### Suggested Fix:

For Linux, add a line in the wiki about using `./configure --disable-openmp` flag (although you can simply use `./configure --help`)

For Windows, add a line explaining how it can be done (I haven't figured out that one yet)

I am more than willing to make a PR for this if you consider it necessary, if not, **how can it be achieved under Windows?**
 @stweil Thank you for the quick response. I am using MSVC. Any parameter for that one? Also how can I set `OMP_THREAD_LIMIT` in Windows?

Doing `SET OMP_THREAD_LIMIT=1` then calling `tesseract.exe` works  @TheSeiko Please try with the changes suggested in https://github.com/tesseract-ocr/tesseract/issues/681#issuecomment-303027906 to see if you get improved recognition of these words without impacting others.
  Please report issue in English  use --oem 1 for the newer lstm engine

use the newly uploaded tessdata/best/chi_sim.traineddata

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, Aug 13, 2017 at 2:26 PM, 一路向北 <notifications@github.com> wrote:

> The original image
> https://image.ibb.co/fEkAma/WX20170813_165113_2x.png
>
> tesseract WX20170813_165113_2x.png stdout
> IH640 done
> ﬁﬁiaiﬂﬁma done
> iiiéimﬁiiﬁﬁ done
> ﬁlm: done
> ﬁEEE done
>
> tesseract WX20170813_165113_2x.png stdout -l chi_sim
> 图片640 done
> 每页记录数可配 done
> 选择城市报错 done
> 地图检索 done
> 弹囡宽度 done
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1078>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o9GpuCNcFLMtQ-Hrn-wYZo0AgWvaks5sXrpMgaJpZM4O1oin>
> .
>
 Use the traineddata from
https://github.com/tesseract-ocr/tessdata/tree/master/best

You can try

https://github.com/tesseract-ocr/tessdata/blob/master/best/HanS.traineddata
and
https://github.com/tesseract-ocr/tessdata/blob/master/best/chi_sim.traineddata

Make sure you have tesseract from latest sources in github



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, Aug 13, 2017 at 2:36 PM, 一路向北 <notifications@github.com> wrote:

> tesseract WX20170813_165113_2x.png stdout -l chi_sim
> read_params_file: parameter not found:
>
> Do I need to upgrade tessdata? @Shreeshrii <https://github.com/shreeshrii>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1078#issuecomment-322030636>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o48H2mkS8HqE0pNTkex27nFZYHuTks5sXryggaJpZM4O1oin>
> .
>
  Are you using the latest source from github for building tesseract? What is your version info, git log info? Also see https://github.com/tesseract-ocr/tesseract/issues/1069
regarding 'Failed to continue from' error That means you have new characters in your training text, so fine tuning
may not work.

On 14-Aug-2017 4:08 PM, "iuriigalaida" <notifications@github.com> wrote:

> After I get latest sourses v4 lstmtraining started work but I almost
> always got errors like
> Encoding of string failed!
> Can't encode transcription:
> Even if I directly set path to unicharset file.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1075#issuecomment-322157912>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_ox6hWfOOzKYP7lTVLo-hmT6EzaRFks5sYCOsgaJpZM4O0iiS>
> .
>
 Unicharset was not attached - 

see https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#training-just-a-few-layers

Have you tried Latin.traineddata - it will have more characters than eng in it. Thanks for the unicharset. I notice it has accented english letters.

1. Test with Latin.traineddata - which is for Latin script not Latin
language (lat).

2. Check that all your additional characters are in best/Latin.traineddata
- use combine_tessdata -u to unpack the traineddata and look at its
unicharset.

I find it easy to sort a copy of the unicharsets and then compare .

3. If Latin.traineddata has all characters you want, then you can do
finetune using Latin.traineddata as the continue_from rather than English.
 Re: 'mgr_.Init(traineddata_path.c_str()):Error:Assert failed:in file ../../../../lstm/lstmtrainer.h, line 110' 

This comes when the path to the traineddata files is incorrect. Check file locations and correct the command for training.  At the moment I have a limited internet access. If you make a pull request I can merge it ;.-)  It would be feasible to add bold and italic attributes by making them a separate output from the model. 
Underline would also be possible.
All these attributes would require changes to the rendering pipeline, and datapath for the ground truth.
Fixed-pitch(monospace), serif and smallcaps would be much more difficult, due to lack of reliable data available for the fonts. It could be possible to re-use the existing fontinfo table for that.
I wouldn't rule it out as impossible, but I will add this request to my list of stoppers for obsoleting the old engine.
I have a bunch of updates to push, which I didn't quite get to before my office move... Yes of course. Just re-order the code in WordFontAttributes. >It would be feasible to add bold and italic attributes by making them a separate output from the model. Underline would also be possible.

You could also take bold/italic into account when people use multiple languages for recognition, because many times the words in the additional language may be emphasized with bold or italics..

For an example, see the image in https://github.com/tesseract-ocr/langdata/pull/4#issuecomment-327760269 where Roman transliteration of Hindi is italicized with English text.

 >it seems to give you font size in the line level

While that would work in most cases, what of an extreme case of text of different size being on the same line - eg. http://www.teach-ict.com/programming/html/intro/step17a.jpg That has always been a problem.
The old code would often output garbage.
The LSTM engine will split the line at such words and recognize them separately, pasting the results back together. It doesn\t give an estimate of the x-height though. The overall accuracy on such images is better though. @theraysmith Please see related issue https://github.com/tesseract-ocr/tesseract/issues/538

regarding recognition problems when an image has many different font sizes in it.  You can use windows binaries linked from
https://github.com/UB-Mannheim/tesseract/wiki

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Aug 10, 2017 at 9:47 PM, iuriigalaida <notifications@github.com>
wrote:

> I tried to train tesseract v4.0 and got 'combine_lang_model not found'
> error. This code was added 4 days ago so I tried to build latest master and
> got '.cppan/storage/etc/static/generate.cmake:106 (message):
> cppan command '0' not found '. Can somone build and share latest tesseract
> binaries for Windows. Thanks in advance.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1071>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1WOfX1TDTRm1RTyJ-aE72OC9GiZks5sWy0dgaJpZM4OzqP3>
> .
>
  Please see https://github.com/tesseract-ocr/tesseract/issues/1060#issuecomment-320720945

Try again after making sure that you are using the latest source code as well as traineddata from github. @ahmed-alaa Please note the commit number since you have isolated the problem.

@theraysmith @stweil FYI. > Failed to continue from: ~/tesstutorial/trainplusminus/eng.lstm

Please make sure that the file is there. Rerun the following and  check.

combine_tessdata -e baseline/eng.traineddata \
    ~/tesstutorial/trainplusminus/eng.lstm

 Please close the issue, since it is solved.  >For the scripts that use a virama character to generate conjunct consonants, (All the Indic scripts plus Myanmar and Khmer) the function NormalizeCleanAndSegmentUTF8 pairs the virama with an appropriate neighbor to generate a more glyph-oriented encoding in the unicharset. To make full use of this improvement, the --pass_through_recoder flag should be set for combine_lang_model for these scripts.

as per https://github.com/tesseract-ocr/tesseract/blob/master/training/tesstrain_utils.sh

```
  local pass_through=""
  # TODO(rays) set using script lang lists.
  case "${LANG_CODE}" in
    asm | ben | bih | hin | mar | nep | guj | kan | mal | tam | tel | pan | \
    dzo | sin | san | bod | ori | khm | mya | tha | lao | heb | yid | ara | \
    fas | pus | snd | urd | div | syr | uig | kur_ara )
      pass_through="--pass_through_recoder" ;;
```

However, I find that while the `san.unicharset` as part of `best traineddata` has only **145** characters, the newly generated starter unicharset through tesstrain.sh has the glyph based larger unicharset of **2308** and the final traineddata also has the larger unicharset of **2308** characters.

@theraysmith 

Is this the desired behavior or are we missing a step to convert the glyph based unicharset to the compressed unicharset after stopping training?

Also, how do we update the version string to be included in the final traineddata after training?

I am using the following command to build the final traineddata.

```
lstmtraining   --stop_training  \
  --continue_from  ../tesstutorial/vedic/santune_checkpoint  \
  --traineddata ../tesstutorial/vedic/san/san.traineddata  \
  --model_output ../tesstutorial/vedic/vedic.traineddata
```

 ```

USAGE: combine_lang_model
  --lang_is_rtl  True if lang being processed is written right-to-left  (type:bool default:false)
  --pass_through_recoder  If true, the recoder is a simple pass-through of the unicharset. Otherwise, potentially a compression of it  (type:bool defa
ult:false)
  --input_unicharset  Unicharset to complete and use in encoding  (type:string default:)
  --script_dir  Directory name for input script unicharsets  (type:string default:)
  --words  File listing words to use for the system dictionary  (type:string default:)
  --puncs  File listing punctuation patterns  (type:string default:)
  --numbers  File listing number patterns  (type:string default:)
  --output_dir  Root directory for output files  (type:string default:)
  --version_str  Version string to add to traineddata file  (type:string default:)
  --lang  Name of language being processed  (type:string default:)
```
 See Ray's comment at https://github.com/tesseract-ocr/tesseract/issues/1065#issuecomment-320709273

Needed to handedit the input unicharset

Will be fixed with new unichar_extractor.  Interesting! Thanks for the pointer.
Recognita had this feature in the mid 1990s.
The best ideas always return.

The difficulty is the same as for any character-level OCR, which is
character segmentation, although it probably can learn the difference
between rn and m, with a few examples.

On Sat, Aug 5, 2017 at 4:28 AM, chris <notifications@github.com> wrote:

> Peace be upon you,
> @theraysmith <https://github.com/theraysmith> here is a feature that I
> think you'll find interesting
>
> Computer Assisted Transcription:
>
>    - The software segments the pages to lines.
>    - Then segments the lines into words.
>    - Later-on, allow the user to transcribe words or glyphs, once the
>    user is satisfied, the software then searches for all instances of presence
>    of such words or glyphs, and automatically transcribe them all in all
>    instances.
>    - Can transcribe both Glyphs & Words, depending on the segmentation
>    level you choose.
>    https://github.com/benedikt-budig/glyph-miner
>
> [image: glyph] <https://www.youtube.com/watch?v=T-p_kIdsn6k>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1066>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AL056ZTjupK40e886o_oXNgeOzkrIfc-ks5sVFHpgaJpZM4OucA6>
> .
>



-- 
Ray.
  @theraysmith 

https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00 says under FINETUNING

>A recognition model can be extracted from an existing traineddata file, using combine_tessdata. Note that it is also necessary to supply the original traineddata file as well, as that contains the unicharset and recoder.

Since the original traineddata includes the recognition model, is it necessary to provide a link to it separately.

```
training/lstmtraining --model_output /path/to/output [--max_image_MB 6000] \
  --continue_from /path/to/existing/model \
  --traineddata /path/to/original/traineddata \
  [--perfect_sample_delay 0] [--debug_interval 0] \
  [--max_iterations 0] [--target_error_rate 0.01] \
  --train_listfile /path/to/list/of/filenames.txt
``` eg. If I want to train for Vedic Sanskrit using the best/Devanagari.traineddata

is the training solely based on `  --train_listfile /path/to/list/of/filenames.txt`
ignoring the unicharset and wordlists used in generating the 'starter' traineddata

```
training/lstmtraining \
  --model_output /vedic \
  [--max_image_MB 6000] \
  --continue_from /best/Devanagari.lstm \
  --traineddata /best/Devanagari.traineddata \
  [--perfect_sample_delay 0] [--debug_interval 0] \
  [--max_iterations 0] [--target_error_rate 0.01] \
  --train_listfile /vedic/filenames.txt
```

What about the starter traineddata generated by tesstrain.sh?

Or do the plusminus instructions apply here?

```
training/lstmtraining --model_output /path/to/output [--max_image_MB 6000] \
  --continue_from /path/to/existing/model \
  --traineddata /path/to/traineddata/with/new/unicharset \
  --old_traineddata /path/to/existing/traineddata \
  [--perfect_sample_delay 0] [--debug_interval 0] \
  [--max_iterations 0] [--target_error_rate 0.01] \
  --train_listfile /path/to/list/of/filenames.txt
``` Using this now

```
nice lstmtraining \
  --old_traineddata ../tessdata/best/san.traineddata \
  --continue_from ../tessdata/best/san.lstm \
  --traineddata ../tesstutorial/vedic/san/san.traineddata  \
  --train_listfile ../tesstutorial/vedic/san.training_files.txt \
  --eval_listfile ../tesstutorial/vedic/san.eval_files.txt \
  --model_output ../tesstutorial/vedic/santune \
  --max_iterations 2000 \
  --debug_interval -1

lstmtraining   --stop_training  \
  --continue_from  ../tesstutorial/vedic/santune_checkpoint  \
  --traineddata ../tesstutorial/vedic/san/san.traineddata  \
  --model_output ../tesstutorial/vedic/vedic .traineddata
 ```

```
Loaded file ./tess4training-save/tess4training-vedic/tessdata/best/san.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Code range changed from 145 to 2308!!
Num (Extended) outputs,weights in Series:
1,36,0,1:1, 0
Num (Extended) outputs,weights in Series:
C3,3:9, 0
Ft16:16, 160
Total weights = 160
[C3,3Ft16]:16, 160
Mp3,3:16, 0
Lfys48:48, 12480
Lfx96:96, 55680
Lrx96:96, 74112
Lfx192:192, 221952
Fc2308:2308, 445444
Total weights = 809828
Previous null char=2 mapped to 2
Continuing from ./tess4training-save/tess4training-vedic/tessdata/best/san.lstm
Loaded 138/138 pages (1-138) of document ./tess4training-save/tess4training-vedic/san.AA_NAGARI_SHREE_L3.exp0.lstmf
Loaded 138/138 pages (1-138) of document ./tess4training-save/tess4training-vedic/san.AA_NAGARI_SHREE_L3.exp-1.lstmf
Loaded 137/137 pages (1-137) of document ./tess4training-save/tess4training-vedic/san.AA_NAGARI_SHREE_L3.exp-2.lstmf
``` ```
lstmtraining \
  --continue_from ./tess4training-save/tess4training-vedic/tessdata/best/${CONTINUE_FROM_LANG}.lstm \
  --traineddata ./tess4training-save/tess4training-vedic/san/${LANG}.traineddata  \
  --append_index 5 --net_spec '[Lfx512 O1c150]' \
  --train_listfile ./tess4training-save/tess4training-vedic/${LANG}.training_files.txt \
  --eval_listfile ./tess4training-save/tess4training-vedic/${LANG}.eval_files.txt \
  --model_output ./tess4training-save/${LANG}${TYPE} \
  --debug_interval ${DEBUG_INTERVAL} \
  --max_iterations ${MAX_ITERATIONS}
```

```
$ bash ./4runtesseract.sh
Loaded file ./tess4training-save/tess4training-vedic/tessdata/best/san.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from ./tess4training-save/tess4training-vedic/tessdata/best/san.lstm
Appending a new network to an old one!!Warning: given outputs 150 not equal to unicharset of 2308.
Num outputs,weights in Series:
  Lfx512:512, 1247232
  Fc2308:2308, 1184004
Total weights = 2431236
Built network:[1,36,0,1[C3,3Ft16]Mp3,3Lfys48Lfx96Lrx96Lfx512Fc2308] from request [Lfx512 O1c150]
Training parameters:
  Debug interval = 0, weights = 0.1, learning rate = 0.001, momentum=0.5
null char=2
Loaded 138/138 pages (1-138) of document ./tess4training-save/tess4training-vedic/san.AA_NAGARI_SHREE_L3.exp0.lstmf
Loaded 138/138 pages (1-138) of document ./tess4training-save/tess4training-vedic/san.AA_NAGARI_SHREE_L3.exp-1.lstmf
Loaded 138/138 pages (1-138) of document ./tess4training-save/tess4training-vedic/san.AA_NAGARI_SHREE_L3.exp1.lstmf
``` There are several questions that you raise here:
>Since the original traineddata includes the recognition model, is it necessary to provide a link to it separately.
Yes. The unicharset and recoder are **only** provided by the --traineddata arg. The checkpoints don't contain a copy of that data. Neither does the .lstm component, which acts as a training checkpoint when modifying a previously-trained model. It is able to _detect_ that you have changed the unicharset size and complain if you don't provide --old_traineddata.

>is the training solely based on --train_listfile /path/to/list/of/filenames.txt
ignoring the unicharset and wordlists used in generating the 'starter' traineddata
Same answer.

>What about the starter traineddata generated by tesstrain.sh?
>Or do the plusminus instructions apply here?
If the vedic accents are already included in the unicharset, then you can treat it as finetuning for impact, but if they are not (I don't see them) then you must treat it as plusminus. In that case, you must generate a starter traineddata, either with tesstrain.sh or with combine_lang_model directly.

>Using this now
That command line looks correct, **but did you see this?:**
>Code range changed from 145 to 2308!!
This is not plus-minus a few characters!
Your unicharset is "old style". It needs updating.
unicharset_extractor needs updating, as it doesn't let you generate the "new style" unicharsets.
Before I set to work on that, how did you generate your unicharset? Did you use unicharset_extractor?

To re-run your experiment, you could just hand-edit the Devanagari unicharset to insert the vedic accents that you need. I mentioned this briefly somewhere before.
To hand-edit the unicharset, you can just copy-paste the lines you need, change the characters at the beginning of the lines, and update the number on the first line to match the new size.
**The other fields in the unicharset file will be automatically corrected by combine_lang_model**
 I used tesstrain.sh for creating the lstmf and starter data.

I will try the handedit for unicharset.

On 07-Aug-2017 9:46 PM, "theraysmith" <notifications@github.com> wrote:

> There are several questions that you raise here:
>
> Since the original traineddata includes the recognition model, is it
> necessary to provide a link to it separately.
> Yes. The unicharset and recoder are *only* provided by the --traineddata
> arg. The checkpoints don't contain a copy of that data. Neither does the
> .lstm component, which acts as a training checkpoint when modifying a
> previously-trained model. It is able to *detect* that you have changed
> the unicharset size and complain if you don't provide --old_traineddata.
>
> is the training solely based on --train_listfile
> /path/to/list/of/filenames.txt
> ignoring the unicharset and wordlists used in generating the 'starter'
> traineddata
> Same answer.
>
> What about the starter traineddata generated by tesstrain.sh?
> Or do the plusminus instructions apply here?
> If the vedic accents are already included in the unicharset, then you can
> treat it as finetuning for impact, but if they are not (I don't see them)
> then you must treat it as plusminus. In that case, you must generate a
> starter traineddata, either with tesstrain.sh or with combine_lang_model
> directly.
>
> Using this now
> That command line looks correct, *but did you see this?:*
> Code range changed from 145 to 2308!!
> This is not plus-minus a few characters!
> Your unicharset is "old style". It needs updating.
> unicharset_extractor needs updating, as it doesn't let you generate the
> "new style" unicharsets.
> Before I set to work on that, how did you generate your unicharset? Did
> you use unicharset_extractor?
>
> To re-run your experiment, you could just hand-edit the Devanagari
> unicharset to insert the vedic accents that you need. I mentioned this
> briefly somewhere before.
> To hand-edit the unicharset, you can just copy-paste the lines you need,
> change the characters at the beginning of the lines, and update the number
> on the first line to match the new size.
> *The other fields in the unicharset file will be automatically corrected
> by combine_lang_model*
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1065#issuecomment-320709273>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o3CFEzXvmfs24sf1pF22WXYKml9Iks5sVzhugaJpZM4OuX02>
> .
>
 Thanks for the explanation, Ray. @theraysmith 

>To re-run your experiment, you could just hand-edit the Devanagari unicharset to insert the vedic accents that you need. I mentioned this briefly somewhere before.
To hand-edit the unicharset, you can just copy-paste the lines you need, change the characters at the beginning of the lines, and update the number on the first line to match the new size.
>The other fields in the unicharset file will be automatically corrected by combine_lang_model

This does not work. The program crashes in unicharset.cpp


```
 combine_lang_model    \
>  --input_unicharset  ../tesstutorial/san.unicharset  \
>  --script_dir "../langdata"   \
>  --words "../langdata/san/san.wordlist" \
>  --numbers "../langdata/san/san.numbers"   \
>  --puncs "../langdata/san/san.punc" \
>  --output_dir ../tesstutorial/sanskrit2003   \
0>  --lang "san"     --pass_through_recoder \
>      --version_str "4.0.0alpha-20170817 sanskrit2003"
other_case < unicharset_size:Error:Assert failed:in file unicharset.cpp, line 882
Segmentation fault (core dumped)
root@All-in-1-Touch:/mnt/c/Users/Us
```

I had to change all the added lines so that the numbers after the scriptname are incremented sequentially, only then it worked ok.

( Update: I had copied lines from another unicharset which had a different line number  for other case. That caused the error.  It works as described by Ray when other case line number is same.) > Appending a new network to an old one!!Warning: given outputs 150 not equal to unicharset of 2308.
Num outputs,weights in Series:
  Lfx512:512, 1247232
  Fc2308:2308, 1184004
Total weights = 2431236
Built network:[1,36,0,1[C3,3Ft16]Mp3,3Lfys48Lfx96Lrx96Lfx512Fc2308] from request [Lfx512 O1c150]

@Shreeshrii are you using Unicharset Compression-recoding? .I'm using output of combine_lang_model  for describe what i modifier in last lstm layer, example "jpn.charset_size=414.txt".  i using this last 3 digits of number for append last lstm layer . in mycase is [Lfx512 O1c414] . when i doing this i don't get a warning like [Warning: given outputs 150 not equal to unicharset of 2308] anymore 
Is this right ? 
Thanks! That number is just a warning, but helpful info.

Please see https://github.com/Shreeshrii/tess4training-vedic/tree/master/san

tesstrain.sh produced the larger 'old style' unicharset. Then as per Ray's suggestion, I hand edited it to a smaller size. Rewrite of unicharset_extractor coming early next week.

On Tue, Aug 8, 2017 at 8:21 PM, Shreeshrii <notifications@github.com> wrote:

> That number is just a warning, but helpful info.
>
> Please see https://github.com/Shreeshrii/tess4training-vedic/tree/
> master/san
>
> tesstrain.sh produced the larger 'old style' unicharset. Then as per Ray's
> suggestion, I hand edited it to a smaller size.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1065#issuecomment-321141745>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056fnA8yvAzNH7oTI63qhwAafNbXNiks5sWSXLgaJpZM4OuX02>
> .
>



-- 
Ray.
  
combine_tessdata -e ../tessdata/best/eng.traineddata \
  ../tesstutorial/engtrain/besteng.lstm
  
--------------

Finetune using the existing training text  
   
 lstmtraining \
  --model_output ../tesstutorial/engtrain/engtune \
  --continue_from ../tesstutorial/engtrain/besteng.lstm \
  --traineddata ../tessdata/best/eng.traineddata \
  --train_listfile ../tesstutorial/engeval/eng.training_files.txt \
  --debug_interval -1 \
  --max_iterations 400

  ---------------------------------------
   
 with files created using modified plusminus trainingtext

 lstmtraining \
  --model_output ../tesstutorial/engtrain/engplusminus \
  --continue_from ../tesstutorial/engtrain/besteng.lstm \
  --traineddata ../tesstutorial/engtrain/eng/eng.traineddata \
  --old_traineddata ../tessdata/best/eng.traineddata \
  --train_listfile ../tesstutorial/engtrain/eng.training_files.txt \
  --debug_interval -1 \
  --max_iterations 3600  
 
 --------------------------------

 with files created using NEW training text - many additions

 lstmtraining \
  --model_output ../tesstutorial/engtrain/englayer \
  --continue_from ../tesstutorial/engtrain/besteng.lstm \
  --traineddata ../tesstutorial/engtrain/eng/eng.traineddata \
  --train_listfile ../tesstutorial/engtrain/eng.training_files.txt \
  --eval_listfile ../tesstutorial/engeval/eng.training_files.txt \
  --append_index 5 \
  --net_spec '[Lfx256 O1c105]' \ Finetune

best traineddata (--traineddata)
lstm extracted from best traineddata
lstmf files for starter traineddata 

--------------------------------

PlusMinus

best traineddata (--old_traineddata)
lstm extracted from best traineddata
lstmf files for starter traineddata 
starter traineddata (--traineddata)

------------------------------

Replace a layer

lstm extracted from best traineddata
lstmf files for starter traineddata 
starter traineddata (--traineddata)
  Please test with the latest models available in
https://github.com/tesseract-ocr/tessdata/tree/master/best

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Aug 1, 2017 at 9:45 PM, TheSeiko <notifications@github.com> wrote:

> Additional example (wW)
> VW-Werk -> VwW-Werk
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1060#issuecomment-319419707>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o9N-1P4rP3fa2mYMcL4zS0Z8LMEYks5sT08WgaJpZM4Op9eR>
> .
>
 Are you using --oem 1?

you can see the contents of the traineddata by 

combine_tessdata -u deu,traineddata 

These are probably only lstm models and do not have the legacy engine which is used via --oem 0 @stweil Have you tested the deu model? @stweil - you may need to update the windows binaries on Uni Mannheim site with the latest updates  from Ray.

@TheSeiko I haven't personally tested the deu model. WIll check and post result. Wondering whether your Windows binary is old.... Looks like you need both deu and frk models

wget -O ./tess4data-save/deubest.traineddata https://github.com/tesseract-ocr/tessdata/blob/master/best/deu.traineddata?raw=true

sudo cp ./tess4data-save/*.traineddata /usr/share/tesseract-ocr/4.00/tessdata

time tesseract ./tif/phototest.tif stdout --oem 1 -l deu
time tesseract ./tif/phototest.tif stdout --oem 1 -l deubest

```
Page 1
This is a lot of 12 point text to test the
ocr code and see if it works on all types
of file format.
The quick brown dog jumped over the
lazy fox. The quick brown dog jumped
over the lazy fox. The quick brown dog
jumped over the lazy fox. The quick
brown dog jumped over the lazy fox.
real	0m1.633s
user	0m2.032s
sys	0m0.492s
Error opening data file /usr/share/tesseract-ocr/4.00/frk.traineddata
Please make sure the TESSDATA_PREFIX environment variable is set to your "tessdata" directory.
Failed loading language 'frk'
Page 1
This is a lot of 12 point text to test the
ocr code and see if it works on all types
of file format.
The quick brown dog jumped over the
lazy fox. The quick brown dog jumped
over the lazy fox. The quick brown dog
Jumped over the lazy fox. The quick
brown dog jumped over the lazy fox.
real	0m2.045s
user	0m2.744s
sys	0m0.612s
``` works on linux - looks for frk traineddata, probably listed in deu.config Thanks!

>I now use semantic versioning, so this is my 4.0.0-alpha.20170804.

:-) Looks like I introduced a bug.
If the traineddata file doesn't exist, it makes an empty one with a version
string in it, instead of complaining about the non-existent file.

On Sun, Aug 6, 2017 at 8:00 PM, TheSeiko <notifications@github.com> wrote:

> @stweil <https://github.com/stweil> Am I doing something wrong?
>
> There's only a version file included in the deu,traineddata when using the
> binaries from 04.08
>
> E:\Tesseract-OCR4.0a2>combine_tessdata -u deu.traineddata tmp/deu
> Extracting tessdata components from deu.traineddata
> Wrote tmp/deu.version
> Version string:4.0.0-alpha.20170804
> 23:version:size=20, offset=192
>
> E:\Tesseract-OCR4.0a2>combine_tessdata -u deu.traineddata tmp/deu.
> Extracting tessdata components from deu.traineddata
> Wrote tmp/deu.version
> Version string:4.0.0-alpha.20170804
> 23:version:size=20, offset=192
>
> E:\Tesseract-OCR4.0a2>combine_tessdata -d deu.traineddata
> Version string:4.0.0-alpha.20170804
> 23:version:size=20, offset=192
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1060#issuecomment-320557491>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056cltXp_aF2gKV9kC0kWvs3JtVSFnks5sVn3ggaJpZM4Op9eR>
> .
>



-- 
Ray.
 Ray,
There have been a number of reports of people not being able to run the
english tutorial training.
Missing eng.config etc
Posted in tesseract-ocr forum

On 07-Aug-2017 9:49 PM, "theraysmith" <notifications@github.com> wrote:

> Looks like I introduced a bug.
> If the traineddata file doesn't exist, it makes an empty one with a version
> string in it, instead of complaining about the non-existent file.
>
> On Sun, Aug 6, 2017 at 8:00 PM, TheSeiko <notifications@github.com> wrote:
>
> > @stweil <https://github.com/stweil> Am I doing something wrong?
> >
> > There's only a version file included in the deu,traineddata when using
> the
> > binaries from 04.08
> >
> > E:\Tesseract-OCR4.0a2>combine_tessdata -u deu.traineddata tmp/deu
> > Extracting tessdata components from deu.traineddata
> > Wrote tmp/deu.version
> > Version string:4.0.0-alpha.20170804
> > 23:version:size=20, offset=192
> >
> > E:\Tesseract-OCR4.0a2>combine_tessdata -u deu.traineddata tmp/deu.
> > Extracting tessdata components from deu.traineddata
> > Wrote tmp/deu.version
> > Version string:4.0.0-alpha.20170804
> > 23:version:size=20, offset=192
> >
> > E:\Tesseract-OCR4.0a2>combine_tessdata -d deu.traineddata
> > Version string:4.0.0-alpha.20170804
> > 23:version:size=20, offset=192
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/1060#
> issuecomment-320557491>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/AL056cltXp_
> aF2gKV9kC0kWvs3JtVSFnks5sVn3ggaJpZM4Op9eR>
> > .
> >
>
>
>
> --
> Ray.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1060#issuecomment-320710053>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o0ifee0UQtGx7y9q_bdy7S-bpU8Gks5sVzkkgaJpZM4Op9eR>
> .
>
 I just made 3 commits that address some of these issues:
Error message for lack of --traineddata arg referring to wiki.
Emphasis that the lack of config file is just a warning.
Detected non-existent traineddata file in combine_tessdata.

It seems the majority of the problems are lack of sync of code/data. There
are dependencies between code and data that have changed due to moving the
unicharset from the LSTM model to the traineddata file.

On Mon, Aug 7, 2017 at 9:27 AM, Shreeshrii <notifications@github.com> wrote:

> Ray,
> There have been a number of reports of people not being able to run the
> english tutorial training.
> Missing eng.config etc
> Posted in tesseract-ocr forum
>
> On 07-Aug-2017 9:49 PM, "theraysmith" <notifications@github.com> wrote:
>
> > Looks like I introduced a bug.
> > If the traineddata file doesn't exist, it makes an empty one with a
> version
> > string in it, instead of complaining about the non-existent file.
> >
> > On Sun, Aug 6, 2017 at 8:00 PM, TheSeiko <notifications@github.com>
> wrote:
> >
> > > @stweil <https://github.com/stweil> Am I doing something wrong?
> > >
> > > There's only a version file included in the deu,traineddata when using
> > the
> > > binaries from 04.08
> > >
> > > E:\Tesseract-OCR4.0a2>combine_tessdata -u deu.traineddata tmp/deu
> > > Extracting tessdata components from deu.traineddata
> > > Wrote tmp/deu.version
> > > Version string:4.0.0-alpha.20170804
> > > 23:version:size=20, offset=192
> > >
> > > E:\Tesseract-OCR4.0a2>combine_tessdata -u deu.traineddata tmp/deu.
> > > Extracting tessdata components from deu.traineddata
> > > Wrote tmp/deu.version
> > > Version string:4.0.0-alpha.20170804
> > > 23:version:size=20, offset=192
> > >
> > > E:\Tesseract-OCR4.0a2>combine_tessdata -d deu.traineddata
> > > Version string:4.0.0-alpha.20170804
> > > 23:version:size=20, offset=192
> > >
> > > —
> > > You are receiving this because you were mentioned.
> > > Reply to this email directly, view it on GitHub
> > > <https://github.com/tesseract-ocr/tesseract/issues/1060#
> > issuecomment-320557491>,
> > > or mute the thread
> > > <https://github.com/notifications/unsubscribe-auth/AL056cltXp_
> > aF2gKV9kC0kWvs3JtVSFnks5sVn3ggaJpZM4Op9eR>
> > > .
> > >
> >
> >
> >
> > --
> > Ray.
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/1060#
> issuecomment-320710053>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/AE2_o0ifee0UQtGx7y9q_
> bdy7S-bpU8Gks5sVzkkgaJpZM4Op9eR>
> > .
>
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1060#issuecomment-320712052>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056bG0VYKti8qB_bH9oO6Ma4ZkT7B_ks5sVzrtgaJpZM4Op9eR>
> .
>



-- 
Ray.
 >It seems the majority of the problems are lack of sync of code/data. There
are dependencies between code and data that have changed due to moving the
unicharset from the LSTM model to the traineddata file.

Yes. That is the problem.

One possible solution that I have been asking for a while is the tagging of "important" commits. Then it would be easy to say, use tesseract, tessdata, langdata as of 4.0.0alpha-20170807  Interesting. I remember from learning German at school that all nouns begin
with a capital, so why do yours not?
I would assume from the errors that you describe that the network has
learned that all nouns begin with a capital, so it hallucinates one even
when it is not there.
If you have a lot of non-capital nouns for some reason, it might do better
in 'Latin' than 'deu'

On Tue, Aug 8, 2017 at 11:57 PM, TheSeiko <notifications@github.com> wrote:

> $$-Jährige <-> $$-jährige
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1060#issuecomment-321170268>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SQxJfQMiipkLavZpbfqC4FgRymLks5sWVhbgaJpZM4Op9eR>
> .
>



-- 
Ray.
  @stweil Please review and make any needed changes.

'make' will build the test.
'make runtest' will run the test.  ```
 make runtest
./tesseracttests
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from TesseractTest
[ RUN      ] TesseractTest.ZeroDummyTestForTesseract
[       OK ] TesseractTest.ZeroDummyTestForTesseract (0 ms)
[ RUN      ] TesseractTest.FirstDummyTestForTesseract
[       OK ] TesseractTest.FirstDummyTestForTesseract (0 ms)
[----------] 2 tests from TesseractTest (1 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (5 ms total)
[  PASSED  ] 2 tests.
``` @theraysmith @stweil 

I have updated the testing framework to use unittest directory.

```
root@All-in-1-Touch:/mnt/c/Users/User/shree/tesseract/unittest# make
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -c ../unittest/sample1.cc
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -c ../unittest/sample1_unittest.cc
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -I../googletest/googletest -g -Wall -Wextra -pthread -O2 -std=c++11 -c \
            ../googletest/googletest/src/gtest-all.cc
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -I../googletest/googletest -g -Wall -Wextra -pthread -O2 -std=c++11 -c \
            ../googletest/googletest/src/gtest_main.cc
ar rv gtest_main.a gtest-all.o gtest_main.o
ar: creating gtest_main.a
a - gtest-all.o
a - gtest_main.o
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -lpthread sample1.o sample1_unittest.o gtest_main.a -o sample
1_unittest
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -c ../unittest/tesseracttests.cpp
ar rv gtest.a gtest-all.o
ar: creating gtest.a
a - gtest-all.o
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -lpthread tesseracttests.o gtest.a -o tesseracttests
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -I../ccstruct  -I../ccutil -c ../unittest/matrix_test.cc
g++ -isystem ../googletest/googletest/include  -DUSE_STD_NAMESPACE -g -Wall -Wextra -pthread -O2 -std=c++11 -lpthread -llept -ltesseract matrix_test.o gtest_main.a -o ma
trix_test
root@All-in-1-Touch:/mnt/c/Users/User/shree/tesseract/unittest# make runtest
./sample1_unittest
Running main() from gtest_main.cc
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 3 tests from FactorialTest
[ RUN      ] FactorialTest.Negative
[       OK ] FactorialTest.Negative (0 ms)
[ RUN      ] FactorialTest.Zero
[       OK ] FactorialTest.Zero (0 ms)
[ RUN      ] FactorialTest.Positive
[       OK ] FactorialTest.Positive (0 ms)
[----------] 3 tests from FactorialTest (1 ms total)

[----------] 3 tests from IsPrimeTest
[ RUN      ] IsPrimeTest.Negative
[       OK ] IsPrimeTest.Negative (0 ms)
[ RUN      ] IsPrimeTest.Trivial
[       OK ] IsPrimeTest.Trivial (0 ms)
[ RUN      ] IsPrimeTest.Positive
[       OK ] IsPrimeTest.Positive (0 ms)
[----------] 3 tests from IsPrimeTest (1 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 2 test cases ran. (3 ms total)
[  PASSED  ] 6 tests.
./tesseracttests
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from TesseractTest
[ RUN      ] TesseractTest.ZeroDummyTestForTesseract
[       OK ] TesseractTest.ZeroDummyTestForTesseract (0 ms)
[ RUN      ] TesseractTest.FirstDummyTestForTesseract
[       OK ] TesseractTest.FirstDummyTestForTesseract (0 ms)
[----------] 2 tests from TesseractTest (1 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (2 ms total)
[  PASSED  ] 2 tests.
./matrix_test
Running main() from gtest_main.cc
[==========] Running 4 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 4 tests from MatrixTest
[ RUN      ] MatrixTest.RotatingTranspose_3_1
[       OK ] MatrixTest.RotatingTranspose_3_1 (0 ms)
[ RUN      ] MatrixTest.RotatingTranspose_2_0
[       OK ] MatrixTest.RotatingTranspose_2_0 (0 ms)
[ RUN      ] MatrixTest.RotatingTranspose_1_3
[       OK ] MatrixTest.RotatingTranspose_1_3 (0 ms)
[ RUN      ] MatrixTest.RotatingTranspose_0_2
[       OK ] MatrixTest.RotatingTranspose_0_2 (0 ms)
[----------] 4 tests from MatrixTest (1 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 1 test case ran. (2 ms total)
[  PASSED  ] 4 tests.
root@All-in-1-Touch:/mnt/c/Users/User/shree/tesseract/unittest#
``` https://github.com/tesseract-ocr/tesseract/blob/0f2500287a30d2b09e8c853b39913a5eb9b03ca2/unittest/Makefile Hmm. That doesn't do it for me either.
It is looking for tesseract in some installed location, when I want it to
refer to the local copy, so I can use test-driven development.
Can't we just have a Makefile.am for it like we do for training?


On Fri, Aug 18, 2017 at 6:29 AM, Shreeshrii <notifications@github.com>
wrote:

> https://github.com/tesseract-ocr/tesseract/blob/
> 0f2500287a30d2b09e8c853b39913a5eb9b03ca2/unittest/Makefile
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/1059#issuecomment-323353515>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SJiMvSFQS5hq43fX1RBewAnnev0ks5sZZGlgaJpZM4OnUW1>
> .
>



-- 
Ray.
  Please see this note in wiki - https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#error-messages-from-training

>No block overlapping textline: occurs when layout analysis fails to correctly segment the image that was given as training data. The textline is dropped. Not much problem if there aren't many, but if there are a lot, there is probably something wrong with the training text or rendering process. Still getting some of these errors for Devanagari, with tif/box pairs generated by text2image. Seems to be around `---------०---------` in training text.

```

No block overlapping textline: ---------०---------
No block overlapping textline: वित्त्येवहि अचूर्यामहि कृतघ्नं शत्रून्द्रुहे शुष्कीकरोति
No block overlapping textline: अर्कैः
No block overlapping textline: ह्यन्बन्त्यांञ्जगृहीतवती शक्तिपीठं ग्न्य छन्दष्ट्य झ
```  Thanks!
But you did not push PR to our repository, so we cannot apply it.
You MUST create PR inside your github repo, not ours, then it should appear here. Fixed https://github.com/tesseract-ocr/tesseract/pull/1054  It is used internally at Google.
Text2image could be modified to use it too. > Text2image could be modified to use it too.

That would be great. This is what I referred to in - https://github.com/tesseract-ocr/tessdata/issues/69#issuecomment-320302092  I had built it locally with autotools, making changes in the makefiles etc. I think once setup the only changes will be in the makefile in the directory where the tests/test cases/test fixtures are kept. So it should not be too difficult.

https://github.com/tesseract-ocr/tesseract/blob/0f2500287a30d2b09e8c853b39913a5eb9b03ca2/unittest/Makefile Please see https://github.com/tesseract-ocr/tesseract/pull/1059

pull request with makefile etc So I tried building a test using just github, and I get stuck before I can even try it.
The code doesn't build under cmake, where it does under autotools.
Since this is my first attempt at using cmake I'm probably doing something wrong, but it seems less reliable/portable than autotools so far:

~/opensrc/git/tesseract/demo$ make
[ 97%] Built target libtesseract
Linking CXX executable bin/tesseract
liblibtesseract.so.4.0.0: undefined reference to `l_generateCIDataForPdf'
liblibtesseract.so.4.0.0: undefined reference to `l_CIDataDestroy'
liblibtesseract.so.4.0.0: undefined reference to `pixReadFromMultipageTiff'
liblibtesseract.so.4.0.0: undefined reference to `pixReadMemFromMultipageTiff'
collect2: error: ld returned 1 exit status

It looks to me like it is picking up the wrong version of leptonica. I think these are new functions in the latest version. I  built and installed the latest leptonica earlier today.
So how do I make it build under cmake? cmake fans speak up before I get turned off it completely!
 No luck. cmake doesn't work on LInux.
Even building leptonica with cmake puts the library in /usr/local, so if
cmake isn't going to look there, it isn't going to find it.

On Fri, Aug 18, 2017 at 3:23 AM, Amit D. <notifications@github.com> wrote:

> Basically, CMake is for Windows&MSVC and autotools is for all other
> environment.
> I'm Talking only about this project, not about other projects build tool
> usage.
>
> Related: DanBloomberg/leptonica#253
> <https://github.com/DanBloomberg/leptonica/issues/253>
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/1051#issuecomment-323317976>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SH7Rdi05R6Ra1KIeRINkliVA_g6ks5sZWYWgaJpZM4OhhM1>
> .
>



-- 
Ray.
 CMake does not work properly on linux. Please, use autotools for some time.
Or is it hard to integrate tests with autotools?
  copied from https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!topic/tesseract-dev/VLwwGFKtdPA

>Ray | Jul 21
>I have the OK to "throw the tests over the wall" already. ie provide them in a non-working form.
There are actually very few copyrighted images that would need to be replaced. Most of the tests run on synthetic data, existing test data, or don't require images.

>**If someone would put together the build pieces necessary to build and run an empty test (using Google test), then I will port at least one example, and then push the rest out.**

>Ray | Jul 13
>There are the order of 50 tests, some of which complete in <1s. Some take ~10mins to run, but very few. You could probably run the whole lot on a single machine in about an hour.

>Jul 13 Jeff Breidenbach 
>First step is to make a working but empty test for Tesseract under this 
framework. Once that exemplar is in place, we can migrate the existing 
tests that currently run inside Google.

 Some tests for tesseract using java as part of Quan's tess4j project

>https://github.com/nguyenq/tess4j/tree/master/src/test/java/net/sourceforge/tess4j https://stackoverflow.com/questions/35998856/how-can-i-use-google-test-with-my-project-that-builds-via-autotools Thanks!
I will port one test, and then determine how easy it is to write portable
tests that work both inside Google and outside.


On Mon, Jul 24, 2017 at 10:51 AM, Stefan Weil <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith>, pull request #1051
> <https://github.com/tesseract-ocr/tesseract/pull/1051> adds the required
> pieces for GoogleTest.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1050#issuecomment-317501971>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056a53GU5T3Bo5ETJ3KJ6XOFSsIQ5Fks5sRNmPgaJpZM4Og8V1>
> .
>



-- 
Ray.
 PR with Makefile.am for unittest

https://github.com/tesseract-ocr/tesseract/pull/1088  Please see https://mail.google.com/mail/u/1/#inbox/15d63efbd84c75e0
https://stackoverflow.com/questions/45051372/image-to-searchable-text-pdf-in-tesseract-3-05

I have seen earlier requests for info for this.

It will be helpful to add an api example for creating searchable pdfs to 

* https://github.com/tesseract-ocr/tesseract/wiki/APIExample

 https://github.com/cppan/tesseract_example/blob/master/with_cmake/src/main.cpp  @theraysmith Are Halfwidth katakana included in your new Japanese training? @hoangtocdo90 Please see https://github.com/tesseract-ocr/langdata/issues/81#issuecomment-320821042 and reply to Ray's questions there.  The problem is that -DUSING_STD_NAMESPACE is now needed for all of the tesseract library, not just for training. I fixed it in the Makefile.am files, but corresponding fixes are needed for windows and other non-autotools platforms. The same define will be required for anything that uses the library too, as
the define causes a using std::string to be declared in platform.h, which
is exactly what you are missing.

On Wed, Jul 19, 2017 at 3:14 PM, Edouard Belval <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith> Is it? Because I had that
> problem about 3 hours ago on Ubuntu. Tesseract does build, it's the modules
> built with its header files that doesn't.
>
> I'll try adding -DUSING_STD_NAMESPACE and report back.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1045#issuecomment-316534228>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Wm5F3tHXCEBRfmSncrNj3xLaO1Jks5sPn-tgaJpZM4OdRN0>
> .
>



-- 
Ray.
 Problem is back, clean pull and build from `master` causes the same error described in the issue. There is probably an include of platform.h needed somewhere.
Can you give us the compiler error? @amitdo Sorry, issue fell off my radar because of another problem.

@theraysmith It's the exact same compiler error that was in the original issue.

@stweil  According to this [issue](https://github.com/sirfz/tesserocr/issues/66#issuecomment-322033488) the last "good" version was [this commit](https://github.com/tesseract-ocr/tesseract/commit/f5c18f78c09ab028791c28c638d5cc2f96c6d6fb). I did not try anything else than the latest version. Will report back with the results. @stweil I'll try it. If it works you might want to edit your comment above to make sure that no one else will make that mistake. @sirfz Can confirm, I built it successfully too.  We do not provide support for 3rd party sw. Use tesseract user forum for asking question or replicate error with tesseract executable.   what is the version of your traineddata files? Download latest version from the tessdata repo. Test with the tif file in testing directory. It works ok for me.
My traineddata files are in ../tessdata directory

```
# tesseract phototest.tif phototest --tessdata-dir ../
Tesseract Open Source OCR Engine v4.00.00dev-2067 with Leptonica
Page 1

# tesseract phototest.tif phototest --tessdata-dir ../ --oem 1
Tesseract Open Source OCR Engine v4.00.00dev-2067 with Leptonica
Page 1

# tesseract phototest.tif phototest --tessdata-dir ../ --oem 2
Tesseract Open Source OCR Engine v4.00.00dev-2067 with Leptonica
Page 1


# tesseract -v
tesseract 4.00.00dev-2067
 leptonica-1.74.4
  libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8

 Found AVX
 Found SSE
``` When you say ' Tesseract 4.00 Git Version' I take it to mean that you are using the latest source from github to build tesseract. build with --enable-debug and run with gdb to get additional info.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Jul 19, 2017 at 11:05 PM, Nick <notifications@github.com> wrote:

> OK. I tested it with the traineddata above. But also it's the same I'm
> using here.
> I also confirmed that tesseract in indeed using the right data folder.
>
> But again the phototest.tif works fine with --oem 0 and results in the
> same error "illegal instructions" for any other --oem option or none
> (default should be --oem 2 if I'm not mistaken)
>
> And although compilation seemed fine. I didn't see an error or warning. So
> I guess there must be some library missing here.
>
> Also I reinstalled Leptonica and Tesseract multiple times now.
>
> Here's how I've installed the tools:
>
> 1. Make sure that the following libraries are installed:
>
>        # nickbe:  I had to replace libpng12-dev for debian jessie
>
> 	apt-get install autoconf-archive automake g++ libtool libleptonica-dev pkg-config
> 	apt-get install libpango1.0-dev
>
> 	# sudo apt-get install g++ # or clang++ (presumably)
> 	sudo apt-get install autoconf automake libtool
> 	sudo apt-get install autoconf-archive
> 	sudo apt-get install pkg-config
> 	sudo apt-get install libpng12-dev
> 	sudo apt-get install libjpeg-turbo
> 	sudo apt-get install libtiff5-dev
> 	sudo apt-get install zlib1g-dev
>
> 	sudo apt-get install libicu-dev
> 	sudo apt-get install libpango1.0-dev
> 	sudo apt-get install libcairo2-dev
>
> 2. Install Leptonica:
>
> 	git clone --depth 1 https://github.com/DanBloomberg/leptonica.git leptonica
> 	cd leptonica
> 	./autobuild
> 	./configure
> 	make
> 	sudo make install
> 	ldconfig
>
> 3. Install Tesseract:
>
>     git clone --depth 1  https://github.com/tesseract-ocr/tesseract.git tesseract-ocr
>     cd tesseract-ocr
>     ./autogen.sh
>
>     ./configure --disable-openmp --disable-shared --disable-static
>     or
>     ./configure        # nickbe: I TESTED BOTH CONFIGURATIONS JUST TO MAKE SURE
>     make
>
>     sudo make install
> 	sudo ldconfig
>
> 	# sudo make training
> 	# sudo make training-install
>
> 	sudo make install-langs      # nickbe: Never does anything so far
>       sudo ldconfig
>
> 4. wget tessdata from https://github.com/tesseract-ocr/tesseract/wiki/Data-Files
>    to /usr/local/share/tessdata
>
>    Example: wget https://github.com/tesseract-ocr/tessdata/raw/4.00/eng.traineddata
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1043#issuecomment-316461110>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oxqBBceuQv-LRbrZSoOB_RyHelUCks5sPj5ngaJpZM4ObwNC>
> .
>
  If you can help for a particular script, please comment below. 

Comments from Ray - copied from https://github.com/tesseract-ocr/tesseract/issues/995
Read the thread for full context.
______________________________________________

it would be useful to have any experts in any of the following scripts 
review the new corpus cleanup code,and make comments:

Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada,Malayalam, 
Sinhala, 
Thai, 
Myanmar, 
Khmer.

There are script-specific cleanup rules in there.
Since I plan to commit new copies of the training data (unicharsets,
wordlists, training text etc) then at that point they will match.

eg.
The code determines what makes a valid/invalid sequence of unicodes in the
script, for instance, is it allowed to have two matras in a row? It gets
more difficult with questions over what category the additional characters
are.

Major new normalization/text cleanup code in training/validat* The best
help with this would be expertise in the various scripts, as previously
discussed.

--------------------------
https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_grapheme.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_grapheme.h

https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_indic.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_indic.h

https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_khmer.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_khmer.h

https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_myanmar.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_myanmar.h

https://github.com/tesseract-ocr/tesseract/blob/master/training/validator.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/training/validator.h Devanagari - Vedic Accents

https://github.com/tesseract-ocr/tesseract/blob/master/training/validator.cpp#L178

```
bool Validator::IsVedicAccent(char32 unicode) {
  return 0x1cd0 <= unicode && unicode < 0x1d00;
}
```

Please see 
* http://www.unicode.org/versions/Unicode10.0.0/ch12.pdf
* http://unicode.org/charts/PDF/U0900.pdf
* http://unicode.org/charts/PDF/UA8E0.pdf
* http://unicode.org/charts/PDF/U1CD0.pdf

The following should also be included as Vedic Accents.

>U+0951..U+0954 are a set of combining marks used in transcription of Sanskrit texts.

```
Vedic tone marks
0951 $॑ DEVANAGARI STRESS SIGN UDATTA
= Vedic tone svarita
• mostly used for svarita, with rare use for udatta
• used also in Vedic texts written in other scripts
→ 1CDA $᳚  vedic tone double svarita
0952 $॒ DEVANAGARI STRESS SIGN ANUDATTA
= Vedic tone anudatta
• used also in Vedic texts written in other scripts
→ 1CDC $᳜  vedic tone kathaka anudatta
```

Possibly also

```
Accent marks
0953 $॓ DEVANAGARI GRAVE ACCENT
→ 0300 $̀  combining grave accent
0954 $॔ DEVANAGARI ACUTE ACCENT
→ 0301 $́  combining acute accent
```

>Devanagari Extended: U+A8E0–U+A8FF
This block of characters is used chiefly for Vedic Sanskrit, although many of the characters
are generic and can be used by other Indic scripts. The block includes a set of combining
digits, letters, and avagraha which is used as a system of cantillation marks in the early
Vedic Sanskrit texts. The Devanagari Extended block also includes nasalization marks (candrabindu),
and a number of editorial marks.

Also include the ranges
*  A8E0-A8F1 Combining Marks
* A8F2-A8F7 Marks of Nasalization

 Devanagari - Words cannot begin with

```
Various signs
0900 $ऀ DEVANAGARI SIGN INVERTED CANDRABINDU
= vaidika adhomukha candrabindu
0901 $ँ DEVANAGARI SIGN CANDRABINDU
= anunasika
→ 0310 $̐  combining candrabindu
0902 $ं DEVANAGARI SIGN ANUSVARA
= bindu
0903 $ः DEVANAGARI SIGN VISARGA

Various signs
093C $़ DEVANAGARI SIGN NUKTA
• for extending the alphabet to new letters
093D ऽ DEVANAGARI SIGN AVAGRAHA
```
and
the various dependent vowel signs.

Most of these maybe covered by https://github.com/tesseract-ocr/tesseract/blob/master/training/validate_grapheme.cpp#L55

```
  if (char_type == U_NON_SPACING_MARK || char_type == U_ENCLOSING_MARK ||
      char_type == U_COMBINING_SPACING_MARK || ch == kZeroWidthNonJoiner ||
      ch == kZeroWidthJoiner)
    return CharClass::kCombiner;
```

Please check about Avagraha - 093D. Devanagari - Eyelash Ra for Marathi

```
R5a For compatibility with The Unicode Standard, Version 2.0, if the dead consonant
RAd precedes zero width joiner, then the half-consonant form RAh , depicted as
eyelash-RA, is used instead of RAsup .
```
Page 13 in http://www.unicode.org/versions/Unicode10.0.0/ch12.pdf

Removal of ZWJ in this case will lead to incorrect results.  check  that your unicharset file is in utf8 encoding and not ANSI

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Fri, Jul 14, 2017 at 10:58 AM, wqret1992 <notifications@github.com>
wrote:

> error_text_when_training_data_generated.txt
> <https://github.com/tesseract-ocr/tesseract/files/1147429/error_text_when_training_data_generated.txt>
> tess_output_for_img_txt.txt
> <https://github.com/tesseract-ocr/tesseract/files/1147428/tess_output_for_img_txt.txt>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1037#issuecomment-315273593>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1mCPmUJx7XNvVmhzWqEYg088ChHks5sNvxpgaJpZM4OX15q>
> .
>
 lstm training with box/tiff pairs is currently not supported. > can you please tell how to generate unicharset file in utf-8 format

Use a text editor which can save file in utf-8 encoding. I use notepad++.  I haven't measured cache performance in detail as you have - more measured
real-time performance under various conditions and speculated as to the
cause ;-) The detail you provide is very interesting.

One thing I have noticed is that performance of OpenMP varies a lot
according to CPU load, so you will get very poor performance trying to run
multiple tesseracts in parallel. A major performance improvement is
possible under such circumstances if you pin OpenMP threads to cores, which
can be done using an environment variable:
https://www.nas.nasa.gov/hecc/support/kb/using-intel-openmp-thread-affinity-for-pinning_285.html
I think what happens is that the OpenMP or the OS moves the threads around
the cores, when it is busy, for no good reason at all, other than wanting
to run other processes, which kills cache performance.

The weight matrices are mostly small enough to fit in core-level cache, but
if they get moved, it will be much slower. The effect may be more intense
with the smaller models that I have coming soon.

I have often wondered whether it would work more reliably faster if
re-written to not use Open MP, but use low-level threads instead. That
would be a lot of work, and may not pay off, so I haven't done it.
Alternatively, if you want to run it on multiple images in parallel using
the parallel command, you might just get better performance by turning off
OpenMP entirely and running it single-threaded.

I agree that some of the (larger) code in functions.h should be de-inlined.

I have a question on you opening statement. When you say I/O bound, I
assume you mean between the core and main memory, not I/O to disk?

On Wed, Jul 12, 2017 at 6:46 PM, Justin Hotchkiss Palermo <
notifications@github.com> wrote:

> From general testing, it appears as though I'm I/O bound while using
> tesseract, and not CPU bound. I checked out the CPU Cache performance
> (profiled with valgrind and perf, but perf's output is easiest to read) and
> found that my machines have a high number of cache misses, and even a high
> number of iTLB misses. After inspecting the code, I also noticed that
> almost every function was defined as 'inline'. The high rate of iTLB misses
> surprised me. Usually iTLB misses are reduced after running a program for a
> longer period of time, but increasing the run time (for instance with an
> image with a lot more text) saw my iTLB miss rate increase, perhaps
> indicating that I am filling my iTLB, which is rare.
>
> The iTLB misses worry me, especially because running a program for a
> longer period of time typically reduces them (one inevitably has cache/tlb
> misses when executing something that the CPU has not seen, but once it is
> in the cache, it should stay in the cache...). iTLB misses are expensive on
> Intel CPUs. I have used OpenMP in the past, but I'm not sure how it handles
> an iTLB miss per 'thread'. If each thread hits a miss and hits a page fault
> exception, and it's non blocking (which makes sense), then it'd mean that
> each thread would cause a page walk for the same instructions. Not sure if
> that's what happens, but I'd love to know what does happen (or a book or
> resource) if someone knows more about how OpenMP handles them.
>
> I have seen code styled similarly at my last job, where the main part of
> the program was written over 25 years ago. At the time I expressed surprise
> (and alarm) to a co-worker, who explained that compilers used to be
> terrible at attempting to inline functions, and it wasn't until GCC 4 that
> there was a decent attempt to improve them. GCC 4 was released in 2005.
> (The version of GCC 4 you are using now has many updates and improvements,
> and is different from the GCC 4 in 2005, so GCC 4 is not that 'outdated',
> but it was the first major time where there were improvements.) So before
> 2005, best practice was to write code similar to how Tesseract identifies
> almost all functions as 'inline'.
>
> The 'inline' specifier carries some interesting aspects too, and was more
> 'important' to use with C, since C++ defaults to 'inline' when... (From
> https://gcc.gnu.org/onlinedocs/gcc-7.1.0/gcc/Inline.html#Inline
> <http://url>)
>
> As required by ISO C++, GCC considers member functions defined within the
> body of a class to be marked inline even if they are not explicitly
> declared with the inline keyword.
>
> Also, GCC can ignore 'inline' as it sees fit. The -O2 optimization option
> includes -finline-small-functions, which inlines functions that do not
> generate additional code. -O3 includes -finline-functions, which is more or
> less the same as writing 'inline' before every function, and only ignoring
> it if GCC decides that it may reduce performance. -O2 is in the default
> configure options, so adding 'inline' before each function is similar to
> just compiling with '-finline-functions', except that it is slightly more
> explicit.
>
> There are many downsides to defining a function as 'inline', but the one
> people usually seem to notice is increased compile time. There's plenty of
> information available on Google/GNU's documentation about inline, the
> history of inline, and slight differences between inline in g++ and GNU C
> back in the day.
>
> The biggest example of what I'd call (modern) "overactive inline use" in
> tesseract is probably in these files:
> https://github.com/tesseract-ocr/tesseract/blob/master/lstm/functions.h
> https://github.com/tesseract-ocr/tesseract/blob/master/lstm/functions.cpp
>
> functions.cpp has two lines of code, with almost everything in functions.h.
>
> This is also an example of compiler output with default options, i.e.
> keeping '-O2' and all of the different functions 'inlined' (with -Winline:
> https://gcc.gnu.org/onlinedocs/gcc-7.1.0/gcc/Inline.html#Inline ),
> defined as:
>
> Using -Winline warns when a function marked inline could not be
> substituted, and gives the reason for the failure.
>
> functions.h:63:15: warning: inlining failed in call to ‘double tesseract::Logistic(double)’: call is unlikely and code size would grow [-Winline]
>  inline double Logistic(double x) {
>                ^~~~~~~~
> functions.h:64:37: note: called from here
>    if (x < 0.0) return 1.0 - Logistic(-x);
>                              ~~~~~~~~^~~~
> functions.h:63:15: warning: inlining failed in call to ‘double tesseract::Logistic(double)’: call is unlikely and code size would grow [-Winline]
>  inline double Logistic(double x) {
>                ^~~~~~~~
> functions.h:64:37: note: called from here
>    if (x < 0.0) return 1.0 - Logistic(-x);
>                              ~~~~~~~~^~~~
> functions.h:63:15: warning: inlining failed in call to ‘double tesseract::Logistic(double)’: call is unlikely and code size would grow [-Winline]
>  inline double Logistic(double x) {
>                ^~~~~~~~
> functions.h:64:37: note: called from here
>    if (x < 0.0) return 1.0 - Logistic(-x);
>                              ~~~~~~~~^~~~
> functions.h:63:15: warning: inlining failed in call to ‘double tesseract::Logistic(double)’: call is unlikely and code size would grow [-Winline]
>  inline double Logistic(double x) {
>                ^~~~~~~~
> functions.h:64:37: note: called from here
>    if (x < 0.0) return 1.0 - Logistic(-x);
>                              ~~~~~~~~^~~~
> functions.h:45:15: warning: inlining failed in call to ‘double tesseract::Tanh(double)’: call is unlikely and code size would grow [-Winline]
>  inline double Tanh(double x) {
>                ^~~~
> functions.h:46:28: note: called from here
>    if (x < 0.0) return -Tanh(-x);
>                         ~~~~^~~~
>
> As a test on my target machines, I copied my 'tesseract' repository,
> removed all (easy to remove...) 'inline' functions, shifted the definition
> to the .cpp file instead of the .h file (leaving the declaration in .h as
> is standard practice), and compiled a separate version, installing it with
> a different prefix. This allowed me to have multiple versions of Tesseract
> on the same machine, and to run tests with all of them.
>
> Next, I made a script. Fairly simple, utilizing GNU Parallel
> CITATION:
>
> O. Tange (2011): GNU Parallel - The Command-Line Power Tool,
> ;login: The USENIX Magazine, February 2011:42-47.
>
> The simple bash scripts look like this:
>
> #!/bin/bash# CONTROL SCRIPT
> TESS=/usr/bin/tesseract
> F_LOC=(location of my test images)
> parallel $TESS $F_LOC/{1} stdout ::: img1.jpg img2.png img3.jpeg
>
> and
>
> #!/bin/bash# TEST SCRIPT
> TESS=/home/hotchkiss/usr/bin/tesseract
> F_LOC=(location of my test images)
> parallel $TESS $F_LOC/{1} stdout ::: img1.jpg img2.png img3.jpeg
>
> I'd run them ~50 times randomly(as random as /dev/urandom can afford on
> two options), 'randomly' defined as I randomly chose which to run, and
> collected the average run times and percentages as an aggregate using perf.
> Each run was after a fresh reboot, with no other programs running other
> than X, i3 (my windows manager), and my default kernel + kernel modules
> (less than 110MB when running 'free -m').
>
> Results look something like:
>
>  Performance counter stats for 'test_tess':
>
>       54913.737862      task-clock:u (msec)       #    6.757 CPUs utilized
>                  0      context-switches:u        #    0.000 K/sec
>                  0      cpu-migrations:u          #    0.000 K/sec
>            136,385      page-faults:u             #    0.002 M/sec
>    161,251,587,471      cycles:u                  #    2.936 GHz                      (29.05%)
>    154,963,746,273      instructions:u            #    0.96  insn per cycle           (36.65%)
>     35,080,176,770      branches:u                #  638.823 M/sec                    (42.77%)
>        306,913,327      branch-misses:u           #    0.87% of all branches          (41.68%)
>     50,274,555,586      L1-dcache-loads:u         #  915.519 M/sec                    (27.81%)
>      5,075,515,864      L1-dcache-load-misses:u   #   10.10% of all L1-dcache hits    (19.72%)
>        450,004,836      LLC-loads:u               #    8.195 M/sec                    (17.70%)
>         23,036,525      LLC-load-misses:u         #   10.24% of all LL-cache hits     (21.92%)
>    <not supported>      L1-icache-loads:u
>         11,674,082      L1-icache-load-misses:u                                       (28.02%)
>     48,676,934,856      dTLB-loads:u              #  886.425 M/sec                    (21.31%)
>          1,322,784      dTLB-load-misses:u        #    0.00% of all dTLB cache hits   (20.86%)
>            264,354      iTLB-loads:u              #    0.005 M/sec                    (17.36%)
>            690,732      iTLB-load-misses:u        #  261.29% of all iTLB cache hits   (21.42%)
>    <not supported>      L1-dcache-prefetches:u
>    <not supported>      L1-dcache-prefetch-misses:u
>
>        8.127386606 seconds time elapsed
>
> As expected, with no compiler optimizations and all functions with the
> 'inline' identifier removed, the performance was worse by a significant
> amount (8% to 10% worse). Also as expected with modern compilers (gcc 6.3
> and gcc 8.0... I haven't run 7.x stable yet, although I could tomorrow),
> the performance seems to be ~the same and (perhaps, although requires more
> testing to be definite) better on a CPU with smaller cache/tlb sizes.
>
> I used parallels to ensure that things were running in parallel, similar
> to what I will be using in production, and, what (hopefully) everyone else
> is doing too. I checked the other things people have posted, and it seems
> as though most others are doing the same too.
>
> I am posting this for a few reasons.
>
>    1. How are you collecting cache and performance information, is it
>    standardized, and how would you like any statistics or data to be
>    submitted/displayed?
>    2. Is it alright if I remove and submit code without 'inline' declared
>    on every function?
>    3. Should I submit code with 'inline' removed from functions others
>    have written?
>    4. What are your (other developer's) target machines like, and are my
>    results and thought process similar?
>
> Personally, I'm not a fan of manually marking all functions as inline, but
> I'm also (quite) used to re-writing code/libraries to optimize performance
> for my use case(s).
>
> I can wait on posting any code changes until @theraysmith
> <https://github.com/theraysmith> has the new beta tag out though. I'm not
> sure how these types of issues are handled on github either. I've spent
> most of my time working at a FFRDC, and haven't submitted much FOSS code
> publicly at all because of it, but I can post what I've done with this,
> which is great. Typically, I'd either write my own tests and submit along
> with compiled code (and a 'NO MERGE' pull request), or follow already
> provided tests and submit according to them, however I haven't seen any.
> What do you all prefer to use, and would this be something you're
> interested in? I absolutely abhor, detest, and strongly oppose programming
> for the compiler, but I feel like these changes would actually do the
> opposite, and allow the compiler to take over.
>
> I'm also profiling functions for more detailed optimization with
> cachegrind, and I'll absolutely post those after the changes have been
> submitted. Again, it's less of a 'logic' change, and in some cases more of
> a logic 'reordering' (like order of conditional if tests, what to
> inline/skip/spend more time on and the like).
>
> Another option that (might?) be decent, if it turns out that removing
> 'inline' does hurt performance on CPUs with large cache/tlb spaces, is that
> I can do something similar to what the Linux Kernel has. There is a "Allow
> gcc to uninline functions marked 'inline'" option (under 'Kernel Hacking')
> because they have run into the same issue.
> ------------------------------
> Environment
>
>    - *Tesseract Version*: 4.0x(latest -dev)
>    - *Platform*: Linux [hostname] 4.12.0 #1
>    <https://github.com/tesseract-ocr/tesseract/issues/1> SMP Tue Jul 11
>    14:56:49 EDT 2017 x86_64 Intel(R) Core(TM) i7-4770HQ CPU @ 2.20GHz
>    GenuineIntel GNU/Linux
>    (and, specs not with me right now, but posting anyway, an Intel Atom,
>    can post later).
>
> tested with:
> gcc version 8.0.0 20170711 (experimental) (GCC)
> gcc version 6.3.0 (Gentoo 6.3.0 p1.0)
> I can test with gcc 7.x later if need be. I haven't switched my
> development system's stable version of gcc to 7 yet, even though I still
> test the experimental svn branch.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1035>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AL056byjoySZrjs2w14KyfUye2KocwQ8ks5sNXbogaJpZM4OWZ7L>
> .
>



-- 
Ray.
  +1
Never like this allocated `char *` value.
It's better to return `std::string`.  What is those `libtesseract304` messages?
And what's the issue with `...leptonica: master`? I'm not aware of this.  We support only for C++. Please replicate problem with tesseract executable or C++ code. Otherwise please use tesseract user forum for your project.  Use forum for asking questions.

See https://github.com/tesseract-ocr/tesseract/wiki/Compiling

Tesseract versions and the minimum version of Leptonica required:
TesseractLeptonicaUbuntu
4.00 1.74.2 Must build from source
3.05 1.74.0 Must build from source
3.04 1.71 Ubuntu 16.04 <http://packages.ubuntu.com/xenial/libtesseract3>
3.03 1.70 Ubuntu 14.04 <http://packages.ubuntu.com/trusty/libtesseract3>
3.02 1.69 Ubuntu 12.04 <http://packages.ubuntu.com/precise/libtesseract3>
3.01 1.67

One option is to install the distro's Leptonica package:

sudo apt-get install libleptonica-dev

but if you are using an oldish version of Linux, the Leptonica version may
be too old, so you will need to build from source.

The sources are at https://github.com/DanBloomberg/leptonica . The
instructions for building are given in Leptonica README
<http://www.leptonica.org/source/README.html>.

Note that if building Leptonica from source, you may need to ensure that
/usr/local/lib is in your library path. This is a standard Linux bug, and
the information at Stackoverflow
<http://stackoverflow.com/questions/4743233/is-usr-local-lib-searched-for-shared-libraries>
is
very helpful.
  Amit,

Which kind of unicharsets does it  merge? 
* The script based ones given in langdata
* The training text based ones created during training process

Does it just append or also eliminate duplicates?

Where would a merged unicharset be used? In that case, we should add it to Makefile.am so that we can test and figure out what it does :-) It could be used to create a combined unicharset for a script-level engine, like the new Latin or Devanagari.
It isn't referenced by the current training documentation, but it might be useful to someone, so it should probably be added to Makefile.am. @theraysmith 

Is there a similar merge_language_model program, used for building a script-level engine?

Recently someone asked me:

```
Let us consider language as lat+san+guj 
```

Where lat is IAST or the roman transliteration of Sanskrit, in Latin script + English
san is Sanskrit in Devanagari script
guj is Gujarati in Gujarati script

So, something like this needs a combining of Devanagari + Gujarati + san_latn or IAST or Latin

What would be the best way to do this?

Can multiple training_files.txt for different languages be given as input for lstmtraining or do they need to be all merged in one big file?

Sample image below:
![multi-language](https://user-images.githubusercontent.com/5095331/30238960-8e4a80fe-956f-11e7-9f1d-28ac9a18caec.png)

Here is the merged unicharset for these languages:


[deva-iast-guj.lstm-unicharset.txt](https://github.com/tesseract-ocr/tesseract/files/1291305/deva-iast-guj.lstm-unicharset.txt)
 >It could be used to create a combined unicharset for a script-level engine, like the new Latin or Devanagari.
It isn't referenced by the current training documentation, but it might be useful to someone, so it should probably be added to Makefile.am.

Added by commit https://github.com/tesseract-ocr/tesseract/pull/1116/commits/9a038f893ad51e35157b393fcd95d1b661c528af  as part of PR https://github.com/tesseract-ocr/tesseract/pull/1116   You are right. The error is also there for pdf. So, even though error messages are shown, empty output is produced.

```
 tesseract nonexisting_image.tiff output pdf
Tesseract Open Source OCR Engine v4.00.00dev-549-g2b854e3 with Leptonica
Error in fopenReadStream: file not found
Error in findFileFormat: image file not found
Error during processing.
```
  You need to create the lstmf files and then do finetuning with same version of tesseract. If there is a mismatch, it may not work since Ray made some changes in the lstmf files format, probably around the following commit 

https://github.com/tesseract-ocr/tesseract/commit/8e79297dcefecdb929d753d28554fec51417ec39  ### Environment

* **Tesseract Version**: 4.00.00-dev
* **Platform**: Ubuntu 14.04

### Current Behavior:

While `finetune` training, `lstmtraining` is running subtrainer. After the regular training iteration reached 60,000, subtrainer training has started from 270 and it is updating subtrainer till it reaches current iteration. 

### Expected Behavior:

Either subtrainer should be run at regular intervals or if needs to catch up for a large number of iterations, it should save checkpoints, so that in case needed, the process can be restarted,

```
UpdateSubtrainer:Sub:At iteration 284/1500/1505, Mean rms=0.089%, delta=1.062%, char train=3.922%, word train=9.339%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 303/1600/1605, Mean rms=0.091%, delta=1.129%,                                                                                                                      char train=4.07%, word train=9.912%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 321/1700/1705, Mean rms=0.093%, delta=1.232%, char train=4.082%, word train=10.139%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 340/1800/1805, Mean rms=0.095%, delta=1.32%, char train=4.324%, word train=10.76%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 359/1900/1905, Mean rms=0.093%, delta=1.22%, char train=4.036%, word train=10.23%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 378/2000/2005, Mean rms=0.094%, delta=1.229%, char train=4.146%, word train=10.413%, skip ratio=0.4%,
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff83 20 ffffffe0 ffffffa4 ffffff95 ffffffe0 ffffffa5 ffffff80 ffffffe0 ffffffa4 ffffffb2 ffffffe0 ffffffa4 ffffff95 ffffffe0 ffffffa4 ffffff82 20 ffffffe0 ffffffa4 ffffffae ffffffe0 ffffffa4 ffffffae
Can't encode transcription: ललिता भट्टारिका देवता, ऐं बीजं क्लीं शक्तिः, सौः कीलकं मम
UpdateSubtrainer:Sub:At iteration 393/2100/2106, Mean rms=0.092%, delta=1.188%, char train=3.956%, word train=9.7%, skip ratio=0.5%,
UpdateSubtrainer:Sub:At iteration 406/2200/2206, Mean rms=0.089%, delta=1.092%, char train=3.606%, word train=9.478%, skip ratio=0.5%,
Encoding of string failed! Failure bytes: 7c
Can't encode transcription: अनेन भवति। अस्ति। " जायते -- सर्वत्र इह फलं सर्वेषां |
UpdateSubtrainer:Sub:At iteration 428/2300/2307, Mean rms=0.091%, delta=1.15%, char train=3.732%, word train=9.588%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 447/2400/2407, Mean rms=0.09%, delta=1.073%, char train=3.477%, word train=9.343%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 463/2500/2507, Mean rms=0.091%, delta=1.114%, char train=3.541%, word train=9.258%, skip ratio=0.2%,
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff82 20 ffffffe0 ffffffa4 ffffff85 ffffffe0 ffffffa4 ffffff99 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffff97 ffffffe0 ffffffa5 ffffff81 ffffffe0 ffffffa4 ffffffb7 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffa0 ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffffad ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffaf ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffff82 20 ffffffe0 ffffffa4 ffffffa8 ffffffe0 ffffffa4 ffffffae ffffffe0 ffffffa4 ffffff83
Can't encode transcription: हौं अङ्गुष्ठाभ्यां नमः
UpdateSubtrainer:Sub:At iteration 481/2600/2608, Mean rms=0.088%, delta=1.054%, char train=3.389%, word train=8.793%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 500/2700/2708, Mean rms=0.085%, delta=0.903%, char train=3.099%, word train=8.728%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 521/2800/2808, Mean rms=0.083%, delta=0.815%, char train=2.82%, word train=8.482%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 538/2900/2908, Mean rms=0.083%, delta=0.798%, char train=2.929%, word train=8.574%, skip ratio=0.3%,
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff83 20 ffffffe0 ffffffa4 ffffffa4 ffffffe0 ffffffa5 ffffff80 ffffffe0 ffffffa4 ffffff95 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffb7 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffa3 ffffffe0 ffffffa4 ffffffbe
Can't encode transcription: रात्रिसमागमप्रविष्टांः तीक्ष्णा
UpdateSubtrainer:Sub:At iteration 564/3000/3009, Mean rms=0.084%, delta=0.81%, char train=2.918%, word train=8.662%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 591/3100/3109, Mean rms=0.088%, delta=0.917%, char train=3.225%, word train=9.282%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 608/3200/3209, Mean rms=0.088%, delta=0.908%, char train=3.315%, word train=9.285%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 631/3300/3309, Mean rms=0.087%, delta=0.838%, char train=3.175%, word train=9.084%, skip ratio=0.2%,
UpdateSubtrainer:Sub:At iteration 660/3400/3409, Mean rms=0.089%, delta=0.87%, char train=3.226%, word train=9.473%, skip ratio=0.2%,
UpdateSubtrainer:Sub:At iteration 682/3500/3509, Mean rms=0.089%, delta=0.847%, char train=3.211%, word train=9.562%, skip ratio=0.2%,
UpdateSubtrainer:Sub:At iteration 703/3600/3609, Mean rms=0.091%, delta=0.942%, char train=3.393%, word train=9.826%, skip ratio=0.1%,
Compute CTC targets failed!
UpdateSubtrainer:Sub:At iteration 722/3700/3710, Mean rms=0.094%, delta=1.085%, char train=3.587%, word train=9.63%, skip ratio=0.2%,
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff82 ffffffe0 ffffffa4 ffffffa4 ffffffe0 ffffffa4 ffffffb0 ffffffe0 ffffffa4 ffffff83 20 ffffffe0 ffffffa4 ffffff96 ffffffe0 ffffffa4 ffffffb5 ffffffe0 ffffffa4 ffffffa4 ffffffe0 ffffffa5 ffffff8d
Can't encode transcription: पूर्णानंदोऽंतरः खवत्
Compute CTC targets failed!
UpdateSubtrainer:Sub:At iteration 741/3800/3812, Mean rms=0.093%, delta=1.055%, char train=3.462%, word train=9.397%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 762/3900/3912, Mean rms=0.095%, delta=1.14%, char train=3.546%, word train=9.602%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 783/4000/4012, Mean rms=0.095%, delta=1.127%, char train=3.544%, word train=9.55%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 799/4100/4112, Mean rms=0.09%, delta=0.988%, char train=3.106%, word train=8.698%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 821/4200/4212, Mean rms=0.09%, delta=0.99%, char train=3.007%, word train=8.819%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 843/4300/4312, Mean rms=0.089%, delta=0.993%, char train=2.846%, word train=8.479%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 865/4400/4412, Mean rms=0.089%, delta=1.027%, char train=2.962%, word train=8.361%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 888/4500/4512, Mean rms=0.089%, delta=1.047%, char train=2.997%, word train=8.497%, skip ratio=0.3%,
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff82 20 ffffffe0 ffffffa4 ffffffb8 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffb5 ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffffb9 ffffffe0 ffffffa4 ffffffbe
Can't encode transcription: । ॐ भुर्भुवःसुवः ह्सौं स्वाहा
UpdateSubtrainer:Sub:At iteration 908/4600/4613, Mean rms=0.089%, delta=0.987%, char train=2.92%, word train=8.331%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 932/4700/4713, Mean rms=0.089%, delta=0.953%, char train=2.907%, word train=8.546%, skip ratio=0.3%,
UpdateSubtrainer:Sub:At iteration 953/4800/4813, Mean rms=0.09%, delta=0.992%, char train=3.047%, word train=8.496%, skip ratio=0.1%,
UpdateSubtrainer:Sub:At iteration 970/4900/4913, Mean rms=0.089%, delta=0.958%, char train=2.918%, word train=8.252%, skip ratio=0.1%,
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff82 20 ffffffe0 ffffffa4 ffffffa8 20 ffffffe0 ffffffa4 ffffff95 ffffffe0 ffffffa5 ffffff81 ffffffe0 ffffffa4 ffffffb0 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffaf ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffffa4 ffffffe0 ffffffa5 ffffff8d 20 ffffffe0 ffffffa4 ffffffa4 ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffffb5 ffffffe0 ffffffa4 ffffffa6 ffffffe0 ffffffa5 ffffff8d 20 ffffffe0 ffffffa4 ffffff86 ffffffe0 ffffffa4 ffffffb5 ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffffaa ffffffe0 ffffffa4 ffffff83 20 ffffffe0 ffffffa4 ffffff95 ffffffe0 ffffffa4 ffffffbe ffffffe0 ffffffa4 ffffffb0 ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffffaf ffffffe0 ffffffa4 ffffff83
Can't encode transcription: यावद्ऽश्वरथद्विपानां युद्धसम्बाधन्ं न कुर्यात् तावद् आवापः कार्यः
Compute CTC targets failed!
Image too small to scale!! (1x48 vs min width of 3)
Line cannot be recognized!!
Image not trainable
UpdateSubtrainer:Sub:At iteration 994/5000/5016, Mean rms=0.088%, delta=0.962%, char train=2.835%, word train=8.095%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 1022/5100/5116, Mean rms=0.091%, delta=1.041%, char train=3.139%, word train=8.845%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 1046/5200/5216, Mean rms=0.091%, delta=1.029%, char train=3.143%, word train=8.942%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 1065/5300/5316, Mean rms=0.091%, delta=0.998%, char train=3.13%, word train=9.112%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 1084/5400/5416, Mean rms=0.09%, delta=0.959%, char train=3.039%, word train=8.949%, skip ratio=0.4%,
UpdateSubtrainer:Sub:At iteration 1111/5500/5516, Mean rms=0.089%, delta=0.931%, char train=2.965%, word train=9.074%, skip ratio=0.4%,

```
 My travis training build times out because of this - see
https://travis-ci.org/Shreeshrii/tess4training/builds/253954146

 I was using the large unicharset generated by the program. Ray suggested to hand-edit the unicharset till he updates unicharset extractor.

@theraysmith Still waiting for that commit.

I will close this as lstm training process has changed now with new starter trained data etc.  \n also need to be added to messages from
https://github.com/tesseract-ocr/tesseract/blob/master/lstm/lstmtrainer.cpp
and
https://github.com/tesseract-ocr/tesseract/blob/master/training/lstmtester.cpp

example of runon messages:

```
At iteration 67503/153800/153948, Mean rms=0.091%, delta=0.592%, char train=1.988%, word train=7.408%, skip ratio=0%,  New worst char error = 1.988At iteration 67166, stage 1, Eval Char error rate=9.8022655, Word error rate=39.409614 wrote checkpoint.
```  Please check if this is fixed by the latest set of commits by Ray.

[jpn-1.txt](https://github.com/tesseract-ocr/tesseract/files/1161897/jpn-1.txt)
[jpn-1.tsv.txt](https://github.com/tesseract-ocr/tesseract/files/1161899/jpn-1.tsv.txt)

```
level	page_num	block_num	par_num	line_num	word_num	left	top	width	height	conf	text
1	1	0	0	0	0	0	0	2550	470	-1	
2	1	1	0	0	0	104	96	2286	348	-1	
3	1	1	1	0	0	111	96	546	49	-1	
4	1	1	1	1	0	111	96	546	49	-1	
5	1	1	1	1	1	111	105	36	37	96	こ
5	1	1	1	1	2	160	100	48	45	96	ん
5	1	1	1	1	3	218	96	49	49	96	ば
5	1	1	1	1	4	272	100	48	45	96	ん
5	1	1	1	1	5	330	100	47	45	96	は
5	1	1	1	1	6	391	105	36	37	95	こ
5	1	1	1	1	7	440	100	48	45	96	ん
5	1	1	1	1	8	498	96	49	49	96	ば
5	1	1	1	1	9	552	100	48	45	96	ん
5	1	1	1	1	10	610	100	47	45	95	は
3	1	1	2	0	0	111	170	962	52	-1	
4	1	1	2	1	0	111	170	962	52	-1	
5	1	1	2	1	1	111	170	43	47	96	ご
5	1	1	2	1	2	158	171	107	50	95	飯
5	1	1	2	1	3	271	171	50	50	96	大
5	1	1	2	1	4	338	174	29	47	96	盛
5	1	1	2	1	5	0	0	2550	470	96	り
5	1	1	2	1	6	385	207	16	15	96	。
5	1	1	2	1	7	439	172	123	50	93	今
5	1	1	2	1	8	0	0	2550	470	95	年
5	1	1	2	1	9	567	171	65	51	96	は
5	1	1	2	1	10	624	173	87	48	96	初
5	1	1	2	1	11	0	0	2550	470	95	め
5	1	1	2	1	12	722	178	43	41	96	て
5	1	1	2	1	13	776	171	48	50	94	恋
5	1	1	2	1	14	832	173	101	48	96	人
5	1	1	2	1	15	944	171	48	50	96	出
5	1	1	2	1	16	1001	173	26	46	96	来
5	1	1	2	1	17	1021	191	25	28	96	た
5	1	1	2	1	18	1057	207	16	15	96	。
3	1	1	3	0	0	106	245	2284	126	-1	
4	1	1	3	1	0	106	245	2284	51	-1	
```
![jpn](https://user-images.githubusercontent.com/5095331/28412178-f63e0cee-6d60-11e7-82e0-7ec82cb13fc0.jpg)
 5	1	1	2	1	1	111	170	43	47	96	ご
5	1	1	2	1	2	158	171	107	50	95	飯
5	1	1	2	1	3	271	171	50	50	96	大
5	1	1	2	1	4	338	174	29	47	96	盛
5	1	1	2	1	5	0	0	2550	470	96	り
2	1	10	624	173	87	48	96	初
5	1	1	2	1	11	0	0	2550	470	95	め
5	1	1	2	1	12	722	178	43	41	96	て


Please check this . Still wrong coordinate in り and め character  I think this happens when the complex characters in your training text are not part of the original Korean Unicharset that the 4.00.00alpha kor.traineddata was trained with.

Do 'replace top layer' training instead of finetune. @abhishekchopde has had good results with it - see https://github.com/tesseract-ocr/tesseract/issues/1009

It will take longer than finetuning.

 also see
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#error-messages-from-training  Similar Issue - https://github.com/tesseract-ocr/tesseract/issues/884  Please use the latest source from master branch of github and inform whether you still get the error.

 what version of c++ are you using?

> two different OS

which ones? I have been able to build on ubuntu 14.04. Travis and appveyor builds are building ok. Also, are you able to run tesseract from command line ?

tesseract -v

also try to OCR the sample image from testing folder. what about

tesseract --list_langs

Are you able to OCR an image from command line with the 4.0 version?

Do you have 4.00.00alpha version of traineddata files? 

Download 4.0 traineddata to a different folder and refer to that  @nachobit Please see Quan's Java JNA wrapper for Tesseract OCR API at https://github.com/nguyenq/tess4j https://github.com/nguyenq/tess4j/commit/74c850980af1268c100f8798ed475e6d69fe34b8

+Version 4.0.0 beta (8 June 2017)
 +- Upgrade to Tesseract 4.0.0 alpha (8c29e68)
 +- Update Lept4J to 1.5.0 (Leptonica 1.74.2)

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Jun 29, 2017 at 3:40 PM, Nacho Romero <notifications@github.com>
wrote:

> Updated to the lastest libs from *Tess4J-3.4.0-src* I get same error when
> launch the OCR from Java code.
>
> From 3.05.01 version, is there any solution to solve the fail recognizing
> "zeros" ( º instead of 0)?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/1010#issuecomment-311923010>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_oxFIpUnAMuY0-FEdUVI_o0dR3pxUks5sI3f8gaJpZM4OH3Lp>
> .
>
 We do not support 3rd party sw including tesseract wrapper. Please reproduce error with c++  * **Tesseract Version**:  Tesseract 4.00 alpha
* **Platform**:  Linux -Ubuntu 14.04 64 bit 

I have trained a few layers of the base tesseract for Korean language. I have completed my training. Now I am testing it for different images containing Korea text. I am getting the excellent character accuracy but it is detecting extra spaces after each character so my word accuracy is more than 100%. I read another issue about the same problem in Chinese OCR but not much was mentioned over there. SO can someone please throw some light on this?
@stweil @theraysmith 

Here is the comparison between groundtruth and my tesseract output:- 
![screenshot from 2017-06-27 16 10 35](https://user-images.githubusercontent.com/26093128/27583775-2f0df1bc-5b53-11e7-843c-7b04570220d3.png)

 Is the problem only with your traineddata or also with Korean traineddata posted for 4.00.00alpha?

Please do a similar test as above with kor.traineddata from tessdata repo. Yes. WIth googles tesseract also I am getting extra spaces. I got the solution for this.
Seems like we have to set the config variable - "preserve_interword_spaces" to 1 who's default value is 0 while running tesseract. The problem is solved by this for Korean language.

#991    Please use tesseract-ocr forum for asking questions and close this issue.

`--fonts_dir=/usr/share/fonts/ --font=Arial`

You should ensure that your fonts path and font name match what is available on your system. eg. If you want to train Arabic language, make sure you use a font which supports Arabic. 

You can use `text2image --list_available_fonts --fonts_dir=/usr/share/fonts/ ` to see what fonts are installed on your system and their names.  1. Ask question in forum.
2. 4.0 was not released e.g. it is not sure if all 3.0 features will be supported in final 4.0...  https://github.com/tesseract-ocr/tesseract/wiki/VGSLSpecs#variable-size-inputs-and-summarizing-lstm

https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/6ModernizationEfforts.pdf @theraysmith is the right person to answer this.  * **Tesseract Version**: 3.05.01 
* **Commit Number**: latest code from 3.05 branch
* **Platform**: ubuntu 14.04

### Current Behavior:

First reported on forum - https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/g8lTxxYiJ2E/KL07qF_LAQAJ

Build is failing with the following:

```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11  -L/usr/local/lib -o tesseract tesseract-tesseractmain.o libtesseract.la  -lrt -lpthread
libtool: link: g++ -g -O2 -std=c++11 -o .libs/tesseract tesseract-tesseractmain.o  -L/usr/local/lib ./.libs/libtesseract.so -lrt -lpthread
/usr/bin/ld: tesseract-tesseractmain.o: undefined reference to symbol 'lept_free'
//usr/local/lib/liblept.so.5: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
make[2]: *** [tesseract] Error 1
make[2]: Leaving directory `/home/sanskrit/tesseract-3.05/api'
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory `/home/sanskrit/tesseract-3.05'
make: *** [all] Error 2
```

This is using the latest code from github for leptonica as well as tesseract 3.05 branch.

Build was done using following commands:

```
git clone -q --branch=3.05 https://github.com/tesseract-ocr/tesseract.git tesseract-3.05
cd tesseract-3.05
./autogen.sh
PKG_CONFIG_PATH=/usr/local/lib/pkgconfig 
LIBLEPT_HEADERSDIR=/usr/local/include
./configure --with-extra-includes=/usr/local/include --with-extra-libraries=/usr/local/lib
make
``` Same leptonica works for tesseract master branch

```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11  -L/usr/local/lib -o tesseract tesseract-tesseractmain.o libtesseract.la -L/usr/local/lib -llept   -fopenmp  -lrt -lpthread
libtool: link: g++ -g -O2 -std=c++11 -o .libs/tesseract tesseract-tesseractmain.o -fopenmp  -L/usr/local/lib ./.libs/libtesseract.so /usr/local/lib/liblept.so -lrt -lpthread -fopenmp
make[2]: Leaving directory `/home/sanskrit/tesseract/api'
Making all in .
make[2]: Entering directory `/home/sanskrit/tesseract'
make[2]: Leaving directory `/home/sanskrit/tesseract'
Making all in tessdata
make[2]: Entering directory `/home/sanskrit/tesseract/tessdata'
Making all in configs
make[3]: Entering directory `/home/sanskrit/tesseract/tessdata/configs'
make[3]: Nothing to be done for `all'.
make[3]: Leaving directory `/home/sanskrit/tesseract/tessdata/configs'
Making all in tessconfigs
make[3]: Entering directory `/home/sanskrit/tesseract/tessdata/tessconfigs'
make[3]: Nothing to be done for `all'.
make[3]: Leaving directory `/home/sanskrit/tesseract/tessdata/tessconfigs'
make[3]: Entering directory `/home/sanskrit/tesseract/tessdata'
make[3]: Nothing to be done for `all-am'.
make[3]: Leaving directory `/home/sanskrit/tesseract/tessdata'
make[2]: Leaving directory `/home/sanskrit/tesseract/tessdata'
Making all in doc
make[2]: Entering directory `/home/sanskrit/tesseract/doc'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/home/sanskrit/tesseract/doc'
make[1]: Leaving directory `/home/sanskrit/tesseract'
``` I notice some differences in the libtool commands for 3.05  and 4.0

3.05
```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11  \
-L/usr/local/lib -o tesseract tesseract-tesseractmain.o libtesseract.la  \
-lrt -lpthread

libtool: link: g++ -g -O2 -std=c++11 -o .libs/tesseract tesseract-tesseractmain.o  \
-L/usr/local/lib ./.libs/libtesseract.so \
-lrt -lpthread
```

4.0
```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11  \
-L/usr/local/lib -o tesseract tesseract-tesseractmain.o libtesseract.la \
-L/usr/local/lib -llept   -fopenmp  \
-lrt -lpthread

libtool: link: g++ -g -O2 -std=c++11 -o .libs/tesseract tesseract-tesseractmain.o -fopenmp \
-L/usr/local/lib ./.libs/libtesseract.so \
 /usr/local/lib/liblept.so \
-lrt -lpthread -fopenmp
``` I was following the instructions from the compiling page. I will try again just with PKG_CONFIG_PATH. I have tried multiple times, different options, still getting same error.

Do not know whether it is because I am using the latest version of leptonica rather than 1.74.2.

@danbloomberg Has anything changed between 1.74.2 to latest code to cause this breakage? Dan, 

I had built leptonica with latest code from github. I was able to build master branch of tesseract with it. However, when I tried building the 3.05 branch it failed with the error related to lept_free().

I now rebuilt leptonica 1.74.2 ( git reset --hard 395728da046016859864eeb99c7a48121acaeb74) and am able to build tesseract 3.05.

```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -std=c++11   -o tesseract tesseract-tesseractmain.o libtesseract.la  -lrt -lpthread
libtool: link: g++ -g -std=c++11 -o tesseract tesseract-tesseractmain.o  ./.libs/libtesseract.a -L/usr/local/lib /usr/local/lib/liblept.so -lz -lpng12 -ljpeg -lgif /usr/lib/x86_64-linux-gnu/libtiff.so -lwebp -lopenjp2 -lrt -lpthread

make[2]: Leaving directory `/home/sanskrit/tesseract-3.05/api'
Making all in .
make[2]: Entering directory `/home/sanskrit/tesseract-3.05'
make[2]: Leaving directory `/home/sanskrit/tesseract-3.05'
Making all in tessdata
```
 ```
$ pkg-config --cflags lept
-I/usr/local/include/leptonica
$ pkg-config --libs lept
-L/usr/local/lib -llept
$ PKG_CONFIG_PATH=/usr/local/lib/pkgconfig pkg-config --cflags lept
-I/usr/local/include/leptonica
$ PKG_CONFIG_PATH=/usr/local/lib/pkgconfig pkg-config --libs lept
-L/usr/local/lib -llept
``` Here is config.log from the build with 1.74.4 - which had the error.

[config.log.txt](https://github.com/tesseract-ocr/tesseract/files/1089334/config.log.txt)

 Dan, Leptonica is building fine. The problem is in building tesseract 3.05 branch.

Stefan, I will try building with your 3.05 and report.

Thank you both for your prompt responses. @stweil Tesseract part of the build happened correctly using your 3.05. 

```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11   -o tesseract tesseract-tesseractmain.o libtesseract.la -L/usr/local/lib -llept     -lrt -lpthread
libtool: link: g++ -g -O2 -std=c++11 -o .libs/tesseract tesseract-tesseractmain.o  ./.libs/libtesseract.so -L/usr/local/lib /usr/local/lib/liblept.so -lrt -lpthread
make[2]: Leaving directory `/home/sanskrit/tesseract-3.05/api'
Making all in .
make[2]: Entering directory `/home/sanskrit/tesseract-3.05'
make[2]: Leaving directory `/home/sanskrit/tesseract-3.05'
Making all in tessdata
...
```

I am now getting similar error with `make training`.

```
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11   -o text2image text2image.o libtesseract_training.la libtesseract_tessopt.la -licui18n -licuuc -licudata   -licuuc -licudata    ../api/libtesseract.la -licuuc -licudata   -lpango-1.0 -lpangocairo-1.0 -lgobject-2.0 -lglib-2.0 -lcairo -lpangoft2-1.0 -lfontconfig  -lpthread
libtool: link: g++ -g -O2 -std=c++11 -o .libs/text2image text2image.o  ./.libs/libtesseract_training.a ./.libs/libtesseract_tessopt.a -licui18n ../api/.libs/libtesseract.so -licuuc -licudata -lpango-1.0 -lpangocairo-1.0 -lgobject-2.0 -lglib-2.0 /usr/lib/x86_64-linux-gnu/libcairo.so -lpangoft2-1.0 -lfontconfig -lpthread
/usr/bin/ld: text2image.o: undefined reference to symbol 'pixGetHeight'
//usr/local/lib/liblept.so.5: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
make[1]: *** [text2image] Error 1
make[1]: Leaving directory `/home/sanskrit/tesseract-3.05/training'
make: *** [training] Error 2
make[1]: Entering directory `/home/sanskrit/tesseract-3.05/training'
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11   -o text2image text2image.o libtesseract_training.la libtesseract_tessopt.la -licui18n -licuuc -licudata   -licuuc -licudata    ../api/libtesseract.la -licuuc -licudata   -lpango-1.0 -lpangocairo-1.0 -lgobject-2.0 -lglib-2.0 -lcairo -lpangoft2-1.0 -lfontconfig  -lpthread
libtool: link: g++ -g -O2 -std=c++11 -o .libs/text2image text2image.o  ./.libs/libtesseract_training.a ./.libs/libtesseract_tessopt.a -licui18n ../api/.libs/libtesseract.so -licuuc -licudata -lpango-1.0 -lpangocairo-1.0 -lgobject-2.0 -lglib-2.0 /usr/lib/x86_64-linux-gnu/libcairo.so -lpangoft2-1.0 -lfontconfig -lpthread
/usr/bin/ld: text2image.o: undefined reference to symbol 'pixGetHeight'
//usr/local/lib/liblept.so.5: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
make[1]: *** [text2image] Error 1
make[1]: Leaving directory `/home/sanskrit/tesseract-3.05/training'
make: *** [training-install] Error 2
``` Thank you, @stweil. I was able to build 3.05 branch with the latest leptonica.

```
~$ tesseract -v
tesseract 3.05.01
 leptonica-1.74.2
  libgif 4.1.6(?) : libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.0 : libopenjp2 2.1.2
```

However, even though I have built the latest leptonica (1.74.4), version info is still reporting 1.74.2. So, you maybe right that there maybe some old  file somewhere ...

Is there a way to check the version info for leptonica, other than through `tesseract -v`?

```
$ ls /usr/local/lib/*lept* -l
-rw-r--r-- 1 root root 16248792 Jun 21 02:56 /usr/local/lib/liblept.a
-rwxr-xr-x 1 root root     1024 Jun 21 02:56 /usr/local/lib/liblept.la
lrwxrwxrwx 1 root root       16 Jun 21 02:56 /usr/local/lib/liblept.so -> liblept.so.5.0.1
lrwxrwxrwx 1 root root       16 Jun 21 02:56 /usr/local/lib/liblept.so.5 -> liblept.so.5.0.1
-rwxr-xr-x 1 root root  9922717 Jun 21 02:56 /usr/local/lib/liblept.so.5.0.1

$ ls /usr/local/bin/tesseract -l
-rwxr-xr-x 1 root root 32298085 Jun 21 05:58 /usr/local/bin/tesseract


$ ls /usr/local/include/*lept* -l
total 576
-rw-r--r-- 1 root root 260377 Jun 21 02:56 allheaders.h
``` ```
$ find /usr -name *lept*
/usr/share/doc/liblept4
/usr/share/doc/liblept5
/usr/lib/liblept.so.4.0.2
/usr/lib/liblept.so.4
/usr/lib/x86_64-linux-gnu/liblept.so.5.0.1
/usr/lib/x86_64-linux-gnu/liblept.so.5
/usr/local/include/leptonica
/usr/local/include/leptonica/leptwin.h
/usr/local/lib/liblept.so.5.0.1
/usr/local/lib/liblept.so.5
/usr/local/lib/liblept.la
/usr/local/lib/pkgconfig/lept.pc
/usr/local/lib/liblept.a
/usr/local/lib/liblept.so

$  ldd /usr/local/bin/tesseract
        linux-vdso.so.1 =>  (0x00007ffe1c126000)
        liblept.so.5 => /usr/local/lib/liblept.so.5 (0x00007f2d0d207000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f2d0cfe9000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f2d0cce4000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f2d0c9de000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f2d0c7c8000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f2d0c3ff000)
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f2d0c1e6000)
        libpng12.so.0 => /lib/x86_64-linux-gnu/libpng12.so.0 (0x00007f2d0bfc0000)
        libjpeg.so.8 => /usr/lib/x86_64-linux-gnu/libjpeg.so.8 (0x00007f2d0bd6a000)
        libgif.so.4 => /usr/lib/x86_64-linux-gnu/libgif.so.4 (0x00007f2d0bb61000)
        libtiff.so.5 => /usr/lib/x86_64-linux-gnu/libtiff.so.5 (0x00007f2d0b8ef000)
        libwebp.so.5 => /usr/lib/x86_64-linux-gnu/libwebp.so.5 (0x00007f2d0b696000)
        libopenjp2.so.7 => /usr/lib/x86_64-linux-gnu/libopenjp2.so.7 (0x00007f2d0b45b000)
        /lib64/ld-linux-x86-64.so.2 (0x000055d6a8161000)
        liblzma.so.5 => /lib/x86_64-linux-gnu/liblzma.so.5 (0x00007f2d0b238000)
        libjbig.so.0 => /usr/lib/x86_64-linux-gnu/libjbig.so.0 (0x00007f2d0b02a000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f2d0ae26000)

$ LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/tesseract -v
tesseract 4.00.00dev-549-g2b854e3
 leptonica-1.74.2
  libgif 4.1.6(?) : libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.0 : libopenjp2 2.1.2

``` But I built using latest leptonica sources, config.log as well as allheaders.h have patch level as 4.

I will rebuild again and see. 

Thanks! I built leptonica again after doing `make distclean` , now I get the correct lib version. Thanks!

```
tesseract -v
tesseract 4.00.00dev-549-g2b854e3
 leptonica-1.74.4
  libgif 4.1.6(?) : libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.0 : libopenjp2 2.1.2

``` @DanBloomberg 

I usually update leptonica by using the following script in the leptonica directory.

```
#!/bin/bash
git pull origin
./autobuild
./configure --disable-dependency-tracking 
#./configure
make
sudo make install
sudo ldconfig
```
I follow a similar build script for tesseract master branch for building 4.0. 

Based on an error report in the forum, I tried building 3.05.01 and received the error related to lept_free (reported above).

As part of the experimentation, I did a hard reset in leptonica directory at one time to get to the commit for 1.74.2 and built leptonica and tesseract 3.05.01 after that without any problem.

@stweil then backported a couple of commits to tesseract 3.05.01 branch to fix the problem regarding lept_free() and I rebuilt leptonica with the latest source and then built tesseract 3.05.01. It built without the lept_free error.

It is possible that at this time, I did not build leptonica using the script which does autobuild and configure, but rather just did
```
git pull origin
make
sudo make install
```
I also built tesseract 4.0 from master branch after that. I noticed in both of these, that leptonica version was reported as 1.74.2.

After following Amit and Stefan's suggestions on ways to check whether I had multiple versions of files causing the older version to display, I decided to rebuild leptonica again. 

This time I did `make distclean` in leptonica directory before running the build script.

Then without needing to rebuild tesseract, `tesseract -v` showed that it was using leptonica 1.74.4. 

@amitdo So, as mentioned by Stefan, it is not necessary to rebuild tesseract to use the newer leptonica library.

@DanBloomberg It is possible, the older version display was because I did not run autobuild and configure when rebuilding leptonica. 
 The wiki pages are public and have been updated by different people at different times.  I did update the Compiling wiki page with the needed leptonica version recently. 

There are multiple Install/Compiling instructions.
I think that https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT.md is better. 

I will try to streamline the compiling page - maybe delete duplicated info and refer to the installation page instead. However, it will require review by @stweil @egorpugin and @zdenop .  Tesseract works best on 300 dpi black and white images.

Preprocess your image, you will get better result.

See attached.

![color-resized-png](https://user-images.githubusercontent.com/5095331/27285596-ced0cec0-551a-11e7-8fc6-195b4365f206.png)

Here is the OCRed result using
Tesseract Open Source OCR Engine v4.00.00dev-549-g2b854e3 with Leptonica

```
13
14 ## Parameters
15
16 _- 'user' - A User struct.
17
18 - ## Examples
19
20 user = %User{name: "Alice Winston" }
21 User. first_name(user)
22 "Alice"
23 serts
24 - def first_name(user) do
25 user
26 [> Bplit
27 [> first
28 end
29
30 @doc nnn
31 Get the last name of a user.
32
33 ## Parameters
34
35 - 'user' - A User struct.
user. exs unix < utf-8 < elixir _ 50%
``` I used irfanview interactively to resize the image to a larger size, increase dpi to 300, convert to grayscale, reduce color depth to 2 and invert the colors.

You should be able to do similar conversion using imagemagick. I had resized to 1920 x 1080 and 300 dpi.

You have to experiment with the settings and see what works best. See
https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality for more
tips.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Jun 20, 2017 at 1:04 PM, Phillipp Ohlandt <notifications@github.com>
wrote:

> @Shreeshrii <https://github.com/shreeshrii> The best I can get is this:
>
> [image: frame_24721-2]
> <https://user-images.githubusercontent.com/3123549/27321677-64de42d2-559b-11e7-8d3a-f405b18d5fbf.png>
>
> Using this command:
>
> convert -units PixelsPerInch input.png -resize 1200 -density 300 -colorspace gray -depth 1 -negate output.png
>
> Not sure if imagemagick can do better.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/997#issuecomment-309669745>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o-0wgvHJPxFAHCJURDfSBTRHEUMzks5sF3XzgaJpZM4N-Jpf>
> .
>
  Many fixes have been made to master branch for 4.0 since the 4.00.00alpha release in November 2016. A number of assertions have been fixed. 

@zdenop Please add a new tag eg. 4.0.0alpha-1 / 2 (numbering as you consider appropriate). Thanks! I have not seen any comments against semver.

Maybe good to setup some kind of autoupdate for increasing the PATCH
version based on commit numbers to reduce manual administrative updates.

@stweil From what I have read about semver, if you were to implement the
zipped traineddata and related changes, it should cause a change in MINOR
version.

So, with that should it be 4.1.0alpha ?

```
Given a version number MAJOR.MINOR.PATCH, increment the:

MAJOR version when you make incompatible API changes,

MINOR version when you add functionality in a backwards-compatible manner,

and
PATCH version when you make backwards-compatible bug fixes.

Additional labels for pre-release and build metadata are available as
extensions to the MAJOR.MINOR.PATCH format.
```
 First 4 version will be 4.0.0. What 4.1.0alpha are you talking about? We don't care about changes in dev branches. > We don't care about changes in dev branches.

OK. 

Still, it will be good to have new tags when changes are substantial enough from previous  commits. For example, 

* change of LSTM mode from --oem 4 to --oem 1 after  removal of cube
* change in .lstmf and .lstm file formats after update regarding endianness
* proposed change in traineddata files to zipped format

That said, I have only done some cursory reading regarding semver. So, I am happy with whatever tag/version is used, as long as there is some demarcation.

The reason for asking for this is that people are using/trying to use master branch/4.0/LSTM and ask questions, where the version info says -alpha or -dev and it difficult to try and figure out what the issue is without knowing the version being used.
 I vote for this format which includes date - easy to identify which version is more recent.

```4.0.0-beta.20170619 ``` Please see https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/V1tyGHIenbI/SUVuXheJAwAJ

An example of how 4.00.00alpha is NOT compatible with the current master branch eg. --oem options. I'm about ready to update the traineddatas. I have a training run almost
complete, and with accuracy that meets with my satisfaction.
There are a few regressions, but not too serious.
First though, I have to get some code reviewed in Google, and then make
some commits to github to match the new traineddatas.
Before that, there is the matter of a major pull...

Here's what's coming:

   - Fix to issue 653: New components in traineddata file for the
   unicharset, recoder and version string. Backwards compatible change, so the
   LSTM component can still read older files.
   - Change in training system. The above change makes open source training
   impossible. Will add a new program to build a starter traineddata from a
   unicharset and optional word lists.
   - New "normalization" code to clean corpus text in all languages. That
   was a big part of the work.
   - Improvements to the trained networks to improve accuracy on single
   characters and single words.
   - 2 parallel sets of tessdata. "best" and "fast". "Fast" will exceed the
   speed of legacy Tesseract in real time, provided you have the required
   parallelism components, and in total CPU only slightly slower for English.
   Way faster for most non-latin languages, while being <5% worse than "best"
   Only "best" will be retrainable, as "fast" will be integer.

I have other stuff that is still incomplete, but that is a good list for
now.

BTW, in case you hadn't noticed, there was a breaking change that made old
lstmf files unusable. That was needed to fix LSTM for OSD. It has to know
the language of each training sample.
The new traineddatas will mostly be smaller than the older ones, as they
won't contain the legacy components, and no bigram dawgs are needed.

On Tue, Jul 11, 2017 at 4:49 AM, Amit D. <notifications@github.com> wrote:

> @zdenop <https://github.com/zdenop>, can you do it, or at least add your
> comment here?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-314419211>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SvL5FeeE09JYW01xQ-dQyILyU8Wks5sM2ExgaJpZM4N9Nel>
> .
>



-- 
Ray.
 @theraysmith Thanks for the update. Look forward to it. Any estimate of expected date?

@zdenop I think this is a good reason to freeze the 'alpha' state by tagging the repo with the current version as 4.0.0-alpha.YYYYMMDD, since Ray is going to be making major changes. @Shreeshrii: I do not understand what do you want. Tag will not freeze anything.  Tag is just specific points in history to mark something important (e.g. new version). Tagging should be driven by developer who knows roadmap and not by users... @zdenop 

>Tag is just specific points in history to mark something important (e.g. new version).

Exactly my point :-)

When Ray makes his next set of commits, that will change the codebase as well as traineddata substantially. I am sure it will be tagged by Ray at that time, probably as a beta or release candidate.

My request to you to tag current commit (as an example) is to mark a point in history where a lot of development has taken place since the original 4.00.00alpha tag. In fact, that original tag just marked the start of the 4.00.00alpha development and many bugs in that original tag (missing lstm.train file etc.) have been fixed later.

Also, if the new changes by Ray will not allow for open source training :-(  then the current github version will be the one which allows users to do their own training. So, it is certainly deserving of a tag in my opinion :-)


 Open source training:
OK, I overstated it a bit.
One of my commits will temporarily break the training process. After doing
so, I will correct the documentation and add the new tool (which I have
already written) as quickly as possible after.

To help:
No more breaking commits! If it doesn't produce perfect results on
phototest, it broke something!
Cutting down on the code cleanup while I am working on it will also help.
When I have committed the new corpus cleanup code, it would be useful to
have any experts in any of the following scripts review the code and make
comments:
Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada,
Malayalam, Sinhala, Thai, Myanmar, Khmer.
There are script-specific cleanup rules in there.
Since I plan to commit new copies of the training data (unicharsets,
wordlists, training text etc) then at that point they will match


Dates:
I was going to get started this week, but now I have to debug my pull from
github, which has broken tests (of the legacy engine), so that will take
time to fix. I'm hoping it's simple, but it is bizarre.
Even when it is fixed, there are 1500 lines of change from github for
someone here to review.
I *really* want to get 4.00 finished (in beta) in the next 5-6 weeks.

On Tue, Jul 11, 2017 at 11:27 PM, Shreeshrii <notifications@github.com>
wrote:

> @zdenop <https://github.com/zdenop>
>
> Tag is just specific points in history to mark something important (e.g.
> new version).
>
> Exactly my point :-)
>
> When Ray makes his next set of commits, that will change the codebase as
> well as traineddata substantially. I am sure it will be tagged by Ray at
> that time, probably as a beta or release candidate.
>
> My request to you to tag current commit (as an example) is to mark a point
> in history where a lot of development has taken place since the original
> 4.00.00alpha tag. In fact, that original tag just marked the start of the
> 4.00.00alpha development and many bugs in that original tag have been fixed
> by now.
>
> Also, if the new changes by Ray will not allow for open source training
> :-( then the current github version will be the one which allows users to
> do their own training. So, it is certainly deserving of a tag in my opinion
> :-)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-314667002>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056e0RzuP9Hpok6mT4eU026fofCwaBks5sNGdRgaJpZM4N9Nel>
> .
>



-- 
Ray.
 >When I have committed the new corpus cleanup code, it would be useful to
have any experts in any of the following scripts review the code and make
comments:
Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada,
Malayalam, Sinhala, Thai, Myanmar, Khmer.
There are script-specific cleanup rules in there.

​What kind of expertise do you need regarding the Indic scripts?​


ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Jul 12, 2017 at 10:58 PM, theraysmith <notifications@github.com>
wrote:

> Open source training:
> OK, I overstated it a bit.
> One of my commits will temporarily break the training process. After doing
> so, I will correct the documentation and add the new tool (which I have
> already written) as quickly as possible after.
>
> To help:
> No more breaking commits! If it doesn't produce perfect results on
> phototest, it broke something!
> Cutting down on the code cleanup while I am working on it will also help.
> When I have committed the new corpus cleanup code, it would be useful to
> have any experts in any of the following scripts review the code and make
> comments:
> Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada,
> Malayalam, Sinhala, Thai, Myanmar, Khmer.
> There are script-specific cleanup rules in there.
> Since I plan to commit new copies of the training data (unicharsets,
> wordlists, training text etc) then at that point they will match
>
>
> Dates:
> I was going to get started this week, but now I have to debug my pull from
> github, which has broken tests (of the legacy engine), so that will take
> time to fix. I'm hoping it's simple, but it is bizarre.
> Even when it is fixed, there are 1500 lines of change from github for
> someone here to review.
> I *really* want to get 4.00 finished (in beta) in the next 5-6 weeks.
>
> On Tue, Jul 11, 2017 at 11:27 PM, Shreeshrii <notifications@github.com>
> wrote:
>
> > @zdenop <https://github.com/zdenop>
> >
> > Tag is just specific points in history to mark something important (e.g.
> > new version).
> >
> > Exactly my point :-)
> >
> > When Ray makes his next set of commits, that will change the codebase as
> > well as traineddata substantially. I am sure it will be tagged by Ray at
> > that time, probably as a beta or release candidate.
> >
> > My request to you to tag current commit (as an example) is to mark a
> point
> > in history where a lot of development has taken place since the original
> > 4.00.00alpha tag. In fact, that original tag just marked the start of the
> > 4.00.00alpha development and many bugs in that original tag have been
> fixed
> > by now.
> >
> > Also, if the new changes by Ray will not allow for open source training
> > :-( then the current github version will be the one which allows users to
> > do their own training. So, it is certainly deserving of a tag in my
> opinion
> > :-)
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/995#
> issuecomment-314667002>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/
> AL056e0RzuP9Hpok6mT4eU026fofCwaBks5sNGdRgaJpZM4N9Nel>
> > .
> >
>
>
>
> --
> Ray.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-314839820>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o71HIG266aj--aGRLLsL6s9gxF_Xks5sNQIjgaJpZM4N9Nel>
> .
>
 The code determines what makes a valid/invalid sequence of unicodes in the
script, for instance, is it allowed to have two matras in a row? It gets
more difficult with questions over what category the additional characters
are.

On Wed, Jul 12, 2017 at 6:40 PM, Shreeshrii <notifications@github.com>
wrote:

> >When I have committed the new corpus cleanup code, it would be useful to
> have any experts in any of the following scripts review the code and make
> comments:
> Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada,
> Malayalam, Sinhala, Thai, Myanmar, Khmer.
> There are script-specific cleanup rules in there.
>
> ​What kind of expertise do you need regarding the Indic scripts?​
>
>
> ShreeDevi
> ____________________________________________________________
> भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com
>
> On Wed, Jul 12, 2017 at 10:58 PM, theraysmith <notifications@github.com>
> wrote:
>
>
> > Open source training:
> > OK, I overstated it a bit.
> > One of my commits will temporarily break the training process. After
> doing
> > so, I will correct the documentation and add the new tool (which I have
> > already written) as quickly as possible after.
> >
> > To help:
> > No more breaking commits! If it doesn't produce perfect results on
> > phototest, it broke something!
> > Cutting down on the code cleanup while I am working on it will also help.
> > When I have committed the new corpus cleanup code, it would be useful to
> > have any experts in any of the following scripts review the code and make
> > comments:
> > Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada,
> > Malayalam, Sinhala, Thai, Myanmar, Khmer.
> > There are script-specific cleanup rules in there.
> > Since I plan to commit new copies of the training data (unicharsets,
> > wordlists, training text etc) then at that point they will match
> >
> >
> > Dates:
> > I was going to get started this week, but now I have to debug my pull
> from
> > github, which has broken tests (of the legacy engine), so that will take
> > time to fix. I'm hoping it's simple, but it is bizarre.
> > Even when it is fixed, there are 1500 lines of change from github for
> > someone here to review.
> > I *really* want to get 4.00 finished (in beta) in the next 5-6 weeks.
> >
> > On Tue, Jul 11, 2017 at 11:27 PM, Shreeshrii <notifications@github.com>
> > wrote:
> >
> > > @zdenop <https://github.com/zdenop>
> > >
> > > Tag is just specific points in history to mark something important
> (e.g.
> > > new version).
> > >
> > > Exactly my point :-)
> > >
> > > When Ray makes his next set of commits, that will change the codebase
> as
> > > well as traineddata substantially. I am sure it will be tagged by Ray
> at
> > > that time, probably as a beta or release candidate.
> > >
> > > My request to you to tag current commit (as an example) is to mark a
> > point
> > > in history where a lot of development has taken place since the
> original
> > > 4.00.00alpha tag. In fact, that original tag just marked the start of
> the
> > > 4.00.00alpha development and many bugs in that original tag have been
> > fixed
> > > by now.
> > >
> > > Also, if the new changes by Ray will not allow for open source training
> > > :-( then the current github version will be the one which allows users
> to
> > > do their own training. So, it is certainly deserving of a tag in my
> > opinion
> > > :-)
> > >
> > > —
> > > You are receiving this because you were mentioned.
> > > Reply to this email directly, view it on GitHub
> > > <https://github.com/tesseract-ocr/tesseract/issues/995#
> > issuecomment-314667002>,
> > > or mute the thread
> > > <https://github.com/notifications/unsubscribe-auth/
> > AL056e0RzuP9Hpok6mT4eU026fofCwaBks5sNGdRgaJpZM4N9Nel>
> > > .
> > >
> >
> >
> >
> > --
> > Ray.
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/995#
> issuecomment-314839820>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/AE2_o71HIG266aj--
> aGRLLsL6s9gxF_Xks5sNQIjgaJpZM4N9Nel>
> > .
>
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-314945111>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056cfTz_q0IPjUvI65YCy4HVMGAjH2ks5sNXWDgaJpZM4N9Nel>
> .
>



-- 
Ray.
 No, it is not valid to have any two matras in a row  - Devanagari 093E-094C.

However, these can be followed by Anusvar, Chandrabindu or Visarga i.e. 0901-0903

In case of Vedic Sanskrit, these can be followed by the Vedic accents eg.  0951, 0952, 1CDA etc

However, I have seen samples in legacy fonts where a number of separate matras are used to create another one eg. using unicode points as example 093E followed by 0947 to create 094b - ा  े to make ो

Similarly in legacy fonts, half letters (letter followed by virama) maybe followed by aa maatraa to create the complete letter in cases such as ga, sha etc.  i.e. 0936 + 094D + 093E to create 0936 for sha

It is possible that some converters from legacy font to unicode retain these errors.

Also, in case of Vedic Sanskrit, the valid order should be matra, combining mark (anusvar, visarga), vedic accent . Some fonts incorrectly use matra, vedic accent and combining mark which will lead to dotted circle. eg. अंशाः॑ vs अंशा॑ः

For a sample of Vedic Sanskrit and its ground truth, see
https://github.com/Shreeshrii/tess4training/blob/master/BRH-test.tif
https://github.com/Shreeshrii/tess4training/blob/master/BRH-test.txt

Will your new sanskrit traineddata be able to OCR this? On Wed, Jul 12, 2017 at 9:39 PM, Shreeshrii <notifications@github.com>
wrote:

> No, it is not valid to have any two matras in a row - Devanagari 093E-094C.
>
> However, these can be followed by Anusvar, Chandrabindu or Visarge i.e.
> 0901-0903
>
It seems that Malayalam is unique in allowing multiple 0d02 (Anusvara)?

> In case of Vedic Sanskrit, these can be followed by the Vedic accents eg.
> 0951, 0952, 1CDA etc
>
> However, I have seen samples in legacy fonts where a number of separate
> matras are used to create another one eg. 093E followed by 0947 to create
> 094b
>
These are specifically dis-allowed by unicode, but the rules seem to be
very script-specific, and not very consistently documented in the unicode
standard. I don't think the rules are addressed properly for all scripts.

> Similarly in legacy fonts, half letters (letter followed by virama) maybe
> followed by aa maatraa to create the complete letter in cases such as ga,
> sha etc. i.e. 0936 + 094D + 093E to create 0936 for sha
>
> It is possible that some converters from legacy font to unicode retain
> these errors.
>
> Also, in case of Vedic Sanskrit, the valid order should be matra,
> combining mark (anusvar, visarga), vedic accent . Some fonts incorrectly
> use matra, vedic accent and combining mark which will lead to dotted
> circle. eg. अंशाः॑ vs अंशा॑ः
>
The code aims to dis-allow text designed for such legacy fonts.
The documentation that I have found is very good for Devanagari, but
lacking for some of the other scripts.
For instance, there is a big table in the unicode standard for Myanmar, (
http://www.unicode.org/versions/Unicode9.0.0/ch16.pdf) but it doesn't cover
any of the extension Myanmar characters, and isn't explicit about whether
the table represents a specific valid order or not. The existence of a lot
of legacy Myanmar text on the web that is designed for non-compliant fonts
doesn't help make it easier to determine whether the filter is correct.

> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-314968713>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SFD_JftIXTWSw6Crvgb1j3-ZBT3ks5sNZ-XgaJpZM4N9Nel>
> .
>



-- 
Ray.
 That is still an open question.
I have limited time to spend on it (therefore resistant to delaying tactics
changing types in the dead code to POSIX).
Whether enough uses of Tesseract can be covered by the new engine is still
being debated, and the new models that I have need to be evaluated before
enough of the community is convinced.
I accept the requirement to add one or more new characters without the need
for full retraining, and will not delete the legacy code until that need is
addressed. (I think it can be done).
The legacy code is used by the OSD model and deletion of the legacy code is
also blocked by a good enough replacement.

On Thu, Jul 13, 2017 at 5:18 AM, Amit D. <notifications@github.com> wrote:

> The new traineddatas will mostly be smaller than the older ones, as they
> won't contain the legacy components, and no bigram dawgs are needed.
>
> Will you remove the *code* of the legacy engine in this round?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-315060862>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056VPOW6xmGYPbAsOF_D3yEFAAfEshks5sNgr6gaJpZM4N9Nel>
> .
>



-- 
Ray.
 >It seems that Malayalam is unique in allowing multiple 0d02 (Anusvara)?

That does not sound right. Please see 
https://en.wikipedia.org/wiki/Malayalam_script#Anusvaram

I did a search on ംം (two anusvarams in malayalam script) and most of them show up in the search result in pdfs.

FYI, pdfs created with documents having text in unicode fonts for complex scripts do not save the unicode text correctly. Devanagari text copied from these pdf is not correct, I assume similarly for malayalam and other Indian scripts, and that might be causing this double anusvar problem. 

newer pdfs created in a special manner, eg. with 'actual text' with xelatex are ok (eg. http://sanskritdocuments.org/doc_devii/annapurna.pdf), but those created from various other software are not (http://www.sanskritweb.net/sansdocs/nala-d.pdf).

@jbreiden can give you the technical reasoning for this. 

Google search does show pdfs as part of the search results, so there is some internal OCR (is it tesseract???) being done on the pdfs, books etc as part of the search process. But it may not be fully correct.

So for the corpus for training, I would suggest to avoid text taken from pdfs (in case it is being used).




 @theraysmith Regarding Malayalam, double anusvara

Please see
http://unicode.org/charts/PDF/U0D00.pdf
http://www.alanwood.net/unicode/malayalam.html
http://www.omniglot.com/language/numbers/malayalam.htm

zero in Malayalam script - pujyam looks very much like the sign for anusvar.

Also, there are different anusvars shown in unicode chart--

0D00 $ഀ MALAYALAM SIGN COMBINING ANUSVARA ABOVE
0D02 $ം MALAYALAM SIGN ANUSVARA
• used in Prakrit language texts to indicate gemination of the following consonant

0D3B $഻ MALAYALAM SIGN VERTICAL BAR VIRAMA
0D3C $഼ MALAYALAM SIGN CIRCULAR VIRAMA

I will look up more info and post under an issue in langdata Direct from the unicode standard:
Anusvara. The anusvara can be seen multiple times after vowels, whether
independent letters or dependent vowel signs, as in vxxxx <0D08, 0D02,
0D02, 0D02, 0D02>. Vowel signs can also be seen after digits, as in 355wx
<0033, 0035, 0035, 0D3E, 0D02>. More generally, rendering engines should be
prepared to handle Malayalam letters (including vowel letters), digits
(both European and Malayalam), dashes, U+00A0 no-break space and U+25CC
dotted circle as base characters for the Malayalam vowel signs, U+0D4D
malayalam sign virama, U+0D02 malayalam sign anusvara, and U+0D03 malayalam
sign visarga. They should also be prepared to handle multiple combining
marks on those bases.

Is it wrong?

On Fri, Jul 14, 2017 at 12:00 AM, Shreeshrii <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith> Regarding Malayalam, double
> anusvara
>
> Please see
> http://unicode.org/charts/PDF/U0D00.pdf
> http://www.omniglot.com/language/numbers/malayalam.htm
>
> zero in Malayalam script - pujyam looks very much like the sign for
> anusvar.
>
> Also, there are different anusvars shown in unicode chart--
>
> 0D00 $ഀ MALAYALAM SIGN COMBINING ANUSVARA ABOVE
> 0D02 $ം MALAYALAM SIGN ANUSVARA
> • used in Prakrit language texts to indicate gemination of the following
> consonant
>
> 0D3B $഻ MALAYALAM SIGN VERTICAL BAR VIRAMA
> 0D3C $഼ MALAYALAM SIGN CIRCULAR VIRAMA
>
> I will look up more info and post under an issue in langdata
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-315286649>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056dtEgSZxvdhC-1CLOZ04nX1xheUPks5sNxIPgaJpZM4N9Nel>
> .
>



-- 
Ray.
 OK, I have pushed this week's changes:
Fixes to pull from github. There were bugs introduced and required code
deleted. Also reformatted/modified according to Google code standards.
Major new normalization/text cleanup code in training/validat* The best
help with this would be expertise in the various scripts, as previously
discussed.
Deleted some code from the LSTM recognizer that was old and unused.
(Backwards compatible change).
Part 1 of the changes required to move the unicharset and recoder so they
are stored in the traineddata and therefore accessible.

I have not searched through my emails to find the relevant issues to update
them yet.
The traineddatas and training source data are not yet updated. That is
probably a while away yet, so the issue about the unicharset and recoder
are not yet fully resolved anyway.
The training process shouldn't be broken by these changes yet, I hope, *but
the documentation is no longer accurate.*
*If you run a new training or incremental/fine tuning training, the new
output files will be a traineddata directly, not an LSTM traineddata
component.*
That output traineddata should contain some version string and separate
lstm unicharset/recoder.

The next step is to change the lstmtraining program to accept a traineddata
instead of a unicharset, and add a tool to generate the traineddata, then
update the documentation to match.



On Fri, Jul 14, 2017 at 8:52 AM, Ray Smith <rays@google.com> wrote:

> Direct from the unicode standard:
> Anusvara. The anusvara can be seen multiple times after vowels, whether
> independent letters or dependent vowel signs, as in vxxxx <0D08, 0D02,
> 0D02, 0D02, 0D02>. Vowel signs can also be seen after digits, as in 355wx
> <0033, 0035, 0035, 0D3E, 0D02>. More generally, rendering engines should be
> prepared to handle Malayalam letters (including vowel letters), digits
> (both European and Malayalam), dashes, U+00A0 no-break space and U+25CC
> dotted circle as base characters for the Malayalam vowel signs, U+0D4D
> malayalam sign virama, U+0D02 malayalam sign anusvara, and U+0D03 malayalam
> sign visarga. They should also be prepared to handle multiple combining
> marks on those bases.
>
> Is it wrong?
>
> On Fri, Jul 14, 2017 at 12:00 AM, Shreeshrii <notifications@github.com>
> wrote:
>
>> @theraysmith <https://github.com/theraysmith> Regarding Malayalam,
>> double anusvara
>>
>> Please see
>> http://unicode.org/charts/PDF/U0D00.pdf
>> http://www.omniglot.com/language/numbers/malayalam.htm
>>
>> zero in Malayalam script - pujyam looks very much like the sign for
>> anusvar.
>>
>> Also, there are different anusvars shown in unicode chart--
>>
>> 0D00 $ഀ MALAYALAM SIGN COMBINING ANUSVARA ABOVE
>> 0D02 $ം MALAYALAM SIGN ANUSVARA
>> • used in Prakrit language texts to indicate gemination of the following
>> consonant
>>
>> 0D3B $഻ MALAYALAM SIGN VERTICAL BAR VIRAMA
>> 0D3C $഼ MALAYALAM SIGN CIRCULAR VIRAMA
>>
>> I will look up more info and post under an issue in langdata
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-315286649>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AL056dtEgSZxvdhC-1CLOZ04nX1xheUPks5sNxIPgaJpZM4N9Nel>
>> .
>>
>
>
>
> --
> Ray.
>



-- 
Ray.
 Actually, I take that back. I don't think the output from --stop_training
is different to what it was before. It is still an LSTM traineddata
component.

On Fri, Jul 14, 2017 at 11:35 AM, Ray Smith <rays@google.com> wrote:

> OK, I have pushed this week's changes:
> Fixes to pull from github. There were bugs introduced and required code
> deleted. Also reformatted/modified according to Google code standards.
> Major new normalization/text cleanup code in training/validat* The best
> help with this would be expertise in the various scripts, as previously
> discussed.
> Deleted some code from the LSTM recognizer that was old and unused.
> (Backwards compatible change).
> Part 1 of the changes required to move the unicharset and recoder so they
> are stored in the traineddata and therefore accessible.
>
> I have not searched through my emails to find the relevant issues to
> update them yet.
> The traineddatas and training source data are not yet updated. That is
> probably a while away yet, so the issue about the unicharset and recoder
> are not yet fully resolved anyway.
> The training process shouldn't be broken by these changes yet, I hope, *but
> the documentation is no longer accurate.*
> *If you run a new training or incremental/fine tuning training, the new
> output files will be a traineddata directly, not an LSTM traineddata
> component.*
> That output traineddata should contain some version string and separate
> lstm unicharset/recoder.
>
> The next step is to change the lstmtraining program to accept a
> traineddata instead of a unicharset, and add a tool to generate the
> traineddata, then update the documentation to match.
>
>
>
> On Fri, Jul 14, 2017 at 8:52 AM, Ray Smith <rays@google.com> wrote:
>
>> Direct from the unicode standard:
>> Anusvara. The anusvara can be seen multiple times after vowels, whether
>> independent letters or dependent vowel signs, as in vxxxx <0D08, 0D02,
>> 0D02, 0D02, 0D02>. Vowel signs can also be seen after digits, as in 355wx
>> <0033, 0035, 0035, 0D3E, 0D02>. More generally, rendering engines should be
>> prepared to handle Malayalam letters (including vowel letters), digits
>> (both European and Malayalam), dashes, U+00A0 no-break space and U+25CC
>> dotted circle as base characters for the Malayalam vowel signs, U+0D4D
>> malayalam sign virama, U+0D02 malayalam sign anusvara, and U+0D03 malayalam
>> sign visarga. They should also be prepared to handle multiple combining
>> marks on those bases.
>>
>> Is it wrong?
>>
>> On Fri, Jul 14, 2017 at 12:00 AM, Shreeshrii <notifications@github.com>
>> wrote:
>>
>>> @theraysmith <https://github.com/theraysmith> Regarding Malayalam,
>>> double anusvara
>>>
>>> Please see
>>> http://unicode.org/charts/PDF/U0D00.pdf
>>> http://www.omniglot.com/language/numbers/malayalam.htm
>>>
>>> zero in Malayalam script - pujyam looks very much like the sign for
>>> anusvar.
>>>
>>> Also, there are different anusvars shown in unicode chart--
>>>
>>> 0D00 $ഀ MALAYALAM SIGN COMBINING ANUSVARA ABOVE
>>> 0D02 $ം MALAYALAM SIGN ANUSVARA
>>> • used in Prakrit language texts to indicate gemination of the following
>>> consonant
>>>
>>> 0D3B $഻ MALAYALAM SIGN VERTICAL BAR VIRAMA
>>> 0D3C $഼ MALAYALAM SIGN CIRCULAR VIRAMA
>>>
>>> I will look up more info and post under an issue in langdata
>>>
>>> —
>>> You are receiving this because you were mentioned.
>>> Reply to this email directly, view it on GitHub
>>> <https://github.com/tesseract-ocr/tesseract/issues/995#issuecomment-315286649>,
>>> or mute the thread
>>> <https://github.com/notifications/unsubscribe-auth/AL056dtEgSZxvdhC-1CLOZ04nX1xheUPks5sNxIPgaJpZM4N9Nel>
>>> .
>>>
>>
>>
>>
>> --
>> Ray.
>>
>
>
>
> --
> Ray.
>



-- 
Ray.
 Ray,
You are right. Looks like Malayalam does have different rules, including repeated vowels. 

Please see section 8.4.3 in http://thottingal.in/documents/Fontbook.pdf by @santhoshtr.

>In samvruthokaram - ◌ു് virama is applied to a vowel sign 

>Another exception is у. This combination of a long vowel sign and anusvara is used to denote "nth" like, 16у or 16-у meaning 16th.

>Repeated vowel signs are used to denote elongation of a vowel pronunciation

Request Santhosh Thottingal @santhoshtr to  comment regarding multiple anusvars. See https://github.com/tesseract-ocr/langdata/issues/35#issuecomment-320330996

for Ray's comments about next set of changes See https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-dev/_s0TOmDlEAs/uRJ-Ozi8AAAJ for updates -msgs from Jeff Breidenbach

----------------
Aug 28, 2017

Alexander Pozdnyakov has done a really good job packing Tesseract in his
Personal Package Archive (PPA). I think it is getting to be time for wider usage,
so I'm working with him to promote these to official packages. First step is 
Debian Experimental. That's a good place to work out problems, and hopefully
something can be ready for real users within a few weeks.

----------------------
Sep 7, 2017

we will have three sets of .traineddata
files on GitHub in three separate repositories. Most users
will want LSTM Fast and that is what will be shipped as
part of Linux distributions. LSTM Best is for people willing
to trade a lot of speed for slightly better accuracy. It is also
better for certain retraining scenarios for advanced users.
The third set is for the legacy recognizer.

--------------------
Sep 15, 2017

Populated the new repositories, and removed the LSTM files from tessdata.
I'm sure documentation needs updating.



 https://github.com/tesseract-ocr/tessdata_best
https://github.com/tesseract-ocr/tessdata_fast
and
https://github.com/tesseract-ocr/tessdata Sorry, I don't follow. Which parts are identical?

```
$ du -sh best fast
1.7G	best
657M	fast
``` @amitdo  I can't seem to write a single comment without editing it three times to fix mistakes.
@roozgar Models using integer arithmetic (traineddata_fast) are smaller than ones using floating point. @stweil You had asked somewhere about tools for converting to fast/integer models... Can't find that comment to reply to. The training wiki has the answer ...

https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#lstmtraining-command-line


stop_training | bool | false | Convert the training checkpoint in --continue_from to a recognition model.
-- | -- | -- | --
convert_to_int | bool | false | With stop_training, convert to 8-bit integer for greater speed, with slightly less accuracy.

  Sorry, I do not familiar with opencl setup. Feel free to contribute required changes.  please use the latest source from master branch in github, and leptonica
1.74.2

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Jun 15, 2017 at 6:39 PM, ibr123 <notifications@github.com> wrote:

> Hi,
>
> while i was training the LSTM through this command
> lstmtraining -U ~/tesstutorial/jpntrain/jpn.unicharset \ --script_dir
> /home/ibr/langdata --debug_interval 0 \ --continue_from
> tesstutorial/jpn.lstm \ --append_index 5 --net_spec '[Lfx256 O1c105]' \
> --model_output /home/ibr/tesstutorial/new_jpn_lstm \ --train_listfile
> ~/tesstutorial/jpneval/jpn.training_files.txt \ --eval_listfile
> ~/tesstutorial/jpntrain/jpn.training_files.txt \ --max_iterations 5000
> &>~/tesstutorial/basetrain.log
>
> it went fine and started the iterations until the iteration 900 it gave
> the error:
>
> [image: lstm dumping]
> <https://user-images.githubusercontent.com/26926171/27182531-27761344-51e4-11e7-889e-ae91bad36f3a.png>
>
> although the error came, it created checkpoint file, I'm using Tesseract
> 4.00alpha, leptonica 1.74.1 and OS is Ubuntu 14.04
>
> i got the command from this website
> <https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00---Replace-Top-Layer>
>
> any ideas?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/993>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1fAdCNaNN8B9u4uc_ynY9MOoHNsks5sES0JgaJpZM4N7Me6>
> .
>
 As stated in the error, you are missing

https://github.com/tesseract-ocr/langdata/blob/master/radical-stroke.txt Please copy the error text from terminal rather than adding a screenshot. You can mark it as ```code```. 1. You have not mentioned the environment hardware/software on which you are running this?

2. Please rerun tesstrain.sh to recreate the lstmf files using the latest version of tesseract and then try training. @ibr123  Is your problem solved? Please mention the solution and close the issue.  Check that the lstmf file exists in the location referred to.

What environment, o/s, hardware are you running this on?
  @theraysmith Is there any config which will change this?  https://github.com/tesseract-ocr/tesseract/issues/988 regarding Japanese is also about similar results.

Could it be related in any way to handling of non-space delimited languages/CJK? Possibly because of 

>>We also want to force a separate word for every non
      // space-delimited character when not in a dictionary context.

https://github.com/tesseract-ocr/tesseract/blob/a1c22fb0d0f6bde165ec7b7c3125420b0ba1d541/lstm/recodebeam.cpp https://github.com/tesseract-ocr/tesseract/issues/1009

Similar problem with Korean. Using the solution suggested in https://github.com/tesseract-ocr/tesseract/issues/1009
The extra spaces are removed and only interword-spaces are preserved.

See attached output files:

`--oem 1 --psm 6 -l chi_sim`
[chineses-1.txt](https://github.com/tesseract-ocr/tesseract/files/1108719/chineses-1.txt)


`--oem 1 --psm 6 -l chi_sim -c preserve_interword_spaces=1`

[chineses-spaces.txt](https://github.com/tesseract-ocr/tesseract/files/1108731/chineses-spaces.txt)


Please note that this behavior is different from the behaviour of `preserve_interword_spaces` in 3.05 branch, where it preserves multiple spaces between words and does not compress them to single space.

@theraysmith  Is this intentional for 4.0? Since there are reports of `preserve_interword_spaces` not working with 4.0 - see https://github.com/tesseract-ocr/tesseract/issues/781

 Sorry, @hoangtocdo90 . I do not know about `ResultIterator`   As suggested in https://github.com/tesseract-ocr/tesseract/issues/1009

use `-c preserve_interword_spaces=1` to remove extra spaces

Also see https://github.com/tesseract-ocr/tesseract/issues/991

  AFAIR somebody wanted to have option compile only individual parts of tesseract. 
IMO it does not make sense and I think it should go away - there is other way how do deal with it (e.g. we implement disable graphics, disable cube)  See https://github.com/tesseract-ocr/tesseract/issues/961

What is the  hardware you are using?
Is this being run under WSL?

I saw improvement by disabling openmp, you can gI've it a try.
 Try using the latest code from github instead of the initial alpha, there have been many changes and bug fixes.  use leptonica 1.74.2

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Jun 6, 2017 at 6:58 PM, ibr123 <notifications@github.com> wrote:

> Hi,
>
> I wanted to install and compile leptonica and then tesseract on a server
> following this website
> <https://medium.com/@lucas63/installing-tesseract-3-04-in-ubuntu-14-04-1dae8b748a32>
> i installed and downloaded leptonica 1.74.1 and tesseract 4.00.00alpha on
> VM and VB before following the same steps exactly, yet today i found this
> error coming out on the terminal
> [image: env_error]
> <https://cloud.githubusercontent.com/assets/26926171/26831244/5b97d520-4ad4-11e7-9a87-bf86bc754e79.png>
>
> i tried to comment the error section but that caused another problems, i
> even tried to get the leptonica.gz again and running the steps once more,
> yet the same problem.
>
> keep in mind that the OS on the server is Ubuntu 14.04, which is the same
> OS that i used on VM and OS on VB and worked on both cases, is this issue
> cause by a dependency or what exactly?
> Please any ideas? as i need to get the server up and running as soon as
> possible
> Thanks
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/981>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o8TMqtSnbNnqZqtpRT5UpylV_vXBks5sBVQTgaJpZM4NxV7H>
> .
>
 https://github.com/DanBloomberg/leptonica/releases/tag/1.74.2

I think it is a full release.

Please see https://github.com/tesseract-ocr/tesseract/pull/979
for fixes made recently that require this change


ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Jun 6, 2017 at 7:17 PM, ibr123 <notifications@github.com> wrote:

> thanks, but what is the command to get the newer version, its not:
> wget http://www.leptonica.com/source/leptonica-1.74.2.tar.gz
>
> and is it an alpha version too or a full release?
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/981#issuecomment-306491228>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o784IlARn2PAu-5TMVhpXcgWd_QGks5sBVh7gaJpZM4NxV7H>
> .
>
 You are not able to build leptonica => you need to solve it  with leptonica project team (e.g. not here).  Someone broke my leptonica cmake scripts? :) > Should we refuse builds with Leptonica < 1.74.2?

We can do anything, because it's dev branch.
+1 from me  I am still getting the error as this is not merged yet.

```
Breakpoint 7, tesseract::LSTMTrainer::ReadSizedTrainingDump (this=this@entry=0x7f2a7cb9f680, data=0x0, size=0) at lstmtrainer.cpp:924
924     bool LSTMTrainer::ReadSizedTrainingDump(const char* data, int size) {
(gdb) backtrace
#0  tesseract::LSTMTrainer::ReadSizedTrainingDump (this=this@entry=0x7f2a7cb9f680, data=0x0, size=0) at lstmtrainer.cpp:924
#1  0x0000000000428cc1 in tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f2a7cb9f680, data=..., trainer=trainer@entry=0x7f2a7cb9f680)
    at lstmtrainer.cpp:921
#2  0x000000000040af96 in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffe865b310, iteration=1, training_errors=<optimized out>, model_data=...,
    training_stage=1) at lstmtester.cpp:86
#3  0x000000000040b476 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffe865b310) at lstmtester.cpp:123
#4  0x00007f2a82be8184 in start_thread (arg=0x7f2a7cba0700) at pthread_create.c:312
#5  0x00007f2a820ca37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) info frame
Stack level 0, frame at 0x7f2a7cb9f540:
 rip = 0x428c40 in tesseract::LSTMTrainer::ReadSizedTrainingDump (lstmtrainer.cpp:924); saved rip = 0x428cc1
 called by frame at 0x7f2a7cb9f540
 source language c++.
 Arglist at 0x7f2a7cb9f530, args: this=this@entry=0x7f2a7cb9f680, data=0x0, size=0
 Locals at 0x7f2a7cb9f530, Previous frame's sp is 0x7f2a7cb9f538
(gdb) up
#1  0x0000000000428cc1 in tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f2a7cb9f680, data=..., trainer=trainer@entry=0x7f2a7cb9f680)
    at lstmtrainer.cpp:921
921       return trainer->ReadSizedTrainingDump(&data[0], data.size());
(gdb) info frame
Stack level 1, frame at 0x7f2a7cb9f540:
 rip = 0x428cc1 in tesseract::LSTMTrainer::ReadTrainingDump (lstmtrainer.cpp:921); saved rip = 0x40b476
 tail call frame, caller of frame at 0x7f2a7cb9f540
 source language c++.
 Arglist at unknown address.
 Locals at unknown address, Previous frame's sp is 0x7f2a7cb9f540
(gdb) up
#2  0x000000000040af96 in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffe865b310, iteration=1, training_errors=<optimized out>, model_data=...,
    training_stage=1) at lstmtester.cpp:86
86        if (!trainer.ReadTrainingDump(model_data, &trainer)) {
(gdb) info frame
Stack level 2, frame at 0x7f2a7cb9feb0:
 rip = 0x40af96 in tesseract::LSTMTester::RunEvalSync (lstmtester.cpp:86); saved rip = 0x40b476
 called by frame at 0x7f2a7cb9fee0, caller of frame at 0x7f2a7cb9f540
 source language c++.
 Arglist at 0x7f2a7cb9f538, args: this=this@entry=0x7fffe865b310, iteration=1, training_errors=<optimized out>, model_data=..., training_stage=1
 Locals at 0x7f2a7cb9f538, Previous frame's sp is 0x7f2a7cb9feb0
 Saved registers:
  rbx at 0x7f2a7cb9fe78, rbp at 0x7f2a7cb9fe80, r12 at 0x7f2a7cb9fe88, r13 at 0x7f2a7cb9fe90, r14 at 0x7f2a7cb9fe98, r15 at 0x7f2a7cb9fea0, rip at 0x7f2a7cb9fea8
(gdb)
```  Tesseract is OCR engine and it requires image with text input (e.g. it is not text detection tool). So it not is not a bug, but tesseract is not not tool you expected.   Sorry for not having noticed the banner. 

I will add that line and link.  Add an Issue Template
Reorganize Readme.MD Thanks for your feedback.

I was looking at the Readme from the point of view of someone who is
looking at it on github.com and thought that the latest Readme.md would be
visible to them there, hence moved it to the bottom.

This text as well as the badges seemed to be taking up lots of space when
looking at the page, right on top. Maybe there is a different way to
distinguish between the two sets of badges, without the huge headings.

Please feel free to change the PR and squash it too - I did't realize that
multiple changes I made in the fork will all become part of the PR.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Jun 1, 2017 at 2:20 AM, Egor Pugin <notifications@github.com> wrote:

> *@egorpugin* commented on this pull request.
> ------------------------------
>
> In README.md
> <https://github.com/tesseract-ocr/tesseract/pull/964#discussion_r119470371>
> :
>
> > @@ -1,16 +1,9 @@
>  # Tesseract OCR
>
> -For the latest online version of the README.md see:
> -
> -https://github.com/tesseract-ocr/tesseract/blob/master/README.md
> -
> -### Build
> -[![Build Status](https://travis-ci.org/tesseract-ocr/tesseract.svg?branch=master)](https://travis-ci.org/tesseract-ocr/tesseract)
> -[![Build status](https://ci.appveyor.com/api/projects/status/miah0ikfsf0j3819/branch/master?svg=true)](https://ci.appveyor.com/project/zdenop/tesseract/)
> -
> -### Other
>
> Where? The idea behind this caption was to split badges.
> In this PR they are again near each other.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/964#discussion_r119470371>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o0cXztdqlgCKlXwmpmHI-VcH1EtZks5r_dKugaJpZM4NrqXr>
> .
>
 Badges look ugly now.
https://github.com/Shreeshrii/tesseract/blob/master/README.md

Current form is better. @egorpugin I have reverted the change for badges and updated link for 3.05.01 release.

Changed page will look like https://github.com/Shreeshrii/tesseract/blob/master/README.md

Please make any other changes that you see fit.

Also, please add your name as well as those of other maintainers/members with write access to the repo.  works with latest code from master branch, using the LSTM engine ie. --oem 1

$ tesseract test.png stdout --oem 1 --psm 6 -l eng
Warning. Invalid resolution 0 dpi. Using 70 instead.
_SA1C17010068

$ tesseract test.png stdout --oem 0 --psm 6 -l eng
Warning. Invalid resolution 0 dpi. Using 70 instead.
mgAﬁ'Ei'ib'fboss
 please use tesseract forum for asking questions.  TesseractConsole.exe is not part of tesseract project. We do not support 3rd party programs. Please contact its author.  Using the latest code from master branch seems to be slower than from commits a few days back. Training seems to hang at times. I am trying to get an objective measure by running sample training against different builds and will post results here. ```
training/tesstrain.sh \
  --fonts_dir  /mnt/c/Windows/Fonts \
  --tessdata_dir ./tessdata \
  --training_text ../langdata/eng/eng.training_text \
  --langdata_dir ../langdata \
  --lang eng  \
  --linedata_only \
  --noextract_font_properties \
  --exposures "0"    \
  --fontlist "Arial" \
  --output_dir ~/tesstutorial/engtest
  
training/tesstrain.sh \
  --fonts_dir  /mnt/c/Windows/Fonts \
  --tessdata_dir ./tessdata \
  --training_text ../langdata/eng/eng.training_text \
  --langdata_dir ../langdata \
  --lang eng  \
  --linedata_only \
  --noextract_font_properties \
  --exposures "0"    \
  --fontlist "Arial" \
  "Courier New" \
  --output_dir ~/tesstutorial/engeval

rm -rf  ~/tesstutorial/engtuned_from_engtest 

mkdir -p ~/tesstutorial/engtuned_from_engtest 

combine_tessdata -e ../tessdata/eng.traineddata \
  ~/tesstutorial/engtuned_from_engtest/eng.lstm
 ```
engtest was built with   --fontlist "Arial" .
engeval was built with   --fontlist "Arial"   "Courier New" .

command on non-debug build, under WSL on Windows 10
using unchanged training_text and other files from langdata repo.

```
time lstmtraining \
  --continue_from ~/tesstutorial/engtuned_from_engtest/eng.lstm \
  --train_listfile ~/tesstutorial/engtest/eng.training_files.txt \
  --eval_listfile ~/tesstutorial/engeval/eng.training_files.txt \
  --model_output ~/tesstutorial/engtuned_from_engtest/engtuned \
  --debug_interval 0 \
 --max_image_MB 1000 \
 --perfect_sample_delay 19 \
 --max_iterations 1000
```
 on
x86_64 x86_64 x86_64 GNU/Linux (WSL)
on Windows 10 Home
AMD A4-5000 APU @1.50 GHz
4 GB RAM

as of https://github.com/tesseract-ocr/tesseract/commit/42066ce69057dc81ae6cbc176300bb20223c7d0b

```

2 Percent improvement time=1, best error was 100 @ 0
At iteration 1/900/900, Mean rms=0.153%, delta=0%, char train=0.01%, word train=0.036%, skip ratio=0%,  New best char error = 0.01Deserialize failed w
rote best model:/home/shree/tesstutorial/engtuned_from_engtest/engtuned0.01_1.lstm wrote checkpoint.

Finished! Error rate = 0.01

real    64m31.894s
user    185m23.578s
sys     0m41.766s

``` on
x86_64 x86_64 x86_64 GNU/Linux (WSL)
on Windows 10 Home
AMD A4-5000 APU @1.50 GHz
4 GB RAM

as of https://github.com/tesseract-ocr/tesseract/commit/5a06417eb273a3cc3e5f720ca8fa4bbaa7940ae6

```
2 Percent improvement time=1, best error was 100 @ 0
At iteration 1/900/900, Mean rms=0.153%, delta=0%, char train=0.01%, word train=0.036%, skip ratio=0%,  New best char error = 0.01Deserialize failed wrote best model:/home/shree/tesstutorial/engtuned_from_engtest/engtuned0.01_1.lstm wrote checkpoint.

Finished! Error rate = 0.01

real    51m58.330s
user    148m10.469s
sys     0m43.609s
``` on
x86_64 x86_64 x86_64 GNU/Linux (WSL)
on Windows 10 Home
AMD A4-5000 APU @1.50 GHz
4 GB RAM

as of https://github.com/tesseract-ocr/tesseract/commit/b86b4fa06ba4d2afa00c53470a19f6630e638f66

```

2 Percent improvement time=1, best error was 100 @ 0
At iteration 1/900/900, Mean rms=0.153%, delta=0%, char train=0.01%, word train=0.036%, skip ratio=0%,  New best char error = 0.01Deserialize failed wrote best model:/ho
me/shree/tesstutorial/engtuned_from_engtest/engtuned0.01_1.lstm wrote checkpoint.

Finished! Error rate = 0.01

real    58m48.689s
user    157m45.719s
sys     0m51.891s
``` If  it does not add too much overhead, I would suggest adding some kind of regression testing as part of continuous integration.

Also, will be helpful for others to test to confirm, since I ran this under WSL on windows 10. >Shree, did you run those tests with WSL?

Yes. I did not set anything special for OPENMP though my PC probably supports it.  on 
Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-75-generic x86_64)
Intel(R) Pentium(R) Dual CPU E2220 @ 2.40GHz
4 GB RAM, 

with git master
tesseract 4.00.00alpha-538-g42066ce-1995

```
2 Percent improvement time=1, best error was 100 @ 0
At iteration 1/800/800, Mean rms=0.157%, delta=0%, char train=0.009%, word train=0.033%, skip ratio=0%,  New best char error = 0.009Deserialize failed wrote best model:/home/shree/tesstutorial/engtuned_from_engtest/engtuned0.009_1.lstm wrote checkpoint.

Finished! Error rate = 0.009

real    35m51.291s
user    58m50.184s
sys     4m23.416s
``` on
Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-75-generic x86_64)
Intel(R) Pentium(R) Dual CPU E2220 @ 2.40GHz
4 GB RAM,

reset hard to 5a06417eb273a3cc3e5f720ca8fa4bbaa7940ae6

shree@sanskrit:~/tesseract$ tesseract -v
tesseract 4.00.00alpha-533-g5a06417-1990
 leptonica-1.74.2
 
```
2 Percent improvement time=1, best error was 100 @ 0
At iteration 1/800/800, Mean rms=0.157%, delta=0%, char train=0.009%, word train=0.033%, skip ratio=0%,  New best char error = 0.009Deserialize failed wrote best model:/home/shree/tesstutorial/engtuned_from_engtest/engtuned0.009_1.lstm wrote checkpoint.

Finished! Error rate = 0.009

real    35m41.198s
user    58m38.460s
sys     4m9.208s
``` Closing as problem not reproducable on other systems. FYI 

I built with --disable-openmp under WSL. Now it is taking less time.

```

2 Percent improvement time=1, best error was 100 @ 0
At iteration 1/900/900, Mean rms=0.153%, delta=0%, char train=0.01%, word train=0.036%, skip ratio=0%,  New best char error = 0.01Deserialize failed wrote best model:/ho
me/shree/tesstutorial/engtuned_from_engtest/engtuned0.01_1.lstm wrote checkpoint.

Finished! Error rate = 0.01

real    65m34.655s
user    63m39.250s
sys     0m7.453s
```
 I am reopening the issue, because there is some impact on `lower level cpus` which support `openmp`.

By disabling openmp, the `user` time has become almost one third on WSL on my AMD Windows 10 PC.

from
```
real    64m31.894s
user    185m23.578s
sys     0m41.766s
```

to

```
real    65m34.655s
user    63m39.250s
sys     0m7.453s
```

>User+Sys will tell you how much actual CPU time your process used.

Maybe, need to add some recommendation in wiki to `--disable-openmp` I think it would he helpful to mention on the 'Compiling' pages to build with `--disable-openmp`.

@stweil IS it ok to do so? Are there any particular conditions under which it should be recommended?

 on
Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-75-generic x86_64)
Intel(R) Pentium(R) Dual CPU E2220 @ 2.40GHz
4 GB RAM,
```
lstmtraining for tesseract training
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
15229 shree     20   0 1152728 468280   6720 R  98.0 11.6 788:35.04 lstmtraining
```
It has been taking 92-98% CPU.

@stweil Any recommendation on how to reduce cpu usage? 
I haven't tried with disable-openmp on that machine. --disable-openmp improved performance of tesseract in recognizing images. But lstmtraining still taking 99% cpu. I use that machine remotely. 

I am now running the command as `nice lstmtraining` - so it should be using the default nice value of 10. It still shows cpu usage upto 99-100%, but if another process eg. firefox is used locally on that machine, I am hoping that it will get priority and not degrade the browsing performance.

Is there a valgrind or other debugging command which will help isolate what is causing this high cpu usage?
  Tested user-words option with 3.05.01 on windows (using binaries by @stweil)

Works ok. See attached test image.

bazaar config file as used (uses system dictionary + user words)
```
load_system_dawg     T
load_freq_dawg       T
user_words_suffix    user-words
user_patterns_suffix user-patterns
```

eng.user-words as used
```
Online
the
quick
brown
fox
jumped
```

image used for recognition

![test](https://cloud.githubusercontent.com/assets/5095331/26752834/7c1a3366-4876-11e7-90ae-66fb6d7a5cdb.png)

Output without user-words- Notice `Dnline` instead of `Online`
```
tesseract test.png stdout
shared Guruvayoor Dnline Friends's post
```

Output with user-words - `Online` recognized correctly 
```
tesseract test.png stdout  bazaar
shared Guruvayoor Online Friends's post
```

So `Online` from eng.user-words was used, when using the `bazaar` config file, and led to improved accuracy. I tested for user-patterns just now with versions 3.02 and 3.05.01, both for windows so that I didn't have to worry about correct versions of leptonica. The test image is attached.

There is no change in output with the user-patterns option in both. So, if this feature worked, it would be before 3.02. 

However, just by resizing the image to 200%, the dates  are correctly recognized. 

![date](https://cloud.githubusercontent.com/assets/5095331/26753470/976e8802-4884-11e7-82aa-5cb4bc1b75e5.png)
![date-small](https://cloud.githubusercontent.com/assets/5095331/26753469/976c1a5e-4884-11e7-8db0-6eefa4624a54.png)
 @zdenop @amitdo @stweil Have you used user-patterns option? If so, with which version? > I also have a scenario where working user patterns would help.

@stweil Interesting project :-)

https://groups.google.com/forum/#!searchin/tesseract-ocr/user$20patterns$20%7Csort:date/tesseract-ocr/S9CIK3jOMWw/u7dnVDASFLgJ

>The ability to use user patterns was added by Tesseract 3.01, and now has a little documentation.  See the comment in dict/trie.h:

http://code.google.com/p/tesseract-ocr/source/browse/tags/release-3.01/dict/trie.h

So it broke somewhere between 3.01 and 3.02... I did not use it either.
But as far as I understand: "user patterns" just help to extend tesseract dictionary. 
And as it is known putting word to dictionary does not mean tesseract will recognize it (or other way around disabling dictionaries will not cause 0% recognition). => I do not know if the feature is working at all, but I would not expect significant effect on result from it.  Tesseract 3 has different versions and the accuracy and performance differs
in those too.

See https://groups.google.com/forum/#!topic/tesseract-dev/LErriuT-sck

for posts regarding Tesseract performance and accuracy across versions 2.04
- 3.04.01

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, May 29, 2017 at 10:14 PM, vaasily <notifications@github.com> wrote:

> Hi Amit,
>
> Thanks for the answer.
>
> I'm going to compare the following OCR engines:
>
>    1. Tesseract 4
>    2. Tesseract 3
>    3. ABBYY Cloud OCR SDK
>    4. Microsoft Computer Vision API
>    5. Google Cloud Vision API
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/959#issuecomment-304698989>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o0X2uz8jHoQHeWaL-ZexujSg03q3ks5r-vXZgaJpZM4NpdrT>
> .
>
  which traineddata file did u use? Did you use `--oem 1` to use the LSTM engine and LSTM laanguage model?

I get the following output 

```
हसेरै बिदा गर्छु भन्थे तिमीलाई

तर मुस्कान कतै डेरा सर्यो ||

रोक्छु भन्थे यी मनभित्र मारिएका बादललाई
तर आशु बनी बर्सिदियो ||
```
command used was:

tesseract nep.png nep -l nep --oem 1 --psm 6 --tessdata-dir ../tessdata
 Please close the issue.  What version of leptonica you use?
Did you run `./autogen.sh` (in tesseract) before running `./configure` ? > checking for l_generateCIDataForPdf in -llept... no

Do you have only one installation of leptonica in your system? Because current [leptonica has this function](https://github.com/DanBloomberg/leptonica/blob/master/src/allheaders.h#L1410).

> leptonica builded from source does not create liblept.so, instead it creates libleptonica.so

I can not check it at the moment, but [source code show liblept is output of compilation](https://github.com/DanBloomberg/leptonica/blob/master/src/Makefile.am#L4). How did you build leptonica? @zdenop Tagging 3.05.01 release will help. Build leptonica with autotools, not cmake if  you're using tess autotools. Otherwise use cmake+cmake build.  you can get the latest appveyor build artifacts from

win32

https://ci.appveyor.com/project/zdenop/tesseract/build/master.993/job/c8rtk21uqliscckn/artifacts

win64

https://ci.appveyor.com/project/zdenop/tesseract/build/master.993/job/c6uplulmcrk7bew8/artifacts @weituotian: we provide source code only - e.g. we do not produce and distribute windows libraries (with header files), that you can use in your project. @weituotian. you can try to ask on tesseract user forum.  > AppVeyor build failed I gave it a try with eurotext.tif with -l eng and san001.tif with -l hin. I did not find much difference between the `optimized code' and code from the earlier commit, though there are variations in different runsof each  from command line.

See attached log files with numbers.
[test-san001.txt](https://github.com/tesseract-ocr/tesseract/files/1033492/test-san001.txt)
[test-eurotext.txt](https://github.com/tesseract-ocr/tesseract/files/1033491/test-eurotext.txt)

As has been previously noted, legacy engine is faster for -l eng and LSTM engine is faster for -l hin. What is status of this PR? At least it needs to resolve conflicts....  There are multiple pages with installation instructions and they are sometimes in conflict.

https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL
https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT.md

https://github.com/tesseract-ocr/tesseract/wiki/Compiling-%E2%80%93-GitInstallation
https://github.com/tesseract-ocr/tesseract/wiki/Compiling

@stweil It will be great if we can merge all current and correct info in one page and then redirect the other pages to it. That way only one page needs maintaining on regular basis.

Also readme.md can be linked to that page.

Thaks!  Have you looked at tsv and hocr ouputs? --help option does not document pdf neither. So there is no difference between pdf, tsv and hocr ;-)  Please use tesseract user forum for asking support  1. you can install git for windows with bash option and then you will be
able to run tesstrain.sh script under windows.

2. your lstmtraining command is incorrect.

*--train_listfile ara.training_text.txt*

*the listfile has a list of lstmf files created using box/tiff files by
tesseract with lstm.train config gile.*

*See **https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00---Finetune
<https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00---Finetune>*

*and follow the tutorial *

2. your lstmtraining command is incorrect.
 https://git-scm.com/downloads

download git for windows
install including the option for bash



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, May 23, 2017 at 10:59 PM, ShreeDevi Kumar <shreeshrii@gmail.com>
wrote:

> 1. you can install git for windows with bash option and then you will be
> able to run tesstrain.sh script under windows.
>
> 2. your lstmtraining command is incorrect.
>
> *--train_listfile ara.training_text.txt*
>
> *the listfile has a list of lstmf files created using box/tiff files by
> tesseract with lstm.train config gile.*
>
> *See **https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00---Finetune
> <https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00---Finetune>*
>
> *and follow the tutorial *
>
> 2. your lstmtraining command is incorrect.
>
>
>
 --train_listfile ara.nicidprint.exp0.lstmf 

is incorrect. you need a text file which has the filename in it.

See attached zip file for a sample.
[ara.zip](https://github.com/tesseract-ocr/tesseract/files/1024546/ara.zip)


EDIT: ara.zip file is from May 2017. Ray has changed the training method in July/Aug 2017 or so. So, files from that will not work. Please see the wiki page on training for latest info. > ..\combine_tessdata -o ..\tessdata\ara.traineddata ara.lstm
ara.unicharset ara.number-dawg ara.punc-dawg ara.word-dawg

-o option is for overwrite

so it looks for an ara.traineddata in specified directory, renames it to
tmp version and then creates the new one.

check that  u have an old ara.traineddata
check u have write access to directory

u may have to remove old tmp files, if any.

check the options of combine_tessdata

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, Jun 4, 2017 at 5:20 PM, chris <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/947#issuecomment-306035336>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o9cN923d1y6gHq1VcjJ4uXB3QLDQks5sApoDgaJpZM4Njq05>
> .
>
 Both Fine Tune and replacing the top layer training start with the .lstm
file from the existing traineddata. Training builds on top of that. So the
files will be about the same size.

If you only want to make traineddata from your trained files, do not use

```
Command I used:
..\combine_tessdata -o ..\tessdata\ara.traineddata ara.lstm ara.unicharset
ara.number-dawg ara.punc-dawg ara.word-dawg
```

Instead, just combine the files you need

```
..\combine_tessdata ara.
```
where you have all the required files ie. ara.lstm ara.unicharset
ara.number-dawg ara.punc-dawg ara.word-dawg

combine-tessdata finds ara.* files (the ones required for traineddata and
combines them).

Your file size should be smaller.

You can compare what's included in the ara.traineddata from tessdata repo
by the following command:
-u option unpacks the data
```
combine_tessdata -u ara.traineddata ara.
Extracting tessdata components from ara.traineddata
Wrote ara.config
Wrote ara.unicharset
Wrote ara.punc-dawg
Wrote ara.word-dawg
Wrote ara.number-dawg
Wrote ara.freq-dawg
Wrote ara.lstm
Wrote ara.lstm-punc-dawg
Wrote ara.lstm-word-dawg
Wrote ara.lstm-number-dawg
0:config:size=545, offset=168
1:unicharset:size=7831, offset=713
6:punc-dawg:size=1066, offset=8544
7:word-dawg:size=6303746, offset=9610
8:number-dawg:size=426, offset=6313356
9:freq-dawg:size=1346, offset=6313782
17:lstm:size=5324626, offset=6315128
18:lstm-punc-dawg:size=1466, offset=11639754
19:lstm-word-dawg:size=895354, offset=11641220
20:lstm-number-dawg:size=658, offset=12536574
```
 Please see
https://github.com/tesseract-ocr/tesseract/blob/master/training/tesstrain_utils.sh

It is a copy of the word-dawg based on the wordlist, but using the
unicharset which was used for lstm training (this unicharset could be
different from the one used for tesseract or cube training).

Also, please note that currently the lstm training process still has some
'bugs' and you may not get better results than the 4.0 traineddata provided
by Google in the repo.



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Jun 5, 2017 at 4:36 PM, LatifWirelessMarketer <
notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> Thank You soo much brother.
>
> Btw do you know what is the difference between ara.word-dawg &
> ara.lstm-word-dawg?
>
> I mean I know how to generate word dawg as it is generated based on word
> list but what about lstm-word list?
>
> Thank You
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/947#issuecomment-306162441>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o0p1c04ZQsgzTFr4qnfgAUYGfbVZks5sA-FCgaJpZM4Njq05>
> .
>
  Please use tesseract user forum for asking support.  We do not support 3rd party project. Please use tesseract user forum.  Yes I've done quite a lot of performance vs accuracy testing.

Memory new/delete:
The network system uses a rather complex scratchpad mechanism to minimize
the number of allocations/deallocations. It helped a lot with both speed
and peak memory.
I'd be curious to know where the current bottleneck is is you have
specifics of the stack where the allocations take place.

UnicharIdArrayUtils:
Hmm. Looks like it may be doing too many id_to_unichar (). Again
callers/stack traces would be useful.

snprintf:
Really? where from? Are you running with some debug flags on?

DotProduct:
Float vs double:

   - Running a large dot product strictly in float (ie float += (float =
   float * float)) is an unwise thing to do. (Ahem, no comment on any other nn
   libraries that do that.)
   - I found significant accuracy impacts using float in the multiply-add
   accumulation, and neither SSE nor AVX provide a double = float x float
   operation, (analogous to the 32 = 16x16 integer instruction in SSE) which
   is what is really needed.
   - The SSE/AVX float->double cast is extremely slow - slower than reading
   a double from memory.

The code is therefore optimized to minimize the memory bandwidth, and the
number of float->double conversions, while using double += double*double in
the AVX code.
(Since writing the AVX dot product, I thought of a better way of doing it,
but haven't had time to implement that yet.)
In any case, the time savings are small, a factor of <2.

A good integer implementation may squeeze better out of it, but I haven't
seen it yet.OTOH, I haven't tried it lately.
AVX2 and AVX512 extend the integer operations beyond the ones available on
SSE (not on base AVX) and may make it worth it, when I get  machine with
AVX512.
The additional benefit of (8 bit) integer is that it reduces the size of
everything, making it more likely that data will stay in a higher-level
cache.

*Far greater performance improvements can be made by making the network
smaller.*
As I already indicated, I have had some very good results in this area,
with a network 3x faster than the legacy code (for English) and *much*
faster than the legacy code for complex scripts.

I since messed up the synthetic data pipeline and training trying to
improve Indic, but I have major improvements in some other languages, so
when I get back to the best results for everything, I think you'll like it.

BTW I get a significant speed-up in walltime with the open MP code running.
(Better than 2x) Did you compile and link it correctly with OpenMP?
I have noticed that the CPU profile with OpenMp running is of little
practical use.

On Mon, May 22, 2017 at 1:38 AM, Stefan Weil <notifications@github.com>
wrote:

> Performance is important for real time OCR, mass production OCR and
> training.
>
> In this RFC I'd like to discuss performance bottlenecks and potential
> improvements.
>
> See also the Tesseract wiki
> <https://github.com/tesseract-ocr/tesseract/wiki/4.0-Accuracy-and-Performance>
> .
>
> According to my tests with Valgrind's tool callgrind, these functions
> have the largest computational costs (ordered by decreasing time):
>
>    - memory allocations / deallocations
>    - tesseract::UnicharIdArrayUtils::compare
>    - tesseract::DotProductAVX (or the other implementations of the dot
>    product)
>    - vfprintf (called from snprintf)
>
> tesseract::UnicharIdArrayUtils::compare and memory allocations /
> deallocations are also the functions which are called most often.
>
> I recently had a closer look at the dot product calculations and noticed
> that at least some input vectors are converted from float to double
> (which takes time). The dot product is always done with double values
> (more expensive than float). If memory bandwidth is the limiting factor,
> using double means doubled time compared with float. The current code
> uses 4 parallel threads. I have run some timing tests without that
> parallelization and got nearly the same execution time. @theraysmith
> <https://github.com/theraysmith>, did you try using float for the dot
> product, and do you get better performance from parallelization in that
> part of the OCR process?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/943>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AL056TVhtIV-0-2TldC4EuvYMqx71Tb7ks5r8Ul6gaJpZM4NiCwj>
> .
>



-- 
Ray.
 Is this for --oem 0 or --oem 1 ?

I thought that Unicharambigs is not used  by LSTM engine... @stweil 

Is that with `debug`? What about without it?  Please see https://github.com/tesseract-ocr/tesseract/issues/961#issuecomment-305420753

for another example of decrease in user time with --disable-openmp.

> By disabling openmp, the user time has become almost one third on WSL on my AMD Windows 10 PC.  https://github.com/tesseract-ocr/tesseract#supported-compilers  Please use tesseract user forum for asking support Tesseract langdata has two languages for kurdish

https://github.com/tesseract-ocr/langdata/tree/master/kur
https://github.com/tesseract-ocr/langdata/tree/master/kur_ara

Please review the training text, wordlist etc there and if there is a mix in those files, please file a issue under langdata. Thanks.  This is not tesseract problem. We will not release tesseract outside of github. Yes you can do it. 
This is the way how binary version of tesseract is distributed: volunteers (responsible for fixing distribution/packaging) create packages and put them to distribution channel of their choice.  Please use tesseract user forum for asking support  If you copy the url and paste, it works. It is a page in chinese (i think).   I agree that this would be a useful feature. Specially since tesseract already detects when the image has a low dpi.

Example Image where this would help - https://github.com/tesseract-ocr/tesseract/issues/997 Another related addition could be allowing pdf input. leptonica can be used for converting pdf to multipage tiff - it could be controlled via a config if needed.

https://github.com/DanBloomberg/leptonica/blob/master/prog/pdf2mtiff

Also,  leptonica functions could be used for converting images to 300 dpi / larger size within tesseract itself, which will improve results for a lot of users. Again, it could be implemented via a config, so that those  who prefer to pre-process by themselves for better control can do so.  Copied from: https://github.com/tesseract-ocr/tesseract/pull/911#issuecomment-302068376

> @stweil - How would I start Tesseract to process page1.png and page2.png in a single run?
> 
> @amitdo - Prepare a text file that has the path to each image:
> 
> path/to/1.png
> path/to/2.png
> path/to/3.tiff
> 
> Save it, and then give its name as input file to Tesseract.
> 
> tesseract savedlist output
> 
> @stweil - Thank you, good to know that. It looks like the ChangeLog, other documentation and the program help text need an update.
> 
> Currently all pages are written to one output file (per format). Some formats include page information (hOCR, PDF). Others like TXT don't, but could use a page separator character (ASCII 0x0C = FF). Would it help to support an output base parameter with placeholders like page number or image base name to generate one output file per input image?
> 
> The multi-page feature was added in 2014 by commit 25a8c7b. @stweil 

>Would it help to support an output base parameter with placeholders like page number or image base name to generate one output file per input image?

I can think of two cases for using this.

1. A document for which each page is available as a separate image.
2. Separate documents images which need to be run in a batch

For the first, current processing is sufficient, maybe with an additional  page separator character .

For the second, it would be useful to have the option you suggest. savedlist

```
Sachitra_Saraswati_Prasad_004932_HR-g4_page0001_1L.tif
Sachitra_Saraswati_Prasad_004932_HR-g4_page0002_2R.tif
Sachitra_Saraswati_Prasad_004932_HR-g4_page0004_2R.tif
Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_1L.tif
Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_2R.tif
```

1. `tesseract savedlist output`

Current implementation creates output file `output.txt` with concatenated OCR text.

Console displays filenames being processed

```
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 0 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0001_1L.tif
Page 1 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0002_2R.tif
Page 2 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0004_2R.tif
Page 3 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_1L.tif
Page 4 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_2R.tif
```

2. `tesseract savedlist savedlist'

Suggested option for additional implementation, to create separate output files for each line in the `savedlist` list file. 

The syntax of the command could be different - either not specifying the outputbase or using the same name as the listfile or something else - whatever is easy to implement.

The output files will be based on the filenames in the listfile, same basename with change of extenstion.

The listfile in the above two cases can be created by eg. by `ls -l *.tif >savedlist`

3. option suggested above by @stweil 

> get pairs of image file name and output base from the file list instead of image files only. 

This would require change to listfile  format. 

Optionally, if output base is not given separately in listfile, then it will create file based on filenames in list.



 @stweil Please compare the time taken for the list option vs multipage tif vs processing each file separately in a loop to see what may be a recommended option.

here is the result of my singular test using Hindi traineddata.

```
 ----------------------------------------------------
Sachitra_Saraswati_Prasad_004932_HR-g4_page0001_1L.tif
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1

real    0m24.880s
user    1m11.375s
sys     0m0.813s
Sachitra_Saraswati_Prasad_004932_HR-g4_page0002_2R.tif
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1

real    0m37.305s
user    1m49.734s
sys     0m0.891s
Sachitra_Saraswati_Prasad_004932_HR-g4_page0004_2R.tif
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1

real    0m43.672s
user    2m1.156s
sys     0m1.078s
Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_1L.tif
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1

real    0m39.755s
user    1m46.578s
sys     0m0.922s
Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_2R.tif
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1

real    0m23.214s
user    1m6.891s
sys     0m0.781s
-------------------------------------------------
list.txt
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 0 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0001_1L.tif
Page 1 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0002_2R.tif
Page 2 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0004_2R.tif
Page 3 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_1L.tif
Page 4 : Sachitra_Saraswati_Prasad_004932_HR-g4_page0005_2R.tif

real    3m26.132s
user    9m32.531s
sys     0m2.594s

---------------------------------------------------
multitest.tif
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Page 1
Page 2
Page 3
Page 4
Page 5

real    3m35.027s
user    9m44.297s
sys     0m2.781s

---------------------- Page numbering in this option starts with 0. Should be changed to 1 (similar to multipage tif).

If a multipage tiff is listed as one of the files in the listfile, only its first page is processed.

 @jbarlow83 

I ran the test just once under WSL on Windows10 for language Hindi - there may have been other processes running at the same time which might have impacted the numbers. Hence my request to @stweil to test and compare the features.

Do you find individual files to get processed faster? Amitdo, thanks for adding me to this Request For Comments.

I think it is a very good idea to change the text output format to use the form feed character (U+000C) mark page boundaries. Hopefully this is very easy.

Reasonable people can disagree, but I don't think Tesseract should support an output base parameter with placeholders like page number.  There's a lot of combinations already between inputs and outputs. Single page input images, multipage input images, lists of images in a file, lists of images on stdin, streaming, various output format. Combinations are tricky, and it is a big reason why still haven't restored the "OCR to memory buffer" feature that has been mentioned so many times. 

If this is mostly about increasing throughput by eliminating initialization time, a common thing to do is to create an "OCR service" where a warmed up Tesseract daemon runs all the time. This type of program would make calls to libtesseract, but is otherwise a separate program. Not an additional feature to tesseractmain.cpp
 tesseract writes the file names to console, these can be combined with the output. 

`tesseract list.txt  stdout  > output.txt 2>&1`

or 

 `tesseract list.txt  stdout -c include_page_breaks=1 > output.txt 2>&1`

 >I think it is a very good idea to change the text output format to use the form feed character (U+000C) mark page boundaries. Hopefully this is very easy.

@stweil PR, please! I think the question is whether adding of page breaks should be the default in text mode, similar to HOCR or PDF.

If FF is added after each page then the empty line may not be required.
 https://github.com/tesseract-ocr/tesseract/commit/8bb5a89d5ac0e0c5e94fc566dff70ee1fffc95d1 by @stweil 

```
Don't add empty line to text output
Empty lines in text output are needed to separate paragraphs,
but there should not be an empty line at the end of the text.
```

What about the other changes? I agree.
 Thanks!  Please use tesseract user forum for asking support (especially for your own projects). This not tesseract issue.  + @theraysmith  for review and response.  Did you run autogen.sh again after installing autoconf-archive?

 @oscaroxy The master branch is the development branch for 4.0 alpha. Please try to build using a commit before the one mentioned by @amitdo . >OS: Centos 7
Compiler: gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)

I have built only on ubuntu, not centos. Maybe there are differences in the o/s causing the issue.

Since now you have downloaded source using git clone, you can use git to go back to an older commit and try the build there.

```
git reset --hard 2008dafa7310970002302083659575819e57bb67
```
will take you back to a commit about 4 days back before the commit @amitdo referred to. 
you can give the following command to verify

```
git log -1
```

Please try your build there again.
 @wildloop Thanks!

I have added the info on wiki page at https://github.com/tesseract-ocr/tesseract/wiki/4.0-Dockerfile I have successful build on Centos:   7.3.1611

please reference: https://ivanzz1001.github.io/records/post/ocr/2017/09/08/tesseract-install  Thanks! No crash now.

```
tesseract p002-crop.bmp missing/bmp --oem 1 --psm 6 -l hin
Tesseract Open Source OCR Engine v4.00.00alpha-520-gffb1ec3 with Leptonica
Error during processing.
```

A more descriptive error message will be helpful.   I was trying to make a debug build as per https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-301166173

Configure gave the following error

```
bash ./debug.sh
checking for g++... g++
checking whether the C++ compiler works... yes
checking for C++ compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
Using git revision: 4.00.00alpha-512-g6bebe71
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking for style of include used by make... GNU
checking whether make supports nested variables... yes
configure: error: source directory already configured; run "make distclean" there first
make: *** No targets specified and no makefile found.  Stop.
```

So I ran `make distclean` which ended with the following error

```
 make distclean
Making distclean in arch
make[1]: Entering directory `/mnt/c/Users/User/shree/tesseract-head/arch'
make[2]: Entering directory `/mnt/c/Users/User/shree/tesseract-head/arch'
test -z "" || rm -f
rm -rf .libs _libs
test -z "libtesseract_avx.la libtesseract_sse.la libtesseract_arch.la" || rm -f libtesseract_avx.la libtesseract_sse.la libtesseract_arch.la

...

make[1]: Leaving directory `/mnt/c/Users/User/shree/tesseract-head/api'
Making distclean in .
make[1]: Entering directory `/mnt/c/Users/User/shree/tesseract-head'
rm -rf .libs _libs
make[2]: Entering directory `/mnt/c/Users/User/shree/tesseract-head/training'
Makefile:960: ../vs2010/port/.deps/strcasestr.Plo: No such file or directory
make[2]: *** No rule to make target `../vs2010/port/.deps/strcasestr.Plo'.  Stop.
make[2]: Leaving directory `/mnt/c/Users/User/shree/tesseract-head/training'
make[1]: *** [clean-local] Error 2
make[1]: Leaving directory `/mnt/c/Users/User/shree/tesseract-head'
make: *** [distclean-recursive] Error 1
``` ```
./configure  --enable-debug --disable-shared --disable-static CXXFLAGS="-g"
```
works fine.

The error comes when trying to build in a subdir with 

```
../../configure  --enable-debug --disable-shared --disable-static CXXFLAGS="-g"
``` Thanks!  What's this? We get very long build times with it.
See https://travis-ci.org/tesseract-ocr/tesseract/builds/232993174
cc: @zdenop @stweil @amitdo   Do we care about such leaks? 
api and renderer hold something important? Probably exit()s can be replaced with exceptions?
And print errors in main. Ah, yes, remember their policy. :(  Yes, there have been requests for more compact/compressed traineddata files.

Another Qn. 

* Should the new format be limited to tesseract 4.0 or also applied to 3.05? >libminizip-dev was added in Ubuntu Xenial (16.04), so the current Travis build environment which is based on Ubuntu Trusty does not provide it.

Why not use a different compression library that is available on different o/s as well as older ubuntu versions? Please move discussion to tesseract-dev forum. This is significant change. What about lz4?

---

btw, libarchive handles all formats.
https://github.com/libarchive/libarchive libarchive supported since very early ubuntu versions and in almost any other linuxes.
http://packages.ubuntu.com/search?suite=precise&searchon=names&keywords=libarchive

Personally, I'm using libarchive in cppan. The code for working with any formats is very simple, see:
https://github.com/egorpugin/primitives/blob/master/pack/include/primitives/pack.h
https://github.com/egorpugin/primitives/blob/master/pack/src/pack.cpp

Of course, this is pack/unpack archive code, but streaming code should be pretty similar and simple too. Actually I wanted to say lzma, which is .xz/.7z extensions.
Sorry. :) What libraries are currently in use in your PR?
libarchive? minizip? libzip? 
I see libarchive in build scripts, but not in code.
Maybe it worth it to use only one implementation (library)? I don't like multiple implementations for same thing. Please also try the test with a different language. Maybe one which has the
largest traineddata size, to see if filesize has any impact to the relative
speeds
Thanks.
 lzma compresses slower but better? Or is it also decompress slower? @theraysmith  wrote on 4/18/14

>I have no objection to switching to zip (with no tar) for the tessdata files. That should be usable by everybody more easily.

and on 4/20/14

>I spent some time looking at zlib. It doesn't seem to make it easy to randomly access named entities in >a gzip file, unless I am missing something. The memory compress/uncompress functions are quite nice >though.
>
> For the next version it would be nice to:
> Update tessdatamanager to cope with compressed components.
> Eliminate fread/fscanf from file input code and allow everything to read from a memory buffer.
> These can probably both be achieved with the TFile class that I added for 3.03.
> 
> This is a change in direction from my previous work with new classifier experiments, where I have been writing everything to use Serialize/DeSerialize and FILE streams, but this doesn't seem to be as portable as I had hoped, due to its reliance on fmemopen. It seems it would be better to make everything use memory buffers and push the file I/O responsibility out to TessDataManager/TFile, which could then just as easily deal with compressed files or in-memory data.

@stweil Do all the methods you tested support ` randomly accessing named entities`?

@theraysmith Is there a particular reason for ` zip (with no tar)`? >I'd drop support for the old format
and remove combine_tessdata.

combine_tessdata is used for extracting components, replacing some, etc. 

Probably first step can be a zippeddata file which after unzipping provides the traineddata.

Unless, there are zip/unzip equivalent features for combine_tessdata functionality. Would same extension cause confusion as to which version of traineddata file was being used? I would suggest to use different file extensions (e.g. traineddata4). This would help us to have installation of 3.05 and 4.00 at the same time and to avoid mistakes like using combine_tessdata on compressed data.

I would suggest to use zip compression as default because it seems to be supported on all platforms easily... Maybe support for multiple compression formats would be interesting (e.g. if somebody will be focused on size and not on speed), but this is not must at the moment. On Windows, that would require a rename with .zipextension, or the
> installer must register .traineddata as a zipped data type. Otherwise the
> Windows explorer won't decompress such files easily. Personally I prefer to
> keep the .traineddata extension even for zip files (like Java does it
> with its .jar extension).
>
>
> This comment has me confused. Are we talking about
1. compressing the files just for installation. after installation files
are uncompressed
2. compressing the components of a file using zlib and changing
tessdatamanager to allow that. FIles are otherwise uncompressed.
3. Leaving compressed files on disk after installation and relying on some
auto decompression feature of the OS to decompress the files on reading
?

I thought we were talking about 2, but registering something sounds like 3.
If we allow a backwards-breaking change, we could change the format of
traineddata, add a version number, and allow compression of components. It
would be fairly easy.


-- 
Ray.
 >  I don't know the situation for Visual Studio, but expect that small libraries like libzzip for example are much easier to build than larger libraries with more functionality like libarchive.

We are using cppan build for Visual Studio, libarchive is already there. It can be included with adding 1-3 lines. Other small libs you listed are not a problem too if needed. The most important features for me are:
TessdataManager::LoadMemBuffer must still work, ie the compression library must still be able to work on a pre-loaded file
Speed. There is little point compressing if the decompression speed is poor.
Command-line and API must still work. If combine_tessdata goes away, then there must be an easy-to-use replacement, and Tessdatamanager::SaveFile must still work.
Minimal extra dependencies - that was the main motivation for using zlib, as it is already an indirect dependency.
Summary: implement inside TessdataManager, and keep its existing interface in tact. libarchive choice is obvious. > With a standard archive format (like zip), it would be easy to write additional information into the archive files, for example a README or a VERSION file. Tesseract would ignore any such component which it does not know.

There is already possibility to include `config` file to traineddata. I would avoid suggest to add other file and made the `config `file as required with needed information (e.g. version number of data file, minimum (maximum?) version of tesseract, author, licence etc...) Just for the note: Google released during last years these compression algorithms:

- [zopfli](https://github.com/google/zopfli) focused on size
- [brotli](https://github.com/google/brotli) focused on [speed](https://opensource.googleblog.com/2015/09/introducing-brotli-new-compression.html)

I found out [result of Compression Benchmark](https://quixdb.github.io/squash-benchmark/#results-table), if somebody would like to make more test... @amitdo I have added a separate issue re multipage feature - https://github.com/tesseract-ocr/tesseract/issues/928

Rather than a new repo, we can use RFC: for issues in this repo itself, unless @theraysmith or @zdenop have an objection. @stweil Please also make a PR for tessdata to convert all current 4.0 traineddata files in new zipped format.

@zdenop I would also suggest that a new tag/release be made 4.0 (alpha1/2 or beta) for tesseract, tessdata as well as langdata with this new PR.

Thanks! Please also add version info as part of traineddata. 
I think @zdenop had suggested adding it to the config files.  re: >waiting for my pull requests to be merged:

@rfschtkt see comment by Ray at  https://github.com/tesseract-ocr/tesseract/pull/744#issuecomment-284917919

>>Yes, each commit gets reviewed by me and someone else at Google. Those warnings must be fixed sometime. Even in leptonica from which many warnings come too.  Please use tesseract user forum for asking support.  We do not provide support for 3rd party project (Tess4j). Please consult issue at Tess4j authors.  Please use syntax which will work with all of the following.

Supported Compilers

   - GCC 4.8 and above
   - Clang 3.4 and above
   - MSVC 2015, 2017


ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, May 9, 2017 at 6:18 PM, rfschtkt <notifications@github.com> wrote:

> Yes, I think it's a bug in that g++ version. I had successfully built with
> 5.4.0. I'll try again with an explicit move constructor...
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/897#issuecomment-300152139>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o-UsQwL8k-hPWd3NDI4loF080sLUks5r4GCXgaJpZM4NVPlU>
> .
>
 Oh, btw.
Isn't GetUTF8Text() should return C++-ish std::string? Instead of that stupid non-owning `char*` memory ptr.
It would be way better fix for all of this.  @rfschtkt: there is merge a conflict in api/pdfrenderer.cpp. Can you fix it?  Default setting of page segmentation treats the image as columns of text. If you want the whole line recognized as a unit, try with --psm 6.

The Eastern Arabic numerals are currently not recognized, it is a known issue - https://github.com/tesseract-ocr/langdata/issues/72

 @zdenop Issue Resolved. Please close.  If you already built it under Ubuntu, you should be able to use it directly.

On May 7, 2017 1:14 PM, "speedfl" <notifications@github.com> wrote:

> Hello,
>
> First thanks for your job. I am trying to run tesseract 4 but I am getting
> an issue:
>
> Info in bmfCreate: Generating pixa of bitmap fonts from string terminate
> called after throwing an instance of 'std::bad_alloc' what():
> std::bad_alloc Aborted (core dumped)
>
> Step to reproduce (with a docker file):
>
> `
> FROM ubuntu
>
> RUN apt-get update && apt-get install -y \
> 	autoconf \
> 	automake \
> 	libtool \
> 	autoconf-archive \
> 	pkg-config \
> 	libpng12-dev \
> 	libjpeg8-dev \
> 	libtiff5-dev \
> 	zlib1g-dev \
> 	libicu-dev \
> 	libpango1.0-dev \
> 	libcairo2-dev \
> 	git \
> 	curl && \
> 	rm -rf /var/lib/apt/lists/*
>
> RUN curl http://www.leptonica.org/source/leptonica-1.74.1.tar.gz -o leptonica-1.74.1.tar.gz && \
> 	tar -zxvf leptonica-1.74.1.tar.gz && \
> 	cd leptonica-1.74.1 && ./configure && make && make install && \
> 	cd .. && rm -rf leptonica*
>
> RUN git clone --depth 1 https://github.com/tesseract-ocr/tesseract.git && \
> 	cd tesseract && \
> 	./autogen.sh && \
> 	./configure --enable-debug && \
> 	LDFLAGS="-L/usr/local/lib" CFLAGS="-I/usr/local/include" make && \
> 	make install && \
> 	ldconfig && \
> 	make training && \
> 	make training-install && \
> 	cd .. && rm -rf tesseract
>
> # Get basic traineddata
> RUN curl https://github.com/tesseract-ocr/tessdata/raw/master/eng.traineddata > eng.traineddata && \
> 	mv eng.traineddata /usr/local/share/tessdata/
>
> RUN curl https://github.com/tesseract-ocr/tessdata/raw/master/fra.traineddata > fra.traineddata && \
> 	mv fra.traineddata /usr/local/share/tessdata/
>
> `
>
> Then:
>
> docker build -t tesseract4 docker run tesseract4 docker run -t -i
> tesseract4 /bin/bash mkdir test cd test curl http://tleyden-misc.s3.
> amazonaws.com/blog_images/ocr_test.png > test.png tesseract test.png out
>
> Can someone explain me what is happening?
>
> For information I have 2471 megabytes of memory remaning
>
> Thanks in advance
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/893>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o4aI-YeOByGqOjU1pYXUCW5nMGQ0ks5r3XZPgaJpZM4NTDSf>
> .
>
 I do not know about docker images. 

I thought @amitdo was referring to --enable-debug option of configure. Error message `Info in bmfCreate: Generating pixa of bitmap fonts `

is similar to https://github.com/tesseract-ocr/tesseract/issues/873

 That error/info message is from Leptonica

https://github.com/cotdp/leptonica/blob/master/src/bmf.c Please check where leptonica is installed. Do you have multiple versions? @speedfl No need to rebuild. I have not used docker so was just guessing.

@xlight https://github.com/tesseract-ocr/tesseract/issues/817#issuecomment-299716089 maybe able to help. BTW, the info message from leptonica is probably not related to the terminate error.

Please try to build again with latest source of tesseract from github. what output do you get for 

tesseract -v
 Also, have you tried a tiff or jpg file or the test files from tesseract
repo.

Could be an issue with the version of libpng ??
 @zdenop  Issue can be closed. https://github.com/tesseract-ocr/tesseract/issues/893#issuecomment-299856133 Added page to wiki - https://github.com/tesseract-ocr/tesseract/wiki/4.0-Dockerfile @stweil Should the basic traineddata be osd and eng in the dockerfile that I posted on the wiki?

@amitdo Is the tessdata a tiff file??? Thanks! I have updated https://github.com/tesseract-ocr/tesseract/wiki/4.0-Dockerfile

Please review and add any other required files (eg. configs etc.) to the docker container. >https://github.com/tesseract-ocr/tesseract/issues/919#issuecomment-302300156

Definitions of Docker containers and scripts that help to compile and run Tesseract 4 are available at:

https://github.com/tesseract-shadow/tesseract-ocr-compilation  @bmwmy please see https://github.com/tesseract-ocr/tesseract/issues/648#issuecomment-3180952  applied.

Zdenko

On Sat, May 6, 2017 at 9:02 AM, Stefan Weil <notifications@github.com>
wrote:

> The Appveyor cache <https://www.appveyor.com/docs/build-cache/> does not
> seem to work currently. According to the documentation
> <https://www.appveyor.com/blog/2016/09/28/the-new-build-cache/>, it is
> only enabled for new user accounts by default. @zdenop
> <https://github.com/zdenop>, you have to apply to get it enabled for your
> account.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/890#issuecomment-299620799>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAjCzAfq1jpyiE1PGrRmM0lNTIWV4O1Dks5r3BsigaJpZM4NSpiD>
> .
>
 So, can we merge this?
 Ok, cache is up'n'running, so build time for windows is only 4 minutes now.
https://ci.appveyor.com/project/zdenop/tesseract/build/master.843 Yes. :) Thanks, Stefan!
I'm activating training tools for building too. Ok, done. Also this means that we have full set of training tools in artifacts section of every build.
https://appveyorcidata.blob.core.windows.net/zdenop-27740/tesseract/master-844/m23d78nft28axsd6/build/bin/tesseract-master.844.zip?sv=2015-12-11&sr=c&sig=rR1rlQbih6341cXryCQphaH4YWr7n3Enudndwmvb8xQ%3D&st=2017-05-06T11%3A43%3A20Z&se=2017-05-06T11%3A49%3A20Z&sp=r
So people could download binaries from there. This is great. Is there a generic link to artifacts section that can be
added to the wiki?

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, May 6, 2017 at 5:19 PM, Egor Pugin <notifications@github.com> wrote:

> Also this means that we have full set of training tools in artifacts
> section of every build.
> https://appveyorcidata.blob.core.windows.net/zdenop-27740/
> tesseract/master-844/m23d78nft28axsd6/build/bin/tesseract-master.844.zip
> So people could download binaries from there.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/890#issuecomment-299634620>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o7k6256ztJKTo_rYGqI1ITy54m--ks5r3F4tgaJpZM4NSpiD>
> .
>
 FYI: According reply from Appveyor: 

> Build cache is already enabled for all AppVeyor accounts.
> The issue is that it won’t be saved during PR build. Won't be saved, but will be loaded.
We care only about loading cache. Hi Egor,

I downloaded the zip file for tesseract-master.848.zip
It has some of the training tools, but not text2image .
 True. I'll fix. Ok, fixed. Thanks, @Shreeshrii!
Also backported to 3.05 branch. I tried just now to use `git bash' (https://git-for-windows.github.io/) under windows with tesstrain.sh script for training. Works after putting the exe and dll files in the bin directory. 

This makes it possible for those using windows to try LSTM training using tesstrain.sh.

Thank you all for your work in making it possible. @egorpugin https://ci.appveyor.com/project/zdenop/tesseract/build/3.05.851 failed. Fixed. Please also check if some setting needs to be changed so that 'info'
messages from leptonica are not displayed. The version built with automake
does not have these.

eg.

File
C:/Users/vvkum/AppData/Local/Temp/tmp.Q59kWa5fdq/hin/hin.FreeSerif.exp0.lstmf
page 218 :
Mean rms=5.246%, delta=46.464%, train=104.341%(100%), skip ratio=0%
Info in fopenReadFromMemory: work-around: writing to a temp file
Iteration 188: ALIGNED TRUTH : गई हैं. एशिया डैनियल श्रिया था. हैं.
Iteration 188: BEST OCR TEXT :
File
C:/Users/vvkum/AppData/Local/Temp/tmp.Q59kWa5fdq/hin/hin.FreeSerif.exp0.lstmf
page 94 :
Mean rms=5.245%, delta=46.466%, train=104.318%(100%), skip ratio=0%
Info in fopenReadFromMemory: work-around: writing to a temp file
Iteration 189: ALIGNED TRUTH : फ्लेमिंग लिया। बर्बाद स्पोर्ट्स %>
Iteration 189: BEST OCR TEXT :
File
C:/Users/vvkum/AppData/Local/Temp/tmp.Q59kWa5fdq/hin/hin.FreeSerif.exp0.lstmf
page 208 :
Mean rms=5.243%, delta=46.436%, train=104.295%(100%), skip ratio=0%
Info in fopenReadFromMemory: work-around: writing to a temp file
Iteration 190: ALIGNED TRUTH : हैप्पी सुर्ख घर मिलता हुई। के
Iteration 190: BEST OCR TEXT :
 https://github.com/tesseract-ocr/tesseract/pull/543

related to info messages from leptonica

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, May 7, 2017 at 11:57 PM, ShreeDevi Kumar <shreeshrii@gmail.com>
wrote:

> Please also check if some setting needs to be changed so that 'info'
> messages from leptonica are not displayed. The version built with automake
> does not have these.
>
> eg.
>
> File C:/Users/vvkum/AppData/Local/Temp/tmp.Q59kWa5fdq/hin/hin.FreeSerif.exp0.lstmf
> page 218 :
> Mean rms=5.246%, delta=46.464%, train=104.341%(100%), skip ratio=0%
> Info in fopenReadFromMemory: work-around: writing to a temp file
> Iteration 188: ALIGNED TRUTH : गई हैं. एशिया डैनियल श्रिया था. हैं.
> Iteration 188: BEST OCR TEXT :
> File C:/Users/vvkum/AppData/Local/Temp/tmp.Q59kWa5fdq/hin/hin.FreeSerif.exp0.lstmf
> page 94 :
> Mean rms=5.245%, delta=46.466%, train=104.318%(100%), skip ratio=0%
> Info in fopenReadFromMemory: work-around: writing to a temp file
> Iteration 189: ALIGNED TRUTH : फ्लेमिंग लिया। बर्बाद स्पोर्ट्स %>
> Iteration 189: BEST OCR TEXT :
> File C:/Users/vvkum/AppData/Local/Temp/tmp.Q59kWa5fdq/hin/hin.FreeSerif.exp0.lstmf
> page 208 :
> Mean rms=5.243%, delta=46.436%, train=104.295%(100%), skip ratio=0%
> Info in fopenReadFromMemory: work-around: writing to a temp file
> Iteration 190: ALIGNED TRUTH : हैप्पी सुर्ख घर मिलता हुई। के
> Iteration 190: BEST OCR TEXT :
>
>
 #543 is present in master and 3.05.  https://github.com/tesseract-ocr/tesseract/issues/720#issuecomment-281981614 Thanks, @stweil . I am closing this PR as not required.  This is probably because LSTM engine trains on text lines rather than separate letters.

@theraysmith can clarify.   @zdenop  Issue can be closed. @stweil Please update the windows binaries on your site with the latest version. Please also check that lstm.train config file is being included. (saw an error on another issue related to missing lstm.train - not sure which package it was from). Thanks! Thanks, Stefan!  ```
gdb --args lstmtraining -U ~/tess4training/eng/eng.unicharset \
>   --script_dir ../langdata   \
>   --append_index 5 --net_spec '[Lfx256 O1c105]' \
>   --continue_from ~/tess4training/englayer_from_eng/eng.lstm \
>   --model_output ~/tess4training/englayer_from_eng/englayer \
>   --train_listfile ~/tess4training/eng/eng.training_files.txt \
>   --target_error_rate 0.01 \
>   --debug_interval  -1
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from lstmtraining...done.
(gdb) run
Starting program: /usr/local/bin/lstmtraining -U /home/shree/tess4training/eng/eng.unicharset --script_dir ../langdata --append_index 5 --net_spec \[Lfx256\ O1c105\] --c
ontinue_from /home/shree/tess4training/englayer_from_eng/eng.lstm --model_output /home/shree/tess4training/englayer_from_eng/englayer --train_listfile /home/shree/tess4t
raining/eng/eng.training_files.txt --target_error_rate 0.01 --debug_interval -1
warning: Error disabling address space randomization: Success
warning: linux_ptrace_test_ret_to_nx: PTRACE_KILL waitpid returned -1: Interrupted system call
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Loaded file /home/shree/tess4training/englayer_from_eng/eng.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tess4training/englayer_from_eng/eng.lstm
Other case É of é is not in unicharset
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Num outputs,weights in serial:
  Lfx256:256, 394240
  Fc105:105, 26985
Total weights = 421225
Built network:[1,0,0,1[C5,5Ft16]Mp3,3Lfys64Lfx128Lrx128Lfx256Fc105] from request [Lfx256 O1c105]
Training parameters:
  Debug interval = -1, weights = 0.1, learning rate = 0.0001, momentum=0.9
[New Thread 0x7fea20f40700 (LWP 1158)]
Loaded 104/104 pages (1-104) of document /home/shree/tess4training/eng/eng.Arial.exp0.lstmf
[Thread 0x7fea20f40700 (LWP 1158) exited]
[New Thread 0x7fea1bff0700 (LWP 1159)]
Loaded 104/104 pages (1-104) of document /home/shree/tess4training/eng/eng.Times_New_Roman.exp0.lstmf
[Thread 0x7fea1bff0700 (LWP 1159) exited]
[New Thread 0x7fea1bff0700 (LWP 1160)]
[New Thread 0x7fea20f40700 (LWP 1161)]
[New Thread 0x7fea1acf0700 (LWP 1162)]
Iteration 0: ALIGNED TRUTH : used through € between NEW % J. should when High when We it
Iteration 0: BEST OCR TEXT : St$Zv#kv#qSZ]$vUZUyvHvkyEHkSZUZtvyvyvHvq$Rwq,qtHgE#gqZUSUZ#EqgqSqSqyvHvt$q£$R$RqtS$U,#,v$USUHZq,c$U$qSq,$#vyvHvSHZSZ=Z$[ [vESUSZUyvHvSqSqyHZ
vt,q€U€vlvyvySyHvqtSUZtyHvSqSqvqHq,U$U,UegqgqveZS
File /tmp/tmp.E8R3inud3B/eng/eng.Arial.exp0.lstmf page 6 :

Program received signal SIGSEGV, Segmentation fault.
operator+= (addend=..., this=0xb92048) at ../ccstruct/matrix.h:256
256               (*this)(x, y) += addend(x, y);
(gdb) backtrace
#0  operator+= (addend=..., this=0xb92048) at ../ccstruct/matrix.h:256
#1  tesseract::WeightMatrix::Update (this=0xb92048, learning_rate=<optimized out>, momentum=0.89999997615814209, num_samples=<optimized out>) at weightmatrix.cpp:261
#2  0x00007fea254b7083 in tesseract::Plumbing::Update (this=0xb91e80, learning_rate=1.24999988e-05, momentum=0.899999976, num_samples=1) at plumbing.cpp:219
#3  0x00007fea254b7083 in tesseract::Plumbing::Update (this=0xb91230, learning_rate=9.99999975e-05, momentum=0.899999976, num_samples=1) at plumbing.cpp:219
#4  0x00007fea254a8469 in tesseract::LSTMTrainer::TrainOnLine (this=this@entry=0x7ffff03ebaf0, trainingdata=<optimized out>, batch=batch@entry=false)
    at lstmtrainer.cpp:813
#5  0x0000000000406ff4 in TrainOnLine (batch=false, samples_trainer=0x7ffff03ebaf0, this=0x7ffff03ebaf0) at ../lstm/lstmtrainer.h:273
#6  main (argc=1, argv=0x7ffff03ec488) at lstmtraining.cpp:198
(gdb) up
#1  tesseract::WeightMatrix::Update (this=0xb92048, learning_rate=<optimized out>, momentum=0.89999997615814209, num_samples=<optimized out>) at weightmatrix.cpp:261
261       if (momentum > 0.0) wf_ += updates_;
(gdb) print num_samples
$1 = <optimized out>
(gdb) print momentum
$2 = 0.89999997615814209
(gdb) print wf_
$3 = {_vptr.GENERIC_2D_ARRAY = 0x7fea257bf230 <vtable for GENERIC_2D_ARRAY<double>+16>, array_ = 0xb92230, empty_ = 0, dim1_ = 16, dim2_ = 26, size_allocated_ = 416}
(gdb) print updates_
$4 = {_vptr.GENERIC_2D_ARRAY = 0x7fea257bf230 <vtable for GENERIC_2D_ARRAY<double>+16>, array_ = 0x0, empty_ = 0, dim1_ = 0, dim2_ = 0, size_allocated_ = 0}
(gdb) quit

``` This was the with the latest code from github, including Ray's latest commit regarding endian support.

You will need to change the fonts dir in the commands below.

```
rm -rf ~/tess4training/eng  
  
training/tesstrain.sh \
   --fonts_dir ~/.fonts \
   --lang eng  \
   --noextract_font_properties \
   --linedata_only \
   --exposures "0" \
   --langdata_dir ../langdata \
   --tessdata_dir ../tessdata \
   --fontlist "Arial"  "Times New Roman,"  \
   --output_dir ~/tess4training/eng
  
rm -rf ~/tess4training/englayer_from_eng    

mkdir -p ~/tess4training/englayer_from_eng 

combine_tessdata -e ../tessdata/eng.traineddata \
  ~/tess4training/englayer_from_eng/eng.lstm

lstmtraining -U ~/tess4training/eng/eng.unicharset \
  --script_dir ../langdata   \
  --append_index 5 --net_spec '[Lfx256 O1c105]' \
  --continue_from ~/tess4training/englayer_from_eng/eng.lstm \
  --model_output ~/tess4training/englayer_from_eng/englayer \
  --train_listfile ~/tess4training/eng/eng.training_files.txt \
  --target_error_rate 0.01 \
  --debug_interval  -1
``` Thanks, Amit. I had seen that comment but thought that it was related to training of new language models. I am not sure what is included in `ImageData`.

The first part of the training commands (tesstrain.sh) runs text2image and creates box/tiff pairs and these are then used to create unicharset, dawg and lstmf files. I have not used any external box/tiff pairs.

These lstmf files are used along with the extracted lstm file from the traineddata from the repo.

`combine_tessdata -e ../tessdata/eng.traineddata 
  ~/tess4training/englayer_from_eng/eng.lstm`

Possibly, this could be the cause of incompatibility, since these files were built with earlier code. the `lstmtraining` command for this issue is for `replacing a top layer`. lstmtraining command for replacing a top layer is working now after the changes by Ray.

```
 lstmtraining -U ~/tess4training/eng/eng.unicharset \
>   --script_dir ../langdata   \
>   --append_index 5 --net_spec '[Lfx256 O1c105]' \
>   --continue_from ~/tess4training/englayer_from_eng/eng.lstm \
 >   --model_output ~/tess4training/englayer_from_eng/englayer \
>   --train_listfile ~/tess4training/eng/eng.training_files.txt \
>   --target_error_rate 0.01 \
>   --debug_interval  -1
Loaded file /home/shree/tess4training/englayer_from_eng/eng.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tess4training/englayer_from_eng/eng.lstm
Other case É of é is not in unicharset
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Num outputs,weights in serial:
  Lfx256:256, 394240
  Fc105:105, 26985
Total weights = 421225
Built network:[1,0,0,1[C5,5Ft16]Mp3,3Lfys64Lfx128Lrx128Lfx256Fc105] from request [Lfx256 O1c105]
Training parameters:
  Debug interval = -1, weights = 0.1, learning rate = 0.0001, momentum=0.9
Loaded 104/104 pages (1-104) of document /home/shree/tess4training/eng/eng.Arial.exp0.lstmf
Loaded 104/104 pages (1-104) of document /home/shree/tess4training/eng/eng.Times_New_Roman.exp0.lstmf
Iteration 0: ALIGNED TRUTH : (GeneRIF) World and ABOUT time October the service through is Help
Iteration 0: BEST OCR TEXT : #yvtEktgqtHv$gqy$g$ScqESvq,UgU,UZHZtZ€U Zq,yqaqtqHt SUctéHS$Ht:HtS$Uu5,q,UvUtHSqgqtqH\t$gZUHZtZHZqZqZqEZUyHEqgq€EgtqZtZv#
v\EyZqgqEUyHvHEHZHSZ$ZtvyHZvUv#qtq€U€e#qevZH
File /home/shree/tess4training/eng/eng.Arial.exp0.lstmf page 0 :
Mean rms=8.666%, delta=100%, train=301.515%(100%), skip ratio=0%
Iteration 1: ALIGNED TRUTH : through they not this our English ° 14 ® Development of 8 What's an
Iteration 1: BEST OCR TEXT : kZyHZkEZkSZ$ZtqtyHZq]UtyHESqS#tHvZ:ZSZqUZyHZvk#t4SZSZqZqyq4,q,eHvZvtv€yveS#tyHZq$qSq4$SU,Uq#R#$SHtU,$,H\Sq7Zt#q¥SHtvtktaH
vHSZtHZSZqS:SZSv#$S,tU$U5UH\HvZSZUHvqEyqHZH
File /home/shree/tess4training/eng/eng.Times_New_Roman.exp0.lstmf page 0 :
Mean rms=8.647%, delta=100%, train=303.743%(100%), skip ratio=0%
Iteration 2: ALIGNED TRUTH : good 25 now this [J User INDEX had help 6 all iGoogle if in Ca¥ years
Iteration 2: BEST OCR TEXT : StvEHE#Zt]$v4#kqUklvtEHEHZSUZ#tEZUyHvZv#vq,uc$,4[$[e#v#ZqZqyq$Sq$u$H\R$q$w$,tyHv#qHSc ZUtHvHZgqZvZHq]UlvE4qZ[$vUvUv$vHg\E
H\tv\HZqgq4SvUvevHqt$,SqHS$,q,S#taqSqHqH$S
File /home/shree/tess4training/eng/eng.Arial.exp0.lstmf page 1 :
Mean rms=8.648%, delta=100%, train=301.046%(100%), skip ratio=0%
Iteration 3: ALIGNED TRUTH : were jobs BBC get is" University A 1998 fixed Search And under .
Iteration 3: BEST OCR TEXT : ,SkZ]EoZqyES$qEvyvtHZRHZHkvqt$u$vt/$He}$,$vt,tvytZqZHqUvkv#$q]oq4U$tetHvHvZtZtqZtZHv#veZSyS#tq#}éHSqU [$Uk$SCpH.$vqUyHv$H
#qS€ Sv,$évEqg€qZqyt$tyHSqU$téHtHZHc SU,SvZtvHc]$eSZqZqy{q,S
File /tmp/tmp.2xbvLE29x7/eng/eng.Times_New_Roman.exp0.lstmf page 25 :
Mean rms=8.696%, delta=100%, train=306.644%(100%), skip ratio=0%

```  ```
gdb --args lstmtraining    --debug_interval -1   --continue_from ~/tess4training/englayer_from_eng/eng.lstm    --
train_listfile ~/tess4training/eng/eng.training_files.txt   --model_output ~/tess4training/englayer_from_eng/englstm   --target_error_rate 0.01
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from lstmtraining...done.
(gdb) run
Starting program: /usr/local/bin/lstmtraining --debug_interval -1 --continue_from /home/shree/tess4training/englayer_from_eng/eng.lstm --train_listfile /home/shree/tess4
training/eng/eng.training_files.txt --model_output /home/shree/tess4training/englayer_from_eng/englstm --target_error_rate 0.01
warning: Error disabling address space randomization: Success
warning: linux_ptrace_test_ret_to_nx: PTRACE_KILL waitpid returned -1: Interrupted system call
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Loaded file /home/shree/tess4training/englayer_from_eng/eng.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tess4training/englayer_from_eng/eng.lstm
[New Thread 0x7fdbdcf40700 (LWP 1141)]
Loaded 104/104 pages (1-104) of document /home/shree/tess4training/eng/eng.Arial.exp0.lstmf
[Thread 0x7fdbdcf40700 (LWP 1141) exited]
[New Thread 0x7fdbd7ff0700 (LWP 1142)]
Loaded 104/104 pages (1-104) of document /home/shree/tess4training/eng/eng.Times_New_Roman.exp0.lstmf
[Thread 0x7fdbd7ff0700 (LWP 1142) exited]
[New Thread 0x7fdbd7ff0700 (LWP 1143)]
[New Thread 0x7fdbdcf40700 (LWP 1144)]
[New Thread 0x7fdbd6770700 (LWP 1145)]
Iteration 0: ALIGNED TRUTH : used through € between NEW % J. should when High when We it
Iteration 0: BEST OCR TEXT : used through € between NEW % J. should when High when We it
File /tmp/tmp.E8R3inud3B/eng/eng.Arial.exp0.lstmf page 6 (Perfect):
Mean rms=0.152%, delta=0%, train=0%(0%), skip ratio=0%
Iteration 1: ALIGNED TRUTH : site May May 3. group 28 long any 2, to 2006 or into Search us Sports
Iteration 1: BEST OCR TEXT : site May May 3. group 28 long any 2, to 2006 or into Search us Sports
File /tmp/tmp.E8R3inud3B/eng/eng.Arial.exp0.lstmf page 1 (Perfect):
u.dim1() == num_outputs:Error:Assert failed:in file weightmatrix.cpp, line 227

Program received signal SIGSEGV, Segmentation fault.
ERRCODE::error (this=this@entry=0x7fdbe1a1ee48 <_ZL13ASSERT_FAILED>, caller=caller@entry=0x7fdbe1534d25 "u.dim1() == num_outputs", action=action@entry=ABORT,
    format=format@entry=0x7fdbe15164d4 "in file %s, line %d") at errcode.cpp:86
86            if (!*p)
(gdb) backtrace
#0  ERRCODE::error (this=this@entry=0x7fdbe1a1ee48 <_ZL13ASSERT_FAILED>, caller=caller@entry=0x7fdbe1534d25 "u.dim1() == num_outputs", action=action@entry=ABORT,
    format=format@entry=0x7fdbe15164d4 "in file %s, line %d") at errcode.cpp:86
#1  0x00007fdbe14c3ccf in tesseract::WeightMatrix::SumOuterTransposed (this=0x12613d8, u=..., v=..., in_parallel=<optimized out>) at weightmatrix.cpp:227
#2  0x00007fdbe1492742 in tesseract::FullyConnected::Backward (this=0x1261390, debug=<optimized out>, fwd_deltas=..., scratch=<optimized out>, back_deltas=0x1271f60)
    at fullyconnected.cpp:232
#3  0x00007fdbe14c18ba in tesseract::Series::Backward (this=0x1234ce0, debug=<optimized out>, fwd_deltas=..., scratch=0x7fffe65c6308, back_deltas=0x7fffe65c5ce0)
    at series.cpp:128
#4  0x00007fdbe14a8432 in tesseract::LSTMTrainer::TrainOnLine (this=this@entry=0x7fffe65c5fa0, trainingdata=<optimized out>, batch=batch@entry=false)
    at lstmtrainer.cpp:811
#5  0x0000000000406ff4 in TrainOnLine (batch=false, samples_trainer=0x7fffe65c5fa0, this=0x7fffe65c5fa0) at ../lstm/lstmtrainer.h:273
#6  main (argc=1, argv=0x7fffe65c68f8) at lstmtraining.cpp:198
(gdb) up
#1  0x00007fdbe14c3ccf in tesseract::WeightMatrix::SumOuterTransposed (this=0x12613d8, u=..., v=..., in_parallel=<optimized out>) at weightmatrix.cpp:227
227       ASSERT_HOST(u.dim1() == num_outputs);
(gdb) print num_outputs
$1 = 0
(gdb) quit
``` ```
   
rm -rf ~/tess4training/eng  
  
training/tesstrain.sh --fonts_dir ~/.fonts --lang eng  \
  --noextract_font_properties \
    --linedata_only \
  --exposures "0" \
   --langdata_dir ../langdata --tessdata_dir ./tessdata \
    --fontlist "Arial"  "Times New Roman,"  \
   --output_dir ~/tess4training/eng
  
rm -rf ~/tess4training/englayer_from_eng    

mkdir -p ~/tess4training/englayer_from_eng 

combine_tessdata -e ../tessdata/eng.traineddata \
  ~/tess4training/englayer_from_eng/eng.lstm
  
lstmtraining \
   --debug_interval -1 \
  --continue_from ~/tess4training/englayer_from_eng/eng.lstm \
   --train_listfile ~/tess4training/eng/eng.training_files.txt \
  --model_output ~/tess4training/englayer_from_eng/englstm \
  --target_error_rate 0.01
``` I have tessdata both as a peer level to tesseract (../tessdata) and as child level (./tessdata).

I keep the tessdata from repo at ../tessdata and keep the generated ones from training in ./tessdata.

I think it is only used here for the config files. the `lstmtraining` command here is the one for `fine tune`. Oops. Some changes got left out. I have a careful rethink of SetEnableTraining that didn't make it.
Reverting that line is not the solution.
Will fix it as soon as I check that the change doesn't break anything... OK, fixed with with https://github.com/tesseract-ocr/tesseract/commit/4fa463cd71366c2854090b38f25f94f3765d54b0  on what operating system are you trying this?

what commit number for source code are you using? You have used the complete net_spec. With append_index, it has to be just the top layers, eg.

```
lstmtraining -U ~/tesstutorial/ara/ara.unicharset \
  --script_dir ../langdata   \
  --append_index 5 --net_spec '[Lfx256 O1c105]' \
  --continue_from ~/tesstutorial/aralayer_from_ara/ara.lstm \
  --model_output ~/tesstutorial/aralayer_from_ara/aralayer \
  --train_listfile ~/tesstutorial/ara/ara.training_files.txt \
  --target_error_rate 0.01 \
  --debug_interval  -1
``` @zdenop  Issue can be closed.  I'm working towards fast and not silent builds on appveyor, but it could take 1-2 month. (cppan related)
But it's possible that your commit really broke something on windows. (I did not do any cppan/appveyor changes during last 1-1.5 month.) This will make build times up to near 1 hour or even 1+ which is impossible on appveyor.
There are too many warnings, so pushing all of them to stdout/err makes such slow build times. Nice. I'll investigate this, thanks! The best time to switch to POSIX types is after the legacy engine is
removed, otherwise it is just a waste of time merging it and reviewing it.

On Thu, Jul 13, 2017 at 1:19 AM, Stefan Weil <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith>, as you are just preparing
> a new larger code change: is it possible to use POSIX data types for the
> new / modified code, and can we switch to POSIX for the rest of the code
> directly after your update?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/878#issuecomment-315006966>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056WoPKurkhF6p7FaQbU71cN6cqaAfks5sNdMigaJpZM4NRHiF>
> .
>



-- 
Ray.
 Are we allowed to apply now? Or still no? A colleague volunteered to review such a change, but he is away at the
moment.
I think it will be faster for me to generate the change myself than to
up-integrate it from github, so I plan to make the edit next week. Maybe in
about 2 weeks I will be ready to push it to github.
In the mean time, I'm still maintaining a diff against github from the last
lot of edits while I find somebody to review those, and I have several
changes still in the pipeline with things like the language model changes
for the new release.

On Mon, Jul 24, 2017 at 12:46 PM, Stefan Weil <notifications@github.com>
wrote:

> I'm sure that some parts of the PR are not related to the legacy engine.
> Maybe those can be applied now.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/878#issuecomment-317533515>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Vr5jppDyQSKNK39T1PyoXNUh8dYks5sRPR4gaJpZM4NRHiF>
> .
>



-- 
Ray.
  Please use tesseract user forum for asking support  @amitdo is right. It is unrelated. Sorry for the confusion.

@ibr123 It is possible that the source for the two installs is from different commits from master. You are getting tesseract source by

`git clone https://github.com/tesseract-ocr/tesseract.git`

As you can see from https://github.com/tesseract-ocr/tesseract/commits/master
there have been many changes to 4.0 code over last one month.

What you could do is note the commit number from an install which works for you, and then use source as of that commit for your installation. Please note that 4.0 code is still in alpha stage and may change quickly.

config.log should show you the commit - search for GIT-REV

```
| #define GIT_REV "4.00.00alpha-458-g2ea946d"
``` Sorry, I do not know enough about VMware and Virtual Box to identify the
problem.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, May 10, 2017 at 1:54 PM, ibr123 <notifications@github.com> wrote:

> Hi,
>
> i looked at the config.log of the tesseract revision that works, took the
> revision number and switched the tesseract to that branch while installing
> the tesseract, and that revision was *"4.00.00alpha-367-g5baa8c8"*
> i made sure that its the correct revision by making detection of an image,
> i noticed that on the terminal the tesseract prints its revision number
> [image: ocrerror]
> <https://cloud.githubusercontent.com/assets/26926171/25889397/49639c4e-3572-11e7-909e-5c7fcf8eab3c.png>
> yet the same error was generated at the phase E where creating lstm files
> i even tried to install the suggested revision:
> *"4.00.00alpha-458-g2ea946d"* and the result was the same error but
> different line number at the tessedit.cpp, at the revision
> *"4.00.00alpha-367-g5baa8c8"* the error is: *stm_recognizer_->DeSerialize(&fp):Error:Assert
> failed:in file tessedit.cpp, line 202*
> and at the revision *"4.00.00alpha-458-g2ea946d"* the error was
> *stm_recognizer_->DeSerialize(&fp):Error:Assert failed:in file
> tessedit.cpp, line 193*
> i even deleted that line from the tessedit.cpp and the error stayed the
> same, after that i tried to delete the whole tessedit.cpp and same problem,
> so is it loaded on somewhere ? or is it not compatible with VMware because
> the revision *"4.00.00alpha-367-g5baa8c8"* works fine with the virtual
> box but not with the VMware
> Thanks
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/873#issuecomment-300411320>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_owNFvS9fAL3rZWahuof6a8Kz4xzHks5r4XQsgaJpZM4NQWtW>
> .
>
 @ibr123 I am able to run training with the following version -

 tesseract -v
tesseract 4.00.00alpha-460-gb86b4fa
 leptonica-1.74.1
  libgif 5.0.5 : libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.0 : libopenjp2 2.1.0

Please try with this commit. If it doesn't work, share your input files/font. ```

=== Phase E: Generating lstmf files ===
Using TESSDATA_PREFIX=../tessdata
[Thu May 11 13:55:50 DST 2017] /usr/local/bin/tesseract /tmp/tmp.D3w1Zt6gOu/ara/ara.Arabic_Typesetting.exp0.tif /tmp/tmp.D3w1Zt6gOu/ara/ara.Arabic_Typesetting.exp0 lstm.
train ../langdata/ara/ara.config
Tesseract Open Source OCR Engine v4.00.00alpha-460-gb86b4fa with Leptonica
Page 1
Page 2
Loaded 56/56 pages (1-56) of document /tmp/tmp.D3w1Zt6gOu/ara/ara.Arabic_Typesetting.exp0.lstmf
Page 3
Loaded 117/117 pages (1-117) of document /tmp/tmp.D3w1Zt6gOu/ara/ara.Arabic_Typesetting.exp0.lstmf

``` When you use

git clone https://github.com/tesseract-ocr/tesseract.git

It will pickup the latest version of source from GitHub. Later, you can
update by

Git pull origin

Or do a new clone of the repository.

If you configure with --enable-debug it will show the git revision for

tesseract -v

On May 11, 2017 3:02 PM, "ibr123" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/Shreeshrii> are the libraries and
> leptonica version at this site
> <https://medium.com/@lucas63/installing-tesseract-3-04-in-ubuntu-14-04-1dae8b748a32>
> are valid? meaning did you install them with the tesseract
> v4.00.00alpha-460-gb86b4fa ?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/873#issuecomment-300736463>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o0n-18Py4b97w1XxTU2QCdM_GoVZks5r4tW3gaJpZM4NQWtW>
> .
>
  Current stable release is 3.05. Soon will (within week) be released 3.05.1 version with more bugfixes....  No man pages  are there for the following programs in https://github.com/tesseract-ocr/tesseract/tree/master/doc 

* classifier_tester
* lstmeval
* lstmtraining
* set_unicharset_properties
* text2image


```
libtool: install: /usr/bin/install -c .libs/ambiguous_words /usr/local/bin/ambiguous_words
libtool: install: /usr/bin/install -c .libs/classifier_tester /usr/local/bin/classifier_tester
libtool: install: /usr/bin/install -c .libs/cntraining /usr/local/bin/cntraining
libtool: install: /usr/bin/install -c .libs/combine_tessdata /usr/local/bin/combine_tessdata
libtool: install: /usr/bin/install -c .libs/dawg2wordlist /usr/local/bin/dawg2wordlist
libtool: install: /usr/bin/install -c .libs/lstmeval /usr/local/bin/lstmeval
libtool: install: /usr/bin/install -c .libs/lstmtraining /usr/local/bin/lstmtraining
libtool: install: /usr/bin/install -c .libs/mftraining /usr/local/bin/mftraining
libtool: install: /usr/bin/install -c .libs/set_unicharset_properties /usr/local/bin/set_unicharset_properties
libtool: install: /usr/bin/install -c .libs/shapeclustering /usr/local/bin/shapeclustering
libtool: install: /usr/bin/install -c .libs/text2image /usr/local/bin/text2image
libtool: install: /usr/bin/install -c .libs/unicharset_extractor /usr/local/bin/unicharset_extractor
libtool: install: /usr/bin/install -c .libs/wordlist2dawg /usr/local/bin/wordlist2dawg
``` ```
USAGE: classifier_tester [.tr files ...]
  --debug_level  Level of Trainer debugging  (type:int default:0)
  --load_images  Load images with tr files  (type:int default:0)
  --clusterconfig_min_samples_fraction  Min number of samples per proto as % of total  (type:double default:0.625)
  --clusterconfig_max_illegal  Max percentage of samples in a cluster which have more than 1 feature in that cluster  (type:double default:0.05)
  --clusterconfig_independence  Desired independence between dimensions  (type:double default:1)
  --clusterconfig_confidence  Desired confidence in prototypes created  (type:double default:1e-06)
  --classifier  Classifier to test  (type:string default:)
  --lang  Language to test  (type:string default:eng)
  --tessdata_dir  Directory of traineddata files  (type:string default:)
  --configfile  File to load more configs from  (type:string default:)
  --D  Directory to write output files to  (type:string default:)
  --F  File listing font properties  (type:string default:font_properties)
  --X  File listing font xheights  (type:string default:)
  --U  File to load unicharset from  (type:string default:unicharset)
  --O  File to write unicharset to  (type:string default:)
  --output_trainer  File to write trainer to  (type:string default:)
  --test_ch  UTF8 test character string  (type:string default:)
```

from https://github.com/tesseract-ocr/tesseract/blob/master/training/classifier_tester.cpp

```

// This program has complex setup requirements, so here is some help:
// Two different modes, tr files and serialized mastertrainer.
// From tr files:
//   classifier_tester -U unicharset -F font_properties -X xheights
//     -classifier x -lang lang [-output_trainer trainer] *.tr
// From a serialized trainer:
//  classifier_tester -input_trainer trainer [-lang lang] -classifier x
//
// In the first case, the unicharset must be the unicharset from within
// the classifier under test, and the font_properties and xheights files must
// match the files used during training.
// In the second case, the trainer file must have been prepared from
// some previous run of shapeclustering, mftraining, or classifier_tester
// using the same conditions as above, ie matching unicharset/font_properties.
//
// Available values of classifier (x above) are:
// pruner   : Tesseract class pruner only.
// full     : Tesseract full classifier.
//            with an input trainer.)
``` ```
USAGE: lstmeval [.tr files ...]
  --max_image_MB  Max memory to use for images.  (type:int default:2000)
  --debug_level  Level of Trainer debugging  (type:int default:0)
  --load_images  Load images with tr files  (type:int default:0)
  --clusterconfig_min_samples_fraction  Min number of samples per proto as % of total  (type:double default:0.625)
  --clusterconfig_max_illegal  Max percentage of samples in a cluster which have more than 1 feature in that cluster  (type:double default:0.05)
  --clusterconfig_independence  Desired independence between dimensions  (type:double default:1)
  --clusterconfig_confidence  Desired confidence in prototypes created  (type:double default:1e-06)
  --model  Name of model file (training or recognition)  (type:string default:)
  --eval_listfile  File listing sample files in lstmf training format.  (type:string default:)
  --configfile  File to load more configs from  (type:string default:)
  --D  Directory to write output files to  (type:string default:)
  --F  File listing font properties  (type:string default:font_properties)
  --X  File listing font xheights  (type:string default:)
  --U  File to load unicharset from  (type:string default:unicharset)
  --O  File to write unicharset to  (type:string default:)
  --output_trainer  File to write trainer to  (type:string default:)
  --test_ch  UTF8 test character string  (type:string default:)
```

USAGE: lstmeval [.tr files ...] 

Should it be .lstmf files? ```
USAGE: lstmtraining [.tr files ...]
  --debug_interval  How often to display the alignment.  (type:int default:0)
  --train_mode  Controls gross training behavior.  (type:int default:80)
  --net_mode  Controls network behavior.  (type:int default:192)
  --perfect_sample_delay  How many imperfect samples between perfect ones.  (type:int default:4)
  --max_image_MB  Max memory to use for images.  (type:int default:6000)
  --append_index  Index in continue_from Network at which to attach the new network defined by net_spec  (type:int default:-1)
  --max_iterations  If set, exit after this many iterations  (type:int default:0)
  --debug_level  Level of Trainer debugging  (type:int default:0)
  --load_images  Load images with tr files  (type:int default:0)
  --target_error_rate  Final error rate in percent.  (type:double default:0.01)
  --weight_range  Range of initial random weights.  (type:double default:0.1)
  --learning_rate  Weight factor for new deltas.  (type:double default:0.0001)
  --momentum  Decay factor for repeating deltas.  (type:double default:0.9)
  --clusterconfig_min_samples_fraction  Min number of samples per proto as % of total  (type:double default:0.625)
  --clusterconfig_max_illegal  Max percentage of samples in a cluster which have more than 1 feature in that cluster  (type:double default:0.05)
  --clusterconfig_independence  Desired independence between dimensions  (type:double default:1)
  --clusterconfig_confidence  Desired confidence in prototypes created  (type:double default:1e-06)
  --stop_training  Just convert the training model to a runtime model.  (type:bool default:false)
  --debug_network  Get info on distribution of weight values  (type:bool default:false)
  --net_spec  Network specification  (type:string default:)
  --continue_from  Existing model to extend  (type:string default:)
  --model_output  Basename for output models  (type:string default:lstmtrain)
  --script_dir  Required to set unicharset properties or use unicharset compression.  (type:string default:)
  --train_listfile  File listing training files in lstmf training format.  (type:string default:)
  --eval_listfile  File listing eval files in lstmf training format.  (type:string default:)
  --configfile  File to load more configs from  (type:string default:)
  --D  Directory to write output files to  (type:string default:)
  --F  File listing font properties  (type:string default:font_properties)
  --X  File listing font xheights  (type:string default:)
  --U  File to load unicharset from  (type:string default:unicharset)
  --O  File to write unicharset to  (type:string default:)
  --output_trainer  File to write trainer to  (type:string default:)
  --test_ch  UTF8 test character string  (type:string default:)
```
USAGE: lstmtraining [.tr files ...]

Should it be .lstmf files? ```

USAGE: set_unicharset_properties
  --debug_level  Level of Trainer debugging  (type:int default:0)
  --load_images  Load images with tr files  (type:int default:0)
  --clusterconfig_min_samples_fraction  Min number of samples per proto as % of total  (type:double default:0.625)
  --clusterconfig_max_illegal  Max percentage of samples in a cluster which have more than 1 feature in that cluster  (type:double default:0.05)
  --clusterconfig_independence  Desired independence between dimensions  (type:double default:1)
  --clusterconfig_confidence  Desired confidence in prototypes created  (type:double default:1e-06)
  --script_dir  Directory name for input script unicharsets/xheights  (type:string default:)
  --configfile  File to load more configs from  (type:string default:)
  --D  Directory to write output files to  (type:string default:)
  --F  File listing font properties  (type:string default:font_properties)
  --X  File listing font xheights  (type:string default:)
  --U  File to load unicharset from  (type:string default:unicharset)
  --O  File to write unicharset to  (type:string default:)
  --output_trainer  File to write trainer to  (type:string default:)
  --test_ch  UTF8 test character string  (type:string default:)
``` ```

USAGE: text2image
  --exposure  Exposure level in photocopier  (type:int default:0)
  --resolution  Pixels per inch  (type:int default:300)
  --xsize  Width of output image  (type:int default:3600)
  --ysize  Height of output image  (type:int default:4800)
  --margin  Margin round edges of image  (type:int default:100)
  --ptsize  Size of printed text  (type:int default:12)
  --leading  Inter-line space (in pixels)  (type:int default:12)
  --box_padding  Padding around produced bounding boxes  (type:int default:0)
  --glyph_resized_size  Each glyph is square with this side length in pixels  (type:int default:0)
  --glyph_num_border_pixels_to_pad  Final_size=glyph_resized_size+2*glyph_num_border_pixels_to_pad  (type:int default:0)
  --tlog_level  Minimum logging level for tlog() output  (type:int default:0)
  --char_spacing  Inter-character space in ems  (type:double default:0)
  --underline_start_prob  Fraction of words to underline (value in [0,1])  (type:double default:0)
  --underline_continuation_prob  Fraction of words to underline (value in [0,1])  (type:double default:0)
  --min_coverage  If find_fonts==true, the minimum coverage the font has of the characters in the text file to include it, between 0 and 1.  (type:double default:1)
  --degrade_image  Degrade rendered image with speckle noise, dilation/erosion and rotation  (type:bool default:true)
  --rotate_image  Rotate the image in a random way.  (type:bool default:true)
  --strip_unrenderable_words  Remove unrenderable words from source text  (type:bool default:true)
  --ligatures  Rebuild and render ligatures  (type:bool default:false)
  --find_fonts  Search for all fonts that can render the text  (type:bool default:false)
  --render_per_font  If find_fonts==true, render each font to its own image. Image filenames are of the form output_name.font_name.tif  (type:bool default:true)
  --list_available_fonts  List available fonts and quit.  (type:bool default:false)
  --render_ngrams  Put each space-separated entity from the input file into one bounding box. The ngrams in the input file will be randomly permuted before rendering (so
 that there is sufficient variety of characters on each line).  (type:bool default:false)
  --output_word_boxes  Output word bounding boxes instead of character boxes. This is used for Cube training, and implied by --render_ngrams.  (type:bool default:false)
  --bidirectional_rotation  Rotate the generated characters both ways.  (type:bool default:false)
  --only_extract_font_properties  Assumes that the input file contains a list of ngrams. Renders each ngram, extracts spacing properties and records them in output_base/
[font_name].fontinfo file.  (type:bool default:false)
  --output_individual_glyph_images  If true also outputs individual character images  (type:bool default:false)
  --text  File name of text input to process  (type:string default:)
  --outputbase  Basename for output image/box file  (type:string default:)
  --writing_mode  Specify one of the following writing modes.
'horizontal' : Render regular horizontal text. (default)
'vertical' : Render vertical text. Glyph orientation is selected by Pango.
'vertical-upright' : Render vertical text. Glyph  orientation is set to be upright.  (type:string default:horizontal)
  --font  Font description name to use  (type:string default:Arial)
  --unicharset_file  File with characters in the unicharset. If --render_ngrams is true and --unicharset_file is specified, ngrams with characters that are not in unicha
rset will be omitted  (type:string default:)
  --fontconfig_tmpdir  Overrides fontconfig default temporary dir  (type:string default:/tmp)
  --fonts_dir  If empty it use system default. Otherwise it overrides system default font location  (type:string default:)
```  https://tesseract-ocr.github.io/index.html 

was Generated on Mon Jul 20 2015 18:38:23 by  doxygen 1.8.8

Please update. Thanks! Documentation of tesseract generated from source code by insight.io can be found at https://www.insight.io/github.com/tesseract-ocr/tesseract

Both of these are linked from 
https://github.com/tesseract-ocr/tesseract/wiki/Documentation If https://www.insight.io/github.com/tesseract-ocr/tesseract
provides the same functionality as Doxygen, then this issue can be closed. Could a travis script such as the following be used to automatically update doxygen documentation?

https://github.com/miloyip/rapidjson/blob/master/travis-doxygen.sh

https://gist.github.com/vidavidorra/548ffbcdae99d752da02

http://blog.gockelhut.com/2014/09/automatic-documentation-publishing-with.html Thanks, @stweil . I have added to documentation page on wiki.

> New documentation is also visible here.

Is that as of master branch on May 20th or relative to some tag? Thanks!  Try without the patch. Maybe that change has already been applied.

On May 2, 2017 8:13 PM, "jsl303" <notifications@github.com> wrote:

> I was able to install head with brew before, but now I get this error. :(
> Could someone help?
> Thanks!
> $ brew install tesseract --HEAD
> ==> Auto-updated Homebrew!
> Updated 1 tap (homebrew/core).
> No changes to formulae.
>
> ==> Using the sandbox
> ==> Cloning https://github.com/tesseract-ocr/tesseract.git
> Cloning into '/Users/user/Library/Caches/Homebrew/tesseract--git'...
> remote: Counting objects: 724, done.
> remote: Compressing objects: 100% (694/694), done.
> remote: Total 724 (delta 88), reused 163 (delta 12), pack-reused 0
> Receiving objects: 100% (724/724), 3.74 MiB | 0 bytes/s, done.
> Resolving deltas: 100% (88/88), done.
> Checking connectivity... done.
> ==> Checking out branch master
> ==> Downloading https://github.com/tesseract-ocr/tesseract/commit/b18cad4.
> patch
> ########################################################################
> 100.0%
> ==> Patching
> ==> Applying b18cad4
> <https://github.com/tesseract-ocr/tesseract/commit/b18cad4cda217c15b0aad333ef7200024d2ecac3>
> .patch
> patching file configure.ac
> Hunk #1 <https://github.com/tesseract-ocr/tesseract/issues/1> FAILED at
> 220.
> 1 out of 1 hunk FAILED -- saving rejects to file configure.ac.rej
> patching file api/Makefile.am
> Hunk #1 <https://github.com/tesseract-ocr/tesseract/issues/1> FAILED at
> 45.
> 1 out of 1 hunk FAILED -- saving rejects to file api/Makefile.am.rej
> Error: Failure while executing: /usr/bin/patch -g 0 -f -p1 -i
> /private/tmp/tesseract--patch-20170502-16295-cpuv7w/b18cad4.patch
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/864>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o3IfozN9lAP88gjs9wh-NvF1aYnUks5r10D5gaJpZM4NOPNX>
> .
>
 see https://github.com/Homebrew/homebrew-core/blob/master/Formula/tesseract.rb @stweil Please see the recent post at https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/H9JmIRU2dQk/oMtSIvtHAwAJ regarding problem with install of training tools on mac os. Thanks!  Ref: 
* https://en.wikipedia.org/wiki/Arabic_numerals
* https://en.wikipedia.org/wiki/Eastern_Arabic_numerals
* https://en.wikipedia.org/wiki/Arabic_script_in_Unicode

> The Arabic numeral glyphs 0–9 are encoded in ASCII and Unicode at positions 0x30 to 0x39, matching up with the second hexadecimal digit for convenience:

> The Eastern Arabic numerals (also called Arabic–Indic numerals and Arabic Eastern numerals) are the symbols used to represent the Hindu–Arabic numeral system, in conjunction with the Arabic alphabet. 

> Each numeral in the Persian variant has a different Unicode point even if it looks identical to the Eastern Arabic numeral counterpart. However the variants used with Urdu, Sindhi, and other South Asian languages are not encoded separately from the Persian variants. 

> See U+0660 through U+0669 and U+06F0 through U+06F9.

So, basically, there are three unicode ranges with numerals used in Arabic, Persian etc.

* 0x30 to 0x39
* U+0660 through U+0669 
* U+06F0 through U+06F9

If the fonts are putting ` Eastern Arabic numerals` U+0660 through U+0669  in the `Arabic numerals` range of 0x30 to 0x39, that would cause confusion during training.

https://github.com/tesseract-ocr/langdata/blob/master/ara/ara.training_text has  'Arabic numerals' range of 0x30 to 0x39. You can check whether it as  ( ٠ ١ ٢ ٣ ٤ ٥ ٦ ٧ ٨ ٩) and add it, if you want to include it for training. @reza1615 

Are these getting recognized in the best traineddata? 
Are they being recognized as Arabic unicode numbers? @theraysmith 
Please update the desired characters for persian for the persian unicode range of numbers and ignore the unicode arabic number range for fas (persian), as mentioned above. Thanks! Question from Ray in tesseract-ocr/langdata#72

> Anyone know which digits are needed for the other Arabic languages?
kur_ara, pus, uig  Any objection to merge this PR? Making all in doc
make[2]: Entering directory `/mnt/c/Users/User/shree/tesseract/doc'
make[2]: *** No rule to make target `cntraining.1', needed by `all-am'.  Stop.
make[2]: Leaving directory `/mnt/c/Users/User/shree/tesseract/doc'
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory `/mnt/c/Users/User/shree/tesseract'
make: *** [all] Error 2
 Making install in doc
make[1]: Entering directory `/mnt/c/Users/User/shree/tesseract/doc'
make[1]: *** No rule to make target `cntraining.1', needed by `all-am'.  Stop.
make[1]: Leaving directory `/mnt/c/Users/User/shree/tesseract/doc'
make: *** [install-recursive] Error 1
 The above errors came while building using the regular automake method with plain ./configure Thanks! I usually follow the following for rebuilding tesseract.

```
git pull origin

./autogen.sh
./configure 
LDFLAGS="-L/usr/local/lib" CFLAGS="-I/usr/local/include" make
sudo make install
sudo ldconfig
make training
sudo make training-install
```  For code style clang-format could be applied.  The newer tesseract code uses the Google C++ style guide:
https://google.github.io/styleguide/cppguide.html
and we have an internal tool that reformats your code for you, hence the
occasional commit with the formatting changes.
Anyway style guide has nothing to do with using the correct name for
Leptonica classes, and Dan says we should be using Pix in C++, not PIX.

On Fri, Apr 28, 2017 at 6:12 AM, Amit D. <notifications@github.com> wrote:

> https://github.com/DanBloomberg/leptonica/blob/master/style-guide.txt
>
> (c) Use typedefs for structs like Pix; e.g.,
> function(PIX *pixs)
> Do not do this (omitting the 'struct' keyword); it is valid C++,
> but not C:
> function(Pix *pixs)
>
> https://github.com/DanBloomberg/leptonica/blob/
> 150c8d005132c85dac0c6388b8408a485f2b65ac/src/bmf.h#L45
>
> Since Tesseract code is based on C++, we should use a C++ style guide.
> https://google.github.io/styleguide/cppguide.html#Type_Names
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/848#issuecomment-297993835>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056YU7BU-qAN6kjGkgqqZgKgfYw9tCks5r0eWwgaJpZM4NJwso>
> .
>



-- 
Ray.
  read_params_file: Can't open lstm.train

You need config file lstm.train

  @bertspaan :-) 

Since you already have an OCR process working, I suggest you wait for Ray to update code for training from scanned images and improve traineddata to support 1/2.

My hacked training is only proof of concept (i trained till about 2% accuracy) so while it recognizes 1/2 as %, other letters may not be as accurate as the traineddata from the repo. Please see the wiki page on training, there have been changes made to LSTM training process. https://github.com/tesseract-ocr/langdata

is the script_dir.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Aug 16, 2017 at 12:39 PM, CoCa520 <notifications@github.com> wrote:

> combine_lang_model which takes as input an input_unicharset and script_dir
> (script_dir points to the langdata directory) and optional word list
> files...
> I have got input_unicharset, but I don't know how can I get script_dir .
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/841#issuecomment-322685646>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_owv9Cyw11QES-BfmZH8KPSm_b_Ijks5sYpWlgaJpZM4NIBJS>
> .
>
 @CoCa520 Also see https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-322685025 tesseract eng.font.exp0.tif eng.font.exp0.box             lstm.train

you need a space after box to give the name of config file.

Best method is to follow the training tutorial. If you want more pages,
change tesstrain_utils.sh for max_page

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Aug 30, 2017 at 2:02 PM, CoCa520 <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> Thank you！
> BUT
> I really can't understand how can I create lstm files.
> Can you show me the code.
>
> I have tried:
> tesseract eng.font.exp0.tif eng.font.exp0.box.lstm.train
> But it gives:
> Error during processing.
> ObjectCache(0x7f098f0849a0)::~ObjectCache(): WARNING! LEAK! object
> 0x29173e0 still has count 1 (id /usr/local/tesseract/share/
> tessdata/eng.traineddatapunc-dawg)
> ObjectCache(0x7f098f0849a0)::~ObjectCache(): WARNING! LEAK! object
> 0x2916420 still has count 1 (id /usr/local/tesseract/share/
> tessdata/eng.traineddataword-dawg)
> ObjectCache(0x7f098f0849a0)::~ObjectCache(): WARNING! LEAK! object
> 0x2916240 still has count 1 (id /usr/local/tesseract/share/tessdata/eng.
> traineddatanumber-dawg)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/841#issuecomment-325921815>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_ozyFOlba1PIkz4zDYhN6YsomGE8Eks5sdR44gaJpZM4NIBJS>
> .
>
 4.0 training with tif/box pairs is not yet supported.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Wed, Aug 30, 2017 at 2:52 PM, CoCa520 <notifications@github.com> wrote:

> Training tutorial ?
> Do you mean https://github.com/tesseract-ocr/tesseract/wiki/
> TrainingTesseract-4.00 <http://url>
> but I just have tif/box pairs, so i come here for more information。
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/841#issuecomment-325934619>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o9zx0GhGvGJcTyC7Cp40PtGBxTKhks5sdSnrgaJpZM4NIBJS>
> .
>
 Others who have done licensed plate recognition may be able to give you
better tips.

For your user case, I think using an older version of tesseract, specially
one which supports the 'digits' config file for limiting output to numbers
may be a better choice than using 4.0alpha.

On 31-Aug-2017 9:27 AM, "CoCa520" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii>
> I want to use tesseract4.00 to recognize models of machines. All model
> information are combines whit characters and numbers and located in
> somewhere of nameplate, so I have collected lots of pictures which contains
> various nameplate of each machine.
> After a series of processing, I have got lots pictures of model as follows:
> [image: image]
> <https://user-images.githubusercontent.com/22894599/29905526-d8207088-8e41-11e7-8a94-60661df186c8.png>
> [image: image]
> <https://user-images.githubusercontent.com/22894599/29905561-ff00a722-8e41-11e7-934d-8e87c61433df.png>
> And then I put all model pictures into tesseract for recognize, but the
> accuracy is not so good, so I am trying to train teaaeract4.00 with model
> pictures.
> The tesseract4.0 training tutorial said that there are two ways to create
> training data, and I use the first option: each line in the box file
> matches a 'character' (glyph) in the tiff image.
>
> If 4.0 training with tif/box pairs is not yet supported then how can I do
> to raise the accuracy?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/841#issuecomment-326182878>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o6HKRmwkXximPpL9HyJPhhWwtyPRks5sdi85gaJpZM4NIBJS>
> .
>
 also see https://github.com/openalpr/openalpr which uses tesseract-ocr

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Aug 31, 2017 at 9:27 AM, CoCa520 <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii>
> I want to use tesseract4.00 to recognize models of machines. All model
> information are combines whit characters and numbers and located in
> somewhere of nameplate, so I have collected lots of pictures which contains
> various nameplate of each machine.
> After a series of processing, I have got lots pictures of model as follows:
> [image: image]
> <https://user-images.githubusercontent.com/22894599/29905526-d8207088-8e41-11e7-8a94-60661df186c8.png>
> [image: image]
> <https://user-images.githubusercontent.com/22894599/29905561-ff00a722-8e41-11e7-934d-8e87c61433df.png>
> And then I put all model pictures into tesseract for recognize, but the
> accuracy is not so good, so I am trying to train teaaeract4.00 with model
> pictures.
> The tesseract4.0 training tutorial said that there are two ways to create
> training data, and I use the first option: each line in the box file
> matches a 'character' (glyph) in the tiff image.
>
> If 4.0 training with tif/box pairs is not yet supported then how can I do
> to raise the accuracy?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/841#issuecomment-326182878>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o6HKRmwkXximPpL9HyJPhhWwtyPRks5sdi85gaJpZM4NIBJS>
> .
>
  There's parallel set of functions for reading TIFF files.

https://github.com/tesseract-ocr/tesseract/blob/master/opencl/openclwrapper.cpp
https://github.com/DanBloomberg/leptonica/blob/master/src/tiffio.c

OpenclDevice::pixReadTiffCl() and friends
pixReadTiff() and friends

That whole section is a little weird for a few reasons.

1) TIFF decode is not very expensive, why bother with hardware assist?

2)  The OpenCL TIFF code has fallen behind Leptonica proper, enough to be currently disabled. (By the way, I'm probably capable of helping it catch up, but not very motivated.)

and most interestingly to me

3) All the actual computation in TIFF decode isn't even done by Leptonica, it is done by libtiff. So the ideal scenario for hardware assist would be something like a "libtiff-turbo" library. (Somewhat similar to how libjepg-turbo has replaced libjpeg in many linux systems). Putting it in Tesseract is really weird, and as a side effect causes a bunch of code duplication.

For all these reasons, if we did decide to prune out some OpenCL code, I think the TIFF portion is the best place to start. I'd roughly guess that includes about a dozen methods.


 This is the slide in question.


![slide](https://cloud.githubusercontent.com/assets/4961958/25460868/a1ea84fa-2a9a-11e7-8782-368554cf81f6.png)

I think this confirms TIFF decode is relatively inexpensive, and is an even smaller overall portion for Tesseract 4.x.

```
# This is a different TIFF, just playing around. 5.7 MB,  1678x2590, LZW TIFF, 3.2Ghz Intel processor
$ time tifftopnm lzw.tif > /dev/null
tifftopnm: writing PPM file

real	0m0.213s
user	0m0.172s
sys	0m0.008s
```

 Great job on this. I suspect next step is to remove the #include tiff.h lines and the build dependency on libtiff.  Ray's comment copied from https://github.com/tesseract-ocr/tesseract/issues/670 (closing that issue now as duplicate)

> unicharset_extractor doesn't read the WordStr box file format.
Sorry this is an un-tested path.
Furthermore, it isn't just a case of modifying unicharset_extractor.
For the Indic languages, the unicharset needs to know the syllable/grapheme clusters, and it can't get that from the Wordstr box file format. The best it can do is extract the unicodes used in the WordStr box file or you start with an existing unicharset for that training path. @amitdo Will your changes - https://github.com/amitdo/tesseract/commit/8fe2d918e447
work now that the new unicharset_extractor has been posted.  We could try to build it with cppan, but I have some libtiff linking issues. One could try to build tess on mac as on windows with cppan guide and see those errors. Please see reply and script used for building at

https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/b2YFzN7MtJg/HkcTYJyKCQAJ

https://mail.google.com/mail/u/0/#inbox/15b9c152e2a586c3?projector=1

> Hi Shree
> 
> Sorry for the delay in replying but I'm struggling to get a successful build now.  I'm attaching my shell script for you to look at but the failure seems to be related to aclocal being called inside autogen.sh.
> 
> To be honest I'm not confident that things are building OK elsewhere as I see a variety of error and warning messages appear even though the relevant script finishes with "success"! I'm not a C/C+ etc coder, I do all my programming using LiveCode.  I'm just trying to get a reliable build of a portable version of Tesseract that I can drive through a command-line interface.  The OCR capability I'm trying to provide using Tesseract is just a part of a much larger app.  LiveCode allows me to build an app on my Mac to be deployed on Mac, Windows & Linux.  In each case the only code I need for each target deployment is a command line or two that runs Tesseract with a given set of source files that my app has extracted/created elsewhere. In order to make installation easy I include a portable version of Tesseract amongst the resources for my app.
> 
> As I'm not a C etc. coder (I last wrote serious C several decades ago!) I'm not able to judge which error/warning messages are significant or figure out how to fix them.  I was hoping to follow a recipe that would reliably build a portable Tesseract for the Mac and Windows.  I'm just trying different combinations of sub-builds until I find one that works, which is why I ended up with a combination of older versions of the dependencies. So I'm not a good person to ask to build this and report errors etc!
> 
> Best regards
> 
> Peter
  >From what I have read, tesseract v4 greatly improves ocr due to LSTM. If I know that my text is going to be of a certain orientation and script (top to bottom and English), how do I take advantage of the newer engine? 

If you want to OCR English text, use the program (latest version built from master branch in github) with default options or specify language as English.

`tesseract ./testing/eurotext.tif eurotext`

or
`tesseract ./testing/eurotext.tif eurotext --oem 1 --psm 6 -l eng`

 tesseract4.0.0 alpha, execute the following command:
<pre>
[root@localhost workspace]# /opt/tesseract4.0/bin/tesseract pic/tesseract-chinese-1.png stdout --psm 0 
Warning. Invalid resolution 0 dpi. Using 70 instead.
Failed loading language 'osd'
Tesseract couldn't load any languages!
Warning: Auto orientation and script detection requested, but osd language failed to load
Estimating resolution as 219
Segmentation fault (core dumped)
</pre>
But exactly, the osd.traineddata is at the right space:
<pre>
[root@localhost workspace]# ls /opt/tesseract4.0/share/tessdata/
chi_sim.traineddata       chi_tra.traineddata       configs/                  ori.traineddata           pdf.ttf                   
chi_sim_vert.traineddata  chi_tra_vert.traineddata  eng.traineddata           osd.traineddata           tessconfigs/ 
</pre>
then I use the "--oem 0" options, it prints the following:
<pre>
[root@localhost workspace]# /opt/tesseract4.0/bin/tesseract pic/tesseract-chinese-1.png stdout --psm 0 --oem 0
Failed loading language 'eng'
Tesseract couldn't load any languages!
Could not initialize tesseract.
</pre>

Then I use tesseract3.05.1, It seems that tesseract always detect the script is "Latin", not what I expected   ​​
Ray has updated the LSTM training process.

Please install the latest code from github.

Please read the wiki again for the new instructions and try.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Thu, Aug 3, 2017 at 1:07 PM, yoyoshuang <notifications@github.com> wrote:

> Hello,@Shreeshrii <https://github.com/shreeshrii>
> I have the same problem like @am0awad <https://github.com/am0awad> .
>
> ` training/tesstrain.sh \
>
> --fonts_dir /Library/Fonts
> --lang chi_sim
> --linedata_only
> --noextract_font_properties
> --exposures "0"
> --langdata_dir ./langdata
> --tessdata_dir ./tessdata
> --output_dir ./tesstutorial/chi_sim `
>
> I have already download the https://github.com/tesseract-ocr/langdata
> <http://url> to my folder, and I have all the folders:
>
> ./langdata
> ./langdata/chi_sim
> ./tessdata
>
> but I still got this:
> `=== Starting training for language 'chi_sim'
> mktemp: illegal option -- -
> usage: mktemp [-d] [-q] [-t prefix] [-u] template ...
> mktemp [-d] [-q] [-u] -t prefix
> training/tesstrain_utils.sh: line 189: /sample_text.txt: Permission denied
> [2017年 8月 3日 星期四 15时23分24秒 CST] /usr/local/bin/text2image
> --fonts_dir=/Library/Fonts --font=AR PL UKai CN Light
> --outputbase=/sample_text.txt --text=/sample_text.txt --fontconfig_tmpdir=
>
> === Phase I: Generating training images ===
> Rendering using Arial Unicode MS
> Rendering using AR PL UKai CN Light
> [2017年 8月 3日 星期四 15时23分25秒 CST] /usr/local/bin/text2image
> --fontconfig_tmpdir= --fonts_dir=/Library/Fonts --strip_unrenderable_words
> --leading=32 --char_spacing=0.0 --exposure=0 --outputbase=/var/folders/br/
> xq_2dfh91y1gdd90lny2501w0000gn/T/tmp.NTXG7RhZ/chi_sim/chi_sim.AR_PL_UKai_CN_Light.exp0
> --font=AR PL UKai CN Light --text=./langdata/chi_sim/chi_sim.training_text
> [2017年 8月 3日 星期四 15时23分25秒 CST] /usr/local/bin/text2image
> --fontconfig_tmpdir= --fonts_dir=/Library/Fonts --strip_unrenderable_words
> --leading=32 --char_spacing=0.0 --exposure=0 --outputbase=/var/folders/br/
> xq_2dfh91y1gdd90lny2501w0000gn/T/tmp.NTXG7RhZ/chi_sim/chi_sim.Arial_Unicode_MS.exp0
> --font=Arial Unicode MS --text=./langdata/chi_sim/chi_sim.training_text
> ERROR: /var/folders/br/xq_2dfh91y1gdd90lny2501w0000gn/T/
> tmp.NTXG7RhZ/chi_sim/chi_sim.Arial_Unicode_MS.exp0.box does not exist or
> is not readable
> ERROR: /var/folders/br/xq_2dfh91y1gdd90lny2501w0000gn/T/
> tmp.NTXG7RhZ/chi_sim/chi_sim.AR_PL_UKai_CN_Light.exp0.box does not exist
> or is not readable
> ERROR: /var/folders/br/xq_2dfh91y1gdd90lny2501w0000gn/T/
> tmp.NTXG7RhZ/chi_sim/chi_sim.AR_PL_UKai_CN_Light.exp0.box does not exist
> or is not readable`
>
> would anyone like to help me?
>
> Thank you very much!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/819#issuecomment-319892740>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o-thKFCnUFwGRYnGQlFCzaFHXksYks5sUXiggaJpZM4M6-yu>
> .
>
 Look at the tif file in question in tmp folder. Looks like first line has
nulls in it.

On 17-Oct-2017 5:59 PM, "hanikh" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> at Phase I, I got this error:
> Rendered page 2 to file /tmp/tmp.GZVqm2mm3D/fas/fas.
> Times_New_Roman.exp0.tif
> Null box at index 0
> Error: Call PrepareToWrite before WriteTesseractBoxFile!!
> but the process did not stop. and it got this error:
> ERROR: /tmp/tmp.GZVqm2mm3D/fas/fas.Times_New_Roman.exp0.lstmf does not
> exist or is not readable
> probably, the problem is in the very first phase. would you please help me
> solve it?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/819#issuecomment-337215631>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o0n0rSz-Z9XhUN2g4ge8DsQXIZi9ks5stJ25gaJpZM4M6-yu>
> .
>
 Does Times New Roman font support Farsi? You should use fonts that have
support for your training_text language.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Oct 17, 2017 at 9:13 PM, hanikh <notifications@github.com> wrote:

> yes. you're right. in fact, there are just numbers and some marks in tif
> file. why is that happening?
>
> On Tue, Oct 17, 2017 at 5:29 PM, Shreeshrii <notifications@github.com>
> wrote:
>
> > Look at the tif file in question in tmp folder. Looks like first line has
> > nulls in it.
> >
> > On 17-Oct-2017 5:59 PM, "hanikh" <notifications@github.com> wrote:
> >
> > > @Shreeshrii <https://github.com/shreeshrii> at Phase I, I got this
> > error:
> > > Rendered page 2 to file /tmp/tmp.GZVqm2mm3D/fas/fas.
> > > Times_New_Roman.exp0.tif
> > > Null box at index 0
> > > Error: Call PrepareToWrite before WriteTesseractBoxFile!!
> > > but the process did not stop. and it got this error:
> > > ERROR: /tmp/tmp.GZVqm2mm3D/fas/fas.Times_New_Roman.exp0.lstmf does not
> > > exist or is not readable
> > > probably, the problem is in the very first phase. would you please help
> > me
> > > solve it?
> > >
> > > —
> > > You are receiving this because you were mentioned.
> > > Reply to this email directly, view it on GitHub
> > > <https://github.com/tesseract-ocr/tesseract/issues/819#
> > issuecomment-337215631>,
> > > or mute the thread
> > > <https://github.com/notifications/unsubscribe-auth/AE2_o0n0rSz-
> > Z9XhUN2g4ge8DsQXIZi9ks5stJ25gaJpZM4M6-yu>
> > > .
> > >
> >
> > —
> > You are receiving this because you commented.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/819#
> issuecomment-337240137>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/
> AZFiASKi4zAcvT6hL8SkvIybrnjQVMWrks5stLLOgaJpZM4M6-yu>
> > .
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/819#issuecomment-337269936>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5yHcKCu2plw2oV9eKheXyc1opAYks5stMsagaJpZM4M6-yu>
> .
>
  @amitdo

since 4.0 version is alpha, --enable-debug is useful to track errors. Added page to wiki - https://github.com/tesseract-ocr/tesseract/wiki/4.0-Dockerfile See https://github.com/tesseract-ocr/tesseract/issues/919#issuecomment-302300156

>Definitions of Docker containers and scripts that help to compile and run Tesseract 4 are available at:

https://github.com/tesseract-shadow/tesseract-ocr-compilation @zdenop This issue can be closed.  Sorry, did not realize the dockerfile was in tesseract repo. It seems to be related to travis build. Is it being used?


  1. Current stable version is 3.05
2. Banner text output is to stderr because from 3.04 there is possible to output OCR result to stdin and banner text should not be part of  OCR output (stdout)  @stweil 

Please see https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/P-4AqnnGpgc/fMbEr1o9BAAJ

Do you know what could be causing this behaviour for install on windows 10? Thanks!

>Thank you for the reply. I went to https://github.com/UB-Mannheim/tesseract/wiki, download  tesseract-ocr-setup.exe file but when i click on it, it send me to the get apps from the store app. I don't know why it did not install and why it sent me to app store for since I already got the  tesseract-ocr-setup.exe. 
Regards,
Tom  Please use tesseract user forum for asking questions.  Please use tesseract user forum for asking question.  Please use tesseract user forum for asking support  One should open a ticket with build (and other) errors provided and we'll try to help with that. Several people are doing the same things.
Look, we have manual deps management from peirick, vdevan, maybe others.
This just is not worth it.  Why you put this on in the issue tracker? @lvc : Thanks, but there is place for this: [tesseract developer forum.](https://groups.google.com/forum/#!forum/tesseract-dev)
@amitdo : Broken ABI is know issue ;-) Even API is not stable. See dev forum. @lvc If possible, instead of current version, add 'master' and '3.05' branch. Thanks! Thanks.

Actually, 3.05 and master 4.0 are going to be two parralel tracks for a
while.

Zdenko is going to tag 3.05.01 release soon, so that should be compared
with 3.05.0 for changes. Any changes after that on 3.05 branch should
follow as current in that sequence.

Similarly, 4.0.0alpha may be followed by a beta or release, plus changes in
master branch.

If not possible to do in same table, you can make a separate one for 4.0.

Thanks.

On May 8, 2017 4:43 PM, "ABI Laboratory" <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/Shreeshrii>,
>
> Done: https://abi-laboratory.pro/tracker/timeline/tesseract/
>
> Please let me know when it should be switched to 3.06 or 4.0.
>
> [image: tesseract-5]
> <https://cloud.githubusercontent.com/assets/1517837/25801811/724dac08-3400-11e7-9af3-ff379784aeaf.png>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/793#issuecomment-299839596>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o8RhIVFBMs9UWv8yY7aXzTrDJZwmks5r3vjigaJpZM4MrL5o>
> .
>
  This is still happening with the latest code - the eval file was built using a different training text.

**git log -1
commit d18931e86ed9b8fc40299d5494b53901310b4867
Date:   Fri May 5 16:42:44 2017 -0700
    Fixed int types for imported tf networks**

```
  lstmtraining \
>   -U ~/tesstutorial/bihnew/bih.unicharset \
>   --train_listfile ~/tesstutorial/bihnew/bih.training_files.txt \
>   --eval_listfile ~/tesstutorial/bihtest/bih.training_files.txt \
>   --continue_from ~/tesstutorial/bihnewlayer/bih.lstm \
>   --model_output ~/tesstutorial/bihnewlayer/bihlayer \
>   --script_dir ../langdata \
>  --append_index 5 \
>  --net_spec '[Lfx384 O1c105]' \
>  --target_error_rate 0.01 \
>  --debug_interval -1
Loaded file /home/shree/tesstutorial/bihnewlayer/bih.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tesstutorial/bihnewlayer/bih.lstm
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Setting properties for script Devanagari
Setting properties for script Han
Warning: given outputs 105 not equal to unicharset of 145.
Num outputs,weights in serial:
  Lfx384:384, 787968
  Fc145:145, 55825
Total weights = 843793
Built network:[1,0,0,1[C5,5Ft16]Mp3,3Lfys64Lfx128Lrx128Lfx384Fc145] from request [Lfx384 O1c105]
Training parameters:
  Debug interval = -1, weights = 0.1, learning rate = 0.0001, momentum=0.9
Loaded 404/404 pages (1-404) of document /home/shree/tesstutorial/bihnew/bih.AA_NAGARI_SHREE_L3.exp0.lstmf
Loaded 199/199 pages (1-199) of document /home/shree/tesstutorial/bihtest/bih.Chandas.exp0.lstmf
Loaded 404/404 pages (1-404) of document /home/shree/tesstutorial/bihnew/bih.Adobe_Devanagari.exp0.lstmf
Loaded 404/404 pages (1-404) of document /home/shree/tesstutorial/bihnew/bih.CDAC-GISTSurekh.exp0.lstmf
Loaded 404/404 pages (1-404) of document /home/shree/tesstutorial/bihnew/bih.Aksharyogini2.exp0.lstmf
Loaded 411/411 pages (1-411) of document /home/shree/tesstutorial/bihnew/bih.CDAC-GISTYogesh.exp0.lstmf
Loaded 405/405 pages (1-405) of document /home/shree/tesstutorial/bihnew/bih.Annapurna_SIL.exp0.lstmf
Loaded 404/404 pages (1-404) of document /home/shree/tesstutorial/bihnew/bih.Arial_Unicode_MS.exp0.lstmf
Loaded 404/404 pages (1-404) of document /home/shree/tesstutorial/bihnew/bih.Aparajita.exp0.lstmf
Loaded 472/472 pages (1-472) of document /home/shree/tesstutorial/bihnew/bih.Chandas.exp0.lstmf
Loaded 202/202 pages (1-202) of document /home/shree/tesstutorial/bihtest/bih.FreeSerif.exp0.lstmf

At iteration 100/100/100, Mean rms=5.4%, delta=52.955%, char train=110.663%, word train=100%, skip ratio=0%,  New worst char error = 110.663 wrote checkpoint.
At iteration 200/200/200, Mean rms=5.254%, delta=50.469%, char train=105.319%, word train=100%, skip ratio=0%,  New worst char error = 105.319 wrote checkpoint.
At iteration 300/300/300, Mean rms=5.209%, delta=49.853%, char train=103.546%, word train=100%, skip ratio=0%,  New worst char error = 103.546 wrote checkpoint.
At iteration 400/400/400, Mean rms=5.189%, delta=49.729%, char train=102.636%, word train=100%, skip ratio=0%,  New worst char error = 102.636 wrote checkpoint.
At iteration 500/500/500, Mean rms=5.147%, delta=48.821%, char train=102.064%, word train=100%, skip ratio=0%,  New worst char error = 102.064 wrote checkpoint.
At iteration 600/600/600, Mean rms=5.127%, delta=48.496%, char train=101.601%, word train=99.991%, skip ratio=0%,  New worst char error = 101.601 wrote checkpoint.
At iteration 700/700/700, Mean rms=5.107%, delta=48.218%, char train=101.236%, word train=99.992%, skip ratio=0%,  New worst char error = 101.236 wrote checkpoint.
At iteration 800/800/800, Mean rms=5.081%, delta=47.738%, char train=100.547%, word train=99.986%, skip ratio=0%,  New worst char error = 100.547 wrote checkpoint.
At iteration 899/900/900, Mean rms=5.06%, delta=47.379%, char train=100.071%, word train=99.983%, skip ratio=0%,  New worst char error = 100.071 wrote checkpoint.
At iteration 999/1000/1000, Mean rms=5.038%, delta=46.991%, char train=99.425%, word train=99.985%, skip ratio=0%,  New best char error = 99.425 wrote checkpoint.
At iteration 1098/1100/1100, Mean rms=4.971%, delta=45.696%, char train=97.6%, word train=99.979%, skip ratio=0%,  New best char error = 97.6Deserialize failed wrote che
At iteration 1197/1200/1200, Mean rms=4.932%, delta=44.848%, char train=96.57%, word train=99.979%, skip ratio=0%,  New best char error = 96.57Deserialize failed wrote c
At iteration 1296/1300/1300, Mean rms=4.7%, delta=40.982%, char train=90.782%, word train=98.025%, skip ratio=0%,  New best char error = 90.782Deserialize failed wrote c
At iteration 1392/1400/1400, Mean rms=4.388%, delta=36.654%, char train=83.142%, word train=93.749%, skip ratio=0%,  New best char error = 83.142Deserialize failed wrote
At iteration 1491/1500/1500, Mean rms=4.053%, delta=32.455%, char train=74.886%, word train=88.3%, skip ratio=0%,  New best char error = 74.886Deserialize failed wrote c
At iteration 1582/1600/1600, Mean rms=3.71%, delta=28.11%, char train=66.546%, word train=82.7%, skip ratio=0%,  New best char error = 66.546Deserialize failed wrote bes
At iteration 1678/1700/1700, Mean rms=3.366%, delta=23.795%, char train=58.181%, word train=76.531%, skip ratio=0%,  New best char error = 58.181Deserialize failed wrote
At iteration 1772/1800/1800, Mean rms=3.018%, delta=19.642%, char train=49.915%, word train=70.119%, skip ratio=0%,  New best char error = 49.915Deserialize failed wrote
At iteration 1862/1900/1900, Mean rms=2.67%, delta=15.494%, char train=41.584%, word train=63.451%, skip ratio=0%,  New best char error = 41.584Deserialize failed wrote
At iteration 1955/2000/2000, Mean rms=2.321%, delta=11.413%, char train=33.335%, word train=56.515%, skip ratio=0%,  New best char error = 33.335Deserialize failed wrote
At iteration 2049/2100/2101, Mean rms=1.978%, delta=7.632%, char train=25.213%, word train=49.514%, skip ratio=0.1%,  New best char error = 25.213Deserialize failed wrot
At iteration 2137/2200/2201, Mean rms=1.631%, delta=3.903%, char train=17.289%, word train=42.271%, skip ratio=0.1%,  New best char error = 17.289Deserialize failed wrot
At iteration 2230/2300/2301, Mean rms=1.474%, delta=3.148%, char train=14.004%, word train=36.724%, skip ratio=0.1%,  New best char error = 14.004Deserialize failed wrot
``` @stweil Is it related to https://github.com/tesseract-ocr/tesseract/issues/881#issuecomment-299393920 ? @stweil In case you want to reproduce using the files I was using - they are for Bihari/Hindi language, devanagari script. 

http://sanskritdocuments.org/hindi/bihtest.zip
http://sanskritdocuments.org/hindi/bihnew.zip

Zip file was too large to upload here or on my github account. ```
mkdir -p ~/tesstutorial/bihnewlayer

combine_tessdata -e ../tessdata/hin.traineddata \
   ~/tesstutorial/bihnewlayer/bih.lstm
```

I was using the Hindi traineddata from the tessdata repo as the basis for training.  > ~/tesstutorial/bihnewlayer is needed, too.

http://sanskritdocuments.org/hindi/bihnewlayer.zip

it has:

* bih.lstm (lstm model extracted from hin.traineddata)
* bihlayer_checkpoint (current status of the model)
* bihlayer4.005_8369.lstm (latest 'best' model)

I have not included all other intermediate .lstm files, since each is 33+ MB. Thanks for checking, @stweil. I will rebuild with the latest git master and test again as I am still getting `Deserialize Failed` messages, though not for every checkpoint iteration of training. 

Maybe, these are also just 'info' messages!

At iteration 8306/10800/10808, Mean rms=0.765%, delta=1.311%, char train=4.216%, word train=10.221%, skip ratio=0.2%,  New best char error = 4.216 wrote best model:/ho
**At iteration 8369/10900/10908, Mean rms=0.753%, delta=1.217%, char train=4.005%, word train=10.036%, skip ratio=0.2%,  New best char error = 4.005Deserialize failed wr
At iteration 8426/11000/11008, Mean rms=0.752%, delta=1.211%, char train=4.014%, word train=9.962%, skip ratio=0.2%,  New worst char error = 4.014Deserialize failed w**r
At iteration 8482/11100/11109, Mean rms=0.768%, delta=1.319%, char train=4.281%, word train=10.389%, skip ratio=0.3%,  New worst char error = 4.281 wrote checkpoint.
At iteration 8548/11200/11210, Mean rms=0.773%, delta=1.355%, char train=4.347%, word train=10.64%, skip ratio=0.3%,  New worst char error = 4.347 wrote checkpoint.
At iteration 8607/11300/11310, Mean rms=0.772%, delta=1.37%, char train=4.4%, word train=10.701%, skip ratio=0.3%,  New worst char error = 4.4 wrote checkpoint.
At iteration 8666/11400/11410, Mean rms=0.777%, delta=1.359%, char train=4.394%, word train=10.81%, skip ratio=0.3%,  New worst char error = 4.394 wrote checkpoint.
At iteration 8729/11500/11510, Mean rms=0.772%, delta=1.368%, char train=4.385%, word train=10.801%, skip ratio=0.2%,  New worst char error = 4.385 wrote checkpoint.
At iteration 8794/11600/11610, Mean rms=0.772%, delta=1.343%, char train=4.354%, word train=10.848%, skip ratio=0.2%,  New worst char error = 4.354 wrote checkpoint.
At iteration 8857/11700/11710, Mean rms=0.786%, delta=1.415%, char train=4.538%, word train=11.094%, skip ratio=0.2%,  New worst char error = 4.538 wrote checkpoint.
At iteration 8919/11800/11810, Mean rms=0.802%, delta=1.531%, char train=4.968%, word train=11.586%, skip ratio=0.2%,  New worst char error = 4.968 wrote checkpoint.
At iteration 8979/11900/11910, Mean rms=0.8%, delta=1.548%, char train=5.043%, word train=11.359%, skip ratio=0.2%,  New worst char error = 5.043 wrote checkpoint.
At iteration 9034/12000/12010, Mean rms=0.79%, delta=1.526%, char train=4.924%, word train=11.199%, skip ratio=0.2%,  New worst char error = 4.924 wrote checkpoint.
At iteration 9101/12100/12110, Mean rms=0.781%, delta=1.449%, char train=4.766%, word train=11.043%, skip ratio=0.1%,  New worst char error = 4.766 wrote checkpoint.
At iteration 9160/12200/12210, Mean rms=0.784%, delta=1.443%, char train=4.801%, word train=11.025%, skip ratio=0%,  New worst char error = 4.801 wrote checkpoint.
At iteration 9224/12300/12310, Mean rms=0.784%, delta=1.444%, char train=4.728%, word train=10.783%, skip ratio=0%,  New worst char error = 4.728 wrote checkpoint.
At iteration 9285/12400/12410, Mean rms=0.782%, delta=1.477%, char train=4.773%, word train=10.693%, skip ratio=0%,  New worst char error = 4.773 wrote checkpoint.
At iteration 9349/12500/12511, Mean rms=0.776%, delta=1.419%, char train=4.545%, word train=10.359%, skip ratio=0.1%,  New worst char error = 4.545 wrote checkpoint.
At iteration 9405/12600/12611, Mean rms=0.769%, delta=1.421%, char train=4.498%, word train=10.172%, skip ratio=0.1%,  New worst char error = 4.498 wrote checkpoint.
At iteration 9461/12700/12711, Mean rms=0.749%, delta=1.324%, char train=4.276%, word train=9.825%, skip ratio=0.1%,  wrote checkpoint.
**At iteration 9518/12800/12811, Mean rms=0.737%, delta=1.229%, char train=3.922%, word train=9.583%, skip ratio=0.1%,  New best char error = 3.922Previous test incomple
At iteration 9576/12900/12911, Mean rms=0.733%, delta=1.198%, char train=3.746%, word train=9.473%, skip ratio=0.1%,  New best char error = 3.746Previous test incomple
At iteration 9641/13000/13011, Mean rms=0.743%, delta=1.261%, char train=4.006%, word train=9.773%, skip ratio=0.1%,  New worst char error = 4.006Previous test incompl
At iteration 9708/13100/13111, Mean rms=0.742%, delta=1.255%, char train=3.899%, word train=9.567%, skip ratio=0.1%,  New worst char error = 3.899Previous test incompl
At iteration 9766/13200/13211, Mean rms=0.727%, delta=1.193%, char train=3.708%, word train=9.23%, skip ratio=0.1%,  New best char error = 3.708Previous test incomplet
At iteration 9822/13300/13311, Mean rms=0.727%, delta=1.191%, char train=3.717%, word train=9.477%, skip ratio=0.1%,  New worst char error = 3.717Previous test incompl**
At iteration 9880/13400/13411, Mean rms=0.723%, delta=1.151%, char train=3.603%, word train=9.355%, skip ratio=0.1%,  New best char error = 3.603At iteration 8369, sta
ge 1, Eval Char error rate=2.0568204, Word error rate=4.3746914 wrote best model:/home/shree/tesstutorial/bihnewlayer/bihlayer3.603_9880.lstm wrote checkpoint.
**At iteration 9934/13500/13511, Mean rms=0.725%, delta=1.203%, char train=3.786%, word train=9.603%, skip ratio=0%,  New worst char error = 3.786Deserialize failed wrot**
At iteration 9988/13600/13611, Mean rms=0.733%, delta=1.237%, char train=4.018%, word train=9.875%, skip ratio=0%,  New worst char error = 4.018 wrote checkpoint.
At iteration 10036/13700/13711, Mean rms=0.737%, delta=1.256%, char train=4.052%, word train=10.005%, skip ratio=0%,  New worst char error = 4.052 wrote checkpoint.
 >I had to fix the path in ~/tesstutorial/bihtest/bih.training_files.txt,

Did you need to change path in both to match your setup?

>   --train_listfile ~/tesstutorial/bihnew/bih.training_files.txt \
>   --eval_listfile ~/tesstutorial/bihtest/bih.training_files.txt \

or did you change path in bihtest to match bihnew?

I think the problem occurs when the training files and evaluation files are different. The lstmf files in bihnew and bihtest were created using different training texts and font combos.  Thanks, @stweil . However, I can now reproduce the error that you got when using lstmf files from a different version.

@theraysmith I am getting these errors when I use lstmf files created before the commits regarding endianness. However, the location of error is different from the rest reported above in this thread.

```
 lstmtraining  \
>    -U ~/tesstutorial/nyd/eng.unicharset \
>   --train_listfile ~/tesstutorial/nyd/nyd.training_files.txt \
>   --script_dir ../langdata   \
>   --append_index 5 --net_spec '[Lfx256 O1c105]' \
>   --continue_from ~/tesstutorial/nydlayer/eng.lstm \
>   --model_output ~/tesstutorial/nydlayer/nyd \
>   --debug_interval -1 \
>   --target_error_rate 0.01
Loaded file /home/shree/tesstutorial/nydlayer/eng.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tesstutorial/nydlayer/eng.lstm
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Warning: given outputs 105 not equal to unicharset of 75.
Num outputs,weights in serial:
  Lfx256:256, 394240
  Fc75:75, 19275
Total weights = 413515
Built network:[1,0,0,1[C5,5Ft16]Mp3,3Lfys64Lfx128Lrx128Lfx256Fc75] from request [Lfx256 O1c105]
Training parameters:
  Debug interval = -1, weights = 0.1, learning rate = 0.0001, momentum=0.9
```
**Deserialize failed: /home/shree/tesstutorial/nyd/eng.1852nydir.exp1.lstmf read 0/8 pages**
 @stweil @theraysmith 

I tried running using the latest code with enable-debug option - earlier the error was 'deserialize failed' - please see https://github.com/tesseract-ocr/tesseract/issues/792#issuecomment-299809206

With --enable-debug, I get core dumped (same as https://github.com/tesseract-ocr/tesseract/issues/561)

```
Iteration 13398: ALIGNED TRUTH : पंचाग कला की, फ़ अः ग़ुमान आलोचना छूटती के ज़् द्वा अधीन र्द् देहियाँ भजनला
Iteration 13398: BEST OCR TEXT : पंचाग कला की, फ़ अः गुमान आलोचना छूटती के ज़् द्वा अधीन र्द् देहियाँ भजनला
File /tmp/tmp.3zBjAvGc9O/bih/bih.Lohit_Devanagari.exp0.lstmf page 24 :
Mean rms=0.723%, delta=1.151%, train=3.605%(9.362%), skip ratio=0.1%
Iteration 13399: ALIGNED TRUTH : संभावना :
Iteration 13399: BEST OCR TEXT : संभावना :
File /tmp/tmp.3zBjAvGc9O/bih/bih.Mangal.exp0.lstmf page 458 (Perfect):
Mean rms=0.723%, delta=1.151%, train=3.603%(9.355%), skip ratio=0.1%
lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
```
gdb output pasted below:
```
Iteration 13399: ALIGNED TRUTH : संभावना :
Iteration 13399: BEST OCR TEXT : संभावना :
File /tmp/tmp.3zBjAvGc9O/bih/bih.Mangal.exp0.lstmf page 458 (Perfect):
Mean rms=0.723%, delta=1.151%, train=3.603%(9.355%), skip ratio=0.1%
[Thread 0x7fe0cd3e0700 (LWP 135) exited]
[New Thread 0x7fe0cd3e0700 (LWP 136)]
lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.

Program received signal SIGABRT, Aborted.
[Switching to Thread 0x7fe0cd3e0700 (LWP 136)]
0x00007fe0d2886c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace
#0  0x00007fe0d2886c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007fe0d288a028 in __GI_abort () at abort.c:89
#2  0x00007fe0d287fbf6 in __assert_fail_base (fmt=0x7fe0d29d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7fe0d370d6c0 "index >= 0 && index < size_used_", file=file@entry=0x7fe0d370d148 "../ccutil/genericvector.h", line=line@entry=697,
    function=function@entry=0x7fe0d372ea60 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
    at assert.c:92
#3  0x00007fe0d287fca2 in __GI___assert_fail (assertion=0x7fe0d370d6c0 "index >= 0 && index < size_used_", file=0x7fe0d370d148 "../ccutil/genericvector.h", line=697,
    function=0x7fe0d372ea60 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
#4  0x00007fe0d368c803 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0) at ../ccutil/genericvector.h:697
#5  0x00007fe0d368cfb8 in operator[] (this=0x7fffcf2d0bc0, this=0x7fffcf2d0bc0, index=0) at lstmtrainer.cpp:920
#6  tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7fe0cd3df640, data=..., trainer=trainer@entry=0x7fe0cd3df640) at lstmtrainer.cpp:921
#7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffcf2d0b50, iteration=9766, training_errors=<optimized out>, model_data=...,
    training_stage=1) at lstmtester.cpp:86
#8  0x000000000040b539 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffcf2d0b50) at lstmtester.cpp:123
#9  0x00007fe0d0748184 in start_thread (arg=0x7fe0cd3e0700) at pthread_create.c:312
#10 0x00007fe0d294a37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) up
#1  0x00007fe0d288a028 in __GI_abort () at abort.c:89
89      abort.c: No such file or directory.
(gdb) up
#2  0x00007fe0d287fbf6 in __assert_fail_base (fmt=0x7fe0d29d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7fe0d370d6c0 "index >= 0 && index < size_used_", file=file@entry=0x7fe0d370d148 "../ccutil/genericvector.h", line=line@entry=697,
    function=function@entry=0x7fe0d372ea60 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
    at assert.c:92

...

(gdb) up
#7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffcf2d0b50, iteration=9766, training_errors=<optimized out>, model_data=...,
    training_stage=1) at lstmtester.cpp:86
86        if (!trainer.ReadTrainingDump(model_data, &trainer)) {
(gdb) print model_data
$10 = (const GenericVector<char> &) @0x7fffcf2d0bc0: {static kDefaultVectorSize = <optimized out>, size_used_ = 0, size_reserved_ = 0, data_ = 0x0, clear_cb_ = 0x0,
  compare_cb_ = 0x0}
```

@stweil was not able to reproduce this - https://github.com/tesseract-ocr/tesseract/issues/792#issuecomment-299780420

The only difference I can see would be that I run the program under WSL (bash on windows 10).
 Duplicate/Related  https://github.com/tesseract-ocr/tesseract/issues/644 Still getting the error:

```
At iteration 800/800/802, Mean rms=0.934%, delta=47.138%, char train=100.525%, word train=100%, skip ratio=0.25%,  New worst char error = 100.525 wrote checkpoint.
Compute CTC targets failed!
At iteration 900/900/903, Mean rms=0.932%, delta=46.952%, char train=100.461%, word train=99.972%, skip ratio=0.333%,  New worst char error = 100.461 wrote checkpoint.
Warning: data size is zero in LSTMTrainer::ReadTrainingDump
At iteration 1000/1000/1003, Mean rms=0.93%, delta=46.74%, char train=100.415%, word train=99.975%, skip ratio=0.3%,  New worst char error = 100.415 wrote checkpoint.
Warning: LSTMTrainer deserialized an LSTMRecognizer!
2 Percent improvement time=1098, best error was 100 @ 0
Compute CTC targets failed!
Compute CTC targets failed!
At iteration 1098/1100/1103, Mean rms=0.917%, delta=45.445%, char train=99.984%, word train=99.975%, skip ratio=0.3%,  New best char error = 99.984

Deserialize failed wrote checkpoint.
```

ref: https://travis-ci.org/Shreeshrii/tess4train/builds/252343478 Closing Issue since LSTM training process has changed and so it is difficult to duplicate the issue.  This not correct PR  What source are you using? "config" and "m4" is not part of [tesseract distribution.](https://github.com/tesseract-ocr/tesseract)  make a PR please. Hi,

I've activated macos builds (clang).
See
https://travis-ci.org/tesseract-ocr/tesseract/builds/215040103
https://travis-ci.org/tesseract-ocr/tesseract/builds  So far the new script id looks good.
I don't understand why you would ever want to run OSD on license plates? It makes no sense.  First of all: provide details (how you build tesseract, configuration output, libraries version etc.) I am not familiar with building on Mac, but can you please try to install tesseract "by hand" (without homebrew)? 

For linking on macos10 ["-framework OpenCL"](https://github.com/tesseract-ocr/tesseract/blob/3.05/configure.ac#L223) should be used. Your [configure log](https://github.com/RandomDSdevel/Homebrew-Installation-Logs/blob/master/Tesseract/v3.05.00/Third%20Attempt/02.configure) showed it was detected correctly, but I do not see it in your [make log](https://github.com/RandomDSdevel/Homebrew-Installation-Logs/blob/master/Tesseract/v3.05.00/Third%20Attempt/03.make). `configure` uses `pkg-config` to detect `cairo`. It can be overridden by setting `cairo_CFLAGS` and `cairo_LIBS`, but fixing the installation would be better of course. First of all - it is a warning, not an error ;-)
Next: it is not related to originally reported issue (opencl build), so please ignore it. @jbarlow83: Can you have a look on this issue? It seems to me problem with linking (OpenCL build on OS X) I am not OS X user, so maybe I am wrong. But from logs it seems that:

- configure detect OpenCL framework
- make output show there is not linking to against OpenCL framework

  Do you get the same error messages when using text output?

```gm.exe convert -density 300 pdfin.pdf tif:- | tesseract.exe txtout -l deu``` The error messages are harmless, but annoying. This change will get rid off half of them. I'll make a change in Leptonica to get rid of the other half. Thanks for the report. This change is good to go on both branches.

```c++
--- api/pdfrenderer.cpp	2017-01-20 14:57:59.000000000 -0800
+++ api/pdfrenderer.cpp	2017-03-23 08:51:31.000000000 -0700
@@ -709,9 +709,8 @@
   L_COMP_DATA *cid = NULL;
   const int kJpegQuality = 85;
 
-  int format, sad;
-  findFileFormat(filename, &format);
-  if (pixGetSpp(pix) == 4 && format == IFF_PNG) {
+  int sad;
+  if (pixGetSpp(pix) == 4 && pixGetInputFormat(pix) == IFF_PNG) {
     Pix *p1 = pixAlphaBlendUniform(pix, 0xffffff00);
     sad = pixGenerateCIData(p1, L_FLATE_ENCODE, 0, 0, &cid);
     pixDestroy(&p1);
``` Leptonica change will look something like this. These changes  are both forward and backward compatible, no need to adjust version dependencies.

```c++
--- leptonica/src/pdfio2.c	2017-03-09 08:41:53.000000000 -0800
+++leptonica/src/pdfio2.c	2017-03-23 09:32:21.000000000 -0700
@@ -496,7 +496,7 @@
 /*!
  * \brief   l_generateCIDataForPdf()
  *
- * \param[in]    fname
+ * \param[in]    fname [optional]; can be null
  * \param[in]    pix [optional]; can be null
  * \param[in]    quality for jpeg if transcoded; 75 is standard
  * \param[out]   pcid compressed data
@@ -504,11 +504,15 @@
  *
  * <pre>
  * Notes:
+ *      (0) You must set either filename or pix.
  *      (1) Given an image file and optionally a pix raster of that data,
  *          this provides a CID that is compatible with PDF, preferably
  *          without transcoding.
  *      (2) The pix is included for efficiency, in case transcoding
  *          is required and the pix is available to the caller.
+ *      (3) We don't try to open files named "stdin" or "-" for Tesseract
+ *          compatibility reasons. We may remove this restriction
+ *          in the future.
  * </pre>
  */
 l_int32
@@ -526,24 +530,29 @@
     if (!pcid)
         return ERROR_INT("&cid not defined", procName, 1);
     *pcid = NULL;
-    if (!fname)
-        return ERROR_INT("fname not defined", procName, 1);
+    if (!fname && !pix)
+        return ERROR_INT("fname and pix not defined", procName, 1);
 
-    findFileFormat(fname, &format);
-    if (format == IFF_UNKNOWN)
-        L_WARNING("file %s format is unknown\n", procName, fname);
-    if (format == IFF_PS || format == IFF_LPDF) {
-        L_ERROR("file %s is unsupported format %d\n", procName, fname, format);
-        return 1;
+    if (fname && strcmp(fname, "-") != 0 && strcmp(fname, "stdin") != 0) {
+        findFileFormat(fname, &format);
+        if (format == IFF_UNKNOWN)
+          L_WARNING("file %s format is unknown\n", procName, fname);
+        if (format == IFF_PS || format == IFF_LPDF) {
+          L_ERROR("file %s is unsupported format %d\n",
+                  procName, fname, format);
+          return 1;
+        }
+        if (format == IFF_JFIF_JPEG) {
+          cid = l_generateJpegData(fname, 0);
+        } else if (format == IFF_JP2) {
+          cid = l_generateJp2kData(fname);
+        } else if (format == IFF_PNG) {
+          cid = l_generateFlateDataPdf(fname, pix);
+        }
+
     }
 
-    if (format == IFF_JFIF_JPEG) {
-        cid = l_generateJpegData(fname, 0);
-    } else if (format == IFF_JP2) {
-        cid = l_generateJp2kData(fname);
-    } else if (format == IFF_PNG) {  /* use Jeff's special function for png */
-        cid = l_generateFlateDataPdf(fname, pix);
-    } else {  /* any other format ... */
+    if  (!cid) {
         if (!pix)
             pixt = pixRead(fname);
         else
@@ -555,7 +564,7 @@
         pixDestroy(&pixt);
     }
     if (!cid) {
-        L_ERROR("file %s format is %d; unreadable\n", procName, fname, format);
+        L_ERROR("totally kerflummoxed\n", procName);
         return 1;
     }
     *pcid = cid;
``` I didn't realize this until now, but sending image data via stdin will cause image recompression and potentially generational loss. What happens exactly will depend on the file format, but just wanted to raise awareness.

`cat foo.jpg | tesseract - - pdf`

Whereas using filename will not.

`tesseract foo.jpg - pdf`

`echo foo.jpg | tesseract - - pdf`

 > +    if (fname && strcmp(fname, "-") != 0 && strcmp(fname, "stdin") != 0) {

I'd not add filename handling to Leptonica, but do that on the caller side. The caller then has to pass NULL if needed. That would introduce a dependency on Lepntonica HEAD. (NULL will error out in Leptonical 1.74). Let's do this as a 4 step dance. 

(1) Hack "stdin" along with NULL filename support into Leptonica as written above.
(2) Wait a long long time for next Leptonica to release and get everywhere
(3) Switch Tesseract to NULL
(4) Remove hack from Leptonica Leptonica fix is in; Dan will be uploading to github shortly. Very similar to above except a small tweak to not use variable `cid` uninitialized.  Keeping the old enum entries for Cube is not necessarily wrong. If API compatibility is a goal, those old entries must not be reused (but maybe should raise an error message when used from the command line). And of course the comments for the Cube entries also need an update.

Here is a more complete list of files which still contain text / code related to Cube:

    ChangeLog
    ccmain/docqual.cpp
    ccmain/fixspace.cpp
    ccstruct/publictypes.h
    ccstruct/ratngs.h
    ccutil/tessdatamanager.h
    ccutil/unicharset.cpp
    dict/dict.h
    training/tesstrain_utils.sh
    training/text2image.cpp
  \+ Remove support for VS2010.
 No. Not out of the box.
One needs fixing some bugs in tiff library that comes from cppan.
Or there's always manual way of including dependencies. In this case VS2010 might work.

With cppan VS2015 and VS2017 are only supported. VS2010 has [limited support for C++11](https://msdn.microsoft.com/en-us/library/hh567368.aspx), so there should be more problems . I would expect that situation with VS2010 Express (free version of VS2010) is even worse.
  Please create a PR.

On 18-Aug-2017 2:52 PM, "Domingo Alvarez Duarte" <notifications@github.com>
wrote:

> Thanks for pointing out !
> I dived on tesseract source code and found an almost solution to the
> preserve_interword_spaces problem, it seems that when transferring the
> words the spaces were not transfered see patch bellow.
> With this patch the output is almost identical with the 3.05 version
> except for the missing spaces for the first column (need more research to
> see where the first/second word is transfered and why the spaces/blanks are
> not).
>
> @@ -1329,11 +1329,11 @@ void PAGE_RES_IT::ReplaceCurrentWord(
>    WERD_RES* input_word = word();
>    // Set the BOL/EOL flags on the words from the input word.
>    if (input_word->word->flag(W_BOL)) {
>      (*words)[0]->word->set_flag(W_BOL, true);
>    } else {
> -    (*words)[0]->word->set_blanks(1);
> +    (*words)[0]->word->set_blanks(input_word->word->space());
>    }
>    words->back()->word->set_flag(W_EOL, input_word->word->flag(W_EOL));
>
>    // Move the blobs from the input word to the new set of words.
>    // If the input word_res is a combination, then the replacements will also be
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/781#issuecomment-323304593>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o4sRtuKQml61AmRBaijFs5XDubIuks5sZVe9gaJpZM4MmXJm>
> .
>
 Checked with the proposed PR.

[toc-eng.txt](https://github.com/tesseract-ocr/tesseract/files/1292209/toc-eng.txt)
[toc-eng-space.txt](https://github.com/tesseract-ocr/tesseract/files/1292208/toc-eng-space.txt)
![toc](https://user-images.githubusercontent.com/5095331/30270531-835fa0c4-970a-11e7-8403-b3b21a217680.png)

Spaces are preserved now when using tesseract with `--oem 1 --psm 6 -l best/eng -c preserve_interword_spaces=1`

Here is the output:

```
1 First chapter                                   3
1.1 Section One                                 3
1.2 Section Two                                   3
1.3 Section Three                                  3

2 Last chapter                                     5
2.1 Section One                                 5
22 Section Two                                   5
2.3 Section Three                                  5
``` The fix was applied in https://github.com/tesseract-ocr/tesseract/commit/e62e8f5f802c0d8f3dd67da993327cdafaee9763

Build the new version and check. @zdenop You can close this. Thanks!  Did you run `./autogen.sh` _after_ the installation of `autoconf-archive`?  Please don't post problems for which any search engine will give you the answer. See #606, #628, #647, #701 for example. And read the [documentation](https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT.md).  Use tesseract user forum for asking support. 
When installing for source you have to remove (uninstall) previous version (this is valid not only for tesseract).  So you have installed both 3.04 (maybe from your Linux distribution) and 3.05 (built in the steps above). Do you need both, or can you remove 3.04?  I also noticed that the new LSTM recognizer in Tesseract 4 tries to detect characters even in very small elements of the image. In our case, a dot was "recognized" as a P, and indeed it resembled that character at a very high zoom level. Maybe @theraysmith can better explain that effect.   Done  Box files created by tesseract 4.00.00alpha master branch with `makebox` option cannot be used for LSTM training, as they do not have:

* Tab character to mark end of line
* Space between words Also, `makebox` option creates box files with different box sizes and locations on the same image depending on the OEM mode.

![boxfiletest](https://user-images.githubusercontent.com/5095331/27507096-ccec61e0-58e5-11e7-9686-281955978db6.png)
[boxfiletest-1.box.txt](https://github.com/tesseract-ocr/tesseract/files/1099521/boxfiletest-1.box.txt)
[boxfiletest-0.box.txt](https://github.com/tesseract-ocr/tesseract/files/1099520/boxfiletest-0.box.txt)

 Testing with current code, after new unicharset_extractor, there are additional problems in box files for complex scripts such as Devanagari.

The synthetic tif/box files created by `text2image` create one box for each aksara (conjunct-cluster+combining mark). These are broken into components by the unicharcompressor/recoder.

```
जु 110 4633 154 4679 0
ला 147 4645 205 4679 0
ई 198 4640 235 4694 0
- 233 4656 245 4660 0
से 244 4645 288 4692 0
प्टे 281 4645 318 4692 0
म् 311 4655 341 4678 0
ब 330 4645 366 4678 0
र 357 4645 389 4678 0
  389 4644 406 4691 0
```

However, the box files created by `tesseract` using the `makebox` config file, split the components and create a box for each. This happens both with Devanagari and hin traineddatas from tessdata_best.

```
ज 107 4632 124 4695 0
ु 124 4632 150 4695 0
ल 150 4632 168 4695 0
ा 168 4632 187 4695 0
ई 187 4632 213 4695 0
- 233 4656 241 4661 0
स 241 4656 246 4661 0
े 255 4644 274 4693 0
ष 274 4644 286 4693 0
् 286 4644 297 4693 0
ट 297 4644 309 4693 0
े 309 4644 318 4693 0
म 318 4644 328 4693 0
् 328 4644 344 4693 0
ब 344 4644 358 4693 0
र 358 4644 388 4693 0
```
However, the older 4.0alpha hin.traineddata in tessdata repo has the aksharas, similar to the ones by text2image.

```
जु 107 4632 166 4695 0
ला 166 4632 190 4695 0
ई 190 4632 213 4695 0
- 213 4632 234 4695 0
से 239 4656 246 4661 0
प् 274 4644 297 4693 0
टे 297 4644 323 4693 0
म् 323 4644 344 4693 0
ब 344 4644 358 4693 0
र 358 4644 382 4693 0
```
@ @ The `tesseract` `makebox` created box files for Devanagari script get the following errors related to normalization regarding combiner marks:

```
=== Phase UP: Generating unicharset and unichar properties files ===
[Mon Oct 16 17:35:49 DST 2017] /usr/local/bin/unicharset_extractor --output_unicharset /tmp/tmp.fN2Jr9NHXJ/bih/bih.unicharset --norm_mode 2 /tmp/tmp.fN2Jr9NHXJ/bih/bih.m
akebox.exp0.box /tmp/tmp.fN2Jr9NHXJ/bih/bih.Siddhanta.exp0.box
Extracting unicharset from box file /tmp/tmp.fN2Jr9NHXJ/bih/bih.makebox.exp0.box
Word started with a combiner:0x941
Normalization failed for string 'ु'
Word started with a combiner:0x93e
Normalization failed for string 'ा'
Word started with a combiner:0x947
Normalization failed for string 'े'
Word started with a combiner:0x94d
Normalization failed for string '्'
```

So, some change needs to be made where the box file is being written that it combines the consonant clusters and combining marks and then creates the boxes.

Possibly this will also happen for other complex scripts. 
  Please use tesseract user forum for asking support/questions.  Please use tesseract user forum for asking questions/support  While running the training for `frk` with a longer training text, I get [assertions in stringrenderer.cpp](https://github.com/tesseract-ocr/tesseract/blob/master/training/stringrenderer.cpp#L553) for 5 of 12 fonts. They can be reproduced by running `text2image` for that text with a single language:

    $ text2image --fonts_dir=/usr/share/fonts --font="Proclamate Light, Light" --text=../langdata/frk/frk.training_text --outputbase=/tmp/frk
    Stripped 157 unrenderable words
    Rendered page 0 to file /tmp/frk.tif
    # ...
    Rendered page 1163 to file /tmp/frk.tif
    Stripped 57 unrenderable words
    cluster_text.size() == start_byte_to_box.size():Error:Assert failed:in file ../training/stringrenderer.cpp, line 553

 Yes, thanks. I fixed it in my original post.  Please use tesseract user forum for asking questions  ​you can add per+eng as language parameter  ​
 @Shreeshrii accuracy in persian and arabic language is poor yet
but its really better then 3.02 it seems per is better for 3.02 a and fas better for 4​
 ​per is a seprated project on github

also i checked now and
fas+eng returned some cropped data​...
 @theraysmith Is there a `Persian.traineddata` which handles Persian and English together? @theraysmith

Please inclufe a Persian script based traineddata which has fas and eng in
your next training.



On 11-Aug-2017 2:00 PM, "peiman F" <notifications@github.com> wrote:

> @amitdo <https://github.com/amitdo> no,i checked
> the Arabic train data cant detect the symbols that is included in Persian
> and not in Arabic
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/762#issuecomment-321757389>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o1GBn-sysVkGYv6My3NSCU8ZmTkBks5sXBExgaJpZM4McLHL>
> .
>
   @voyageur: How do you build tesseract on windows? AFAIK autotools (should) do the same jobs regardless platform.... if libtiff is needed only for windows build than it should be fixed on correct place - I did it in f30cac479d1e1f857528da57ba82977b830db87a  Please use tesseract user forum for asking support. Make sure you read relevant wiki first.  Running lstmtraining for frk language with 50000 iterations terminated with an assertion.

    $ lstmtraining -U /home/stweil/src/github/tesseract-ocr/tesseract/frk/train/frk.unicharset --script_dir ~/src/github/tesseract-ocr/langdata --net_spec '[1,36,0,1 Ct5,5,16 Mp3,3 Lfys64 Lfx128 Lrx128 Lfx256 O1c105]' --model_output /home/stweil/src/github/tesseract-ocr/tesseract/frk/output/base --train_listfile /home/stweil/src/github/tesseract-ocr/tesseract/frk/train/frk.training_files.txt --eval_listfile /home/stweil/src/github/tesseract-ocr/tesseract/frk/train/frk.training_files.txt --max_iterations 50000
    ...
    At iteration 15778/49900/49900, Mean rms=0.37%, delta=0.112%, char train=0.381%, word train=1.506%, skip ratio=0%,  wrote checkpoint.
    
    At iteration 15788/50000/50000, Mean rms=0.363%, delta=0.104%, char train=0.346%, word train=1.387%, skip ratio=0%,  wrote checkpoint.
    
    Finished! Error rate = 0.26
    num_docs > 0:Error:Assert failed:in file ../../../../ccstruct/imagedata.cpp, line 648

I used latest Tesseract sources, a slightly modified font list and a longer training text for frk training.
A previous run with 10000 iterations and nearly the same conditions did not raise the assertion:

    ...
    2 Percent improvement time=807, best error was 3.911 @ 8211
    At iteration 9018/10000/10000, Mean rms=0.835%, delta=0.465%, char train=1.729%, word train=6.095%, skip ratio=0%,  New best char error = 1.729Deserialize failed wrote best model:/home/stweil/src/github/tesseract-ocr/tesseract/tutorial/frkoutput/base1.729_9018.lstm wrote checkpoint.
    
    Finished! Error rate = 1.729
 The training machine has 23 GiB RAM plus 23 GiB swap, and about 35 GiB of that memory are available for the training. Maybe some Debian GNU Linux defaults set a smaller limit for single processes, but I don't think we have a memory problem. According to [font_properties](https://github.com/tesseract-ocr/langdata/blob/master/font_properties), Ray used about 6000 fonts. I used 12 fonts. The training result was pretty good for the old engine and unusable for LSTM. A server which wants to process [700000 pages](http://digi.bib.uni-mannheim.de/import/Reichsanzeiger/ocr/film/001-1879/) of a journal printed in fraktur. > I have found that the error goes away when NOT using --eval_listfile, please try without the following ...

Yes, the assertion does not occur when I omit `--eval_listfile`.
 @Shreeshrii, that's a nice collection of Fraktur fonts, but several of the image not even include all normal ASCII characters. All images are missing the long s character (ſ) which is very important for all Fraktur texts. Also missing are all forms of ligatures (combinations of certain characters, like for example ffi, which need a special rendering). Did you see the new [page about fonts](https://github.com/tesseract-ocr/tesseract/wiki/Fonts) which I added to the wiki? Maybe you want to add information there. Many thanks, that's a really very useful document which might allow us to find the exact list of Fraktur fonts used for the German newspaper editions printed from 1900 up to 1945. @stweil I got the same error now. Though the same files and commands worked before and after.

```
Warning: LSTMTrainer deserialized an LSTMRecognizer!
2 Percent improvement time=1100, best error was 100 @ 0
At iteration 1100/1100/1100, Mean rms=0.821%, delta=44.564%, char train=99.966%, word train=100%, skip ratio=0%,  New best char error = 99.966 wrote checkpoint.
Finished! Error rate = 99.966
num_docs > 0:Error:Assert failed:in file imagedata.cpp, line 650
./4runtesseract.sh: line 16:  5559 Segmentation fault      (core dumped) lstmtraining --script_dir ./tess4training-save -U ./tess4training-save/bih.unicharset --continue_from ./tess4training-save/bih.lstm --train_listfile ./tess4training-save/bih.training_files.txt --eval_listfile ./tess4training-save/bih.eval_files.txt --model_output ./tess4training-save/bihlayer --append_index 5 --net_spec '[Lfx384 O1c105]' --debug_interval 0 --perfect_sample_delay 19 --max_iterations 1000
```
Ref: https://travis-ci.org/Shreeshrii/tess4train/builds/249914589 It would be expected to get such a low error rate on your training set, but
has it overfitted? How does it do on different test data?

On Tue, Aug 1, 2017 at 6:13 AM, hanikh <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> I am trying to fine-tune
> tesseract for Arabic and Persian. I have used 4000 text lines and about 40
> fonts. and I set the max-error-rate=0.001. the error rate of 0.002 has been
> recorded. but after finishing of the training process I got error-rate=0!
> Is it reasonable?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/757#issuecomment-319365858>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056TlrhrROzIm6VOXVoiHjygnInOUGks5sTyRvgaJpZM4Ma6ed>
> .
>



-- 
Ray.
  The [original image](https://digi.bib.uni-mannheim.de/~stweil/tesseract/0604.jp2) in JPEG 2000 format includes two pages from a newspaper. This image is processed correctly by latest Tesseract. Tesseract fails with the same image in TIFF, JPEG or [PNG](https://digi.bib.uni-mannheim.de/~stweil/tesseract/0604.png) format and reports two empty pages.

This happens also with older versions of Tesseract. checking... Quick side note. It's good to see that there is resolution metadata in the JP2. Remember to carry that over to other formats during conversion. It did not make it to the PNG file.

```
$ jhove ~/Downloads/0604.jp2  | grep -i sampling
      SamplingFrequencyUnit: centimeter
      XSamplingFrequency: 118.11
      YSamplingFrequency: 118.11
``` Tesseract is able to find text when resolution metadata is properly set. Result is 54 megabytes, so a little too big to attach. But it works and you should be able to reproduce.

```
$ mogrify -density 300x300 -units PixelsPerInch 0604.png
$ tesseract -l ger 0604.png 0604 pdf
```

 Do you think that Tesseract could handle missing resolution information in a more user friendly way? I created the test images using `convert 0604.jp2 0604.png` (or similar for other formats). I could imagine Tesseract trying 300 dpi in addition to the 70 dpi which it claims to use:

    tesseract 0604.png /tmp/0604-png
    Info in bmfCreate: Generating pixa of bitmap fonts from string
    Tesseract Open Source OCR Engine v4.00.00alpha-332-g4c5d0b5 with Leptonica
    Warning. Invalid resolution 0 dpi. Using 70 instead.
    Empty page!!
    Empty page!!

Does 70 dpi as default value make sense at all? And why does the resolution matter? Will Tesseract detect only characters of a certain size? Maybe 70 was chosen because it was screen resolution back when dinosaurs walked the earth, and Tesseract was first written? Why does resolution matter? I'm guessing there are complicated heuristics somewhere in the code that tries to guess at likely font sizes. For example, if I crop out a small piece of the newspaper and set to 0 dpi, we get results. Sounds like investigation is needed. Or we can ask Ray.

PS. Irrespective of this bug, try to use good hygiene with resolution metadata. Maybe some day later you'll want to know what size the fonts are. Or something where you might regret losing the resolution metadata. I've seen it happen far too many times.

```
$ tesseract -l ger_old /tmp/foo.png -
Warning. Invalid resolution 0 dpi. Using 70 instead.
Magdeburg. [55996]

In das iit heute
bei der _ unter RNr. 151 verzeichneten
Fort-
fchritt, eingetragene
mit befohräufter Heofipflicht' in Dl.
venftedt eingetragen worden: Die Ge-
nofenfhaft ift durd BVefhluf der Ge-
neralverfammlung vom 16. Uuguft 1920
aufgelöft. Anuguft Üterwedde und Leo
Krötfi, beide in Olvenfiedt, find zu
Liquidatoren bejielt.

Magdeburg, dem 19. AÄuguft 1920.
OVa& IAmtenericht A A
```


![foo](https://cloud.githubusercontent.com/assets/4961958/23814766/a4524000-0599-11e7-8bcb-333ede147b8a.png)

 The resolution is only used by layout analysis.
It sets the threshold size at which to call possible text so ridiculously
small that it can't possibly be text. I.e. it helps to distinguish text
from noise.
There is also some auto scaling somewhere in the preprocessing to magnify
low resolution text that is not needed by the LSTM engine, but is needed by
the legacy engine.

On Sun, Apr 23, 2017 at 6:53 AM, Stefan Weil <notifications@github.com>
wrote:

> Maybe. It is not clear why the dpi information is needed at all. I can
> read text of any dpi (just have to adapt the reading distance or get some
> glasses) without knowing the actual dpi value, and ideally OCR software can
> do that, too.
>
> If the dpi value is important, we need an option to set it for images
> without (or with wrong) resolution metadata.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/756#issuecomment-296444866>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056e75bpdwBby3WvUvTDgGbwjSMLBnks5ry1fjgaJpZM4MZ1WT>
> .
>



-- 
Ray.
 Sounds like implementation is disagreeing with intention. Where's the layout analysis resolution code? At the time that 70 minimum was set, it was a question of how to cover the most probable case.
Back in the day when most inputs came from a flatbed scanner, the resolution was provided.
Most images that did not have a resolution were screenshots at ~70ppi. In theory processing a 300ppi image at 70 should be less damaging to accuracy than processing a 300ppi image at 70, but that seems not the case in the original post. It would be worth taking a look at why that happens. There might be an easy fix.
Incidentally, most monitors today still give you not much more than 70ppi, (maybe 150) but they give you a bigger screen with even more small text on it. Only phones manage ~300ppi and maybe my new laptop, which has more pixels than my 24" monitors in less than half the area.

Now, when a lot of images come from camera phones, the resolution is largely unknown, and layout analysis requires some more work.

Incidentally, there is an easy way in to set resolution. Set it in the Pix before passing it to TessBaseAPI. I now have a reasonably general fix for the resolution issue.

There are multiple unsolved problems with the original 0604 image though:
There are large gaps between words, but tiny gaps between columns. That was
causing column finding to fail, causing the blank page determination. The
problem is that it sees the large gaps between words, which at 70 ppi look
huge, and decides that it shouldn't merge them into textlines. Although
that should be fixed, it is a highly dangerous thing to try without very
careful testing.
The columns aren't straight. The layout analysis is fundamentally broken in
such cases. It can't cut a straight line (even at an angle) through the
very narrow bent gap between columns.

A general fix for resolution is to estimate the resolution based on the
measured body text size, which is available before the column finder is
constructed. That makes for an easy fix.
On the original 0604 image, it estimates the resolution to be 470 ppi but
still generates a poor layout analysis, due to the above problems.

On Tue, Apr 25, 2017 at 5:20 AM, Amit D. <notifications@github.com> wrote:

> https://github.com/tesseract-ocr/tesseract/search?q=resolution
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/756#issuecomment-297012426>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056WiqW85XlvkQ5YbrmK2x46GsQvNEks5rzeUIgaJpZM4MZ1WT>
> .
>



-- 
Ray.
  LSTM training takes a lot of time and has no well defined duration. Therefore saving intermediate results (with a configurable frequency) is a good thing. It allows tests based on those intermediate results, and those results can also be used to resume the learning process if it was interrupted for whatever reason. Thanks @stweil .  Maybe add them to https://github.com/tesseract-ocr/tesseract/wiki/Data-Files?  This fixes several compiler warnings. > The whole 'classify' directory will most likely be removed by Ray (it is part of the old engine). There is no point in applying any change to files under this directory.

I disagree. Maybe that directory will be removed in the future. That's not a good reason why unused code should not be removed now. Maybe we could support a Tesseract 4 with both old and new recognizer engine at least for the next year (thus eliminating the need to support Tesseract 3 for a longer time span). I'd appreciate that scenario, but if we prevent any code maintenance of the old engine in Tesseract 4, we'll never know whether that might be possible.

See also #744 which I'd apply for the same reason. My code for big endian support (#703) also changes both recognizers. I don't. As far as I have understood Ray, he wants a Tesseract which works really good. LSTM in Tesseract 4 is a large step forward. Nevertheless, there is obviously still some need for the old engine until we can be sure that LSTM outperforms it in all OCR applications (see issue #707). Based on my experience I'd try to keep both engines for some time to allow easy verification of the LSTM performance. In issue #518, Ray said that he wants to remove the old engine, but is open for a discussion based on facts, and he also said what would have to be done if the old engine were kept. I don't know.
The issue is currently in cppan. I've just found it, probably will fix it tomorrow or till Monday. IMO removing unused function from directory, that _should be removed_, is not big issue. OK, I only skimmed this to get the gist.
Yes I'm open to discussion on the future of the legacy engine in 4.00 and beyond, but my aim is to prove to the community that it can be safely removed, so my favorite kind of discussion is "it doesn't work well for this image" with example.
I'm not against removal of unused code from the dead code.
What I'm not keen on is major edits to the dead code that don't remove dead code.

I have a commit coming (next week?) that fulfills a long-standing request - to be able to load from a trainneddata file in memory (or via a file-reader function pointer). This involves a major edit to the dead code, but I had to do it to keep Android portability.
This will also showcase my clean and simple fix to the endian issue that Stefan raised.
I haven't yet done the endian fix to the LSTM code, but it is *easy* now.  @theraysmith Are there plans to support this for LSTM? In response to https://groups.google.com/forum/#!topic/tesseract-ocr/-oeCTcojYfw

You can try the plus-minus type of training if you just want a digits type of traineddata.

Your training_text can contain numbers in the format you need and you can train with a font matching your images.

For proof of concept you can try my experimental version at 

https://github.com/Shreeshrii/tessdata_shreetest  That's a rather lengthy commit messages, and it is not so easy for me to get the essentials from it. The old code works, but throws a nasty and confusing warning on systems which provice `glibtoolize` instead of `libtoolize`. This is fixed by your pull request. Re-opened to re-check CI (win). By the way: it's `autogen.sh`, not `autopen.sh`. Thank you for the updates. I have no strong opinion regarding the lengthy commit message – the code changes are the important point, and a long commit message is better than a too short one. And whoever (@zdenop, @egorpugin) pulls the PR can squash the commits when doing so. Did you check 3.05? What is missing there? @RandomDSdevel : I tried to backport as much as I recognize. Unfortunately I do not have a lot of time and there could be thing I overlook. I appreciate if somebody does checks afterwards.  which tesseract version (revision) you use? Please submit config.log and output of this commnad (onfigure.log):
`./configure --prefix=/usr 2>& | tee configure.log`  You posted it already to forum. Read doc before asking help.  Both functions simply call malloc, free.

Remove also unneeded null pointer checks and use calloc where possible.

Signed-off-by: Stefan Weil <sw@weilnetz.de> @amitdo, do you think that the PR should not be applied because it changes code which might be removed later? @amitdo, a fair assessment.
Although I haven't made any major changes to the codebase in the last few weeks, yes I would still like to remove the old classifier and take out a lot of code with it.

I'm going to review the replies to my request for "old better than new", and thanks to those that provided them, with a view to making new better than old on those problems. I haven't got to that yet, as I have been totally occupied by fixing many problems I have found with the synthetic training data generation pipeline. Since training takes about 2 weeks, fixes to the source data get priority treatment over other code changes.

Yes, each commit gets reviewed by me *and* someone else at Google.  Weird. Let's get rid of that limit. https://github.com/DanBloomberg/leptonica/commit/fd65f6b374f4e63ee55ff90a8524bee4a4381b18

Same change needed in opencl/openclwrapper.cpp (alternative: kill OpenCL/TIFF entirely) @bstace Are you getting any benefit from the OpenCL? Is it faster? How much faster?  Could you squash or rebase or do whatever to get a single commit with your change?  Tesseract does not reference any libtiff symbols. I don't understand this at all.

$ find  | xargs grep TiffFdOpen Thanks @amitdo, I stand corrected. All that code is currently unused but you are correct, it is right there. In general I think we may push needed changes to 3.05 branch and tag patch releases (3.05.01, 3.05.02, ...). PR is welcomed.
BTW: At this stage of support of OpenCL I suggest to remove OpenCL from 3.05 and 4.0x  > BTW: At this stage of support of OpenCL I suggest to remove OpenCL from 3.05 and 4.0x

OpenCL worked for me (after I had spent some time on it and contributed several pull requests) at least until end of last year, both with 3.05 and with 4.0x. Depending on the graphic card, it can be a great gain. That is relevant for some OCR projects which would take years on a single CPU.

A test which I just have done with latest Tesseract code was not successful, but I estimate that it can be fixed again. @RandomDSdevel, I don't think that you need this fix to get 3.05 running with Homebrew. The error message caused by the missing `libtoolize` on macOS is confusing, but it's only a cosmetic problem. And if you don't use OpenCL (`--enable-opencl`), building Tesseract should work. There are good reasons why OpenCL is disabled by default. 1. In case of OpenCL, "-ltiff" is added automatically by [configure](https://github.com/tesseract-ocr/tesseract/blob/3.05/configure.ac#L81). So there is no need to add linking to libtiff because of OpenCL (if yes then there is other problem that should be fixed!)
2. I just checked compilation of 3.05 on openSUSE linux and it works without problem/modification. Linking to libtiff is done "automatically" (based on leptonica?): 
```
libtool: link: ranlib .libs/libtesseract_api.a
libtool: link: ( cd ".libs" && rm -f "libtesseract_api.la" && ln -s "../libtesseract_api.la" "libtesseract_api.la" )
/bin/sh ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11 -version-info 3:5 -no-undefined   -o libtesseract.la -rpath /usr/lib64  libtesseract_api.la ../ccmain/libtesseract_main.la ../textord/libtesseract_textord.la ../wordrec/libtesseract_wordrec.la ../classify/libtesseract_classify.la ../dict/libtesseract_dict.la ../ccstruct/libtesseract_ccstruct.la ../cutil/libtesseract_cutil.la ../viewer/libtesseract_viewer.la ../ccutil/libtesseract_ccutil.la ../opencl/libtesseract_opencl.la ../cube/libtesseract_cube.la ../neural_networks/runtime/libtesseract_neural.la -llept -lpthread 
libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib64/gcc/x86_64-suse-linux/4.8/../../../../lib64/crti.o /usr/lib64/gcc/x86_64-suse-linux/4.8/crtbeginS.o  -Wl,--whole-archive ./.libs/libtesseract_api.a ../ccmain/.libs/libtesseract_main.a ../textord/.libs/libtesseract_textord.a ../wordrec/.libs/libtesseract_wordrec.a ../classify/.libs/libtesseract_classify.a ../dict/.libs/libtesseract_dict.a ../ccstruct/.libs/libtesseract_ccstruct.a ../cutil/.libs/libtesseract_cutil.a ../viewer/.libs/libtesseract_viewer.a ../ccutil/.libs/libtesseract_ccutil.a ../opencl/.libs/libtesseract_opencl.a ../cube/.libs/libtesseract_cube.a ../neural_networks/runtime/.libs/libtesseract_neural.a -Wl,--no-whole-archive  -Wl,-rpath -Wl,/usr/lib64 -Wl,-rpath -Wl,/usr/lib64 /usr/lib64/liblept.so -lz -lpng -ljpeg -lgif -ltiff -lwebp -lopenjp2 -lpthread -L/usr/lib64/gcc/x86_64-suse-linux/4.8 -L/usr/lib64/gcc/x86_64-suse-linux/4.8/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib64/gcc/x86_64-suse-linux/4.8/../../../../x86_64-suse-linux/lib -L/usr/lib64/gcc/x86_64-suse-linux/4.8/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib64/gcc/x86_64-suse-linux/4.8/crtendS.o /usr/lib64/gcc/x86_64-suse-linux/4.8/../../../../lib64/crtn.o  -O2   -Wl,-soname -Wl,libtesseract.so.3 -o .libs/libtesseract.so.3.0.5
```
So I guess there should be some other problem... @mkszuba: Did you compile leptonica with libtiff support?  Please use tesseract user forum for asking questions  LSTM training process has been changed. Hence closing this issue.  Please use tesseract user forum to ask questions.  - enable to set up producer property in Info dict. 1. What is the reason for this change?  Shouldn't Tesseract get credit for its PDF output?
2. As written, this creates a new security vulnerability. See issue #636.  Ok, I understand. Please reject this PR. > If someone embeds libtesseract in his his product 'myOwnOCR', he probably prefer that any output will use this name instead of 'Tesseract'...

Exactly, that is my case. I use libtesseract and in"about" of my app is mentioned that's is based on tesseract. I would like to set another producer as "tesseract <version>" Aside from any credit issues, there is also a matter of compatibility. We know that 3.0.4 Tesseract PDF does better with Firefox, and 3.0.5 Tesseract PDF does better with ghostscript. Maybe 3.0.6 will finally do better with Arabic / Hebrew on Adobe Reader. That sort of information gets lost when we override Producer. That said, reasonable people can probably disagree here.

I think the escaping issue is very clear. We don't want to create more opportunities for creating corrupt or malicious PDF files, and therefore issue #636 is a hard requirement.  There is now a fix for issue #636 which demonstrates how to safely set a metadata field. OK, I think you can close/deny this PR.   Try using the best/Latin traineddata

You can use the command

Combine_tessdata -u to unpack the traineddata file and check that the
unicharset has the bullet character.


On 07-Aug-2017 9:36 PM, "yuvarajanS" <notifications@github.com> wrote:

> Hi @Shreeshrii <https://github.com/shreeshrii> ,
> I am looking for a trained data for the bullet character. It should work
> even if we are using mulitple language in a single string.
>
> Can you please provide the trained data for bullet character or guide me
> how to do do this?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/730#issuecomment-320706511>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o4te3XfB8tMMEcnBGB1mOcKR4a88ks5sVzXugaJpZM4MF7F5>
> .
>
 what is the unicode number for bullet character? Please see https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#fine-tuning-for--a-few-characters

and follow instructions there for the bullet character.  Similarly for Nepali - please see https://groups.google.com/forum/#!topic/tesseract-ocr/m2Dx6a-srvo

> I got the same result as yours with hin.traineddata which is better than nep.traineddata. I think the langdata need some revisions

 Ok. Will do.  There should be possibility to produce OCR result without hyphenated words in case of hocr output or in case of paragraph(RIL_PARA)/block(RIL_BLOCK) page iterator level.

Test image:
![paragraph_sk](https://cloud.githubusercontent.com/assets/574156/23105439/8cbce864-f6df-11e6-9759-e7595154381d.png)

Output ( (`tesseract paragraph_sk.png paragraph_sk -l slk hocr`):

> ty, tak to nerob ty sám. Ak si nepraješ, aby sa k tebe zacho- vali tak, ako to ty chceš urobiť druhému, potom či nie je lep- šie, ak to zastavíš? Premýšľaj, ako by si sa ty cítil, keby zverejnili to, čo máš v úmysle teraz o niekom povedať? Nebol by si urazený alebo zranený? Ak áno, tak radšej buď, prosím, ticho.

  Try current code from 3.05 branch (or apply localy changes in https://github.com/tesseract-ocr/tesseract/commit/e85a7e2529416a12e806a8df2e9fdf92f7e39a5f)  No current plans with respect to Tesseract. I think the two best approaches would be

1. Running Tesseract output through a PDF->PDF compressor, to produce JBIG2 PDF. These exist commercially, not aware of an open source implementation.

2. Feed JBIG2 to Tesseract as an input format. This would require teaching Leptonica to decompress JBIG2, and teaching Tesseract to copy over the input JBIG2 over to the output PDF. Downsides include the complexity of dealing with the multipage aspect, and inability to expand this approach towards mixed raster content.

3. Start with JBIG2 PDF without a text layer (somehow). Rasterize to images. Ask tesseract to produce invisible-text-only PDF output. (This is already supported). Merge the two PDFs together. (Open source tools including pdftk can already do this.)

>flate-compression currently used

Tesseract will carry forward CCITT Group 4 compression if handed TIFF G4 input.  Why don't you use zxing which is built specifically for this? OCR is  Optical Character Recognition. Anyway - use tesseract user forum for asking questions.  I am against to show GIT_REV in release mode:

- Experienced users/testers should know what they are doing, what their are testing
- Inexperienced users should use released stable version  There had been some [code improvements](https://github.com/tesseract-ocr/tesseract/commits/3.05) for OpenCL in 3.05. Maybe you want to try that version. I think the OpenCL implementation is at risk in general. The OpenCL TIFF decoder has fallen behind the C implementation and is currently disabled.  There is no OpenCL implementation for the LSTM recognizer in Tesseract 4.x. I'm not sure what performance we expect from OpenCL on the Tesseract 3.x recognizer, but your report is not giving me much confidence.

If I were in charge of OpenCL stuff on Tesseract (and I am definitely not!) I would look at the LSTM recognizer. Specifically the portion that is currently using AVX2 assembly instructions, which is the primary hotspot.
 OpenCL is just experimental feature, so having no effect is possible results.
Please read other issues/tesseract forums about OpenCL. Tesseract tests whether OpenCL is better than the CPU and calculates a score based on weighted timings of several different basic operations.

Here is a typical result on a MacBook Pro:

    [DS] Device: "Iris" (OpenCL) evaluation...
    [DS] Device: "Iris" (OpenCL) evaluated
    [DS]          composeRGBPixel: 0.009361 (w=1.2)
    [DS]            HistogramRect: 0.167102 (w=2.4)
    [DS]       ThresholdRectToPix: 0.004588 (w=4.5)
    [DS]        getLineMasksMorph: 0.015459 (w=5.0)
    [DS]                    Score: 0.510220

    [DS] Device: "(null)" (Native) evaluation...
    [DS] Device: "(null)" (Native) evaluated
    [DS]          composeRGBPixel: 0.022881 (w=1.2)
    [DS]            HistogramRect: 0.074672 (w=2.4)
    [DS]       ThresholdRectToPix: 0.042662 (w=4.5)
    [DS]        getLineMasksMorph: 0.140875 (w=5.0)
    [DS]                    Score: 1.103025

The CPU is faster for `HistogramRect` and slower for all other operations. The weighted score favors OpenCL. If `HistogramRect` is used very often, it would be better to use the CPU. This is what I observe for a larger image.

Instead of using a weighted score, Tesseract could always use the faster option. I expect that this potential mix of CPU and OpenCL would give the shortest execution time.  Please use tesseract use forum for askiny question. Please read wiki first.  
[test.pdf](https://github.com/tesseract-ocr/tesseract/files/770565/test.pdf)
![test](https://cloud.githubusercontent.com/assets/25739284/22877715/9d091ece-f1ce-11e6-867c-dd7fe106ec43.png)

[test.hocr.txt](https://github.com/tesseract-ocr/tesseract/files/770570/test.hocr.txt)


Hi

In the attached hocr you can search for "bbox 0 0" and see that there are multiple occurrences of ocrx_word artifacts where the bbox coordinates are the same as those of the ocr_page artifact.

These ocrx_word occurrences seem to be occurring in relation to vertical and horizontal lines in the image.

If you search for "Party" in the pdf, you will see that the bbox for the last two instances goes out to the end of the underlining, despite the fact that the text has been correctly identified as "Party A" and "Party B"

The image was redacted using Paint and Windows clipboard, but despite any damage I have inflicted on the file, the text is still correctly identified.
Whatever the state of the image, it can't be correct for an ocrx_word to have the same coordinates as that of the ocr_page.


I downloaded the Windows Installer version from UB Mannheim on 18/1/17 (the current version doesn't run on my system!):

tesseract 4.00.00alpha
 leptonica-1.73
  libgif 4.1.6(?) : libjpeg 8d (libjpeg-turbo 1.4.2) : libpng 1.6.20 : libtiff 4.0.6 : zlib 1.2.8 : libwebp 0.4.3 : libopenjp2 2.1.0

Windows 10
Version	10.0.14393 Build 14393
  
The command I'm running is:
"C:\Program Files (x86)\Tesseract-OCR\tesseract.exe" -oem 4 -psm 6 -c tessedit_create_hocr=1 -c tessedit_create_pdf=1 test.png test


I've turned the auto psm off, since it seems to lose the right-hand column of tables completely, and I'd rather carry out my own psm.

OEM 4 is just amazing. Great work!!


Major B
 Ok. Looks like I need to wait for a working build of the latest version (I'm a java-restricted etl/sql developer).

If I use the latest UB Mannheim build with:

"C:\Program Files (x86)\Tesseract-OCR\tesseract.exe" -oem 1 -psm 3 -c tessedit_create_hocr=1 -c tessedit_create_pdf=1 test.png test

I get:

read_params_file: parameter not found: ëPNG


And if I try:

"C:\Program Files (x86)\Tesseract-OCR\tesseract.exe" test.png test

I get:

Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
DotProductAVX can't be used on Android

And the exe bombs out. I think this is a known issue, and I just need to wait.

Thanks "C:\Program Files (x86)\Tesseract-OCR\tesseract.exe" test.png test --oem 1 --psm 3 -c tessedit_create_hocr=1 -c tessedit_create_pdf=1
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
DotProductAVX can't be used on Android

This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
DotProductAVX can't be used on Android

This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
DotProductAVX can't be used on Android

This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
DotProductAVX can't be used on Android

This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
ObjectCache(01210A88)::~ObjectCache(): WARNING! LEAK! object 05981790 still has count 1 (id C:\Program Files (x86)\Tesseract-OCR/tessdata/eng.traineddatalstm-punc-dawg)
ObjectCache(01210A88)::~ObjectCache(): WARNING! LEAK! object 05963128 still has count 1 (id C:\Program Files (x86)\Tesseract-OCR/tessdata/eng.traineddatalstm-word-dawg)
ObjectCache(01210A88)::~ObjectCache(): WARNING! LEAK! object 059631D8 still has count 1 (id C:\Program Files (x86)\Tesseract-OCR/tessdata/eng.traineddatalstm-number-dawg)

I think we're flogging a dead horse with this build.

I'll have a go at doing a java build tonight.

Thanks for your assistance.
Major B It's not java, is it? Doh! 
Whatever, I'll try to build from source tonight.
Cheers. @Shreeshrii, the output which you got is not from our [UB Mannheim](https://github.com/UB-Mannheim/tesseract/wiki) build (dated 2017-02-02). I assume that binary was build with `cmake` (which still is buggy for 32 bit Windows).
@wadex1, all builds with `configure` should work with Windows. I just repeated the test with 32 bit Tesseract on 64 bit Windows, and it works well. > the output which you got is not from our UB Mannheim build

I now see what is different: I tested `tesseract --version` while you accidentally called `tesseract -- version` (note the blank between `--` and `version`). Hi

I see there is a new UB Mannheim version (that works!). Thank you.

I still get one instance of "bbox 0 0 2496 3457" (ie page size) on an ocrx_word item. 
(see attached)


[test.hocr.txt](https://github.com/tesseract-ocr/tesseract/files/780304/test.hocr.txt)

Regards Lots of examples in this one. 
[test1.hocr.txt](https://github.com/tesseract-ocr/tesseract/files/780337/test1.hocr.txt)
![test1](https://cloud.githubusercontent.com/assets/25739284/23024243/332fed4a-f451-11e6-8e18-87d61fb5342c.png)

 Interestingly, they are all line segments interpreted as "I" with high word confidence! My previous tests on 64 bit Windows were invalid, because my computer obviously uses an Intel CPU without AVX 2.1 support. All my previous installers include indeed a tesseract.exe which fails with an AVX 2.1 capable CPU (Haswell or newer?). This is fixed in a new installer which I just finished. ![capture](https://cloud.githubusercontent.com/assets/25739284/23043204/7a309cde-f493-11e6-8d27-d08e90712f57.JPG)

It's not "I". It's pipes, underscores and hyphens.  Feedback for best/traineddata is now in tessdata repo.  You could create a PR with these changes. :) Thinking about his, I have never had an easy time doing a new Tesseract release for Debian. Maybe shipping a new version right at the cutoff date is not such a smart idea.

https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=794489
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=816857
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=815056
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=815860
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=815970
 Done: https://github.com/tesseract-ocr/tesseract/releases/tag/3.05.00 Being super cautious, I am comparing symbols from libtesseract.so.3.0.4 and libtesseract.so.3.0.5. Here are symbols that disappeared in 3.0.5.

```
-_Z14WriteParamDescP8_IO_FILEtP10PARAM_DESC@Base
-_Z9read_listPKc@Base
-_ZN16GENERIC_2D_ARRAYIN9tesseract17TrainingSampleSet13FontClassInfoEE6ResizeEiiRKS2_@Base
-_ZN8WERD_RES19FakeWordFromRatingsEv@Base
-_ZN9tesseract11ObjectCacheINS_4DawgEEC1Ev@Base
-_ZN9tesseract11ObjectCacheINS_4DawgEEC2Ev@Base
-_ZN9tesseract13DocumentCache13LoadDocumentsERK13GenericVectorI6STRINGEPKcPFbRKS2_PS1_IcEE@Base
-_ZN9tesseract13DocumentCache15GetPageBySerialEi@Base
-_ZN9tesseract17ViterbiStateEntryD1Ev@Base
-_ZN9tesseract17ViterbiStateEntryD2Ev@Base
-_ZN9tesseract18DawgPositionVectorD1Ev@Base
-_ZN9tesseract18DawgPositionVectorD2Ev@Base
-_ZN9tesseract4Dict4LoadEPNS_9DawgCacheE@Base
-_ZNK9tesseract4Dict19ProcessPatternEdgesEPKNS_4DawgERKNS_12DawgPositionEibPNS_18DawgPositionVectorEP12PermuterType@Base
-_ZNK9tesseract9ImageData8PreScaleEiPP3PixPiS4_P13GenericVectorI4TBOXE@Base
```

[3.0.4.symbols.amd64.txt](https://github.com/tesseract-ocr/tesseract/files/784668/3.0.4.symbols.amd64.txt)
[3.0.5.symbols.amd64.txt](https://github.com/tesseract-ocr/tesseract/files/784670/3.0.5.symbols.amd64.txt) Now I want to test against gimagereader, but can't from my chroot jail. It fails on both `3.0.4` and `3.0.5`

```
$ /usr/bin/gimagereader-gtk
terminate called after throwing an instance of 'std::logic_error'
  what():  basic_string::_M_construct null not valid
Aborted (core dumped)
``` This is the release candidate for 3.0.5 for Debian. I'm going to do a compatibility check with the gimagereader package maintainer. Others are also very welcome to test if they are set up for it.

[tesseract-3.0.5-candidate.zip](https://github.com/tesseract-ocr/tesseract/files/784689/tesseract-3.0.5-candidate.zip)
 EDITED 

Okay, I got gimagereader working for 3.0.4 in the Debian Sid chroot jail.

https://help.ubuntu.com/community/BasicChroot#Accessing_graphical_applications_inside_the_chroot

However, it crashes during recognition on 3.0.5.  This is automatic stop-ship with respect to Debian.

[crashlog.txt](https://github.com/tesseract-ocr/tesseract/files/784709/crashlog.txt)
 I'd like to figure things out such that gimagereader doesn't need to be rebuilt. Upstream was hoping 3.0.5 would be application binary interface (ABI) compatible with 3.0.4. Am I reading the crashlog correctly that the unhappy symbol is RecogAllWordsPassN? Because I don't think that has changed. Is there a preferred ABI compatibility checking tool? I ask because the other approach is fixing compatibility and a 3.0.5.1 that does not bump soname. Hi,

I've heard about this ABI tracker. Maybe it's possible to add tesseract (& leptonica) there somehow.
https://lvc.github.io/abi-compliance-checker/
https://abi-laboratory.pro/tracker/
https://abi-laboratory.pro/tracker/timeline/qt/ (qt example) Is there anything we can do (in tesseract project) to fix this issue?  Terrific, we want that. Thanks.  As I think you know, Ken was already instrumental in our most recent invisible font iteration. Can you confirm that the problems you are seeing are true with HEAD (either the 3.0.5 or 4.x branch) as opposed to something older like 3.0.4? I want to make sure you are working with our the very latest compatibility tweaks to font metrics. Attaching an example document to this bug doesn't hurt. Thanks, that is very clear. I'm always happy to tweak things on the Tesseract side to improve compatibility, but it does require careful testing. The tool of choice is `ttx` from `fonttools` which can transform the font `pdf.ttf` into an editable XML representation and back. I don't think we had anything missing with respect to font metrics, but you never know. Not sure when I might have time to play with this, but anyone is welcome to try. I'm somewhat hesitant to bother Ken more after all his contributions, but maybe that is just shyness. Chrome uses pdfium, Firefox uses pdf.js. Will take a closer look when I get a chance. Thanks for investigating. Only the null character is used. Here's a control vs. experiment for compatibility testing. I took a quick look at Acroread, Chrome, Firefox, evince on Linux and did not notice a difference. Need testing on all the other popular platforms (including the  mobile PDF viewers) to feel comfortable. I'd also like to know exactly what you did when you said "Search ... seems to be broken."

```diff
--- pdf.ttx.orig	2017-02-10 09:35:03.000000000 -0800
+++ pdf.ttx	2017-02-10 09:25:06.000000000 -0800
@@ -122,7 +122,7 @@
 
   <hmtx>
     <mtx name=".notdef" width="0" lsb="0"/>
-    <mtx name=".null" width="0" lsb="0"/>
+    <mtx name=".null" width="1024" lsb="0"/>
   </hmtx>
 
   <cmap>
```

[control.pdf](https://github.com/tesseract-ocr/tesseract/files/767371/2.pdf)

[experiment.pdf](https://github.com/tesseract-ocr/tesseract/files/767375/2.pdf)

 Can someone please test on iOS? Sorry for not being more clear. I need testing of control.pdf against experiment.pdf on iOS before we can submit the change. Thank you very much. Okay, no known regressions, so let's get that revised font (pdf.ttf) in, snapshot the 3.0.5 branch, and ship to millions of users. 
[pdf.ttf.zip](https://github.com/tesseract-ocr/tesseract/files/772035/pdf.ttf.zip)
 done  We do not support 3rd party solution. Please contact author of your solution or use tesseract user forum.  We do not support 3rd party solution. Please contact author of your solution or use tesseract user forum.  Any variables which are read from a file must use a portable data type
with a well defined size.

Signed-off-by: Stefan Weil <sw@weilnetz.de> This is currently only a theoretical problem, because all relevant hosts which I know use a 32 bit `int` data type. Lets keep it in 3.05 branch...  `+` for dropping (in case of better results of lstm engine) It will support vertical text.
I have an experimental implementation that treats it as an additional
language, but it would be possible to make it depend on the layout analysis
instead.

On Wed, Feb 8, 2017 at 6:18 AM, Atsuyoshi SUZUKI <notifications@github.com>
wrote:

> I cannot agree with removing old ocr engine, until new lstm engine has
> support vertical text.
>
> Of course I know that the new LSTM engine is very good ( in Japanese text
> including English words especially).
> In the meantime, maintaining the old engine provides the option of using
> the old OCR engine only for vertical text.
>
> c.f. #627 <https://github.com/tesseract-ocr/tesseract/issues/627> , #641
> <https://github.com/tesseract-ocr/tesseract/issues/641>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/707#issuecomment-278340436>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SgL-wwCYGswOMqxb7FNmr6OIYonks5rac68gaJpZM4L50TV>
> .
>



-- 
Ray.
 If 3.05 should be the last version with legacy OCR Engine (old engine) then there should be possibility to read [OCR result from memory](https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-dev/mZ2IUsvWgbY/2yz-b1cUAgAJ).

Also it would be great if 3.05 and 4.0 version could be installed at the same time (AFAIK there are conflict with tessdata filenames: they are the same but they are not compatible) I would prefer to be as much consistent as possible: e.g. if 3.02 and 3.04 use tessdata also 3.05 should. So 4.0 should start with change... Yes, if later we'll have 5.0 with different data files, they'll use `tesseract5` and this won't break anything.
If we have `tesseract` for 4.0, then it will be renamed to `tesseract4` again, and `tesseract` for 5.0 - that's not good. A simple solution could be using `tessdata/4`, `tessdata/5` and so on for new major versions, so we continue using a `tessdata` directory at the same location as before, but automatically add the major version as the name of a subdirectory. If Tesseract uses semantic versioning in the future, I see no need to add a second number (although that would be possible, resulting in `tesseract/4.0`).

For the program names, we can look for existing examples. I just checked my `/usr/bin/*[0-9]` files and found names like `clang-3.8`, `gcc-6`, `php5`, `php-7.0`, `ruby2.1`. So there is no clear convention whether to separate name and version by a dash or not and whether to use major version only or both major and minor version. With semantic versioning the major version should be sufficient again. I'm thinking of using the same traineddata file format for 4.0, but adding
some new subfiles, including a version string, as has been requested.
The LSTM-only engine would then store the unicharset, recoder and dawgs as
separate traineddata components, also satisfying the need to get at the
unicharset.
With an additional subfile to store the trainer-specific data, it should be
possible use the traineddata file format as a checkpoint format during
training, which gets rid of a layer of complexity.
I had thought of going with a different filename extension, but the
versioned subdir seems like a good idea too.

In any case, we should roll back the existing traineddata files for 3.05.


On Wed, Feb 8, 2017 at 10:54 AM, Stefan Weil <notifications@github.com>
wrote:

> A simple solution could be using tessdata/4, tessdata/5 and so on for new
> major versions, so we continue using a tessdata directory at the same
> location as before, but automatically add the major version as the name of
> a subdirectory. If Tesseract uses semantic versioning in the future, I see
> no need to add a second number (although that would be possible, resulting
> in tesseract/4.0).
>
> For the program names, we can look for existing examples. I just checked
> my /usr/bin/*[0-9] files and found names like clang-3.8, gcc-6, php5,
> php-7.0, ruby2.1. So there is no clear convention whether to separate
> name and version by a dash or not and whether to use major version only or
> both major and minor version. With semantic versioning the major version
> should be sufficient again.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/707#issuecomment-278425371>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056UcVHoxoH9rE_svO5yvu4FrJSrGVks5rag9PgaJpZM4L50TV>
> .
>



-- 
Ray.
 https://wiki.ubuntu.com/ZestyZapus/ReleaseSchedule

Feb 16 is the final deadline for changess to Ubuntu 17.04. I am not comfortable shipping anything from 4.x to these users, but we can consider taking a snapshot of the 3.0.5 branch. It does have some bug and compatibility fixes that are good for users. Regarding training data, I would not ship an update that at all. This would be purely be a code update.

I know the long standing issue has been restoring an API call (last seen in version 3.0.2) to send results to memory instead of file. I respect that idea, but we don't have it, and it's not that easy to add.  I think it is fair to say that it would be impossible before deadline. So the question is, do we ship an update to users this cycle or not. And if so, should I take a snapshot? And if so, what would it be called? 

A few more thoughts that are somewhat related

- I see no reason that this has to be the last ever release on the 3.0.x branch.
- My guess is by the next next release in Oct 2017 that 4.x will be ready for the vast majority of users
- I'm not planning to ship both 3.0.x and 4.x at the same time with Debian/Ubuntu. I think it will be very rare for people to want both, and those who do will be advanced users who can work from source code.


 So there should be changes in 4.0 code so tesseract 4.x and 3.05.x could be installed at the same time Yes, who wants old engine could use 3 series when lstm will be available in 4. I can confirm all problems reported above by @uvius. In addition, some training files currently only exist for 3.x (notably deu_frak) or have a bad quality (deu), so 4.0 does not improve the results for those languages.

I also had an example where a larger part of a page was missing in the output from LSTM while the old recognizer got most of that part correctly, but I am still searching to find that example again. I recently tried to improve the training model for a language (`frk`). That's rather easy and does not need much compute time (~ minutes) or other resources for the old engine. Especially adding more characters which should be recognized is a simple task as soon as the general infrastructure (Tesseract binaries, small number of fonts) is available.

For the new LSTM engine, this is totally different. As far as I know it is not possible to add a missing character to an existing trained LSTM, so new training from scratch is needed. This requires a lot of resources (much more training text, a huge number of fonts, compute time ~days / ~weeks) and cannot be done by most users. Maybe @theraysmith or users who have successfully trained LSTM can provide more detailed numbers.

My conclusion is that most users of the new LSTM will be restricted to the available trained data either from @theraysmith or from third parties. If the old engine is removed, it will no longer be possible to optimize OCR for documents with unusual or rare characters. Calling Tesseract with more than one language can only partially solve such situations. LSTM currently does not work with all languages (see issue #682). That's related to my previous comment: adding (good) LSTM support for a language is much more difficult than for the old engine. Of course the existing languages will be fixed one day, but there still remain more exotic languages which are not covered today, and people won't be able to add them to Tesseract. We could tell users to use Tesseract 3.x for those cases, but would that really save development resources when there is the need to maintain both versions 3 and 4? It seems clear that having two major versions of Tesseract requires more work for Linux distributions. Neither is it a good solution for users who have to install and use both versions and don't know what to do when texts require both versions. I believe I am on a path to make the LSTM engine work with *many*
languages, and possibly unseen languages in the same script.
I agree that training from scratch is much more difficult than for the old
engine, but I think the obtainable accuracy makes it worth leaving the old
engine behind.
I think that the fine tuning and/or replacing just one layer training may
be adequate for adding new fonts or new characters, with a bit more work on
my part.
A big part of my desire to drop the old engine is that it would enable a
much better solution to multi-lang/multi-script and plug-replaceable
language models. While the legacy engine remains in place, there is a lot
of work-around to do to integrate the LSTM engine that does not fit with my
ideas for fixing this problem properly.

On Wed, Mar 15, 2017 at 2:42 AM, Stefan Weil <notifications@github.com>
wrote:

> LSTM currently does not work with all languages (see issue #682
> <https://github.com/tesseract-ocr/tesseract/issues/682>). That's related
> to my previous comment: adding (good) LSTM support for a language is much
> more difficult than for the old engine. Of course the existing languages
> will be fixed one day, but there still remain more exotic languages which
> are not covered today, and people won't be able to add them to Tesseract.
> We could tell users to use Tesseract 3.x for those cases, but would that
> really save development resources when there is the need to maintain both
> versions 3 and 4? It seems clear that having two major versions of
> Tesseract requires more work for Linux distributions. Neither is it a good
> solution for users who have to install and use both versions and don't know
> what to do when texts require both versions.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/707#issuecomment-286690362>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Qz-VC6VezMBvacI2nBojn_NJ8FAks5rl7JqgaJpZM4L50TV>
> .
>



-- 
Ray.
  done Looks ok on web interface:

![tmp](https://user-images.githubusercontent.com/5095331/29661044-80c31798-88e0-11e7-9f7b-e9c8ceb9b8bf.png)
  Hi, for what platform are you trying to build opencv.text?

You could try to build it using CPPAN (https://cppan.org/). 
I'm using opencv.text on windows. All is fine.
https://cppan.org/pvt.cppan.demo.intel.opencv.contrib.text/version/3.2.0

To build it with cppan, download the client and run
`cppan --build 	pvt.cppan.demo.intel.opencv.contrib.text-3.2.0`.  Known problem. Root cause is PDF spec which forces heuristics into text extraction, and Preview is well known to have some of the wonkiest heuristics. Definitely not related to opencl.  @yoyoshuang I don't think tesseract allows for that from command line. There maybe some config variables, but I have no idea about it. > failed to write checkpoint

The output directory needs to be created before running the lstmtraining command.
If directory does not exist, you will get this error - see eg. below

```
2 Percent improvement time=63, best error was 100 @ 0
At iteration 63/100/100, Mean rms=0.718%, delta=2.212%, char train=5.292%, word train=17.556%, skip ratio=0%,  
New best char error = 5.292 
Transitioned to stage 1 
failed  to write best model:../tesstutorial/san_deva/san_deva5.292_63.checkpoint 
failed to write checkpoint.

```  Post your PDF please.  Not tested, but in general should be ok.
I remember some issues with libtiff dependency, but I fixed something, so 100% not sure. I've added a section with simple tess library build.
https://github.com/tesseract-ocr/tesseract/wiki/Compiling#windows It depends on what your are trying to do.
If you need to build user-app, follow one of these guides 
https://github.com/cppan/tesseract_example/tree/master/
https://github.com/cppan/tesseract_example/tree/master/with_cppan
 @hanikh Please do not complain here that some tutorial outside of this project does not work for you. (Especially when you decide to ignore instructions provided here on github)  How did you install tesseract and on what operating system?  Also are you just talking about the warning message or its something else going wrong as well? The best thing for bug reports is to attach your input data (and try to make the smallest, simplest example that reproduces the problem) along with exact commands used, and the output data. For support questions, use the forum. I'm going to close this bug now, but please re-open this bug if you think you've found a problem.

Note that the particular warning you saw will go away in a future Tesseract, and will be replaced with one that says something like "Unknown image resolution, making a guess". Which is hopefully a little more clear. PS. You can use jhove to validate JPEG 2000 files.  The latest version of the code does not read lstmf files created prior to Ray's recent commits.

```
Deserialize header failed: ~/tesstutorial/tess4training-vedic/san.Akshar_Unicode.exp-2.lstmf
[Thread 0x7ffff2bb3700 (LWP 8327) exited]
[New Thread 0x7fffef194700 (LWP 8328)]
Deserialize header failed: ~/tesstutorial/tess4training-vedic/san.Aksharyogini2.exp0.lstmf
[New Thread 0x7ffff2bb3700 (LWP 8329)]
Load of page 0 failed!
Load of images failed!!
[Thread 0x7fffef194700 (LWP 8328) exited]

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ffff2bb3700 (LWP 8329)]
0x00007ffff70be464 in truncate (size=0, this=0x30790e8) at ../ccutil/genericvector.h:497
497           delete GenericVector<T*>::data_[i];
(gdb) backtrace
#0  0x00007ffff70be464 in truncate (size=0, this=0x30790e8) at ../ccutil/genericvector.h:497
#1  tesseract::DocumentData::ReCachePages (this=0x30790e0) at imagedata.cpp:523
#2  0x00007ffff70be839 in tesseract::ReCachePagesFunc (data=<optimized out>) at imagedata.cpp:371
#3  0x00007ffff419d184 in start_thread (arg=0x7ffff2bb3700) at pthread_create.c:312
#4  0x00007ffff6390ffd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
```
 Ok. Closing the issue.  Please use tesseract user forum for asking support/questions.  Here you are
https://www.dropbox.com/s/obiqvrt4m53pmoz/tesseract-4.0.0-alpha.zip?dl=1 There is also a new installer in our [wiki](https://github.com/UB-Mannheim/tesseract/wiki). > There is also a new installer in our wiki.

Now an updated version which includes PR #698 is available. It no longer tries to use AVX. We currently build only installers with 32 bit executables (which work also on 64 bit Windows like yours). Building both 32 and 64 bit installers is possible, but requires more resources. No, we did not try training on Windows. The problem which you observe could be related to several issues which I noticed while working on Tesseract 4 for big endian machines, more concretely on wrong assumptions about the size of an enum variable. You can compare binary files created on Windows with files created on Linux using `cmp -l`. I'd expect that the Linux files have some additional 0 bytes, so they are a little bit larger.  Yes. Please, download cppan client and run `cppan --build pvt.cppan.demo.google.tesseract-master` First, provide what output you get from those commands. 1. build them - Yes (Also turn on that check box.)
2. Select tesseractmain project in Solution Explorer - in the context menu set it as default project. This is because of early error of cppan.
Try to open `cppan.yml` in some editor and save with unix line endings (LF).
Also try to upgrade cppan `cppan --self-upgrade`. notepad++  Sounds to me like we should retire **hocr_font_info**, and remove the related HOCR output code.  AFAICT I have fixed the issues with Khmer training text. It remains to be seen whether the results of training are improved.  I think the cause of all of these is the precision-recall tradeoff that takes place in linerec.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/ccmain/linerec.cpp#L313

The underlying question is, if there is a word that is almost certainly incorrect, would it be better to have it with the error, or have it disappear?

Historically Tesseract has not dropped words in such a way, but when I tried it, I found that almost every word dropped was incorrect anyway, so the word-level precision was improved without hurting recall.
Character-level recall OTOH **is** reduced by this word-dropping, since the rest of the characters in a word are usually correct.

I might disable the word dropping altogether, or maybe it would work better for deleting garbage if most of the characters are bad instead of just the worst.

While I am looking at this though, I am not convinced that the unicharset and/or compression are applied correctly to Kannada, which might explain its rather stubborn refusal to improve in accuracy. >>The underlying question is, if there is a word that is almost certainly incorrect, would it be better to have it with the error, or have it disappear?

>I do not think it should disappear. However, if the word is almost certainly incorrect, then it should be marked in some easy way for users to fix the OCRed txt.

For `hOCR` output, the answer is simple: output the word and add the information on the certainty as additional information (I think this is already one). For other formats which don't support such additional information, a special textual mark might be considered, but I consider that a very special case (so no special handling would be fine for me, too).  I made a discovery yesterday that the web-derived text corpus for Kannada
is missing ZWNJ, which AFAICT is an *essential* unicode character in
Kannada.
The same applies to other Indic languages, although the use varies, and
some use ZWJ as well.

I'm still working on this and investigating where they are lost.
The implications for fixing it could be higher accuracy in several
languages, although I don't know by how much, as I haven't measured the
frequency of ZWNJ in my test sets.

On Fri, Feb 17, 2017 at 12:45 AM, Shreeshrii <notifications@github.com>
wrote:

> More Kannada OCR related papers:
>
> http://mile.ee.iisc.ernet.in/mile/publications/softCopy/
> DocumentAnalysis/Madhav_SPCOM2014.pdf
>
> http://mile.ee.iisc.ernet.in/mile/publications/softCopy/
> DocumentAnalysis/Nethra_ICFHR2010_Data.pdf
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/681#issuecomment-280590084>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056SQADDz1OYsnSygJmWFUA7rCatPiks5rdV4PgaJpZM4LuojH>
> .
>



-- 
Ray.
 See https://github.com/tesseract-ocr/tesseract/issues/664 for details about dropped words during Devanagari recognition.
 >I think the cause of all of these is the precision-recall tradeoff that takes place in linerec.cpp
https://github.com/tesseract-ocr/tesseract/blob/master/ccmain/linerec.cpp#L313

@theraysmith Are there any config values I can change so that words are not dropped? Ok, I changed some constants and now the words are being dropped rarely.

@theraysmith Is this the right approach? It will be good if these values can be changed via config variables rather than needing recompile to test different values.

I changed the following:

https://github.com/tesseract-ocr/tesseract/blob/a1c22fb0d0f6bde165ec7b7c3125420b0ba1d541/lstm/recodebeam.cpp#L32

changed from -20.0f to -50.0f
```
// Clipping value for certainty inside Tesseract. Reflects the minimum value
// of certainty that will be returned by ExtractBestPathAsUnicharIds.
// Supposedly on a uniform scale that can be compared across languages and
// engines.
const float RecodeBeamSearch::kMinCertainty = -50.0f;
```
https://github.com/tesseract-ocr/tesseract/blob/master/ccmain/linerec.cpp#L36-40

changed from 5.0f to 0.0f
```
// Arbitarary penalty for non-dictionary words.
// TODO(rays) How to learn this?
const float kNonDictionaryPenalty = 0.0f;
```

changed from -25.0f to -99.0f
```
// Worst acceptable certainty for a dictionary word.
const float kWorstDictCertainty = -99.0f;
```

 @theraysmith  

>Will changes in training also support scanned box/tiff pairs?

In addition to training for unknown fonts by using scanned box/tiff pairs, they would also be useful for 'printing conventions' which may not be in concert with the current unicode conventions.

See attached image, where the anusvar is printed before the reph whereas the valid rendering would display anusvar later. Training by using only synthetic images and valid cases will not train for such cases. Currently these words get dropped during recognition.

![srisubodhini00vall_0013](https://user-images.githubusercontent.com/5095331/28249585-288ba9ec-6a76-11e7-99ad-e4763829ed33.png)
 @theraysmith 

Do the changes so far address the missing text / dropped words issue? Should I test these or wait for new models?  Please try master and report if problem is fixed.  Please use tesseract user forum for asking questions/support  @amitdo is correct. unicharset_extractor doesn't read the WordStr box file format.
Sorry this is an un-tested path.
Furthermore, it isn't just a case of modifying unicharset_extractor.
For the Indic languages, the unicharset needs to know the syllable/grapheme clusters, and it can't get that from the Wordstr box file format. The best it can do is extract the unicodes used in the WordStr box file or you start with an existing unicharset for that training path. Related - https://github.com/tesseract-ocr/tesseract/issues/832 @theraysmith Please also see https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/Xu4_aOCFhlQ/Yb2G59zTAgAJ about 
Training Tesseract4.0 (LSTM) on word level bounding boxes

Will this be addressed when you update the unicharset_extractor?

I am wondering whether there is a way to use the `text2image` algorithm to create box files given image and ground truth files.   v3.0.4 works, v4.0.0 fails

```
$ echo phototest.tif > manifest
$ tesseract manifest - -
This is a lot of 12 point text to test the
ocr code and see if it works on all types
of file format.

The quick brown dog jumped over the
lazy fox. The quick brown dog jumped
over the lazy fox. The quick brown dog
jumped over the lazy fox. The quick
brown dog jumped over the lazy fox.
```

```
$ echo phototest.tif > manifest
$ tesseract manifest - -
Error during processing.
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a69316a0 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatalstm-punc-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a694a140 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatalstm-word-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a6dc2650 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatalstm-number-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a6dc25c0 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatapunc-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a6de0260 still has count 1 (id third_party/tesseract/tessdata/eng.traineddataword-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a69314f0 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatanumber-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a6de1e80 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatabigram-dawg)
ObjectCache(0xb408678)::~ObjectCache(): WARNING! LEAK! object 0x7ff6a6de1d60 still has count 1 (id third_party/tesseract/tessdata/eng.traineddatafreq-dawg)
``` Here's a fix. I don't know if 3.0.5 is busted or not (if I had to guess, yes)

```diff
--- api/baseapi.cpp	2016-12-15 15:28:37.000000000 -0800
+++ api/baseapi.cpp	2017-01-18 15:03:52.000000000 -0800
@@ -1083,7 +1083,15 @@
 
   // Maybe we have a filelist
   if (r != 0 || format == IFF_UNKNOWN) {
-    STRING s(buf.c_str());
+    STRING s;
+    if (stdInput) {
+      s = buf.c_str();
+    } else {
+      std::ifstream t(filename);
+      std::string u((std::istreambuf_iterator<char>(t)),
+                    std::istreambuf_iterator<char>());
+      s = u.c_str();
+    }
     return ProcessPagesFileList(NULL, &s, retry_config,
                                 timeout_millisec, renderer,
                                 tesseract_->tessedit_page_number);
``` thanks!  Closing this and linking to issue #681  Using Image linked above https://cloud.githubusercontent.com/assets/5095331/22055988/c65e0f96-dd83-11e6-9f06-bea70dd85be6.png

```
Best choice certainty=-3.09489, space=-0.195364, scaled=-21.6642, final=-21.6642
 : शूण्वन्तु : R=13.0464, C=-3.09489, F=1, Perm=2, xht=[0,3.40282e+38], ambig=0
pos     NORM    NORM    NORM    NORM    NORM
str     शू      ण्      व       न्      तु
state:  1       1       1       1       1
C       -3.095  -0.260  -0.219  -0.298  -0.195
Deleting word with certainty -21.6642
 : शूण्वन्तु : R=13.0464, C=-21.6642, F=1, Perm=2, xht=[0,3.40282e+38], ambig=0
pos     NORM    NORM    NORM    NORM    NORM
str     शू      ण्      व       न्      तु
state:  1       1       1       1       1
C       -3.095  -0.260  -0.219  -0.298  -0.195
Best choice certainty=-1.30628, space=-0.195364, scaled=-9.14397, final=-9.14397
 : क्रषय: : R=5.64425, C=-1.30628, F=1, Perm=2, xht=[0,3.40282e+38], ambig=0
pos     NORM    NORM    NORM    NORM    NORM
str     क्      र       ष       य       :
state:  1       1       1       1       1
C       -1.306  -0.262  -0.229  -0.195  -0.204
```

The correct words are 

```
शृण्वन्तु 
श ृ ण् व न् तु 
```
and
```
ऋषयः 
ऋ ष य ः
```
  I think the patch is good and only requires a small clean-up (removing the `__MINGW32__` test).
It builds fine with clang on macOS (tested with [MacPorts](https://www.macports.org/)).

I also suggest to squash all commits before applying them. @jbarlow83, is [this commit](https://github.com/stweil/tesseract/commit/fa677f1ba183c6265fb11a14328dca6b8dd34368) fine for you? If yes, I'd make a pull request for it. I kept the original date and you as the author.  I'm very reluctant to make Tesseract PDF generation fancy. I wonder if we can do an image swap like this outside of Tesseract, using one of the PDF manipulation toolkits. Really? That's interesting, qpdf is very well written. Maybe the right thing to do is allow Tesseract to produce a multi-page PDF with invisible symbolic text PDF only, no images. Then another tool (perhaps an enhanced qpdf tool) would merge and composite two PDFs together. One being the original image-only PDF, and the other an invisible-text-only  PDF. What do you think, @jbarlow83? Please point me at the relevant qpdf API calls if you happen to know them.

 This sounds reasonable to me. I'll try to find time over this coming week to make an experimental invisible-text-only PDF that we can play with. All the other pieces of the puzzle are there; for example Leptonica already ships with a images->pdf tool that avoids transcoding for PNG, JP2K, and JPEG. It would be cool to use qpdf for the merge step because it is already so useful for linearizing. But it's great that there are more options. The qpdf author is extremely friendly in my experience, in case we eventually chat with him. Oh, I now vaguely remember that PDFBox had something for merging as well, but I've never tried it and can't find it at the moment. Here's an experimental PDF pair, image-only and text-only. Let the merging begin!

[images.pdf](https://github.com/tesseract-ocr/tesseract/files/712576/images.pdf)
[text.pdf](https://github.com/tesseract-ocr/tesseract/files/712628/text.pdf)

 This works brilliantly. I will implement for real if someone promises that they will use it. Also, what do we call the configuration option? My best idea so far to describe a PDF that has invisible text only is 'naked'. I'm sure someone has a better idea. 

```
$ time pdftk text.pdf multibackground images.pdf output full.pdf
real	0m0.253s
```

Actually this works better the other way around, for preserving the bookmarks and things like that.

```
 pdftk  images.pdf multibackground text.pdf output full.pdf
```
 Implementation complete and under review by Ray. @jbarlow83 this is a good time to look at the samples above and make sure they meet your needs.

```
tesseract -c naked_pdf=true HelloWorld.png HelloWorld pdf
``` Spectral writing. Perhaps a kind of ghost script, if you will. Hmmm, an invisible text layer, invisible text, let's see ...  iText? Anyway, I'll pick something. There is zero chance that a PDF rasterizer will ever be part of Tesseract or Leptonica. In theory one could write an PDF image extractor for Leptonica, but there isn't really enough motivation to do so.  Ray will eventually merge this patch, but it is hard to predict when. I am posting here for anyone who is impatient or excited.

```diff
--- api/pdfrenderer.cpp	2016-12-13 14:43:24.000000000 -0800
+++ api/pdfrenderer.cpp	2017-01-19 14:50:56.000000000 -0800
@@ -178,10 +178,12 @@
  * PDF Renderer interface implementation
  **********************************************************************/
 
-TessPDFRenderer::TessPDFRenderer(const char* outputbase, const char *datadir)
+TessPDFRenderer::TessPDFRenderer(const char *outputbase, const char *datadir,
+                                 bool textonly)
     : TessResultRenderer(outputbase, "pdf") {
   obj_  = 0;
   datadir_ = datadir;
+  textonly_ = textonly;
   offsets_.push_back(0);
 }
 
@@ -326,7 +328,11 @@
   pdf_str.add_str_double("", prec(width));
   pdf_str += " 0 0 ";
   pdf_str.add_str_double("", prec(height));
-  pdf_str += " 0 0 cm /Im1 Do Q\n";
+  pdf_str += " 0 0 cm";
+  if (!textonly_) {
+    pdf_str += " /Im1 Do";
+  }
+  pdf_str += " Q\n";
 
   int line_x1 = 0;
   int line_y1 = 0;
@@ -832,6 +838,7 @@
 bool TessPDFRenderer::AddImageHandler(TessBaseAPI* api) {
   size_t n;
   char buf[kBasicBufSize];
+  char buf2[kBasicBufSize];
   Pix *pix = api->GetInputImage();
   char *filename = (char *)api->GetInputName();
   int ppi = api->GetSourceYResolution();
@@ -840,6 +847,9 @@
   double width = pixGetWidth(pix) * 72.0 / ppi;
   double height = pixGetHeight(pix) * 72.0 / ppi;
 
+  snprintf(buf2, sizeof(buf2), "XObject << /Im1 %ld 0 R >>\n", obj_ + 2);
+  const char *xobject = (textonly_) ? "" : buf2;
+
   // PAGE
   n = snprintf(buf, sizeof(buf),
                "%ld 0 obj\n"
@@ -850,19 +860,18 @@
                "  /Contents %ld 0 R\n"
                "  /Resources\n"
                "  <<\n"
-               "    /XObject << /Im1 %ld 0 R >>\n"
+               "    %s"
                "    /ProcSet [ /PDF /Text /ImageB /ImageI /ImageC ]\n"
                "    /Font << /f-0-0 %ld 0 R >>\n"
                "  >>\n"
                ">>\n"
                "endobj\n",
                obj_,
-               2L,            // Pages object
-               width,
-               height,
-               obj_ + 1,      // Contents object
-               obj_ + 2,      // Image object
-               3L);           // Type0 Font
+               2L,  // Pages object
+               width, height,
+               obj_ + 1,  // Contents object
+               xobject,   // Image object
+               3L);       // Type0 Font
   if (n >= sizeof(buf)) return false;
   pages_.push_back(obj_);
   AppendPDFObject(buf);
@@ -899,13 +908,15 @@
   objsize += strlen(b2);
   AppendPDFObjectDIY(objsize);
 
-  char *pdf_object;
-  if (!imageToPDFObj(pix, filename, obj_, &pdf_object, &objsize)) {
-    return false;
+  if (!textonly_) {
+    char *pdf_object = nullptr;
+    if (!imageToPDFObj(pix, filename, obj_, &pdf_object, &objsize)) {
+      return false;
+    }
+    AppendData(pdf_object, objsize);
+    AppendPDFObjectDIY(objsize);
+    delete[] pdf_object;
   }
-  AppendData(pdf_object, objsize);
-  AppendPDFObjectDIY(objsize);
-  delete[] pdf_object;
   return true;
 }
 

--- api/renderer.h	2016-11-07 07:44:03.000000000 -0800
+++ api/renderer.h	2017-01-19 14:50:56.000000000 -0800
@@ -186,7 +186,7 @@
  public:
   // datadir is the location of the TESSDATA. We need it because
   // we load a custom PDF font from this location.
-  TessPDFRenderer(const char *outputbase, const char *datadir);
+  TessPDFRenderer(const char* outputbase, const char* datadir, bool textonly);
 
  protected:
   virtual bool BeginDocumentHandler();
@@ -196,20 +196,20 @@
  private:
   // We don't want to have every image in memory at once,
   // so we store some metadata as we go along producing
-  // PDFs one page at a time. At the end that metadata is
+  // PDFs one page at a time. At the end, that metadata is
   // used to make everything that isn't easily handled in a
   // streaming fashion.
   long int obj_;                     // counter for PDF objects
   GenericVector<long int> offsets_;  // offset of every PDF object in bytes
   GenericVector<long int> pages_;    // object number for every /Page object
   const char *datadir_;              // where to find the custom font
+  bool textonly_;                    // skip images if set
   // Bookkeeping only. DIY = Do It Yourself.
   void AppendPDFObjectDIY(size_t objectsize);
   // Bookkeeping + emit data.
   void AppendPDFObject(const char *data);
   // Create the /Contents object for an entire page.
-  static char* GetPDFTextObjects(TessBaseAPI* api,
-                                 double width, double height);
+  char* GetPDFTextObjects(TessBaseAPI* api, double width, double height);
   // Turn an image into a PDF object. Only transcode if we have to.
   static bool imageToPDFObj(Pix *pix, char *filename, long int objnum,
                           char **pdf_object, long int *pdf_object_size);

--- api/tesseractmain.cpp	2016-12-15 15:28:37.000000000 -0800
+++ api/tesseractmain.cpp	2017-01-19 14:50:56.000000000 -0800
@@ -337,8 +337,10 @@
 
     api->GetBoolVariable("tessedit_create_pdf", &b);
     if (b) {
-      renderers->push_back(
-          new tesseract::TessPDFRenderer(outputbase, api->GetDatapath()));
+      bool textonly;
+      api->GetBoolVariable("textonly_pdf", &textonly);
+      renderers->push_back(new tesseract::TessPDFRenderer(
+          outputbase, api->GetDatapath(), textonly));
     }
 
     api->GetBoolVariable("tessedit_write_unlv", &b);

--- ccmain/tesseractclass.cpp	2017-01-19 11:57:09.000000000 -0800
+++ ccmain/tesseractclass.cpp	2017-01-19 18:15:57.000000000 -0800
@@ -391,6 +391,8 @@
                   this->params()),
       BOOL_MEMBER(tessedit_create_pdf, false, "Write .pdf output file",
                   this->params()),
+      BOOL_MEMBER(textonly_pdf, false, "Invisible text only for PDF",
+                  this->params()),
       STRING_MEMBER(unrecognised_char, "|",
                     "Output char for unidentified blobs", this->params()),
       INT_MEMBER(suspect_level, 99, "Suspect marker level", this->params()),

--- ccmain/tesseractclass.h	2017-01-19 11:57:09.000000000 -0800
+++ ccmain/tesseractclass.h	2017-01-19 16:31:04.000000000 -0800
@@ -1027,6 +1027,7 @@
   BOOL_VAR_H(tessedit_create_hocr, false, "Write .html hOCR output file");
   BOOL_VAR_H(tessedit_create_tsv, false, "Write .tsv output file");
   BOOL_VAR_H(tessedit_create_pdf, false, "Write .pdf output file");
+  BOOL_VAR_H(textonly_pdf, false, "Invisible text only for PDF");
   STRING_VAR_H(unrecognised_char, "|",
                "Output char for unidentified blobs");
   INT_VAR_H(suspect_level, 99, "Suspect marker level");
``` pdfimages from poppler-utils will do image extraction as well. And pdfium offers API calls for image extraction. I am sure there are many others. Have fun. merged to master.
@Wikinaut: try master now. I clearly did not test well enough to find the embarrassing /XObject bug. Thank you for finding and fixing that @jbarlow83. Regarding capi.h and capi.cpp, the development branch that I am sharing with @theraysmith  doesn't have those files. I don't know what the story is with that, maybe Ray does. @Wikinaut, there are problems with "send one image through OCR and insert a different image in the output PDF". Image management becomes difficult, especially considering TIFF and PDF are multipage formats.  On top of that, certain image formats require transcoding. I think the textonly_pdf approach makes a building block that plays well with other tools, and is the right way to go. It is super simple to implement, and is especially well suited for turning scanned pdf into to searchable pdf while preserving metadata. That's something that increases in importance every single day.
 There is a much simpler way to achieve the same results. Remember, for most image formats Tesseract will not mangle the images in any way when creating a PDF file.

1. Extract the images from the PDF file (don't render!). For this example, we'll assume jpeg.
2. `ls *.jpg | tesseract - result pdf`   

Now it is a little more complicated if you want what was described in the top level comment. Which is to OCR a different image than what ends up in the PDF file. For that, it would look like this.

1. Extract the images from the PDF file (don't render!)
2. Merge them into an image-only PDF, using something like `converttopdf` from `leptonica-progs`
3. Apply your favorite image processing operations to the extracted images
4. Generate a text-only PDF from the extracted images
5. Merge your image-only and text-only PDF

Bottom line, you want your images completely unmolested during this process. No format conversion, no uncompress + decompress cycles. Nothing. Hands off. And it will work for most normal starting points. And honestly, you can almost certainly outsource all these details to software written by @jbarlow83  A image-only PDF file is a bag of images. If the bag is holding a bunch of JPEG images, extract them as-is. Don't convert. Don't recompress. Just empty the PDF bag and get your images out. If it is holding JPEG2000, then just get those out. Same with PNG. Let's shift this discussion back to the forum. Please re-ask your most recent question there; I don't follow exactly what you are asking. C-API should be fixed now. Thanks for finding this wikinaut.
@jbreiden capi.cpp and capi.h are C-API for tesseract that is used for tesseract wrappers (python etc.)
@Wikinaut as pointed by Jeff, please move back this discussion to [tesseract user forum](https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/vvMldrkcuOQ/3cyAq4jSAQAJ). Yes. The final solution was to implement `tesseract -c textonly_pdf=1` > The textonly_pdf parameter is only available on the HEAD (4.00)

@zdenop Please backport for 3.05. Thanks! done. Thanks, @zdenop. Please also make a 3.05.01 release with the latest commit in 3.05 branch so that all these enhancements are easily accessible.  This kind of retraining would be desirable, but is not available.

In your case, you don't need it though, as 4.00 works for all the examples
of "für" that you provided.
You just need to make sure you are using the latest code and data.

As Amit points out e diaresis is not in the German alphabet. I successfully
correctly got
"Citroën"
 by using fra+deu as the language. Unfortunately, it doesn't work with
deu+fra, and neither works for the 2nd example.
BTW this needed a bug fix for multi-language, which I will check in soon.

On Mon, Jan 16, 2017 at 9:26 AM, Wikinaut <notifications@github.com> wrote:

> @theraysmith <https://github.com/theraysmith> You appear to be the expert
> for answering my question, if such a procedure for re-training (tesseract +
> LSTM) is easily possible, or not:
>
> (I described it already above:)
>
> Can I "quickly" retrain my "deu" (or "deu+eng") training data (or a copy
> of it) with a corrected text ?
>
>    - in.pdf -> tesseract -> out.txt
>    - out.txt -> manually corrected -> *corrected.txt*
>    - retraining tesseract (to get tesseract' )with these inputs: in.pdf +
>    *corrected.txt*
>
> re-running re-trained tesseract should in the best case result in
>
>    - in-pdf -> tesseract' -> corrected.txt
>
> I found but do not (yet) understand the present training explanations in
> the Wiki, and perhaps is my idea not yet covered.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/659#issuecomment-272920831>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056ZP1oiFo1bk3xNLf3lGGBiL7G-ODks5rS6hbgaJpZM4LigRu>
> .
>



-- 
Ray.
 Some of the problems with German texts were addressed in https://github.com/tesseract-ocr/langdata/pull/54, https://github.com/tesseract-ocr/langdata/pull/56 and https://github.com/tesseract-ocr/langdata/pull/57. I don't know whether those fixes are sufficient to improve future trainings. I addressed the more general question whether all European languages should support all typical diacritical characters in the [tesseract-dev](https://groups.google.com/forum/#!topic/tesseract-dev/8H_4K3vPRJE) forum and need information from @theraysmith to proceed. > I successfully correctly got "Citroën" by using fra+deu as the language.

I expect that using additional languages has more side effects than recognizing additional characters, because they also add word list, unigram frequencies, word bigrams and so on for that languages which might have a negative effect on OCR results for texts which are mainly written in a single language but make sparely use of additional languages. Examples of such texts are German texts with foreign person or trade mark names, but also English scientific texts with additional Greek characters (a combination often used in mathematics and physics). @Wikinaut 

>I found certain groups of ocr failures in my scan case, two examples which were always wrongly detected
"Citroén" instead of the original word "Citroën"
"fiir" instead of "für"

Does it work now with best traineddata?

Can I close this issue? I have not tried the latest version. Pls. let this open - I will close it, if it's solved. @stweil I now use the new https://github.com/tesseract-ocr/tessdata_best data, and found that a problem with lowercase vs. uppercase "s" exists, in a 1000-page text, 

typical incorrectly detected word patterns are:

* "Sich" instead of "sich"
* "Sie" instead of "sie"
* "Sagte" instead of "sagte"
* "Sagen" instead of "sagen"
* "Sah" instead of "sah"
* "ICh" instead of "Ich"
* "80" instead of "so"
 Please see https://github.com/tesseract-ocr/tesseract/issues/960

I guess, you can make the following two config variables as false to not load the wordlist dawg files.

load_system_dawg     T
load_freq_dawg       T Those config variables related to the legacy engine. New traineddata files have a different lstm-word-dawg and have no freq-dawg files. So, I am not sure whether they will work. I haven't tried it yet. It is not that the wordlist is different, but the fact that the legacy engine and LSTM models might be using different unicharsets.

The creation and unpacking of dawgs requires unicharsets, that's why there are two sets of dawg files, even for numbers and punctuation, in addition to the wordlist.  Diacritics often get separated into their own text lines. Not so good as you might think?
Aren't the 3 yellow lines near the top and the 3 orange lines at the bottom
supposed to be different colors?
I think they have been fused into one line.

On Mon, Jan 16, 2017 at 9:20 PM, Shreeshrii <notifications@github.com>
wrote:

> I tried arabic_lines with both arabic diacritics and devanagari sample and
> it is marking the texlines well. Results attached.
> [image: result-arabic-diacritics]
> <https://cloud.githubusercontent.com/assets/5095331/22008592/a8101138-dca2-11e6-8d85-a0cbcc078304.png>
> [image: result-deva]
> <https://cloud.githubusercontent.com/assets/5095331/22008595/a814004a-dca2-11e6-99e6-26cedd4bc4e3.png>
> [image: textlines-arabic-diacritics]
> <https://cloud.githubusercontent.com/assets/5095331/22008594/a8129854-dca2-11e6-848e-4378a24beb26.png>
> [image: textlines-deva]
> <https://cloud.githubusercontent.com/assets/5095331/22008593/a811e328-dca2-11e6-9f39-977c3942e622.png>
>
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/657#issuecomment-273025064>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Up0oVZPWQ2YcPpA-pm4Ju1d_k_lks5rTE-BgaJpZM4LiGKh>
> .
>



-- 
Ray.
 Thanks for the information. Please take a look at the attached unicharset,
and let me know if you see any deficiencies. I notice that ZWJ and ZWNJ are
not there, but 202c(Pop directional formatting) is.
It seems to contain all the diacritics that you mentioned.

On Fri, May 5, 2017 at 7:39 AM, bmwmy <notifications@github.com> wrote:

> I would like to give these hints to the developers:
> In Arabic there are to kinds of diacritics
> 1- letter attached diacritics (dots like ب ت ج ث and أ آ ؤ) which stick to
> the letter and is mandatory
> 2. Vowel diacritics like ( ْ ّ َ ً ِ ٍ ) used with letters any letter can
> be conjunct/combined with it and is optional. Kids learn it to read
> properly as it help get rid of ambiguity, because عَلم and عِلم are two
> different words but we use the context to distinguish when vowel diacritics
> are absent.
>
> N.B.
> لَاْ إِلَهَ إٍلا الله note that this َ ِ are different vowels same shape
> exactly but used differently e.g. أَ is pronounced a while ِأ pronounced e.
> one used above letter latter used below letter.
>
> bottom line: vowel diacritics in Arabic should be recognized alone (e.g
> separate box) because it can be on any letter and is limited ( ّ َ ً ُ ٌ ِ
> ٍ ْ ) special case also this ّ can be conjunct/combined with other vowel
> diacritics also ًّ ّْ
>
> hope this could help Tesseract developers
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/657#issuecomment-299482582>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056byXKSTYnwgO3_kA5fGf_x4WGKp8ks5r2zSLgaJpZM4LiGKh>
> .
>



-- 
Ray.
 @theraysmith 

>  Please take a look at the attached unicharset,

No file was attached.

Please also see https://github.com/tesseract-ocr/tesseract/issues/894#issue-226872462

  use tesseract user forum for asking support  That sounds like its working as intended.  The LSTM recognizer is currently trained to recognize the sequence of *unicodes* for Indic languages. This reduces the size of the output softmax of the network from the 5000+ elements in the unicharset to ~140. (There is an analogous process for Chinese, Japanese, and Korean, that doesn't use the unicode encoding, but it is a similar idea, and the codes are strictly limited in length.)
The unicharset is used as a *filter* in the beam search to allow only sensible grapheme/syllable combinations of unicodes, so it doesn't output complete garbage text.

The consequence of this recoding is that it runs a lot faster, but it has to learn to output a long sequence for each grapheme/syllable.
The recoding system that maps from unicharset elements to the sequence of unicodes currently only allows a maximum of 9 unicodes per grapheme/syllable, including any viramas.

I'm running a new training experiment this weekend to try a new coding scheme, in which `<virama><consonant>` pairs are mapped to a single code, allowing a long CVCVCVC string to be encoded using just CCCC, cutting down from 7 codes to 4. This will probably increase the size of the output softmax to ~170, but reduce the length of the average code sequence by about 1/3, which might be easier for it to learn, without slowing it down much.

It will take a couple of weeks to tell if it works, but if it does I will check in the code, and upload new traineddatas, and close this issue. If it doesn't work, I will have to think again... The text corpus is from *all* the www, taken several years ago, plus more
recent data from wiki-something.
The text is divided by language automatically, so there is a separate
stream for each of the Devanagari-based languages (as there is for the
Latin-based languages) and clipped to 1GB for each language.
For each language, the text is frequency counted and cleaned by multiple
methods, and sometimes this cleaning is too stringent automatically, or not
stringent enough, so forbidden_characters and desired_characters are used
as a guide in the cleanup process. There are other lang-specific numbers
like a 1-in-n discard ratio for the frequency.
For some languages, the amount of data produced at the end is very thin.

The unicharset is extracted from what remains, and the wordlist that is
published in langdata.
For the LSTM training, I resorted to using Google's parallel infrastructure
to render enough text in all the languages.
However much or little corpus text there is, the rendering process makes
50000 chunks of 50 words to render in a different combination of font and
random degradation, which results in 400000-800000 rendered textlines.
The words are chosen to approximately echo the real frequency of conjunct
clusters (characters in most languages) in the source text, while also
using the most frequent words.

This process is all done without significant manual intervention, but
counts of the number of generated textlines indicates when it has gone
badly, usually due to a lack of fonts, or a lack of corpus text.
I recently stopped training chr, iku, khm, mya after discovering that I
have no rendered textlines that contain anything other than digits and
punctuation.

Community input is therefore extremely useful, and usually results in edits
to forbidden_characters and desired_characters, which in turn guides the
filtration process.
Community-provided corpus text would be useful for languages that have very
little or no training data, given appropriate copyright/licensing clearance.
The languages with very little corpus text are:
bih
chr
dzo
iku
snd
syr
tgk
tir
so these are likely to have poor recognition accuracy.

On Sat, Jan 21, 2017 at 7:46 AM, Shreeshrii <notifications@github.com>
wrote:

> Ray,
>
> Thank you for explaining regrading unicharset compression and your new
> strategy for Indic graphemes.
>
> Since the unicharset is being used as a filter, it will be important to
> include the most common conjunct clusters in it, which may differ from
> language to language.
>
> Some more questions
>
> Are the desired_characters and forbidden_characters used in the process of
> creating the text corpus for different languages?
>
> How many text lines are you using for training of Devanagari, e.g.
> Sanskrit, Hindi, Marathi etc. Is it all/only from Wikipedia?
>
>
>
> - excuse the brevity, sent from mobile
>
> On 21-Jan-2017 3:34 AM, "theraysmith" <notifications@github.com> wrote:
>
> > The LSTM recognizer is currently trained to recognize the sequence of
> > *unicodes* for Indic languages. This reduces the size of the output
> > softmax of the network from the 5000+ elements in the unicharset to ~140.
> > (There is an analogous process for Chinese, Japanese, and Korean, that
> > doesn't use the unicode encoding, but it is a similar idea, and the codes
> > are strictly limited in length.)
> > The unicharset is used as a *filter* in the beam search to allow only
> > sensible grapheme/syllable combinations of unicodes, so it doesn't output
> > complete garbage text.
> >
> > The consequence of this recoding is that it runs a lot faster, but it has
> > to learn to output a long sequence for each grapheme/syllable.
> > The recoding system that maps from unicharset elements to the sequence of
> > unicodes currently only allows a maximum of 9 unicodes per
> > grapheme/syllable, including any viramas.
> >
> > I'm running a new training experiment this weekend to try a new coding
> > scheme, in which pairs are mapped to a single code, allowing a long
> CVCVCVC
> > string to be encoded using just CCCC, cutting down from 7 codes to 4.
> This
> > will probably increase the size of the output softmax to ~170, but reduce
> > the length of the average code sequence by about 1/3, which might be
> easier
> > for it to learn, without slowing it down much.
> >
> > It will take a couple of weeks to tell if it works, but if it does I will
> > check in the code, and upload new traineddatas, and close this issue. If
> it
> > doesn't work, I will have to think again...
> >
> > —
> > You are receiving this because you authored the thread.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/654#
> issuecomment-274192153>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/AE2_o-xusyCIFbh-
> wE4T4cp4mVb4oBWWks5rUS9vgaJpZM4LhbNY>
> > .
>
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/654#issuecomment-274269267>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056XOUmyQKlAM4aHUJc-jTRmhEwWOxks5rUihVgaJpZM4LhbNY>
> .
>



-- 
Ray.
 Update: after going back to the www to get fresh data, I believe that my corpus text is now good for:
chr
dzo
iku
snd
syr
tgk
tir
I have put a lot of time into cleaners/filters for languages that use 'virama' characters.
I am not convinced that they are perfect, but I will add the code to the github repo in due course, so experts/native speakers can offer suggestions/fixes to make them better. Myanmar in particular needs improvement, as the www data is littered with dotted circles, and the unicode book does not adequately describe the syntax for a well-formed grapheme in Myanmar (or any other language for that matter).
 @theraysmith You wrote in january:

>The LSTM recognizer is currently trained to recognize the sequence of unicodes for Indic languages. This reduces the size of the output softmax of the network from the 5000+ elements in the unicharset to ~140. (There is an analogous process for Chinese, Japanese, and Korean, that doesn't use the unicode encoding, but it is a similar idea, and the codes are strictly limited in length.)

In recent trainings, I still see large unicharsets (eg, with ALL akshara combinations from the training_text in Devanagari). 

```
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Setting properties for script Devanagari
Setting properties for script Han
Unichar 1945=र्त्स्न्ये->र्त्स्न्ये is too long to encode!!
Warning: given outputs 105 not equal to unicharset of 3784.
```

Depending on the training text, this number can go as high as 6-7000. I thought the intention was to reduce this number.

Also, when training with hin.lstm as the starting point for replace top layer training, while the original .lstm file is about 8 MB, the intermediate .lstm files are about 80 MB and the _checkpoint file is about 160MB.

Is this to be expected or is something wrong with the training process? >I'm running a new training experiment this weekend to try a new coding scheme, in which <virama><consonant> pairs are mapped to a single code, allowing a long CVCVCVC string to be encoded using just CCCC, cutting down from 7 codes to 4. This will probably increase the size of the output softmax to ~170, but reduce the length of the average code sequence by about 1/3, which might be easier for it to learn, without slowing it down much.

@theraysmith Did the above approach work?

In https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/7Building%20a%20Multi-Lingual%20OCR%20Engine.pdf, you have desscribed what's a character in Devanagari and used the following example:

rdvika - र्द्विक - 0930 094D 0926 094D 0935 093F 0915

I would actually split the above as two aksharas, each ending in the either the implicit a or a maatraa or a combining mark.

So the above would be:

rdvi - र्द्वि - 0930 094D 0926 094D 0935 093F 
ka - क -  0915

To reduce the various akshara combinations, i would suggest splitting 
CVCVCVC - consonant clusters
and
Maatra and Combining marks separately

eg. possible combinations with ka (and these do not include the combining vedic accents!!)

```
क का कि की कु कू कृ के कै को कौ कँ काँ किँ कीँ कुँ कूँ कृँ केँ कैँ कोँ कौँ कं कां किं कीं कुं कूं कृं कें कैं कों कौं कः काः किः कीः कुः कूः कृः केः कैः कोः कौः 
```
Imagine these for every consonant cluster with every vowel sign (or matra), other signs like candrabindu, anusvara and visarga to each combination. the number of combinations will be HUGE. 

By splitting consonant cluster part separately from maatraa and other signs combination, the number of combinations can be cut down drastically.

```
   ा  ि  ी  ु  ू  ृ  े  ै  ो  ौ  ँ  ाँ  िँ  ीँ  ुँ  ूँ  ृँ  ेँ  ैँ  ोँ  ौँ  ं  ां  िं  ीं  ुं  ूं  ृं  ें  ैं  ों  ौं  ः  ाः  िः  ीः  ुः  ूः  ृः  ेः  ैः  ोः  ौः 
```
and consonants and consonant clusters such as 
```
क ख ग घ ङ च छ ज झ ञ ट ठ ड ढ ण त थ द ध न प फ ब भ म य र ल व श ष स ह ळ
क्क क्क्य क्ख क्ख्य क्त क्त्य क्त्र क्त्र्य क्त्व क्थ क्न क्न्य क्प क्प्य क्फ क्फ्य क्म क्म्य क्य क्र क्र्य क्ल क्ल्य क्व क्व्य
क्ष क्ष्ण क्ष्म क्ष्म्य क्ष्य क्ष्व क्स ख्य ग्द ग्द्य ग्ध ग्ध्य ग्ध्व ग्न ग्न्य ग्ब ग्ब्र ग्भ ग्भ्य ग्म ग्म्य ग्य ग्र ग्र्य ग्ल ग्व
घ्न घ्म घ्य घ्र ङ्क ङ्क्त ङ्क्ष ङ्क्ष्व ङ्ख ङ्ख्य ङ्ग ङ्ग्य ङ्ग्र ङ्ग्र्य ङ्घ ङ्घ्र ङ्ङ ङ्म ङ्स
च्च च्छ च्छ्य च्छ्र च्छ्व च्म च्य छ्य छ्र छ्र्य छ्व ज्ज ज्ज्ञ ज्ज्व ज्झ ज्ञ ज्ञ्य ज्म ज्य ज्र ज्व
ञ्च ञ्च्य ञ्छ ञ्छ्र ञ्ज ञ्ज्म ञ्ज्य ञ्झ ञ्श
ट्क ट्ट ट्य ट्व ट्स ठ्य ठ्र ड्र ड्ड ड्य ढ्य ढ्र ढ्व ण्ट ण्ठ ण्ड ण्ढ ण्ण ण्म ण्य ण्व
त्क त्त त्त्य त्त्र त्त्व त्थ त्न त्न्य त्प त्प्र त्प्र्य त्फ त्म त्म्य त्य त्र त्र्य त्व त्स त्स्र त्स्र्य त्स्य त्स्व
थ्य द्ग द्ग्र द्द द्द्य द्द्र द्द्व् द्ध द्ध्य द्ध्व द्ध्व्य द्न द्न्य द्ब द्ब्र द्भ द्भ्य द्म द्य द्र द्र्य द्व् द्व्य द्व्र ध्न ध्म ध्य ध्र ध्व
न्त न्त्य न्त्र न्त्स न्थ न्द न्द्ध न्द्र न्द्व् न्ध न्ध्य न्ध्र न्ध्व न्न न्न्य न्प न्प्र न्फ न्म न्य न्व न्स
प्त प्त्य प्त्र प्न प्प प्फ प्म प्य प्र प्ल प्स फ्य ब्घ ब्ज ब्द ब्ध ब्ध्व ब्ब ब्भ ब्य ब्र ब्न भ्य ब्र भ्व
म्न म्प म्प्र म्ब म्ब्य म्भ म्ब्र म्म म्य म्र म्ल
य्य य्व ल्क ल्ग ल्प ल्म ल्य ल्ल ल्व ल्ह व्य व्र व्व श्च श्च्य श्न श्न्य श्म श्य श्र श्र्य श्ल श्व श्व्य श्श
ष्क ष्क्र ष्ट ष्ट्य ष्ट्र ष्ट्र्य ष्ट्व ष्ठ ष्ठ्य ष्ठ्र ष्ण ष्ण्य ष्प ष्प्र ष्म ष्य ष्व
स्क स्क्र स्ख स्त स्त्य स्त्र स्त्व स्थ स्थ्य स्र स्प स्प्र स्फ स्म स्म्य स्य स्र स्व स ह्न ह्म ह्य ह्र ह्ल ह्व
```
and (with reph)

```
 र्क र्ख र्ग र्घ र्ङ र्च र्छ र्ज र्झ र्ञ र्ट र्ठ र्ड र्ढ र्ण र्त र्थ र्द र्ध र्न र्प र्फ र्ब र्भ र्म र्य र्र र्ल र्व र्श र्ष र्स र्ह र्ळ
 र्क्क र्क्क्य र्क्ख र्क्ख्य र्क्त र्क्त्य र्क्त्र र्क्त्र्य र्क्त्व र्क्थ र्क्न र्क्न्य र्क्प र्क्प्य र्क्फ र्क्फ्य र्क्म र्क्म्य र्क्य र्क्र र्क्र्य र्क्ल र्क्ल्य र्क्व र्क्व्य
 र्क्ष र्क्ष्ण र्क्ष्म र्क्ष्म्य र्क्ष्य र्क्ष्व र्क्स र्ख्य र्ग्द र्ग्द्य र्ग्ध र्ग्ध्य र्ग्ध्व र्ग्न र्ग्न्य र्ग्ब र्ग्ब्र र्ग्भ र्ग्भ्य र्ग्म र्ग्म्य र्ग्य र्ग्र र्ग्र्य र्ग्ल र्ग्व
 र्घ्न र्घ्म र्घ्य र्घ्र र्ङ्क र्ङ्क्त र्ङ्क्ष र्ङ्क्ष्व र्ङ्ख र्ङ्ख्य र्ङ्ग र्ङ्ग्य र्ङ्ग्र र्ङ्ग्र्य र्ङ्घ र्ङ्घ्र र्ङ्ङ र्ङ्म र्ङ्स
 र्च्च र्च्छ र्च्छ्य र्च्छ्र र्च्छ्व र्च्म र्च्य र्छ्य र्छ्र र्छ्र्य र्छ्व र्ज्ज र्ज्ज्ञ र्ज्ज्व र्ज्झ र्ज्ञ र्ज्ञ्य र्ज्म र्ज्य र्ज्र र्ज्व
 र्ञ्च र्ञ्च्य र्ञ्छ र्ञ्छ्र र्ञ्ज र्ञ्ज्म र्ञ्ज्य र्ञ्झ र्ञ्श
 र्ट्क र्ट्ट र्ट्य र्ट्व र्ट्स र्ठ्य र्ठ्र र्ड्र र्ड्ड र्ड्य र्ढ्य र्ढ्र र्ढ्व र्ण्ट र्ण्ठ र्ण्ड र्ण्ढ र्ण्ण र्ण्म र्ण्य र्ण्व
 र्त्क र्त्त र्त्त्य र्त्त्र र्त्त्व र्त्थ र्त्न र्त्न्य र्त्प र्त्प्र र्त्प्र्य र्त्फ र्त्म र्त्म्य र्त्य र्त्र र्त्र्य र्त्व र्त्स र्त्स्र र्त्स्र्य र्त्स्य र्त्स्व
 र्थ्य र्द्ग र्द्ग्र र्द्द र्द्द्य र्द्द्र र्द्द्व् र्द्ध र्द्ध्य र्द्ध्व र्द्ध्व्य र्द्न र्द्न्य र्द्ब र्द्ब्र र्द्भ र्द्भ्य र्द्म र्द्य र्द्र र्द्र्य र्द्व् र्द्व्य र्द्व्र र्ध्न र्ध्म र्ध्य र्ध्र र्ध्व
 र्न्त र्न्त्य र्न्त्र र्न्त्स र्न्थ र्न्द र्न्द्ध र्न्द्र र्न्द्व् र्न्ध र्न्ध्य र्न्ध्र र्न्ध्व र्न्न र्न्न्य र्न्प र्न्प्र र्न्फ र्न्म र्न्य र्न्व र्न्स
 र्प्त र्प्त्य र्प्त्र र्प्न र्प्प र्प्फ र्प्म र्प्य र्प्र र्प्ल र्प्स र्फ्य र्ब्घ र्ब्ज र्ब्द र्ब्ध र्ब्ध्व र्ब्ब र्ब्भ र्ब्य र्ब्र र्ब्न र्भ्य र्ब्र र्भ्व
 र्म्न र्म्प र्म्प्र र्म्ब र्म्ब्य र्म्भ र्म्ब्र र्म्म र्म्य र्म्र र्म्ल
 र्य्य र्य्व र्ल्क र्ल्ग र्ल्प र्ल्म र्ल्य र्ल्ल र्ल्व र्ल्ह र्व्य र्व्र र्व्व र्श्च र्श्च्य र्श्न र्श्न्य र्श्म र्श्य र्श्र र्श्र्य र्श्ल र्श्व र्श्व्य र्श्श
 र्ष्क र्ष्क्र र्ष्ट र्ष्ट्य र्ष्ट्र र्ष्ट्र्य र्ष्ट्व र्ष्ठ र्ष्ठ्य र्ष्ठ्र र्ष्ण र्ष्ण्य र्ष्प र्ष्प्र र्ष्म र्ष्य र्ष्व
 र्स्क र्स्क्र र्स्ख र्स्त र्स्त्य र्स्त्र र्स्त्व र्स्थ र्स्थ्य र्स्र र्स्प र्स्प्र र्स्फ र्स्म र्स्म्य र्स्य र्स्र र्स्व र्स र्ह्न र्ह्म र्ह्य र्ह्र र्ह्ल र्ह्व
```
 Please see pages 48-75 of http://www.sanskritweb.net/itrans/itmanual2003.pdf
for the varied rendition of consonant cluster ligatures in different Devanagari fonts.  An encoded lstm recognizer (or trainer) includes its own unicharset.
There is currently no easy way to get the unicharset out to use it with the lstm dawg files.
Is this necessary?
If so, how should it be done? tesstrain_utils.sh needs to copy lang.unicharset as lang.lstm-unicharset, similar to the dawg files.

Then it can be added and extracted from the traineddata.

```
combine_tessdata -u hin.traineddata   hinnew.
Extracting tessdata components from hin.traineddata
Wrote hinnew.config
Wrote hinnew.lstm
Wrote hinnew.lstm-punc-dawg
Wrote hinnew.lstm-word-dawg
Wrote hinnew.lstm-number-dawg
Wrote hinnew.lstm-unicharset
Wrote hinnew.version
Version string:4.00.00alpha
0:config:size=739, offset=192
17:lstm:size=8874565, offset=931
18:lstm-punc-dawg:size=4322, offset=8875496
19:lstm-word-dawg:size=73098, offset=8879818
20:lstm-number-dawg:size=114, offset=8952916
21:lstm-unicharset:size=127137, offset=8953030
23:version:size=12, offset=9080167
``` Also missing is, 22:lstm-recoder Coming soon is a program that will take as input:
 a unicharset,
 optional list of wordlists (words, puncs, numbers)
 command-line flags to control the recoder

and outputs a traineddata containing the lstm uincharset, recoder, and the
dawgs if wordlists were supplied.

The resultant traineddata will be input to lstmtraining, in place of a
unicharset, thus achieving independence of the unicharset and recoder from
the lstm trainer. The trained lstm model will be added to the traineddata
by the --stop_training flag.

This is the change in training that I was talking about Friday.

On Sat, Jul 15, 2017 at 3:55 AM, Shreeshrii <notifications@github.com>
wrote:

> Also missing is, 22:lstm-recoder
>
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/653#issuecomment-315526522>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056a2iTeHmAhkTsCYhmLfBOsbZMnrBks5sOJqRgaJpZM4LhL-s>
> .
>



-- 
Ray.
 @theraysmith  Will changes in training also support scanned box/tiff pairs,  preferably with lineboxes having just the groundtruth for the whole line followed by tab character for EOL rather than the split into characters/aksharas as proposed by the WordStr format. Yes, the change is only in the setup of the language model components.
The training process will still take lstmf files, and they will still be
able to be created from tif.box pairs using either character or Wordstr
format box files.

On Sun, Jul 16, 2017 at 3:11 AM, Shreeshrii <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith> Will changes in training
> also support scanned box/tiff pairs using the proposed Wordstr format?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/653#issuecomment-315599304>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056ThVwWjh8kBXxDCe2bAHCO6z-yaBks5sOeHlgaJpZM4LhL-s>
> .
>



-- 
Ray.
 Is Wordstr format supported with the new commits?

It would be helpful if you can provide a sample Wordstr format box file for Hindi? Thanks!  It means you are posting issue in wrong project. This is tesseract OCR.  Added this to the training instructions:
`<Undecodable>` can occur in either the ALIGNED_TRUTH or OCR TEXT output early
in training. It is a consequence of unicharset compression and CTC training.
(See Unicharset Compression and train_mode above). This should be harmless and
can be safely ignored. Its frequency should fall as training progresses.
  There *is* RTL-specific processing in text2image.
Pango renders RTL text RTL, and the post-processing is intended to re-order
everything strictly LTR.
*If it isn't doing that then there is a bug.*

Here is why:
Old Tesseract (3.05) is only learning individual character shapes, so the
order of training data is somewhat irrelevant.
New Tesseract (LSTM/4.00) is learning to identify the text characters in
the (LTR) order they appear in the image, so the truth transcription should
be LTR, and the words in the dawg should be reversed.
There is Bidi processing inside the post-recognition processing of
Tesseract that reprocesses/re-orders the text for output, so it appears in
the correct order.
I wonder if the bidi integration is working correctly for LSTM, as the
accuracy with Arabic is unsatisfactory.

In light of this design, please take another look, and let me know if there
is anything systematically wrong in the output that might provide some
hints as to where to look.
Thanks.

On Wed, Jan 11, 2017 at 5:08 AM, Amit D. <notifications@github.com> wrote:

> What you show here is 'by design'. This should not cause any problem in
> training process and characters recognition for RTL languages.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/648#issuecomment-271864082>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056RlP7ZpsIBdJ7ooGiEN1KuDDG54Kks5rRNRggaJpZM4Lf-kT>
> .
>



-- 
Ray.
 Thanks for the work on this.
I've learned 2 things so far:

   - U+640 (tatweel) is a very special case that I need to think how to
   handle - it needs to be preserved in the training text, so it gets
   rendered, but needs to disappear everywhere else, as if it doesn't exist.
   - The diacritics are currently excluded from the unicharset, probably
   because they are only rarely used, but need to be included. There may not
   be enough text with them included in the text corpora.


Questions:
Is that all of U+64b->U+652 inclusive?
Applied uniformly to all Arabic languages?
(ara+div+fas+kur_ara+pus+snd+syr+uig+urd)

On Wed, Jan 11, 2017 at 7:30 AM, Shreeshrii <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith> Hope you have seen comments
> by Chris on the other thread also - #642
> <https://github.com/tesseract-ocr/tesseract/issues/642>
>
> i was merging the letter extender with the Arabic letter into one single
> box, and putting that Arabic letters as the character of the box,
> basically, i was trying to train the engine to recognize that Arabic letter
> in it's multiple positions, as you know the Arabic letters have multiple
> forms based which is based on it's position in the word ( beginning,
> middle, ending, isolated )
> Example:
> ( كـ ) is not ( ك + ـ ) in the box file, it should be ( ك )
> also ( ـكـ ) or ( ـك ) they are a single character ( ك ) in different
> positions, this is important in the box file.
>
> Which also means that ( كَـ ) is not ( ك + ـَ ), it is ( كَ )
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/648#issuecomment-271898975>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056fQCae8sI16xZmZURI7_cApxZrHzks5rRPWmgaJpZM4Lf-kT>
> .
>



-- 
Ray.
 @amitdo Hebrew seems to be OK. It is certainly ahead of 3.05.
I have some detailed results, but they aren't very meaningful without being able to look at the actual errors to see how many are actually due to the ground truth, or some strange disagreement on whitespace.
The gist is that 4.00 is less good than it could be on:
Arabic (all langs)
Indic (all langs)
Chinese, Japanese.
The problems with Arabic may be explained by this thread.
Chinese, Japanese are troubled by the used of radical-stroke encoding. I need to switch to a better scheme.
Indic may be troubled by the length of the compressed codes used. I need to switch to a better scheme. Thanks for forwarding that @Shreeshrii, there is a bug there that needs to
be fixed.

On Fri, Mar 10, 2017 at 2:25 AM, Shreeshrii <notifications@github.com>
wrote:

> https://groups.google.com/forum/?utm_medium=email&utm_
> source=footer#!msg/tesseract-ocr/EOwF1GnOcS0/My_SUf1vEQAJ
>
> tesseract4 reads الأ as األ which is pretty close, because we need to
> switch the position of the last 2 letters to have ا ل أ, this happens with
> similar word forms too like لا reads as ال and should be ل ا, and i wish to
> correct it.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/648#issuecomment-285633162>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056TBSuVaAYXREd1MJEUZz-KfW_b_pks5rkSUQgaJpZM4Lf-kT>
> .
>



-- 
Ray.
 I've started training with a fix for this.

On Fri, Mar 31, 2017 at 7:00 AM, Amit D. <notifications@github.com> wrote:

> Here are some libraries that implement the Unicode Bidi algorithm.
>
> https://github.com/behdad/pybyedie
> Copyright (C) 2013 Google
> License: MIT
> Written in: Python
>
> https://github.com/servo/unicode-bidi
> By Mozilla
> License: Apache 2.0 / MIT
> Written in: Rust
>
> https://github.com/behdad/fribidi
> License: LGPL 2.1
> Written in: C
>
> https://github.com/MeirKriheli/python-bidi
> License: LGPL 3.0
> Written in: Python
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/648#issuecomment-290720020>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056WfXs2mhFutZnpOnUujEFyxXRg_Oks5rrQcTgaJpZM4Lf-kT>
> .
>



-- 
Ray.
 I have! At the second attempt, I found the cause to be that unicodes within a ligature were not reversed.
There are some changes to the code (probably early next week) to fix this, and the dropped last space on each line of RTL text output, (is there an issue for that, or am I the only one that noticed?), and the corresponding traineddatas will be coming soon too. Please help me understand your comment on diacritics.
I understand that tatweel is a rendering artifact, that should be rendered
for training, but not occur in the output text (or in the language model).

My previous understanding of vowels in Hebrew and Arabic, is that they are
diacritical marks that are optional, and used mainly for disambiguation,
children's text and in historical text.

Are you referring to some different diacritics other than vowels, or are
you suggesting that vowels should never appear, or that some different
diacritics exist that should never appear in the output text?

Here are some examples of test data with diacritics:


Truth: שֶׁהוּא נָס מִפָּנָיו, אָמַר לוֹ מֹשֶה: כָּל הַיּוֹם הָיִיתִי אוֹמֵר
לְךָ בְּשֵׁם הַקֹּדֶשׁ וְלֹא הָיִיתָ
OCR: שָהוּא נס מִפָּנִיו, אָמַר לו משָה: כָּל הַיום הָיִיתי אוּמר לֶך
בָּשַם הקדש ולא הָיִיתָ
Confs: 0.84 0.56 0.64 0.93 0.96 0.77 0.88 0.76 0.63 0.64 0.54 0.45 0.91
0.88 0.58
Diff: שָהוּא נָס מִפָּנִיו, אָמַר לוֹ מֹשָה: כָּל הַיּוֹם הָיִיתִי אוּמֵר לֶ
ךָ בָּשַם הַקֹּדֶשׁ וְלֹא הָיִיתָ
Recall Errors = 12
Precision Errors = 2



Truth: ותופחים בבטנים ובשירים שארכם כארך סיגריה,
OCR: וְתופחים בַּבַּטָנִיס וּבשירים שאַרכֶּם כַארְף סִיגְרִיה,
Confs: 0.71 0.71 0.91 0.8 0.56 0.56
Diff: וְתופחים בַּבַּטָנִיס וּבשירים שאַרכֶּם כַארְף סִיגְרִיה,
Recall Errors = 6
Precision Errors = 1



Truth: له يزيد الرجال وركبوا الطريقَ فلم يشعر بهم العدو حتى ركبوا
OCR: له يزيج الرجال وركبوز الطريق فلم يشعر بهم العدر حتى ركبو
Confs: 0.82 0.9 0.96 0.4 0.9 0.96 0.93 0.97 0.64 0.94 0.71
Diff: له يزيج الرجال وركبوز الطريقَ فلم يشعر بهم العدر حتى ركبوا
Recall Errors = 5

In all these cases, tesseract gets a poor result.
In case 1, the diacritics are in the truth text, and Tesseract gets them
badly wrong.
In case 2, the diacritics are NOT in the truth text, and Tessseract
suggests some anyway.
I don't think that both of these truth texts can be "correct" in the sense
that one has the diacritics and the other does not. Which way should it be
and why?
In case 3 (Arabic) there are diacritics in the truth text (or are these not
what you are talking about?) and Tesseract does badly anyway.

The tatweel and ligature problem are fixed and will be corrected in the new
traineddatas coming soon, but it will have the above errors in.
According to my measurements, accuracy on Hebrew/Yiddish and Arabic/Farsi
are much improved, but still behind most of the LTR scripts.
If there is still something systematically wrong, your comments could help
further improve it!
Thanks,

On Wed, Jul 26, 2017 at 8:48 AM, chris <notifications@github.com> wrote:

> @theraysmith <https://github.com/theraysmith> @Shreeshrii
> <https://github.com/shreeshrii>
> Note that Arabic & Hebrew diacritics have the same issue of the U+640
> (tatweel), therefore I suggest that they need to be preserved in the
> training text so that they get rendered, but need to disappear everywhere
> else as if they doesn't exist.
> Currently, when introducing diacritics to a training model, the
> recognition rate falls drastically, thus I suggest Playing it Safe for
> now, they must be rendered, but disappear elsewhere.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/648#issuecomment-318095244>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056U2hHxhVyRranKuDmk3b59uCZ3eIks5sR1-pgaJpZM4Lf-kT>
> .
>



-- 
Ray.
 @theraysmith 

fyi - https://github.com/tesseract-ocr/tesseract/issues/892#issuecomment-318063065 http://blogs.transparent.com/arabic/2-arabic-diacritics-al-tashkeel-%D8%A7%D9%84%D9%80%D8%AA%D9%80%D8%B4%D9%80%D9%83%D9%80%D9%8A%D9%80%D9%80%D9%80%D9%84/

Good explanation with examples.  https://github.com/tesseract-ocr/tesseract/commit/3e63918f9db4150a3d1ff6136df9b753e507ae41

Please test with latest source from github. The commit by @theraysmith  fixes the issue. @theraysmith 

Thanks for these updates.

Does the complete fix for RTL languages also require new traineddata created with these fixes? 

Will you be uploading a new version of traineddata? @hanikh You are right, Ray had uploaded the best traineddata files on August 1.

However, I think that the wordlists in traineddata for RTL languages in that still had some errors in order of characters in ligatures.

I am hoping that @theraysmith will upload fixed versions of traineddata to the new repos for LSTM traineddata - 

https://github.com/tesseract-ocr/tessdata_best
and
https://github.com/tesseract-ocr/tessdata_fast  Please use tesseract user forum for asking questions, support...  @theraysmith Any update on this? @stweil Have you come across this problem while training? Any solution? Please try the following making appropriate changes for your fonts directory and tessdata directory

```
training/tesstrain.sh \
  --fonts_dir /c/Windows/Fonts \
  --tessdata_dir ./tessdata \
  --training_text ../langdata/ara/ara.training_text \
  --langdata_dir ../langdata \
  --lang ara  \
  --linedata_only \
  --noextract_font_properties \
  --exposures "0"    \
  --fontlist "Arial" \
  --output_dir ~/tesstutorial/aratest
  
training/tesstrain.sh \
  --fonts_dir /c/Windows/Fonts \
  --tessdata_dir ./tessdata \
  --training_text ../langdata/ara/ara.training_text \
  --langdata_dir ../langdata \
  --lang ara  \
  --linedata_only \
  --noextract_font_properties \
  --exposures "0"    \
  --fontlist "Arial" \
  "Arial Unicode MS" \
  "Calibri" \
  "Courier New" \
  --output_dir ~/tesstutorial/araeval

mkdir -p ~/tesstutorial/aratuned_from_aratest 

combine_tessdata -e ../tessdata/ara.traineddata \
  ~/tesstutorial/aratuned_from_aratest/ara.lstm
  
lstmtraining \
  --continue_from ~/tesstutorial/aratuned_from_aratest/ara.lstm \
  --train_listfile ~/tesstutorial/aratest/ara.training_files.txt \
  --eval_listfile ~/tesstutorial/araeval/ara.training_files.txt \
  --model_output ~/tesstutorial/aratuned_from_aratest/aratuned \
  --target_error_rate 0.01 
  ```

On windows using the binaries from appveyor artifacts, I am getting

```
$ lstmtraining \
>   --continue_from ~/tesstutorial/aratuned_from_aratest/ara.lstm \
>   --train_listfile ~/tesstutorial/aratest/ara.training_files.txt \
>   --eval_listfile ~/tesstutorial/araeval/ara.training_files.txt \
>   --model_output ~/tesstutorial/aratuned_from_aratest/aratuned \
>   --target_error_rate 0.01
Segmentation fault
```

I will check with tesseract built under WSL and report separately. I think crash happens in builds with debug. Non-debug builds get some error messages but continue. ```
(gdb) run
Starting program: /usr/local/bin/lstmtraining --continue_from /home/shree/tesstutorial/aratuned_from_aratest/ara.lstm --train_listfile /home/shree/tesstutorial/aratest/a
ra.training_files.txt --eval_listfile /home/shree/tesstutorial/araeval/ara.training_files.txt --model_output /home/shree/tesstutorial/aratuned_from_aratest/aratuned --ta
rget_error_rate 0.01
warning: Error disabling address space randomization: Success
warning: linux_ptrace_test_ret_to_nx: PTRACE_KILL waitpid returned -1: Interrupted system call
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Loaded file /home/shree/tesstutorial/aratuned_from_aratest/aratuned_checkpoint, unpacking...
Successfully restored trainer from /home/shree/tesstutorial/aratuned_from_aratest/aratuned_checkpoint
[New Thread 0x7f4e34110700 (LWP 24)]
Loaded 81/81 pages (1-81) of document /home/shree/tesstutorial/aratest/ara.Arial.exp0.lstmf
[Thread 0x7f4e34110700 (LWP 24) exited]
[New Thread 0x7f4e34110700 (LWP 25)]
Loaded 81/81 pages (1-81) of document /home/shree/tesstutorial/araeval/ara.Arial.exp0.lstmf
[Thread 0x7f4e34110700 (LWP 25) exited]
[New Thread 0x7f4e33900700 (LWP 26)]
Loaded 81/81 pages (1-81) of document /home/shree/tesstutorial/araeval/ara.Arial_Unicode_MS.exp0.lstmf
[Thread 0x7f4e33900700 (LWP 26) exited]
[New Thread 0x7f4e33900700 (LWP 27)]
[New Thread 0x7f4e34110700 (LWP 28)]
[New Thread 0x7f4e325c0700 (LWP 29)]
[New Thread 0x7f4e315a0700 (LWP 30)]
lstmtraining: ../ccutil/genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.

Program received signal SIGABRT, Aborted.
[Switching to Thread 0x7f4e315a0700 (LWP 30)]
0x00007f4e36ff6c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace
#0  0x00007f4e36ff6c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f4e36ffa028 in __GI_abort () at abort.c:89
#2  0x00007f4e36fefbf6 in __assert_fail_base (fmt=0x7f4e371403b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x4f48b8 "index >= 0 && index < size_used_", file=file@entry=0x4f489a "../ccutil/genericvector.h", line=line@entry=713,
    function=function@entry=0x4f55a0 <GenericVector<char>::operator[](int) const::__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
    at assert.c:92
#3  0x00007f4e36fefca2 in __GI___assert_fail (assertion=0x4f48b8 "index >= 0 && index < size_used_", file=0x4f489a "../ccutil/genericvector.h", line=713,
    function=0x4f55a0 <GenericVector<char>::operator[](int) const::__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
#4  0x0000000000406bef in GenericVector<char>::operator[] (this=0x7ffffd882960, index=0) at ../ccutil/genericvector.h:713
#5  0x000000000043d308 in tesseract::LSTMTrainer::ReadTrainingDump (this=0x7f4e3159f650, data=..., trainer=0x7f4e3159f650) at lstmtrainer.cpp:921
#6  0x000000000040b8cb in tesseract::LSTMTester::RunEvalSync (this=0x7ffffd8828f0, iteration=24, training_errors=0x7ffffd882fb8, model_data=..., training_stage=1)
    at lstmtester.cpp:86
#7  0x000000000040bc2f in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7ffffd8828f0) at lstmtester.cpp:123
#8  0x00007f4e37de8184 in start_thread (arg=0x7f4e315a0700) at pthread_create.c:312
#9  0x00007f4e370ba37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb)
``` @Stweil Unrelated to this issue, one of the recent commits seems to have caused the program to slow down a lot - while training it seems to hang for a while and even OCRing images seems to take longer. I am not sure how to verify/confirm this. @stweil I don't see any patch here. >#6  0x000000000040b8cb in tesseract::LSTMTester::RunEvalSync (this=0x7ffff40f2d00, iteration=0, training_errors=0x7ffff40f33c8, model_data=..., training_stage=0)     at lstmtester.cpp:86

Should the model_data here be pointing to --eval_listfile or lstmf files within it? @stweil Thank you for looking into this. https://github.com/tesseract-ocr/tesseract/issues/956

may also be related.  @theraysmith 

Under what conditions should eval be run from trainer? 

Will training work if it is done without eval? https://github.com/DanBloomberg/leptonica/releases/tag/1.74.3

Should we be using this for 4.00? @stweil 

When training with --eval_listfile  in addition to --train_listfile, I have noticed that while all files from --train_listfile are loaded at beginning of training process, only first two from --eval_listfile are loaded. However, I have not found any images from the --eval_listfile being used in the training (I am going by the detailed log per iteration which is displayed with --debug_interval -1.

Does this mean that eval is still not being run?

Or, does the eval process not write the log message?

If it is a question of not logging, is it possible to modify to add the log message during evaluation?
 > does the eval process not write the log message?

Answered in https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00

With --debug_interval -1, the trainer outputs verbose text debug for **every training iteration.**

@stweil  Eval seems to be running now (though there are no debug messages to verify it). Thanks for the fix. 

Should I close this issue or does it need any other verification? 

 @xlight Thanks to patches by @stweil and leptonica 1.74.2, the assertion and problem with uninitialized data related to this issue are fixed when using the latest code from github.. 

@theraysmith will have to verify that his original problem has been fixed, since I do not know what exact test he was referring to in his comment ...
>it doesn't run the eval from the trainer.
 2 Percent improvement time=12184, best error was 8.073 @ 23307

**Warning: LSTMTrainer deserialized an LSTMRecognizer!**

At iteration 35491/53200/53202, Mean rms=0.165%, delta=1.673%, char train=6.063%, word train=19.746%, skip ratio=0%,  New best char error = 6.063

**At iteration 33032, stage 1, Eval Char error rate=19.697385, Word error rate=42.197884**

 wrote checkpoint.

At iteration 35535/53300/53302, Mean rms=0.166%, delta=1.706%, char train=6.108%, word train=19.751%, skip ratio=0%,  New worst char error = 6.108

**At iteration 34063, stage 1, Eval Char error rate=20.580924, Word error rate=41.515177**

 wrote checkpoint.


_________________________________________

Eval is being run from trainer. Closing the issue. Thanks, @stweil.
 @theraysmith 

Latest code change has reverted the fix.

```
Iteration 99: ALIGNED TRUTH : च छ ज झ ञ उ ऊ ट ठ ड ढ ण ऋ ॠ त थ
Iteration 99: BEST OCR TEXT :
File /tmp/tmp.m82dWGBYZW/mar/mar.Aparajita.exp0.lstmf page 3 :
Mean rms=5.427%, delta=53.011%, train=110.2%(100%), skip ratio=0%
lstmtraining: genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
``` Yes a new bug. I was mystified for a while, but it is very simple.
Fixed in 4b6f0b95..f4f66f8f Fixed by 45fb7dd  @WilliamTambellini: can you please post config.log and output from configure? Just a side though: adding that test will add a 2nd dependency on autoconf-archive. This is fine (autoconf-archive should be available for all relevant hosts) – but as autoconf-archive is often not installed, that case needs a more user friendly error message than today. If we had a simple way to detect whether `autoconf-archive` is missing, `autogen.sh` could also download the missing m4 flies when needed. I think this is possible even with a less permissive license. We already have test for c++11 support (without dependency on autoconf-archive). So it should be enough to modify test to produce error in case of missing c++11 support...  Please use tesseract user forum for asking questions/support.  Please provide files (input files, output files) that demonstrate your issue. also provide more information about you OS, tesseract etc.  Please use tesseract user form for asking questions, support...  We're going to get corrupt output if an open paren is passed as a title.
Proper escaping look like this. The leading FEFF is boilerplate that 
signifies the byte order, everything else is UTF-16BE. The title in this
case is "ru"

```
<< /CreationDate (D:20170103154208-08'00') /Producer (Tesseract 2000) /Title <FEFF00720075> >>
```

https://github.com/tesseract-ocr/tesseract/blob/master/api/pdfrenderer.cpp#L963 Dan's Leptonica change is for a different code path and is not reusuable, because it only works for ASCII. The Tesseract fix needs to use UTF16-BE, which is fortunately already used elsewhere in pdfrenderer.cpp. Fix written, under review. EDIT: I've revised the fix to remove a compiler warning.

```
--- tesseract/api/pdfrenderer.cpp	2017-03-30 16:10:23.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp	2017-03-31 13:16:22.000000000 -0700
@@ -159,7 +159,7 @@
 
 OK there is a small problem there, if I use GID 0 then Acrobat gets
 upset about it and complains it cannot extract the font. If I set the
-CIDToGIDMap so that all the entries are 1 instead, its happy. Totally
+CIDToGIDMap so that all the entries are 1 instead, it's happy. Totally
 mad......
 
 */
@@ -169,10 +169,15 @@
 // Use for PDF object fragments. Must be large enough
 // to hold a colormap with 256 colors in the verbose
 // PDF representation.
-const int kBasicBufSize = 2048;
+static const int kBasicBufSize = 2048;
 
 // If the font is 10 pts, nominal character width is 5 pts
-const int kCharWidth = 2;
+static const int kCharWidth = 2;
+
+// Used for memory allocation. A codepoint must take no more than this
+// many bytes, when written in the PDF way. e.g. "<0063>" for the
+// letter 'c'
+static const int kMaxBytesPerCodepoint = 20;
 
 /**********************************************************************
  * PDF Renderer interface implementation
@@ -303,6 +308,23 @@
   if (rise < 2.0 && 2.0 < run)
     *line_y1 = *line_y2 = (y1 + y2) / 2;
 }
+
+bool CodepointToUtf16be(int code, char utf16[kMaxBytesPerCodepoint]) {
+  if ((code > 0xD7FF && code < 0xE000) || code > 0x10FFFF) {
+    tprintf("Dropping invalid codepoint %d\n", code);
+    return false;
+  }
+  if (code < 0x10000) {
+    snprintf(utf16, kMaxBytesPerCodepoint, "%04X", code);
+  } else {
+    int a = code - 0x010000;
+    int high_surrogate = (0x03FF & (a >> 10)) + 0xD800;
+    int low_surrogate = (0x03FF & a) + 0xDC00;
+    snprintf(utf16, kMaxBytesPerCodepoint,
+             "%04X%04X", high_surrogate, low_surrogate);
+  }
+  return true;
+}
 
 char* TessPDFRenderer::GetPDFTextObjects(TessBaseAPI* api,
                                          double width, double height) {
@@ -442,25 +464,13 @@
       if (grapheme && grapheme[0] != '\0') {
         GenericVector<int> unicodes;
         UNICHAR::UTF8ToUnicode(grapheme, &unicodes);
-        char utf16[20];
+        char utf16[kMaxBytesPerCodepoint];
         for (int i = 0; i < unicodes.length(); i++) {
           int code = unicodes[i];
-          // Convert to UTF-16BE https://en.wikipedia.org/wiki/UTF-16
-          if ((code > 0xD7FF && code < 0xE000) || code > 0x10FFFF) {
-            tprintf("Dropping invalid codepoint %d\n", code);
-            continue;
+          if (CodepointToUtf16be(code, utf16)) {
+            pdf_word += utf16;
+            pdf_word_len++;
           }
-          if (code < 0x10000) {
-            snprintf(utf16, sizeof(utf16), "<%04X>", code);
-          } else {
-            int a = code - 0x010000;
-            int high_surrogate = (0x03FF & (a >> 10)) + 0xD800;
-            int low_surrogate = (0x03FF & a) + 0xDC00;
-            snprintf(utf16, sizeof(utf16), "<%04X%04X>",
-                     high_surrogate, low_surrogate);
-          }
-          pdf_word += utf16;
-          pdf_word_len++;
         }
       }
       delete []grapheme;
@@ -471,9 +481,9 @@
           kCharWidth * prec(100.0 * word_length / (fontsize * pdf_word_len));
       pdf_str.add_str_double("", h_stretch);
       pdf_str += " Tz";          // horizontal stretch
-      pdf_str += " [ ";
+      pdf_str += " [ <";
       pdf_str += pdf_word;       // UTF-16BE representation
-      pdf_str += " ] TJ";        // show the text
+      pdf_str += "> ] TJ";       // show the text
     }
     if (last_word_in_line) {
       pdf_str += " \n";
@@ -960,15 +970,27 @@
   offsets_.back() += pages_objsize;    // manipulation #2
 
   // INFO
+  STRING utf16_title = "FEFF";  // byte_order_marker
+  GenericVector<int> unicodes;
+  UNICHAR::UTF8ToUnicode(title(), &unicodes);
+  char utf16[kMaxBytesPerCodepoint];
+  for (int i = 0; i < unicodes.length(); i++) {
+    int code = unicodes[i];
+    if (CodepointToUtf16be(code, utf16)) {
+      utf16_title += utf16;
+    }
+  }
+
   char* datestr = l_getFormattedDate();
   n = snprintf(buf, sizeof(buf),
                "%ld 0 obj\n"
                "<<\n"
                "  /Producer (Tesseract %s)\n"
                "  /CreationDate (D:%s)\n"
-               "  /Title (%s)"
+               "  /Title <%s>\n"
                ">>\n"
-               "endobj\n", obj_, TESSERACT_VERSION_STR, datestr, title());
+               "endobj\n",
+               obj_, TESSERACT_VERSION_STR, datestr, utf16_title.c_str());
   lept_free(datestr);
   if (n >= sizeof(buf)) return false;
   AppendPDFObject(buf);
--- tesseract/api/renderer.h	2017-03-30 16:10:23.000000000 -0700
+++ tesseract/api/renderer.h	2017-03-31 10:34:40.000000000 -0700
@@ -57,6 +57,7 @@
     /**
      * Starts a new document with the given title.
      * This clears the contents of the output data.
+     * Title should use UTF-8 encoding.
      */
     bool BeginDocument(const char* title);

```  https://github.com/tesseract-ocr/tesseract/issues/233
https://github.com/tesseract-ocr/tesseract/commit/11f205707eda769dbe6cc7d6839745f1b01a1d76

When this patch was submitted, we reduced multipage TIFF seeks O(n).
But that's only on the regular code path. The OpenCL code path is still O(n^3).
That's not really sensible. Recommend removing the OpenCL path from 
ProcessPagesMultipageTiff() and then thinking about next steps.

To be honest, it's pretty hard to keep code synchronized and I'm a little
curious why current approach was chosen, versus integrating
OpenCL calls directly into much lower level libraries such as Libtiff.
Libtiff is doing the actual computational work, such G4 decompression. Getting back to ProcessPagesMultipageTiff(), there is no work being done by Leptonica. It is just TIFF decode and all computation is being done by Libtiff. Anyway, the OpenCL code is now behind. It should either be re-synchronized, migrated to libtiff, or disabled. No sense in keeping it as an O(n^3) code path. This would disable the O(n^3) codepath (where n is the number of pages inside the multipage tiff). It would have the side effect of turning fairly large quantities of Tesseract's OpenCL code into unreachable dead code.

```diff
--- baseapi.cpp.orig	2017-01-26 13:10:45.011521878 -0800
+++ baseapi.cpp	2017-01-26 13:11:12.511896362 -0800
@@ -980,26 +980,13 @@
                                             int tessedit_page_number) {
 #ifndef ANDROID_BUILD
   Pix *pix = NULL;
-#ifdef USE_OPENCL
-  OpenclDevice od;
-#endif  // USE_OPENCL
   int page = (tessedit_page_number >= 0) ? tessedit_page_number : 0;
   size_t offset = 0;
   for (; ; ++page) {
     if (tessedit_page_number >= 0)
       page = tessedit_page_number;
-#ifdef USE_OPENCL
-    if ( od.selectedDeviceIsOpenCL() ) {
-      pix = (data) ?
-          od.pixReadMemTiffCl(data, size, page) :
-          od.pixReadTiffCl(filename, page);
-    } else {
-#endif  // USE_OPENCL
       pix = (data) ? pixReadMemFromMultipageTiff(data, size, &offset)
                    : pixReadFromMultipageTiff(filename, &offset);
-#ifdef USE_OPENCL
-    }
-#endif  // USE_OPENCL
     if (pix == NULL) break;
     tprintf("Page %d\n", page + 1);
     char page_str[kMaxIntSize];
``` @jbreiden: Thanks. Can you please discuss with Ray about rest of OpenCl code? It is simple not maintained (I took 2 years to get fixes for reported issues) Last week Ray said he was still looking into removing the non LSTM recognizer, which is still in use for OSD which is orientation script detection. If that is successful, I think that 100% of the opencl code will be unused.  If that happens I suspect the next step is removal. (In general I like both AMD and the idea of faster computation to the GPU, but not enough to keep unmaintained code.)  Correct. Changing instructions now...  ​​
​it seems this problem occur in other multi lingual process to
i face this kind of error on Arabic+English scan
 ​im working on ubuntu16 and teseract is maked from latest source​...
 Mostly fixed by commit b453f74.  Debug mode is known to be ~50x slower than optimized mode.
Please re-open if you are seeing this slow-down in optimized mode.  This looks like an error for 32 bit x86 platforms. You can work around it by removing the line `#  define X86_BUILD 1` from `arch/simddetect.cpp`. @Izaron, yes, but it will be much slower.

@amitdo, there are several options:

* Let the compiler do the optimization (and forget that explicit SSE/AVX code). This is my preferred solution as it also works for ARM and other architectures.

* Fix the current code, so 32 bit Intel does not use AVX (it could use SSE).

* Fix the current code, so 32 bit Intel can use AVX, but avoids unavailable 64 bit intrinsics.

No need to fix this this year – for the rest of the year I have other priorities. Happy new year!
 Yes, sure. You need the compiler option `-ftree-vectorize`. In addition, the compiler must know which kind of FPU is available. The compiler option `-O3` includes `-ftree-vectorize`. So for an Intel CPU, `-O3 -mtune=native -mavx` should work. An ARM CPU could use `-O3 -mfpu=neon -funsafe-math-optimizations`. See https://gcc.gnu.org/onlinedocs/gcc/ARM-Options.html for more information. There's huge variation of SIMD support even in modern processors. For example I have an Intel CPU purchased in 2016 (Pentium G4400 Skylake) that doesn't support AVX instructions at all. Statistically speaking, most Tesseract users run precompiled binaries. Finally, Tesseract ships on more than 20 hardware platforms for Debian GNU/Linux alone, not to mention all those other operating systems.

https://buildd.debian.org/status/package.php?p=tesseract&suite=unstable

This is a tough combination. Ideally a binary will uses the fastest SIMD instructions available for the processor, and determines those instructions at runtime rather than at compile time. There's lots of discussion on the web about this, but it is not clear to me what is best practice.

 Yes, binary distributions must support a wide range of hardware and ideally chose the best code at runtime.
That's why we need `simddetect.cpp` (with extensions for non Intel platforms).

Nevertheless there is also the need for highly optimized Tesseract installations used for training or mass production of OCR on know hardware. > The non-SIMD version of the dotproduct method can be significantly sped up by doing manual loop unrolling.

Which test scenario do you use? This is again (or still) a bug for 32 bit Intel platforms. I'll have a look how to fix it later. This issue is addressed by pull request #698. @const-volatile, did you use `cmake`? Then that still needs a fix. PR #698 only addressed builds with `configure`. @nguyenq, could you please try replacing `#if !defined(__AVX__) || defined(__i386__)` in file `arch/dotproductavx.cpp` by a simple `#if !defined(__AVX__)` and report whether the resulting `tesseract.exe` works with AVX? It would also be interesting how the OCR speed compares with your current `tesseract.exe`.

Recently support for AVX with 32 bit executables was added, but it needs the above modification to get activated. Run `tesseract -v` to see whether you have a CPU which supports AVX or SSE. The error message which you get indicates that you have AVX support. Please try the even shorter variant `#if 0` to enforce that the AVX code is included (and not the _DotProductAVX can't be used on Android_ error message). It looks as if your compiler does not define the macro `__AVX__`. This modification might set `__AVX__` (I cannot test it because I don't have MSVC):

    diff --git a/CMakeLists.txt b/CMakeLists.txt
    index d227b7d..1805170 100644
    --- a/CMakeLists.txt
    +++ b/CMakeLists.txt
    @@ -220,6 +220,7 @@ if (WIN32)
         if (MSVC)
             set_source_files_properties(
                 ${CMAKE_CURRENT_SOURCE_DIR}/arch/dotproductavx.cpp
    +            PROPERTIES COMPILE_DEFINITIONS __AVX__)
                 PROPERTIES COMPILE_FLAGS "/arch:AVX")
         endif()
     endif()
 It's strange that the macro `__AVX__` is not set automatically by MSVC. The compiler option `/arch:AVX` should normally do that (see [documentation](https://msdn.microsoft.com/en-us/library/7t5yh4fd.aspx)). With the latest code, you should point your tessdata_dir to tessdata/best

On 17-Aug-2017 10:41 AM, "Amit D." <notifications@github.com> wrote:

> From where did you get the chi_tra and eng traineddata?
> From here:
> https://github.com/tesseract-ocr/tessdata/tree/3.04.00
> Or from here:
> https://github.com/tesseract-ocr/tessdata/tree/master/best
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/631#issuecomment-322970548>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o5gN3GvgUYBBX6NKL_cYbnaZVL2dks5sY8uYgaJpZM4LYhc->
> .
>
  ​i checked Arabic today
with default trained data it have about 80% accuracy
what are you looking for?​
 ​as i said i got the result with official trainedata
i dont started to my own training yet...​
 ​@shree ubuntu 16lts​
  Closing as it is in 'nice to have' category rather than a bug.  -1 for laziness (not reading [doc](https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT.md) and not searching other issues e.g. #601 and #610). Information in INSTALL.GIT.md
<https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT.md> is
there already 4 days...And of course: installing from source expect that
user is  familiar with compiling software on your operation system.


Zdenko

On Fri, Dec 30, 2016 at 6:40 PM, Amit D. <notifications@github.com> wrote:

> Yesterday I added the missing dependency to this wiki page:
> https://github.com/tesseract-ocr/tesseract/wiki/Compiling
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/628#issuecomment-269799789>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAjCzKAoOyhzzY3iubDssVKSAJrJVGFWks5rNUIVgaJpZM4LX3JC>
> .
>
 Replying in reverse order ;-) (based on importance):

1. Patches are welcomed!
2. Error message is output of autoconf. Maybe other test for compiler parameter would be more suitable, but this one seems to be standard (based on few minutes of googling).
3. Installing sw from source without tracking dependencies (or at lease reading INSTALL file) is IMO nonsense.  Vertical Japanese is not yet trained on LSTM.
  How did you run Tesseract (which command line options, which image)? I can reproduce the problem. It only happens when Tesseract is built with a shared library (`libtesseract.so`). The crash occurs because the constructor of the global variable `debug_file` was not called at the time when `SIMDDetect::SIMDDetect` is executed. For the moment, removing the `tprintf` statements in `arch/simddetect.cpp` helps. Fixed with pull request #625.  This is not related to tesseract project  Please read the [documentation](https://github.com/tesseract-ocr/tesseract/wiki/Data-Files).  That's not code from latest Tesseract. Please close this issue here and report it at the right place (wherever that is). @ruochenxing : can you please clarify if there was problem with tesseract (if yes which branch you used)? and how did you fix it?  Please use tesseract user forum for asking support.  First of all: It seem like output from some tesseract wrapper. We do not provide support for 3d party sw. 
Next: Use tesseract user forum for asking support. Please follow [instruction](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md) and make sure you read [wiki first](https://github.com/tesseract-ocr/tesseract/wiki) (at least FAQ and ImproveQuality).  Modify also the code to use a singleton. This simplifies the code as
no locking is needed. It also slightly improves the performance because
no check whether the architecture was tested is needed.

Signed-off-by: Stefan Weil <sw@weilnetz.de>   This PR addresses issue #517. The detection code was tested with Mingw-w64 (which supports both the gcc and the MSVC specific code), but I cannot test builds with MSVC. [AppVeyor](https://ci.appveyor.com/project/zdenop/tesseract/build/3.05.560) tests that and passed both for 32 and 64 bit. @zdenop, https://ci.appveyor.com/project/zdenop/tesseract/build/3.05.560 still includes 3.05 in the URL. @stweil I change build version format to "{branch}.{build}" (from "3.05..{build}". Lets see if it helps. Looks good, thank you. Good idea. Done in PR #618.  Please use tesseract user forum for asking support  text2image cannot be built with cmake on windows.
Let me try  to build tess with your steps. (I'll report here.) Works for me. What version of tesseract are you using?
I've already fixed issues you report on master, but not on the other branches. Oh, you probably have old cmake.
Which cmake version do you have?
3.6 is known to not work with tess right now (your errors). I'll add an error message for cmake 3.6 (on cppan side).  Problem is that you modified code without knowing what you are doing. Did you check comment on https://github.com/tesseract-ocr/tesseract/issues/601? @Vidushi12, please don't post your question on several places (even if it is urgent for you).

Did you compile Tesseract yourself or do you use a pre-build version? Can you use Tesseract 3.05 or an earlier version? Tesseract 4.x is experimental and not for end users currently. @Vidushi12, I suggest to use the Tesseract provided by Ubuntu and don't build it yourself. Just run `apt-get install tesseract-ocr tesseract-ocr-eng` (add more languages as needed) as root user. You can either remove `/usr/local/bin/tesseract`, or don't uninstall it and call `/usr/bin/tesseract` explicitly. Yes, that's it.  @amitdo:
What is benefit/difference between `[lept >= 1.74]` and `[lept >= 1.74.0]`?
BTW: `pkg-config --modversion lept` shows 1.74 Well [allheaders.h](https://github.com/DanBloomberg/leptonica/blob/master/src/allheaders.h#L31) defines only LIBLEPT_MAJOR_VERSION (1) and LIBLEPT_MINOR_VERSION (74), so I see no reason to change... It's a switch to semver also. 1.74.0 tag in git and version numbers in code/build tools. For cmake `1.74` and `1.74.0` are equal. And travis needs precise git tag to work. At least for cmake 3.6.2 only 1.74 but not 1.74.0 works. Tested on Raspbian. I am not near a computer to check. Does Tesseract 3.04 build against
Leptonica 1.74? It is this version number stuff causing trouble?

On Dec 26, 2016 6:31 AM, "Stefan Weil" <notifications@github.com> wrote:

At least for cmake 3.6.2 only 1.74 but not 1.74.0 works. Tested on Raspbian.

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub
<https://github.com/tesseract-ocr/tesseract/pull/608#issuecomment-269216105>,
or mute the thread
<https://github.com/notifications/unsubscribe-auth/AEu2phQlhZsUnHI3zUF9VY1LXj19gYi-ks5rL8-jgaJpZM4LVW_Z>
.
 I just ran a test with cmake 3.7.1-1 on Debian GNU Linux. It also works only with 1.74, but not with 1.74.0. I am in the snowy hinterlands, won't be able to advise until new year. So
far I can't tell if this is important or not. If I had to guess, we are
probably fine, especially if the autoconf build on Tesseract had no issues.
(I just want to keep the chaos level low in the world of Debian packages)

On Dec 28, 2016 9:00 AM, "Dan Bloomberg" <notifications@github.com> wrote:

Seems like we have a problem with the 1.74.0 version numbering. What do
you suggest that we do here? New github release? Replacement? Nothing?

On Mon, Dec 26, 2016 at 2:10 PM, Stefan Weil <notifications@github.com>
wrote:


> I just ran a test with cmake 3.7.1-1 on Debian GNU Linux. It also works
> only with 1.74, but not with 1.74.0.
>
> —
> You are receiving this because you were mentioned.

> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/608#
issuecomment-269245065>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AP6mLNTIpQ9_
c7fCyh6OFkdZidkx24NCks5rMDtvgaJpZM4LVW_Z>
> .
>

—
You are receiving this because you commented.

Reply to this email directly, view it on GitHub
<https://github.com/tesseract-ocr/tesseract/pull/608#issuecomment-269506129>,
or mute the thread
<https://github.com/notifications/unsubscribe-auth/AEu2prjMBcM6137QaP9GwOgubdDmKBDUks5rMpW7gaJpZM4LVW_Z>
.
 Dan, the issue (if any) is probably somewhere in autotools config on lept side. No need to rename git tag, it's fine. In case of any critical changes, you could remove tag and add it again, or just make 1.74.1 release with fixes. Yes, I think this is the reason why 1.74.0 did not work for me. I suggest using 1.74.1. Fixes are needed for `README.html`, `configure.ac` and `version-notes.html`. ... and `CMakeLists.txt`. @DanBloomberg: if you decide to implement version format "1.74.1" do not forget to implement it into allheaders.h as LIBLEPT_MAJOR_VERSION (1), LIBLEPT_MINOR_VERSION (74) and LIBLEPT_MICRO_VERSION (1)...
 `LIBLEPT_MICRO_VERSION` would be a new interface. IMHO it is not necessary, because C code should not depend on the patch level or micro version. Or better LIBLEPT_PATCH_VERSION as in semver (major.minor.patch).  This was fixed in commit 53c572b4.  I cannot say whether this is an error without seeing the screenshot – did it contain text?

Usually text in a screen-shot has a low resolution, something typically at most 120 dpi. For Tesseract, 300 dpi are suggested. You can try to rescale your screen-shot. Or you can try the experimental Tesseract 4.  see comments in https://github.com/tesseract-ocr/tesseract/issues/373  @yakeer: if tesseract is dependency for other library/application I would suggest to use stable release [3.04.01](https://github.com/tesseract-ocr/tesseract/releases/tag/3.04.01) or [3.05 branch](https://github.com/tesseract-ocr/tesseract/tree/3.05) (bug fixes for 3.04 version with small enhancement of API) @yakeer: please check current master if it compile on Raspberry Pi 3. `sudo apt-get install autoconf-archive`
see: http://stackoverflow.com/questions/30412576/autotools-syntax-error-with-ax-check-compile-flag Is this still an issue, or can it be closed? I could successfully compile Tesseract on Raspbian using configure / make.  Use tesseract user forum for asking support  Please use tesseract user forum for asking question/support.  No, it's on our side. Working on it.  This fixes a build regression caused by commit
d70f3c36635c251ac3286063c6c29d10b815a565.

Signed-off-by: Stefan Weil <sw@weilnetz.de> Do we still need autotools?
What do you think? Maybe elaborate more on cmake? Personally I'm more used to autotools (> 10 years) than to cmake (< 1 year). Therefore I usually build with configure && make. But of course supporting both autotools and cmake wastes resources and requires more testing. Nice, but too heavy. It uses python.
Build system (and other technical tools) for a general-purpose language must be written in the same language or in more low level language, but not higher level lang. than this. Examples:
autotools is written in C
cmake in C++
We use c++, so we need something in C++,C,ASM. :) And that's not good.
CMake is C++ (with some C libs) and no other deps. > And that's not good.
it depends on criteria ;-) On linux that are (almost) always installed smoothly. 
With cmake I had issues - e.g. when I build and installed some basic lib cmake on windows (AFAIR png) - cmake was not able to find it after installation. It never happens with autotools on linux. I several times started to use cmake, but...

I am glad for cmake support in tesseract because of windows, but I would keep autotools because of linux. And I am willing to maintained it ;-)
  > And I am willing to maintained it ;-)

Ok, not a problem.

> With cmake I had issues 

Me too. That's why I'm developing CPPAN. We already can build the same training tools as with cmake  on windows. So, we have parity at the moment. And in the future I hope will be able to build the last training tool - text2image on windows - which is very hard with pure cmake. The [UB Mannheim Windows binaries](https://github.com/UB-Mannheim/tesseract/wiki) include `text2image.exe` and were built using autotools. They are built with gcc (mingw) which is not a big advantage.
We don't need mingw (cygwin/other linux port stuff) build. We need msvc build. Actually, it requires pango, cairo and some other libs. See https://github.com/tesseract-ocr/tesseract/blob/master/training/CMakeLists.txt#L265
And in total all their deps contain about 15 indirect deps.

You may notice that building deps on windows (msvc) is not so easy task.
You should find every lib, make it compile on windows, include to your project and to each other etc.  I'd like to propose changes to tesseract source tree structure.
Today the common way is to have `src` folder with all program stuff and `include` folder with public headers. Now we have a lot of dirs in the root - that's very annoying.
On the first stage I propose:
1. move all sources into `src`
2. move training tools from `training` to `tools/training`

Later we can try to move public headers to `include` directory.

The new look will be like:
![pic](http://i.imgur.com/K8NkI8T.png)

If there are no objections, I'll commit changes. Make sure to get positive confirmation from Ray before doing something like this. He's considering a different, very disruptive change involving the non-LSTM recognizer. It would collide very badly with source reorganization. Yes, I'll act only in case of positive reply from Ray.  @theraysmith @zdenop 

This PR replaces all 4.00.00 version numbers to 4.0.0.
The idea is to follow semantic versioning since 4.0.0 branch. https://semver.org
Following git tags should be also named as `X.Y.Z[-name]`, e.g. `4.0.0`, `4.0.1`, `4.0.0-beta`, `4.0.0-rc.1` etc. @egorpugin Isn't it better to change to the new versioning scheme while 4.0 is still in development branch?

Does @zdenop need to change the git tag? No need to change old git tags (even 4.00.00alphas), but we need to decide versioning until release.  Try to update tesseract from master, upgrade cppan (if you're not on the latest client `cppan --self-upgrade`), run cmake as `cmake ..` and not `cmake .. -DSTATIC=1`. I've removed STATIC flag from wiki page. Can you do please following steps and send `1.txt` and `2.txt` to me?
```
cppan --clean-packages .*unicode.*
cppan
mkdir win32
cd win32
cmake .. > 1.txt 2>&1
cmake --build . --config Release > 2.txt 2>&1
```
 What VS do you have? Is it VS 2015?
The error is:
```
c:\users\home\.cppan\storage\src\2f\16\8f89\libtiff\tiffiop.h(56): error C2143: ?????????????? ??????: ?????????? ")" ????? "*" (????????????? ???????? ???? 
```
It's hard to understand it from gist. :) I see. Probably there are some issues with your environment.
```
-- include search.h - not found
-- function lfind - not found
```
This file and function cannot be found on your system for some reason, but actually they're available.
I'm thinking on how we can fix this. How do you want to use tess?
If you have a project with cmake build, see this example https://github.com/cppan/tesseract_example/tree/master/with_cmake

If you want to build that file https://github.com/cppan/tesseract_example/blob/master/with_cppan/main.cpp
run from command line 
```
cppan --build https://github.com/cppan/tesseract_example/raw/master/with_cppan/main.cpp
```
The VS solution appear in that folder (and the file itself). Could you contact me in some IM chat?
Probably we could find solution faster.
Maybe in skype (username - egor.pugin). Sorry, updated previous comment with correct username.  See https://github.com/tesseract-ocr/tesseract/issues/832 
for LSTM : Training - Support WordStr Box file option #832

Closing this issue as LSTM training process has changed.  Changing tesstrain_utils.sh for 

`  common_args+=" --leading=${LEADING} --xsize 2550"`
fixes this.
 Usually this happens for just a few lines of an image - tesseract splits the input image into separate image  per line.

It could be when layout analysis has wrongly segmented the page  or a line has been detected as having hundreds of diacritics.

If it is just a few messages, you could ignore.

@theraysmith Any update regarding new line detection algorithm? The exact error message would greatly help diagnose the problem.

On Tue, Aug 8, 2017 at 10:28 PM, Amit D. <notifications@github.com> wrote:

> Image too large to learn!! Size = 2758x48
> Image not trainable
>
> @hanikh <https://github.com/hanikh>, please paste a short example for the
> errors you get.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-321156352>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056TBM3518EXdJE7-KA44mvwgN2Mx2ks5sWUNhgaJpZM4LQsPF>
> .
>



-- 
Ray.
 > I finetuned tesseract for farsi (40 fonts on 6000 text lines)

I think this maybe too much for finetuning.

I noticed that tesstrain.sh is limiting  text2image generated images to
just 3 pages - that would be only max 150 lines per font.

With that much input, you can try replace a layer training to see if that
gets you better results.



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, Aug 12, 2017 at 3:27 PM, hanikh <notifications@github.com> wrote:

> @theraysmith <https://github.com/theraysmith> would you please help me,
> how many text line is appropriate?
> thanks
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-321970660>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o4rV-DPLTiSAqgSTy9dJdA3Oek6iks5sXXcJgaJpZM4LQsPF>
> .
>
 @hanikh I suggest to wait till Ray updates the langdata and also uploads
the new version of unichar_extractor. Befroe that training for RTL
languages may not be give useful results.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, Aug 12, 2017 at 4:04 PM, ShreeDevi Kumar <shreeshrii@gmail.com>
wrote:

> > I finetuned tesseract for farsi (40 fonts on 6000 text lines)
>
> I think this maybe too much for finetuning.
>
> I noticed that tesstrain.sh is limiting  text2image generated images to
> just 3 pages - that would be only max 150 lines per font.
>
> With that much input, you can try replace a layer training to see if that
> gets you better results.
>
>
>
> ShreeDevi
> ____________________________________________________________
> भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com
>
> On Sat, Aug 12, 2017 at 3:27 PM, hanikh <notifications@github.com> wrote:
>
>> @theraysmith <https://github.com/theraysmith> would you please help me,
>> how many text line is appropriate?
>> thanks
>>
>> —
>> You are receiving this because you modified the open/close state.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-321970660>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AE2_o4rV-DPLTiSAqgSTy9dJdA3Oek6iks5sXXcJgaJpZM4LQsPF>
>> .
>>
>
>
 Initial problem: (Image too small to scale)
Those images are ridiculously small at 3x48 pixels. Something is going
wrong somewhere with the images.
Are they oriented vertically? The input scaling scales the height to 48,
whatever it starts as, so it looks like your textlines are vertical.

Fine tuning problem:
The problem is most likely too many iterations. It will hone its accuracy
to whatever training data you give it if you run it for too many iterations.
See how few iterations are used in the training tutorial for fine tuning.

On Sat, Aug 12, 2017 at 5:19 AM, hanikh <notifications@github.com> wrote:

> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Compute CTC targets failed!
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> Image too small to scale!! (3x48 vs min width of 3)
> Line cannot be recognized!!
> Image not trainable
> 2 Percent improvement time=0, best error was 2.167 @ 14
> At iteration 14/1100/20884, Mean rms=0.049%, delta=0%, char train=0%, word
> train=0%, skip ratio=1798.6%, New best char error = 0 wrote best
> model:/home/fanasa/tesstutorial/fastuned_from_fas/fastuned-plates0_14.lstm
> wrote checkpoint.
>
> Finished! Error rate = 0
> this is the error I got during training for licence plates.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-321977639>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056ZvLnyg_aC1mUg2gH34puAGpWdOOks5sXZhHgaJpZM4LQsPF>
> .
>



-- 
Ray.
 Ray,

I have seen line too small to be recognized when building box/tiff pairs
using tesstrain.sh - it is usually related to 'nnn diacritics found' - so
it  may be related to accents being treated as a separate line.

Regarding finetuning, I have experimented a lot with Devanagari - with
smaller number of iterations, the reported error rate is higher. And it
takes tens of thosands of iterations for it to get more accuracy on
training set - not sure of its effect on samples it has not seen. - see
https://github.com/Shreeshrii/tess4training/blob/master/README.md



ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sun, Aug 13, 2017 at 9:44 AM, theraysmith <notifications@github.com>
wrote:

> Initial problem: (Image too small to scale)
> Those images are ridiculously small at 3x48 pixels. Something is going
> wrong somewhere with the images.
> Are they oriented vertically? The input scaling scales the height to 48,
> whatever it starts as, so it looks like your textlines are vertical.
>
> Fine tuning problem:
> The problem is most likely too many iterations. It will hone its accuracy
> to whatever training data you give it if you run it for too many
> iterations.
> See how few iterations are used in the training tutorial for fine tuning.
>
> On Sat, Aug 12, 2017 at 5:19 AM, hanikh <notifications@github.com> wrote:
>
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Compute CTC targets failed!
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > Image too small to scale!! (3x48 vs min width of 3)
> > Line cannot be recognized!!
> > Image not trainable
> > 2 Percent improvement time=0, best error was 2.167 @ 14
> > At iteration 14/1100/20884, Mean rms=0.049%, delta=0%, char train=0%,
> word
> > train=0%, skip ratio=1798.6%, New best char error = 0 wrote best
> > model:/home/fanasa/tesstutorial/fastuned_from_
> fas/fastuned-plates0_14.lstm
> > wrote checkpoint.
> >
> > Finished! Error rate = 0
> > this is the error I got during training for licence plates.
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/590#
> issuecomment-321977639>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/AL056ZvLnyg_
> aC1mUg2gH34puAGpWdOOks5sXZhHgaJpZM4LQsPF>
> > .
> >
>
>
>
> --
> Ray.
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-322020794>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o3ztjvMQKBue5JIqMU9Qrfx4ng_Mks5sXng2gaJpZM4LQsPF>
> .
>
 > where can the lang.lstm-unicharset file be found ?

`combine_tessdata -u lang.traineddata lang.`

It will create lang.* files , including the unicharset.

You can use dawg2wordlist to see the wordlist used

 > how can combine_lang_model be used? 

```
 combine_lang_model    \
 --input_unicharset  ../tesstutorial/sanskrit2003/san/san.unicharset  \
 --script_dir "../langdata"   \
 --words "../langdata/san/san.wordlist" \
 --numbers "../langdata/san/san.numbers"   \
 --puncs "../langdata/san/san.punc" \
 --output_dir ../tesstutorial/sanskrit2003   \
 --lang "san"     --pass_through_recoder \
     --version_str "4.0.0alpha-20170816 sanskrit2003"
```

For RTL languages, there is an additional flag. Please see https://github.com/tesseract-ocr/tesseract/blob/master/training/tesstrain_utils.sh for details.

I used a  hand-edited unicharset, because the unicharset generated from the current training process is `old style`.  You should wait for @theraysmith to update the unichar_extractor and other langdata files. @theraysmith 

Please let us know whether it is worthwhile to try and train (finetune/replace layer) for RTL languages or should the users wait for your updates to langdata, unichar extractor programs. Need to wait for unichar_extractor to be fixed.

See https://github.com/tesseract-ocr/tesseract/issues/1114

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Tue, Sep 12, 2017 at 4:29 PM, hanikh <notifications@github.com> wrote:

> @Shreeshrii <https://github.com/shreeshrii> would you please help me with
> using "replacing layers" as I asked before?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/590#issuecomment-328818316>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o39Bj_jpcJV98kEOyDdv8rMKfFUvks5shmQbgaJpZM4LQsPF>
> .
>
 @theraysmith Please see https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/QC3WY48SicI/ZococRbTBAAJ

regarding question about finetuning training for  chi_sim.traineddata model  Try to run `cppan --clean-packages .*jpeg.*`, then run `cppan` in tesseract folder and then check your solution again. Also make sure you have the latest cppan client installed: `cppan --self-upgrade` Kill those processes. It's MSBuild.exe/CL.exe issue. Clean `tiff` now. :)
```
cppan --clean-packages .*tiff.*
cppan
```

You may try instead of typing these commands remove the whole storage `c:\users\zhivko\.cppan\storage`.
Or try to run `cppan --clean-packages .*`. Did you remove storage?
If no, do 
```
cppan --clean-packages .*lepton.*
cppan
```

Issue is somewhere between tiff/leptonica. Can you go into that file and a picture of code line? Oh, by the way. libtiff is known to not work on MSVC < 2015 (2013,2012,2010 etc.).
I don't have VS2013 to fix the issue, but it should not be very hard. The issue is in snprintf function or macro. It is changed in VS2015. What version do you have? 2013 or 2015? or even 2017? No need to downgrade. It should work fine on VS2015. Should we close this ticket? 1.74 means >=1.74.* i.e.
when we have 1.74.1, 1.74==1.74.1
when we have 1.74.2, 1.74==1.74.2

As for 3.05+vs2010, one must gather (download, build) all deps manually.
3.05+vs2015+cppan works fine.   IMO support for VS2010 should be dropped in 4.00 because of  [c++11](https://msdn.microsoft.com/en-us/library/hh567368.aspx)... That would simplify things. That dir (`port`) should be just reorganized. VS solutions could be removed because we have cmake.

By the way, did you decide how to version 4 branch? 4.00 or 4.0.0 ? 4.0.0 could be a start for [semantic versioning](http://semver.org/) (like it is used by many other projects). That's why I'm asking. My vote is for 4.0.0. There is [4.00.00alpha](https://github.com/tesseract-ocr/tesseract/blob/master/api/baseapi.h#L23) and  [4.00.00dev](https://github.com/tesseract-ocr/tesseract/blob/master/configure.ac#L9) version  info. If there is change request - create PR and ask Ray for merge... @stweil: will you update this PR according comments (VS removal, port reorganized)? > If there is change request - create PR and ask Ray for merge...

Sure. Will do it if there are no serious objections. Also following tags must be also named as 4.0.0-smth.
https://github.com/tesseract-ocr/tesseract/releases @zdenop, I removed the VS related commit from this PR now. Removing VS and moving port can be handled in a new PR. thanks @zdenop 
By the way, maybe you'd like to reorganize sources also?
1. move all sources to `src` dir.
2. (optional) add `include` dir with all public stuff - headers. I can do this, but someone should fix autotools after the changes. @egorpugin : reorganization of sources should be discussed with Ray... Ok, I'll ask him a bit later when fixing version numbers.  They were removed in commit d70f3c36635c251ac3286063c6c29d10b815a565.
The old code implicitly added `-llept` by using the `AC_CHECK_LIB` macro.

Signed-off-by: Stefan Weil <sw@weilnetz.de> (untested)

    git checkout -b stweil master
    git pull https://github.com/stweil/tesseract.git
    git checkout master
    git cherry-pick 7c684be7245a24bd4c97878424fb14bc394fef4e
  Currently we see tons of warnings during MSVC builds.
They should be fixed during `modernizing code` phase.
See this log for warnings https://ci.appveyor.com/api/buildjobs/3v0vmv6xjy658f79/log Similar warnings can be produced on Linux using clang++. But even the default debug build with gcc produces tons of warnings (mainly signed/unsigned mismatch). I've also removed /W1 (warning level 1, default is 3) in leptonica for MSVC.
So, @DanBloomberg probably should look at those warnings in lept too.
https://ci.appveyor.com/api/buildjobs/0s1o3vud8x1e1pe6/log * 1.0 -> 1.0f
* make casts explicit

What do you think? Mixing float and double can result in additional code (and execution time) for conversions between both types.  This is caused by a missing `--llept` linker option. I have that problem, too (not in all configurations), and I am still searching why this worked before. Yes, I thought so, as it was the only related commit. The strange thing is that builds with `--enable-opencl` (default when OpenCL development files are installed) work. That's why I only see it with cross builds for Windows. Nevertheless, I'll send a PR soon. See pull request #585 for a fix.  GITREV should be used for debuging and not in release mode.
tag in master created:
https://github.com/tesseract-ocr/tesseract/tree/4.00.00alpha
also visible in https://github.com/tesseract-ocr/tesseract/releases  You also have to build `ScrollView.jar`:

    make ScrollView.jar

In addition, it is necessary to set the environment variable `SCROLLVIEW_PATH` to the directory with `ScrollView.jar`. This information is currently missing in the Wiki page.  Did you compile Tesseract yourself or did you use binaries from the net? Could you try a newer version? The image link is 404, so impossible to make progress. The image is available [here](http://i.imgur.com/fDnFq1a.jpg). It is a JPEG image, not a PNG (as the title of the issue suggests). @vikinsights, where did you get your Tesseract for Windows from (or how did you bulld it if you did this yourself)?  You report issue in wrong project ;-) Tesseract provides information about version. See:
[const char* TessBaseAPI::Version()](https://github.com/tesseract-ocr/tesseract/blob/master/api/baseapi.cpp#L139) and how to [use it](https://github.com/tesseract-ocr/tesseract/blob/master/api/tesseractmain.cpp#L52) (in c++).  > install-dataDATA
This indicates problem with configuration. How did you configured  build? Extract from your build protocol:

    make[3]: Entering directory '/work/usr/local/src/tesseract/tessdata/configs'
    make[3]: Nothing to be done for 'install-exec-am'.
     /bin/mkdir -p '/usr/local/share/tessdata/configs'
     /usr/bin/install -c -m 644 inter makebox box.train unlv ambigs.train api_config kannada box.train.stderr quiet logfile digits hocr tsv linebox pdf rebox strokewidth bigram txt '/usr/local/share/tessdata/configs'
    /usr/bin/install: „inter“ und „/usr/local/share/tessdata/configs/inter“ sind die gleiche Datei

So you are building in `/work/usr/local/src/tesseract` and want to install to `/usr/local`. The installer says that `/work/usr/local/src/tesseract/tessdata/configs/inter` and `/usr/local/share/tessdata/configs/inter` are the same file and fails therefore. Do you have a symbolic link somewhere (either in your build directory or in the installation directory) which connects both directory hierarchies? Well, make did catch that mistake and said that the installation failed because for some files source and destination were identical ("sind die gleiche Datei").   Yes, that could be done as well. Maybe we should require a Leptonica installations with pkg-config. That would simplify that part of the configuration a lot. Then let's remove the unneeded code. :-)  Both older (Debian libicu-dev 52.1-8+deb8u4) and newer (Debian libicu-dev 57.1-5) versions work for me. There are known issues for Windows which are addressed in PR #574.  Can you try revert this https://github.com/tesseract-ocr/tesseract/commit/7755e05e506ea7a31ff50dd7a8b86dfd885c639c ? Current Mingw-w64 as included in Cygwin includes these libraries in package `mingw64-x86_64-icu`:  libicudata.dll.a  libicui18n.dll.a  libicuio.dll.a  libicule.dll.a  libiculx.dll.a  libicutest.dll.a  libicutu.dll.a  libicuuc.dll.a, so that should work. It looks as if you got an older version of icu. Which version of Mingw-w64 did you use?
 @zdenop, reverting that commit will work with old versions of icu. We don't support them for Linux, and for Windows there are also newer versions available. If support for both old and new versions is required, we need a test in configure. If you use MSYS / MinGW from http://www.mingw.org/, then even the latest installation is old. :-(
That packages are not suitable for builds of Tesseract (old libraries, old compiler, only 32 bit). Don't use them for Tesseract 4.

Use Mingw-w64 from https://mingw-w64.org/ or install Cygwin from https://cygwin.com/ with the mingw64-* packages. Both solutions allow building 32 or 64 bit Tesseract for Windows. I'll have a look tomorrow. Good night for today. msys2.github.io provides a recent package `mingw64/mingw-w64-x86_64-icu` 57.1-1 with these libraries in `/mingw64/lib`:

    libicudt.dll.a  libicuin.dll.a    libicuio.dll.a  libicule.dll.a
    libiculx.dll.a  libicutest.dll.a  libicutu.dll.a  libicuuc.dll.a

This results in the reported build error.

Cygwin has a package `mingw64-x86_64-icu` 57.1-2 with these files in `/usr/x86_64-w64-mingw32/sys-root/mingw/lib`:

    libicudata.dll.a  libicuio.dll.a  libiculx.dll.a    libicutu.dll.a
    libicui18n.dll.a  libicule.dll.a  libicutest.dll.a  libicuuc.dll.a

Note that the versions are nearly identical. I was wrong when I thought that it was a problem of old versions of libicu. We need a `configure` which supports both variants. I'll send a PR. PR #572 addresses all issues which I had with the MSYS2 Installer from https://msys2.github.io/. @djusHa, the latest version in git includes the changes now. Don't forget to run `./autogen.sh` before trying a new build.  Can you provide a stack trace? Still getting this error with latest build with --enable-debug

```
Iteration 699: ALIGNED TRUTH : नियन कुदकि के टपि जा ... हइऽऽऽयाँ ! पैदल तऽ हइयाँ, बकि
Iteration 699: BEST OCR TEXT : नियन कदक के पि जा हाल त हइया बकि
File /tmp/tmp.21BVvgsmzO/bih/bih.SakalBharati.exp0.lstmf page 28 :
Mean rms=4.633%, delta=42.555%, train=94.938%(98.768%), skip ratio=0%
[Thread 0x7f5cbb4d0700 (LWP 17917) exited]
[New Thread 0x7f5cbb4d0700 (LWP 17918)]
lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.

Program received signal SIGABRT, Aborted.
[Switching to Thread 0x7f5cbb4d0700 (LWP 17918)]
0x00007f5cc0486c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace
#0  0x00007f5cc0486c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f5cc048a028 in __GI_abort () at abort.c:89
#2  0x00007f5cc047fbf6 in __assert_fail_base (fmt=0x7f5cc05d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=file@entry=0x7f5cc130df28 "../ccutil/genericvector.h", line=line@entry=697,
    function=function@entry=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
    at assert.c:92
#3  0x00007f5cc047fca2 in __GI___assert_fail (assertion=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=0x7f5cc130df28 "../ccutil/genericvector.h", line=697,
    function=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
#4  0x00007f5cc128fb63 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0) at ../ccutil/genericvector.h:697
#5  0x00007f5cc1290318 in operator[] (this=0x7fffd06bde40, this=0x7fffd06bde40, index=0) at lstmtrainer.cpp:920
#6  tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f5cbb4cf640, data=..., trainer=trainer@entry=0x7f5cbb4cf640) at lstmtrainer.cpp:921
#7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffd06bddd0, iteration=0, training_errors=<optimized out>, model_data=...,
    training_stage=0) at lstmtester.cpp:86
#8  0x000000000040b539 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffd06bddd0) at lstmtester.cpp:123
#9  0x00007f5cbe348184 in start_thread (arg=0x7f5cbb4d0700) at pthread_create.c:312
#10 0x00007f5cc054a37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb)
```
```

(gdb) frame 1
#1  0x00007f5cc048a028 in __GI_abort () at abort.c:89
89      abort.c: No such file or directory.
(gdb) frame 2
#2  0x00007f5cc047fbf6 in __assert_fail_base (fmt=0x7f5cc05d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=file@entry=0x7f5cc130df28 "../ccutil/genericvector.h", line=line@entry=697,
    function=function@entry=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
    at assert.c:92
92      assert.c: No such file or directory.
(gdb) frame 3
#3  0x00007f5cc047fca2 in __GI___assert_fail (assertion=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=0x7f5cc130df28 "../ccutil/genericvector.h", line=697,
    function=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
101     in assert.c
(gdb) frame 4
#4  0x00007f5cc128fb63 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0) at ../ccutil/genericvector.h:697
697       assert(index >= 0 && index < size_used_);
(gdb) frame 5
#5  0x00007f5cc1290318 in operator[] (this=0x7fffd06bde40, this=0x7fffd06bde40, index=0) at lstmtrainer.cpp:920
920                                        LSTMTrainer* trainer) {
(gdb) frame 6
#6  tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f5cbb4cf640, data=..., trainer=trainer@entry=0x7f5cbb4cf640) at lstmtrainer.cpp:921
921       return trainer->ReadSizedTrainingDump(&data[0], data.size());
(gdb) frame 7
#7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffd06bddd0, iteration=0, training_errors=<optimized out>, model_data=...,
    training_stage=0) at lstmtester.cpp:86
86        if (!trainer.ReadTrainingDump(model_data, &trainer)) {
(gdb) frame 8
#8  0x000000000040b539 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffd06bddd0) at lstmtester.cpp:123
123       lstmtester->test_result_ = lstmtester->RunEvalSync(
(gdb) frame 9
#9  0x00007f5cbe348184 in start_thread (arg=0x7f5cbb4d0700) at pthread_create.c:312
312     pthread_create.c: No such file or directory.
(gdb)
``` Sorry fixed in
https://github.com/tesseract-ocr/tesseract/commit/b86b4fa06ba4d2afa00c53470a19f6630e638f66
Please try again. I could do with a test for that...

On Mon, May 8, 2017 at 5:03 AM, Shreeshrii <notifications@github.com> wrote:

> Still getting this error with latest build with --enable-debug
>
> Iteration 699: ALIGNED TRUTH : नियन कुदकि के टपि जा ... हइऽऽऽयाँ ! पैदल तऽ हइयाँ, बकि
> Iteration 699: BEST OCR TEXT : नियन कदक के पि जा हाल त हइया बकि
> File /tmp/tmp.21BVvgsmzO/bih/bih.SakalBharati.exp0.lstmf page 28 :
> Mean rms=4.633%, delta=42.555%, train=94.938%(98.768%), skip ratio=0%
> [Thread 0x7f5cbb4d0700 (LWP 17917) exited]
> [New Thread 0x7f5cbb4d0700 (LWP 17918)]
> lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
>
> Program received signal SIGABRT, Aborted.
> [Switching to Thread 0x7f5cbb4d0700 (LWP 17918)]
> 0x00007f5cc0486c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
> 56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
> (gdb) backtrace
> #0  0x00007f5cc0486c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
> #1  0x00007f5cc048a028 in __GI_abort () at abort.c:89
> #2  0x00007f5cc047fbf6 in __assert_fail_base (fmt=0x7f5cc05d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
>     assertion=assertion@entry=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=file@entry=0x7f5cc130df28 "../ccutil/genericvector.h", line=line@entry=697,
>     function=function@entry=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
>     at assert.c:92
> #3  0x00007f5cc047fca2 in __GI___assert_fail (assertion=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=0x7f5cc130df28 "../ccutil/genericvector.h", line=697,
>     function=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
> #4  0x00007f5cc128fb63 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0) at ../ccutil/genericvector.h:697
> #5  0x00007f5cc1290318 in operator[] (this=0x7fffd06bde40, this=0x7fffd06bde40, index=0) at lstmtrainer.cpp:920
> #6  tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f5cbb4cf640, data=..., trainer=trainer@entry=0x7f5cbb4cf640) at lstmtrainer.cpp:921
> #7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffd06bddd0, iteration=0, training_errors=<optimized out>, model_data=...,
>     training_stage=0) at lstmtester.cpp:86
> #8  0x000000000040b539 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffd06bddd0) at lstmtester.cpp:123
> #9  0x00007f5cbe348184 in start_thread (arg=0x7f5cbb4d0700) at pthread_create.c:312
> #10 0x00007f5cc054a37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
> (gdb)
>
>
> (gdb) frame 1
> #1  0x00007f5cc048a028 in __GI_abort () at abort.c:89
> 89      abort.c: No such file or directory.
> (gdb) frame 2
> #2  0x00007f5cc047fbf6 in __assert_fail_base (fmt=0x7f5cc05d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
>     assertion=assertion@entry=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=file@entry=0x7f5cc130df28 "../ccutil/genericvector.h", line=line@entry=697,
>     function=function@entry=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
>     at assert.c:92
> 92      assert.c: No such file or directory.
> (gdb) frame 3
> #3  0x00007f5cc047fca2 in __GI___assert_fail (assertion=0x7f5cc130e4a0 "index >= 0 && index < size_used_", file=0x7f5cc130df28 "../ccutil/genericvector.h", line=697,
>     function=0x7f5cc132f480 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
> 101     in assert.c
> (gdb) frame 4
> #4  0x00007f5cc128fb63 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0) at ../ccutil/genericvector.h:697
> 697       assert(index >= 0 && index < size_used_);
> (gdb) frame 5
> #5  0x00007f5cc1290318 in operator[] (this=0x7fffd06bde40, this=0x7fffd06bde40, index=0) at lstmtrainer.cpp:920
> 920                                        LSTMTrainer* trainer) {
> (gdb) frame 6
> #6  tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f5cbb4cf640, data=..., trainer=trainer@entry=0x7f5cbb4cf640) at lstmtrainer.cpp:921
> 921       return trainer->ReadSizedTrainingDump(&data[0], data.size());
> (gdb) frame 7
> #7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffd06bddd0, iteration=0, training_errors=<optimized out>, model_data=...,
>     training_stage=0) at lstmtester.cpp:86
> 86        if (!trainer.ReadTrainingDump(model_data, &trainer)) {
> (gdb) frame 8
> #8  0x000000000040b539 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffd06bddd0) at lstmtester.cpp:123
> 123       lstmtester->test_result_ = lstmtester->RunEvalSync(
> (gdb) frame 9
> #9  0x00007f5cbe348184 in start_thread (arg=0x7f5cbb4d0700) at pthread_create.c:312
> 312     pthread_create.c: No such file or directory.
> (gdb)
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/561#issuecomment-299848534>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056bih7QudDYsxtY8yGjr7FhdJIQxgks5r3wSDgaJpZM4LJ7Lz>
> .
>



-- 
Ray.
 Still getting the error

```
tesseract -v
tesseract 4.00.00alpha-460-gb86b4fa
 leptonica-1.74.1
  libgif 5.0.5 : libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.0 : libopenjp2 2.1.0

 Found AVX
 Found SSE
```
```
Iteration 699: ALIGNED TRUTH : कृषिक्षॆत्रंचित्रपटाने साथ नयी पोस्टेड से ओलंपिक सेभोजनादिकम् सीप
Iteration 699: BEST OCR TEXT : किससिसिस
File /tmp/tmp.FOELAYZPOv/bih/bih.Samanata.exp0.lstmf page 215 :
Mean rms=4.813%, delta=46.035%, train=99.32%(99.97%), skip ratio=0%
[Thread 0x7f618d0e0700 (LWP 170) exited]
[New Thread 0x7f618d0e0700 (LWP 171)]
lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.

Program received signal SIGABRT, Aborted.
[Switching to Thread 0x7f618d0e0700 (LWP 171)]
0x00007f6191886c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace
#0  0x00007f6191886c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f619188a028 in __GI_abort () at abort.c:89
#2  0x00007f619187fbf6 in __assert_fail_base (fmt=0x7f61919d03b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7f619270d6c0 "index >= 0 && index < size_used_", file=file@entry=0x7f619270d148 "../ccutil/genericvector.h", line=line@entry=697,
    function=function@entry=0x7f619272ea60 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]")
    at assert.c:92
#3  0x00007f619187fca2 in __GI___assert_fail (assertion=0x7f619270d6c0 "index >= 0 && index < size_used_", file=0x7f619270d148 "../ccutil/genericvector.h", line=697,
    function=0x7f619272ea60 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
#4  0x00007f619268c803 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0) at ../ccutil/genericvector.h:697
#5  0x00007f619268cfb8 in operator[] (this=0x7fffe8f33fc0, this=0x7fffe8f33fc0, index=0) at lstmtrainer.cpp:920
#6  tesseract::LSTMTrainer::ReadTrainingDump (this=this@entry=0x7f618d0df640, data=..., trainer=trainer@entry=0x7f618d0df640) at lstmtrainer.cpp:921
#7  0x000000000040b03e in tesseract::LSTMTester::RunEvalSync (this=this@entry=0x7fffe8f33f50, iteration=0, training_errors=<optimized out>, model_data=...,
    training_stage=0) at lstmtester.cpp:86
#8  0x000000000040b539 in tesseract::LSTMTester::ThreadFunc (lstmtester_void=0x7fffe8f33f50) at lstmtester.cpp:123
#9  0x00007f618f748184 in start_thread (arg=0x7f618d0e0700) at pthread_create.c:312
#10 0x00007f619194a37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) quit
``` I tried with a different set of input files for training and eval, same error. **Seems to happen at first checkpoint writing after train % falls below 100.** 

```
Iteration 496: ALIGNED TRUTH : ग़, ज़, और फ़। इसलिए आपको केवल इन पाँचों पर ध्यान
Iteration 496: BEST OCR TEXT :  ज र  सलि आपको केवल न पावो पर ध्यान
File /tmp/tmp.VitmU3pEv2/bih/bih.Mangal.exp0.lstmf page 1889 :
Mean rms=4.735%, delta=39.487%, train=92.02%(98.26%), skip ratio=0%
Iteration 497: ALIGNED TRUTH : वर्गमीटर मरे यथाशक्ति न्यायशास्त्री फैक्स। प्रेममयी सोद्देश्यवादी
Iteration 497: BEST OCR TEXT : वमीदर मरे यधाशि नयायशा कस पेममय सोदियवादी
File /tmp/tmp.VitmU3pEv2/bih/bih.Siddhanta.exp0.lstmf page 747 :
Mean rms=4.731%, delta=39.423%, train=91.922%(98.234%), skip ratio=0%
Iteration 498: ALIGNED TRUTH : महीपतिया हो जुआर जसे हारे तइसे ले गआव मोरा पास जहंवा
Iteration 498: BEST OCR TEXT : महीपतिया हो जआर जसे हार तसे ले व मोरा पास जहवा
File /tmp/tmp.VitmU3pEv2/bih/bih.Mangal.exp0.lstmf page 30 :
Mean rms=4.726%, delta=39.356%, train=91.76%(98.129%), skip ratio=0%
Iteration 499: ALIGNED TRUTH : रद्दा। रार धडधड धड घ्रघ्र धडल्ला घ्रघर्र दरारदार ऋणाधार
Iteration 499: BEST OCR TEXT : रदा रार धध ध धरधर ला धरधर दरारदार णाधार
File /tmp/tmp.VitmU3pEv2/bih/bih.Siddhanta.exp0.lstmf page 1157 :
Mean rms=4.721%, delta=39.3%, train=91.663%(98.088%), skip ratio=0%
lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
``` Checked with eng, same set of input files. fails when I give the --eval_listfile.

Only train_listfile - NO ERROR

```
 lstmtraining  \
>    -U ~/tesstutorial/nyd/eng.unicharset \
>   --train_listfile ~/tesstutorial/nyd/eng.training_files.txt \
>   --script_dir ../langdata   \
>   --append_index 5 --net_spec '[Lfx256 O1c105]' \
>   --continue_from ~/tesstutorial/nydlayer/eng.lstm \
>   --model_output ~/tesstutorial/nydlayer/nyd \
>   --debug_interval -1 \
>   --target_error_rate 0.01
Loaded file /home/shree/tesstutorial/nydlayer/eng.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tesstutorial/nydlayer/eng.lstm
Other case É of é is not in unicharset
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Num outputs,weights in serial:
  Lfx256:256, 394240
  Fc105:105, 26985
Total weights = 421225
Built network:[1,0,0,1[C5,5Ft16]Mp3,3Lfys64Lfx128Lrx128Lfx256Fc105] from request [Lfx256 O1c105]
Training parameters:
  Debug interval = -1, weights = 0.1, learning rate = 0.0001, momentum=0.9
...
Mean rms=2.758%, delta=16.418%, train=35.971%(49.658%), skip ratio=0%
Iteration 999: ALIGNED TRUTH : MySQL { & Advanced her BOX me Management your new have Post by
Iteration 999: BEST OCR TEXT : MySQL ( & Advanced her BON me Management your new have Post by
File /tmp/tmp.RQfY2cDM00/eng/eng.Arial.exp-2.lstmf page 31 :
Mean rms=2.757%, delta=16.403%, train=35.942%(49.624%), skip ratio=0%
2 Percent improvement time=49, best error was 39.615 @ 789
At iteration 838/1000/1000, Mean rms=2.757%, delta=16.403%, char train=35.942%, word train=49.624%, skip ratio=0%,  New best char error = 35.942 wrote best model:/home/s
hree/tesstutorial/nydlayer/nyd35.942_838.lstm wrote checkpoint.

Iteration 1000: ALIGNED TRUTH : Bailey Joshua, mate, 190 Eldridge
Iteration 1000: BEST OCR TEXT : Bailey Joshua, mate, 190 Eldridge
File /home/shree/tesstutorial/nyd/eng.1852nydir.exp0.lstmf page 26 (Perfect):
```
 **ERROR - even when using both train and eval with same set of files** 

```
 lstmtraining  \
>    -U ~/tesstutorial/nyd/eng.unicharset \
>   --train_listfile ~/tesstutorial/nyd/eng.training_files.txt \
>     --eval_listfile ~/tesstutorial/nyd/eng.training_files.txt \
>   --script_dir ../langdata   \
>   --append_index 5 --net_spec '[Lfx256 O1c105]' \
>   --continue_from ~/tesstutorial/nydlayer/eng.lstm \
>   --model_output ~/tesstutorial/nydlayer/nyd \
>   --debug_interval -1 \
>   --target_error_rate 0.01
Loaded file /home/shree/tesstutorial/nydlayer/eng.lstm, unpacking...
Warning: LSTMTrainer deserialized an LSTMRecognizer!
Continuing from /home/shree/tesstutorial/nydlayer/eng.lstm
Other case É of é is not in unicharset
Appending a new network to an old one!!Setting unichar properties
Setting properties for script Common
Setting properties for script Latin
Num outputs,weights in serial:
  Lfx256:256, 394240
  Fc105:105, 26985
Total weights = 421225
Built network:[1,0,0,1[C5,5Ft16]Mp3,3Lfys64Lfx128Lrx128Lfx256Fc105] from request [Lfx256 O1c105]
Training parameters:
  Debug interval = -1, weights = 0.1, learning rate = 0.0001, momentum=0.9
Loaded 29/29 pages (1-29) of document /home/shree/tesstutorial/nyd/eng.1852nydir.exp0.lstmf
Loaded 8/8 pages (1-8) of document /home/shree/tesstutorial/nyd/eng.1852nydir.exp1.lstmf
Loaded 12/12 pages (1-12) of document /home/shree/tesstutorial/nyd/eng.1852nydir.exp2.lstmf
Loaded 29/29 pages (1-29) of document /home/shree/tesstutorial/nyd/eng.1852nydir.exp-1.lstmf
Loaded 29/29 pages (1-29) of document /home/shree/tesstutorial/nyd/eng.1852nydir.exp0.lstmf
Loaded 104/104 pages (1-104) of document /home/shree/tesstutorial/nyd/eng.Arial.exp-2.lstmf
Loaded 29/29 pages (1-29) of document /home/shree/tesstutorial/nyd/eng.1852nydir.exp-1.lstmf

...
Iteration 298: ALIGNED TRUTH : Bailey Julia, 167% WWooster
Iteration 298: BEST OCR TEXT : Baiiey aiia, % ooter
File /home/shree/tesstutorial/nyd/eng.1852nydir.exp2.lstmf page 11 :
Mean rms=5.44%, delta=46.985%, train=96.919%(99.93%), skip ratio=0%
Iteration 299: ALIGNED TRUTH : AA Q WW E R T Y UU II O P L KK J HH GG F D S ZZ X C V B NN MMM 0 9 8 7 7 6 5 4 3
Iteration 299: BEST OCR TEXT :     B            o      B
File /tmp/tmp.RQfY2cDM00/eng/eng.Arial.exp-2.lstmf page 81 :
Mean rms=5.435%, delta=46.865%, train=96.775%(99.92%), skip ratio=0%
lstmtraining: ../ccutil/genericvector.h:697: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
``` Related https://github.com/tesseract-ocr/tesseract/issues/644 @theraysmith

Latest code change has reverted the fix for this issue

```
Iteration 99: ALIGNED TRUTH : च छ ज झ ञ उ ऊ ट ठ ड ढ ण ऋ ॠ त थ
Iteration 99: BEST OCR TEXT :
File /tmp/tmp.m82dWGBYZW/mar/mar.Aparajita.exp0.lstmf page 3 :
Mean rms=5.427%, delta=53.011%, train=110.2%(100%), skip ratio=0%
lstmtraining: genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
``` Fixed in 4b6f0b95..f4f66f8f

On Sat, Jul 15, 2017 at 1:04 AM, Shreeshrii <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith>
>
> Latest code change has reverted the fix for this issue
>
> Iteration 99: ALIGNED TRUTH : च छ ज झ ञ उ ऊ ट ठ ड ढ ण ऋ ॠ त थ
> Iteration 99: BEST OCR TEXT :
> File /tmp/tmp.m82dWGBYZW/mar/mar.Aparajita.exp0.lstmf page 3 :
> Mean rms=5.427%, delta=53.011%, train=110.2%(100%), skip ratio=0%
> lstmtraining: genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
> Aborted (core dumped)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/561#issuecomment-315518059>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056YjQzLN_XVXLe8ENvek_1eABxTHTks5sOHJ2gaJpZM4LJ7Lz>
> .
>



-- 
Ray.
 Still getting the error, as of commit f4f66f8:

```
Iteration 99: ALIGNED TRUTH : पुनर्व्यवस्थित अमेरिकी इंडोनेशिया
Iteration 99: BEST OCR TEXT :
File /tmp/tmp.7Z9YtK1Bru/hin/hin.Siddhanta.exp0.lstmf page 519 :
Mean rms=5.531%, delta=52.419%, train=109.818%(100%), skip ratio=0%
lstmtraining: ../ccutil/genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
```
Version
```
shree@sanskrit:~/tesseract$ tesseract -v
tesseract f4f66f8
 leptonica-1.74.4
  libgif 4.1.6(?) : libjpeg 8d (libjpeg-turbo 1.3.0) : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.0 : libopenjp2 2.1.2
```
Training command used
```
nice lstmtraining   -U ~/tesstutorial/hintest/hin.unicharset   --train_listfile ~/tesstutorial/hintest/hin.training_files.txt   --eval_listfile ~/tesstutorial/hineval/hin.training_files.txt   --continue_from ~/tesstutorial/hintest/hin.lstm   --model_output ~/tesstutorial/hintest/hinlayer   --script_dir ../langdata   --append_index 5   --net_spec '[Lfx384 O1c105]'   --target_error_rate 0.01   --perfect_sample_delay 19   --debug_interval -1
``` Error happens later with finetune command - same set of files

```
nice lstmtraining    --train_listfile ~/tesstutorial/hintest/hin.training_files.txt   --eval_listfile ~/tesstutorial/hineval/hin.training_files.txt   --continue_from ~/tesstutorial/hintest/hin.lstm   --model_output ~/tesstutorial/hintest/hintune   --target_error_rate 0.01   --perfect_sample_delay 19   --debug_interval -1
```
Error message - core dumped
```
Iteration 199: ALIGNED TRUTH : मुद्राओं घुसपैठ व्हिटफोर्ड इंटरनेट
Iteration 199: BEST OCR TEXT : मुद्राओं घुसपैठ व्हिटफोर्ड इंटरनेट
File /tmp/tmp.7Z9YtK1Bru/hin/hin.FreeSans.exp0.lstmf page 905 (Perfect):
Mean rms=0.276%, delta=0.118%, train=0.285%(1.056%), skip ratio=2%
lstmtraining: ../ccutil/genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
Aborted (core dumped)
``` There might be another call site. Can you generate a stack trace?

On Sat, Jul 15, 2017 at 9:48 PM, Shreeshrii <notifications@github.com>
wrote:

> Error happens later with finetune command - same set of files
>
> nice lstmtraining    --train_listfile ~/tesstutorial/hintest/hin.training_files.txt   --eval_listfile ~/tesstutorial/hineval/hin.training_files.txt   --continue_from ~/tesstutorial/hintest/hin.lstm   --model_output ~/tesstutorial/hintest/hintune   --target_error_rate 0.01   --perfect_sample_delay 19   --debug_interval -1
>
> Error message - core dumped
>
> Iteration 199: ALIGNED TRUTH : मुद्राओं घुसपैठ व्हिटफोर्ड इंटरनेट
> Iteration 199: BEST OCR TEXT : मुद्राओं घुसपैठ व्हिटफोर्ड इंटरनेट
> File /tmp/tmp.7Z9YtK1Bru/hin/hin.FreeSans.exp0.lstmf page 905 (Perfect):
> Mean rms=0.276%, delta=0.118%, train=0.285%(1.056%), skip ratio=2%
> lstmtraining: ../ccutil/genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.
> Aborted (core dumped)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/561#issuecomment-315585338>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056b9wIbO9wayBkXcecWY-qlLPdjx2ks5sOZYfgaJpZM4LJ7Lz>
> .
>



-- 
Ray.
 with finetune command

```
lstmtraining: ../ccutil/genericvector.h:713: T& GenericVector<T>::operator[](int) const [with T = char]: Assertion `index >= 0 && index < size_used_' failed.

Program received signal SIGABRT, Aborted.
0x00007ffff628bc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace
#0  0x00007ffff628bc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff628f028 in __GI_abort () at abort.c:89
#2  0x00007ffff6284bf6 in __assert_fail_base (fmt=0x7ffff63d9018 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7ffff70f5be0 "index >= 0 && index < size_used_",
    file=file@entry=0x7ffff70f5668 "../ccutil/genericvector.h", line=line@entry=713,
    function=function@entry=0x7ffff7116d20 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:92
#3  0x00007ffff6284ca2 in __GI___assert_fail (assertion=0x7ffff70f5be0 "index >= 0 && index < size_used_",
    file=0x7ffff70f5668 "../ccutil/genericvector.h", line=713,
    function=0x7ffff7116d20 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
#4  0x00007ffff7071983 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0)
    at ../ccutil/genericvector.h:713
#5  0x00007ffff7074dd5 in operator[] (this=<optimized out>, this=<optimized out>, index=<optimized out>) at lstmtrainer.cpp:1335
#6  tesseract::LSTMTrainer::UpdateErrorGraph (this=this@entry=0x7fffffffd780, iteration=iteration@entry=11,
    error_rate=error_rate@entry=0.28499999999999998, model_data=..., tester=tester@entry=0x845ce0) at lstmtrainer.cpp:1272
#7  0x00007ffff70780e7 in tesseract::LSTMTrainer::MaintainCheckpoints (this=this@entry=0x7fffffffd780, tester=tester@entry=0x845ce0,
    log_msg=log_msg@entry=0x7fffffffd350) at lstmtrainer.cpp:338
#8  0x0000000000407712 in main (argc=1, argv=0x7fffffffe408) at lstmtraining.cpp:197
``` with replace top layer command

```
(gdb) backtrace
#0  0x00007ffff628bc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff628f028 in __GI_abort () at abort.c:89
#2  0x00007ffff6284bf6 in __assert_fail_base (fmt=0x7ffff63d9018 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
    assertion=assertion@entry=0x7ffff70f5be0 "index >= 0 && index < size_used_",
    file=file@entry=0x7ffff70f5668 "../ccutil/genericvector.h", line=line@entry=713,
    function=function@entry=0x7ffff7116d20 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:92
#3  0x00007ffff6284ca2 in __GI___assert_fail (assertion=0x7ffff70f5be0 "index >= 0 && index < size_used_",
    file=0x7ffff70f5668 "../ccutil/genericvector.h", line=713,
    function=0x7ffff7116d20 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
#4  0x00007ffff7071983 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0)
    at ../ccutil/genericvector.h:713
#5  0x00007ffff7074dd5 in operator[] (this=<optimized out>, this=<optimized out>, index=<optimized out>) at lstmtrainer.cpp:1335
#6  tesseract::LSTMTrainer::UpdateErrorGraph (this=this@entry=0x7fffffffd6d0, iteration=iteration@entry=100,
    error_rate=error_rate@entry=109.818, model_data=..., tester=tester@entry=0x845d00) at lstmtrainer.cpp:1272
#7  0x00007ffff70780e7 in tesseract::LSTMTrainer::MaintainCheckpoints (this=this@entry=0x7fffffffd6d0, tester=tester@entry=0x845d00,
    log_msg=log_msg@entry=0x7fffffffd2a0) at lstmtrainer.cpp:338
#8  0x0000000000407712 in main (argc=1, argv=0x7fffffffe398) at lstmtraining.cpp:197
(gdb)
``` Did my most recent commit fix it?
https://github.com/tesseract-ocr/tesseract/commit/45fb7dde495df31629b7b5ca36b654a6bccfb762

On Sat, Jul 15, 2017 at 10:53 PM, Shreeshrii <notifications@github.com>
wrote:

> with replace top layer command
>
> (gdb) backtrace
> #0  0x00007ffff628bc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
> #1  0x00007ffff628f028 in __GI_abort () at abort.c:89
> #2  0x00007ffff6284bf6 in __assert_fail_base (fmt=0x7ffff63d9018 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
>     assertion=assertion@entry=0x7ffff70f5be0 "index >= 0 && index < size_used_",
>     file=file@entry=0x7ffff70f5668 "../ccutil/genericvector.h", line=line@entry=713,
>     function=function@entry=0x7ffff7116d20 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:92
> #3  0x00007ffff6284ca2 in __GI___assert_fail (assertion=0x7ffff70f5be0 "index >= 0 && index < size_used_",
>     file=0x7ffff70f5668 "../ccutil/genericvector.h", line=713,
>     function=0x7ffff7116d20 <_ZZNK13GenericVectorIcEixEiE19__PRETTY_FUNCTION__> "T& GenericVector<T>::operator[](int) const [with T = char]") at assert.c:101
> #4  0x00007ffff7071983 in GenericVector<char>::operator[] (this=<optimized out>, this=<optimized out>, index=0)
>     at ../ccutil/genericvector.h:713
> #5  0x00007ffff7074dd5 in operator[] (this=<optimized out>, this=<optimized out>, index=<optimized out>) at lstmtrainer.cpp:1335
> #6  tesseract::LSTMTrainer::UpdateErrorGraph (this=this@entry=0x7fffffffd6d0, iteration=iteration@entry=100,
>     error_rate=error_rate@entry=109.818, model_data=..., tester=tester@entry=0x845d00) at lstmtrainer.cpp:1272
> #7  0x00007ffff70780e7 in tesseract::LSTMTrainer::MaintainCheckpoints (this=this@entry=0x7fffffffd6d0, tester=tester@entry=0x845d00,
>     log_msg=log_msg@entry=0x7fffffffd2a0) at lstmtrainer.cpp:338
> #8  0x0000000000407712 in main (argc=1, argv=0x7fffffffe398) at lstmtraining.cpp:197
> (gdb)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/561#issuecomment-315587741>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056drHnXKXmXSALw-Y7CNl8ynuEGB5ks5sOaVhgaJpZM4LJ7Lz>
> .
>



-- 
Ray.
 Yes, it did. Thank you, @stweil and @theraysmith .  I do not have the files saved to reproduce the problem. I will reopen if it occurs again.

There are also other error/warning messages in this test run, which may/may not be related.

>Unichar 1481=र्ब्रह्मघा->र्ब्रह्मघा is too long to encode!!

>Deserialize header failed:

>First document cannot be empty!!  @theraysmith 

These two Leptonica calls have optional debugging parameters.

pixGenerateHalftoneMask
pixGenHalftoneMask

They are found in imagefind.cpp in code and comments, to support an optional
parameter called textord_tabfind_show_images

https://github.com/tesseract-ocr/tesseract/blob/master/textord/imagefind.cpp#L86

In general, the debug paths in Leptonica are not great from security perspective
and setting that debug parameter to null or false is recommended. If possible
I encourage removing the the textord_tabfind_show_images feature entirely.

In addition, there is significant use of pixDisplayWrite for debugging purposes.
This is deprecated and also a security headache, and I encourage removing
all calls to this function.

It is possible that this list is not exhaustive but it probably is. The rule of thumb 
is try to avoid debug-specific Leptonica code paths wherever possible in release builds.

 Fixed in commit a1c22fb.  Still getting large number of these errors. The box file was created by text2image.

```

Loaded 17903/17903 pages (1-17903) of document /tmp/tmp.A6VtVWpCVT/san/san.Sanskrit_Text.exp0.lstmf
Loaded 14102/14102 pages (1-14102) of document /tmp/tmp.A6VtVWpCVT/san/san.Santipur_OT_Medium.exp0.lstmf
Bad box coordinates in boxfile string! ति झ. पाठः॥ 67-5 क्षयं 478 567 1002 625 83
Bad box coordinates in boxfile string! विचाराचार अध्याय श्रवस्यः योऽयं 117 3530 800 3593 259
Bad box coordinates in boxfile string! न्धिमहोदयस्य च जनवरी-मार्च 426 1260 1016 1332 345
Bad box coordinates in boxfile string! ण्टिर्गौ न्प्रा स्फो  त्फ द्भ्या अस्योद्यानं 190 2392 908 2450 434
Bad box coordinates in boxfile string! ष्पित॥ यस्य १२ मया। अर्क 418 456 969 511 463
No block overlapping textline: ओदनभोजिकाभिः स्कौनगरिकस्य गुरुपत्नीं षट्तन्त्रीसारे।
``` @theraysmith 

I looked at the over 2000 textlines which are getting this error for Devanagari training that I am trying to do right now.

The common thread in all of these is that these textlines begin with words beginning with `i` matraa, which is the only combining mark in Devanagari which is rendered before the consonant it applies to.

Same was the case in the first post here as well as in https://github.com/tesseract-ocr/tesseract/issues/555
  See new section Error Messages From Training  Getting same error even with WordStr format box files. 

Also, unicharset extraction from the box/tiff pairs following the one  in error does not take place.

```
Utf8 buffer too big, size=61 for श्री तुलस्युपाख्यानम्
Utf8 buffer too big, size=40 for सप्तदशोऽध्याय:
Utf8 buffer too big, size=34 for न व म स्क न्धे
Extracting unicharset from /tmp/tmp.qLNJdHgcKr/san/Deva-test.box
Extracting unicharset from /tmp/tmp.qLNJdHgcKr/san/Deva-test-wordstr.box
```
box file used
```
WordStr 27 258 389 325 0 #श्री तुलस्युपाख्यानम्
	 399 258 409 312 0
WordStr 34 183 340 232 0 #सप्तदशोऽध्याय:
	 350 188 361 209 0
WordStr 39 102 254 156 0 #न व म स्क न्धे
	 264 102 291 156 0
WordStr 32 30 184 67 0 #अ ध्या याः
	 194 33 204 64 0
```
![deva-test-wordstr](https://user-images.githubusercontent.com/5095331/28318769-93a6bcca-6be9-11e7-8c45-ea92b12e0486.png)
  Edits made. Filed question over unicharset extraction as a separate issue (https://github.com/tesseract-ocr/tesseract/issues/653)  See new section in trainingtesseract-4.00 Changes are pushed now. I got called away yesterday before I was able to do
it.

On Thu, Jan 12, 2017 at 2:36 AM, Amit D. <notifications@github.com> wrote:

> I don't see the changes either.
>
> The wiki can be cloned as a git repo. Ray probably did some edits locally,
> but didn't 'push' them.
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/549#issuecomment-272130094>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056X0eolRJLjvYL3TR3hp1-wfTyoGKks5rRgJFgaJpZM4LIjyK>
> .
>



-- 
Ray.
 The tab character (9) at the beginning of the list of failure bytes is a
dead giveaway.

On Sat, Jan 21, 2017 at 6:15 AM, Shreeshrii <notifications@github.com>
wrote:

>
> Encoding of string failed! Failure bytes: 9 31 32 30 30 45 6d 69 6c 69 65 2c 68 61 6e 73 4b 6f 6e 65 2e
> Can't encode transcription: Møller.     1200Emilie,hansKone.
>
>
> when trying to train frk
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/549#issuecomment-274264239>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Z_ATRDHUb3698yrRFfl1XSJTJM3ks5rUhMAgaJpZM4LIjyK>
> .
>



-- 
Ray.
 @theraysmith 

I am still getting this error, for a new replace top layer training for Devanagari script, where the eval_listfile is based on a different training text. eg.

```
Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff81 ffffffe0 ffffffa4 ffffff9a ffffffe0 ffffffa5 ffffff8d ffffffe0 ffffffa4 ffffff9a ffffffe0 ffffffa5 ffffff88 ffffffe0 ffffffa4 ffffff95 ffffffe0 ffffffa5 ffffff8b 20 ffffffe0 ffffffa4 ffffff9c ffffffe0 ffffffa5 ffffff80 ffffffe0 ffffffa4 ffffffb5 ffffffe0 ffffffa4 ffffffa8
Can't encode transcription: वैशाख साल देखि साथै यो साँच्चैको जीवन

Encoding of string failed! Failure bytes: ffffffe0 ffffffa4 ffffff81 ffffffe0 ffffffa4 ffffffa6 ffffffe0 ffffffa4 ffffffbe
Can't encode transcription: रूपांतरित जैबुन्निसा केंद्रित छँदा
````
While each unicode character (स ा ँ ) is there in the Devanagari unicharset, the combined akshara (साँ, छँ) is not there as part of training text/unicharset, but is there as part of eval text/unicharset.

The training unicharset is of the following format:

```
3784
NULL 0 NULL 0
Joined 7 0,69,188,255,486,1218,0,30,486,1188 Latin 1 0 1 Joined	# Joined [4a 6f 69 6e 65 64 ]a
|Broken|0|1 f 0,69,186,255,892,2138,0,80,892,2058 Common 3625 10 3625 |Broken|0|1	# Broken
र्ध्रु 1 0,64,61,197,280,356,0,0,280,356 Devanagari 18 0 18 र्ध्रु	# र्ध्रु [930 94d 927 94d 930 941 ]x
र्बृ 1 3,64,61,197,181,236,0,0,181,236 Devanagari 18 0 18 र्बृ	# र्बृ [930 94d 92c 943 ]x
श्चु 1 0,64,61,197,251,303,0,12,251,291 Devanagari 240 0 240 श्चु	# श्चु [936 94d 91a 941 ]x
श्चौ 1 3,65,61,255,294,367,0,12,294,355 Devanagari 240 0 240 श्चौ	# श्चौ [936 94d 91a 94c ]x
श्च् 1 3,64,61,197,251,303,0,12,251,291 Devanagari 240 0 240 श्च्	# श्च् [936 94d 91a 94d ]x
य 1 63,64,192,192,114,142,0,0,111,133 Devanagari 8 0 8 य	# य [92f ]x
श्रीः 1 3,74,61,253,295,412,0,12,295,400 Devanagari 240 0 240 श्रीः	# श्रीः [936 94d 930 940 903 ]x
ष्ठु 1 0,75,61,197,204,243,0,0,204,243 Devanagari 241 0 241 ष्ठु	# ष्ठु [937 94d 920 941 ]x
ष्ठौ 1 3,75,61,255,247,307,0,0,247,307 Devanagari 241 0 241 ष्ठौ	# ष्ठौ [937 94d 920 94c ]x
स्रैः 1 3,76,61,255,243,449,0,0,243,449 Devanagari 280 0 280 स्रैः	# स्रैः [938 94d 930 948 903 ]x
...
```

Does this mean that the training text needs to be expanded to include all possible akshara combinations?  Posted on 
https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/bVq_mGnPsN4/LqYyZg1ICQAJ  Please use tesseract user forum for asking support.  pixaGetCount errors fixed.
For no block overlapping textline, see new section in training tesseract 4.00. As mentioned by Ray in the comment above, please see https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00#error-messages-from-training

>**No block overlapping textline:** occurs when layout analysis fails to correctly segment the image that was given as training data. The textline is dropped. Not much problem if there aren't many, but if there are a lot, there is probably something wrong with the training text or rendering process.  This report should be in tessdata or langdata  repository   Disable debugging and informational messages from Leptonica
for release builds.

Signed-off-by: Stefan Weil <sw@weilnetz.de> Using the environment variable would imply that you always get at least one message from Leptonica because the code which enables / disables warnings also writes messages. :-( That's why I decided against that feature for the moment.
Leptonica needs a modification which allows suppressing messages from setMsgSeverity, no matter whether the environment variable exists or not and no matter which message severity level is set. Yes, I'd prefer keeping the interface unchanged and disabling the messages (at least for arguments in the valid range). An unset environment variable should not result in a warning, as this is the normal case. With the latest Leptonica code this requires unnecessarily tricky code. I'd use `L_INFO` instead of `L_WARNING`.  Fixed in https://github.com/tesseract-ocr/langdata/commit/3299c600323a511486fdab58c8e31258c308a7bc.
I'm retesting now. It seems the tutorial works without it, so I imagine the accuracy numbers in the tutorial will come out different. I've just updated the numbers in the training tutorial.
It seems to work apart from one thing that needs looking at - it doesn't run the eval  from the trainer.
It doesn't harm the tutorial, but will be required before people start serious training.  Not 100% sure, but I think I already have a requirement on 1.73 to support PDF generation. That is pretty aggressive. Hope the multipage TIFF users appreciate it.

On Dec 24, 2016 11:10 PM, "Amit D." <notifications@github.com> wrote:

> Since commit 11f2057
> <https://github.com/tesseract-ocr/tesseract/commit/11f205707eda769dbe6cc7d6839745f1b01a1d76>
> we depend on 1.74.0
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/540#issuecomment-269112619>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEu2ppN95ut9tOVVOanEd-g9HWJuJB6Dks5rLhbXgaJpZM4LGDsQ>
> .
>
 1.73 has O (n^3) for seeks on multi page TIFF. 1.74 drops that to O(n^2)
for the existing API and O(n) for some new API calls.

I have never made a PPA before and am not sure what is involved. Priority
is official Leptonica packaging. Unclear if l will even get that far before
the new year, due to travel.

So the answer is maybe, but I wasn't planning on it, and certainly not
right now.

On Dec 25, 2016 11:57 AM, "Amit D." <notifications@github.com> wrote:

> Jeff,
> Any chance you will provide a Leptonica 1.74 PPA for Ubuntu 16.04?
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/540#issuecomment-269135418>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEu2pj7OCq7Ics1Saik6-5_FHHfRaFm7ks5rLsrCgaJpZM4LGDsQ>
> .
>
 There are still numerous platforms without prebuild Leptonica 1.74.x. I just suggested updating the version for [MacPorts](https://github.com/macports/macports-ports/pull/180). Maybe fallback code which is slower for multi page tiffs but which works with older Leptonica versions would really be a good idea.  I'm totally fine with any decision, including reverting the new TIFF calls until Leptonica 1.74  is more widespread. This was the changelist.

https://github.com/tesseract-ocr/tesseract/commit/245eebdf293ac19f1fb85c36e51daaaa1b5e5a3e  >psm 3 - recognizes text at large font size
psm 6 - recognizes text at smaller font size

The result is similar with the best traineddata and current code, though there are differences between best/Devanagari, best/hin and best/san.

psm 3 treats smaller text as 'diacritics' - jpg at 600dpi

```
**************************** ./fontsize.jpg **********************************
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Detected 29 diacritics

real    0m14.309s
user    0m13.406s
sys     0m0.656s
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Detected 145 diacritics

real    0m8.970s
user    0m8.469s
sys     0m0.359s
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Detected 145 diacritics

real    0m4.188s
user    0m3.813s
sys     0m0.375s
``` Same image at 300 dpi, gets fewer blobs recognized as diacritics.

```
**************************** ./fontsize.jpg **********************************
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Detected 3 diacritics

real    0m18.550s
user    0m17.688s
sys     0m0.688s
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Detected 17 diacritics

real    0m16.757s
user    0m15.203s
sys     0m0.438s
Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica
Detected 17 diacritics

real    0m6.672s
user    0m6.125s
sys     0m0.438s
```  Let's get a backtrace for that error message, so we can understand context and make a smart decision. I am not sure how serious a problem this is.

```
(gdb) backtrace
#0  pixClone (pixs=0x0) at third_party/leptonica/src/pix1.c:523
#1  0x000000000056c149 in tesseract::Tesseract::set_pix_original (this=0x7ffff56b5020, original_pix=0x0)
    at ccmain/tesseractclass.h:206
#2  0x00000000005636f1 in tesseract::TessBaseAPI::SetInputImage (this=0x7fffffffd8f0, pix=0x0)
    at api/baseapi.cpp:894
#3  0x0000000000569045 in tesseract::TessBaseAPI::Clear (this=0x7fffffffd8f0) at api/baseapi.cpp:2039
#4  0x0000000000562099 in tesseract::TessBaseAPI::End (this=0x7fffffffd8f0) at api/baseapi.cpp:2049
#5  0x0000000000562073 in tesseract::TessBaseAPI::~TessBaseAPI (this=0x7fffffffd8f0) at api/baseapi.cpp:133
#6  0x0000000000561429 in main (argc=5, argv=0x7fffffffdae8) at api/tesseractmain.cpp:515
``` Best guess now is that the bug is harmless, and that this is the right fix. I didn't think about this too deeply.
(Edit: Increasingly confident about harmless and that this is the right fix.)

```diff
--- tesseract/ccmain/tesseractclass.h	2017-01-26 15:05:52.000000000 -0800
+++ tesseract/ccmain/tesseractclass.h	2017-01-27 14:01:50.000000000 -0800
@@ -203,7 +203,8 @@
     pix_original_ = original_pix;
     // Clone to sublangs as well.
     for (int i = 0; i < sub_langs_.size(); ++i)
-      sub_langs_[i]->set_pix_original(pixClone(original_pix));
+      sub_langs_[i]->set_pix_original(
+          original_pix ? pixClone(original_pix) : nullptr);
   }
   // Returns a pointer to a Pix representing the best available (original) image
   // of the page. Can be of any bit depth, but never color-mapped, as that has
``` Committed. Let's wait what happens ;-)   Reports  `Pre-4.0.0` for traineddata from 4.00.00alpha

```
combine_tessdata -u ../tessdata/eng.traineddata eng.
Extracting tessdata components from ../tessdata/eng.traineddata
Wrote eng.unicharset
Wrote eng.unicharambigs
Wrote eng.inttemp
Wrote eng.pffmtable
Wrote eng.normproto
Wrote eng.punc-dawg
Wrote eng.word-dawg
Wrote eng.number-dawg
Wrote eng.freq-dawg
Wrote eng.cube-unicharset
Wrote eng.cube-word-dawg
Wrote eng.shapetable
Wrote eng.bigram-dawg
Wrote eng.lstm
Wrote eng.lstm-punc-dawg
Wrote eng.lstm-word-dawg
Wrote eng.lstm-number-dawg
Wrote eng.version
Version string:Pre-4.0.0
1:unicharset:size=7477, offset=192
2:unicharambigs:size=1047, offset=7669
3:inttemp:size=976552, offset=8716
4:pffmtable:size=844, offset=985268
5:normproto:size=13408, offset=986112
6:punc-dawg:size=4322, offset=999520
7:word-dawg:size=1082890, offset=1003842
8:number-dawg:size=6426, offset=2086732
9:freq-dawg:size=1410, offset=2093158
11:cube-unicharset:size=1511, offset=2094568
12:cube-word-dawg:size=1062106, offset=2096079
13:shapetable:size=63346, offset=3158185
14:bigram-dawg:size=16109842, offset=3221531
17:lstm:size=5390718, offset=19331373
18:lstm-punc-dawg:size=4322, offset=24722091
19:lstm-word-dawg:size=7143578, offset=24726413
20:lstm-number-dawg:size=3530, offset=31869991
23:version:size=9, offset=31873521
``` So there is no way to distinguish 3.0x traineddata from 4.00.00alpha traineddata.

I guess, program could check if component 17:lstm is there. If not there version will be 3.0x. Correct. That is exactly the plan.

On Sat, Jul 15, 2017 at 9:33 AM, Amit D. <notifications@github.com> wrote:

> The current traineddata files for 4.0 will be obsolete once Ray will push
> the new ones, which will probably have the version file.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/534#issuecomment-315545661>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056dfEz45RTZMO9q2Ke_chDarvPOkQks5sOOnOgaJpZM4LD3In>
> .
>



-- 
Ray.
 >Please add version info to traineddata files and check for correct version during runtime. 

Version Info has been added to traineddata. Thanks, @theraysmith 

Does the program check the traineddata version and give error if it is unsupported/old format?   This also fixes issue #96. What about moving to `#pragma once`? Ah,ok. No objections in this case. `#pragma once` is widely supported, but not a standard, while the include guards using macros only use standard code. Modern compilers like `gcc` or `clang` know include guards, so for those compilers `#pragma once` has no performance advantage. But less to write :) True. @zdenop, I think this PR is ready to get merged. Issue #96 can be closed then.   @rfschtkt What about these 'LEAK' related warnings? Here is some backtrace info from gdb.

It is easy to reproduce the problem. Give a non-existant filename as input for tesseract. 

gdb --args tesseract lorem1.png lorem

```
(gdb)
Tesseract Open Source OCR Engine v4.00.00alpha-496-g2b373d1 with Leptonica
506         bool succeed = api.ProcessPages(image, NULL, 0, renderers[0]);
(gdb)
Error in fopenReadStream: file not found
Error in findFileFormat: image file not found
507         if (!succeed) {
(gdb) backtrace
#0  main (argc=<optimized out>, argv=0x7ffff5d1c0f8) at tesseractmain.cpp:507

(gdb) step
508           fprintf(stderr, "Error during processing.\n");
(gdb)
fprintf (__fmt=0x4037bc "Error during processing.\n", __stream=<optimized out>) at /usr/include/x86_64-linux-gnu/bits/stdio2.h:98
98                              __va_arg_pack ());
------------------------------------------------------------
(gdb) backtrace
#0  pthread_mutex_lock (mutex=0x7f5e2efaec60 <tesseract::tprintfMutex>) at forward.c:192
#1  0x00007f5e2ea8dc0f in tprintf_internal (
    format=format@entry=0x7f5e2eab8510 "ObjectCache(%p)::~ObjectCache(): WARNING! LEAK! object %p still has count %d (id %s)\n") at tprintf.cpp:42
#2  0x00007f5e2e9f19b9 in ~ObjectCache (this=0x7f5e2ef9cda0 <tesseract::Dict::GlobalDawgCache()::cache>, __in_chrg=<optimized out>)
    at ../ccutil/object_cache.h:42
#3  tesseract::DawgCache::~DawgCache (this=0x7f5e2ef9cda0 <tesseract::Dict::GlobalDawgCache()::cache>, __in_chrg=<optimized out>) at dawg_cache.h:30
#4  0x00007f5e2daac1a9 in __run_exit_handlers (status=1, listp=0x7f5e2de2e6c8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true)
    at exit.c:82
#5  0x00007f5e2daac1f5 in __GI_exit (status=<optimized out>) at exit.c:104
#6  0x00000000004022fc in main (argc=<optimized out>, argv=0x7ffff5d1c0f8) at tesseractmain.cpp:435

------------------------
(gdb) backtrace
#0  _IO_no_init (fp=fp@entry=0x7ffff5d1bc00, flags=flags@entry=32768, orientation=orientation@entry=-1, wd=wd@entry=0x0, jmp=jmp@entry=0x0)
    at genops.c:644
#1  0x00007f5e2db79129 in ___vsnprintf_chk (
    s=s@entry=0x7f5e2efeef80 <tprintf_internal(char const*, ...)::msg> "Tesseract Open Source OCR Engine v4.00.00alpha-496-g2b373d1 with Leptonica\n",
 maxlen=<optimized out>, maxlen@entry=65536, flags=flags@entry=1, slen=slen@entry=65537,
    format=format@entry=0x7f5e2eab8510 "ObjectCache(%p)::~ObjectCache(): WARNING! LEAK! object %p still has count %d (id %s)\n",
    args=args@entry=0x7ffff5d1bd78) at vsnprintf_chk.c:53
#2  0x00007f5e2ea8dc59 in vsnprintf (__ap=0x7ffff5d1bd78,
    __fmt=0x7f5e2eab8510 "ObjectCache(%p)::~ObjectCache(): WARNING! LEAK! object %p still has count %d (id %s)\n", __n=65536,
    __s=0x7f5e2efeef80 <tprintf_internal(char const*, ...)::msg> "Tesseract Open Source OCR Engine v4.00.00alpha-496-g2b373d1 with Leptonica\n")
    at /usr/include/x86_64-linux-gnu/bits/stdio2.h:78
#3  tprintf_internal (format=format@entry=0x7f5e2eab8510 "ObjectCache(%p)::~ObjectCache(): WARNING! LEAK! object %p still has count %d (id %s)\n")
    at tprintf.cpp:56
#4  0x00007f5e2e9f19b9 in ~ObjectCache (this=0x7f5e2ef9cda0 <tesseract::Dict::GlobalDawgCache()::cache>, __in_chrg=<optimized out>)
    at ../ccutil/object_cache.h:42
#5  tesseract::DawgCache::~DawgCache (this=0x7f5e2ef9cda0 <tesseract::Dict::GlobalDawgCache()::cache>, __in_chrg=<optimized out>) at dawg_cache.h:30
#6  0x00007f5e2daac1a9 in __run_exit_handlers (status=1, listp=0x7f5e2de2e6c8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true)
    at exit.c:82
#7  0x00007f5e2daac1f5 in __GI_exit (status=<optimized out>) at exit.c:104
#8  0x00000000004022fc in main (argc=<optimized out>, argv=0x7ffff5d1c0f8) at tesseractmain.cpp:435
(gdb) next
```

 Thanks. I had also noticed the -O2 and -O0 combinations while building with
enable-debug, and was going to pose a question,  since i dont know about it.

On May 12, 2017 8:19 PM, "rfschtkt" <notifications@github.com> wrote:

> Well, I was thinking more along the line of inspecting the offending
> object. Apparently fixing the problem I perceived didn't solve the problem,
> so I'll try gdb myself. Unfortunately ./configure --enable-debug doesn't
> seem to work, because there are -O2 arguments after the -O0 ones, and the
> last one in the room wins, so my workaround for that is to edit configure
> and configure.ac. Stay tuned...
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-301097602>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o6xmD6TVpQL98C2obU2Us2UcXXe6ks5r5HFlgaJpZM4LDjOe>
> .
>
 Thank you! 

This fixes the problem in the testcase I had mentioned `Give a non-existant filename as input for tesseract.`

```
shree@ALL-IN-1-TOUCH:/mnt/c/Users/User/shree/tesseract-head$ tesseract lorem1.png lorem
Tesseract Open Source OCR Engine v4.00.00alpha-512-g6bebe71 with Leptonica
Error in fopenReadStream: file not found
Error in findFileFormat: image file not found
Error during processing.
```

However, there are a few other cases where the error was occuring, eg. Missing output directory, please see https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-269325872

In that case now I get a different error:

```
 gdb --args tesseract p002-crop.bmp missing/bmp --oem 1 --psm 6 -l hin

(gdb) run
Starting program: /usr/local/bin/tesseract p002-crop.bmp missing/bmp --oem 1 --psm 6 -l hin
warning: Error disabling address space randomization: Success
warning: linux_ptrace_test_ret_to_nx: PTRACE_KILL waitpid returned -1: Interrupted system call
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

Tesseract Open Source OCR Engine v4.00.00alpha-512-g6bebe71 with Leptonica
Error during processing.

Program received signal SIGSEGV, Segmentation fault.
_IO_new_fclose (fp=0x0) at iofclose.c:54
54      iofclose.c: No such file or directory.

(gdb) backtrace
#0  _IO_new_fclose (fp=0x0) at iofclose.c:54
#1  0x000000000041a617 in tesseract::TessResultRenderer::~TessResultRenderer (this=0x1bb0160, __in_chrg=<optimized out>) at renderer.cpp:51
#2  0x000000000041b28f in tesseract::TessTextRenderer::~TessTextRenderer (this=0x1bb0160, __in_chrg=<optimized out>) at renderer.h:141
#3  0x000000000041b2be in tesseract::TessTextRenderer::~TessTextRenderer (this=0x1bb0160, __in_chrg=<optimized out>) at renderer.h:141
#4  0x00000000004083d6 in GenericVector<tesseract::TessResultRenderer*>::delete_data_pointers (this=0x9095c0 <main::renderers>)
    at ../ccutil/genericvector.h:874
#5  0x0000000000407f48 in tesseract::PointerVector<tesseract::TessResultRenderer>::clear (this=0x9095c0 <main::renderers>)
    at ../ccutil/genericvector.h:522
#6  0x0000000000407c7c in tesseract::PointerVector<tesseract::TessResultRenderer>::~PointerVector (this=0x9095c0 <main::renderers>,
    __in_chrg=<optimized out>) at ../ccutil/genericvector.h:456
#7  0x00007f7a7a58c1a9 in __run_exit_handlers (status=1, listp=0x7f7a7a90e6c8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true)
    at exit.c:82
#8  0x00007f7a7a58c1f5 in __GI_exit (status=<optimized out>) at exit.c:104
#9  0x0000000000407863 in main (argc=9, argv=0x7ffff50bc518) at tesseractmain.cpp:518
(gdb)
```

 @rnmanhon Please check your testcase also. https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-299127136

I am not getting the error with the latest code. I had not tested it earlier.

```
$ cp  p002-crop.bmp /tmp/tess__cwb36mk.bmp
$ tesseract /tmp/tess__cwb36mk.bmp output.txt
Tesseract Open Source OCR Engine v4.00.00alpha-512-g6bebe71 with Leptonica
$ These changes may also need to be backported for 3.05. 

Testcase https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-294188223 @stweil @rfschtkt :~ObjectCache(): WARNING! LEAK! problem is fixed. So I am closing this issue.

Thanks! Is it from https://github.com/UB-Mannheim/tesseract/wiki

http://digi.bib.uni-mannheim.de/tesseract/

http://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-setup-4.00.00dev.exe

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, Jun 17, 2017 at 5:02 PM, mgrint2 <notifications@github.com> wrote:

> Installed package tesseract-ocr-setup-4.00.00dev
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-309209698>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_o76bGYd0ZIXge4CI7u_1HWCCyZgiks5sE7ligaJpZM4LDjOe>
> .
>
 Error in fopenReadStream: file not found
Error in findFileFormat: image file not found

You need to give correct location of image file.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Sat, Jun 17, 2017 at 5:32 PM, ShreeDevi Kumar <shreeshrii@gmail.com>
wrote:

> Is it from https://github.com/UB-Mannheim/tesseract/wiki
>
> http://digi.bib.uni-mannheim.de/tesseract/
>
> http://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-
> setup-4.00.00dev.exe
>
> ShreeDevi
> ____________________________________________________________
> भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com
>
> On Sat, Jun 17, 2017 at 5:02 PM, mgrint2 <notifications@github.com> wrote:
>
>> Installed package tesseract-ocr-setup-4.00.00dev
>>
>> —
>> You are receiving this because you modified the open/close state.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/tesseract-ocr/tesseract/issues/529#issuecomment-309209698>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AE2_o76bGYd0ZIXge4CI7u_1HWCCyZgiks5sE7ligaJpZM4LDjOe>
>> .
>>
>
>
  So here's how it works:
The LSTM unicharset is embedded inside the .lstm component itself. That enables it to be different from the unicharset for base tesseract, without a proliferation of new traineddata components.
It was convenient at the time for the trainer to be able to save and load checkpoints in a single file.
The dawgs offer only a slight improvement over the LSTM model itself, as that is very good on its own, and will easily nail phototest.tif without the language models. (Even the vie lstm model will nail phototest.tif.)

Closing as working as intended. Reopen if you really need to get at the unicharset for some reason.  Fixed in https://github.com/tesseract-ocr/tesseract/commit/65517794f9bbc41a202d1fe410ff269f00257e57
  @joshimar, if you updated from an older version of Tesseract, you have to re-run `./autogen.sh` to create an updated `configure` script. The latest Tesseract version from Git always shows the reason why training tools cannot be built, so either you are using an old and buggy Tesseract code or you missed updating `configure`, too. Can you upload your full build log (output from configure and make) somewhere, so I might have a look? I need the make output as well. configure shows that OpenMP is supported. Your problem is caused by partial OpenMP support. I think that `configure --disable-openmp` will work for you (but result in slow binaries). Yes, you already said that training tools cannot be build with your 3.05 (is it the latest 3.05?).

`configure --disable-openmp` was meant to be used with 4.0. This is caused by https://github.com/tesseract-ocr/tesseract/commit/6140be6a5575e9159e3678adf4ee9e673b3ff2cc. If I cherry-pick it to 3.05 build will failed with gcc... OpenMP never worked for 3.05. Commit 6140be6 fixed OpenMP support partially. I'll send updates which fix the remaining issue. @stweil:  Thanks. At this stage I am more concerned with build process than if feature works ;-) @joshimar, please try latest versions from git (master and 3.05) and close this issue if it works now. `configure` uses `pkg-config` to find Leptonica and libicu, so at least the *.pc files must be found by `pkg-config`. They are not found because you had chosen a different install path. Either copy the *.pc files to the correct location, or tell `pkg-config` how to find them (please look into the manual for that). The first check is from pkg-config (failed), the second one is the old fallback check (which is removed in PR #577). That's strange. You can debug the problem like this:

* Edit `configure.am`: remove the first line with `AM_CONDITIONAL([ENABLE_TRAINING], false)`
* Run `./autogen.sh`
* Try `./configure`
* Repeat all three steps until you find which line caused the problem
 checkout master. Should be fixed.  What is an issue?  1. use tesseract user forum for asking question.
2. this is more leptonica build problem (allowed warnings)
3.  Did you tried https://github.com/tesseract-ocr/tesseract/wiki/FAQ#how-can-i-suppress-tesseract-info-line? > They will not appear in Linux. 
Because that problem is related to windows... The warnings are an indicator for potential optimizations and require more examination by developers, so I see there an issue to be discussed here.

AFAIK Leptonica warns when an image file is going to be mapped to memory, something that is unsupported for Leptonica's Windows code which uses a temporary file copy as an alternative.

If OCR of a single image results in many (number of lines) of those warnings, that might be caused by the same image being opened very often, or Tesseract acts on in-memory line images, but Leptonica for Windows has to write those images to disk. Then it is clear that at least for Windows the performance suffers and something should be done - either in Leptonica code or in Tesseract. > Is the version of leptonica included with the windows binaries a debug version?

No, it is production code. Such warnings also occurr(ed) with Tesseract 3.05 and earlier, for example when processing JPEG 2000 images (but only once per image). Some improvements in newer Leptonica code reduced the cases where this kind of warnings are shown. > How did you compile Leptonica?

Basically I used `./configure: make; make install`, but in a cross build on Debian GNU Linux. Technically, the warnings occur on any platform which does not have a `fmemopen` function. Leptonica outputs them using a macro `L_WARNING`. It is possible to call `setMsgSeverity` from Tesseract code to disable any warning message from Leptonica (I don't think that would be a good idea). That would be possible, yes. But without those nagging messages, I'd never have had a look on that part of Leptonica. @DanBloomberg : Thanks for support. Maybe it would be good to enable all warning for debug builds and hide them for release builds.  Yes!
I've been working on it in conjunction with the removal of cube, and it's
getting close.

On Fri, Dec 2, 2016 at 12:31 AM, Amit D. <notifications@github.com> wrote:

> @theraysmith <https://github.com/theraysmith>, any chance you'll fix that
> for the final 4.0 release?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/521#issuecomment-264400486>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056a_L8S11vxRB-0KJWJoTowd2O8Q0ks5rD9dHgaJpZM4LCRAs>
> .
>



-- 
Ray.
 Partly. I haven't closed it because you can't *create* a traineddata file
with just the LSTM part(s) yet, and I am working on fixing that as well.

On Tue, Dec 6, 2016 at 1:42 AM, Amit D. <notifications@github.com> wrote:

> @theraysmith <https://github.com/theraysmith>, can you confirm that
> 5deebe6
> <https://github.com/tesseract-ocr/tesseract/commit/5deebe6c279f70215935c1f86baa7e7016c7f2a7>
> solved this issue?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/521#issuecomment-265104978>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056XMVGWYPqK1Pc8XzqTxrzYjHZxgZks5rFS4MgaJpZM4LCRAs>
> .
>



-- 
Ray.
 Sorry, I wasn't aware of that.
Added.
I don't know of anything else missing. Please let me know if you can't get
through the tutorials.
I will fix the doc when I have the LSTM-only training working properly.


On Tue, Dec 6, 2016 at 8:10 AM, Shreeshrii <notifications@github.com> wrote:

> @theraysmith
>
> Ray, I am not able to try out LSTM training as repo is missing LSTM.train
> config file. Are there other pieces also that you need to add?
>
> On 06-Dec-2016 8:59 PM, "theraysmith" <notifications@github.com> wrote:
>
> > Partly. I haven't closed it because you can't *create* a traineddata file
> > with just the LSTM part(s) yet, and I am working on fixing that as well.
> >
> > On Tue, Dec 6, 2016 at 1:42 AM, Amit D. <notifications@github.com>
> wrote:
> >
> > > @theraysmith <https://github.com/theraysmith>, can you confirm that
> > > 5deebe6
> > > <https://github.com/tesseract-ocr/tesseract/commit/
> > 5deebe6c279f70215935c1f86baa7e7016c7f2a7>
> > > solved this issue?
> > >
> > > —
> > > You are receiving this because you were mentioned.
> > > Reply to this email directly, view it on GitHub
> > > <https://github.com/tesseract-ocr/tesseract/issues/521#
> > issuecomment-265104978>,
> > > or mute the thread
> > > <https://github.com/notifications/unsubscribe-auth/
> > AL056XMVGWYPqK1Pc8XzqTxrzYjHZxgZks5rFS4MgaJpZM4LCRAs>
> > > .
> > >
> >
> >
> >
> > --
> > Ray.
> >
> > —
> > You are receiving this because you authored the thread.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/tesseract-ocr/tesseract/issues/521#
> issuecomment-265179343>,
> > or mute the thread
> > <https://github.com/notifications/unsubscribe-auth/AE2_
> o3NKfgM6IGnUvuVAqYrFirHq_0FMks5rFX9IgaJpZM4LCRAs>
> > .
>
> >
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/521#issuecomment-265191525>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Tf7yXmVGGBTEDzc-k9-4lyMf6wyks5rFYkGgaJpZM4LCRAs>
> .
>



-- 
Ray.
 It did. It should now be possible to create a traineddata containing just a .lstm component and run recognition with it.  There are different approaches possible to get support for big endian machines:

1. Write training data files in native endian byte order. When reading that data Tesseract must automatically detect the endianness used and convert it to native byte order if necessary, that means if training (=writing) machine and OCR (=reading) machine use different byte order.
2. Write training data files with fixed endianness. When reading that data Tesseract must only convert the data if it uses a different byte order.

The current code obviously tries to implement the first variant: it uses `swap` parameters for the functions which read data and sets the value of `swap` based on the training data.

I prefer the second variant and suggest to always use little endian training data files. Then the most common little endian platforms can use the data without the need to do byte swaps. Big endian hosts can use fixed code to convert the data when reading or writing them. This results in less complex code: the `swap` parameters are no longer needed.

My [`endian` branch](https://github.com/stweil/tesseract/tree/endian) includes experimental changes which remove (most) `swap` parameters and implement the read part for big endian machines. I could successfully run `api/tesseract testing/phototest.tif stdout` on a big endian S390X (QEMU emulation).

@theraysmith, if you agree to switch to that different solution for the endianness problem, I'd continue with the write part. In a first PR I'd remove the current `swap` code. How does the big-endian machine know which values are of which sizes, and types and therefore how to swap? Please explain how that works if the swaps are all removed from the deserialize methods.
I couldn't find any replacement swapping code in your pull request. That's done by the function `convert2le` (see PR #703) which is overloaded for the relevant data types. Depending on the macro `WORDS_BIGENDIAN` that function either swaps bytes or does nothing. You said you successfully ran on phototest.tif on a big-endian machine.
Is that using the LSTM implementation?
I don't see how that could possibly succeed as separated deserialize/swap, since matrix.h has to swap the sizes that it deserializes in order to know how big a matrix to deserialize.
I don' think this is a good idea. Most of the swap code required for the LSTM implementation is already in place, so to support big-endian from here isn't difficult really. Certainly a lot easier than making this work. My test used the default (Tesseract + LSTM). My code includes changes to `matrix.h` which do the swapping needed. Did you read my comments above about the drawbacks of the current implementation? Three small points.

From a software distribution standpoint, it is pretty burdensome to require 
different data files for big endian architectures. If that's true for 3.04 then
I've been shipping broken software.

Microsoft and Torvalds have almost entirely killed off big endian machines 
by now. For example, Solaris development was killed off in December 
according to the press.

Any efforts on this topic get cut in half if the non-LSTM recognizer is removed. I strongly vote against removing non-LSTM as we currently still get better results with it in some cases.

Technically it is possible to have BE and LE machines creating different training data as long as both are able to fix that during read. But this is one of the drawbacks: each Tesseract must be able to read both variants of training files (which results in less efficient code). In addition it is more difficult to compare the training output from LE and BE machines. @theraysmith, a new test with explicit `--OEM 1` just passed successfully on my BE machine. > Ray, you can see online Stefan's suggested changes here.

or [here](https://github.com/tesseract-ocr/tesseract/pull/703/files). In the meantime I have improved that experimental code further (based on PR #706) and will send an update later. It still only addresses reading, but adding write support to enforce little endian training data files is rather easy following the same scheme.

@jbreiden, did you ever test 3.05 or older versions with big endian machines? If not, I can run a test on my s390x emulation.
 I assume that most binary file formats do. See also https://en.wikipedia.org/wiki/Endianness#Files_and_byte_swap. **Please provide examples of where you get better results with the old engine.**
Right now I'm trying to work on getting rid of redundant code, rather than spending time fighting needless changes that generate a lot of work. I have recently tested an LSTM-based OSD, and it works a lot better than the old, so that is one more use of the old classifier that can go. AFAICT, apart from the equation detector, the old classifier is now redundant.

I disagree with the assessment that protocol buffers use Stefan's method, as I still haven't had an explanation of nor seen code to show how the reading of a little-endian file on a big-endian machine works. This comment on proto buffers "Yep.  On the wire, things are encoded little-endian, but the encoding and decoding routines will convert to and from your machine's format themselves, so you don't need to worry about it." sounds exactly like what the code currently does.

I don't think having a collection of big-endian data files works if that is the proposal. That would be very ugly. **What exactly is the proposal for classes like Matrix?** Basically the swaps have to stay in place, but could be predicated on an #ifdef instead of runtime data. Even that would make the code ugly, and I don't see a huge amount of CPU being burnt testing if (swap), so I don't really see what all the fuss is about over code efficiency compared to the wasted effort messing about with the code.

In summary, I haven't seen a coherent, convincing argument that there is anything wrong with the current solution of the code only swaps the data if it needs to, which most of the time it doesn't because all data files are little-endian and almost all machines are little-endian. > Please provide examples of where you get better results with the old engine.

I'll do that in the discussion of the new issue #707.

> What exactly is the proposal for classes like Matrix?

The current implementation uses `DeSerialize` functions with `swap` parameters and calls `ReverseN` if that parameter is true, see [example](https://github.com/tesseract-ocr/tesseract/blob/master/ccstruct/matrix.h#L155).

My [experimental implementation](https://github.com/stweil/tesseract/tree/endian) removes the `swap` parameters and the code based on them in `ccstruct/matrix.h` which IMHO makes the code much prettier. Of course we still need swapping for big endian machines (otherwise my OCR tests on such machines would fail). That is done by replacing all `fread` function calls by different ones which I also called `fread` in my experimental code but which should be renamed into `DeSerialize`, see [example](https://github.com/stweil/tesseract/blob/endian/ccstruct/matrix.h#L154). Those new functions both read (using the normal `fread` function) and convert to little endian (using new functions [`convert2le`](https://github.com/stweil/tesseract/blob/endian/ccutil/tessio.h#L15)). They are implemented in the new file [`ccutil/tessio.cpp`](https://github.com/stweil/tesseract/blob/endian/ccutil/tessio.cpp).

The new code simply uses new functions for all reads from file, so getting all cases where swapping is needed is much simpler than in the current implementation. The same can be done on writing to achieve little endian data files, no matter what endianness the host is using. >did you ever test 3.05 or older versions with big endian machines?

No, I don't currently have my hands on a big endian machine. I'm pretty sure that I could get my hands on one, but so far I have not made the effort. As a side note, I'm helping ship Tesseract is on 23 different architectures, some of them big endian. They all share the same data files. However, the big endian platforms are very rarely used and there may not be bug reports, even if totally broken.

https://buildd.debian.org/status/package.php?p=tesseract&suite=unstable I now have run a test (`tesseract testing/phototest.tif stdout`) with Tesseract 3.05 on big endian s390x (QEMU emulation) successfully. OK, now I finally understand your proposal, I like some aspects of it, but I have some suggestions for making it better:
1. Since all data files are de-facto little-endian, the overloads are only needed on a big-endian machine. I haven't yet found the source code for convert2le, but I get the gist of what you are trying to do.
2. Move all the overloads of fread to overloads of TFile::FRead. (See ccutil/serialis.h).
3. Hide all the overloads behind an #ifdef __BIG_ENDIAN__ so the little-endian machines just call the generic (void*) version of FRead. (I haven't researched it thoroughly, but this posting might be better: http://esr.ibiblio.org/?p=5095)
4. Anything needed by the new engine (should be just the Network and subclasses, Dict/Dawg and UNICHARSET) make sure it loads from a TFile and convert it if it doesn't.
5. Instead of modifying the old engine parts, concentrate on convincing me why it should stay first, but if you really want to do it, map all the I/O in the old engine to TFile and get rid of the use of FILE.
6. Does convert2le have any practical use at that point? Surely just call ReverseN directly from the overloads? (Like I said I haven't found the code for convert2le, and I missed the link for it if you provided it.)

Then I agree it would be cleaner, smaller, and more efficient, as well as future-proofed. Fixed in commit https://github.com/tesseract-ocr/tesseract/commit/8e79297dcefecdb929d753d28554fec51417ec39 The code that is there now is far simpler and cleaner than anything that
was there before.
While there is still minor overhead in deserializing, it is small, as there
is only one check for each array.
To write only little-endian files would be a lot more work on the dead
code, so it isn't worth it. This was already a lot of work on the dead code
as it is.

On Wed, May 3, 2017 at 10:02 PM, Stefan Weil <notifications@github.com>
wrote:

> Ray, the new code still uses a dynamic detection in
> TessdataManager::LoadMemBuffer to decide whether swapping is needed or
> not. This implies that the code supports both big and little endian data
> files. The drawback is additional runtime code on all kinds of machines.
>
> Are you planning more changes? I'd drop support of big endian data files
> in 4.0 and add code to always write little endian ones. Then static
> swapping code would only be needed on big endian machines, and the large
> majority of machines would not need any swap code at all.
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/518#issuecomment-299097442>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056UZK6qi93qEj5bdhFUjJ6ULWHoxpks5r2VvVgaJpZM4LBZNs>
> .
>



-- 
Ray.
  I've now fixed it properly.
When I did my original pull, all I got was the double free.
The solution that frees outside of GetAdaptiveFeatures is not as clean as freeing properly inside.  Also @zdenop 

I propose to replace all such stuff with c++11 unique/shared ptrs for the future versions.
If we get agreement on this I could start modernizing codebase. I just need a greenlight.
I'm almost sure that other people will like it and help with the process. Let wait for a while with this changes:

1. It seems Ray will push more code soon...
2. I tried to transfer fixes to 3.05 branch. 3.05 should be possible to build without c++11 need (I am slow because I have limited time and I can test (just build) it on VS2010 in limited scope...)  The alternative variant of InitCharset is used internally at Google to save accessing the script-dir directory from a trainer running in a Google data center. It is easier to pack the unicharset and recoder in a proto buffer in a separate step. If anything the other variant could prepare the recoder and then call the one that takes a recoder. I rebased the PR now to fix a merge conflict in `lstm/lstmtrainer.cpp`. Continuous integration fails because of missing `tesseract::LSTMTester::LSTMTester(long)`. This is unrelated to this PR. I rebased to fix the build with CMake in the continuous integration. Thanks, I knew that. In this case the trick would not have worked because the CI problem was only fixed in git master.  Rebased PR to fix merge conflict.  You could use https://cppan.org/ also. It's already present in repository. I won't be adding vcpkg port. If someone else needs this, please do it.  See [forum](https://groups.google.com/forum/#!topic/tesseract-ocr/e__2DN1GQb0) or [wiki info](https://github.com/tesseract-ocr/tesseract/wiki/NeuralNetsInTesseract4.00#for-open-source-contributors).  The  « un animal politique » is right there in your example. Not sure what the problem is.
Works for me on eurotext.tif  Please use tesseract user forum for asking support.  Current master repository is for tesseract developers and not for end users.  Please use teseract forums for asking questions... 
Changing urls of old(er) pages is IMO bad idea - it will break links on other pages/projects... @amitdo: can you please clarify? I think  fresh new wiki page titles could be changed...  You can sent pull requests...  Please use tesseract user forum for asking support.  Please provide all input files Please provide also information about tesseract version and OS. @sjaanus Is your issue resolved? Please mention the solution and close the issue.  AFAIK there is the limit for [system variable](https://blogs.msdn.microsoft.com/oldnewthing/20100203-00/?p=15083) - but I am not sure if this is a case also in Windows 10.
Anyway this is not tesseract project issues - so I am closing issue.
  Please contact those you created your package. We do not created linux packages.
  I can not reproduce described behavior. Please upload somewhere your files, that demonstrate problem.
 Closed because of no proof of described error.   AC_CHECK_LIB macro tests whether the library (leptonica) is providing function (l_generateCIDataForPdf). if l_generateCIDataForPdf is not present in leptonica configure will fail (because tesseract compilation would fail for the same reason). So adding linking to libpng is IMO wrong - png library is not required by tesseract. configure is just testing if function l_generateCIDataForPdf is present (e.g. we have correct version of leptonica) or not (we have wrong version of leptonica).
And this is not fixing issue #435 - if you use wrong (old) library it does not matter if libpng is present or not.
  Please do not use issue tracker for asking question - use teseract user forum
  IMO this check has nothing to do with tesseract.
@egorpugin  can you have a look on this?
 Tess uses lept and the process does appropriate checks for each dependent project.
@datalurkur Do you use the latest CPPAN client? If not try to update it. If yes I need to investigate this more closely, maybe with more help from you.
 It's cppan issue. I'm investigating.
 @datalurkur Is it possible to chat with you somewhere (maybe skype etc.) and also could you share your desktop with me (in skype again or teamviewer)?
Because it works for me on my machine.
And I need some infos from you:
1. cmake version
2. list of processes (maybe cppan or cmake hangs)
3. files on disk in `c:\Users\u\AppData\Local\Temp\cppan\vars\`
 > Regarding files in that directory, are you looking for a recursive list?

That would be nice. But before, please remove all stuff in `c:\Users\u\AppData\Local\Temp\cppan\vars\`, re-run cmake as you did earlier. When cmake hangs, check task manager (or process explorer) and make sure that cmake processes performs nothing (cpu usage will be 0 instead of 13% in case of 8 cores). So, when cmake children perform nothing do a recursive list of vars dir (maybe with `tree` command).

If cmake processes are running probably the issue somewhere in its scripts (they're generated by cppan). cppan process just waits for them.
 From this text it's not clear where 3.6.2 folders reside. Are they under CMakeFiles dirs?
Please do `tree /F`. It will show files too.
The output should be something like that http://imgur.com/4bPuWJB
 Looks terrible. :) And not useful. Could you please post a screenshot of tree output and remove that text from your comment?
 Not good. Could you post a screenshot like this? http://imgur.com/4bPuWJB
 It's ok now, thanks! Everything seems to be fine there (atleast in dir `0`).
 Could you also please send me a screenshot of task manager with list of processes (details tab on win10) when hang occurs? I'm interested in cmake/cppan processes with command lines (right click on columns at the top of the table - select columns - command line or command args).
 Image is empty for me :(
 Is that all? Only 3 cmake processes?

Could you try to clear `c:\Users\u\AppData\Local\Temp\cppan\` dir, re-run `cmake .. -DSTATIC=1` and make same screenshot again? I'm asking this because I see the same dir name `0XektimK` in your messages. Maybe fresh dir could say me something.

Please do two screens: one right after parallel checks start and after 3-5 mins (when there's 0% cpu load by cmake processes).

And more here - could you please then (after hang occurs) archive the whole `c:\Users\u\AppData\Local\Temp\cppan\vars\SomeRandomLetters` dir and send it to me (attach here).
 Are there any 'cl.exe' processes hanging around? Maybe `MSBuild.exe`?
It's better to check this when no other Visual Studio instances exist.
I suspect that:
1. initial cmake call is waiting for cppan
2. cppan is waiting for cmake children
3. these hanged cmake processes are waiting for visual studio `cl.exe` or `MSBuild.exe` or even `cmd.exe` or `conhost.exe`.
 Try to kill all conhosts one by one. On some step you should see some progress with cmake processes (increased cpu usage or death of that cmake process or smth else).
 That's true - cmakes are waiting for someone or deadlocked inside.
 Could you try Process Explorer to check out children of those cmake processes? https://technet.microsoft.com/en-us/sysinternals/processexplorer.aspx

You should see process tree like this http://imgur.com/FDhmOHQ
Please, attach a screenshot of cmake call tree (maybe re-run it to keep your main cmd.exe window and full process tree online).
 No children? :) Looks like cmake issue.
 Ok, next step:
1. Keep in mind the number of hanged cmake process. It's a number of a worker (0-7) - last digit or dir name in cmake command line (e.g. `c:\Users\u\AppData\Local\Temp\cppan\vars\SomeRandomLetters\7` - 7 is the needed number).
2. Kill all hanged cmake processes.
3. Go to `c:\Users\u\AppData\Local\Temp\cppan\vars\SomeRandomLetters\YouCmakeHangProcessLastDirNumber`)
4. Run in that dir `cmake .`
 I've already seen bugs in cmake while developing cppan. Fixes will appear in cmake-3.7. But those are not issues you hit.
The best way in your case is to build debug version of cmake, put in into `c:\program files (or x86)\cmake\bin`, run cmake on tess library as you're doing and attach to hanged cmake processes.
 Try to go to `c:\users\u\.cppan\storage\cfg` and remove it completely (`cfg` dir). Then re-run cmake as you did.
 Yes, you can try to remove the whole storage dir at `c:\users\u\.cppan\storage\`.
 To keep you up to date.

I was hit twice by this deadlock or whatever it is.
On my system it's very rare and not repeatable.
Second time it was "unlocked" accidentally in 30-45 seconds without my actions but with error `process cannot be started`.
It seems the issue in cmake's startup process. In other words it hangs when it invokes msvc compiler.
And it also possible that this is not cmake fault but `cl.exe` or `Windows` issue itself. Ok, I've hit by deadlock again and debugged cmake process.
I filed an issue in cmake tracker, but don't know if they'll be able to fix it in the near future.
https://gitlab.kitware.com/cmake/cmake/issues/16461

As a temporal solution here I will add an option for performing checks in single thread. This will prevent such errors (I hope). It will be available with the next cppan client release. I'll ping about it here. Yes. And more to say if you kill cppan process (responsible for parallel var checks) the overall process won't fail, it will switch automatically to single thread mode.

I've added njobs flag already, but the client version is not uploaded yet. I'm fixing the rest of the bugs. Hi there!
0.2.0 is out!
To lower number of parallel jobs, put to the config in `c:\users\u\.cppan\cppan.yml` line `var_check_jobs: N`, `N` is your number.
But...
CMake devs told me that they also have such hangs, so even 1 thread may hang.  AFAIK this should be initial support for OpenCL. Unfortunately it was not extended until this time.
  > A generic version that can be compiled for either Windows code pages or Unicode

This should be used by default. No suffix - compiler will choose it without user.
No need to rename functions.

Please, provide your way of building tess/lept. So, types should be fixed, not functions.  This is old error that was already fixed.
  Since tess is using c++11 we need to `s/NULL/nullptr/`.
 It's all about Ray. Such changes can break his merges.  Please use teeseract user forum for asking support.
  "Units: Undefined" is not so great. If you set it, things work correctly. 
Will have to look more carefully what the unit possibilities are for PDF to
see if we want to make a code change or not.

```
$ mogrify -units PixelsPerInch --density 300x300 pngfromtif.png
$ identify -verbose pngfromtif.png
  Geometry: 2479x3500+0+0
  Resolution: 118.11x118.11
  Print size: 20.9889x29.6334
  Units: PixelsPerCentimeter

$ tesseract  pngfromtif.png correct pdf
$ pdfinfo correct.pdf
Producer:       Tesseract 3.04.00
Page size:      594.96 x 840 pts
```
 Haven't had time to look at TIFF, but the PNG behaviour looks right. Spec says we know nothing about image resolution. Common practice from time immemorial is to default to some hopelessly wrong value. I could go trace code to find out what number was used, but honestly this is a garbage in, garbage out situation. Not sure it is worth spending time on. Are you in contact with the authors of the program that is producing the bad metadata? Fixing that is top priority.

```
The following values are legal for the unit specifier:
   0: unit is unknown
   1: unit is the meter
When the unit specifier is 0, the pHYs chunk defines pixel aspect ratio only; the actual 
size of the pixels remains unspecified.
```
 1) Why use two convert commands instead of just one?
2) I suggest PNG over uncompressed TIFF. Filesize of the PDF should be smaller, and because Tesseract  can skip transcoding the image, there will be some CPU savings as well.
  true
  Please use tesseract user forum for asking support.
  Please use tesseract user forum for asking support.
  > Is it correct that there's a PSM 11 and 12 mode?

Yes. and PSM 13 too.
You revealed our secret! :)
https://github.com/tesseract-ocr/tesseract/blob/8d6dbb133b41/api/tesseractmain.cpp#L115

I'm sorry, I do not have good answers to your other questions.
 master(4.00) & 3.05 repository produce help message for psm 11-13  Which fonts are used there?
  Please use tesseract user forum for asking support.
  > Tesseract just spent seven hours trying to do OCR on the attached document. It's five pages long. 

Wow, this is really extreme!
 Well, I tested it and it takes less than 5 minutes...

Tesseract (the official command line tool) does not accept pdf as input, so how did you convert the pdf to a format that Tesseract accepts?

Here is what I did:

```
convert gov.uscourts.ctd.18812.88.0.pdf gov.png
```

This command will create 5 'gov-n.png' images.

First page:

```
tesseract gov-0.png gov-0
```

time: 1 minute and 5 seconds
 ```
gs -dQUIET -dSAFER -dBATCH -dNOPAUSE -sDEVICE=tiffgray -r300x300 -o gov2.tiff gov.pdf
```

Your command creates a **730 MB** tiff file, while my command creates **5 200-300 kB** png files.
 ![gov-0](https://cloud.githubusercontent.com/assets/13571208/18798851/1cd055f8-81dd-11e6-8f33-9f38db58d84c.png)

Image properties:
Width: 35.417 Height: 45.834
DPI: 72 X 72

This is equivalent to:
Width: 8.5 Height: 11
DPI: 300 X 300
 `gs -dQUIET -dSAFER -dBATCH -dNOPAUSE -sDEVICE=tiffgray -r72 -o gov.tiff gov.pdf`
OR
`gs -dQUIET -dSAFER -dBATCH -dNOPAUSE -sDEVICE=tiffgray -o gov.tiff gov.pdf`

This command creates a **42 MB** tiff file. The size **in pixels** of each page is the same as with my PNGs.

It takes 4 minutes and 29 seconds to Tesseract to read this tiff.
 @Shreeshrii commented:

> To get accurate results, you will need to preprocess the images too to get
> rid of the background speckles.

I'm guessing that it will run faster too.

BTW, Here is what Tesseract outputs in the console:

```
time tesseract gov.tiff gov
Tesseract Open Source OCR Engine v3.05.00dev with Leptonica
Page 1
Detected 1875 diacritics
Page 2
Detected 1338 diacritics
Page 3
Detected 1885 diacritics
Page 4
Detected 658 diacritics
Page 5
Detected 213 diacritics

real    4m29.118s
user    4m28.972s
sys 0m0.152s
```

It 'thinks' the speckles are diacritics...
 This PDF file is just a bag of images. This is very common and was probably produced by
a photocopier or sheetfed scanner. Some fax machines make these too. It is entirely
black and white. If you know you are working with black and white images, you can
save a ton of space by using appropriate compression. This command renders 100% 
equivalent images for 2.3MB.

  gs -dQUIET -dSAFER -dBATCH -dNOPAUSE -sDEVICE=tiffg4 -r300x300 -o gov2.tiff gov.pdf

That said, best practice for known 'bag of images' PDF is not to render anything. It is to
extract the images, undisturbed. If necessary, adjust their header so that their resolution
(e.g. 300 dpi) agrees with what the PDF was claiming. In an ideal world they would always
be consistent already, but programmers screw this up all the time. That's the thing you
feed to Tesseract (assuming you don't want to do any additional cleaning or something.) 
This workflow is kind of sophisticated and maybe not easy for everyone. But it makes 
more sense than potentially rescaling the images by rendering to a different dpi.

I just want to put this here because there are several different references being cited in this 
bug report about work flow. Please  consider this one authoritative.

It does not however address the core question about dots, which seems like a legitimate
concern. This will be an interesting test document for future development.
 @jbreiden

> This command renders **100% equivalent** images for 2.3MB.

`gs -dQUIET -dSAFER -dBATCH -dNOPAUSE -sDEVICE=tiffg4 -r300x300 -o gov2.tiff gov.pdf`

This command will upscale the original images. It will make them more than 4 times larger. This is unnecessary because the DPI of the original images inside the pdf is 300X300, although the pdf itself falsely 'claims' that the DPI for these images is 72X72.
 I was just encouraging -sDEVICE=tiffg4 over -sDEVICE=tiffgray for known black and white images. You are right, care should be taken to avoid rescaling, and that's the primary reason image extraction is safer than rendering.
 https://github.com/DanBloomberg/leptonica/search?q=noise
 In this buggy broken world, do whatever it takes to get the resolution right. I rescind my recommendation to honor the PDF settings.  If you crack open gov.uscourts.ctd.18812.88.0.pdf, you can see that it really does contain black and white images. The telltale is BitsPerComponent 1 and the internal use of CCITTFaxDecode, which only works on black and white. 

```
<<
/Type /XObject
/Filter [/CCITTFaxDecode]
/Length 60 0 R
/Height 3300
/BitsPerComponent 1
/ColorSpace [/DeviceGray]
/DecodeParms [61 0 R]
/Subtype /Image
/Name /Im1
/Width 2550
>>
```

http://stackoverflow.com/questions/2641770/extracting-image-from-pdf-with-ccittfaxdecode-filter

The embedded black and white image inside the PDF is already dithered. Ghostscript is innocent. Normally I prefer to feed Tesseract images that have been messed with as little as possible, but this may just be the exception. Tesseract is not trained on dithered text. Good luck with this one!

![foo](https://cloud.githubusercontent.com/assets/4961958/18856632/7324d61e-8411-11e6-953d-d3864e9c16f6.png)
 If you choose to use morphology to remove the dots and undo the dither, Leptonica is very strong library for C or C++ programmers. A few morphology operations (erosions and dilations) hopefully would do the trick. 
 Leptonica is responsible for decoding image file formats. The list of supported formats is here. Discard PDF (IFF_LPDF) and PS (IFF_LS ) because they are write-only, and discard SPIX because it is Leptonica specific. This support assumes that Leptonica is built with all imaging dependencies, which are optional. If you are running the Tesseract that ships on linux distributions such as Debian or Ubuntu, there should be no problems. You might have less support on cygwin or similar, depending on how Leptonica was built. 

https://github.com/DanBloomberg/leptonica/blob/master/src/imageio.h#L92
 .--. .-. --- -..- .. -- .- / -.-. . -. - .- ..- .-. .. / -...

I've made a few in-place edits on the bug to
clarify the wording. Hopefully makes more
sense now.

 ... . -. -.. / ... .--. .- -.-. . / -- .- .-. .. -. . ...
 > I've made a few in-place edits on the bug to clarify the wording. Hopefully makes more sense now.

I deleted my previous message just after you made the edits. I thought that you didn't like my little joke...
Clearly, I was wrong!

For the benefit of humankind, here it is again...

> @jbreiden
> 
> Jeff, your last two messages look cryptic...
> 
> If you have been abducted by aliens, try give us a sign and we will rescue you! :)

<

> .--. .-. --- -..- .. -- .- / -.-. . -. - .- ..- .-. .. / -...
> 
> ... . -. -.. / ... .--. .- -.-. . / -- .- .-. .. -. . ...

Jeff, we are coming, stay calm!

LOL
 It's good to know Morse code, or maybe just to find an online Morse code translator... :) 
 Even if you use `-sDEVICE=tiffgray`, you might want to use `-sCompression=lzw`.
 For generating many 1-page images files, instead of one multi-page tiff file, use `-o img-%d.tiff`.
 My first patch (dated March 28)  in bug https://github.com/tesseract-ocr/tesseract/issues/233 will reduce RAM use with TIFF input. It stops Tesseract from buffering the input file before decompression. The patch should also should make the LZW case equal to the non-LZW case with respect to RAM. Note that I haven't tested on this particular example, so I'm saying "should" rather than "does".
 The plan was for Ray to commit that patch. However, he has been too busy with the upcoming Tesseract 4.0 and over six months have passed. I think it is okay if someone wants to commit the patch. Please do not commit the second patch, though; that should wait until after the next Leptonica release.
  Hopefully, a  shiny new code that includes a LSTM based OCR engine will land in this repo very soon. It will be in alpha state initially.

[Modernization Efforts](https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/6ModernizationEfforts.pdf)
[Building a Multilingual OCR Engine](https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/7Building%20a%20Multi-Lingual%20OCR%20Engine.pdf)
 Please use tesseract user forum for asking questions
 In general, only Ray can share a roadmap for new releases. You can ask him to do so in the dev forum. 
https://groups.google.com/d/forum/tesseract-dev

> and inform users about future features which they can expect. I could not find one for

For the next release the major feature is the LSTM based engine, which users can read about in the DAS 2016 slides.

Regarding sharing the dates for releases, some open source projects do share a schedule, while others just declare "We'll have a new release when it's ready...".

The next release version will probably be `4.0`.
 +1 for tagging the tip of the repo before the completely new code as `3.04.2` or `3.05`.  
  You are right and the patch looks OK to me.
 thanks
  Your issue seems to be related to #235

Please read this:
https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md

> Make sure you are able to replicate the problem with Tesseract command line program. For external programs that use Tesseract (including wrappers and your own program, if you are developer), report the issue to the developers of that software if it's possible. You can also try to find help in the Tesseract forum.

https://groups.google.com/d/forum/tesseract-ocr
 Also, Arabic uses a special OCR engine 'Cube' which isn't maintained anymore. It will be replaced by a better engine in the next release.
  ```
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
```

Add a white/black frame to the image and no error messages will appear.

```
convert  427-1.jpg  -bordercolor White -border 10x10 427-1b.jpg
```

Strange behaviour...
 > The biggest problem for me, however, is that in OCRopus they don't even get OCRed.

This place is for bug reports about **Tesseract**, not  OCRopus.
  Thanks for reporting! We need to fix it.

FYI, OpenCL support is [quite buggy](https://github.com/tesseract-ocr/tesseract/issues?q=is%3Aissue+is%3Aopen+label%3AOpenCL)
 Haven't tried that. If someone wants to try it...
 Does it also fail when OpenCL is disabled?
 @zdenop, did you lately compile Tesseract with OpenCL enabled on Linux / Windows?
 I tried it few minutes ago on linux. Build process looks fine, but OCR segfault... 
 Just wanted to know that it is at least compiles on Linux. Thanks.
 Sure – just need some time. Not today. Good night.  There are clear instruction for training and [how to generate training images](https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#generate-training-images-and-box-files). Other approaches are not supported.
  Thank you for you investigation.
Can you please post it to tesseract-dev forum for discussion, so developers of tesseract wrappers can be aware of change?
 I must admit that I had to look into the code what "OS" means in the context of OCR. Maybe it is possible to find a better name with "Orientation and Script" ("OaS"?).

PS. I just noted that the title of this issue also had the information which I was looking for. @jbarlow83 will you create PR? fixed with 3a47adcbe13e34  Tesseract training tool text2image uses these two functions: 
[pango_glyph_item_iter_init_start](https://github.com/tesseract-ocr/tesseract/blob/193032a7786f/training/pango_font_info.cpp#L455)
[pango_glyph_item_iter_next_cluster](https://github.com/tesseract-ocr/tesseract/blob/193032a7786f/training/pango_font_info.cpp#L458)

 That means it requires Pango >=1.22.0:
https://developer.gnome.org/pango/stable/api-index-1-22.html
https://developer.gnome.org/pango/stable/pango-Glyph-Storage.html#pango-glyph-item-iter-init-start
https://developer.gnome.org/pango/stable/pango-Glyph-Storage.html#pango-glyph-item-iter-next-cluster

However, the comments and code in the method [`bool FontUtils::IsAvailableFont()`](https://github.com/tesseract-ocr/tesseract/blob/193032a7786f/training/pango_font_info.cpp#L509) in `/training/pango_font_info.cpp` refer to older Pango versions:

> // The generated list of font families and faces includes "synthesized" font
> // faces that are not truly loadable. **Pango versions >=1.18** have a
> // pango_font_face_is_synthesized method that can be used to prune the list.
> // **Until then**, we are restricted to using a hack...

`if (PANGO_VERSION <= 12005) {`

`12005` means 1.20.5
https://github.com/GNOME/pango/blob/1147da131ad13/pango/pango-utils.h#L136
  Please use tesseract user forum for asking support
 https://github.com/tesseract-ocr/tesseract/issues/410#issuecomment-244120707
  Please use tesseract user forum for asking support.
  A contribution of direct ALTO support would be welcome. Recommend implementing in a separate file api/altorenderer.cpp rather than adding to api/baseapi.cpp.
  Please you tesseract user forum for asking support
  Hi,
These logs are not very useful as does not contain actual build logs.
Could you please try to do:

```
cppan --self-upgrade
cd your_tesseract_dir
cppan
```

and try to rebuild VS solution?

On my system I don't see any errors.
 If you following instructions from here https://github.com/tesseract-ocr/tesseract/wiki/Compiling#master-branch-305-and-later the `tesseract.sln` is in `build` directory inside your tesseract dir.
 Could you try to remove `C:/Users/jarek/.cppan/storage` completely and try again?
 Can you provide full log of `cmake ..` command from `build` dir?
 It seems that first error is in the zlib.

```
-- Preparing build tree for pvt.cppan.demo.zlib-1.2.8 with config amd64-msvc-19.0-32
-- 
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jarek/.cppan/storage/obj/82/1e/e49e/build/amd64-msvc-19.0-32
-- Configuring incomplete, errors occurred!
See also "C:/Users/jarek/.cppan/storage/obj/ec/64/e827/build/amd64-msvc-19.0-32/CMakeFiles/CMakeOutput.log".

```

Could you also attach `C:/Users/jarek/.cppan/storage/obj/ec/64/e827/build/amd64-msvc-19.0-32/CMakeFiles/CMakeOutput.log` and `CMakeError.log` from there?
 Hmmmm. I'm trying to repro the issue locally and cannot.
I removed whole storage couple of times and it works everytime.
 Try `cppan --self-upgrade` once again and run without removing storage. I've slightly updated the client, maybe the issue is gone. At the moment that's all I can advise. Need to think more.
 To use tesseract in your application in theory you could use CPPAN just in case if it will be working for you.
See https://github.com/cppan/tesseract_example
Also https://github.com/cppan/tesseract_example/tree/master/with_cppan - that's a very simple example to build small programs with cppan. It's like scripting but with C++.
 I've added these 3 lines https://github.com/cppan/tesseract_example/blob/master/with_cppan/main.cpp#L2
CPPAN will create a solution for you near the main.cpp :)
Like this http://imgur.com/Qu13mMT
  thanks!
  fixed by #415 
  @egorpugin - Can you have a look at this?
 Hi, sure.
The issue is that I removed USES_CPPAN definition and you downloaded the new version.
The modern variable is 'CPPAN_BUILD'.
I'll fix this and close the PR.
@zhantong  thanks for pointing this out!
 See 193032a
 Try to follow this guide https://github.com/tesseract-ocr/tesseract/wiki/Compiling#windows Did you do all steps with `cppan` decribed there?  Please use tesseract user forum for asking support
  You can't use a traineddata file which was prepared with a version of Tesseract that is newer than the version you use to do OCR. 
 https://sourceforge.net/projects/tesseract-ocr-alt/files/

https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md
  copied from #396

@nguyenq wrote:

Clearly, the tool produces inconsistencies in font names. Why is "Times New Roman," a valid name, especially it's a plain style?

```
298: Times New Roman,
299: Times New Roman, Bold
300: Times New Roman, Bold Italic
301: Times New Roman, Italic
302: Trebuchet MS
303: Trebuchet MS Bold
304: Trebuchet MS Bold Oblique
305: Trebuchet MS Oblique
306: Verdana
307: Verdana Bold
308: Verdana Bold Oblique
309: Verdana Oblique
310: Yu Gothic
311: Yu Gothic Bold
312: Yu Gothic Bold Oblique
313: Yu Gothic Light, Light
314: Yu Gothic Medium, Medium
315: Yu Gothic Medium, Medium Oblique
316: Yu Gothic Oblique
```
 This is also visible on linux, but only for a few fonts (windows origin?):

```
 39: Arial Black,
 40: Arial Black, Bold
221: Latin Modern Mono Light,
222: Latin Modern Mono Light, Bold
223: Latin Modern Mono Light, Bold Oblique
224: Latin Modern Mono Light, Oblique
227: Latin Modern Mono Prop Light,
228: Latin Modern Mono Prop Light, Bold
229: Latin Modern Mono Prop Light, Bold Oblique
230: Latin Modern Mono Prop Light, Oblique
246: Latin Modern Roman,
247: Latin Modern Roman, Bold
248: Latin Modern Roman, Bold Italic
249: Latin Modern Roman, Italic
335: Nimbus Sans L Condensed,
336: Nimbus Sans L Condensed, Bold
337: Nimbus Sans L Condensed, Bold Italic
338: Nimbus Sans L Condensed, Italic
347: Open Sans Condensed, Bold Condensed
364: Roboto Condensed,
365: Roboto Condensed, Bold
366: Roboto Condensed, Bold Italic
367: Roboto Condensed, Italic
368: Roboto Condensed, Semi-Light
369: Roboto Condensed, Semi-Light Italic
442: Times New Roman,
443: Times New Roman, Bold
444: Times New Roman, Bold Italic
445: Times New Roman, Italic
467: Ubuntu Condensed,
468: Ubuntu Condensed, Bold
```

Font description comes from pango library function `pango_font_face_describe`(https://github.com/tesseract-ocr/tesseract/blob/master/training/pango_font_info.cpp#L619).
 "Problem" is with later pango function: [pango-font-description-to-string](https://developer.gnome.org/pango/stable/pango-Fonts.html#pango-font-description-to-string) adds a trailing comma to a family name when the last word in a family list is a valid style attribute.

But the real problem is reverse function (setting font from string): [pango_font_description_from_string ()](https://developer.gnome.org/pango/stable/pango-Fonts.html#pango-font-description-from-string) - it needs the same string as produced by pango-font-description-to-string. 
This can be tested with "Arial Black" vs. "Arial Black," or "Times New Roman Bold" vs "Times New Roman, Bold".

It seams that solution would be to rewrite whole process printing font name (pango_font_description_get_family,  pango_font_description_get_weight, pango_font_description_get_style) but it would also require to rewrite parsing string to font. And I do not think it is worthy of time (only few fonts are effected and user can copy and paste the font string).
 I remember that in an older version of Gedit (in Ubuntu 14.04?) "Times New Roman**,**" (with a comma) was also present in font selection. With Ubuntu 16.04 newer version of Gedit, it is now listed without a comma. 
 https://github.com/tesseract-ocr/tesseract/blob/61032d9b14e1/training/pango_font_info.cpp#L523
 > Update: I came up with a simple workaround: If it failed with a san-comma fontname, I'd try again with the comma added.

Maybe that's what we should do in Tesseract...
  You forget to read instructions ;-)
  thanks
 https://github.com/amitdo/text2tif/blob/master/port/strcasestr.cpp
This is based on a more updated version of the EFL code. You might want to compare it with your version.
 OK :-)
 The lesson we should learn from this is: 
If you take a piece of code from another codebase and copy it into your codebase, you need to follow the other codebase for updates and [bug fixes](https://github.com/tasn/efl/commit/b457dff840ff1e).
  What version of tesseract you try to build? 
 did you run ./autogen.sh as stated in https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT.md?
 @billydekid
What's the output of `dpkg -l | grep pkg-config` ?
 @billydekid : I am sorry I misunderstood your comment - I thought it is fixed.
 @billydekid: Does it mean that have to install pango-dev to fix problem? If yes, that it is bug that need to be fixed! (in this case please reopen this issue)
 `libcogl-pango-dev` is not needed by Tesseract!

`libcogl-pango-dev` has these dependencies:

```
libpango1.0-dev
libcairo2-dev
pkg-config (recommended)


```
  thanks
  Question:
There are 2 ways these things could work:
1. FORCE the output to match the provided pattern(s) and/or word(s). With this option, you **can't get anything else out**, whatever is in the image.
2. Use the user-patterns and user-words as a hint. Other things could be output, if it thinks it is more likely. The hint can be made stronger, but **there will always be inputs that produce something outside of the patterns supplied.**
Which is it to be?
Can someone familiar with the above discussions please summarize for me, and if the consensus is 1 above, then it could be made to happen, or else it might be possible to increase the strength of the hint.

BTW, this could behave differently for base tesseract vs LSTM.  thanks.
  I believe that currently Tesseract still supports older C and C++ standards. Your patch will change this situation...
 Next version of tesseract (4.0) will need  C++11. 
3.x branch should be maintained that way it will be possible to build with VS2010...
 @stweil : thanks for clarification. I was able to build tesseract with this patch and VS2010.
  It does not crash on openSUSE linux:

[text2image_log.txt](https://github.com/tesseract-ocr/tesseract/files/443295/text2image_log.txt)
 @stweil, right!
See #345, #349
 @stweil OK
  Training on Windows is not officially supported (but we accept patches):
1. Some tools use libraries that are not common on Windows. I do not think it is worth to invest time  in testing windows implementation of these libraries.
2. Training is need only in special cases. In such cases it is much efficient to use VirtualBox (or similar tool) with linux. 
3. We are not aware about anybody who would like to provide Windows support.

@Shreeshrii @stweil  : please create separate issue for command that crash on linux, so we can track it.
 @nguyenq

```
text2image --text=vie-data.txt --outputbase=vie.arial.exp1 --font=Arial --fonts_dir=C:\Windows\Fonts
```

try `--font="Arial"` instead of `--font=Arial`.

```
text2image --text=vie-data.txt --outputbase=vie.arial.exp1 --font="Times New Roman" --fonts_dir=C:\Windows\Fonts
Could not find font named Times New Roman.Please correct --font arg.
```

As @Shreeshrii said, try  `--font="Times New Roman,"`.
 @Shreeshrii: "--fonts_dir=" is wong argument
 @Shreeshrii: These errors are comming from external library (Pango/FontConfig?), which are IMO not common on Windows.  IMO tessting&issue reporting should be reported there.
 @nguyenq

> The boxes in the generated box file were not as tight as they could be.

What do you mean? Is this also happening in Linux?
 @Shreeshrii: I need to correct my statement:

>  "--fonts_dir=" is wong argument
1.  I found out it is interpreted as --fonts_dir=""
2.  I found out that `--fonts_dir=""` reset fonts_dir variable to system default e.g. if --fonts_dir argument is not use text2image is looking for fonts in `/auto/ocr-data/tesstraining/fonts`
  Please use tesseract user forum for asking support (please read&use [relevant wiki](https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality) before).
  We do not provide support for tesseract wrapers. Please use tesseract user forum.
  user

On 13 Aug 2016 19:00, "songgl" notifications@github.com wrote:

> �what is the cause of this error?
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> https://github.com/tesseract-ocr/tesseract/issues/387#issuecomment-239630954,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AAjCzHOS0TfKBkP8jOIhr9ptK-YhkkcDks5qffgugaJpZM4Ji1Ec
> .
  1. Use tesseract user forum for asking question/support
2. We do not provide support for external project.
 @wanghaisheng, short answer: No.
 My user name is @amitdo, not amido :)
  1. Use tesseract forum for asking support
2. Do not use this issue tracker for project (if you have issue with tess-two - report it to tess-two authors/maintainers)
  Use tesseract user forum for asking support (search it before asking question - there were experiences/suggestion about 7 segment OCR in past)
  IMO this is not tesseract problem but pdf rendering problem. I tried your file in acrobat reader and here is copied text from it:
An exact method is presented for numerically calculating, within the framework of the
stochastic formulation of chemical kinetics, the time evolution of any spatially homogeneous
mixture of molecular species which interreact through a specified set of coupled

Here is screenshot:

![image](https://cloud.githubusercontent.com/assets/574156/17740797/20c8ab48-649a-11e6-8456-448bffb04598.png)
 The vertical problem in pdf.js was fixed by adjusting font metrics for Tesseract. The fix was handed off to Ray and is pending a code merge from Ray. I suspect the horizontal problem is due to a flaw pdf.js when interpretting the Tz directive, but am not sure. I'd be happy to work with a programmer on the pdf.js  side to figure this out for sure, but we have to find one first!
 @jbreiden: Is this fixed or are you still waiting for Ray? Good. Can you create Pull request against 3.05 branch?  thanks. Fixes #380
 The dotted circle fix might solve things for Mac users also (#195). 
  None of these issues has been solved.

At least the first one probably affects Tesseract running in MinGW and Mac.

> Fast Solution: specify fc backend

https://github.com/GNOME/pango/blob/master/pango/pangocairo-fontmap.c#L48

Something like this should be put in `text2image.cpp`:

```
#ifdef _WIN32
 putenv("PANGOCAIRO_BACKEND=fc");
#else
  setenv("PANGOCAIRO_BACKEND", "fc", 1);
#endif // _WIN32
```

Should be tested on Mac and MinGW before committing this code.
This issue does not affect Linux.
 AFAIK vidiecan is using VS. Is there any report from mingw users?
  This is not issue according https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md
  > the command: mftraining -F font_properties -U unicharset -O lang.unicharset lang.fontname.exp0.tr lang.fontname.exp1.tr ...
> needs to be corrected to add suffix .txt to font_properties to make it work

It only needs the `.txt` suffix if you saved the file as `font_properties.txt`...
 > In the command: training/set_unicharset_properties -U input_unicharset -O output_unicharset --script_dir=training/langdata
> The output is generated to output_unicharset.This output_unicharset should be given as input to the mftraining command. i.e instead of -U unicharset one should give -U output_unicharset

From the training guide:

```
mftraining -F font_properties -U unicharset -O lang.unicharset lang.fontname.exp0.tr lang.fontname.exp1.tr ...
```

This gives you the command line usage. You don't supposed to cut and paste it as-is.

The following paragraph explains it.

> The -U file is the unicharset generated by unicharset_extractor above, and lang.unicharset is the output unicharset that will be given to combine_tessdata.
 > the command : unicharset_extractor lang.fontname.exp0.box lang.fontname.exp1.box ... didn't work the first tim.Hence, i have to prefix training/ to make it work.

https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#building-the-training-tools

```
make training
sudo make training-install
```

If you run the second line, you should be able to run all the training commands without the prefix.
 @vkoushik: if it throwed you error it means you made something wrong (installation). Fix your installation instead of  adjusting manual to your mistakes. Your suggestion would not work for those who use packaged prepared by their distributor (windows or linux).
  Please do me a favor and take a look at 2.pdf which is an attachment towards the bottom of
the following bug. Tell me if that demonstrates the same incompatibility.

https://github.com/mozilla/pdf.js/issues/6863
 > 2.pdf doesn't show the problems in both pdf.js and when the text is extracted with pdfbox.

Good, because that is the future for Tesserct PDF output. 2.pdf has minor changes in the metrics of both the embedded font and the metrics in the PDF. I can't guarantee that this is going to work with every document, because PDF text extraction relies heavily in heuristics. (Root cause: PDF spec)
 It's more of a tweak than a rewrite. For logistical reasons, I hand all my changes to Ray who then merges them into the git repo. Ray is awesome in almost every way, but he is notoriously slow at this. I've already done the handoff.
 > Tesseract seems to ignore resolution information from PNG files.

Wait, what? That's not expected at all. Please provide an example PNG file demonstrating
the problem, and it will get attention right away.
 new pdf.ttf came to master and 3.05 branch.  
@ebogaard: Can you re-test? Please test only tesseract and  please provide command (how you run tesseract).  You used wrong command. It should be something like this:
  tesseract pdfsandwich45aaf9.tif pdfsandwich45aaf9 -l nld+eng pdf In tessdata repository there are 4.00 data files and you use 3.05 tesseract...
This is not supported. You need to use data files from the same or lower tesseract version (e.g. 3.04)  There is a limit for the number of **fonts** you can train Tesseract, which is 64 fonts.
 > There is a limit for the number of **fonts** you can train Tesseract, which is 64 fonts.

This was a reply to your question:

> Is there a limit on the number of .tr files i can give as input?

AFAIK, there is no hard limit on the number of `.tr` files.
 Did you test that your training data actually works?
 This output does not look like it is coming from Tesseract command line program...

In any case, as I said there is a 64 fonts limit.
 From Ray Smith DAS2014 Slides:

> Training Fundamentals
> ● Character samples must be segregated by font
> => Trained on synthetic (rendered, distorted) data
> ● Few samples required (4-10 of each combination is good. 1 is OK)
> ● Not many fonts required. (32 used for Latin)
> ● Not many fonts allowed. (MAX_NUM_CONFIGS=64: Long story.)
> ● Number of different “characters” now limited only by memory.

The `MAX_NUM_CONFIGS` is still set to `64` in the current codebase.
  I wonder why Gentoo, which is supposed to be a bleeding edge distro, does not have a package for the newer Tesseract version 3.04**.01**.

Try this:
`tesseract eng.Sans-serif.exp0.tif eng.Sans-serif.exp0 box.train`
 1. you did not provided your input files
2. you did not explain how you created input files
3.  command "tesseract eng.Sans-serif.exp0.tif - box.train" is not according  [training instruction](https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#run-tesseract-for-training)
 Version 3.04.01 is now available as a package for Gentoo (it is marked as 'testing'):
https://packages.gentoo.org/packages/app-text/tesseract
  Please read this:
https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md

Try asking your question in the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr)
  If you think that's exciting, check out the PDF output. Looks like we have at least one bug on our hands. Thanks for reporting this.

[foo.pdf](https://github.com/tesseract-ocr/tesseract/files/369763/foo.pdf)

![bug](https://cloud.githubusercontent.com/assets/4961958/16924868/2d239046-4cd6-11e6-92a6-a808d5beb5ab.png)
 This will take care of the PDF portion (so that we blend with white instead of just stripping the entire alpha channel). The recognition part is probably a discussion with Ray. Reading the source code, it is not clear to me why we don't recognize. Alpha looks like it is treated like any other color channel as opposed to something special.

``` c++
--- tesseract/api/pdfrenderer.cpp   2016-07-06 13:19:57.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp   2016-07-20 16:23:13.000000000 -0700
@@ -690,8 +690,9 @@
   int format, sad;
   findFileFormat(filename, &format);
   if (pixGetSpp(pix) == 4 && format == IFF_PNG) {
-    pixSetSpp(pix, 3);
-    sad = pixGenerateCIData(pix, L_FLATE_ENCODE, 0, 0, &cid);
+    Pix *p1 = pixAlphaBlendUniform(pix, 0xffffff00);
+    sad = pixGenerateCIData(p1, L_FLATE_ENCODE, 0, 0, &cid);
+    pixDestroy(&p1);
   } else {
     sad = l_generateCIDataForPdf(filename, pix, kJpegQuality, &cid);
   }
```
  Language data path is hardcoded in linux build see:
- https://github.com/tesseract-ocr/tesseract/blob/master/ccutil/Makefile.am#L6
- https://github.com/tesseract-ocr/tesseract/blob/master/ccutil/mainblk.cpp#L77
 Best way is to test it ;-).
 There is. Debian uses -DTESSDATA_PREFIX=/usr/share/tesseract-ocr/
 zdenko is the build king, he can decide such things
 All the ways to set  `TESSDATA_PREFIX` should be documented in the wiki.
 TESSDATA_PREFIX is set (via autotool variable @datadir@) to directory where autotools installed tessdata (at the time when tessdata were part of tesseract library repository). Because there was no installation on Windows (Visual studio) TESSDATA_PREFIX is (was?)  set to "./" 
There is [logic](https://github.com/tesseract-ocr/tesseract/blob/master/ccutil/mainblk.cpp#L58) of how to find tessdata location:
1. Use tessdata prefix from the command line.
2. Use tessdata prefix from the environment.
3. (Windows):  Look for tessdata in directory of executable.
4. Use tessdata prefix which was compiled in.

If you need to add something to wiki or configure help, you are welcomed to send PR ;-)
  TIFF gui warnings probably should be turned off during tiff library compilation with definition TIF_PLATFORM_CONSOLE.
 Your commit introduces dependency on tiff library for tesseract.
Now by default tesseract uses only leptonica library.
 I think there is another bug about better memory usage with multipage tiff, where the solution is also a tiff library dependency. Will try to find it.
 Yes, it was bug #223. There is an enhancement request to improve performance on multipage tiff reads. It is currently slow to read the latter images in multipage tiff. The only way to accomplish this is with a direct libtiff dependency. I didn't implement because I wasn't sure if it was worth it.
 I mean bug #233
 Yes, but the code won't be active almost always. Users should care about tiff library, download it, deploy, link to 'tesseractmain' or their own binary.
Also what is HAVE_TIFFIO_H? It is not defined in tesseract build system, so it will be turned off.

Better solution I provided when building tesseract with CPPAN.
See:
https://github.com/tesseract-ocr/tesseract/issues/209 (bottom comments)
https://github.com/tesseract-ocr/tesseract/wiki/Compiling#master-branch-305-and-later
and
https://github.com/DanBloomberg/leptonica/blob/master/cppan.yml#L62 this line will disable gui notifications from tiff library completely.
 Ah, I see now, sorry.
Well, it's ok then.
 Wait a second. Does this mean it is okay for me to use direct calls to libtiff if I 
put it inside HAVE_TIFFIO_H? If so, I can go make the multipage TIFF reader
vastly faster. 
 yes

On 18 Jul 2016 19:23, "jbreiden" notifications@github.com wrote:

> Wait a second. Does this mean it is okay for me to use direct calls to
> libtiff if I
> put it inside HAVE_TIFFIO_H? If so, I can go make the multipage TIFF reader
> vastly faster.
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> https://github.com/tesseract-ocr/tesseract/pull/367#issuecomment-233397033,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AAjCzNcT2CXvmIoUIyBJdt-DlI4Ur6ueks5qW7aNgaJpZM4JN-Iw
> .
 My vote for removing HAVE_TIFFIO_H (and removing this commit).
It is not used anywhere in tesseract except this commit.
Tesseract uses leptonica API, so maybe it's better to contribute there and use its API later.
 It seems HAVE_TIFFIO_H was a legacy check, that was automatically imported to cmake to save compatibility with autotools build. Now I see that it's not used.
 Well, this is a core decision. If and only if Tesseract allows a direct TIFF 
dependency, then I am going to use it to significantly speed up OCR of 
multipage TIFF as part of bug #233.
 Is it possible via leptonica API? I see some comments from Dan B. there.

Upd.: Ahh, Dan said that you really need direct libtiff dependency. No more questions from me.
 Yeah, Dan and I talked about this extensively and eventually came to agreement. It does not seem possible through Leptonica, even if we modify Leptonica. The issue is there is a libtiff data structure that must be held in memory, and must be iterated through to get successive images in multipage TIFF. Leptonica doesn't have the ability to keep this data structure around, because Leptonica is functional C program. So you can't stash it as a member variable as a Leptonica class or something like that. And we're sure as heck not going to put it in a global because that isn't threadsafe. Which means that, currently, Leptonica has to start over from the beginning every time. When someone asks image 2348 in a multipage TIFF, we have to start at the beginning and do 2348 seeks to get to data. 
 I suppose it is possible we are wrong. I'll look one last time, starting here.  Sorry for hijacking this bug.

http://www.libtiff.org/libtiff.html#Dirs
http://www.asmail.be/msg0054682091.html
 @egorpugin: libtiff is needed for [tesseract opencl](https://github.com/tesseract-ocr/tesseract/blob/master/opencl/openclwrapper.h#L5) implementation. So you need to remove much more than this commit. 
 It's strange a bit. Does OpenCL really need this? I.e. leptonica can not provide required interfaces? Or opencl is only implemented for tiff images?

Also with CPPAN build I mentioned above tiff dependency can be added in the simplest possible way (add one line to cppan.yml).

---

I'm trying to understand what do you need to get custom pages (images, directories) from tiff image. Looks like it's `TIFFSetDirectory(TIFF *tif, int dir_n)` [1,2] call. It reads image file from the beginning, iterates over each directory until requested dir number and reads it (`TIFFReadDirectory(tif)` [3]). Then it can be read by leptonica with `pixReadFromTiffStream(tif)` like here [4]. I do not understand why leptonica has one more cycle here [5]. It gives us O(N^2) complexity instead O(N), isn't it?

So, for example if we want to read custom dirs (images) from tiff via leptonica, the complexity is O(N). But if we want to read tiff sequentially we can add a new function to lept. For example, `pixReadStreamTiffNext()` which will iterate to the next dir (image) in the tiff stream via one `TIFFReadDirectory()` call because it increases `tif->tif_nextdiroff` offset.

Correct me if I'm wrong somewhere.

[1] http://www.libtiff.org/man/TIFFSetDirectory.3t.html
[2] https://github.com/vadz/libtiff/blob/master/libtiff/tif_dir.c#L1531
[3] http://www.libtiff.org/man/TIFFReadDirectory.3t.html
[4] https://github.com/DanBloomberg/leptonica/blob/master/src/tiffio.c#L375
[5] https://github.com/DanBloomberg/leptonica/blob/master/src/tiffio.c#L409
 The OpenCl code (amongst other things) provides hardware accelerated alternatives to a very small subset of Leptonica calls.

You are right,  Leptonica's pixReadStreamTiff() is adding lots of unnecessary overhead. The only reason we didn't notice it was all this stuff gets cached in memory, hence the massive amount of seeking is a little bit hidden. I already have a patch out to Dan to call TIFFSetDirectory() only once per image.  However, even a single call to TIFFSetDirectory() is expensive. 

Your suggestion about calling TIFFReadDirectory(TIFF *tif) to get down to linear will not work unless someone is able to hold onto the TIFF *tif struct. Tesseract can do this, but only if it is allowed to directly link to libtiff. Leptonica cannot do this, because it has nowhere persistent to hold it. Everything is completely reset for every image read,  including the call to TIFFOpen. All that said, I now see that we can potentially work around this by having Tesseract keep track of file offsets. Need to think this over.

``` c
#include <stdio.h>
#include <tiffio.h>

const char *testfile = "test.tiff";

size_t PrimeThePump() {
  TIFF *tiff = TIFFOpen(testfile, "r");
  TIFFSetDirectory(tiff, 0);
  size_t offset = TIFFCurrentDirOffset(tiff);
  TIFFClose(tiff);
  return offset;
}

size_t ThankYouSirMayIHaveAnother(size_t offset) {
  TIFF *tiff = TIFFOpen(testfile, "r");
  TIFFSetSubDirectory(tiff, offset);
  TIFFReadDirectory(tiff);
  offset = TIFFCurrentDirOffset(tiff);
  TIFFClose(tiff);
  return offset;
}

int main(void) {
  size_t offset = PrimeThePump();
  while (offset = ThankYouSirMayIHaveAnother(offset)) {
    printf("offset=%lu\n", offset);
  }
}
```
 So I see the following possible solution. We need to query all offsets in tiff file atleast once. We could do this in leptonica. Also we can add a function that will read tiff image by dir offset.

Code flow:

``` c
// tesseract side
FILE *fp = fopen(filename, "rb");
int n_dirs = 0;

// call to leptonica
// fp is opened, leptonica will call fast? TIFFClientOpen()
uint64_t *offsets = pixReadTiffOffsets(fp, &n_dirs);

// read interesting image O(1)
// fp is opened, leptonica will call fast? tif = TIFFClientOpen()
// TIFFSetSubDirectory(tif, offset)
// TIFFReadDirectory(tif)
// pixReadFromTiffStream()
PIX *image = pixReadTiffDir(fp, offsets[interesting_image_number]);

// free() offsets as C does not have C++ delete[]
free(offsets);
```

What do you think?
 I dunno; we almost always read all the images in a loop, so putting functions in Leptonica somewhat similar to my example would also do the trick.  This is ultimately Dan's decision. But also consider that it's kind of a logistics pain to put something new in Leptonica and then make Tesseract depend on it. So for Tesseract, we may be better off doing the direct libtiff calls.

[EDIT: Took a look at this. Impossible to keep everything on Tesseract's side due to pixReadFromTiffStream() is not exposed] 
 Added 1..74.2, tess will automatically use it now.  Please use tesseract user forum for asking support.
 `Warning: No shape table file present: shapetable`

This warning message is indeed confusing, but everyone gets it, so unless you are training Tesseract for an Indic script, you should ignore it.

For the second part of this issue, follow @zdenop advice.
 Did you used the `-F` flag?

`mftraining -F font_properties...`

`font_properties` should be the file name (including any extension like `.txt`) you saved the  font_properties file.
  Please provide detail (OS detail, tesseract version, how you compile, logs...)
 use the current version. Before compiling from source remove all other version of tesseract.
 1. Ubuntu has recent tesseract packages
2. 3.03 version is old - not officially produced version. Nobody will look at it.
  All the errors except the last one are coming from [leptonica](https://github.com/DanBloomberg/leptonica).

Check with a png image, just to make sure the source of the problem is not reading the tiff image.
 File input/output is handles by leptonica.
Anyway your experience is strange. I build tesseract (and leptonica) as non privileged user and install it with `sudo` and I never have this problem.
Can you please remove/uninstall leptonica and tesseract and than clone last version of leptonica and tesseract and build everything under userx , install with sude and try again your test?
If you have still problem can you provide your in.tiff file?
 Try converting the image with other programs like gimp or pngtopnm (netpbm).
 Try [pamditherbw](http://netpbm.sourceforge.net/doc/pamditherbw.html) on the pgm file.
 This bug report is confusing. Please generate shareable test images that give trouble, and post to this bug. Also, please report your platform details. Is this Linux or something else? If there is an obvious Leptonica problem, I'll help fix it.
 Thanks. By the way, the "root-only" thing suggests that there may be some sort of tmpfile involved that is running into a permission problem. I'll be excited if I can reproduce.
 Leptonica is trying to write its temporary files in /tmp/lept and can't because root owns it. This is going to require a Leptonica change. There are 11 calls to genTempFilename() in Leptonica.

https://github.com/DanBloomberg/leptonica/blob/master/src/pdfio2.c#L1240
https://github.com/DanBloomberg/leptonica/blob/master/src/pdfio2.c#L1200

```
sudo sudo -u nobody strace tesseract test-source2.pbm /tmp/q pdf
mkdir("/tmp", 0777)                     = -1 EEXIST (File exists)
mkdir("/tmp/lept", 0777)                = -1 EEXIST (File exists)
open("/tmp/lept/868647_30487_temp.tif", O_RDWR|O_CREAT|O_TRUNC, 0666) = -1 EACCES (Permission denied)
write(2, "Error in fopenWriteStream: strea"..., 45Error in fopenWriteStream: stream not opened
```
 This is a temporary, Linux specific Leptonica patch as a placeholder until upstream
does something official.

``` c
--- leptonica/src/utils.c   2016-06-13 16:53:20.000000000 -0700
+++ leptonica/src/utils.c   2016-07-19 12:19:39.000000000 -0700
@@ -3152,6 +3152,10 @@
  *          (b) multiple threads from a single process call this function, or
  *          (c) there is the possibility of an attack where the intruder
  *              is logged onto the server and might try to guess filenames.
+ *
+ * IMPORTANT: This function has been modified as a temporary Debian
+ * specific workaround for Debian Bug #830660, while upstream works on
+ * an official solution. All input parameters are disabled.
  * </pre>
  */
 char *
@@ -3160,42 +3164,23 @@
                 l_int32      usetime,
                 l_int32      usepid)
 {
-char     buf[256];
-char    *newpath;
-l_int32  i, buflen, usec, pid, emptytail;
-
+char    *pathout;
+l_int32  size;
     PROCNAME("genTempFilename");
-
-    if (!dir)
-        return (char *)ERROR_PTR("dir not defined", procName, NULL);
-    if (dir && strlen(dir) == 1 && dir[0] == '/')
-        return (char *)ERROR_PTR("dir == '/' not permitted", procName, NULL);
-    if (tail && strlen(tail) > 0 && stringFindSubstr(tail, "/", NULL))
-        return (char *)ERROR_PTR("tail can't contain '/'", procName, NULL);
-    emptytail = tail && (strlen(tail) == 0);
-    if (!usetime && !usepid && (!tail || emptytail))
-        return (char *)ERROR_PTR("name can't be a directory", procName, NULL);
-
-    if (usepid) pid = getpid();
-    buflen = sizeof(buf);
-    for (i = 0; i < buflen; i++)
-        buf[i] = 0;
-    l_getCurrentTime(NULL, &usec);
-
-    newpath = genPathname(dir, NULL);
-    if (usetime && usepid)
-        snprintf(buf, buflen, "%s/%d_%d_", newpath, usec, pid);
-    else if (usetime)
-        snprintf(buf, buflen, "%s/%d_", newpath, usec);
-    else if (usepid)
-        snprintf(buf, buflen, "%s/%d_", newpath, pid);
-    else
-        snprintf(buf, buflen, "%s/", newpath);
-    LEPT_FREE(newpath);
-
-    return stringJoin(buf, tail);
+#ifdef _WIN32
+  #error "mkstemp is probably not supported on Windows"
+#endif  /*  _WIN32 */
+    char pattern[] = "/tmp/lept.XXXXXX";
+    int fd = mkstemp(pattern);
+    if (fd == -1) {
+      return (char *)ERROR_PTR("failed to get tempfile", procName, NULL);
+    }
+    close(fd);
+    if ((pathout = (char *)LEPT_CALLOC(sizeof(pattern), sizeof(char))) == NULL)
+      return (char *)ERROR_PTR("pathout not made", procName, NULL);
+    strncpy(pathout, pattern, sizeof(pattern) - 1);
+    return pathout;
 }
-

 /*!
  * \brief   extractNumberFromFilename()
```
 > This is a temporary, Linux specific Leptonica patch as a placeholder until upstream
> does something official.

```
This function has been modified as a temporary Debian
+ * specific workaround for Debian Bug #830660, while upstream works on
+ * an official solution.

```

@DanBloomberg
Do you remember this issue?
 OK. Thank you Dan!
  please follow [rules](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md).
  please follow [rules](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md).
  Right-to-left languages are incredibly important. Arabic,  for example, has hundreds of
millions of speakers and is the fifth most widely used language when including all variants.
I recommend looking at the DAS tutorial slides, they are interesting reading. The last 
set 'Building a Multi-Lingual OCR Engine' talks about in-progress numbers with Farsi 
and other languages that currently perform poorly with Tesseract 3. Machine learning 
has advanced quite a bit recently, and that's where the energy is.

http://www.vistawide.com/languages/top_30_languages.htm
https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016
 Take a look at DAS 2016 slides
  I worked extensively with Ken Sharp on ghostscript compatibility and thought we were in pretty good shape. Not sure what the story is here.
 Well, I've reproduced your problem with a different PDF and the tilt doesn't seem to be a factor. The text is definitely getting represented differently after a pass through ghostscript. I think you should contact Ken and see what he thinks. There's nothing obviously wrong to my eye about the ghostscript respresentation, but text extraction in PDF is often more of an art than a science. (As always, I think the root problem is the PDF specification itself.)

Before:

```
BT
3 Tr 1 0 0 1 82.2 512.8 Tm /f-0-0 10 Tf 140.64 Tz [ <0045><0058><0050><0045><0052><0049><0045><004E><0043><0045> ] TJ 77.16 0 Td 156.802 Tz [ <0041><004E><0044> ] TJ 30.6 0 Td 129.334 Tz [ <0052><0045><004C><0041><0054><0049><0056><0049><0054> ] TJ 58.8 0 Td 141.6 Tz [ <0059> ] TJ 30.36 0 Td 94.8 Tz [ <0035><0039> ] TJ 
ET
```

After

```
BT
/R10 10 Tf
1.4064 0 0 1 82.2 512.8 Tm
3 Tr
[(�E)-500(�X)-500(�P)-500(�E)-500(�R)-500(�I)-500(�E)-500(�N)-500(�C)-500(�E)-500]TJ
1.56802 0 0 1 159.36 512.8 Tm
[(�A)-500(�N)-500(�D)-500]TJ
1.29334 0 0 1 189.96 512.8 Tm
[(�R)-500(�E)-500(�L)-500(�A)-500(�T)-500(�I)-500(�V)-500(�I)-500(�T)-500]TJ
1.416 0 0 1 248.76 512.8 Tm
[(�Y)-500]TJ
0.948 0 0 1 279.12 512.8 Tm
[(�5)-500(�9)-500]TJ
```
 @jbarlow83 Can you please do a compatibility check on 2.pdf as described in this thread?
I want to get as many reports as possible before making a change.

https://groups.google.com/forum/#!topic/tesseract-dev/2EmMMoR3QGs
  Sorry, I'm still busy with other issues. This PR comes later.   > I just tried to build with Cygwin (64 bit, tesseract + training) and had not problems with the unpatched code.

hmmm....

@matzeri?
 @matzeri
The patch for `training/pango_font_info.cpp` in this PR is different from yours. Please have a look at it and test it. 
 @stweil

> T_WIN is set to false by default, so this line could simply be removed (if false is the correct value).

Fixed. Thanks.
 @stweil
Please confirm that this PR compiles with Cygwin 64, including training tools.
 @Shreeshrii reported that this patch https://github.com/amitdo/text2tif/commit/0473f14f, which is same as the above patch works on Cygwin...

I'll change it anyway.
 ~~Something went wrong with the update. I'll try again soon.~~
Strange. It looks fine now.
  Currently Tesseract needs some patches so it can be built for Cygwin.
See here: https://github.com/matzeri/cygwin-pkg/tree/master/tesseract

I want to fix Tesseract's Cygwin compatibility, so no patches will be needed anymore.

Pinging @matzeri...
Hi Marco!

I have a question for you - why is the ['tesseract-undefined.patch'](https://github.com/matzeri/cygwin-pkg/blob/master/tesseract/tesseract-undefined.patch) needed?
 Some links related to the ['tesseract-training.patch'](https://github.com/matzeri/cygwin-pkg/blob/master/tesseract/tesseract-training.patch):

PR https://github.com/tesseract-ocr/tesseract/pull/60

My own patch to 'pango_font_info.cpp'
https://github.com/amitdo/text2tif/commit/0473f14f
 See PR #351.
  Hi Tom!

You need to install more packages:
https://github.com/tesseract-ocr/tesseract/wiki/Compiling#dependencies

After that, re-install leptonica and then tesseract.

If you have further questions about this issue, please use the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr).

[Here is our guide](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md) for reporting issues and asking questions.
  Please use the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr).

[Here is our guide](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md) for reporting issues and asking questions.
  Try padding the image and use the '-psm' command line flag.
If you have further questions about this issue please use the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr).
  > Trying to familiarize with the source code I came across some discrepancies between the file names in the file header comments and the actual file names. Maybe it's pedantic but I thought having the correct names might facilitate reading the code especially when using printouts.

+1 for the suggestion to fix it!

but because your patches touch too many files and for other reasons I prefer that a core developer from Google to take it.
 this PR can not be merged to current code. Can you please update it? The request is reasonable, but I would rather spend my limited time fixing actual bugs than up-integrating/merging such large changes (and asking colleagues to review them) that perform little real useful purpose, especially when there is a chance that a large chunk of the code could go away in the near future.  Please send your question to the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr).

See [CONTRIBUTING](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md).
  Please send your question to the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr).

https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md
  Android has (had?) similar behaviour.
https://code.google.com/p/android/issues/detail?id=16672
  I'm sorry, but we have a clear policy - we don't want to see questions here, only bug reports. I believe your problem is not a bug in Tesseract.

https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md

> Sometimes you will not get a respond to your issue or question. We apologize in advance! Please don't take it personally. There can be many reasons for this, including: time limits, no one knows the answer (at least not the ones that are available at that time) or just that your question has been asked (and has been answered) many times before...
  @egorpugin

> Try these binaries compiled by me.
> https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1
> You have to install VC2015 x86 redist from microsoft.com in order to run them.
> Leptonica is built with all libs except for libjp2k.

@nickbe

> This build does not create PDFs, but the cygwin from http://3.onj.me/tesseract/ build does.
> So currently the VS build is not usable.

@jbreiden

> What is this business about certain builds not supporting PDF output? There is nothing special or weird with that section of code. Are they just old builds from before the feature was created, or is there something else going on?

@egorpugin

> Is there a simple command to check pdf generation?
> I'll try to debug the issue (if any).
 @egorpugin

```
tesseract /path/to/phototest.tif phototest pdf
```

https://github.com/tesseract-ocr/tesseract/raw/master/testing/phototest.tif
 These traineddata files are required: eng, osd
 Ok, and how should I understand what is incorrect? Where is tesseract output, error message, anything?
Entered this command, did some preparations and pdf is generated.
Works for me.
Latest binaries are here: https://www.dropbox.com/s/pxu2hp6mg1a64zj/tesseract-3.05.00dev-win32-vc19-2016-jun-03.zip?dl=1
 @egorpugin
How did you build these two (old and new) binaries?
 Built liblept with every image library except jpeg2k, built tesseract with that lept. VS2015 Update 2.
I said that in previous thread (issue).
If tesseract.exe creates pdfs and other binaries not, give me simple repro for one of them, so I'll check.
 Since the problem does not exist in your **newer** build, maybe we should close this issue.
 It works also on the old build. I checked.
 Do you have any guess why it does not work for @nickbe (the old build)?
 Nobody wants to provide simple repro of this issue. I cannot guess what's the problem.
 I don't have Windows so I can't reproduce it myself... 
 Since both @nickbe and @egorpugin confirm that the PDF renderer feature works in the newer VC build, I'm closing this issue for now.
  You mentioned in another issue that you are using Cygwin, so I guess this issue also happens with Cygwin, right?

Does this issue happen when you give it a page with many lines?

Could you upload the original image **and** the PDF file?
 Which PDF viewer are you using? Adobe Acrobat Reader?

I checked the first PDF file with Chrome browser and Evince on Ubuntu 16.04. With these viewers the issue you describe does not exist.
 The PDF renderer feature is tested with these viewers:
- Adobe Acrobat Reader
- PDFium (Chrome/Chromium)
- OS X Preview
- pdf.js (Firefox)
- Evince (poppler)
 Just for a reference:
https://github.com/sumatrapdfreader/sumatrapdf/issues/544
 If the SumatraPDF engineer wants someone to talk to, send them my way.
  https://github.com/blog/1184-contributing-guidelines

This should replace https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice

Any comments?
 I would keep it in FAQ too, or provide link with short description...
 Yeah, this was my intention :-)
 I'm more in favor for the link option.
 @zdenop
Yesterday there was an option to "squash and merge" in the PR web interface. I don't see it right now. Did you change the settings?
https://github.com/blog/2141-squash-your-commits
 No I did not make any changes.
 Okay :)
 I wanted to choose this option, but accidentally push the button itself instead of the arrow :(
Not a big deal...
 IMHO, this is bad UX design.
 Right. But it's too tempting  to use the GUI - just a few clicks...
 https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  Hi Stefan!

You missed `AC_PREREQ`.
https://www.gnu.org/software/autoconf/manual/autoconf-2.69/html_node/Versioning.html
 Thanks!
  Please use [Tesseract users forum](https://groups.google.com/d/forum/tesseract-ocr) and ask this question (and other questions you might have) there.
  Please use tesseract user forum for asking support (see: [FAQ#rules-and-advices](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice))
  Thanks for the patch!
    @stweil
A few lines below my new code:

```
unsigned int i = 1;
  for (i = 1; i < *argc; ++i) {
    const char* current_arg = (*argv)[i];
    ...
    // If this is asking for usage, print the help message and abort.
    if (!strcmp(current_arg, "help") ||
        !strcmp(current_arg, "helpshort")) {
      tprintf("USAGE: %s\n", usage);
      PrintCommandLineFlags();
      exit(0);
    }
```
 Anyway, you'll have to submit a new PR, since Zdenko already merged this one. 
 Also check what we are doing here:
https://github.com/tesseract-ocr/tesseract/blob/master/api/tesseractmain.cpp
 You also might want to read about `EXIT_FAILURE` and `EXIT_SUCCESS`
https://www.google.co.il/search?q=EXIT_FAILURE
`EXIT_SUCCESS` is used in `text2image.cpp`.
 Another issue.

What do you think about the code in https://github.com/tesseract-ocr/tesseract/pull/330#issuecomment-220828244

It searches for 'help' string in all `argv` strings. I think it should only check `argv[1]`. This is what is being done in `tesseractmain.cpp`.
 Okay...
  Hi!
Good question. The right place to ask this question is in the [tesseract-dev](https://groups.google.com/d/forum/tesseract-dev) mailing list. I hope you'll get an answer from Ray Smith.
  Please read and follow [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) before creating issue.
  please follow [FAQ#rules-and-advices](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice)
 @reubano
I added  'tesserocr' to the AddOns wiki.
https://github.com/tesseract-ocr/tesseract/wiki/AddOns#tesseract-wrappers
 I added this example to [APIExample](https://github.com/tesseract-ocr/tesseract/wiki/APIExample)
 > It would be helpful to have a CONTRIBUTING file with that information so that github will show it on the issues page.

@reubano, thanks for the suggestion.
Now [we have it](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md)! :-)
  Run: `tesseract --list-langs`
Do you get `ara` in the output?
 And the message is clear: there is no /opt/local/share/tessdata/eng.traineddata 
English traineddata is must for correct running of tesseract (because it is used for fallback).
 Do you have installed arabic cube files (e.g. [ara.cube.lm](https://github.com/tesseract-ocr/tessdata/blob/master/ara.cube.lm))?
 But your original issue (wrong installation) was solved.
For asking support use tesseract user forum - there are more people...
  Please read this:
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#generate-the-unicharset-file
 ... and this:
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#questions-about-the-training-process
 Your unicharset file looks wrong.

Either you didn't run `set_unicharset_properties` or you did run it, but in a wrong way.

A debugger won't help you here. A human 'debugger' might help... :) 

https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#set_unicharset_properties
  See issue #318
 ```
Warning: No shape table file present: shapetable
...
Warning: no protos/configs for Joined in CreateIntTemplates()
Warning: no protos/configs for |Broken|0|1 in CreateIntTemplates()
```

These specific warnings are confusing, but everyone gets them, so they should be ignored.
 So, you can now change the text to 'can do it!'... :)
 Good luck with the internship!
  Use [tesseract user forum](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) for asking support.
  `script_dir` in `set_unicharset_properties` should point to a directory that contains a `*.unicharset` file. For English and other Latin based scripts, the file is `Latin.unicharset`.
You can find the `*.unicharset` files here: https://github.com/tesseract-ocr/langdata
 @ggdhines

Try this: 

```
/home/ggdhines/github/tesseract/training/set_unicharset_properties -U unicharset -O new_unicharset --script_dir=/home/ggdhines/github/langdata
```

Since your unicharset file has glyphs which belong to the `Common` script I think you should also put the `Common.unicharset` in the script dir.
https://raw.githubusercontent.com/tesseract-ocr/langdata/master/Common.unicharset
 @ne0zer0

`unicharset_extractor` produces a `unicharset` file.

You need to pass this file to `set_unicharset_properties`.

> -U **unicharset**

Download these files:
https://raw.githubusercontent.com/tesseract-ocr/langdata/master/Latin.unicharset
https://raw.githubusercontent.com/tesseract-ocr/langdata/master/Common.unicharset

Lets say you put these files in `langdata` directory located under `/home/ne0zer0`.

Now, run this:

> set_unicharset_properties --F font_properties -U unicharset -O output_unicharset --script_dir=**/home/ne0zer0/langdata**
 https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#shapeclustering-new-in-302

> shapeclustering **should not be used except for the Indic languages.**
 > FAIL!
>    APPLY_BOXES: boxfile line 937/1 ((1267,2218),(1310,2289)): FAILURE! Couldn't find a matching blob

And: 

> Boxes read from boxfile:    1727
> Boxes failed resegmentation:       5
> Found 1722 good blobs.

As long as you get only a few of these failures, it's probably OK.

> Warning: no protos/configs for Joined in CreateIntTemplates()
> Warning: no protos/configs for |Broken|0|1 in CreateIntTemplates()

This is also normal.

Overall, the output of the commands looks OK.
 https://github.com/tesseract-ocr/tesseract/blob/a3ba11b030345d32829b1e8355afea5419978d82/doc/unicharset.5.asc

> CAVEATS
> 
> Although the unicharset reader maintains the ability to read unicharsets of older formats and will assign default values to missing fields, the accuracy will be degraded.

ne0zer0:

> What you suggest is likely to produce such degraded result. (what I seem to experiment)

It's the opposite of what you said. You interpreting the above paragraph wrongly.

I'll give you more answers later. 
 The meaning of this 'CAVEAT':

Starting from Tesseract version 3.02 the unicharset file should look like this:

```
110
NULL 0 NULL 0
N 5 59,68,216,255,87,236,0,27,104,227 Latin 11 0 1 N
Y 5 59,68,216,255,91,205,0,47,91,223 Latin 33 0 2 Y
1 8 59,69,203,255,45,128,0,66,74,173 Common 3 2 3 1
9 8 18,66,203,255,89,156,0,39,104,173 Common 4 2 4 9
a 3 58,65,186,198,85,164,0,26,97,185 Latin 56 0 5 a
...
```

If you will use the old format:

```
; 10 Common 46
b 3 Latin 59
W 5 Latin 40
7 8 Common 66
= 0 Common 93
```

the accuracy will be degraded, because the training tool will assign default (suboptimal) values to missing fields.
 `0,255,0,255,0,0,0,0,0,0` is the default (suboptimal) values for glyph_metrics.
 @ne0zer0
Again, you are completely wrong.

You should both be patient and wait for my further answers which may clear things up for you.
 When you run `unicharset_extractor` you get a `unicharset` file in which each line has these fields:
`character` `properties` `glyph_metrics` `script` `other_case` `direction` `mirror` `normed_form`.

In this stage all the fields accept the `character` field are set to their default values. We want to set these fields to their correct values!

So we run `set_unicharset_properties` (we will call it 'the tool'):

```
set_unicharset_properties -U unicharset -O new_unicharset -X xheights --script_dir=/home/myusername/tesseract-ocr/langdata
```

The tool will take our `unicharset` file and add new values in the various fields. We will get a new fixed `new_unicharset` file.

More details:
First, the tool will call a few functions to fill the correct values to these fields:
`properties` `script` `other_case` `direction` `mirror` `normed_form`.

What left to fill are the 10 `glyph_metrics` fields' values.

Tesseract does not provide a training tool that generate the correct values for the `glyph_metrics` fields for the specific trained fonts.

Instead, there is a pre-made unicharset file for each script (the unicharset files in the `langdata` repo) which contains "universal" `glyph_metrics` that have been set from a large number of fonts.

The tool will look for a few files in the directory you told it to search, `/home/myusername/tesseract-ocr/langdata` in this example.

The tool will scan the lines in the `unicharset` file, and for each `character` field in a line it will search the appropriate `Scriptname.unicharset`. For example, for the character 'C' it will read the `Latin.unicharset` and for the character '8' it will read the `Common.unicharset`. The tool will search a matching line in the `Scriptname.unicharset` that has the same character field as the 'current' line in the `unicharset` line. When it find such a match it will take the `glyph_metrics` values located in the same line as the character and implant them in the matching line as new `glyph_metrics` values in the the `unicharset` file line.

A second file that the tool will search is a `Scriptname.xheights` file.
Here is a link to the `Latin.xheights` file.
https://raw.githubusercontent.com/tesseract-ocr/langdata/master/Latin.unicharset

According to Ray Smith, the lead developer of Tesseract:

> If the font you are using is not listed in this file, it will use the mean of the ones that are. IIRC these numbers are used to set up expectations for inter-character spacing. They are for a fixed, quite large size (32 pt??).

('using' = 'training')

The tool will process the info in `Latin.xheights` together  with the new values of the `glyph_metrics` and will output the result to a file, `xheights` in our example.
 That's it. I did my best efforts to explain things you ask about. You can now respond... :)
 > It will be certainly less accurate to use default values; so we cannot get the best result for specific fonts? Unless to waste a lot of time in training Tesseract? For an "uncertain" result?

https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality

> There are a variety of reasons you might not get good quality output from Tesseract. It's important to note that unless you're using a very unusual font or a new language retraining Tesseract is unlikely to help.

*

> Why set_unicharset_properties does not compute such values? It would not be too difficult to develop such a functionality.

If you develop such a tool (or hire someone to do so) we can add a link in the wiki to your site...

My answer for your other questions: 
This is the current situation and you should accept it...

Your last question - you probably did something wrong if you get an empty file. I will try to test it later.  

Two last notes:
I and the other people responding most of the time here and in the mailing-list are volunteers.
Within free (not paid) open source projects, complaining would not help, kind request might help but it's not guaranteed.
 > the file Latin.xheights seems to do nothing... and I always get the same output_unicharset file, with or without Latin.xheights.

The `Latin.xheights` is not supposed to change anything in the `output_unicharset`.
 You seem to think that `/` is the current directory, but it's not.
`/` is your 'root' directory. `./` (or just `.`) is the current directory.
 > What can be done with a filled xheights file?

https://github.com/tesseract-ocr/tesseract/blob/master/doc/mftraining.1.asc
 Nick White @nickjwhite had tried to build the tool you want.
See this old thread:
https://groups.google.com/forum/?hl=en#!searchin/tesseract-ocr/pango|sort:date/tesseract-ocr/QH09G5p1jGI/QcHIJvfzFaYJ
 Hi Nick!
Did you test the impact of using the output of these tools compared to the "universal" unicharset?
 Pinging @nickjwhite ...
  There is no environment variable named "CXPFLAGS".
There are "CFLAGS", "CPPFLAGS",  "CXXFLAGS".
 Didn't try this...
 IMO this is not tesseract issue. If you decided to install libs in non standard location you should know how to deal with it. One of examples suggestion is on [wiki](https://github.com/tesseract-ocr/tesseract/wiki/Compiling#install-elsewhere--without-root). If it does not work - [tesseract user forum](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice).
  Use the official guide at https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract

You missed this step: https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#set_unicharset_properties-new-in-303

See also my answer here: https://github.com/tesseract-ocr/tesseract/issues/318
  You should [read the wiki, search issues (also closed), search in the tesseract forum before you post your issues/question](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice).
Opencl is still experimental feature with a lot of open issues (and original committer is not fixing them...) 
 Hi Zdenko!
I added a note about the state of OpenCL in Tesseract to this wiki page:
https://github.com/tesseract-ocr/tesseract/wiki/TesseractOpenCL
  `unicharset_extractor example.basic.exp0.tiff`

You are giving `unicharset_extractor`a tiff image as input file, but it expects a box (text) file.
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract

If you have further questions, please see:
https://github.com/tesseract-ocr/tesseract#support
  Here is the Tesseract formula:
https://github.com/Homebrew/homebrew-core/blob/master/Formula/tesseract.rb
  See also #135
  Which version of Tesseract are you using?
The `hocr_font_info` parameter is only available in version 3.04.00 and up.
 If you have further questions, please see:
https://github.com/tesseract-ocr/tesseract#support
  Please see:
https://github.com/tesseract-ocr/tesseract#support
 https://github.com/tesseract-ocr/tesseract/blob/d8c04e8/training/language-specific.sh#L308

```
JPN_FONTS=( \
    "TakaoExGothic" \
    "TakaoExMincho" \
    "TakaoGothic" \
    "TakaoMincho" \
    "TakaoPGothic" \
    "TakaoPMincho" \
    "VL Gothic" \
    "VL PGothic" \
    "Noto Sans Japanese Bold" \
    "Noto Sans Japanese Light" \
    )
```

These are the fonts used in the current `jpn.traineddata.`

If you want to train Tesseract, use one of these guides:
https://github.com/tesseract-ocr/tesseract/wiki/tesstrain.sh
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract

For further questions, please use the [Tesseract users forum](https://groups.google.com/d/forum/tesseract-ocr).
  For asking support use tesseract user forum (see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice))
  Please see:
https://github.com/tesseract-ocr/tesseract#support
 What I meant was that I think this kind of problem / question should be sent to the mailing list.
 > But isn't it clearly a bug? ocropus-rpred has no problem with bad.jpg.

There no OCR engine that is 100% accurate. The fact that OCR engine 'A' is more accurate (for a specific input and/or as a whole) than OCR engine 'B' does not make by itself the 'B' a buggy software.

To be fair, I think in this specific case it could be a bug...
 Very true.
  There is no documented way to train 'Cube', so you can't do it.
 Cube training is not supported and cube should be removed in future
  Please use tesseract user forum for asking support
  Try this:
`tesseract arabic_4.jpg stdout -psm 0 -l osd`

You will need the `osd.traineddata`.

FYI, Tesseract 3.04.01 will also output the **name** of the detected script instead of useless script number. 
  I'm sorry, but we don't support 3rd-party wrappers around (lib)tesseract here.
 please report issue at pytesseract project 
  > Basically what I want to achieve is to ask Tesseract to recognize only complete words included in my custom dictionary (lang: chi_sim)

Tesseract can't do this. The dictionaries are just a hint for Tesseract. 
 AFAIK it is not possible within tesseract.
In some extent you can implement it by yourself. E.g. in first stage you do OCR by tesseract and then you can correct recognized text by other tool (e.g. spellchecker with custom dictionary).
  Can you please provide image for testing?
 I understand. But can you modified or create similar image that can demonstrate issue?
 @vidiecan, please, can you respond to @zdenop and @stweil?
  I suggest you repost this here:
https://github.com/tesseract-ocr/langdata
and then close this issue.

but don't repeat calling/mentioning those 2 people.
  https://github.com/tesseract-ocr/tesseract/blob/0afd5939b1e14239d6bfb875e4160b76d3672e02/api/tesseractmain.cpp#L56

It will be useful to have the version info for all binaries including training tools, text2image, lstmeval etc. Also relevant to discussion - Switch to semantic versioning

https://github.com/tesseract-ocr/tesseract/pull/593 > What exactly should be shown? Version number only for tagged release? Branch version? Git release? Compilation date?

IMO, since a large number of people build tesseract from github source it will be useful to have some kind of identifying info in the version being displayed. It would be helpful in troubleshooting.

See related discussion on `Add version and github info always, also when --enable-debug was missing`  https://github.com/tesseract-ocr/tesseract/issues/723 

 > Executable and shared library libtesseract might have different versions.

Can version info also be included for shared libraries? @stweil 

>Support command line options -v, --version for all executables.

Is it possible to add this before release of 3.05.01? In adding the version string to the traineddata, I moved the version data
to a new file ccutil/version.h, so any of the tools could access that to
output their version.

On Fri, Jul 14, 2017 at 9:03 PM, Shreeshrii <notifications@github.com>
wrote:

>  text2image -v
> ERROR: Non-existent flag -v
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/293#issuecomment-315507763>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056RWfgQ6laeiNfg8-OJnW3QuOnWEvks5sODoNgaJpZM4H4kFO>
> .
>



-- 
Ray.
 Please see https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/Qp2cIrcajX4/CF0uohuUAwAJ

It would help if there was some kind of version checking of traineddata files against the --oem mode being used and an appropriate user friendly error message is output (rather than assert) tesseract -v shows the information from ccutil/version.h (ASAIK) and not from traineddata files.

```
#ifndef TESSERACT_CCUTIL_VERSION_H_
#define TESSERACT_CCUTIL_VERSION_H_

#define TESSERACT_VERSION_STR "4.00.00alpha"
#define TESSERACT_VERSION 0x040000
#define MAKE_VERSION(major, minor, patch) \
  (((major) << 16) | ((minor) << 8) | (patch))

#endif  // TESSERACT_CCUTIL_VERSION_H_
```

traineddata version is shown after unpacking with combine_tessdata on console as well as saved in its version file. It is different in tessdata, tessdata_best and tessdata_fast.

```
# combine_tessdata -u ./tessdata/eng.traineddata eng.
Extracting tessdata components from ./tessdata/eng.traineddata
Wrote eng.unicharset
Wrote eng.unicharambigs
Wrote eng.inttemp
Wrote eng.pffmtable
Wrote eng.normproto
Wrote eng.punc-dawg
Wrote eng.word-dawg
Wrote eng.number-dawg
Wrote eng.freq-dawg
Wrote eng.cube-unicharset
Wrote eng.cube-word-dawg
Wrote eng.shapetable
Wrote eng.bigram-dawg
Wrote eng.lstm
Wrote eng.lstm-punc-dawg
Wrote eng.lstm-word-dawg
Wrote eng.lstm-number-dawg
Wrote eng.version
Version string:Pre-4.0.0
1:unicharset:size=7477, offset=192
2:unicharambigs:size=1047, offset=7669
3:inttemp:size=976552, offset=8716
4:pffmtable:size=844, offset=985268
5:normproto:size=13408, offset=986112
6:punc-dawg:size=4322, offset=999520
7:word-dawg:size=1082890, offset=1003842
8:number-dawg:size=6426, offset=2086732
9:freq-dawg:size=1410, offset=2093158
11:cube-unicharset:size=1511, offset=2094568
12:cube-word-dawg:size=1062106, offset=2096079
13:shapetable:size=63346, offset=3158185
14:bigram-dawg:size=16109842, offset=3221531
17:lstm:size=5390718, offset=19331373
18:lstm-punc-dawg:size=4322, offset=24722091
19:lstm-word-dawg:size=7143578, offset=24726413
20:lstm-number-dawg:size=3530, offset=31869991
23:version:size=9, offset=31873521
```

```
 combine_tessdata -u ./tessdata_fast/eng.traineddata eng.
Extracting tessdata components from ./tessdata_fast/eng.traineddata
Wrote eng.lstm
Wrote eng.lstm-punc-dawg
Wrote eng.lstm-word-dawg
Wrote eng.lstm-number-dawg
Wrote eng.lstm-unicharset
Wrote eng.lstm-recoder
Wrote eng.version
Version string:4.00.00alpha:eng:synth20170629
17:lstm:size=401636, offset=192
18:lstm-punc-dawg:size=4322, offset=401828
19:lstm-word-dawg:size=3694794, offset=406150
20:lstm-number-dawg:size=4738, offset=4100944
21:lstm-unicharset:size=6360, offset=4105682
22:lstm-recoder:size=1012, offset=4112042
23:version:size=30, offset=4113054
```

```
combine_tessdata -u ./tessdata_best/eng.traineddata eng.
Extracting tessdata components from ./tessdata_best/eng.traineddata
Wrote eng.lstm
Wrote eng.lstm-punc-dawg
Wrote eng.lstm-word-dawg
Wrote eng.lstm-number-dawg
Wrote eng.lstm-unicharset
Wrote eng.lstm-recoder
Wrote eng.version
Version string:4.00.00alpha:eng:synth20170629:[1,36,0,1Ct3,3,16Mp3,3Lfys64Lfx96Lrx96Lfx512O1c1]
17:lstm:size=11689099, offset=192
18:lstm-punc-dawg:size=4322, offset=11689291
19:lstm-word-dawg:size=3694794, offset=11693613
20:lstm-number-dawg:size=4738, offset=15388407
21:lstm-unicharset:size=6360, offset=15393145
22:lstm-recoder:size=1012, offset=15399505
23:version:size=80, offset=15400517
``` Does it make sense to keep a VERSION text file in tesseract base directory and for all programs, makefiles, cmakelists etc to refer to that to build the Version string.

I had seen an implementation in 
https://github.com/bendiken/templates/blob/master/c%2B%2B.autotools/configure.ac The version string that is generated for older files is:

```
Version string:Pre-4.0.0
```

So, that is an indication of move towards semantic versioning.

@stweil Would you streamline the VERSION handling for the package?   @DanBloomberg, any comment?
 OK. Thanks for your reply, Dan.

Just a quote:

> To spam, or not to spam, that is the question.

;-)
  Did you try read error message???  
IMO it is clear "pango/pango-font.h: No such file or directory". 
  1. Do not change source code unless you are willing to fix consequences by yourself.
2. Make sure that cygwin64 is supported by your opencl provider.
3. Opencl is experimental feature. It was tested on linux and partially on VS2010. Other compilers are not supported - but you can try tesseract user forum for support.
 Thank. Changed
 follow instrustruction for cmake ;-)
  First patch was fixed on 20 Jul 2015! See https://github.com/tesseract-ocr/tesseract/commit/301eaeca5faa4bd1d39a0047471abeec3bb52ac3
  Hi Nick!

> This feature requires Pango 1.38 or newer.

Pango 1.38 was released in 2015-09-21.
It's too new. You should '#ifdef' the code.
 Travis CI on Linux agrees with me :-)

```
/home/travis/build/tesseract-ocr/tesseract/training/stringrenderer.cpp:212:45: error: ‘pango_attr_font_features_new’ was not declared in this scope

       pango_attr_font_features_new(features_);

```
 Here is a standalone version of Tesseract's `text2image`.
https://github.com/amitdo/text2tif
 Off-topic, but this might interest you as well:
https://github.com/eddieantonio/isri-ocr-evaluation-tools
  Hi !

You should give us more details:
- Version of Visual Studio C++.
- Version of Tesseract.
- If this happens with your code, but not with 'tesseract.exe' command line tool, you should provide a sample code that invokes the bug. 
- Output of compiler errors.
 > How can I become a developer of tesseract on github?

You should learn how to use 'pull request' on github.
https://help.github.com/
 > How can I become a developer of tesseract on github?

You can start with creating correct issue report.
It is known that tesseact can be build with problem with VS2010, VS2013 and VS2015.
 http://stackoverflow.com/questions/23740431/tesseract-remove-reference-ambiguous-symbol-in-project-on-visual-studio-2012
 @amitdo: it confirms that it is not tesseract problem but bad practice of writing code using ["namespace std"](http://stackoverflow.com/questions/1452721/why-is-using-namespace-std-in-c-considered-bad-practice) which is real root of ambiguity.
  Please google and read [forum](https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/tesseract-ocr/JZ9G3D5HHNM/A--DXmm2BgAJ) before writing issue
  fixed (moved because of broken link)
  Message is IMO clear: there is no leptonica >= 1.71 in standard location. This is not tesseract problem. 
  What is benefit of this? How to use it?
  Please use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) for asking support (see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) )
  - In training/pango_font_info.cpp
 > Good catch. 

Thanks. I've noticed that missing symbol in #195.

> There seem to be more errors of this kind.

I see you already hunted two... :-)
 Would that flag catch that missing % from format specifier?
  Please use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) for asking support (see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) )
  Please check [the source](https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc#languages) before writing issue.
BTW: current version is 3.04.01.
  Git repository is for developers who know how to deal with their system.
Regular users should use packaging tools of their system.
 Before you run those commands, you should put the `*.traineddata` files in `tessdata`
 subdirectory in the source tree.

The *.traineddata files must be places in the `tessdata` directory together with the config files.  
 > As a regular user I cannot copy the tesddata files to /usr/local/share/tessdata
> 
> $ cp ./tessdata/san.traineddata /usr/local/share/tessdata
> cp: cannot create regular file ‘/usr/local/share/tessdata/san.traineddata’: Permission denied

Well, `/usr` is a system directory so you can't write to it without admin rights. 

To run a command as a root user use `sudo`.

Be careful when running commands with `sudo`, you might damage your system if you type the wrong command!

https://help.ubuntu.com/community/RootSudo
http://manpages.ubuntu.com/manpages/karmic/man8/sudo.8.html
https://www.raspberrypi.org/documentation/usage/terminal/
  I see in file "ccmain/pagesegmain.cpp: line 322 - 333". 

``` c++
  // Leptonica is used to find the rule/separator lines in the input.
  LineFinder::FindAndRemoveLines(source_resolution_,
                                 textord_tabfind_show_vlines, pix_binary_,
                                 &vertical_x, &vertical_y, music_mask_pix,
                                 &v_lines, &h_lines);
  if (tessedit_dump_pageseg_images)
    pixWrite("tessnolines.png", pix_binary_, IFF_PNG);
  // Leptonica is used to find a mask of the photo regions in the input.
  *photo_mask_pix = ImageFind::FindImages(pix_binary_);
  if (tessedit_dump_pageseg_images)
    pixWrite("tessnoimages.png", pix_binary_, IFF_PNG);
```

Please notice to line: 

``` c++
  if (tessedit_dump_pageseg_images)
    pixWrite("tessnoimages.png", pix_binary_, IFF_PNG);
```

I try with configuration:

```
tessedit_dump_pageseg_images    1
```

And images: 
![news](https://cloud.githubusercontent.com/assets/8704662/13735304/7b94f300-e9da-11e5-94f0-c4eeac8ca254.png)

But result in file "tessnoimages.png" is: 
![tessnoimages](https://cloud.githubusercontent.com/assets/8704662/13735340/bf23a256-e9da-11e5-97d1-ee3d6e1e9128.png)

I think before `pixWrite("tessnoimages.png", pix_binary_, IFF_PNG);`, we need something like:

``` c++
if (*photo_mask_pix != NULL) {
    pixSubtract(pix_binary_, pix_binary_, *photo_mask_pix);    
}
```
 At least in [osdetect](https://github.com/tesseract-ocr/tesseract/blob/master/ccmain/osdetect.cpp#L177) it is done that way. Anyway I found out that   pixSubtract(pix_binary_, pix_binary_, *photo_mask_pix) does not removed all images.
@theraysmith: can you have a look at this?
  Please use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) for asking support (see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) )
  fixed with ddd3cad8c6802d016a48aa21e6303e8004d50955
  Which version/commit you use?
Can you do more analysis and send a fix/patch)?
Main development is done on linux and windows developer is needed
  You can try to use 3.02 version if you need only English. AFAIR it was
singnificantly faster on my (old) computer.

Zdenko

On Thu, Mar 10, 2016 at 4:35 PM, younes notifications@github.com wrote:

> I integrated Tesseract C/C++, version 3.x, to read English OCR on images.
> 
> It’s working pretty good, but very slow. It takes close to 1000ms (1
> second) to read the attached image (00060.jpg) on my quad-core laptop.
> 
> I’m not using the Cube engine, and I’m feeding only binary images to the
> OCR reader.
> 
> Any way to make it faster. Any ideas on how to make Tesseract read faster?
> thanks
> [image: 00060]
> https://cloud.githubusercontent.com/assets/9968625/13674495/ac261db4-e6ab-11e5-9b4a-ad91d5b4ff87.jpg
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tesseract-ocr/tesseract/issues/263.
 > ...  3.02 version ... AFAIR it was significantly faster on my (old) computer.

~~3.02~~ 3.02.02 is compiled with '-O3' by default.
https://github.com/tesseract-ocr/tesseract/blob/3.02.02/configure.ac#L161

3.03 and 3.04 are compiled with '-O2' by default.
https://github.com/tesseract-ocr/tesseract/blob/3.03-rc1/configure.ac#L201
https://github.com/tesseract-ocr/tesseract/blob/3.04.01/configure.ac#L300

2.04 and 3.01 are compiled with ~~'-O0'~~ '-O2' by default.
https://github.com/tesseract-ocr/tesseract/blob/2.04/configure.ac
https://github.com/tesseract-ocr/tesseract/blob/3.01/configure.ac
The 'configure.ac' script in these versions does not explicitly set the '-O' level, so autotools will use ~~'-O0'~~  '-O2' as default.
 What I linked to was actually 3.02.**02** 

I think this is 3.02:
https://github.com/tesseract-ocr/tesseract/blob/d581ab7e12a2fac4a73ac0af4ce7ec522b8f3e42/configure.ac

You are right. It does not contain any '-On' flag, ~~so the compiler will use '-O0', which is not good for speed.~~ so if you are using autotools to build Tesseract it will instruct the compiler to use '-O2'.
 I assume you are using Tesseract on Linux / FreeBSD / Mac. On Windows + MS Visual C++ the `configure.ac` file is irrelevant.
 Thanks Shree.

I don't know which optimization level is used for Visual C++.
 VS2010 use optimization flag /O2 (Maximize speed) - other flags are set to default.
In past in forum there were warnings against using compiler optimization flag as they affect also OCR results. This is reason why there are standard optimization flags (-O2 in autotools and /O2 in VS).

I tried to run perf tool on linux:
   `perf record tesseract eurotext.tif eurotext`
and I got this report (`perf report`):

```
  39,77%  tesseract  libtesseract.so.3.0.4  [.] tesseract::SquishedDawg::edge_char_of
  13,98%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ComputeCharNormArrays
  13,09%  tesseract  libtesseract.so.3.0.4  [.] IntegerMatcher::UpdateTablesForFeature
   4,22%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::PruneClasses
   2,66%  tesseract  libtesseract.so.3.0.4  [.] ScratchEvidence::UpdateSumOfProtoEvidences
   1,48%  tesseract  libtesseract.so.3.0.4  [.] ELIST_ITERATOR::forward
   1,16%  tesseract  libc-2.19.so           [.] _int_malloc
   1,15%  tesseract  libtesseract.so.3.0.4  [.] tesseract::ShapeTable::MaxNumUnichars
   1,01%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ExpandShapesAndApplyCorrections
   0,87%  tesseract  liblept.so.5.0.0       [.] rasteropLow
   0,79%  tesseract  libm-2.19.so           [.] __mul
   0,72%  tesseract  libtesseract.so.3.0.4  [.] FPCUTPT::assign
   0,71%  tesseract  libc-2.19.so           [.] _int_free
   0,71%  tesseract  libtesseract.so.3.0.4  [.] ELIST::add_sorted_and_find
   0,61%  tesseract  libtesseract.so.3.0.4  [.] tesseract::AmbigSpec::compare_ambig_specs
   0,57%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ComputeNormMatch
   0,52%  tesseract  libc-2.19.so           [.] memset
   0,49%  tesseract  libc-2.19.so           [.] vfprintf
   0,45%  tesseract  libc-2.19.so           [.] malloc
   0,36%  tesseract  libtesseract.so.3.0.4  [.] SegmentLLSQ
   0,31%  tesseract  libm-2.19.so           [.] __ieee754_atan2_sse2
   0,31%  tesseract  libc-2.19.so           [.] malloc_consolidate
   0,30%  tesseract  libtesseract.so.3.0.4  [.] LLSQ::add
   0,29%  tesseract  libtesseract.so.3.0.4  [.] GenericVector<tesseract::ScoredFont>::operator+=
   0,29%  tesseract  libtesseract.so.3.0.4  [.] _ZN14ELIST_ITERATOR7forwardEv@plt
   0,28%  tesseract  libtesseract.so.3.0.4  [.] tesseract::ComputeFeatures
   0,25%  tesseract  liblept.so.5.0.0       [.] pixScanForForeground
   0,24%  tesseract  libtesseract.so.3.0.4  [.] GenericVector<tesseract::ScoredFont>::reserve
   0,20%  tesseract  libtesseract.so.3.0.4  [.] C_OUTLINE::increment_step
   0,20%  tesseract  [kernel.kallsyms]      [k] clear_page
```

according this report 3 top function consumed 66% of "time".

Then I tried 4 pages (A4 ) tiff (G4 compressed):

```
  52,24%  tesseract  libtesseract.so.3.0.4  [.] tesseract::SquishedDawg::edge_char_of
  12,06%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ComputeCharNormArrays
  10,06%  tesseract  libtesseract.so.3.0.4  [.] IntegerMatcher::UpdateTablesForFeature
   3,57%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::PruneClasses
   1,90%  tesseract  libtesseract.so.3.0.4  [.] ScratchEvidence::UpdateSumOfProtoEvidences
...
```

Then I tried non eng image: `perf record tesseract hebrew.png hebrew -l heb`:

```
  27,79%  tesseract  libtesseract.so.3.0.4  [.] IntegerMatcher::UpdateTablesForFeature
  27,34%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ComputeCharNormArrays
   4,40%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::PruneClasses
   3,98%  tesseract  libtesseract.so.3.0.4  [.] ScratchEvidence::UpdateSumOfProtoEvidences
   3,05%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ComputeNormMatch
   2,36%  tesseract  libtesseract.so.3.0.4  [.] tesseract::ShapeTable::MaxNumUnichars
   2,05%  tesseract  libtesseract.so.3.0.4  [.] tesseract::Classify::ExpandShapesAndApplyCorrections
...
```
 Just for record for possible improvement in this issue: there was interesting information posted in scantailor project: [OpenCL alone only brings ~2x speed-up. Another ~6x speed-up comes from multi-threaded processing.](https://github.com/scantailor/scantailor/issues/236#issuecomment-243178950)
 That is a surprisingly hard question to answer in the Google environment!

I use 'opt' mode which after some digging, I found maps to -O2.
In addition, explicitly added are:
-fopenmp which will deliver a major improvement (3x faster), if you do not
have it, and a corresponding -lgomp for the linker
arch/dotproductavx.cpp is compiled with -mavx
arch/dotproductsse.cpp (and actually all the rest of the code) is compiled
with -msse4.1

I thought all this stuff was in the autotools files already, or are you
looking to convert these to windows?

On Sat, Apr 8, 2017 at 10:50 AM, Stefan Weil <notifications@github.com>
wrote:

> Don't expect much difference between -O2 and -O3. I tried different
> optimizations, and they only have small effects on the time needed for OCR
> of a page. Higher optimization levels can even result in slower code
> because the code gets larger (because of unfolding of loops), so CPU caches
> become less effective. It is much more important to write good code.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/263#issuecomment-292734412>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Qbi9xKk5GXQtfgXVZajN10mksEUks5rt8j6gaJpZM4Ht19x>
> .
>



-- 
Ray.
 OpenMP speeds up training by about 3.5x, since it runs 4 threads (one for
each part of the LSTM) and spends >90% of CPU time computing the LSTM
forward/backward.

On Sat, Apr 15, 2017 at 7:11 AM, Stefan Weil <notifications@github.com>
wrote:

> Yes, for training a single new model OpenMP could perhaps speed up the
> training process. Up to now, OpenMP is only used in ccmain/ and in lstm/.
> I don't know how much that part is used during training, and I never have
> run a performance evaluation for the training process (in fact I‌ have only
> run LSTM training once for Fraktur, and as I already said, it was not
> really successful).
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/263#issuecomment-294295776>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056QxUeSroEmcJmZ30om3_wi6Mlyu5ks5rwNAogaJpZM4Ht19x>
> .
>



-- 
Ray.
 No, it doesn't help. The parallelism is limited by the implementation of
the LSTM as 4 matrix-vector products.
When I experimented with more threads for some of the other operations (eg
the output softmax), it slowed down because the cache coherency was lost.
I also experimented with breaking the matrix-vector products up further (eg
splitting the input from the recurrent part), but openMP doesn't seem too
good at allocating the threads in a way that keeps the cache coherency.
Each thread needs to run the same part of the weights matrix for each
timestep, and that is difficult to achieve with the recurrent nature of the
LSTM.

On Tue, Apr 18, 2017 at 11:09 PM, xlight <notifications@github.com> wrote:

> can I set more than 4 threads for Trainning LSTM?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/263#issuecomment-295112242>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056UAEnzbmZZ5vncaO2zr0ASll1IoCks5rxaUjgaJpZM4Ht19x>
> .
>



-- 
Ray.
 It still works. It just takes longer.

On Wed, Apr 19, 2017 at 10:00 AM, Amit D. <notifications@github.com> wrote:

> What about machine that have only 2 cores?
> Shouldn't the 'num_threads' lowered to 2 in that case?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/263#issuecomment-295345495>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056dmv_0xhpF-2Qt11PJbfyg5Z-Bepks5rxj26gaJpZM4Ht19x>
> .
>



-- 
Ray.
  That error means that you are not linking against wrong version of leptonica  library (<1.71). You need to fix your leptonica instalation
  thanks. Fixed
  Please use tesseract user forum for asking support (see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice)).
Hint: Provide original image you try to OCR not screenshot from you app.
  This is not tesseract problem, but problem how you maintain your system. Compiling from source requires you are familiar with your system.
Building tools requires to links against leptonica. And you system (base how you maintain it) says it should link against liblept-4.dll.
Coping liblept-5.dll as liblept-4.dll is very bad idea.
Maintain you system correctly and you will get good result.
 Did you recompile (./configure, make, sudo make install, sudo ldconfig) tesseract after you recompiled leptonica 1.73?
 rebuilding tesseract does not help unless leptonica instalation is not fixed within msys...
 Do you have liblept.dll?
It should be a symlink to liblept-5.dll
 As zdenko said, something is wrong with your leptonica installation.

Here are the files in my linux system:

/usr/local/lib/liblept.a
/usr/local/lib/liblept.la
/usr/local/lib/liblept.so symlink to /usr/local/lib/liblept.so.5.0.0
/usr/local/lib/liblept.so.5 symlink to /usr/local/lib/liblept.so.5.0.0
/usr/local/lib/liblept.so.5.0.0
 Real problem is that he has several installation of leptonica but msys instruct linker to use older leptonica version (which could be reasonable if older version was installed by msys packagin system and new version "by hand") . There are several ways how to solve it:
1. Learn how your environment/system is working, how to manage it
2. If 1. is not option - wait for official packages and do to try to compile from source
3. Use only one version of software => do not use packaging system, but compile from source. From my experience, this option will sooner or later lead to the current status (something goes wrong and I have not clue why) ;-). 
 Shree,

Here are the commands I ran on Linux:

```
cd path/to/leptonica-1.73

./configure
make
sudo make install
sudo ldconfig

cd path/to/tesseract-ocr
./autogen.sh
./configure  --enable-debug
make
sudo make install
sudo ldconfig
make training
sudo make training-install
```

Did you run `sudo ldconfig` ?
 Shree,

About your `Compiling` wiki edit.

You need to run `sudo ldconfig` after every time you manually (not with package manager) update, add or remove a shared library (*.so / *.dll).

```
./autogen.sh
./configure
make
sudo make install
sudo ldconfig
```

Here `sudo ldconfig` is needed because `sudo make install` installed a library - `libtesseract.so.3`, in your system.

However,  `sudo ldconfig` **is not needed** after

```
make training
sudo make training-install
```

because `sudo make training-install` only install some **programs**, but does not install any **library**. 
 http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html
  Please use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) for asking support (see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) )
  I follow comment in this link: [FAQ There are inconsistent r....](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#there-are-inconsistent-results-from-tesseract-when-the-same-tessbaseapi-object-is-used-for-decoding-multiple-images)

But when I using tesseract with that options: 

```
classify_enable_learning 0
classify_enable_adaptive_matcher 0
```

I received one message like following: 
![segmentationfault](https://cloud.githubusercontent.com/assets/8704662/13592015/cf15c950-e520-11e5-8dd3-cff447e8cc1d.png)

I think this is one bug, because setting in config file is common for user. 
I find on all forum but not have any topic talk about this issue.
 Please provide also input files (test1.tif and config.txt)
 Hi zdenop, thanks for your quick response. 
I would like attacht 2 files 
[config.txt](https://github.com/tesseract-ocr/tesseract/files/163134/config.txt) 

Following tif file, I can not upload to this comment, therefore I upload to my repository. You can access to following link to get tif file.
[https://github.com/nam-leduc/positioning/blob/master/test1.tif](https://github.com/nam-leduc/positioning/blob/master/test1.tif)

Best regards,
Le Duc. Nam
 What OS are you using?
Did you try to install tesseract (as I see you from screenshot you are using not installed tesseract) and than use tesseract?
Do you have more versions of tesseract installed?
 I can reproduce this issue.

[config.txt](https://github.com/tesseract-ocr/tesseract/files/163665/config.txt)

```
tesseract phototest.tif phototest config.txt
Tesseract Open Source OCR Engine v3.05.00dev-266-gb1c1382 with Leptonica
Page 1
Segmentation fault (core dumped)
```

But this one works...

```
tesseract phototest.tif phototest -c classify_enable_learning=0 -c classify_enable_adaptive_matcher=0
Tesseract Open Source OCR Engine v3.05.00dev-266-gb1c1382 with Leptonica
Page 1
Warning in pixReadMemTiff: tiff page 1 not found
```

[phototest.txt](https://github.com/tesseract-ocr/tesseract/files/163680/phototest.txt)
 ```
gdb tesseract


(gdb) run phototest.tif phototest config.txt
Starting program: /usr/local/bin/tesseract phototest.tif phototest config.txt
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Tesseract Open Source OCR Engine v3.05.00dev-266-gb1c1382 with Leptonica
Page 1

Program received signal SIGSEGV, Segmentation fault.
tesseract::Tesseract::recog_all_words (this=0x808c00, page_res=0x81cb90, 
    monitor=monitor@entry=0x0, target_word_box=target_word_box@entry=0x0, 
    word_config=word_config@entry=0x0, dopasses=dopasses@entry=0)
    at control.cpp:320
320     } else if (!AdaptiveClassifierIsEmpty()) {


(gdb) backtrace
#0  tesseract::Tesseract::recog_all_words (this=0x808c00, 
    page_res=0x81cb90, monitor=monitor@entry=0x0, 
    target_word_box=target_word_box@entry=0x0, 
    word_config=word_config@entry=0x0, dopasses=dopasses@entry=0)
    at control.cpp:320
#1  0x00007ffff769929d in tesseract::TessBaseAPI::Recognize (
    this=this@entry=0x7fffffffdce0, monitor=0x0) at baseapi.cpp:902
#2  0x00007ffff76994e4 in tesseract::TessBaseAPI::ProcessPage (
    this=this@entry=0x7fffffffdce0, pix=0x83f110, 
    page_index=page_index@entry=0, 
    filename=filename@entry=0x7fffffffe257 "phototest.tif", 
    retry_config=retry_config@entry=0x0, 
    timeout_millisec=timeout_millisec@entry=0, renderer=renderer@entry=
    0x81cb50) at baseapi.cpp:1231
#3  0x00007ffff7699a5c in tesseract::TessBaseAPI::ProcessPagesMultipageTiff (this=this@entry=0x7fffffffdce0, data=data@entry=0xdde558 "II*", 
    size=38668, filename=filename@entry=0x7fffffffe257 "phototest.tif", 
    retry_config=retry_config@entry=0x0, 
    timeout_millisec=timeout_millisec@entry=0, 
    renderer=renderer@entry=0x81cb50, tessedit_page_number=-1)
    at baseapi.cpp:1064
#4  0x00007ffff769a0c3 in tesseract::TessBaseAPI::ProcessPagesInternal (
    this=this@entry=0x7fffffffdce0, filename=<optimized out>, 
---Type <return> to continue, or q <return> to quit---
    retry_config=retry_config@entry=0x0, 
    timeout_millisec=timeout_millisec@entry=0, renderer=0x81cb50)
    at baseapi.cpp:1183
#5  0x00007ffff769a2f0 in tesseract::TessBaseAPI::ProcessPages (
    this=this@entry=0x7fffffffdce0, filename=<optimized out>, 
    retry_config=retry_config@entry=0x0, 
    timeout_millisec=timeout_millisec@entry=0, renderer=<optimized out>)
    at baseapi.cpp:1081
#6  0x0000000000401f2a in main (argc=<optimized out>, argv=0x7fffffffde78)
    at tesseractmain.cpp:448

```
 But I can not ;-):

```
tesseract test1.tif test1.tif config.txt 
Tesseract Open Source OCR Engine v3.05.00dev with Leptonica
Page 1
Page 2
```

neither on linux (3.05.00dev-266-gb1c1382) or windows 7  (tesseract 3.04.01)
 Use my config.txt ...

He attached his config.txt with only 1 line, but said:

> But when I using tesseract with that options:
> 
> classify_enable_learning 0
> classify_enable_adaptive_matcher 0

My system is Ubuntu 14.04.
 Thanks! Now I am able to reproduce it (crash with config file and no crash with "-c").
 `classify_enable_adaptive_matcher 0` 
in the config file is causing the crash, 
not `classify_enable_learning 0`.

Updated file:
[config.txt](https://github.com/tesseract-ocr/tesseract/files/164139/config.txt)
 Hi @amitdo and @zdenop, 

I'm sorry, I try with other config options for checking what option make crash, but I forget recovering to original config file. 

```
classify_enable_learning 0
classify_enable_adaptive_matcher 0
```
 Even 3.03 crashes with this config file.
 > The reason that it doesn't crash when the config variable is set on the command line is because that's done after the recognizer is initialized, so the necessary data structure has been created.

Can you elaborate on this?
 @tfmorris @amitdo : beside this issues this behaviour should be documented: option "-c" can not be used for [init only](https://github.com/tesseract-ocr/tesseract/wiki/ControlParams#init-only) parameters. Or do we change of parsing of "-c" params?
 I think we should print a warning if someone try to set an init parameter using '-c var=val' in the command line. The relevant function is `SetParam`in `ccutil/params.cpp`.
 About `classify_enable_adaptive_matcher` - Ray should handle it. 
 But the FAQ should be fixed.
  Please create PR. Thanks.
 thanks.
 done.

Zdenko

On Tue, Mar 8, 2016 at 5:03 PM, jbreiden notifications@github.com wrote:

> Please make sure this reaches the 3.04 branch.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tesseract-ocr/tesseract/issues/254#issuecomment-193840163
> .
  Problem is that you [did not followed instruction](https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#generate-training-images) so you are alone with your problem.

Also it looks like your font seem to be common. In such case training is useless (from community experience) - nobody was able to reach quality provided by google traineddata.
 First I all: "you assume" and you ask as to prove it ;-). If you want to create acceptable issue, please use google traineddata and not modified tesseract. 

Next: tesseract is known that it requires to follow training procedure strictly. If you decide to not follow it, please do not create issue, but use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) instead.

Next: Community experience is that if your font looks common (e.g. it is very similar to fonts listed e.g. in https://github.com/tesseract-ocr/tessdata/blob/master/eng.cube.size) training is waste of time (you will get worse results). You should focus on image preprocessing instead.  
 With the default eng.traineddata I get:

```
tesseract z.png - -psm 10
N
```

So use the default traineddata and [Improve the input image](https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality) if needed.
 > ... fonts listed e.g. in https://github.com/tesseract-ocr/tessdata/blob/master/eng.cube.size

A better link:
https://github.com/tesseract-ocr/tesseract/blob/master/training/language-specific.sh

> List of fonts to train on
> LATIN_FONTS=(
>     "Arial Bold" \
>     "Arial Bold Italic" \
>     "Arial Italic" \
>     "Arial" \
>     "Courier New Bold" \
>     "Courier New Bold Italic" \
>     "Courier New Italic" \
>     "Courier New" \
>     "Times New Roman, Bold" \
>     "Times New Roman, Bold Italic" \
>     "Times New Roman, Italic" \
>     "Times New Roman," \
>     "Georgia Bold" \
>     "Georgia Italic" \
>     "Georgia" \
>     "Georgia Bold Italic" \
>     "Trebuchet MS Bold" \
>     "Trebuchet MS Bold Italic" \
>     "Trebuchet MS Italic" \
>     "Trebuchet MS" \
>     "Verdana Bold" \
>     "Verdana Italic" \
>     "Verdana" \
>     "Verdana Bold Italic" \
>     "URW Bookman L Bold" \
>     "URW Bookman L Italic" \
>     "URW Bookman L Bold Italic" \
>     "Century Schoolbook L Bold" \
>     "Century Schoolbook L Italic" \
>     "Century Schoolbook L Bold Italic" \
>     "Century Schoolbook L Medium" \
>     "DejaVu Sans Ultra-Light" \
> )
  And the error message/problem is???
 > pdf.ttx has never been required for Tesseract PDF generation... there is no need to distribute with Tesseract at all.

So why we keep GlyphLessFont.h|cpp? It's not even in sync with the updated pdf.ttf.
 @zdenop, could you remove these two files?
 Done. See cab6de17400f8147fbbd2a56125ba8c23a730e7a
  Did you oversaw #ifndef GRAPHICS_DISABLED ?
  Can you please make pull requests?
  It seem you did not install tesseeract training tool...
 For asking support please use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) - see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice)
  I am not sure if it is an good idea to return valid output (0.0) when tesseract did not produced certainty...
And as you mentioned: this patch does not solve root of problem. I suggest to keep it open for Ray investigation.
  @szpeak 

I'm sorry but this pull request does not make any sense.
  https://github.com/tesseract-ocr/tesseract/blob/master/api/baseapi.cpp#L954

> const char \*  TessBaseAPI::GetDatapath() {
>   return tesseract_->datadir.c_str();
> }
  https://github.com/tesseract-ocr/tesseract/blob/master/ccmain/thresholder.cpp#L54

```
// SetImage makes a copy of all the image data, so it may be deleted
// immediately after this call.
// Greyscale of 8 and color of 24 or 32 bits per pixel may be given.
// Palette color images will not work properly and must be converted to
// 24 bit.
// Binary images of 1 bit per pixel may also be given but they must be
// byte packed with the MSB of the first byte being the first pixel, and a
// one pixel is WHITE. For binary images set bytes_per_pixel=0.
void ImageThresholder::SetImage(const unsigned char* imagedata,
                                int width, int height,
                                int bytes_per_pixel, int bytes_per_line) {
```
 I totally agree...
:-)
 Oops  I made a mistake.

The relevant method takes 'const Pix*'.
https://github.com/tesseract-ocr/tesseract/blob/master/ccmain/thresholder.cpp#L143

My previous comment linked to an overloaded method which takes 'const unsigned char*'.

```
// Pix vs raw, which to use? Pix is the preferred input for efficiency,
// since raw buffers are copied.
// SetImage for Pix clones its input, so the source pix may be pixDestroyed
// immediately after, but may not go away until after the Thresholder has
// finished with it.
void ImageThresholder::SetImage(const Pix* pix) {
  if (pix_ != NULL)
    pixDestroy(&pix_);
  Pix* src = const_cast<Pix*>(pix);
  int depth;
  pixGetDimensions(src, &image_width_, &image_height_, &depth);
  // Convert the image as necessary so it is one of binary, plain RGB, or
  // 8 bit with no colormap.
  if (depth > 1 && depth < 8) {
    pix_ = pixConvertTo8(src, false);
  } else if (pixGetColormap(src)) {
    pix_ = pixRemoveColormap(src, REMOVE_CMAP_BASED_ON_SRC);
  } else {
    pix_ = pixClone(src);
  }
  depth = pixGetDepth(pix_);
  pix_channels_ = depth / 8;
  pix_wpl_ = pixGetWpl(pix_);
  scale_ = 1;
  estimated_res_ = yres_ = pixGetYRes(src);
  Init();
}
```
 The mystery thickens...

The above function seems to be correctly written to deal with colormaps:
- 1st case `(depth > 1 && depth < 8)` : the image is either a low bit-depth greyscale or a colormapped image with at most 128 color indexes (well probably 16 colors in real cases). Then the image is converted to a 256-level gray-scale image (`pixConvertTo8(...,false)`)
- 2nd case: the bit depth is at least 8 and the image has a colormap. As more than 8 bit per channel images are not handled by leptonica, it is either a 256 indexes grayscale image or a 256 indexes RGB image. pixRemoveColormap produces a non-indexed 8bit grayscale or 24bit RGB image.
- last case: at least 8bpp and no colormap, so it is a non-indexed 8bit grayscale or 24bit RGB image.

So the image is now a 8bit per channel raster with no colormap, it seems fit for thresholding.

I checked which way followed both my test images, and I extracted the thresholded image:

```
--- api/baseapi.cpp.orig    2016-02-18 08:48:00.000000000 +0100
+++ api/baseapi.cpp 2016-03-01 23:37:26.247132277 +0100
@@ -2151,6 +2152,9 @@
   }
   tesseract_->set_source_resolution(estimated_res);
   SavePixForCrash(estimated_res, *pix);
+pixWriteTiff("thres.tiff", *pix, IFF_TIFF,"w");
 }

```

The 'ok.png' fits the second case (8bit palette) and exits this function as 24bit RGB (well pixGetDepth(..)=32), here is the thresholded image:
![ok-thresholded](https://cloud.githubusercontent.com/assets/17527508/13445517/e7539042-e00c-11e5-87ad-51af5a70251e.png)

The 'nak.png" fits the first case (4bit palette) and exits this function as a 8bit grayscale. Result of threshold:
![nak-thresholded](https://cloud.githubusercontent.com/assets/17527508/13445556/31b3e2fe-e00d-11e5-916c-b35465a16e4e.png)

Well, the letters in the second image are thinner, but in the first image there are some small peaks under the letters... And for the only letter that is not correctly recognized in these lines, the middle 'a' of the first line, I'm really not sure which version I prefer...

So:
- the result differs if the color to gray conversion is done here or further in the threshold process. 
- in my case it is best that it is not done here, probably because it gives too thin letters
- but maybe that with bold letters the "good" and "bad' cases would be inverted ?

For the moment I can't see a clear flaw in the design of this function and I am not sure of what is the correct strategy in the general case. 
What bugs me is that the results differ according to image encoding and not image content, because it triggers different processing paths.
If we can show that keeping the image RGB at this stage is better, then this function should be corrected. If depends on the type of image, maybe there should be a command line option (--subtitles) ?

After some thoughts, as it concerns only color palette images, it will probably only concern subtitles and screen dumps (who would scan a page or take a picture in a gif-like format ?). So maybe "my case" is the "general case"

Off to bed...
 I converted your nak.png to rgb and gray images using GIMP.
[242-bad-gimp-rgb](https://cloud.githubusercontent.com/assets/13571208/13510330/b88b6180-e198-11e5-8840-d92a48c91cfa.png)
[242-bad-gimp-gray](https://cloud.githubusercontent.com/assets/13571208/13510357/d83809a2-e198-11e5-934c-7f273a4bb35d.png)

```
tesseract -l fra bad-gimp-rgb.png stdout
La mort a eu lieu
il y a 8 à 12 heures.

tesseract -l fra 242-bad-gimp-gray.png stdout
La mort a eu lieu
il y a 8 à 12 heures.

```
 Oh, I missed this part: 
`pngtopnm nak.png | tesseract -l fra stdin stdout`
 ```
tesseract -l fra 242-bad.png stdout -psm 11
La mort a eu lieu

il y a 8 à 12 heures.

```

https://github.com/tesseract-ocr/tesseract/blob/master/api/tesseractmain.cpp#L97
 Well, I did some digging in the image path before character recognition.

I think it all comes from the dark-blue background.

If the image is converted to gray-levels before thresholding (16 color palette image), the threshold is determined via gray-histogram study, and is set to 98
If the image is thresholded as a RGB picture, there are 3 histograms, giving 3 thresholds: 64(R), 64(G) and 132(B)
When the image is binarized (?), if it is an RGB picture, the pixels R,G,B values are tested against these thresholds, in that order. As soon as one of the 3 value is above its threshold, the pixel is considered part of a character.
In this pictures, the characters are outlined in dark gray (31,31,31) the light gray (64,64,64).
The dark-gray is always below thresholds, so it is considered background.
The light-gray is equal to the red threshold, so it is considered part of the character when red is tested in first -> thick letters in RGB mode
But it is below the gray-level threshold (98), so it is considered part of the background -> thin letters in gray-mode.
 I'm not sure if it is a good idea to set a priority in the RGB thresholds so that red is tested before green, and then only blue. This priority seems only chosen by the classical RGB color order:
(from ccmain/thresholder.cpp)

```
      bool white_result = true;
      for (int ch = 0; ch < num_channels; ++ch) {
        int pixel = GET_DATA_BYTE(const_cast<void*>(
                                  reinterpret_cast<const void *>(linedata)),
                                  (x + rect_left_) * num_channels + ch);
        if (hi_values[ch] >= 0 &&
            (pixel > thresholds[ch]) == (hi_values[ch] == 0)) {
          white_result = false;
          break;
        }
      }
```

 In fact the algorithm even calculates the alpha channel histogram, and if the result was not null, it would test the alpha value of the pixel against this threshold.
I think that this is just a waste of time.

Anyway, as for now, tesseract seems to do arbitrary choices in the process of color-images binarization:
- if the image has a small palette it is first converted to gray
- this gray conversion seems to use a more or less random formula: gray = 0.25 R + 0.5 G + 0.25 B
- the first plane which claims to have detected a character wins the test (there could be a vote between the 3)
- alpha gets treated in this process. The only sensible way to treat alpha would be IMHO to treat the pixel as background if alpha is small, so it would have to be tested first, and if small, prevent testing of other planes.

Maybe the sensible advice to users experiencing bad results with color images should be to do the gray conversion by themselves. There are many possible choices in this process: GIMP has tree options in the 'desaturate' dialog; my old scanner was just using the green channel when scanning in grey (as do many copiers). Maybe one of CMYK layers could be good in some cases...

In the case of these subtitles where, who knows why, ProjectX added a dark-blue background, selecting the red or green plane gives the best contrast between text and background, and leads to good results (which happens by chance on 'ok.png', would'nt work if ProjectX had chosen red background)
 Nice analysis!

https://github.com/tesseract-ocr/tesseract/blob/master/README.md
The README includes this sentence:

> You should note that in many cases, in order to get better OCR results, you'll need to [improve the quality](https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality) of the image you are giving Tesseract.

So maybe the ImproveQuality wiki page is the right place to add your advice. You can edit the wiki yourself. The explanation should be as clear and short as possible.
 It is in fact in leptonica.
In ccmain/thresholder.cpp, Tesseract calls leptonica's pixConvertTo8 when the input image has less than 8 bpp (or a colormap with few colors, typically 16):

```
  if (depth > 1 && depth < 8) {
    pix_ = pixConvertTo8(src, false);
```

For indexed-color images, pixConvertTo8 calls pixRemoveColormap (see [https://tpgit.github.io/Leptonica/pixconv_8c_source.html] ) with `type = REMOVE_CMAP_TO_GRAYSCALE`
In that case, the colormap is converted to gray with this code:

```
for (i = 0; i < pixcmapGetCount(cmap); i++) {
      graymap[i] = (rmap[i] + 2 * gmap[i] + bmap[i]) / 4;
}
```

So it is leptonica which uses this quick formula, although speed should not really be a concern here as it is applied just to the colormap, not the full image.

Sorry, I don't have much time this week to follow this thread.
Thanks for the interest.
 @jbreiden: will you make a PR or should I commit patch from above?
 patch committed with c1c1e42  The right place for asking questions is:
https://groups.google.com/d/forum/tesseract-ocr
 Tesseract does not work without leptonica.
  The right place for asking questions is:
https://groups.google.com/d/forum/tesseract-ocr
  Which program are you using to view the PDF?
 It does not look reversed wtth Chrome PDF viewer, just not very accurate...
 @roozgar

It seems that Ray is planning to release soon a new version of Tesseract, that will include a new OCR engine based on LSTM.

With LSTM, OCR for printed Arabic (not real handwrite) can reach 95% character accuracy.

"Offline Printed Urdu Nastaleeq Script Recognition
with Bidirectional LSTM Networks"
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.447.4577&rep=rep1&type=pdf
 > I checked google drive ocr for Arabic and i see it have 100 results for same image..

Neither you or I know what programs they are using to do OCR there...
 @tbadran

> But please note that words are not reversed while viewing the PDF because it contains the original image with text layer.
> I mean when you copy text layer then paste it to any text editor it will be reversed, so now can't search for the text inside the PDF because it is stored revered inside the text layer!

Yes, I know...

Here is a copy of the invisible text layer (copied & pasted):

مداها ينم همهما
اللغة العريية
لغة جهد مه
مسنره هي انحاء العالم

Using Chromium (Google browser) PDF viewer under Linux.

Your original jpg image:
![test_ara](https://cloud.githubusercontent.com/assets/17473681/13320324/bc160e22-dbd0-11e5-8090-6f3728fcc06d.jpg)
 @roozgar 

You can try training Tesseract using the regular engine. Use the the wiki and see #169. I really don't know how good the result will be for Arabic.

Like jbreiden said, the timeline could change...
 Tom, 

Look at the original jpg.
Lines 2 and 4 in Google Chrome look quite similar to lines 2 and 3 in the original jpg. First word in line 3 in the original jpg became first word in line 3 in Google Chrome.
Clearly, that's the 'good' output...
 Again, in Google Chromium.
If I mark the first two lines in the PDF + first word in line 3, 
copy the (invisible) text, paste it to a text file, 
mark the second to last word in line 3 in the PDF, 
copy the (invisible) text, paste it to the text file, I get:

مداها ينم همهما
اللغة العريية
لغة مسنره هي انحاء العالم
 @jbreiden 
I didn't understand you. In one comment you talk about Hebrew and in another one you only referring Arabic. Does Hebrew displayed correctly with Adobe Reader?
 Please make sure that any change you do is not causing any regression with Chrome PDF viewer and OS X Preview. Thanks for your work!
 Maybe explicitly using unicode bidi control characters can help ?
 @jbreiden, any progress? Which way you chose?
Personally, I care about our Hebrew support.
 I am taking a look at this today. With current code, copy-paste works from Chrome, fails from Adobe Reader. Destination is gEdit. All tests are on Linux. I see no difference in Adobe Reader if I insert U+2067 RIGHT-TO-LEFT ISOLATE (RLI) at the beginning of each word, and U+2069 POP DIRECTIONAL ISOLATE (PDI) at the end of each word. It's possible that my copy of Adobe Reader is too old to understand these control characters. Or that I am using them wrong. Too early to tell.

![a](https://cloud.githubusercontent.com/assets/4961958/16635989/323c3664-438a-11e6-9d2c-5ec2d86ce02a.png)

![b](https://cloud.githubusercontent.com/assets/4961958/16636088/c6882ff8-438a-11e6-8a2a-da54d26d696e.png)

![c](https://cloud.githubusercontent.com/assets/4961958/16636079/b413a974-438a-11e6-8a25-e6dca2879ab4.png)
 The PDF 1.7 specification suggests using a left-to-right transformation matrix (Tm) while giving each character a negative width. A very crude experiment along these lines give good results with
 Adobe Reader. But messes up cosmetic highlighting in Chrome and copy-paste is wrong with Evince. Please note that font metrics are inconsistent in this experiment.

```
In writing systems that are read from right to left (such as Arabic or Hebrew), 
one might expect that the glyphs in a font would have their origins at the lower right
and their widths (rightward horizontal displacements) specified as negative. 
[ .. then continues into a horrendous discussion of writing everything backwards ... ]

```

``` c++
--- tesseract/api/pdfrenderer.cpp   2016-07-06 13:19:57.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp   2016-07-06 15:35:12.000000000 -0700
@@ -246,6 +246,7 @@
 void AffineMatrix(int writing_direction,
                   int line_x1, int line_y1, int line_x2, int line_y2,
                   double *a, double *b, double *c, double *d) {
+  writing_direction = WRITING_DIRECTION_LEFT_TO_RIGHT;
   double theta = atan2(static_cast<double>(line_y1 - line_y2),
                        static_cast<double>(line_x2 - line_x1));
   *a = cos(theta);
@@ -527,7 +528,7 @@
                "endobj\n",
                5L,         // CIDToGIDMap
                7L,         // Font descriptor
-               1000 / kCharWidth);
+               - 1000 / kCharWidth);
   if (n >= sizeof(buf)) return false;
   AppendPDFObject(buf);
```

Chrome is unhappy
![f](https://cloud.githubusercontent.com/assets/4961958/16636993/8c713a02-4390-11e6-8e44-a89338f24df6.png)

[heb.pdf](https://github.com/tesseract-ocr/tesseract/files/351045/heb.pdf)
 @jbreiden
The PDF 1.7 spec refer to:

> Unicode Standard Annex #9, The Bidirectional Algorithm, Version **4.0.0**

http://www.unicode.org/reports/tr9/tr9-11.html

Support for RLI and PDI has been added in Unicode **6.3**.
http://www.unicode.org/reports/tr9/tr9-29.html
 I tried the other control characters U+202b RIGHT-TO-LEFT EMBEDDING and U+202e RIGHT-TO-LEFT OVERRIDE. Even when sprinkled all over the place, neither had any effect with Adobe Reader 9. We still get incorrect copy-paste.

``` c++
--- tesseract/api/pdfrenderer.cpp   2016-07-06 13:19:57.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp   2016-07-07 10:55:41.000000000 -0700
@@ -410,6 +410,9 @@
     bool last_word_in_block = res_it->IsAtFinalElement(RIL_BLOCK, RIL_WORD);
     STRING pdf_word("");
     int pdf_word_len = 0;
+    pdf_word += "<202E>";
+    pdf_word_len++;
     do {
       const char *grapheme = res_it->GetUTF8Text(RIL_SYMBOL);
       if (grapheme && grapheme[0] != '\0') {

```

[heb.pdf](https://github.com/tesseract-ocr/tesseract/files/352783/heb.pdf)
 Filed feature request with Adobe to recognize -1 0 0 1 X Y Tm. No idea if they will consider it.
 Hi @Shreeshrii! 

Let's see...

 #169
This is not Arabic specific issue, but an RTL issue. The reported issue was solved.

#212
A question, not an issue.

#238
PDF issue related to RTL. Not Arabic specific issue.

#294
'Moved' to [tesseract-ocr/langdata issues reports](https://github.com/tesseract-ocr/langdata/issues/26).

#302
Seems to be solved.

#325
Original issue was solved.

#361
A broad complaint about bad RTL support.

#410
Not Arabic specific. Can't be solved.

As said before, once the new LSTM code will finally land in Tesseract's public Github repo, the OCR accuracy of Arabic and Persian will be [dramatically improved](https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/7Building%20a%20Multi-Lingual%20OCR%20Engine.pdf). Cube's code will be removed, so any issue with it will be irrelevant.

My conclusion: #238 is the only one in the list we should monitor. 

The big question left is when we will see Tesseract 4.0 code. Unfortunately, Ray does not yet share any planned date with the Tesseract community :(
 Ray shared that he would like to have public alpha version by the end of September.
 @stweil,

> we'll give it a try...

'We'? The @UB-Mannheim team I guess... :)
 I'm currently in discussion with some Adobe folks about this topic.
 Okay, this bug has been open forever. As mentioned before, most PDF files deal with right-to-left (RTL) languages like Hebrew and Arabic by laying out the characters from left-to-right (LTR) but doing it backwards. This offends my programming sensibilities on many levels, and I've resisted this approach. But maybe it is time to swallow pride and wallow in the mud.  Here's a few examples from the test suite. How  is compatibility for search and copy-paste?

Arabic
[ara.pdf](https://github.com/tesseract-ocr/tesseract/files/947818/ara.pdf)

Single word Hebrew
[simplest.pdf](https://github.com/tesseract-ocr/tesseract/files/947819/simplest.pdf)

Hebrew + English
[heb_mivne.pdf](https://github.com/tesseract-ocr/tesseract/files/947854/heb_mivne.pdf)

Hebrew + English, tilted
[heb-tilt.pdf](https://github.com/tesseract-ocr/tesseract/files/947848/heb-tilt.pdf)

English (should be no change from what we do now)
[2.pdf](https://github.com/tesseract-ocr/tesseract/files/947838/2.pdf)

```c++
--- tesseract/api/pdfrenderer.cpp	2017-03-31 14:35:03.000000000 -0700
+++ tesseract/api/pdfrenderer.cpp	2017-04-21 10:16:23.000000000 -0700
@@ -225,14 +225,10 @@
 // left-to-right no matter what the reading order is. We need the
 // word baseline in reading order, so we do that conversion here. Returns
 // the word's baseline origin and length.
-void GetWordBaseline(int writing_direction, int ppi, int height,
+void GetWordBaseline(int ppi, int height,
                      int word_x1, int word_y1, int word_x2, int word_y2,
                      int line_x1, int line_y1, int line_x2, int line_y2,
                      double *x0, double *y0, double *length) {
-  if (writing_direction == WRITING_DIRECTION_RIGHT_TO_LEFT) {
-    Swap(&word_x1, &word_x2);
-    Swap(&word_y1, &word_y2);
-  }
   double word_length;
   double x, y;
   {
@@ -260,15 +256,12 @@
 }
 
 // Compute coefficients for an affine matrix describing the rotation
-// of the text. If the text is right-to-left such as Arabic or Hebrew,
-// we reflect over the Y-axis. This matrix will set the coordinate
+// of the text. This matrix will set the coordinate
 // system for placing text in the PDF file.
 //
-//                           RTL
-// [ x' ] = [ a b ][ x ] = [-1 0 ] [ cos sin ][ x ]
-// [ y' ]   [ c d ][ y ]   [ 0 1 ] [-sin cos ][ y ]
-void AffineMatrix(int writing_direction,
-                  int line_x1, int line_y1, int line_x2, int line_y2,
+// [ x' ] = [ a b ][ x ] = [ cos sin ][ x ]
+// [ y' ]   [ c d ][ y ]   [-sin cos ][ y ]
+void AffineMatrix(int line_x1, int line_y1, int line_x2, int line_y2,
                   double *a, double *b, double *c, double *d) {
   double theta = atan2(static_cast<double>(line_y1 - line_y2),
                        static_cast<double>(line_x2 - line_x1));
@@ -276,17 +269,6 @@
   *b = sin(theta);
   *c = -sin(theta);
   *d = cos(theta);
-  switch(writing_direction) {
-    case WRITING_DIRECTION_RIGHT_TO_LEFT:
-      *a = -*a;
-      *b = -*b;
-      break;
-    case WRITING_DIRECTION_TOP_TO_BOTTOM:
-      // TODO(jbreiden) Consider using the vertical PDF writing mode.
-      break;
-    default:
-      break;
-  }
 }
 
 // There are some really awkward PDF viewers in the wild, such as
@@ -407,15 +389,14 @@
     {
       int word_x1, word_y1, word_x2, word_y2;
       res_it->Baseline(RIL_WORD, &word_x1, &word_y1, &word_x2, &word_y2);
-      GetWordBaseline(writing_direction, ppi, height,
+      GetWordBaseline(ppi, height,
                       word_x1, word_y1, word_x2, word_y2,
                       line_x1, line_y1, line_x2, line_y2,
                       &x, &y, &word_length);
     }
 
     if (writing_direction != old_writing_direction || new_block) {
-      AffineMatrix(writing_direction,
-                   line_x1, line_y1, line_x2, line_y2, &a, &b, &c, &d);
+      AffineMatrix(line_x1, line_y1, line_x2, line_y2, &a, &b, &c, &d);
       pdf_str.add_str_double(" ", prec(a));  // . This affine matrix
       pdf_str.add_str_double(" ", prec(b));  // . sets the coordinate
       pdf_str.add_str_double(" ", prec(c));  // . system for all
@@ -459,23 +440,34 @@
     bool last_word_in_block = res_it->IsAtFinalElement(RIL_BLOCK, RIL_WORD);
     STRING pdf_word("");
     int pdf_word_len = 0;
+    GenericVector<int> unicodes;
+
+    // Gather up unicode codepoints for the word
     do {
       const char *grapheme = res_it->GetUTF8Text(RIL_SYMBOL);
       if (grapheme && grapheme[0] != '\0') {
-        GenericVector<int> unicodes;
         UNICHAR::UTF8ToUnicode(grapheme, &unicodes);
-        char utf16[kMaxBytesPerCodepoint];
-        for (int i = 0; i < unicodes.length(); i++) {
-          int code = unicodes[i];
-          if (CodepointToUtf16be(code, utf16)) {
-            pdf_word += utf16;
-            pdf_word_len++;
-          }
-        }
       }
       delete []grapheme;
       res_it->Next(RIL_SYMBOL);
     } while (!res_it->Empty(RIL_BLOCK) && !res_it->IsAtBeginningOf(RIL_WORD));
+
+
+    // Use primitive "write it backwards" approach for RTL languages
+    if (writing_direction == WRITING_DIRECTION_RIGHT_TO_LEFT) {
+      unicodes.reverse();
+    }
+
+    // Write out the word the way PDF likes it
+    char utf16[kMaxBytesPerCodepoint];
+    for (int i = 0; i < unicodes.length(); i++) {
+      int codepoint = unicodes[i];
+      if (CodepointToUtf16be(codepoint, utf16)) {
+        pdf_word += utf16;
+        pdf_word_len++;
+      }
+    }
+
     if (word_length > 0 && pdf_word_len > 0 && fontsize > 0) {
       double h_stretch =
           kCharWidth * prec(100.0 * word_length / (fontsize * pdf_word_len));

``` So far, just Pdfium/Chrome/Linux   AdobeReader9/Linux  Preivew/MacOSX.  I've asked a few different groups of people to take a look. @christophered Can you please take a look at ara.pdf above and tell me if it works well for you? @christophered Thanks, that's a very helpful report. I expect it to be equal to or better than what Tesseact does now for producing PDF of Arabic.  Note that my change is just about PDF generation and does not touch the recognization process in any way. It is just stock Tesseract 4.0  support for Arabic.

What Tesseract does right now:
[control.pdf](https://github.com/tesseract-ocr/tesseract/files/953433/control.pdf) 


 Thank you for the reports, and sorry for any and all confusion. I'm going to post what Tesseract does right now (CONTROL) and also repost the proposed modification (EXPERIMENT). I am especially interested in regressions, anywhere EXPERIMENT does worse than CONTROL.

CONTROL
[2.pdf](https://github.com/tesseract-ocr/tesseract/files/956615/2.pdf)
[ara.pdf](https://github.com/tesseract-ocr/tesseract/files/956613/ara.pdf)
[heb_mivne.pdf](https://github.com/tesseract-ocr/tesseract/files/956614/heb_mivne.pdf)
[heb-tilt.pdf](https://github.com/tesseract-ocr/tesseract/files/956612/heb-tilt.pdf)
[simplest.pdf](https://github.com/tesseract-ocr/tesseract/files/956616/simplest.pdf)


EXPERIMENT  (repost)
[2.pdf](https://github.com/tesseract-ocr/tesseract/files/956604/2.pdf)
[ara.pdf](https://github.com/tesseract-ocr/tesseract/files/956605/ara.pdf)
[heb_mivne.pdf](https://github.com/tesseract-ocr/tesseract/files/956606/heb_mivne.pdf)
[heb-tilt.pdf](https://github.com/tesseract-ocr/tesseract/files/956608/heb-tilt.pdf)
[simplest.pdf](https://github.com/tesseract-ocr/tesseract/files/956607/simplest.pdf)
 Looking at this again. Slowly losing the remainder of my sanity.  Please use tesseract forum for asking questions/support - see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice).
  Please use tesseract forum for asking support - see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice).
  Seems like yet another OpenCL bug report...

Try this:
`TESSERACT_OPENCL_DEVICE=1 tesseract 11002612_2_0183.jpg 11002612_2_0183 -l ara+fra`
 I suggest changing the title so it will contain the word "Arabic" or "ara".

There were several reports in the past about problems when using Arabic+other lang.
Here is a commit that claims to fix them: 2f197cd6

> Fixed issues 899/1220/1246 (mixed eng+ara)

In general, Arabic uses a special engine called 'Cube', most other languages use another engine.
Cube code is considered obsolete. There is a plan to drop it and replace it with another engine (based on LSTM). It might happen this year.
 Hi @anupamaray !

Please read this:
https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md

Try asking your question in the [users mailing-list](https://groups.google.com/d/forum/tesseract-ocr)
 This is still an issue: it crashes with `--oem 0` or `--oem 2`:

    tesseract /tmp/11002612_2_0183.jpg /tmp/11002612_2_0183 -l ara+fra --oem 0
    Found AVX
    Found SSE
    [DS] Profile read from file (tesseract_opencl_profile_devices.dat).
    [DS] Device[1] 1:Intel(R) HD Graphics IvyBridge M GT2 score is 1.229392
    [DS] Device[2] 0:(null) score is 1.146125
    [DS] Selected Device[2]: "(null)" (Native)
    tessdata_manager.SeekToStart(TESSDATA_INTTEMP):Error:Assert failed:in file classify/adaptmatch.cpp, line 537
    Speicherzugriffsfehler

The crash is obviously unrelated to OpenCL, as it crashed here without using OpenCL. It was also sufficient to specify `-l ara` in my test. Last time we talked Ray said he was leaning toward deletion of the non-LSTM
recognizer.
 ```
C:\Users\vvkum\test>tesseract sepia.jpg sepia --psm 6 -l fra+ara
Tesseract Open Source OCR Engine v3.05.01 with Leptonica
Warning. Invalid resolution 0 dpi. Using 70 instead.
no best words!!

C:\Users\vvkum\test>tesseract sepia.jpg sepia --psm 6 --oem 0 -l fra+ara
tessdata_manager.SeekToStart(TESSDATA_INTTEMP):Error:Assert failed:in file ../../../../classify/adaptmatch.cpp, line 537

C:\Users\vvkum\test>tesseract sepia.jpg sepia --psm 6 --oem 1 -l fra+ara
Tesseract Open Source OCR Engine v3.05.01 with Leptonica
Warning. Invalid resolution 0 dpi. Using 70 instead.
```

Problem still there in 3.05.01

Arabic can be used ONLY in --oem 1 mode (cube in 3.05). 

Combined language mode tries to apply same --oem to both languages. So, if using Arabic as one of the languages, need to use --oem 1.

However, what would be the result if the second language does have --oem 1 option.  First of all: Please use tesseract forum for asking support - see [FAQ](https://github.com/tesseract-
ocr/tesseract/wiki/FAQ#rules-and-advice)
Next PIX is leptonica structure - you can get it with leptonica.pixRead(filename)
  https://groups.google.com/forum/?hl=en#!topic/tesseract-dev/LMo_igM4z90
 Getting back to this now that I (might?) have clearance to make direct libtiff calls. 
First, I see that Ray did not merge my patch above into the github repo. We need to 
get that done before I fix the speed problem with multipage TIFF
 Proof of concept for a future Leptonica interface that would get us down to linear seeks. This evolved from the conversation in bug #367 

``` c
#include <stdio.h>
#include <tiffio.h>

const char *testfile = "test.tiff";

size_t PrimeThePump() {
  TIFF *tiff = TIFFOpen(testfile, "r");
  TIFFSetDirectory(tiff, 0);
  size_t offset = TIFFCurrentDirOffset(tiff);
  TIFFClose(tiff);
  return offset;
}

size_t ThankYouSirMayIHaveAnother(size_t offset) {
  TIFF *tiff = TIFFOpen(testfile, "r");
  TIFFSetSubDirectory(tiff, offset);
  TIFFReadDirectory(tiff);
  offset = TIFFCurrentDirOffset(tiff);
  TIFFClose(tiff);
  return offset;
}

int main(void) {
  size_t offset = PrimeThePump();
  while (offset = ThankYouSirMayIHaveAnother(offset)) {
    printf("offset=%lu\n", offset);
  }
}
```
 This patch reduces  multipage TIFF seeks from O(n^3) to O(n), but requires the not-yet-released Leptonica 1.74. The patch disables the OpenCL accelerated TIFF codec. I'm confident that I could make OpenCL path work with some effort, but it is hard for me to test and I don't know how active and important Tesseract + OpenCL is these days.

``` diff
--- tesseract/api/baseapi.cpp   2016-05-24 15:32:21.000000000 -0700
+++ tesseract/api/baseapi.cpp   2016-09-13 09:21:41.000000000 -0700
@@ -1025,26 +1025,14 @@
                                             int tessedit_page_number) {
 #ifndef ANDROID_BUILD
   Pix *pix = NULL;
-#ifdef USE_OPENCL
-  OpenclDevice od;
-#endif
   int page = (tessedit_page_number >= 0) ? tessedit_page_number : 0;
+  size_t offset = 0;
   for (; ; ++page) {
     if (tessedit_page_number >= 0)
       page = tessedit_page_number;
-#ifdef USE_OPENCL
-    if ( od.selectedDeviceIsOpenCL() ) {
       pix = (data) ?
-          od.pixReadMemTiffCl(data, size, page) :
-          od.pixReadTiffCl(filename, page);
-    } else {
-#endif
-      pix = (data) ?
-          pixReadMemTiff(data, size, page) :
-          pixReadTiff(filename, page);
-#ifdef USE_OPENCL
-    }
-#endif
+          pixReadMemFromMultipageTiff(data, size, &offset) :
+          pixReadFromMultipageTiff(filename, &offset);
     if (pix == NULL) break;
     tprintf("Page %d\n", page + 1);
     char page_str[kMaxIntSize];
@@ -1055,6 +1043,7 @@
     pixDestroy(&pix);
     if (!r) return false;
     if (tessedit_page_number >= 0) break;
+    if (!offset) break;
   }
   return true;
 #else
--- tesseract/classify/mastertrainer.cpp    2016-05-18 14:18:32.000000000 -0700
+++ tesseract/classify/mastertrainer.cpp    2016-09-13 09:30:11.000000000 -0700
@@ -214,10 +214,14 @@
 // Must be called after ReadTrainingSamples, as the current number of images
 // is used as an offset for page numbers in the samples.
 void MasterTrainer::LoadPageImages(const char* filename) {
+  size_t offset = 0;
   int page;
   Pix* pix;
-  for (page = 0; (pix = pixReadTiff(filename, page)) != NULL; ++page) {
+  for (page = 0; ; page++) {
+    pix = pixReadFromMultipageTiff(filename, &offset);
+    if (!pix) break;
     page_images_.push_back(pix);
+    if (!offset) break;
   }
   tprintf("Loaded %d page images from %s\n", page, filename);
 }
```
 Then lets wait for new leptonica release. IMO it would be fine to fix OpenCL too - I got promises that somebody should have a look at opencl issues...
 patch from 2016-03-29 committed as 54fafc4e2e9e2941b643f6cef67a7ec7e0b8bb49 
 @DanBloomberg : Is there any plan for leptonica 1.74 release? I use git version of leptonica. I just want to know when we will solve this issue. Tesseract master is now broken with master leptonica because of changed function(s) in lept's `pageseg.c`.
Because of it I switched cppan windows CI build from master to 1.73. I think no. It's better to fix tesseract code.

---

@zdenop If you'd like to fix tess code, you could use CPPAN. To change leptonica dependency from `1.73` to `master`, fix it on the line https://github.com/tesseract-ocr/tesseract/blob/master/cppan.yml#L137
After this when you compile tesseract with cmake, the build will fail. Future tess. release could use lept 1.74. It will be released soon.
We just need administrative decision: e.g. "Let's fixate on lept1.74 for the time being". Yes, this seems to be ABI breakage. Both tesseract and leptonica do not use semver (X.Y.Z), but use (X.Y), so personally I'm confused. With semver ABI breakage only allowed when increasing X number. So, e.g. leptonica should be versioned as 2.00 or whatever (2.0.0?). It's ok now. master tess + lept work fine. @DanBloomberg: When leptonica 1.74 will be released? Also added to cppan.
https://cppan.org/pvt.cppan.demo.danbloomberg.leptonica/versions

We could stick now to 1.74.0 to prevent possible abi breakages.  See what I wrote about a new ocr engine in #212.
 1. Langdata/tessdata issues should be reported at relevant part of project (https://github.com/tesseract-ocr/tessdata/issues or https://github.com/tesseract-ocr/langdata/issues)
2. Issue tracker should be used for reporting issues and not for asking questions.
 @roozgar

Proposal for Tutorial on Tesseract at DAS2016
http://www.primaresearch.org/das2016/assets/DAS2016_Tutorial_Tesseract.pdf
  There is a config parameter for that purpose: `tessedit_char_whitelist`.

Next time, please use the forum to ask questions about this software.
https://groups.google.com/d/forum/tesseract-ocr
  Tom,
I suggest changing `ltr` to `para_is_ltr`, to make the code more clear.
  > Please make sure the TESSDATA_PREFIX environment variable is set to the **parent directory** of your "tessdata" directory.

So, if your `tessdata` dir is in the `/usr/share/tesseract-ocr` dir, `TESSDATA_PREFIX` should be set to `/usr/share/tesseract-ocr`.
 Also, as @stweil said elsewhere: 

> Setting TESSDATA_PREFIX is not needed as long as your tessdata directory is at the right place ($PREFIX/share/tessdata).
 Also read this:

UNIX export command
http://stackoverflow.com/a/7328289
 @teo1978: well it could be the same story, because config files has to be with expected directory structure in TESSDATA_PREFIX...
 ... so digits file should be in $TESSDATA_PREFIX/tessdata/configs
  Did someone test that there is no regression in pdf output with Adobe Acrobat
and Chromium?
 Okay, but I think you should also test this with an image that has more than two words...
  See #212.
 1. Langdata/tessdata  issues should be reported at relevant part of project (https://github.com/tesseract-ocr/tessdata/issues or https://github.com/tesseract-ocr/langdata/issues)
2. Issue tracker should be used for reporting issues and not for asking questions.
  Try to follow wiki [1] 
Note: This wiki expects you to be familiar with compiling software on your operation system.[1]
https://github.com/tesseract-ocr/tesseract/wiki/Compiling
  @teo1978: this is standard autotools error message. You will get it for all libraries. ./configure is tool not a teacher. Name of needed package could be different base on distribution.

If you are not familiar with compiling software on your operation system it is not tesseract problem.
  > Audiveris is developed in Java, and invokes Google Tesseract (C++) for text OCR.

Tesseract is not used for recognizing the musical notations. Sheet music (songs) also include ordinary text. Tesseract OCR is used for that song text (or any other text like headers, page counts, name of composer, ...).  @lisaied

> Do you have a plan to update Arabic Language files in the coming releases.

There is a plan to add a new ocr engine to tesseract this year. There is a good chance that the accuracy of arabic and indic languages recognition will be improved. 
  ~~From #83, @zdenko's quote:~~

> ~~If you run:
> `tesseract OCR.tif ORIGINAL pdf`
> than ORIGINAL.tif is included in ORIGINAL.pdf WITHOUT any modification.~~
 should be readed as: OCR.tif is included in ORIGINAL.pdf WITHOUT any modification. ;-)
And OCR is run on OCR.tif
IMO dhendrix what something else - he want to run OCR on image_b (improved for OCR), but include image_a (original) to pdf...
 Maybe this tool could help.
 https://github.com/tmbdev/hocr-tools/blob/master/hocr-pdf
  Leptonica can be built on windows. But by default it is built without any image libraries (i.e. cannot load/save images). So, no png,gif,tiff,... functions available.
In such mode tesseract will only handle raw image input from api, not from any exe calls.
 Try these binaries compiled by me.
https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1
You have to install VC2015 x86 redist from microsoft.com in order to run them.
Leptonica is built with all libs except for libjp2k.
 > Also I would like to downoad different versions (2.x, 3.x) to see if any of them works.

http://sourceforge.net/projects/tesseract-ocr-alt/files/
 @bantilan: please add it https://github.com/tesseract-ocr/tesseract/wiki/Compiling#windows (every github used can modify it)
 It's a bad guide. Do not add it anywhere.
1. Windows binaries should be made with MSVC, not cygwin. Why do you install Visual Studio and do not use it? You use only cygwin stuff. Which won't run if you distribute it without cygwin.
2. https://github.com/egorpugin/leptonica.git is not correct leptonica repository anymore. Use official https://github.com/DanBloomberg/leptonica/releases 1.73+ release.
 I uploaded my local branch with almost all leptonica's deps sources to https://github.com/egorpugin/leptonica/tree/with_libs. You can find them in /dep. Most of those were taken from opencv distribution. They can be somewhere misconfigured etc. Also that branch is following master branch of leptonica, so probably it's not so stable as release.
When you build this leptonica branch, result library will support many image libraries, therefore tesseract does too when linked with it.
 No, I can't.
That's the single binary that cannot be easily built with MSVC.
 They are from master branch. Probably on this commit ec44221e336dd9
 @egorpugin, can you confirm this? 
 Did not run tests on my binaries, so it's possible.
 @nickbe, one more (unofficial) option for Windows:
~~https://github.com/UB-Mannheim/tesseract~~
https://github.com/UB-Mannheim/tesseract/wiki
 Yeah. For some reason I omitted the 'wiki' part from the link.
 What is this business about certain builds not supporting PDF output? There is nothing special or weird with that section of code. Are they just old builds from before the feature was created, or is there something else going on?
 Is there a simple command to check pdf generation?
I'll try to debug the issue (if any).
 The discussion about the PDF issue in egor's build should be continued in #338
 Hi!

I'd like to share one more way of building tesseract(+leptonica+other_image_libraries) on Windows. Linux probably will work too.
1. Download and install Git, CMake and put them in PATH.
2. Download the latest CPPAN (C++ Archive Network https://cppan.org/) client from https://cppan.org/client/. CPPAN is a source package distribution system. (Like biicode or conan but for sources. Also the idea came from CPAN/CTAN/CRAN.) Add CPPAN client in PATH too. (VS2015 redist is required.)
3. Run

```
git clone https://github.com/tesseract-ocr/tesseract tesseract
cd tesseract
cppan
mkdir build && cd build
cmake .. -DSTATIC=1
```
1. Build a solution in VS tesseract.sln

So, the build should be pretty straightforward and available for everyone with installed Visual Studio. You can try it on any version supported by tesseract.

If you like the way of building tesseract with CPPAN, I could implement such easy way of integration of tesseract into user (your) apps.

Sorry for the noise.
 @egorpugin: thanks! I tried it in VS2010 and I face one problem: 
this version of giflib can not be built by VS2010 (Cannot open include file: 'stdbool.h': No such file or directory). Is it possible to make it optionable (to ignore gif library) or to use other version of library (older 4.? was possible to build with VS2010)?
 During normal workflow this is not possible.

Because of the following:
tesseract is binded to leptonica (some version, e.g. future 1.74);
leptonica is binded to giflib (e.g. v5.1.2).

So you cannot control leptonica's dependencies.
And you must use appropriate and supported compilers of these libraries to compile your project.
Now you'd like to use an older version of giflib. The idea is clear, I understand this, you want to work on older compiler. But using old version of libraries can bring (and it does) potential security breaches.

But now  it's possible to introduce some changes to **demo** giflib itself. I've added some ifdefs to control usage of stdbool.h, so you can check if it works. Just re-run cppan command in tesseract dir and try to build a VS solution.
 Usually it means that you don't have a letsencrypt certificate installed on your system (old ubuntu etc.). I've added an option ('-k' or '--ignore-ssl-checks') to skip strict ssl checks. Please, re-download the client and run 'cppan -k'. For more info see 'cppan -h'. On windows you can upgrade the client with 'cppan --self-upgrade' (only if this option is available, check 'cppan -h').
 FYI I've added simple guide on how to build tesseract on Windows https://github.com/tesseract-ocr/tesseract/wiki/Compiling#master-branch-305-and-later
Also the simple user app example with tesseract (using cmake+cppan) is available here https://github.com/tesseract-ocr/tesseract/wiki/User-App-Example
https://github.com/cppan/tesseract_example
  For asking support please use the tesseract-ocr user forum (https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice)
  You try to run opencv project, that needs leptonica and you are putting issue to tesseract???
 You wrote: "but when i build opencv project" Or???
Leptonica is external dependency - you need to build it by yourself (and maybe adjust paths according you directory structure). You need to use 1.71 or latter version.
There are several tutorials how to build tesseract from source. Just use google...
  http://sourceforge.net/projects/tesseract-ocr-alt/files/
  ![i202-2](https://cloud.githubusercontent.com/assets/13571208/13125654/71d344e4-d5cf-11e5-9f4b-497087b65507.png)

`tesseract i202-2.png i202-2`
=>
[i202-2.txt](https://github.com/tesseract-ocr/tesseract/files/135210/i202-2.txt)
(garbage)

The font in this image is too degraded and probably too different from the trained data. 
 I'm closing this because as I said, there is a good reason why Tesseract failed in this specific case, so I don't think there is a bug here.
  https://github.com/tesseract-ocr/tesseract/wiki/FAQ#actual_tessdata_num_entries_-tessdata_num_entrieserrorassert-failedin-file-ccutiltessdatamanagercpp-line-55_
  @stweil, you have a typo in the above comment. Where can I send a PR?
:-)
  Maybe you can start with some reading: https://github.com/tesseract-ocr/tesseract/wiki/Compiling#windows
  Can you test this with d4e0c645 ?
I want to know if the crash is not triggered by my commits. I did tests on a pc with linux. I don't have a mac. 
 Please try to run:

> tesseract eng.DejaVuSerif.exp0.tif eng.DejaVuSerif.exp0 box.train

Use the attached files
[i196.zip](https://github.com/tesseract-ocr/tesseract/files/100745/i196.zip)
 I think your patch in #195 is the cause of the problem here.

Here is the text file I used to generate the tif-box files:
https://raw.githubusercontent.com/tesseract-ocr/langdata/master/eng/eng.training_text

> text2image --text=eng.training_text --outputbase=eng.DejaVuSerif.exp0 --font='DejaVu Serif' --fonts_dir=/usr/share/fonts/truetype
 @LinusU: is the issue solved?
 Closing due to lack of response from OP.
  First thing - remove your patch.

Now, run this:

> text2image --list_available_fonts --fonts_dir=/Library/Fonts

Do you get a list of fonts? If the answer is 'yes', then proceed.

Use this file:
https://raw.githubusercontent.com/tesseract-ocr/langdata/master/eng/eng.training_text

> text2image --text=eng.training_text --outputbase=eng.MenloRegular.exp0 --font='Menlo Regular' --fonts_dir=/Library/Fonts

If this doesn't work try with other fonts, but only regular ones, not bold/italic/medium.

If this doesn't work either, download and install the DejaVu fonts.
 Also, please provide any error message you get.
 http://ryanfb.github.io/etc/2014/11/19/installing_tesseract_training_tools_on_mac_os_x.html

> Note that fontconfig font locations and caching are a whole other nightmare, and I seem unable to get text2image to respect/use the --fonts_dir argument on OS X. Your best bet seems to be to install things as system/user fonts (e.g. copy into ~/Library/Fonts) and optionally run fc-cache -frv to force a cache update.

A note for @ryanfb:
    `text2image --list_available_fonts`
Running this command on Ubuntu 14.04 does not produce any output, but running this one does:
    `text2image --list_available_fonts  --fonts_dir=`

cc: @behdad
Behdad, maybe you can help here?
 @ryanfb, did you tried running `tesstrain.sh` on mac?
Maybe it has some tricks that make text2image actually work on mac.
 @LinusU ?
 @jbarlow83, anyone with a mac...
Could you help test and debug this issue?
 HI @atuyosi! 

I don't have 'DejaVu Sans Thin' in my ubuntu system. I have 'DejaVu Sans'.

Please copy the output of
`text2image --list_available_fonts --fonts_dir=/Library/Fonts`
to a new file 'fontlist.txt' and attach this file here. I want to find a font we both have.  
 Please try:
`text2image --text=eng.training_text --outputbase=eng.TimesNewRomanBold.exp0 --font='Times New Roman, Bold' --fonts_dir=/Library/Fonts`
 Another test...

[eng.training.txt](https://github.com/tesseract-ocr/tesseract/files/138769/eng.training.txt)

```
text2image --text=eng.training.txt --outputbase=eng.TimesNewRomanBold.exp0 --font='Times New Roman, Bold' --fonts_dir=/Library/Fonts --tlog_level=3
```
 Could someone with a Mac retest this with the latest commit in the repo?
 @atuyosi, thanks for testing. The output files look okay.
The issue seems to be solved.
  [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice): For general questions and support please use the tesseract-ocr user forum
  @diotteo
This info is indeed outdated. I removed it from that page.

Thanks for reporting.
 Maybe instead of deleting the paragraph it's better to edit it;

> For versions prior to 3.02, any language that has different punctuation and numbers is going to be disadvantaged by some of the hard-coded algorithms that assume ASCII punctuation and digits.
 @zdenop, any preference?
 I edited it again;

> For versions 3.00/3.01, any language that has different punctuation and numbers is going to be disadvantaged by some of the hard-coded algorithms that assume ASCII punctuation and digits. [Fixed in 3.02]
  @egorpugin Can you have a look on this?
 What headers should be public?
I thought all headers from api/

Tesseract really has bad directories structure.
Probably it should be reorganized as in many other projects.
include - public includes
src/[subdirs] - dir with sources and private headers
 At least headers that are mentioned in "include_HEADERS" in Makefile.am should be installed/public.
Each directory has its own Makefile.am...
 While using tesseract with TesseractConfig.cmake (without install), we can include several directories.
So, unintentionally we will give an access to all headers from those dirs. Not only to "include_HEADERS".
Users won't know it and could use noinst headers, so later they'll have issues.
 Fixed in 7b94871ba20
  @ryanfb, FYI

If you close a PR and then reopen it later the tests will rerun.
No need to duplicate a PR.
  All tests failed because @egorpugin made changes in his leptonica repo.

~~https://github.com/egorpugin/leptonica
is empty right now.~~

~~The links in the tests should be updated to:
https://github.com/egorpugin/leptonica_old~~
 Keep calm. We're setting an official leptonica repository this time.
 Ok, you can try to recreate this PR. It should pass CI checks. We'll modify scripts lates, but egorpugin/leptonica is available again.
  hocr2pdf's issues should be reported to hocr2pdf's developers.
  Please use [Tesseract users forum](https://groups.google.com/d/forum/tesseract-ocr) for support.
  I will check this next week.
  See #170.
 Chromium's pdf reader output (cut&paste):

[182-chromium.txt](https://github.com/tesseract-ocr/tesseract/files/115576/182-chromium.txt)
 I run Tesseract (latest commit from the repo) with your jpg image. 

> tesseract i182.jpg i182 -l eng txt pdf hocr

Evince's output (cut&paste):

[182-evince.txt](https://github.com/tesseract-ocr/tesseract/files/115584/182-evince.txt)

Evince is based on Poppler.
 Here are the output files...

[i182.pdf](https://github.com/tesseract-ocr/tesseract/files/77832/i182.pdf)
[i182.txt](https://github.com/tesseract-ocr/tesseract/files/77833/i182.txt)
[i182-hocr.zip](https://github.com/tesseract-ocr/tesseract/files/77840/i182-hocr.zip)
 Did you see my 2 last comments?
The latest commit from the repo produces better pdf results than version 3.04. 
 My libpopler version is 0.24.5. Ubuntu 14.04.

> pdftotext i182.pdf i182t.txt

Here is the pdftotext output:
[i182t.txt](https://github.com/tesseract-ocr/tesseract/files/77872/i182t.txt)
 cc: @jbreiden
jbreiden wrote Tesseract's pdf renderer code.
 From https://groups.google.com/forum/?hl=en#!topic/tesseract-dev/XllxjvK5HtU

> Jeff Breidenbach   7/17/15
> PROBLEM #2: PDF
> I was looking at a PDF problem report and noticed that Tesseract PDF output
> is no longer validating. (It fails qpdf --check). As the author of the pdf module,
> I'm biased, but producing corrupt data is a disaster and I think we need to cut
> a new release once it is figured out. Most PDF viewers will recover and silently 
> ignore, but this is no good at all. I wonder what happened.
 Try this to output to stdout:

>    pdftotext i182.pdf -

Jeff mentioned qpdf.
Links:
 http://qpdf.sourceforge.net 
https://github.com/qpdf/qpdf
 @behdad, try this:

> pdftotext linn.pdf -
 > There is also an alternative invisible font here, that contains an advanceWidth. I think it can be swapped in for tessdata/pdf.ttf. It has a side effect of making highlighting look even more bizarre in evince.

It looks terrible :(
 If you search for a phrase in evince, the highlighting looks more normal.
Strange!
 This was a short discussion... :-)
  [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) : If you found a bug - please create an issue: Please make sure you are able to replicate the problem with tesseract executables on Linux or Windows. For other platforms or external/your programs (including tesseract wrappers) please use the tesseract forums.
  ...waiting for review @theraysmith 
 Now we're synced, yes I think that would be a positive change, but it may be something to keep for 4.00 forward, so as to maintain maximum support for old compilers in the last 3.xx version.
(I already put a few experimental uses of nullptr in 4.00 to see if anyone squeals.)
Not being a git expert yet, I assume I can just hit the pull button and 3.xx can still be unaffected right? Yes, just hit button "Merge pull request" if you wan to include it to currect (4.00) code @theraysmith : Is there plan to commit new code that would interfere with this changes?
@stweil : I would wait for the moment ;-) so there is change to fix building of 4.0 (outside of linux). E.g. 1-2 weeks after lstm data files are available. Training is a different kettle of fish to the recognition engine, as the
latter is currently (3.0x) *much* more portable than the training tools.

 I want to see if anyone squeals when they start porting the 4.00
recognition engine to other platforms, although there are likely to be
howls of protests over the other missing ingredients first, like big-endian
support and SIMD extensions.

Although now I mention that, anyone who has a machine old enough to squeal
over C++11 probably doesn't have the horsepower to run 4.00 at a reasonable
speed.

On Thu, Nov 24, 2016 at 10:38 AM, Stefan Weil <notifications@github.com>
wrote:

> I already put a few experimental uses of nullptr in 4.00 to see if anyone
> squeals.
>
> @theraysmith <https://github.com/theraysmith>,
> training/stringrenderer.cpp already uses nullptr for more than two years
> now. AFAIK nobody complained, so that seems to work. Replacing NULL by
> nullptr would be good, but also touches many files, so this could be done
> in the same action as the switch to POSIX data types.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/pull/177#issuecomment-262830314>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056QF3tQJra8WTACr_cvaeVjUG0yP6ks5rBdmWgaJpZM4G6wao>
> .
>



-- 
Ray.
 FYI: This change breaks several tests in the Google repository because the Google int64 is long long and the posix int64_t is just long, and the compiler says they are not compatible.
Of course they shouldn't have used the wrong type to begin with.
I'm hoping that they can be fixed easily enough and it doesn't open a whole can of worms, but I thought you would be interested that it is not as easy as you thought. Yes, the change to POSIX requires some work (not only in your tests), but it will help in the long run – for example as soon as there is a 128 bit architecture with long being int128_t. And it helps a lot as soon as you want to use Tesseract code in other software. There's also `int_least64_t`. http://en.cppreference.com/w/cpp/types/integer  > tesseract issue174.jpeg issue174 -c hocr_font_info=T hocr

This works with the latest commit in the repo.  No segfault.
 @vov4ik829,
~~Ubuntu 14.10 had reached EOL. You should upgrade the distro.~~
  Use tesseract forum for asking help https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  The [Downloads](https://github.com/tesseract-ocr/tesseract/wiki/Downloads) wiki page has a link to  [Former project page](https://code.google.com/p/tesseract-ocr/downloads/list) under 'Old Downloads'.
There, you can find Windows installer for tesseract 3.02. There is no **official** Windows installer for later versions.

You can build on Windows the latest version from source.
The wiki has a link to [Releases](https://github.com/tesseract-ocr/tesseract/releases)

Still, the wiki needs to be more clear on this.
 I hope it's clearer now.
https://github.com/tesseract-ocr/tesseract/wiki/Downloads
  Evince's pdf support is based on Poppler.

https://en.wikipedia.org/wiki/Evince#Supported_document_formats
https://en.wikipedia.org/wiki/Poppler_%28software%29
 > Maybe Firefox also uses parts of the Poppler code. 

No.
https://github.com/mozilla/pdf.js
 Chromium uses PDFium:
https://pdfium.googlesource.com/pdfium/
https://news.ycombinator.com/item?id=7781878
http://blog.foxitsoftware.com/foxit-pdf-technology-chosen-for-google-open-source/
https://groups.google.com/forum/#!forum/pdfium
 Poppler homepage
http://poppler.freedesktop.org/

Maybe someone wants to fill a bug report?

> Use [bugzilla](http://bugs.freedesktop.org/) to report bugs or suggest enhancements. The component is `poppler`.

From the [TODO](http://cgit.freedesktop.org/poppler/poppler/tree/TODO):

> - Investigate better (that is, normal) text selection.
 @stweil, what @jbreiden is saying is that some pdf viewers have bugs that make them present skewed text lines incorrectly.
  Hi @christophered,

How did you generate the tif images for training? Did you use the `text2image` tool?

Did you use `set_unicharset_properties`?
https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#set_unicharset_properties-new-in-303

From https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#dictionary-data-optional

> For right-to-left languages (RTL) use option "-r 1".

You can also try to use `tesstrain.sh`
https://github.com/tesseract-ocr/tesseract/wiki/tesstrain.sh

In general, the right place to ask questions like this is here:
http://groups.google.com/group/tesseract-ocr

See also:
https://groups.google.com/forum/#!topic/tesseract-ocr/HdT8V1nFTtY
 Try using this config file:
 https://github.com/tesseract-ocr/langdata/blob/master/ara/ara.config

Remove this line:

> tessedit_ocr_engine_mode 1
 The official trained data uses the 'Cube' engine. There is no documented way to train 'Cube' with other fonts.
 I suppose they have a program for that task...
 Developers from Google.
 Both gimagereader and vietocr have versions which use tesseract 4.

ShreeDevi
____________________________________________________________
भजन - कीर्तन - आरती @ http://bhajans.ramparivar.com

On Mon, Jun 19, 2017 at 8:07 PM, adinetoiu <notifications@github.com> wrote:

> Do you have a sample project or link that uses tesseract 4?
>
>
> From: chris <notifications@github.com>
> To: tesseract-ocr/tesseract <tesseract@noreply.github.com>
> Cc: adinetoiu <adinetoiu@yahoo.com>; Mention <mention@noreply.github.com>
> Sent: Monday, June 19, 2017 5:14 PM
> Subject: Re: [tesseract-ocr/tesseract] Arabic Language output is reversed
> (#169)
>
> @adinetoiu I suggest that you skip using Tesseract 3.x for Arabic, instead
> use Tesseract 4.
> a binary is also available at http://digi.bib.uni-mannheim.
> de/tesseract/tesseract-ocr-setup-4.00.00dev.exe—
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/169#issuecomment-309459382>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE2_ozNSJAJOADt4YTvs6TLIAMnHaa7Vks5sFoeSgaJpZM4GzPQj>
> .
>
  See [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice) - use tesseract forum for suppoprt.
  VS 2010 build fails with:
....\ccutil\mainblk.cpp(28): fatal error C1083: Cannot open include file: 'libgen.h': No such file or directory
  I see no error(s) reported here. Is this the full output?

Did you run:

> ./autogen.sh
> ./configure

and then:

> make

?
 Here is my output.

last lines:

```
libtool: link: ranlib .libs/libtesseract.a
libtool: link: rm -fr .libs/libtesseract.lax
libtool: link: ( cd ".libs" && rm -f "libtesseract.la" && ln -s "../libtesseract.la" "libtesseract.la" )
g++ -DHAVE_CONFIG_H -I. -I..  -DNO_CUBE_BUILD -O2 -DNDEBUG -DLOCALEDIR=\"/usr/local/share/locale\" -DUSE_STD_NAMESPACE -I../ccutil -I../ccstruct -I../cube -I../viewer -I../textord -I../dict -I../classify -I../ccmain -I../wordrec -I../cutil -I../opencl    -I/usr/local/include/leptonica -pthread -I/usr/include/pango-1.0 -I/usr/include/glib-2.0 -I/usr/lib/x86_64-linux-gnu/glib-2.0/include   -I/usr/include/cairo -I/usr/include/glib-2.0 -I/usr/lib/x86_64-linux-gnu/glib-2.0/include -I/usr/include/pixman-1 -I/usr/include/freetype2 -I/usr/include/libpng12    -g -O2 -std=c++11 -MT tesseract-tesseractmain.o -MD -MP -MF .deps/tesseract-tesseractmain.Tpo -c -o tesseract-tesseractmain.o `test -f 'tesseractmain.cpp' || echo './'`tesseractmain.cpp
mv -f .deps/tesseract-tesseractmain.Tpo .deps/tesseract-tesseractmain.Po
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2 -std=c++11   -o tesseract tesseract-tesseractmain.o libtesseract.la   -lrt -llept -lpthread 
libtool: link: g++ -g -O2 -std=c++11 -o .libs/tesseract tesseract-tesseractmain.o  ./.libs/libtesseract.so -lrt -llept -lpthread
make[2]: Leaving directory `/home/amit/ocr/tesseract/tesseract/api'
Making all in .
make[2]: Entering directory `/home/amit/ocr/tesseract/tesseract'
make[2]: Leaving directory `/home/amit/ocr/tesseract/tesseract'
Making all in tessdata
make[2]: Entering directory `/home/amit/ocr/tesseract/tesseract/tessdata'
Making all in configs
make[3]: Entering directory `/home/amit/ocr/tesseract/tesseract/tessdata/configs'
make[3]: Nothing to be done for `all'.
make[3]: Leaving directory `/home/amit/ocr/tesseract/tesseract/tessdata/configs'
Making all in tessconfigs
make[3]: Entering directory `/home/amit/ocr/tesseract/tesseract/tessdata/tessconfigs'
make[3]: Nothing to be done for `all'.
make[3]: Leaving directory `/home/amit/ocr/tesseract/tesseract/tessdata/tessconfigs'
make[3]: Entering directory `/home/amit/ocr/tesseract/tesseract/tessdata'
make[3]: Nothing to be done for `all-am'.
make[3]: Leaving directory `/home/amit/ocr/tesseract/tesseract/tessdata'
make[2]: Leaving directory `/home/amit/ocr/tesseract/tesseract/tessdata'
Making all in doc
make[2]: Entering directory `/home/amit/ocr/tesseract/tesseract/doc'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/home/amit/ocr/tesseract/tesseract/doc'
make[1]: Leaving directory `/home/amit/ocr/tesseract/tesseract'
```
 Without full log of all steps you did this issues does not have sense - there could be plenty of reasons why build failed.
  Which file use unicharambigs file v2?
 I did not find (in langdata repository) language that use v2 of unicharambigs... So I am not sure if it make sense to write docs for it (=> propagate it) if it is not used (=> not tested). 
 I hear something like end of September 2016, but you never know ;-) It will big update (probably we will drop support of compilers nor support c++11)...
  You missed this typo:

> Repository it huge...
 > ... as I am not a native speaker

Me neither! :-)
  @zdenop said in #101:

> Future is cmake. There is no sense to make and maintain build system for each version of cmake. cmake correctly detect installed compiler and use it. We are just waiting for leptonica transfer to github, there cmake will be presented to.

So, this PR should be closed.  
 Use cmake for any Visual studio builds.
  Please make sure not to use the 72dpi screen resolution, which is always too small for Tesseract. Simply resize the screenshot (if you use screenshots) by +400% and repeat your tesseract runs, and please report here, if that solved your problem.

See https://github.com/tesseract-ocr/tesseract/wiki/FAQ#is-there-a-minimum-text-size-it-wont-read-screen-text

Imagemagick "convert":

```
convert -resize "400%" -type Grayscale +compress infile outfile
```
 > 10 pt screen resolution works perfectly for English.

You are mixing between two concepts. Screen resolution is measured  in dpi/ppi. 'pt' (point) is another concept used to express fonts size.
 https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality

For questions, please use [Tesseract users forum](https://groups.google.com/d/forum/tesseract-ocr).
  > All the above mentioned files are however packed in one docx file :smiley: 

Please attach a zip file instead.
 Tesseract layout analysis gets really confused with the 'bad' image.
The main difference between the two images is the black frame around the page margins. If you'll remove the black borders Tesseract will produce a better output.
  --print-parameters is option that list all tesseract parameters with its default values.  This is useful if you would like to change tesseract behavior.
 in case of 'single options': it was not expected (tested) their usage in combination with other options.
 @amitdo: tesseract executable was(is) simple example how to use tesseract library. AFAIK nobody plan to implement smart handling of options (e.g. you still need to provide some information in specific order)... So the correct result of your option combination is side effect... ;-)  See e.g. https://github.com/tesseract-ocr/tesseract/issues/147#issuecomment-155892373
  see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice):
- For general questions and support please use the tesseract-ocr user forum.
- Read the wiki, search issues (also closed), search in the tesseract forum before you post your issues/question. Maybe it was solved already.
  Here's a tip how to rescale all pages to e.g. DIN A4:
https://github.com/Wikinaut/utils/wiki#scale_all_pages_in_PDF_to_A4
 Resolution (DPI) is extracted from the header of the input image. If missing, then Tesseract has no choice but to make something up. Don't do that! Many tools can be used to inspect and adjust DPI for an input image file. If you want to use ImageMagick, the commands are "identify -verbose" to inspect and "mogrify -density 300x300 -units PixelsPerInch" to set.
  Tesseract executable is simple example how to use tesseract library. There is no intentional make it more complex (just because of parsing arguments and reporting user mistakes)
  see [FAQ](https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice):
- For general questions and support please use the tesseract-ocr user forum.
- Read the wiki, search issues (also closed), search in the tesseract forum before you post your issues/question. Maybe it was solved already.
  I am not sure if this is the right way of dealing with such warning:
- register is [deprecated in C++11](http://en.cppreference.com/w/cpp/language/storage_duration), but it will exists until C++17
- AFAIK it should be possible to build tesseract without support of C++11
  My suggestion is that if solving of warning does not bring any value, it should be ignored...
  i think you need 10 boxes of each commonly occuring letter. and 5 is enough for rarely occuring letters.

also the first number at the unicharset defines what it is and how tesseract handles it. for example all numbers should have 8. and punctuation 10.

. 10 255 0 0 ....
6 8 255 0 0  ...
 https://code.google.com/p/tesseract-ocr/wiki/TrainingTesseract3
 how do you train?
 I have used this from deu_frak model

buildscript.sh:

#!/bin/sh
LANGCODE=deu_frak
for i in _.tif
  do echo $i:
  tesseract $i ${i%.tif} nobatch box.train
done
unicharset_extractor *.box
cat unicharset | sed -e "s/^([æøåäöüâêàèéçß][a-z]_) 0/\1 3/" \
  -e "s/^([ÆØÅÄÖÜÂÊÀÈÉÇ][a-z]_) 0/\1 5/" \
  -e "s/^([«»„”·§—ɔ]) 0/\1 10/" \
  -e "s/^ɔ 3 /ɔ 10 /" \
  -e "s/^½ 0/½ 8/" | sed -e "s/^([æøåäöüâêàèéçßa-zÆØÅÄÖÜÂÊÀÈÉÇA-Z]._) NULL /\1 Latin /" \
  -e "s/^([«»„”·§—ɔ[:punct:][:digit:]]._) NULL /\1 Common /" \
  -e "s/^(&c ._) Common /\1 Latin /" > unicharset.edited
shapeclustering -F font_properties -U unicharset.edited *.tr
mftraining -F font_properties -U unicharset.edited -O $LANGCODE.unicharset *.tr
cntraining *.tr
for i in inttemp normproto pffmtable shapetable
  do mv -f $i $LANGCODE.$i
done
wordlist2dawg number $LANGCODE.number-dawg $LANGCODE.unicharset
wordlist2dawg punc $LANGCODE.punc-dawg $LANGCODE.unicharset
wordlist2dawg word_list $LANGCODE.word-dawg $LANGCODE.unicharset
wordlist2dawg frequency_list $LANGCODE.freq-dawg $LANGCODE.unicharset
combine_tessdata $LANGCODE.
 If you have problems training Tesseract, please use [Tesseract users forum](https://groups.google.com/d/forum/tesseract-ocr) for support.
  479fe9c37055ecf8004f4eeb9bcb8dcc7c1e89c5 still results in these warnings (only warnings are shown):  http://dpaste.com/3DWZ4VP

Created with

```
(git pull)
./autogen.sh
./configure --enable-debug
make &>make.log
agrep -d "^/bin/sh" warning make.log > make-warnings.txt
(uploaded to http://dpaste.com/3DWZ4VP )
```

ProTip: I used agrep: https://github.com/Wikinaut/agrep because the option "-d" allows to define a "record" which is then printed, if if a match within is found (here: "warning").
 @stweil Stefan, hi. I identified some more remaining warnings. See https://github.com/tesseract-ocr/tesseract/issues/138#issue-115242227
 @stweil I informed you, and listed the remaining warnings, because it looked as if you were "hunting" these last (moretheless cosmetic) problems. I really would like to see them disappearing one after the other... :-))
  I don't think it is a good idea to add this to the hocr output. A better alternative is to add new simple text renderer that outputs only info about symbols including 'alt choices' and confidence.
  We do not provide support for tesseract wrappers[1]. Please use tesseract user forum.
[1] https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
 If you are able to replicate problem with tesseract executable on linux I can try to find reason/solution. Otherwise please use tesseract user forum. There are people that use tesseract in java, so they can provide you support.
  In #129 I reported a configure problem, which stemmed from `g++` being unavailable at that moment on my build system. A test code in configure tried to compile `pixCreate` with `g++` but this failed (because g++ was not there).

Main problem: the output text is misleading ("configure: error: leptonica library missing").

In that case, a special test for presence of g++ should be added **and** the text should better say "g++ (gcc-c++ compiler) not available".

@zdenop pls. let me know, if you want me to add further details. I just wanted to point you to that test, which tests a compilation of `pixCreate`, but this fails only because already the invocation of `g++` fails (not present) - not because missing leptonica code. 
 @arvidj yep. do you, by chance, still have the relevant lines from the config.log , this could be helpful.
 @arvidj the best method is to fix the configure, or autogen.sh but I don't know, how, because I have no knowledge how that works.
 @zdenop Danke - thanks - merci - bedankt....
 (not yet tested, but I trust @zdenop )
 It works (do not forget to run ./autogen.sh first). First is check for g++. If it is not found it looks for clang++... I do not know if this is good or bad ;-) 
When I just used "AC_PROG_CXX" (instead of "AC_PROG_CXX(g++ clang++)") it looks for much more compilers, but it did not found clang++ automatically (which is IMO bad)... 
  @amitdo: I prefer to see whole help as default (as today) when I run "tesseract" or "tesseract -- help".
I am fine if there is '--help-psm', but at the moment the whole help is not very long so we do not need to remove part of it...
 @amitdo: No it is not (at least for me). I wrote: I prefer to see whole help as default (as today) when I run "tesseract" or "tesseract -- help".
 @amitdo: I will recheck it - I got different out when I wrote my comment.
 @amitdo: I did clean fetch of pullrequest and I got the output as you presented. Thanks.
  thanks
  Please check master branch or commits f46dfdc29da2bbacf6635752d9f28133ad4e10fe and 5db760215fa05f8dd722f66bc6a3ac438a5cd8d4.
  https://github.com/tesseract-ocr/tesseract#dependencies-and-licenses
  Project move to github, so there is no reason to use google drive
 Yes, when somebody provide installer for windows it will be publish on Github projekt page. Previous installer/downloads are at original location.
  thanks!
  Please use tesseract forum for asking support: https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  Please use tesseract forum for asking support: https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  Does the word 'tor' appear in one of the word lists?

Tesseract will never try to correct a valid word.
 We do not provide support for custom training (you need to demonstrate bug with traineddata from google).
You can use [tesseract user forum](https://groups.google.com/forum/#!forum/tesseract-ocr) for instead. I would suggest to search that forum before asking question.
  @amitdo: parameter file `quiet` should suppress tesseract banner. AFAIK reorganization (99110df75781c6907c84a3d23695a6900b933a97) caused that parameters are loaded/used after printing banner (e.g. `tesseract eurotext.tif eurotext -c debug_file=/dev/null` does not work too). Can you have a look at that?
 @zdenop
Nothing has changed in that respect. This is the same behaviour as before my commit.
 @zdenop
Anyway, I submitted a PR (#255) which makes `quiet` work as expected regarding tesseract's banner.
 Thanks!
@wasamasa: use can use parameter quiet:  `tesseract eurotext.tif eurotext quiet`
  Please use tesseract forum for asking support. Do not forget provide detail (version of tesseract, OS...)
  According Michael Wootton:

> you almost certainly will not be able to see GPU activity when running OpenCL kernels.  The kernels are running for < 50 ms, the time constant for the activity monitor is in seconds

The segfault should be fixed with 5db760215fa05f8dd722f66bc6a3ac438a5cd8d4
  Please provide full information (full log, all commands you run, version of program autogen, tesseract code revision....)
 What are trying to do???
 First of all - why you use old version of tesseract (anyway it worked also on Mac OS)? Why you do not use version from github???
  What is tesseractd.lib? 
Did you try to use/link release version of dependancies instead of debug version?
 Please use tesseract forum for support.
  This file is part of source distribution and that is reason why its point to "the same" online markdown file
  Please use tesseract forum for asking support: https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  That's a libjpeg warning. Is Tesseract not producing output for you, because it is for me (granted, the output isn't great).
 This page: http://www.artima.com/forums/flat.jsp?forum=1&thread=35958 says "before the EOI marker (0xFF 0xD9) one can include custom data other than data related to image" which seems reasonable.
Anyway... as this isn't a Tesseract issue per se, I'm going to close this issue.
  Please use tesseract-developers forum[1] for this suggestions like this.
[1] http://groups.google.com/group/tesseract-dev/
  This issue is not only about creating windows installer. It is also about providing support. And on windows the there is demand for VS based library (it is quiet easy to create mingw based library with msys, but VS based library need to rebuild all dependencies with VS)
 See also PR #162.
 @amitdo: Future is cmake. There is no sense to make and maintain  build system for each version of cmake. cmake correctly detect installed compiler and use it. We are just waiting for leptonica transfer to github, there cmake will be presented to.
 > Future is cmake. There is no sense to make and maintain build system for each version of cmake. cmake correctly detect installed compiler and use it. We are just waiting for leptonica transfer to github, there cmake will be presented to.

https://github.com/DanBloomberg/leptonica
  Did it happen with google traineddata file (or custom training)?
 Try to set  LC_NUMERIC to C during training
 Why do you think this patch is not in current version??? issue 910 you are reffering has problem with official google traineddata file. This was fixed.
AFAIR problem is in custom training.
 @oelleo: unfortunately tesseract requires (at the moment) training data use dot as decimal separator => you need to correct your custom training data.

I think it could be possible without retraining.  Try to unpack your data (`combine_tessdata -u eng.traineddata tmp/eng.`) and fix decimal separator in `eng.normproto` (replace eng with your name of your custom training)
  Please see [FAQ](https://github.com/tesseract-ocr/tesseract/issues/93): If you found a bug - please create an issue: Please make sure you are able to replicate the problem with tesseract executables on Linux or Windows. For other platforms or external/your programs (including tesseract wrappers) please use the tesseract forums.
  Reopen to check using CI.
 Please, rebase this to the latest source.
 Yes, seems to be my bad.
I tried to find a way to check all PRs received before CI was introduced.
Reopen helps only party because Windows CI (appveyor) does not check such PRs (as we can see).
  I am sorry but we do not provide for custom training.
Try to use forum for help/support. But please make sure you use the latest tesseract code and provide more information (tesseract version, locale settings, how you did training, link to data used for training, waa.traineddata....)
  At the moment stdin is supported only for OCR.
PSM_AUTO_ONLY (0) and PSM_OSD_ONLY (2) expects to read data based on filename
 @jflesch: I am currios: Why you call tesseract executable from python instead of using C-API? IMO it should be faster...
 fixed with 7089c7b 
  @olcc Tesseract is a raw OCR engine. Have a look at my project, OCRmyPDF, which provides a nice wrapper around Tesseract and takes care of many details to improve visualization. </plug>
 @olcc: tesseract puts to pdf image that you provided as input (e.g. file you see in pdf is not optimized for OCR as you claims). If you have another experience - please provide example. Otherwise close the "issue".
 What I want to say is that if you run:
    tesseract OCR.tif ORIGINAL pdf
than ORIGINAL.tif is included in ORIGINAL.pdf WITHOUT any modification. If you want to include ORIGINAL.jpg instead of OCR.tif than it is not tesseract issue ;-)
 @amitdo: it is implemented in [pdfrenderer](https://github.com/tesseract-ocr/tesseract/blob/master/api/pdfrenderer.cpp)

This is not real issue (no bug in tessseract), so I close this issue. Please use tesseract user forum for asking question/support.
  Please use tesseract user forum for support[1]. Also do not forget to search forum for you topic before asking for help.
[1] https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  This was the patch. I strongly suspect it made it in.

```diff
--- tesseract/api/baseapi.cpp   2015-06-12 17:25:51.000000000 -0700
+++ tesseract/api/baseapi.cpp   2015-08-18 17:46:15.000000000 -0700
@@ -991,8 +991,7 @@
   }
 
   // Begin producing output
-  const char* kUnknownTitle = "";
-  if (renderer && !renderer->BeginDocument(kUnknownTitle)) {
+  if (renderer && !renderer->BeginDocument(unknown_title_)) {
     return false;
   }
 
@@ -1166,8 +1165,7 @@
   }
 
   // Begin the output
-  const char* kUnknownTitle = "";
-  if (renderer && !renderer->BeginDocument(kUnknownTitle)) {
+  if (renderer && !renderer->BeginDocument(unknown_title_)) {
     pixDestroy(&pix);
     return false;
   }
--- tesseract/api/baseapi.h     2015-03-09 15:43:40.000000000 -0700
+++ tesseract/api/baseapi.h     2015-08-18 17:52:49.000000000 -0700
@@ -879,6 +879,12 @@
                                  int timeout_millisec,
                                  TessResultRenderer* renderer,
                                  int tessedit_page_number);
+  // There's currently no way to pass a document title from the
+  // Tesseract command line, and we have multiple places that choose
+  // to set the title to an empty string. Using a single named
+  // variable will hopefully reduce confusion if the situation changes
+  // in the future.
+  const char *unknown_title_ = "";
 };  // class TessBaseAPI.
 
 /** Escape a char string - remove &<>"' with HTML codes. */
--- tesseract/api/renderer.h    2014-08-12 11:22:47.000000000 -0700
+++ tesseract/api/renderer.h    2015-08-18 17:17:41.000000000 -0700
@@ -77,7 +77,7 @@
     bool EndDocument();
 
     const char* file_extension() const { return file_extension_; }
-    const char* title() const { return title_; }
+    const char* title() const { return title_.c_str(); }
 
     /**
      * Returns the index of the last image given to AddImage
@@ -126,7 +126,7 @@
 
   private:
     const char* file_extension_;  // standard extension for generated output
-    const char* title_;           // title of document being renderered
+    STRING title_;                // title of document being renderered
     int imagenum_;                // index of last image added
 
     FILE* fout_;                  // output file pointer
``` Yes, it went in as part of this commit.

https://github.com/tesseract-ocr/tesseract/commit/c1c1e426b32794d5e84134ee81bf895ff0228fe5  This is not bug ;-). I you install from git repository, you should follow instruction - https://github.com/tesseract-ocr/tesseract/blob/master/INSTALL.GIT#L16
 git pull can change also autoconf input files (e.g. 628de5ba3fe2879cff80343e327e3bae7f6b0634) so you need to run ./autogen.sh && ./configure...
  Please use tesseract forum for asking support: https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
  committed but it seems that this is only occurrence of OpenMP support...
 Did you run ./autogen.sh after git pull? Was it without error? Can you provide config.log?
 Thanks for testing and report. Should be fixed in f331a57b8e4d1b38980c71f576ec5ffe6ba7cf9e.
 Patches are welcommed ;) Anyway tesseract can use OpenMP only in one place....
  In short: there's too much involved, so don't hold your breath.
  Thanks for testing.
  This is fixed in 27b8a5cc89898c3f12dae57a58b15efddee9fe18
  This is not bug - use tesseract forum for asking support[1]
[1] https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
 There's a quote I like from the movie Jerry Maguire. Right before the more famous 'show me the money' line, there's 'help me help you'. It comes up a lot in open source.

Part of why this is not a bug report is for reasons of semantics: you didn't phrase it as a bug report, you phrased it as a support question, and you needed to add a comment to clarify that you think this is an issue. Help us help you: make your issues look like issues.

That said... issues that involve using Tesseract as a library are second class citizens, because producing a library is not the goal here. A number of people have, over the years, made some steps towards making it suitable for use as a library, though, going by some of the issues that have been filed, it's still not fully suitable for everyone's idea of what a library should be. In those cases, patches/pull requests are welcome, but otherwise, expect to see the issue left open until someone else feels like fixing it.

As for your issue, set the variable 'debug_file' to '/dev/null'. If you don't find that completely suitable, feel free to open another issue - or, better, a pull request.
 @matthill: First of all: there is an strong group of people using tesseract in C# and java (via wrappers of tesseract library) - so using user forum you can gain better support.
Next: I do not believe that nobody except you have this problem and would not report it... AFAIR this is first report regarding this output. So I expected it is connected to your code (e.g. how you use library, maybe some parameters etc.). So as I wrote on forum - please provide simple example to demonstrate problem.
 The debug printing routing will select 'nul' on windows, if 'debug_file' is set to '/dev/null'.

I have no idea why it happens in your compile and not in Ubuntu: you haven't said how you compiled it.
  Can you check 0c3c3eaba862254d1982e3fdaf3490adb50340ad ?
 @zdenop I got the same error as @ryanfb. 

The consensus is that AC_CHECK_LIB doesn't play nicely with `-framework`. Instead you can introduce a new macro, or just bypass the check on OS X. The nice thing about frameworks is that they are internally consistent, where you never what kind of crazy library-header mismatches you might find on a *nix box.
https://lists.apple.com/archives/unix-porting/2009/Jan/msg00026.html
 @zdenop I disabled the AC_CHECK_LIB(OpenCL) and replaced it with `have_opencl_lib=true`. That got me to a new error:

```
configure: error: conditional "OPENMP" was never defined.
Usually this means the macro was only invoked conditionally.
```

To repair that I added the required conditional:

```
@@ -168,12 +168,14 @@ if test "$enable_embedded" = "yes"; then
 fi

 # check whether to build OpenMP support
-AC_OPENMP
+#AC_OPENMP
 AS_IF([test "x$OPENMP_CFLAGS" != "x"],
   AM_CONDITIONAL([OPENMP], test "x$OPENMP_CFLAGS" != "x")
   AC_SUBST(AM_CPPFLAGS,"$OPENMP_CXXFLAGS")
   AC_DEFINE([OPENMP], [], [Defined when compiled with OpenMP support])
 )
+AM_CONDITIONAL([OPENMP], test "x$OPENMP_CFLAGS" != "x")
+
```

After `autoreconf -fvi` I had a working ./configure.

...and make failed with several errors of this form:

```
openclwrapper.cpp:3401:33: error: non-constant-expression cannot be narrowed
      from type 'int' to 'size_t' (aka 'unsigned long') in initializer list
      [-Wc++11-narrowing]
    size_t local_work_size[] = {block_size};
                                ^~~~~~~~~~
```
 @jbarlow83: configure issues should be fixed. Which version of compiler you use?
 The line `if test $host_os != Darwin; then` does not work because `$host_os` is `darwin14.5.0`. If this test is bypassed, configure works and make fails with a new error.

```
In file included from openclwrapper.cpp:11:
./openclwrapper.h:2:10: fatal error: 'allheaders.h' file not found
#include "allheaders.h"
```

That error occurs even though I am running `env LIBLEPT_HEADERSDIR=/usr/local/include/leptonica ./configure --enable-opencl`, the location of allheaders.h. Using CC=gcc-5 (homebrew gcc, not clang) also gives the header error.

Here is clang.

```
$ clang --version
Apple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)
Target: x86_64-apple-darwin14.5.0
Thread model: posix
```
 @jbarlow83: Thanks. Unfortunatelly I do not have Mac, so I can just guess what could work :-(.
If you change test to `if test $host_os != darwin; then` will it help? Or to `if test $host_os != darwin*; then`?

regading leptonica: Try to change line if it helps: 
    `LIBLEPT_HEADERSDIR="/usr/local/include /usr/include /opt/local/include/leptonica"`
to
    `LIBLEPT_HEADERSDIR="/usr/local/include /usr/include /opt/local/include/"`   
 #87 is also required. So I got to compile and apparently run successful OpenCL tests in which it selected my graphics card... but the OCR output was garbage. For example, if I had it generate a PDF, it still produced a valid PDF containing the input image, but the OCR text and its positioning was meaningless. 

If I rebuild the same source with OpenCL my test file was processed correctly.

```
$ /opt/tesseract-opencl/bin/tesseract -l eng ~/tests/resources/LinnSequencer.jpg out

Tesseract Open Source OCR Engine v3.05.00dev with Leptonica
[DS] Profile file not available (tesseract_opencl_profile_devices.dat); performing profiling.

[DS] Device: "Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz" (OpenCL) evaluation...
OpenCL error code is -54 at   when clEnqueueNDRangeKernel .
OpenCL error code is -54 at   when clEnqueueNDRangeKernel kernel_HistogramRectAllChannels .
OpenCL error code is -54 at   when clEnqueueNDRangeKernel kernel_HistogramRectAllChannelsReduction .
OpenCL error code is -54 at   when clEnqueueNDRangeKernel kernel_ThresholdRectToPix .
Setting return value to -1
[DS] Device: "Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz" (OpenCL) evaluated
[DS]          composeRGBPixel: 0.000132 (w=1.2)
[DS]            HistogramRect: 340282346638528859811704183484516925440.000000 (w=2.4)
[DS]       ThresholdRectToPix: 340282346638528859811704183484516925440.000000 (w=4.5)
[DS]        getLineMasksMorph: 0.004014 (w=5.0)
[DS]                    Score: inf

[DS] Device: "GeForce GT 755M" (OpenCL) evaluation...
[DS] Device: "GeForce GT 755M" (OpenCL) evaluated
[DS]          composeRGBPixel: 0.044892 (w=1.2)
[DS]            HistogramRect: 0.026128 (w=2.4)
[DS]       ThresholdRectToPix: 0.053928 (w=4.5)
[DS]        getLineMasksMorph: 0.025597 (w=5.0)
[DS]                    Score: 0.487237

[DS] Device: "(null)" (Native) evaluation...
[DS] Device: "(null)" (Native) evaluated
[DS]          composeRGBPixel: 0.019921 (w=1.2)
[DS]            HistogramRect: 0.063245 (w=2.4)
[DS]       ThresholdRectToPix: 0.034175 (w=4.5)
[DS]        getLineMasksMorph: 0.124600 (w=5.0)
[DS]                    Score: 0.952483
[DS] Scores written to file (tesseract_opencl_profile_devices.dat).
[DS] Device[1] 1:Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz score is inf
[DS] Device[2] 1:GeForce GT 755M score is 0.487237
[DS] Device[3] 0:(null) score is 0.952483
[DS] Selected Device[2]: "GeForce GT 755M" (OpenCL)
Warning in pixReadMemJpeg: work-around: writing to a temp file
```
 @zdenop Our patches to date have fixed the build, but tesseract with OpenCL on Mac outputs gibberish as the OCR results (on an image that passes normal OCR). Both the perceived position of text and output text are corrupted. However, a blank image is correctly interpreted as a being blank - for example, tesseract can properly render a blank PDF file. I take this to mean somewhere OpenCL is distorting or corrupting the input during processing.

How I can test it to narrow down where the error actually occurs? 
 AFAIK they should work on this. Did you get wrong result if you use binarized (2 colors) (tif?) image?
 Image I used was:
https://en.wikipedia.org/wiki/LinnSequencer#/media/File:LinnSequencer_hardware_MIDI_sequencer_brochure_page_2_300dpi.jpg

Although it appears to be grayscale it looks like it's formatted as true color. Output is:

```
Tesseract Open Source OCR Engine v3.05.00dev with Leptonica
[DS] Profile read from file (tesseract_opencl_profile_devices.dat).
[DS] Device[1] 1:Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz score is inf
[DS] Device[2] 1:GeForce GT 755M score is 0.487237
[DS] Device[3] 0:(null) score is 0.952483
[DS] Selected Device[2]: "GeForce GT 755M" (OpenCL)
Warning in pixReadMemJpeg: work-around: writing to a temp file
OSD: Weak margin (0.53), horiz textlines, not CJK: Don't rotate.
```

I converted that to a 1-bit TIFF using ImageMagick. That produced no OCR output and some errors:

```
[snipped common header]
Page 1
OSD: Weak margin (0.17), horiz textlines, not CJK: Don't rotate.
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Error in boxClipToRectangle: box outside rectangle
Error in pixScanForForeground: invalid box
Warning in pixGenerateCIData: pixs has cmap; using flate encoding
Warning in pixReadMemTiffCl: tiff page 58428720 not found
```

In both cases, I did output to PDF, and the PDF is formatted correct. The OCR is nonsense. When output to plain text, the OCR is also nonsense.

If Tesseract is built without OpenCL the same image works fine for OCR.
 I tried `tesseract -c tessedit_write_images=1 ...` and got the following result. The image has the same dimensions as the input image and there is a visual correspondence of sorts between the images, but it's very mangled. First image is input, second is "tessinput.tif" after Tesseract's mangling.

@zdenop: If a read and write pointer were strobing the data by different amounts you'd get this sort of thing. This really looks it a single increment somewhere is the wrong size and should be an easy fix for someone who knew where to look in the code base (i.e. not me).

![mono](https://cloud.githubusercontent.com/assets/1825843/9775499/7c03cee2-5704-11e5-90fc-1aafddde9138.png)
![tessinput](https://cloud.githubusercontent.com/assets/1825843/9775505/8a7c59b2-5704-11e5-9d4b-932a8559c4d8.png)
 Please check master branch or commits f46dfdc29da2bbacf6635752d9f28133ad4e10fe and 5db760215fa05f8dd722f66bc6a3ac438a5cd8d4.
  Use tesseract forums for asking support[1]
[1] https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
 Having tarballs is certainly possible, but I don't really see the point for most languages, which would only contain a single file. It does make sense for the languages that use Cube (that is, the trained data consists of a number of files), but there has been some kind of problem with that (I don't know the details), and Cube is expected to go away some time in the future.
As a sidenote, it would have been helpful if you could have made it clear that the issue was, in fact, an issue, by phrasing it as such: "tarballs not provided for traineddata" is an issue; "what is the official source...?" is a support question.
 1. There is an official location for download everything: repository tessdata[1]. Just to point out - you need package at-least eng and osd. Maybe have a look how debian packaged it or cygwin[2].
2. Ray asked[3] for suggestion regarding shipping/packaging problem - so this is IMO right place to discuss possible issues (also in past there was some suggestions who to distribute language files).

[1] https://github.com/tesseract-ocr/tessdata
[2] https://groups.google.com/d/msg/tesseract-ocr/dqT_KjKq0BM/PUMvC7DmIwAJ
[3] https://groups.google.com/d/msg/tesseract-dev/8e0F2cK2YzU/izie4Ysx-HEJ
  You got it wrong: this README is part of distribution packages and that it the reason why it points to its online version.
Another topic is source of information: This README can be modified only by project team members. https://github.com/tesseract-ocr/tesseract/wiki can be modified by any github users.
  I can not reproduce it on openSUSE 13.2. Try to provide more details.
 @vzani: when you start gdb, please enter command:
run
 If you still have a problem please try current stable release (3.04.01). 
Please uninstall all old tesseract installations before installing new tesseract version.
  I am not able to reproduce error with clang version 3.7.0 on opensuse 42.1 64bit... 
  This is not correct issue - please write what it wrong and what does not work exactly!
  @rodrigosalinas
The issue was solved (again).
Try latest code from the repo.
 You should provide more details.

> I have the latest code from the repo.

Which commit exactly, 1826ac14 ?

What OS do you use and the specific version of this OS. For example: Ubuntu 14.04, OS X 10.11, Windows 10.

Also, please provide the exact command you used and attach as zip file the tif and box files.
 I have Ubuntu 14.04 64 bit.

Commit  1826ac1.

> tesseract ult.dejavu.exp0.tif ult.dejavu.exp0 box.train

Output:

> Tesseract Open Source OCR Engine v3.05.00dev with Leptonica
> Page 1
> APPLY_BOXES:
>    Boxes read from boxfile:    1360
>    Found 1360 good blobs.
> Generated training data for 292 words
> Warning in pixReadMemTiff: tiff page 1 not found

[ult.dejavu.exp0.tr.zip](https://github.com/tesseract-ocr/tesseract/files/106960/ult.dejavu.exp0.tr.zip)

My guess is that you didn't install  tesseract properly.

Did you run all these commands last time you installed  tesseract?

> ./autogen.sh
> ./configure --disable-cube
> make
> sudo make install
> sudo ldconfig
> make training
> sudo make training-install
 if you have previously installed  tesseract system-wide (for example, under /usr/local) the command 'tesseract ...' will use that system-wide executable.
 Try this command to see if you have another installation of Tesseract in your machine.

> find /usr -name "tesseract"
 What's the output of:

`printenv TESSDATA_PREFIX`

`tesseract --list-langs`
 `tesseract ult.dejavu.exp0.tif ult txt hocr`

Does this command produce ult.txt and ult.hocr ?

Maybe you should try to set TESSDATA_PREFIX environment variable or use --tessdata-dir parameter.
 OK, I was able to reproduce the issue.

I will try to figure out why this is happening when installing tesseract in 
`/home/myusername/tessbin`
 I solved @aiwaz issue. More info will follow later.
 @aiwaz, please test my fix.
 zdenko merged my fix to master.
 :+1:
Thanks for reporting the issue.
  Did you read https://groups.google.com/d/msg/tesseract-ocr/ToWcnyHqF4c/P7HDEKsR1cEJ ?
 I have a number of tempfile patches already written for Leptonica to make these calls more 
secure and less brittle, and there is ongoing work on this topic. I actually don't know if 
cygwin is using the Unix or Windows code path for temporary files, but just want to 
mention that there is activity. Don't know why you are getting bad results compared to
other cygwin users.

https://sources.debian.net/src/leptonlib/1.73-5/debian/patches/
  Can you try the patch in #60 to see if that fixes it?
 Ok, that's a different error. Do you have ICU4C installed?
 Sorry, it only complained about one of the ICU libraries. 

I think that the problem is that the library is named 'libicui18n' under Cygwin, as it is on Linux etc.
 Can you try with the patch from #62 ?
 Do the training tools build now? (Zdenko merged the pull request, so there's no need to mess around with branches)
 Ok, I'll close this. Thanks for testing!
  See: https://groups.google.com/d/msgid/tesseract-ocr/55B12C3C.3010908%40vol.at

```
pango_font_info.cpp:223:46: error: 'strcasestr' was not declared in this scope
   is_fraktur_ = (strcasestr(family, "Fraktur") != NULL);
```
 Just for future investigation: https://stackoverflow.com/questions/9935642/how-do-i-use-strcasestr
  Not generally useful, IMO - I don't see there being a whole lot of demand for this.
 Sure, the FAQ seems a good place for this information.
  that means that leptonica was not compiled with tiff support
 what does it mean "i have tried"?
If you compile leptonica without libtiff support it does not help to try any version...
This is definitely not tesseract issue.
  Not enough input. In short, box.train needs both an image, and a box file, and from those it creates training data. For a more complete explanation, see the wiki: https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#run-tesseract-for-training
 Re-opening, as requested.
 Something like this? (Using the variable also supports box.train.stderr)

```
diff --git a/api/tesseractmain.cpp b/api/tesseractmain.cpp
index e7abadf..7ddbc93 100644
--- a/api/tesseractmain.cpp
+++ b/api/tesseractmain.cpp
@@ -306,6 +306,11 @@ int main(int argc, char **argv) {
   if (b) renderers.push_back(new tesseract::TessBoxTextRenderer(outputbase));
   api.GetBoolVariable("tessedit_create_txt", &b);
   if (b) renderers.push_back(new tesseract::TessTextRenderer(outputbase));
+  api.GetBoolVariable("tessedit_train_from_boxes", &b);
+  if (b && !strcmp(outputbase, "-")) {
+    fprintf(stderr, "Box input from stdin not supported in box training.\n");
+    exit(1);
+  }
   if (!renderers.empty()) {
     // Since the PointerVector auto-deletes, null-out the renderers that are
     // added to the root, and leave the root in the vector.
```
 I think it's a matter for broader discussion.

On the one hand, it's The Right Thing, and I've already done The Wrong Thing by closing an issue that mentions a segfault... but it's an exceptional case. One, because it overloads what is normally the output file position to be a secondary input, and two, because it's not a frequent use case.
 I tried it on opensuse 13.2 64bit and it did not crashed:

> tesseract testing/phototest.tif - box.train
> Page 1
> APPLY_BOXES:
>    Boxes read from boxfile:     225
>    Found 225 good blobs.
> Generated training data for 60 words

Warning in pixReadMemTiff: tiff page 1 not found

> ls -t | head -n 1
> -.tr

Just OCR to stdout worked as expected:

> tesseract testing/phototest.tif -
> Page 1
> This is a lot of 12 point text to test the
> ocr code and see if it works on all types
> of file format.

The quick brown dog jumped over the
lazy fox. The quick brown dog jumped
over the lazy fox. The quick brown dog
jumped over the lazy fox. The quick
brown dog jumped over the lazy fox.

Warning in pixReadMemTiff: tiff page 1 not found
 @jbreiden: In openSUSE 13.2 I do not have api/.libs/lt-tesseract just api/.libs/tesseract. And you have two times, so output is testing/phototest.tif.tr 

And I got this:

```
TESSDATA_PREFIX=/usr/share/ valgrind api/.libs/tesseract testing/phototest.tif testing/phototest.tif - box.train
==21845== Memcheck, a memory error detector
==21845== Copyright (C) 2002-2013, and GNU GPL'd, by Julian Seward et al.
==21845== Using Valgrind-3.10.0 and LibVEX; rerun with -h for copyright info
==21845== Command: api/.libs/tesseract testing/phototest.tif testing/phototest.tif - box.train
==21845== 
Tesseract Open Source OCR Engine v3.05.00dev-11-gd937659 with Leptonica
Page 1
APPLY_BOXES:
   Boxes read from boxfile:     225
   Found 225 good blobs.
Generated training data for 60 words
Warning in pixReadMemTiff: tiff page 1 not found
==21845== 
==21845== HEAP SUMMARY:
==21845==     in use at exit: 19,795,264 bytes in 33 blocks
==21845==   total heap usage: 874,639 allocs, 874,606 frees, 60,486,144 bytes allocated
==21845== 
==21845== LEAK SUMMARY:
==21845==    definitely lost: 0 bytes in 0 blocks
==21845==    indirectly lost: 0 bytes in 0 blocks
==21845==      possibly lost: 19,753,408 bytes in 24 blocks
==21845==    still reachable: 41,856 bytes in 9 blocks
==21845==         suppressed: 0 bytes in 0 blocks
==21845== Rerun with --leak-check=full to see details of leaked memory
==21845== 
==21845== For counts of detected and suppressed errors, rerun with: -v
==21845== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```

For "stdout version" I got this:

```
TESSDATA_PREFIX=/usr/share/ valgrind api/.libs/tesseract testing/phototest.tif - box.train
==11238== Memcheck, a memory error detector
==11238== Copyright (C) 2002-2013, and GNU GPL'd, by Julian Seward et al.
==11238== Using Valgrind-3.10.0 and LibVEX; rerun with -h for copyright info
==11238== Command: api/.libs/tesseract ../tesseract-ocr/testing/phototest.tif - box.train
==11238== 
Page 1
APPLY_BOXES:
   Boxes read from boxfile:     225
   Found 225 good blobs.
Generated training data for 60 words









Warning in pixReadMemTiff: tiff page 1 not found
==11238== 
==11238== HEAP SUMMARY:
==11238==     in use at exit: 19,795,264 bytes in 33 blocks
==11238==   total heap usage: 874,629 allocs, 874,596 frees, 60,485,260 bytes allocated
==11238== 
==11238== LEAK SUMMARY:
==11238==    definitely lost: 0 bytes in 0 blocks
==11238==    indirectly lost: 0 bytes in 0 blocks
==11238==      possibly lost: 19,753,408 bytes in 24 blocks
==11238==    still reachable: 41,856 bytes in 9 blocks
==11238==         suppressed: 0 bytes in 0 blocks
==11238== Rerun with --leak-check=full to see details of leaked memory
==11238== 
==11238== For counts of detected and suppressed errors, rerun with: -v
==11238== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```
 @jbreiden, please test with latest commit.
 I can reproduce this.

I reread this issue. Jim's explanation is still true.

> box.train needs both an image, and a box file, and from those it creates training data. For a more complete explanation, see the wiki: https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract#run-tesseract-for-training
 Tested. It works - no segfault.
  it worked for me on some images (there are some issues reported already - IMO 2 color tiff should work). Last week it segfault on linux. I will try windows today...
Also on forum there are some experiments[1]
[1] https://mail.google.com/mail/u/0/#search/opencl
 I am not sure what do you mean with  "any other dependencies except leptonica" - to run tesseract with opencl support you need libtiff and opencl sdk that support your hardware...
 @jbreiden:
it works for me on windows:
tesseract-opencl.exe ....\testing\phototest.tif -
[OD] Load opencl.dll successful!
[DS] Profile file not available (tesseract_opencl_profile_devices.dat); performi
ng profiling.

[DS] Device: "Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz" (OpenCL) evaluation...
[OD] write binary[kernel-Intel(R)_Core(TM)_i7-4800MQ_CPU_@_2.70GHz.bin] succesfully
[DS] Device: "Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz" (OpenCL) evaluated
[DS]          composeRGBPixel: 0.038657 (w=1.2)
[DS]            HistogramRect: 0.325462 (w=2.4)
[DS]       ThresholdRectToPix: 0.042622 (w=4.5)
[DS]        getLineMasksMorph: 0.119543 (w=5.0)
[DS]                    Score: 1.617007

[DS] Device: "Intel(R) HD Graphics 4600" (OpenCL) evaluation...
[OD] write binary[kernel-Intel(R)_HD_Graphics_4600.bin] succesfully
[DS] Device: "Intel(R) HD Graphics 4600" (OpenCL) evaluated
[DS]          composeRGBPixel: 0.153912 (w=1.2)
[DS]            HistogramRect: 0.449184 (w=2.4)
[DS]       ThresholdRectToPix: 0.048737 (w=4.5)
[DS]        getLineMasksMorph: 0.027403 (w=5.0)
[DS]                    Score: 1.619068

[DS] Device: "(null)" (Native) evaluation...
[DS] Device: "(null)" (Native) evaluated
[DS]          composeRGBPixel: 0.088750 (w=1.2)
[DS]            HistogramRect: 0.388795 (w=2.4)
[DS]       ThresholdRectToPix: 0.182945 (w=4.5)
[DS]        getLineMasksMorph: 0.613772 (w=5.0)
[DS]                    Score: 4.931717
[DS] Scores written to file (tesseract_opencl_profile_devices.dat).
[DS] Device[1] 1:Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz score is 1.617007
[DS] Device[2] 1:Intel(R) HD Graphics 4600 score is 1.619068
[DS] Device[3] 0:(null) score is 4.931717
[DS] Selected Device[1]: "Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz" (OpenCL)
Page 1
This is a lot of 12 point text to test the
ocr code and see if it works on all types
of file format.

The quick brown dog jumped over the
lazy fox. The quick brown dog jumped
over the lazy fox. The quick brown dog
jumped over the lazy fox. The quick
brown dog jumped over the lazy fox.
 Please check master branch or commits f46dfdc29da2bbacf6635752d9f28133ad4e10fe and 5db760215fa05f8dd722f66bc6a3ac438a5cd8d4.
  https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advises
  autotool requires README!
 File was renamed to README.md (in a0ea634) therefore there is reference to README.md in README(it is fixed in 4f082a9). 
But autogen.sh fails without README, so I put it back (in 9b7f252)
 http://stackoverflow.com/questions/15013672/use-autotools-with-readme-md says 

```
AM_INIT_AUTOMAKE([foreign])
```

is enough to get around the need for README.
  For general questions and support please use tesseract-ocr user forum. See FAQ[1]

[1] https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advises
 From the looks of it, you haven't set the include path for leptonica's headers, but it's hard to tell from the screenshot. In general, console logs are much more useful than screenshots, so when you write to the forum, please try to include a log.
  Can you please provide error message from MSVC2013? 
  For support please use tesseract-ocr user forum. See FAQ[1]

[1] https://github.com/tesseract-ocr/tesseract/wiki/FAQ#rules-and-advice
 This issue is currently the top search result for 'ocr_float'; it lacks a simple summary: Tesseract (currently) does not support ocr_float.
 That seems a bit redundant; I was merely summarising what you were told there :)
  That error means that you are not linking against leptonica 1.72 library (but probably against some older version). It is not tesseract problem. 
  According Ray cube is a dead-end and it can be removed soon from the code(see e.g. [1]), so you can not expect any progress...

[1] https://groups.google.com/d/msg/tesseract-dev/mtIJXoUpfJc/6f0EwVNXOM8J
 lets wait for Ray...
 It's 'going away' for several years now... :-)
 Actually I have a comment for this.
There is one reason why cube has survived this long: For Hindi cube+tesseract has half the error rate of either on their own. I haven't actually tested that against the new LSTM engine yet, but I will on Monday, and if the new LSTM engine is better, then yes, cube is likely to get the chop for 4.00, and the ifdefs will be very useful. Tests complete. Decision made. Cube is going away in 4.00.
Results:
**Engine** | **Total char errors** | **Word Recall Errors** | **Word Precision Errors** | **Walltime** | *CPUtime**
:---------------: | :-----: | :-----:| :-----:| :-----: | :-------------------
Tess 3.04 | 13.9 | 30 | 31.2 | 3.0 | 2.8
Cube | 15.1 | 29.5 | 30.7 | 3.4 | 3.1
Tess+Cube | 11.0 | 24.2 | 25.4 | 5.7 | 5.3
LSTM | 7.6 | 20.9 | 20.8 | 1.5 | 2.5

Note in the above table that LSTM is **faster** than Tess 3.04 (without adding cube) in both wall time and CPU time! For wall time by a factor of 2.
 Can you provide some details about used hardware for test?
Did you made test also on single core CPU to see difference? OK, the big test I ran in a Google data center.
I just ran a test on my machine (HP Z420) on a single Hindi page for
comparison, ran each one 3 times (using time), and took the median.
My machine has AVX, so that will have still speeded it up a bit, so I tried
without AVX/SSE as well:
I disabled OpenMP by adding #undef _OPENMP in functions.h, line 33, and
disabled AVX/SSE in weightmatrix.cpp, line 66,67.

Test Mode                      Real User
Default (-oem 3 = cube + tess) 7.6  7.3
Base Tess (-oem 0)             2.9  2.6
Cube (-oem 1)                  5.4  4.9
LSTM With OpenMP+AVX           1.8  3.8
LSTM No OpenMP with AVX        2.7  2.4
LSTM No OpenMP with SSE        3.1  2.7
LSTM No OpenMP no SIMD at all  4.6  4.1

I think these tests nail cube as being slower and less accurate.

There may be a debate as to the value of the old Tesseract engine for its
speed vs the new one for its accuracy.

I'm going to push the data files now.



On Mon, Nov 28, 2016 at 10:40 AM, Stefan Weil <notifications@github.com>
wrote:

> And what about the language model used for the test? Is it already
> available so I can use it for my own tests?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/40#issuecomment-263355535>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056W2jegBHZM59ZxH-U5q22tSBy-HZks5rCyAxgaJpZM4FOBFi>
> .
>



-- 
Ray.
 On Mon, Nov 28, 2016 at 1:49 PM, Stefan Weil <notifications@github.com>
wrote:

> I'm going to push the data files now.
>
> Got the first ones. My first test with a simple screenshot gave
> significant better results with LSTM, but needed 16 minutes CPU time
> (instead of 9 seconds) with a debug build of Tesseract (-O0). A release
> build (-O2) needs 17 seconds with LSTM, 4 seconds without for the same
> image.
>
The slow speed with debug is to be expected. The new code is much more
memory intensive, so it is a lot slower on debug (also openmp is turned off
by choice on debug).
The optimized build speed sounds about right for Latin-based languages. It
is the complex scripts that will run faster relative to base Tesseract.

> Are there also new data files planned for old German (deu_frak)? I was
> surprised that the default English model with LSTM could recognize some
> words.
>
I don't think I generated the original deu_frak. I have the fonts to do so
with LSTM, but I don't know if I have a decent amount of corpus data to
hand. With English at least, the language was different in the days of
Fraktur (Ye Olde shoppe). I know German continued to be written in Fraktur
until the 1940s, so that might be easier. Or is there an old German that is
analogous to Ye Old Shoppe for English?

> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/40#issuecomment-263405208>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056Ti1gWSSG6BfuBbL68EE7RYfsItOks5rC0xWgaJpZM4FOBFi>
> .
>



-- 
Ray.
 I think it would be great to move this discussion to (developers) forum. we are already out scope of original issue post and much more people should be interested in "Faktur topic"... I have a new training md file in prep with an update to the code to make it
all work correctly.
It is going through our review process, and then I will need to sync again
with the changes that have happened since my last sync, but it should be
available late this week.
The md file documents the training process in tutorial detail, but line
boxes and transcriptions sounds perfect!

500k lines should make it work really well. I would be happy to take it and
help you, but we would have to get into licenses, copyright and all that
first.
For now it might be best to hang on for the instructions.

On Tue, Nov 29, 2016 at 2:05 PM, Johannes Baiter <notifications@github.com>
wrote:

> @theraysmith <https://github.com/theraysmith>
>
> I don't think I generated the original deu_frak. I have the fonts to do so
> with LSTM, but I don't know if I have a decent amount of corpus data to
> hand.
>
> I have a decent amount of corpus data for Fraktur (from scanned books) at
> hand (hOCR files with line boxes and transcriptions), it's about 500k lines
> and 50GB. I've yet to publish it, but if you have somewhere where I could
> send/upload it, I'd be glad to.
>
> Or is there a way to create the neccessary training files myself? I've had
> a cursory look through the OCR code and it looked like it needed lstmf
> files, but I haven't yet found what these are supposed to look like.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tesseract-ocr/tesseract/issues/40#issuecomment-263714590>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AL056WSh7RpK3l-EHGgky_xbByYqhbwSks5rDKGOgaJpZM4FOBFi>
> .
>



-- 
Ray.
 Cube is gone! Removal completed as of 9d6e4f6  I think I fixed this issue with the code I pushed in 4.00.  If there is issue with tesseract, please report it (your current report is useless) or make pull request.
  it seems that the z prefix isn't defined in MS Visual Studio  [size specification](https://msdn.microsoft.com/en-us/library/tcxf1dw6.aspx).
 It seems that @stweil solved this with this commit:
https://github.com/tesseract-ocr/tesseract/commit/997c4a6
  Requested in https://code.google.com/p/tesseract-ocr/issues/detail?id=1378
  https://code.google.com/p/tesseract-ocr/issues/detail?id=1351
(http://web.archive.org/web/20150413012200/https://code.google.com/p/tesseract-ocr/issues/detail?id=1351)
 I tested this patch and IMO it is only partial solution (for some cases). Issue was escalated to original OpenCL contributor.
 Merged, so we have at least partial fix...
  This adds support from the ocricola project to use finite state transducers in HFST format instead of DAWGs.

Branch is currently quite stale, but while I'm pulling all these pull requests together, I thought I should put this one in too.
 Things to do:
- [x] Match code formatting to rest of Tesseract
- [x] Remove exceptions
- [ ] Remove STL
 Not needed. The new engine works well with the dawgs. This is intended more to be an alternative to dawgs for morphologically complex languages that have a morphology available in HFST format. @mpsilfve do you have anything to add?  https://code.google.com/p/tesseract-ocr/issues/detail?id=1353

When training batch boxes and tifs this causes training to fail after 2nd iteration. The issue is that the tessopt iterator index is a global. The fix is to 1.) reset before each tessopt usage 2.) wrap the iterator state in a class/generator. Low priority, but useful if we want to generate a bunch of traineddatas in the same process or even multiple threads.

(http://web.archive.org/web/20150413012203/https://code.google.com/p/tesseract-ocr/issues/detail?id=1353)
  Split commits; builds on -p1, so includes that patch. Updated to current version
  1. memry removal.

This should be no-brainer. These functions never seemed to do anything useful. The only oddity was with alloc_string which rounded the allocation upwards to nearest multiple of 4. Based on my review of code, I could not identify any value in this feature, so even alloc_string was simply transformed to malloc.

memry.\* are removed. All code is adjusted to simply call malloc and free instead.

listio.\* are removed. The list reading function in these files appears to have been unused, and contained the only invocation of strsave define, which was one of the alloc_string users, so I removed both of them.

https://code.google.com/p/tesseract-ocr/issues/detail?id=1199
  …ble in textord/tospace.cpp

https://code.google.com/p/tesseract-ocr/issues/detail?id=1106

Reported by ettl.martin78, Feb 11, 2014
Please review the attached patch. It fixes a potential usage of an uninitialized bool variable ('fuzzy_sp'). The fix simply initialized the variable by default to 'FALSE'. Before the fix, the value of 'fuzzy_sp' was not set, but used in the else-branch in line 1107:

```
      else {
        prev_blanks = blanks;
        prev_fuzzy_sp = fuzzy_sp;
        prev_fuzzy_non = fuzzy_non;
      }
```

Best regards and many thanks

Feb 11, 2014
#1 ettl.martin78

The updated patch fixes two more uninitialized variable usages in the same function.

Many thanks for reviewing.

(http://web.archive.org/web/20150509223835/https://code.google.com/p/tesseract-ocr/issues/detail?id=1106)
  https://code.google.com/p/tesseract-ocr/issues/detail?id=1341 (http://web.archive.org/web/20150413012149/https://code.google.com/p/tesseract-ocr/issues/detail?id=1341)
  https://code.google.com/p/tesseract-ocr/issues/detail?id=1139

Reported by nick.white@durham.ac.uk, Apr 8, 2014
The ok_count == 2 part of the test always failed, which prevented the kerning variations from ever being printed. As far as I can tell
it wasn't doing anything useful, but my C++ is (still) rusty, so
some light checking would be wise.

The patch also updates the function description to be in line with the current output.

Apr 24, 2014
#1 theraysmith

This could significantly change results after training.
Going to apply this change in a carefully controlled environment.

Apr 25, 2014
#2 nick.white@durham.ac.uk

This patch did make the example in the comments (A+V) output as expected, but it doesn't seem to have affected the fact that no kerning variations are printed for any ancient greek digrams. I'm not sure why, whether it's the ancient greek font, or there is something still not right about the code.

(http://web.archive.org/web/20150510031850/https://code.google.com/p/tesseract-ocr/issues/detail?id=1139)
 I think so. Even the legacy engine didn't use that data much, the new engine doesn't use it at all.  Migrating from https://code.google.com/p/tesseract-ocr/issues/detail?id=1316
 For posterity: http://web.archive.org/web/20150509202204/https://code.google.com/p/tesseract-ocr/issues/detail?id=1316
