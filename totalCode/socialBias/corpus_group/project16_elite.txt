  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  `activate` is typically only used if you install TensorFlow in a virtualenv. Since you don't appear to have done this, you should be able to use TensorFlow straight away, e.g. by running `python` and typing `import tensorflow as tf` at the prompt.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 Failing test is a known flake with fix underway.
  what additional support is needed? It's API compatible with 5.0.  Closing unless there's something new.
  fixes #2703 
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Jenkins, test this please
 CLABot, you crazy.
  Closing as duplicate of #2813
  Apparently we do have some Fractional pooling ops in development which should (hopefully) make it into the external tree shortly.  Will leave this open as a tracking bug.
  0.8.0 doesn't contain (all of) tf.contrib.learn. Can you check with 0.9?
 Because we're not in pypi, sadly.
On Mon, Jun 20, 2016 at 07:06 Mia Hunsicker notifications@github.com
wrote:

> works! thank you. I didn't think I missed that because when I prompted
> "pip install tensorflow --upgrade" it returned tf 0.8.0 was up to date.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2952#issuecomment-227151786,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_YYU_CGN5lm_h7_uxFpUoVuzo80Dks5qNp5QgaJpZM4I5QyO
> .
  Can you post the output of pip --version? It's possible that your pip is installed for python3.
 Just run "easy_install-2.7 pip" (or similar), that should make a pip2.7 and
probably also moves the pip symlink to the py2 version.
On Mon, Jun 20, 2016 at 12:32 cyriltw notifications@github.com wrote:

> @martinwicke https://github.com/martinwicke yep spot on that seems to
> be the issue.
> 
> pip 8.1.2 from
> /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages
> (python 3.5)
> 
> I want to use Tensorflow on Python2. which version of pip is suitable for
> this?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2951#issuecomment-227244935,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_ZiYNx08fLTW6NOfth9v9jlcL40Eks5qNurEgaJpZM4I5NTm
> .
  @ilblackdragon  could you please take a look at this?  
 @RafaelCosman what is the version of TF are you running?

Let me try it out to see what's happening.
 @ilblackdragon There were some PRs regarding resnet.py. Seems like unnecessary indentation was introduced from some previous internal merges. 
  BTW PR #2747 has logic to detect when Python function call would create an op that's already in the graph and reuse that op if so
 This is really a general usage question and as such is better suited for StackOverflow. Please consider re-asking there and tag it with the `tensorflow` tag.

TensorFlow graphs are extensively rewritten before execution, and one of the optimizations is common subexpression elimination which would remove duplicate code in cases where it does not affect the result.  So, in most cases this should not be a _performance_ issue.  

If you have a specific model where this is a performance problem, please consider capturing an execution Timeline using the RunOptions to session.Run() and reopen this issue.  

Similarly, if the graphdef is excessively large, please consider posting an example of the code idiom which is resulting in duplication.
  Jenkins, test this please.
 Jenkins, test this please.
  Jenkins, test this please.
 @tensorflow-jenkins test this please.
 Jenkins, test this please.
 LGTM pending Jenkins.
  In order to understand what is happening here, you will at least need to provide the command lines you are using and the full output of the commands which are failing.  (e.g. There is not enough information in the two screenshots above)

@lukaszkaiser  Are there any known issues with running this model multi-GPU?    
 I don't know of any issues. Could you provide a text log with full error message? (The screens don't seem to include it all, a text file with the whole error message would be more helpful.)
  This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.
  See same thing on today head
 After investigating this, it seems most likely that the cause is heap fragmentation, arising from the creation of a large number of NumPy arrays. The current `tf.Session` implementation is (inadvertantly) copying the incoming `feed_dict` values into new NumPy arrays on every step, and I've got a fix for that pending. This leads to a large amount of churn on the heap, and the default `malloc()` implementation doesn't appear to handle this well.

Once the fix is in, your example code should not leak, but we would recommend using [`tcmalloc`](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) for more realistic programs. Running the same code with `tcmalloc` enabled shows no leak at all with unmodified TensorFlow 0.9. An alternative workaround is to set the `malloc()` options to be more aggressive about `mmap()`-ing for large allocations. For example, setting following the environment variable also eliminated the leak for me:

```
$ MALLOC_MMAP_THRESHOLD_=100000 python ...
```
  I need to multiply a 3D tensor by a 2D weight matrix, then feed it to dynamic_rnn.
Below is the code.

```
inputSize = 1000
embeddingSize = 100

batchX = tf.placeholder(tf.float32, [None, None, inputSize])
#The maximum length of the sequences and the size of the mini-batch are dynamic
#batchX is a time-major 3D tensor

batchXLenghts = tf.placeholder(tf.int32, [None,])
#A list of integers indicating the length of each sequence in batchX

maxLength = tf.shape(batchX)[0]
batchSize = tf.shape(batchX)[1]

with tf.variable_scope('embedding') as scope:
    W_emb = tf.get_variable('W_emb', [inputDimSize, embDimSize], initializer=tf.truncated_normal_initializer())
    b_emb = tf.get_variable('b_emb', [embDimSize,], initializer=tf.constant_initializer())

emb = tf.reshape(tf.matmul(tf.reshape(batchX, [maxLength*batchSize, inputDimSize]), W_emb) + b_emb, [maxLength, batchSize, embDimSize])
#embedding step. There are two reshape operations because I am doing np.dot(3D, 2D)

cell = GRUCell(embDimSize)
outputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')

# calculating logits, loss, etc.
...
...
```

When I run this code, I get the following error

```
Traceback (most recent call last):
  File "feedTestDynamicRnn.py", line 127, in <module>
    train(xFile=xFile, yFile=yFile)
  File "feedTestDynamicRnn.py", line 98, in train
    batchX, batchXLengths, batchY, logits, mean_loss = inference(options)
  File "feedTestDynamicRnn.py", line 57, in inference
    outputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py", line 580, in dynamic_rnn
    swap_memory=swap_memory, sequence_length=sequence_length)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py", line 630, in _dynamic_rnn_loop
    "Input size (depth of inputs) must be accessible via shape inference, "
ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.
```

When I comment out the embedding step, and feed `batchX` directly to `dynamic_rnn`, there is no problem. So it seems that `tf.reshape()` loses shape information as told by [this post](http://stackoverflow.com/questions/35374958/reshape-tensor-using-placeholder-value). 

The problem is, I cannot fix `maxLength` and `batchSize` to a static value, because they need to change. Also, I need to do embedding, so directly feeding `batchX` to the RNN is not an option. Is there a work around for this?
 @jlowin Thank you!! This would have never ocurred to me. I tried `emb.set_shape([maxLength, batchSize, embDimSize])` but I got an error saying I cannot use 'Tensor' in the `set_shape` argument. Then I gave up hope. Thank you very much again!!
 @jlowin Re-opened the issue. Thanks!
 It may be possible that tf.reshape needs more aggressive shape inference code.  I'll take a look and see how much we can infer at graph construction time.
  In general, TensorFlow prefers its own implementation on GPU, if the performance is close enough. This is because we would have the source code for customization. Conv is a good example where it would take a lot of effort to replicate Cudnn's performance. 

There is still an ongoing investigation how TensorFlow would expose a fused RNN/LSTM implementations. The current Cudnn interface does not provide enough customization to accommodate common variations. In the mean time, it is possible that we can add a wrapper op in TensorFlow/contrib around the current Cudnn implementation, before we can decide what is the best way for long-term support in the TensorFlow core. 
 @bhack, in case of LSTM, the biggest concern is lack of customization. In general, having access to the source code gives us better ability us to tweak our code to better serve TensorFlow users. 

That said, we won't hesitate to pick any good functionalities from any library including Cudnn, if there is evidence showing a sizeable benefit.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Jenkins, test this please.
 @nanddalal can you sign the CLA?
  In Python it's call  tf.contrib.quantization.python.quantize_v2.
 Please let me know if it works for you. Thanks.
  Sorry you're hitting problems, and thanks for the very clear report!

Because most mobile use cases are focused on inference, we don't support gradient ops like Conv2DBackpropInput. In this case though, the op is actually used under the hood by conv2d_transpose(), and so isn't removed when freeze_graph strips out training operations.

As a temporary solution, you should be able to add tensorflow/core/kernels/conv_grad_ops.cc to this file and re-run build_all_ios.sh:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_cc_files.txt#L77

I'll work on getting a proper fix in.
 Never mind, the op is already listed in that file. I'll keep digging to see what's actually wrong here.
 I wasn't able to reproduce this locally, and on a deeper look I discovered that I'd fixed this in #2936 when I regenerated the file lists from the latest Bazel build. If you get top-of-tree, conv_grad_ops.cc is included and you should no longer see this problem.

Can you confirm updating fixes this for you too?
  Hi @chasep255. If it's a runtime decision, you should do

last_out = tf.cond(t > 0, lambda: decode_output[t-1], lambda: tf.zeros((batch_size, 128))

Could you please give it a try and see if it works for you? If not, could you cut and paste the complete program? Thanks.
 Glad you managed to get it working.  I think your initial post has some confusion between the statically known shapes of ops/tensors in the graph, and the dynamically computed shapes of tensors evaluated when the graph is executed  (via session.run, feeding in placeholders) ?

e.g. The input_tensor is a placeholder with partially specified shape.  We don't know until the input is fed what the actual batch size is (and it may vary from run to run).  You then take the _dynamic_ shape of the input (using a tf.shape op in the graph)  and slice off the first element.  

```
input_tensor = tf.placeholder(tf.float32, (None, TIME_STEPS, 128), 'input_tensor')
shape_of_input_tensor = tf.shape(input_tensor)
batch_size = shape_of_input_tensor[0]
```

If we examine 'shape_of_input_tensor' we see that it is a vector whose value which cannot be computed until the input is fed into the graph.

```
>>> print shape_of_input_tensor
Tensor("Shape_3:0", shape=(3,), dtype=int32)
```

Note that batch_size is actually the output of a dynamic 'Slice' operation 

```
>>> print batch_size
Tensor("Squeeze_2:0", shape=(), dtype=int32)
>>> print batch_size.op                                                                                                                                                        
name: "Squeeze"
op: "Squeeze"
input: "Slice"
attr {
  key: "T"
  value {
    type: DT_INT32
  }
}
attr {
  key: "squeeze_dims"
  value {
    list {
      i: 0
    }
  }
}
```

In contrast, 'state.get_shape()' is a static property of the graph and so cannot know the batch size until the placeholder is fed at runtime.  See here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L329

I couldn't reproduce further without know what 'encoder_multi_rnn.state_size' returns.  It clearly can't be a simple integer:

```
>>> STATE_SIZE=27
>>> state = tf.zeros((batch_size, STATE_SIZE), tf.float32)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py", line 622, in zeros
    shape = ops.convert_to_tensor(shape, name="shape")
  File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py", line 179, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py", line 162, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))
  File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py", line 332, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py", line 269, in _AssertCompatible
    raise TypeError("List of Tensors when single Tensor expected")
TypeError: List of Tensors when single Tensor expected
```

Note that the args of tf.zeros are passed through [tf.convert_to_tensor](https://www.tensorflow.org/versions/r0.9/api_docs/python/framework.html#convert_to_tensor):

Without the entire original program, it's hard to follow why shape inference of last_shape (all the way through your encoder) was not working.  

Could you provide a simple repro test case?  Failing that I propose we close this issue.
  Sorry you're hitting problems! We actually have a few different ways to reduce memory usage. One of them is to run tensorflow/contrib/quantization/tools/quantize_graph with --mode=weights to shrink the size of the model on disk by quantizing the weights to eight bits. I'm also working on folding in the batch normalization ops into the weights, which will reduce memory pressure.

We also have memory-mapped constants, which you can try by running this script on your graph:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/convert_graphdef_memmapped_format.cc

I haven't tried this myself yet on iOS, but with previous projects it was a great way to reduce memory problems since mapped files don't seem to count towards your overall usage, and are automatically swapped out when pressure is high.
  Please feel free to send a PR.

Added @yuanbyu just in case.
  Excellent question! The short answer is that it's not a bug. We always start an in-process server on localhost. So to use this correctly, you actually need to have a separate script which logs onto www.a.com and start the server. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md
for details.

Here's the slightly longer answer should you be interested:

When this line is called on www.a.com,
  server = tf.train.Server(cluster_spec, job_name="worker", task_index=0)
a GrpcServer is created with target grpc://localhost:2222. This server knows how to talk to the tasks in the same job via GrpcChannels:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L150

Hope that helps. :)

Sherry
 > If the server.target has to be localhost:$post, the client has to be in the same host with the worker. Is that right?

No, the client and worker do not have to be on the same host. The `tf.train.Server` class is a Python wrapper for a server in the _local process_, so it always binds to a port on `localhost`. (It ignores the hostname that you specify for the corresponding task in the `tf.train.ClusterSpec`, so that you can use the same `ClusterSpec` in all processes.)

The client only needs to create a `tf.Session`, and does not need to create a `tf.train.Server` (though it can if you like). For example, you can run a server on `"www.a.com"` that listens on port 2222, and connect to it from another machine by creating a `tf.Session("grpc://www.a.com:2222")`.
  Thanks, @jinfengfeng . 
  You will need to either name your input_data "input", or change the demo code to feed "input_placeholder:0".
  convert_to_tensor expects value to be "An object whose type has a registered `Tensor` conversion function.", not an op.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L572
  @jyshee go for it if you need it.
  Feed_dict does a single-threaded memcpy of contents from Python runtime into TensorFlow runtime. If data is needed on GPU, then you'll have an additional CPU->GPU transfer. I'm used to seeing up to 10x improvement in performance when switching from feed_dict to native TensorFlow (Variable/Queue)
 @nanddalal extra copy in `np.array` seems like a bug, good catch. 

As far as feasibility of removing the extra copy, I believe the memcpy is happening in [TF_wrapper_helper](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/python/client/tf_session_helper.cc#L445). There's a note from @mrry that memcpy is there to prevent having to acquire GIL on deallocation.

Since numpy array object is owned by Python runtime, when it's destroyed Python will try to de-allocate the data. If there's a way to tell numpy to release the ownership of the data buffer, that could be a work-around.

Perhaps an intermediate step would be to replace memcpy with multi-threaded memcpy. There's a six-year old [post on SO](http://stackoverflow.com/questions/4260602/how-to-increase-performance-of-memcpy) suggesting 6x improvement
  @rmlarsen Could you take a quick look pls?
  Jenkins, test this please.
 Jenkins, test this please.
  Jenkins, test this please.
 Jenkins, test this please.
  Commit https://github.com/tensorflow/tensorflow/commit/f45874b4a969a60fa9ced86e6769012e4912b5ca broke existing GPU builds on MacOS with CUDA 7.5

It nows fails
`I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.1.dylib; dlerror: dlopen(libcuda.1.dylib, 5): image not found
`
A work-around is to create a symlink

```
cd /usr/local/cuda/lib/
ln -s libcuda.dylib libcuda.1.dylib

```
 cc @3XX0 and @rdadolf 

life is hard
 I think the .1 makes more sense, so I'd rather someone figure out a way to special-case this for OS X rather than change it back.
 @3XX0: Can you clarify what things TensorFlow should do and what needs to be fixed elsewhere?  I've seen your comments in other github threads but I've had trouble understanding what your full set of recommendations is (for cuda, cudnn, etc).

Also @zheng-xq for TF opinions
 I agree add ".1" for OSX is a good idea. 

Adding the stream-executor owner, @henline here. 
  @kirilg, @nfiedel is there still something amiss with the session_bundle?
 @dansbecker can you tell us which bazel version you're using?
  @ebrevdo is there a conversion howto? That would be helpful.
 The RNN api wasn't documented until 0.9, so it was liable to change.  Now that it is documented and "officially released", it has the same guarantees as the rest of core.

That said, none of the functionality has changed - it was just moved.

You can use tensorflow.nn.rnn_cell.*, tensorflow.nn.rnn(), tensorflow.nn.dynamic_rnn(), etc.  You do not have to call the cell directly.

initial_state is available to tensorflow.nn.rnn()

@martinwicke for whatever reason, the rnn documentation does not show up [here](https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html).  Do these api docs need to be updated?
 r0.9 docs reflect changes to the docs made before we branched. Are those
changes on the r0.9 branch?
On Thu, Jun 16, 2016 at 20:25 ebrevdo notifications@github.com wrote:

> The RNN api wasn't documented until 0.9, so it was liable to change. Now
> that it is documented and "officially released", it has the same guarantees
> as the rest of core.
> 
> That said, none of the functionality has changed - it was just moved.
> 
> You can use tensorflow.nn.rnn_cell.*, tensorflow.nn.rnn(),
> tensorflow.nn.dynamic_rnn(), etc. You do not have to call the cell directly.
> 
> initial_state is available to tensorflow.nn.rnn()
> 
> @martinwicke https://github.com/martinwicke for whatever reason, the
> rnn documentation does not show up here
> https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html. Do
> these api docs need to be updated?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2905#issuecomment-226671773,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_eUHKwyTKg2qhJinJ2MSVlLbFb_Lks5qMhPBgaJpZM4I3kCq
> .
 The documentation will be fixed by #2933 
  @tensorflow-jenkins test this please.
 Thanks for the cleanup!
  What version is this about? r0.9? master? If master, at which commit?
 @nfiedel, we disabled this is in 0.9, and I thought this was fixed in master, but seems that maybe it isn't.
 Which version of bazel are you using?
  1. You are repeatedly creating the Saver, that's why you have 8 global_steps. You can fix it by either explicitly creating a new graph tf.Graph(), or reset the default graph by calling tf.reset_default_graph().

If you print out your graph now by calling tf.train.write_graph, you will see that you have 8 copies of what you are building.
1. The saver should only be called once after you have created the graph, and must be called before init = tf.initializes_all_variable() because without arguments the Saver adds operations to the graph.
2. tf.initialize_all_variables() just creates an op. It doesn't initialize anything. You need to explicitly run it by calling

sess.run(init)
or
init.run()

Hope that helps.

Sherry
  Closing since this is effectively a duplicate of #2937.
  Thanks for the report! This should be fixed in #2936, closing now.
  @tensorflow-jenkins test this please.
 Thanks for this cleanup!
  You'll need CUDA 8.0 and latest NVidia drivers. Here are instructions I followed to install them
https://gist.github.com/yaroslavvb/07b2d9a828f82792ebef2a080e814f0f

A small side-effect is that X server will no longer work after this, but you'll be able to run TensorFlow through SSH
 I will consider this resolved. :)
  Apologies about the breakage! I'm working to make the update a bit less of a manual process.

I'll investigate how we can avoid the new zlib dependency (since we don't want additional libraries to link in), and hopefully have an answer soon.
 Thanks everyone! I've tested this again and it should be fixed in #2936, so I'll close this. Comment or reopen if you're still hitting problems.
 Shoot, probably my fault, I'll send a fix.
 I'm actually looking at it now too. It builds fine using the makefile on Linux, so it's not just a new file added. I'm running build_all_ios.sh to repro the problem, and I'll update with more details when I have them.
 Actually, I can't find any reference to those symbols -- my change removed the old APIs and converted to the new ones, so I'm not sure why we're seeing that.
 Great to hear, and no worries! Thanks for the update and the report.
  Could you please cut and paste your complete program and errors? Thanks.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Thanks!

@tensorflow-jenkins test this please
 @craigcitro our internal flags library doesn't do this though, so we're unlikely to ever be able to accept code into our repo that has dashes in the flag name, but I guess it's still useful for everyone else?
 @vrv That was my thought -- actually, flags with dashes work just fine in both cases. The only wrinkle here is that externally, we can refer to `FLAGS.name_with_dashes`, whereas internally it would only be `FLAGS['name-with-dashes'].value`. Given that there _is_ a sane way to make it work in both cases, does that seem reasonable enough?
  Jenkins, test this please.
 Thanks for this PR!

Instead of creating a separate script with summaries, could you replace the existing retrain.py with the one with summaries? This will be easier to review, and to maintain. If we have two copies of the script (one with summaries and one without) they will go out of sync, and become very confusing.
  Did you follow the instructions in [Create the pip package and install](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#create-the-pip-package-and-install)? This step is necessary for your default Python installation to be able to `import tensorflow`.
 That's no problem - I'm glad it's working, and don't feel bad!
  Jenkins, test this please.
  Sorry you're hitting problems! Since DecodeJpeg isn't supported as part of the core, you'll need to strip it out of the graph first. I'm working on a more user-friendly approach, but you should be able to run the strip_unused script on it, something like this:

```
bazel build tensorflow/python/tools:strip_unused && \
bazel-bin/tensorflow/python/tools/strip_unused \
--input_graph=your_retrained_graph.pb \
--output_graph=stripped_graph.pb \
--input_node_names=Mul \
--output_node_names=final_result \
--input_binary=true
```

Let me know if that helps.
 Great! You should just need to update the input and output layer names to "Mul" and "final_result" respectively, here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ios_examples/camera/CameraExampleViewController.mm#L300
 Ah yes! The input sizes need to be 299, not 224. You'll also need to change the mean and std values both to 128. Here's the code I think you'll need:

```
  const int wanted_width = 299;
  const int wanted_height = 299;
  const int wanted_channels = 3;
  const float input_mean = 128.0f;
  const float input_std = 128.0f;
```

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ios_examples/camera/CameraExampleViewController.mm#L272

We will be collecting this in proper documentation soon too, but thanks for testing this out.
  Could you run "nvidia-smi" and report back what GPU is available? 

TensorFlow successfully load alll the Cuda libraries. But failed to find any GPU. We need to make sure GPUs are actually visible there. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAbot can be slow with company CLAs, don't worry about it. 
  @jdoerrie Yep, good catch. Would you have time to send a PR?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  @tensorflow-jenkins test this please
  Indeed
  A very simple test to exercise this code path would be great!
 Woohoo!  Thanks.  @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
  This is definitely useful.  Are you interested in piping this through the parser library?
  @caisq @jendap @gunan Any idea why we are missing this in CI? 

I'm reopening this and changing title appropriately. (I merged the fixing PR, thanks @rdadolf!)
 We have a way to enforce the GPU being used (force_gpu arg for test
session), but we have no test for it. We should make such a test.
On Thu, Jun 16, 2016 at 04:27 Bob Adolf notifications@github.com wrote:

> Thanks, @martinwicke https://github.com/martinwicke.
> 
> Also, I alluded to this in the original post, but I think the CI issue
> boils down to the tests checking for correctness on the output of the test,
> whereas in this case, we actually care that the output was correct _and_
> that the output was generated by code on the GPU. Since TF fails gracefully
> back to the CPU when the cuda libraries aren't found, the tests run along
> happily (and no non-zero return code is generated).
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2865#issuecomment-226458745,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_fy7njvqpSMDp588QYjyBhF1FjCKks5qMTMEgaJpZM4I1wXq
> .
  Ref #2950
 @gideonite @ilai-deutel I pretty much disabled restore code, because it's not really working well with current way of handling the graph. This example should remove usage of restore and just point into the same directory. If you want to change it or I can create a PR with your commits and the change in saving/restoring.
  Could you try running:

sudo pip install protobuf

and retry your command?
  Examples are supposed to cover common use cases. If you do expect your experiments to overflow a 32-bit integer, then yes, please explicitly set it to int64.
  How do you want to change the weights?
 You can do something like:

v = tf.Variable(1.0)
c = tf.constant(2.0)
c2 =  tf.cond(v < c, lambda: tf.constant(3.0), lambda: tf.constant(4.0))
v2 = tf.assign(v, c2)
with tf.Session() as sess:
  sess.run(tf.initialize_all_variables())
  print sess.run(v2)
  Jenkins, test this please.
  Lukasz, ptal?
 I made a code change correcting it (adding output_state to EmbeddingWrapper), so that should be working now on master. Closing, please re-open if it's still not working.
  For d, you can follow instructions here to get profiling information:

http://stackoverflow.com/questions/37751739/tensorflow-code-optimization-strategy/37774430#37774430
 There are a number of operators (especially those working on integer types) which are not registered for GPU, and for some fairly complex reasons this is not likely to change in the near future.

Many of the general questions above are better suited for StackOverflow. Please could you ask them there and tag them with the `tensorflow` tag.
  This is indeed the expected behavior.   Control dependencies only constrain the order of evaluation of the compute ops in your program.  The device to host memcpy operations are executed as _Send/_Recv ops which are automatically inserted when the graph is partitioned between cpu and gpu devices.  

The _Recv ops  have zero inputs and so are schedulable immediately.  The _Send ops (and hence the transfers) are schedulable as soon as the input data has been produced (in this case by Identity ops with names like 'variable/read' which take a single "snapshot" of each variable).  

The order that the transfers happen _should_ have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency.  (You appear to have some _huge_ transfers!)

In the distributed setting, GraphOptions.enable_recv_scheduling can be used to constrain the order of _Recv nodes.  However, this doesn't currently influence intra-machine transfers.

If you really care about hand-constraining the order of the GPU transfers, you could _probably_ write something using a bunch of _additional_ identity ops on the cpu and gpu device and explicitly put control dependencies on these.   

@yuanbyu  might have more to say on this issue?
 > Having said that, would there be scope for improving the scheduling of intra-machine send/recv ops?

In the general case, doing a good job of this requires having an accurate cost model including the computation cost of ops and the shapes of all Tensors.  In the presence of custom ops and dynamically fed tensors (whose shape we often don't know until runtime), there is a lot of missing information.  Even in the presence of full information, computing a good schedule is an expensive algorithm which needs to trade off a lot of variables (e.g. peak GPU memory usage, critical path latency, potential concurrency)  

Having said that, there are probably a bunch of fairly simple heuristics which may work well in simple cases like this... 
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
  This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.
  Jenkins, test this please.
 this is incomplete -- there are a bunch of other places to make this change.

@martinwicke @caisq should we merge the version change from the r0.9 back into master?  Should this have been done first before branching?
 In the past we have merged back after the actual release. There's no reason
not to merge back more often. I'd like the actual version number change to
be a commit on the release branch but that's really only an aesthetic
preference.
On Tue, Jun 14, 2016 at 11:57 Vijay Vasudevan notifications@github.com
wrote:

> this is incomplete -- there are a bunch of other places to make this
> change.
> 
> @martinwicke https://github.com/martinwicke @caisq
> https://github.com/caisq should we merge the version change from the
> r0.9 back into master? Should this have been done first before branching?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2842#issuecomment-225981568,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_Wp9ETWIwRI44jjshRY56JsrzsaYks5qLvmAgaJpZM4I01TI
> .
 We'll revert this, it breaks a bunch of stuff.
 Don't worry. We shouldn't have merged it.
On Fri, Jun 17, 2016 at 11:56 Robin-des-Bois notifications@github.com
wrote:

> I did not realize that the version number should also get updated in all
> these other places. I should have checked that. I was also not sure about
> where to base the pull request.
> 
> I am sorry for your inconvenience.
> 
> —
> You are receiving this because you commented.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2842#issuecomment-226852705,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_eWJpjHUdN-BeGfDeRXrMU0k4uvvks5qMu3igaJpZM4I01TI
> .
  @MInner, the error comes from stream-executor. I will try to loop in some developers to help there. 

Something seems wrong here. If you don't run this script under "gdb", what is the output? Could you post the entire log somewhere, and post the link here? 
 Swapping my action with Jason, who owns stream-executor. Feel free to assign back to me when we feel TensorFlow is at fault here. 

The segfault is suspicious. Well behaving TensorFlow or stream-executor code should check for error conditions and quit with a clear error message. 
 Really adding Jason. +@henline
  Is this request specifically for truncated BPTT?  or something more general?
  Jenkins, please test this
 Jenkins, test this please.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  The problem is that `(None)` evaluates to `None`, and not a `tuple` containing a single element that is `None`. To define a single element tuple, you need to type `(None,)`. 

This is due to Python's syntax, not the placeholder implementation:

``` python
>>> print type(None)
<type 'NoneType'>
>>> print type((None))
<type 'NoneType'>
>>> print type((None,))
<type 'tuple'>
>>> print type([None])
<type 'list'>
>>> print type([None,])
<type 'list'>
```
  I ran `bazel test syntaxnet/... util/utf8/...` and it gave a few errors . It gave me this output:

```
FAIL: //syntaxnet:parser_trainer_test (see /home/me/.cache/bazel/_bazel_rushat/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log).
INFO: Elapsed time: 2179.396s, Critical Path: 1623.00s
//syntaxnet:arc_standard_transitions_test                                PASSED in 0.7s
//syntaxnet:beam_reader_ops_test                                         PASSED in 24.1s
//syntaxnet:graph_builder_test                                           PASSED in 14.6s
//syntaxnet:lexicon_builder_test                                         PASSED in 6.1s
//syntaxnet:parser_features_test                                         PASSED in 5.8s
//syntaxnet:reader_ops_test                                              PASSED in 9.4s
//syntaxnet:sentence_features_test                                       PASSED in 0.2s
//syntaxnet:shared_store_test                                            PASSED in 41.7s
//syntaxnet:tagger_transitions_test                                      PASSED in 5.2s
//syntaxnet:text_formats_test                                            PASSED in 6.1s
//util/utf8:unicodetext_unittest                                         PASSED in 0.4s
//syntaxnet:parser_trainer_test                                          FAILED in 0.5s
  /home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log

Executed 12 out of 12 tests: 11 tests pass and 1 fails locally.
There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.
```

If you want the output of `--test_verbose_timeout_warnings` then please ask

I have no idea what these mean...please help me :pray:

PS: If this is the wrong place to ask, then please direct me where to post this(but kindly suggest an answer)
 Could you please attach the log?

/home/me/.cache/bazel/_bazel_rushat/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log
 This is probably better filed at tensorflow/models, since this is a different repo.
  @MrSlayer02 Eigen::half runs on both CPU and GPU. The code resides in a directory called CUDA purely for historical reasons. Moreover tensorflow 0.9 pulls a recent version of Eigen that contains the CUDA directory whether or not TensorFlow is compiled with CUDA support.

Can you document what you're trying to do as well as the symptoms that you experience ?
 This could be because the makefile used to compile TensorFlow on Raspberry Pi isn't up to date, so we end up pulling a very old version of Eigen instead of the correct one. Can you send me the command(s) you use to launch your build ? I'll take a look.
  Jenkins, test this please.
 Jenkins, test this please.
  _reverse_seq returns a list already. So remove [] from output_feed:

output_feed = rnn._reverse_seq(encoder_inputs,tf.Variable([2,2]))
  I can't reproduce what you're seeing. Could you add a small code snippet to repro? 

```
tf.reset_default_graph()
sess = tf.InteractiveSession()

x = tf.Variable(3.0)
ema = tf.train.ExponentialMovingAverage(0.9)
maintain_averages_op = ema.apply([x])

sess.run(tf.initialize_all_variables())
sess.run(x.assign(x+1.0))
sess.run(maintain_averages_op)
print 'x', sess.run(x)
print 'avg', sess.run(ema.average(x))

```

This returns:

```
x 4.0
avg 3.1
```
  This is on the near-future TODO list.
 We already have interest from about 2 folks internally to implement this.
Please be patient and we'll have something "soonish".

On Sun, Jun 12, 2016 at 8:20 PM, Bin Wang notifications@github.com wrote:

> Where can I see the TODO list? I'm interesting in implement this.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2823#issuecomment-225481836,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtimzwFpoEiihHDl58C8jZYsD0u64Kbks5qLMxhgaJpZM4Iz71c
> .
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We're not quite done with r0.9, we'll merge it when we publish the non-RC.
  Could you please cut and paste the code that you believe is doing this incorrectly?

Could you please also include a unit test demonstrating the problem? Thanks.
  Jenkins, test this please.
  I think I"ve seen this error when "git pull" overwrote files like CROSSTOOL
that ./configure sets up, can you try again after ./configure?

On Sun, Jun 12, 2016 at 9:07 AM, Michael Hofmann notifications@github.com
wrote:

> Hi,
> Installing TensorFlow from source fails on my system. There seems to be
> some Python-related error when building 'half_plus_two'.
> See (hopefully) all relevant output below.
> Environment info
> 
> Operating System:
> 
> lsb_release -a
> No LSB modules are available.
> Distributor ID: Ubuntu
> Description: Ubuntu 16.04 LTS
> Release: 16.04
> Codename: xenial
> 
> uname -a
> Linux the-beast 4.4.0-24-generic #43
> https://github.com/tensorflow/tensorflow/issues/43-Ubuntu SMP Wed Jun 8
> 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
> 
> Installed version of CUDA and cuDNN:
> (please attach the output of ls -l /path/to/cuda/lib/libcud*):
> 
> ls -l /usr/local/cuda-8.0/lib64/libcud*
> -rw-r--r-- 1 root root 560184 Jun 12 16:53
> /usr/local/cuda-8.0/lib64/libcudadevrt.a
> lrwxrwxrwx 1 root root 16 Jun 12 16:53
> /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
> lrwxrwxrwx 1 root root 19 Jun 12 16:53
> /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
> -rwxr-xr-x 1 root root 394472 Jun 12 16:53
> /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
> -rw-r--r-- 1 root root 737516 Jun 12 16:53
> /usr/local/cuda-8.0/lib64/libcudart_static.a
> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54
> /usr/local/cuda-8.0/lib64/libcudnn.so
> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54
> /usr/local/cuda-8.0/lib64/libcudnn.so.5
> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54
> /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5
> -rw-r--r-- 1 root root 68709594 Jun 12 16:54
> /usr/local/cuda-8.0/lib64/libcudnn_static.a
> 
> If installed from sources, provide the commit hash:
> bf83048
> https://github.com/tensorflow/tensorflow/commit/bf83048d5f28b7fa0f39df799bd01a8fc581f5cf
> Steps to reproduce
> 1. Clone TensorFlow and follow instructions to build from source:
>    https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installation-for-linux
> 2. Execute command bazel build -c opt --config=cuda
>    //tensorflow/tools/pip_package:build_pip_package which ultimately fails.
> 
> Logs or other output that would be helpful
> 
> This should be the relevant part:
> 
> michael@the-beast:~/devel/tensorflow$ bazel build -c opt --config=cuda
> //tensorflow/tools/pip_package:build_pip_package --verbose_failures
> WARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/protobuf/WORKSPACE:1:
> Workspace name in
> /home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/external/protobuf/WORKSPACE
> (@__main_) does not match the name given in the repository's definition (
> @protobuf https://github.com/protobuf); this will cause a build error
> in future versions.
> WARNING: /home/michael/devel/tensorflow/util/python/BUILD:11:16: in
> includes attribute of cc_library rule //util/python:python_headers:
> 'python_include' resolves to 'util/python/python_include' not in
> 'third_party'. This will be an error in the future.
> WARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/highwayhash/WORKSPACE:1:
> Workspace name in
> /home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/external/highwayhash/WORKSPACE
> (@__main_) does not match the name given in the repository's definition
> (@highwayhash); this will cause a build error in future versions.
> WARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/re2/WORKSPACE:1:
> Workspace name in
> /home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/external/re2/WORKSPACE
> (@__main_) does not match the name given in the repository's definition
> (@re2); this will cause a build error in future versions.
> INFO: Found 1 target...
> ERROR:
> /home/michael/devel/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1:
> Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two
> failed: bash failed: error executing command
> (cd /home/michael/.cache/bazel/
> 
> _bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow && \
> exec env - \
> PATH=/home/michael/torch/install/bin:/home/michael/devel/jdk1.8.0_77/bin:/home/michael/devel/bazel/output:/usr/local/cuda-8.0/bin:/home/michael/devel/gocode/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
> \ /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh;
> rm -rf /tmp/half_plus_two; /usr/bin/python
> bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two;
> cp -r /tmp/half_plus_two/_
> bazel-out/local_linux-opt/genfiles/tensorflow/contrib/session_bundle/example/half_plus_two'):
> com.google.devtools.build.lib.shell.BadExitStatusException: Process exited
> with status 1. I tensorflow/stream_executor/dso_loader.cc:108] successfully
> opened CUDA library libcublas.so.8.0 locally I
> tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA
> library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:108]
> successfully opened CUDA library libcufft.so.8.0 locally I
> tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA
> library libcuda.so locally I tensorflow/stream_executor/dso_loader.cc:108]
> successfully opened CUDA library libcurand.so.8.0 locally Traceback (most
> recent call last): File
> "/home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/contrib/session_bundle/example/export_half_plus_two.py",
> line 32, in import tensorflow as tf File
> "/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/__init*.py",
> line 23, in
> from tensorflow.python import *
> File "/home/michael/.cache/bazel/
> *bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/__init_*.py",
> line 52, in
> from tensorflow.core.framework.graph_pb2 import *
> File "/home/michael/.cache/bazel/
> 
> _bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py",
> line 16, in from tensorflow.core.framework import attr_value_pb2 as
> tensorflow_dot_core_dot_framework_dot_attr__value__pb2 File
> "/home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/attr_value_pb2.py",
> line 16, in from tensorflow.core.framework import tensor_pb2 as
> tensorflow_dot_core_dot_framework_dot_tensor__pb2 File
> "/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/tensor_pb2.py",
> line 16, in from tensorflow.core.framework import tensor_shape_pb2 as
> tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2 File
> "/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/tensor_shape_pb2.py",
> line 22, in
> serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\"z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02
> \x03(\x0b\x32
> .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03
> \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01
> \x01(\x03\x12\x0c\n\x04name\x18\x02
> \x01(\tB/\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01\x62\x06proto3')
> TypeError: __init_() got an unexpected keyword argument 'syntax'
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 16.639s, Critical Path: 8.93s
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2816, or mute the thread
> https://github.com/notifications/unsubscribe/AABaHLTP5M78Q1FjR7RiIt_HwW-wGriNks5qLC7EgaJpZM4Iz0xs
> .
  Hello ,I am try to run my code on TensorFlow. When I run it, it works fine. But after 30 minutes, I get this error:

```
*** glibc detected *** python3: invalid fastbin entry (free): 0x00007fae00288af0 ***
======= Backtrace: =========
/lib64/libc.so.6[0x3ba6276166]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6Tensor16CopyFromInternalERKS0_RKNS_11TensorShapeE+0x109)[0x7fae8eebecf9]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x1cdb96e)[0x7fae8ee2796e]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x1cd0cf5)[0x7fae8ee1ccf5]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPool4Impl10WorkerLoopEv+0x17b)[0x7fae8ef4639b]
/gpfs/home/chaowei/software/gcc-6.1.0/lib64/libstdc++.so.6(+0xbe232)[0x7faebd7b4232]
/lib64/libpthread.so.0[0x3ba66079d1]
/lib64/libc.so.6(clone+0x6d)[0x3ba62e8b6d]
======= Memory map: ========
00400000-007f8000 r-xp 00000000 00:15 3515267                            /gpfs/home/chaowei/development/python/python3_tf_cpu_vir/bin/python3.4
009f8000-00a69000 rw-p 003f8000 00:15 3515267                            /gpfs/home/chaowei/development/python/python3_tf_cpu_vir/bin/python3.4
00a69000-00a87000 rw-p 00000000 00:00 0 
010b3000-2970f000 rw-p 00000000 00:00 0                                  [heap]
3ba5a00000-3ba5a20000 r-xp 00000000 08:05 18612623                       /lib64/ld-2.12.so
3ba5c1f000-3ba5c20000 r--p 0001f000 08:05 18612623                       /lib64/ld-2.12.so
3ba5c20000-3ba5c21000 rw-p 00020000 08:05 18612623                       /lib64/ld-2.12.so
3ba5c21000-3ba5c22000 rw-p 00000000 00:00 0 
3ba5e00000-3ba5e02000 r-xp 00000000 08:05 18612630                       /lib64/libdl-2.12.so
3ba5e02000-3ba6002000 ---p 00002000 08:05 18612630                       /lib64/libdl-2.12.so
3ba6002000-3ba6003000 r--p 00002000 08:05 18612630                       /lib64/libdl-2.12.so
3ba6003000-3ba6004000 rw-p 00003000 08:05 18612630                       /lib64/libdl-2.12.so
3ba6200000-3ba638b000 r-xp 00000000 08:05 18612624                       /lib64/libc-2.12.so
3ba638b000-3ba658a000 ---p 0018b000 08:05 18612624                       /lib64/libc-2.12.so
3ba658a000-3ba658e000 r--p 0018a000 08:05 18612624                       /lib64/libc-2.12.so
3ba658e000-3ba658f000 rw-p 0018e000 08:05 18612624                       /lib64/libc-2.12.so
3ba658f000-3ba6594000 rw-p 00000000 00:00 0 
3ba6600000-3ba6617000 r-xp 00000000 08:05 18612626                       /lib64/libpthread-2.12.so
3ba6617000-3ba6817000 ---p 00017000 08:05 18612626                       /lib64/libpthread-2.12.so
3ba6817000-3ba6818000 r--p 00017000 08:05 18612626                       /lib64/libpthread-2.12.so
3ba6818000-3ba6819000 rw-p 00018000 08:05 18612626                       /lib64/libpthread-2.12.so
3ba6819000-3ba681d000 rw-p 00000000 00:00 0 
3ba6a00000-3ba6a83000 r-xp 00000000 08:05 18612635                       /lib64/libm-2.12.so
3ba6a83000-3ba6c82000 ---p 00083000 08:05 18612635                       /lib64/libm-2.12.so
3ba6c82000-3ba6c83000 r--p 00082000 08:05 18612635                       /lib64/libm-2.12.so
3ba6c83000-3ba6c84000 rw-p 00083000 08:05 18612635                       /lib64/libm-2.12.so
3ba6e00000-3ba6e07000 r-xp 00000000 08:05 18612627                       /lib64/librt-2.12.so
3ba6e07000-3ba7006000 ---p 00007000 08:05 18612627                       /lib64/librt-2.12.so
3ba7006000-3ba7007000 r--p 00006000 08:05 18612627                       /lib64/librt-2.12.so
3ba7007000-3ba7008000 rw-p 00007000 08:05 18612627                       /lib64/librt-2.12.so
3ba7200000-3ba7215000 r-xp 00000000 08:05 18612638                       /lib64/libz.so.1.2.3
3ba7215000-3ba7414000 ---p 00015000 08:05 18612638                       /lib64/libz.so.1.2.3
3ba7414000-3ba7415000 r--p 00014000 08:05 18612638                       /lib64/libz.so.1.2.3
3ba7415000-3ba7416000 rw-p 00015000 08:05 18612638                       /lib64/libz.so.1.2.3
3ba7600000-3ba763b000 r-xp 00000000 08:05 5769740                        /usr/lib64/libxslt.so.1.1.26
3ba763b000-3ba783b000 ---p 0003b000 08:05 5769740                        /usr/lib64/libxslt.so.1.1.26
3ba783b000-3ba783d000 rw-p 0003b000 08:05 5769740                        /usr/lib64/libxslt.so.1.1.26
3ba7e00000-3ba7e02000 r-xp 00000000 08:05 18612339                       /lib64/libutil-2.12.so
3ba7e02000-3ba8001000 ---p 00002000 08:05 18612339                       /lib64/libutil-2.12.so
3ba8001000-3ba8002000 r--p 00001000 08:05 18612339                       /lib64/libutil-2.12.so
3ba8002000-3ba8003000 rw-p 00002000 08:05 18612339                       /lib64/libutil-2.12.so
3ba8200000-3ba8203000 r-xp 00000000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0
3ba8203000-3ba8402000 ---p 00003000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0
3ba8402000-3ba8403000 r--p 00002000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0
3ba8403000-3ba8404000 rw-p 00003000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0
3bab600000-3bab610000 r-xp 00000000 08:05 18612329                       /lib64/libbz2.so.1.0.4
3bab610000-3bab80f000 ---p 00010000 08:05 18612329                       /lib64/libbz2.so.1.0.4
3bab80f000-3bab811000 rw-p 0000f000 08:05 18612329                       /lib64/libbz2.so.1.0.4
3bace00000-3bacf48000 r-xp 00000000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6
3bacf48000-3bad147000 ---p 00148000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6
3bad147000-3bad151000 rw-p 00147000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6
3bad151000-3bad152000 rw-p 00000000 00:00 0 
7fadb0000000-7fadb3ff7000 rw-p 00000000 00:00 0 
7fadb3ff7000-7fadb4000000 ---p 00000000 00:00 0 
7fadb4000000-7fadb4ac5000 rw-p 00000000 00:00 0 Aborted (core dumped)

```

I don't see this error before when I use TF. It is the first time to see it on running TF and I am not sure weather it's the problem of TF or not. BTW, I am trying to run my code again in order to see if the error can appear again.

The version of TF is 0.8 and the version of gcc is 6.1
 @bnaul Wow, I am also use lstm and of 4 runs of same model in different  parameter. But when I run again, this error disappear.
 Where does the tensorflow binary that you use come from? Did you build it from source? If so, what's the git hash? Or did you download a pip file and install it? If so, which version? 
 @caisq  I downloaded it from github and built it from source. But I am sorry I don't know how to check git hash.
  @danmane  This link still appears broken... 
  Which version of SWIG are you using? Can you try 3.0.8+? 
  Swapping my action with Benoit. All the misaligned memory reads came from Eigen kernels in the attached error messages. 
 @johnfrombluff, my earlier comment only meant to point out which part of the program is triggering the error to my colleague. It didn't imply you did something wrong. 

Your GPU is GTX 750 Ti, which is gm107. It is supported in theory. But it is a low-end GPU, out of which you might not see a very big speedup. 

Since we never saw this problem before, and were unable to reproduce it, the only way to root cause is to ask you to run experiments. However, some steps are not the easiest for users who are not familiar with GPU programming. 

Alternatively, you can try a different GPU. Both Titan-X and GTX 1080 are very popular choices. If you still see the same problem with the latest Cuda driver, Cuda SDK and more recent GPU, we would definitely like to investigate. 
  Maybe it's possible that the reference (slow) implementations are used when not running on Android/ARM.  @petewarden would know more.
 Can you try using https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark and let me know what you get? There are some known optimizations for eight-bit CPU we're working on, but it would be useful to see where the time is going from the profiling logs you'll get from that tool.
  There is no difference. \* is sugar for tf.mul.

tf.mul does backpropagate gradients correctly.
  We have some internal code that does this, am planning to open source it someone this summer.
  The message indicates that the container started successfully. You should now be able to connect to the Jupyter kernel running inside the container.
  This appears to be due to an incorrect iOS Makefile that attempts to link against version d02e6a705c30 instead of version 5f86b31739cd of the Eigen codebase. This is fixed in [pull request 2805](https://github.com/tensorflow/tensorflow/pull/2805/files).
 Closing as fixed.  Please sync to most recent versions, and reopen if the problem reoccurs.
  Can you add a derivation/explanation on how the code correctly implements the Nesterov-style momentum updates? On the face of it, it isn't clear how the correct gradient gets computed so explaining the reparameterizaiton you use and proving it is equivalent to the obvious implementation would be helpful.
 I am going to ICML and I won't get to look at this until I return.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks, looking good.
  It seems reasonable to add a clip_gradient op.  That said, you can always look through the output of tf.gradients(), find the gradient you want to clip, and pass it through a thresholding function before passing it to the optimizer.  That'll do what you want.  See ops/clip_ops.py to get a sense of how one may inspect the set of gradients and add modifications to some/all of them.

Marking as "contributions welcome" for the clip_gradient op.  Happy to review.
  @tensorflow-jenkins test this please
 exp(0) == 1, and this implementation is incorrect.  a correct implementation would basically make one giant dense SparseTensor (and is therefore not very useful).
  @ebrevdo  I think you modified parts of map_fn recently - do you have any idea what might be happening here?
  @caisq Can you comment?
  Are you interested in contributing a fix?
 Wonderful!  For now I'll mark this as "contributions welcome", and feel free to contribute this fix into core and/or any other types of dropout (citing your work, of course), if you like, in tf.contrib.rnn.
  You'll need to provide some details about what you are doing.  The most common hanging issue is that you've created an empty queue and are waiting to dequeue elements from it without any enqueues.
 You could also create all your sessions with
"config.operation_timeout_in_ms=60000" for your ConfigProto so you crash
with DeadlineExceeded instead of hanging forever when your queues get
full/empty

On Fri, Jun 10, 2016 at 2:21 PM, alexatknit notifications@github.com
wrote:

> I do have a queue for preloading data, though the model is hanging after
> everything is executed, meaning that it already successfully executed the
> dequeue node. I'll try it without preloading, to remove any queue ops from
> the graph and see what happens.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2788#issuecomment-225298850,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHAvpGO-lRCwSw2KhmDo8FZxaIn7Fks5qKdVZgaJpZM4IzLWA
> .
 Is this a CUDA-related issue, or is it possible to reproduce with just CPU?  There could be a bug here, but I think we'd need a somewhat minimized test case in order to reproduce on our end.
 @mrry Suggestions?  Seems like this one is going to be hard to debug.  Assigning to you for now.
 (Reassigning to Benoit as my best guess is that it's an Eigen threadpool issue.)

It might be a red herring, but the "busy" thread has a stack that includes the new Eigen threadpool, and perhaps it could be livelocking? @dvyukov, @rmlarsen, or @benoitsteiner would be best placed to comment on whether this is possible.

Just for clarification, when you say "virtual cores" do you mean hyperthreads, or are you running in a virtualized environment (and if so, which one)?

One possible workaround would be to build with the old Eigen threadpool. I believe all you need to do is add

``` c++
#define EIGEN_USE_SIMPLE_THREAD_POOL
```

...wherever the Eigen threadpool is used. At the very least this would include [this line in `threadpool.cc`](https://github.com/tensorflow/tensorflow/blob/7644b3dd001355bf5e3734e541d9955277447601/tensorflow/core/lib/core/threadpool.cc#L18), but I'm not sure what others would need to be modified.
 The cifar model uses local response normalization operation. Since this op doesn't have a GPU implementation it is being run on the CPU and therefore a fairly high average CPU utilization is expected.
However since most of the graph is being processed on the GPU I expect that the threads will often be waiting for actual work to do,  and therefore the stack trace will often show them stuck in sched_yield() or std::condition_variable::wait().

The thing that I can't explain in your stack trace is that we have 10 calls to CopyTensor::ViaDMA running at the same time. Are you creating multiple towers ?
 Most of the CPU to GPU and GPU to CPU copies come from the fact that the exponential moving average variables are all placed on CPU, which causes a lot of small data transfers. Can you try to move it to GPU and see if that helps reduce the likelihood of the GPU hanging ? It will also help the model train faster.
  @yuanbyu we may have to either plumb infer_shape=True through scan/map_fn/etc or just set infer_shape=False everywhere.  what do you think is best?
 We may have pushed a fix for this in an unrelated bugfix to TensorArray.  It hasn't been merged into github yet.  Try this in a couple of days on the version of TF in master and report back.
  The failing file unittest_proto3_arena_lite.proto comes the protobuf library itself, and was added at the same time that lite+proto3 support was added.  So it seems like a mismatch between the version of protobuf downloaded by download_dependencies.sh, and the protoc compiler used to run the unit test - although I would have expected the test to use the protoc built from that code.

Just to check, what does   protoc --version   give for you?

@petewarden
 My usual recommendation is to make sure that you do a host install of the protobuf version you've downloaded, to make sure that the headers, libraries, and protoc binary that's in /usr/\* are compatible with the version we're compiling for Android. This may be overkill, and I'm looking at better ways of handling this (e.g. compiling a local copy of the host protobuf and using that local protoc), but it does simplify things a lot.
  @petewarden  - do you know what the current status is of TF on RaspberryPi?
 It's a bit tricky to build using Bazel, since you need a USB drive plugged in so that in can swap beyond the 1GB of RAM on the Pi. @samjabrahams has a great rundown here:
https://github.com/samjabrahams/tensorflow-on-raspberry-pi

If you only need to run models, not train them, we're have makefile support for the core inference part of TensorFlow here:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples
 Closing since this isn't a specific bug, and RaspberryPi is not an offically supported platform (afaik).  
It makes sense to redirect discussion to https://github.com/samjabrahams/tensorflow-on-raspberry-pi
  @keveman Can you take a look?
@vrv, @martinwicke If real this could be another release blocker.
 If this doesn't happen in 0.8, then yes, it's a blocker.
 @girving taking a look.
 0.8.0 and 0.9.0rc0 get the same accuracy (modulo differences due to random initialization). @kingtaurus thanks for checking the learning rate. 
 @keveman What is that same accuracy?  This tutorial is supposed to get to 99.2% accuracy.  If it's the same model as `models/image/mnist/convolutional.py`, that's what it should be getting.
 The models look identical except Deep MNIST for experts uses 1024
https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html#densely-connected-layer
for first FC layer and Vincent's model uses 512
https://github.com/tensorflow/tensorflow/blob/6431560b7ec3565154cb9cdc9c827db78ccfebe7/tensorflow/models/image/mnist/convolutional.py#L174

Also, Deep MNIST uses Adam, whereas convolutional.py uses momentum
with staircase
learning rate decay
https://github.com/tensorflow/tensorflow/blob/6431560b7ec3565154cb9cdc9c827db78ccfebe7/tensorflow/models/image/mnist/convolutional.py#L245

On Fri, Jun 10, 2016 at 3:24 PM, Geoffrey Irving notifications@github.com
wrote:

> @keveman https://github.com/keveman What is that same accuracy? This
> tutorial is supposed to get to 99.2% accuracy. If it's the same model as
> models/image/mnist/convolutional.py, that's what it should be getting.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2781#issuecomment-225310626,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHCP2OjbIFIlkinllWA4jlh27W5Chks5qKeQvgaJpZM4Iy5CE
> .
 @vincentvanhoucke Should the mnist for experts tutorial be the same model as `models/image/mnist/convolutional.py`?

@keveman When you say the accuracy is the same, is it close to 99.2%?
 @girving No, both were close to 90%
 @keveman We should probably fix the tutorial to be 99% then, unless there is a compelling reason to make it 10x worse than `models/image/mnist/convolutional.py`.
 Good catch @kingtaurus. ~99% accuracy confirmed with TF 0.9.
  @ebrevdo Can you comment?  Is `output_size` a new or old thing?
 This is a sort of known issue. Have you tried performing the embedding lookup on all time steps before calling rnn?  Or are you implementing a seq2seq decoder?
 This should have been fixed by @lukaszkaiser recently.  Try using the HEAD master branch.
  Building from master builds "head" which is newer than 0.9.0

On Fri, Jun 10, 2016 at 2:23 AM, JIA Pei notifications@github.com wrote:

> Environment info
> 
> Operating System: Ubuntu 16.04
> 
> Installed version of CUDA and cuDNN:
> cuda 7.5
> cudnn 5.0.5
> 
> Build from source, tensorflow-git-master
> 
> Just use bazel to build tensorflow, which built out a version 0.8.0,
> instead of 0.9.0 ....
> 
> Shouldn't current git master build out 0.9.0, instead of 0.8.0?
> 
> Cheers
> Pei
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2778, or mute the thread
> https://github.com/notifications/unsubscribe/AABaHFnqyuOysiI0m0iMzMSSPxPQQA_xks5qKS0WgaJpZM4IyxXa
> .
 All the binaries you build should be build at version "head" where do u see
0.8?
On Jun 10, 2016 11:26 AM, "JIA Pei" notifications@github.com wrote:

> @yaroslavvb https://github.com/yaroslavvb
> Weird that I just git clone tensorflow, and have it built with bazel
> 3.0.0, only tensorflow-0.8.0 has been built out....
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2778#issuecomment-225259299,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHBpeOFIkhXeYs36Yu3qEOR72TlS5ks5qKaxOgaJpZM4IyxXa
> .
 @jiapei100 The source version will say 0.8 until we release 0.9; currently we're at 0.9.0rc0 < 0.9.
  This sounds like a system configuration issue with either Python or CUDA, especially the bit where PyObject_Malloc crashes.  Do you have multiple versions of Python installed on your machine?  What information did you give to configure?
 It would also be good if you could run "thread all apply backtrace" -- would be good to see where the TensorFlow C++ code is when you get the malloc failure.
  @concretevitamin Looks like `tf.multinomial` doesn't subtract off the maximum logit before taking exponentials.  Can you fix it to do that?  The undocumented error case is also bad; not sure where in the code it's doing that.
 I'm looking into this.
 I pushed a fix via commit [de6ce1de08](https://github.com/tensorflow/tensorflow/commit/de6ce1de08ea97d599687fbbe5196ca4af5232ae).  Please feel free to re-open if related issues come up.  Thanks!
 @concretevitamin In future, please automatically close issues from the commit by saying "Fixes #2774" or similar: https://help.github.com/articles/closing-issues-via-commit-messages
 Yep, if you look at my commit message, I included something similar but
slightly more complicated for Github to grasp ("fixes github issue
#????").  Will make it simpler next time :)

On Thu, Jun 16, 2016 at 11:40 PM, Geoffrey Irving notifications@github.com
wrote:

> @concretevitamin https://github.com/concretevitamin In future, please
> automatically close issues from the commit by saying "Fixes #2774
> https://github.com/tensorflow/tensorflow/issues/2774" or similar:
> https://help.github.com/articles/closing-issues-via-commit-messages
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2774#issuecomment-226524821,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAkLHpDHlnUsuzAfX927wAtVFFh1Ht2Lks5qMW5hgaJpZM4IyloD
> .
  @amineHorseman Could you give run this through gdb and give us a stack trace?  It's not enough information to debug as is.
 It looks like your library path doesn't include `libcudart.7.5.dylib`.
  We wouldn't be able to reproduce this since it involves Windows, but it could be a path issue (maybe LD_LIBRARY_PATH?).
  This is expected.

ScatterUpdate is a stateful operation on a variable.  If you use operations like this in your model then you will not be able to automatically compute the gradient. (You will notice that none of the state related ops have an associated gradient function.)

If you would like to discuss this limitation further, please feel free to ask on StackOverflow and tag it with the `tensorflow` tag.
  Fixed by [f36659](https://github.com/tensorflow/tensorflow/commit/f36659f555027481788d14cc388cec1071a3182a).
  The easiest way to work around this is to specify a list of explicit variable names when creating your `tf.train.Saver()`. Creating a `tf.train.Saver` with no arguments is equivalent to doing:

``` python
saver = tf.train.Saver({v.op.name: v for v in tf.all_variables()})
```

The dictionary keys correspond to the names in the checkpoint file that will be read/written by the saver. You can customize the dictionary so that it matches your old checkpoint. For example:

``` python
names_to_vars = {v.op.name: v for v in tf.all_variables()}
bias_var = names_to_vars["model/attention_decoder/Attention_0/fully_connected/biases"]
names_to_vars["model/attention_decoder/Attention_0/fully_connected/bias"] = bias_var
del names_to_vars["model/attention_decoder/Attention_0/fully_connected/biases"]
saver = tf.train.Saver(var_list=names_to_vars)
```

PS. This is probably more appropriate as a Stack Overflow question, rather than a GitHub issue.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Assigning to RNN API people -- I suspect they are going to push back on these changes without a very compelling reason.  In general, our base libraries should strive to be simple and this seems to be trying to do a little too much.  But we'll see what they say.
 I'm afraid I don't understand the problem this PR is attempting to solve. Initializers are part of variable scope, and all RNN cells rely on this mechanism. If you want to change the initializer, you can set your own function for that in variable scope. Is that mechanism not flexible enough? Why can't you write your own (maybe complex) initializer function and just pass it as initializer to variable scope, so it gets used in all RNN cells?
 It looks like what you're proposing is kind-of a new cell. Maybe a separate cell class for that would be more appropriate? It would be clear then that these are new designs rather than just parameters to re-set initializers -- I think you might prefer that too, right?
 If you put it into contrib, you can also use the linear operators in tf.contrib.layers (which we can't move to because rnn is in core).
 @ebrevdo if you don't see this eventually making it into core, it's best that it lives as a useful library outside of this repo.
 This does seem to be very specialized code that belongs in its own repository.
 Closing this for now. If you feel strongly that this belongs in tf.contrib.rnn, consider resubmitting a pr to that location and meeting the python pep8 guidelines.
  Can you elaborate?  TensorFlow is already a dataflow system.  Are you imagining building a separate repository on top of TensorFlow with alternate semantics?
  @ebrevdo Do you think this is a reasonable approach?  I don't know enough about the API to answer.
 Please see my comment on the pull request. Closing this for now, feel free to reopen once you've decided on a way forward.
  @tensorflow-jenkins test this please
  W'hat is the output of `python -c "import tensorflow; print(tensorflow.__version__)"`?
 The problem is possibly caused by Apache or Flask not providing the necessary value of `LD_LIBRARY_PATH` to your Flask app. There are a couple of suggestions in [this Stack Overflow answer](http://stackoverflow.com/a/37731516/3574081).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @ilblackdragon @martinwicke  are there really no tests for this? :)
 I feel like some test should be written that exercises this behavior, but I'm not too familiar with this code so I've assigned those who might know more.
 That's why we're having a special event next week to add tests. :/
 @vrv @martinwicke Let's start with class weights were tested until we stopped supporting them. (see tensorflow/contrib/learn/python/learn/tests/base_test.py?l=58) and this before was a deprecation warning which wasn't failing them.

@tensorflow-jenkins test this please!
 PS. @vrv @martinwicke  really need that test coverage showing here if you really want to keep test coverage up. Jenkins will do
 @ilblackdragon the code is wrong and there is no test for it, so we need a test to validate this is workign as intended before accepting.  Can you suggest a good place for a test?
 We deprecated it in our interface so it's quiet hard to test :)
 @petrjanda you can implement test that uses custom Estimator with class weight - that should hit this code.
 you probably want to revert this, the check is the reverse of what you had before
  Some of this is covered #206.  For single variables and unordered sets of elements `tf.scatter_update` is the right thing, and #206 proposes `tf.assign_slice_*` which cover what you want.  Our hope is to make most of these available with something like `x[i].assign(...)`, but there are a few steps before we get there.

If your satisfied with #206 I'll close as duplicate, but let me know if not!
  Jenkins, test this please!
  Are you trying to run a Tensorflow server on Android? Currently we only support the named Android libraries such as //tensorflow/core:android_tensorflow_lib -- it's not expected that others will compile for Android.

Also, as a note, Bazel 0.2.3 and below do not support Android NDK11. The next release of Bazel should, though.
  @martinwicke This seems like a Bazel issue.  Have you seen it before?
 also pinging @damienmg and @kchodorow  (did something change in bazel 0.2.3?)
 @damienmg could we have a sh_test that simply runs build_pip_package? That
should have caught this one at least, right?

On Fri, Jun 10, 2016 at 6:50 AM samfux84 notifications@github.com wrote:

> I have this version and encountered the problem with the missing
> "external" directory.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2751#issuecomment-225187004,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_dDhd9UQSPXzyzw0j6NFR0pKOEACks5qKWupgaJpZM4IxqNV
> .
 Good question. What's the easiest way to do that?
On Fri, Jun 10, 2016 at 08:32 Damien Martin-Guillerez <
notifications@github.com> wrote:

> Yes this is possible,
> 
> By the way,
> bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles is a
> bit weird, we generally use $PYTHON_RUNFILES if it exisrts or $0.runfiles
> instead. Every other file should be included as inputs of the target.
> Seeing the build_pip_package.sh script I actually wonder why it is not a
> build step for bazel?
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2751#issuecomment-225215659,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_cnd4UVgiUsuScvDXK885sk-jyMgks5qKYOpgaJpZM4IxqNV
> .
 Awesome! That would be a great improvement of our installation process.
On Fri, Jun 10, 2016 at 09:35 Damien Martin-Guillerez <
notifications@github.com> wrote:

> (sorry I have to leave, my fast test show it should not be too hard, I'll
> try to send a PR for doing so on monday).
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2751#issuecomment-225232212,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_btKNE1mBCo1rDEALu5YVfh7zeLOks5qKZJjgaJpZM4IxqNV
> .
 Can you check your bazel version just to be sure nothing is getting picked up by accident?
 @damienmg do we effectively require bazel 0.3 at this point? 
  @cgarciae That sounds like a great contribution.  What do you mean by hosting a patch?  It seems like it would be best as a separate repository.
 @cgarciae We wouldn't be able to support it, but it's possible it could go in `tf.contrib`.  @martinwicke What do you think?
 Since we cannot support it (and we would like to avoid confusion about that), it should not go into contrib. For now, keeping it in your repo makes the most sense. This is somewhat similar to PrettyTensor, but the DSL aspect is certainly nice -- it's been requested a lot.
 It is very cool. I have to look at the code more, we have always had issues
with special casing ops that don't work well with chaining (first arg not
what you expect, etc).
On Thu, Jun 9, 2016 at 10:57 cgarciae notifications@github.com wrote:

> 1. I'll maintain it for the moment but leave the invitation open.
> 2. @martinwicke https://github.com/martinwicke Yeah, its actually
>    inspired by PrettyTensor (initially) but with some changes, the original
>    README had this comment
> 
> TensorBuilder takes inspiration from prettytensor
> https://github.com/google/prettytensor but its internals are simpler,
> its API is smaller but equally powerfull, its branching mechanism is more
> expresive and doesn't break the fluent API, and its immutable nature helps
> avoid a lot of conceptual complexity.
> 
> A lot of problems I found with PrettyTensor is that its API was mutable,
> where as Tensor operations are immutable. So TensorBuilder is immutable,
> this makes creating an arbitrary number of branches trivial, you also don't
> need the Templates concept to specify a procedure, you just use a plain old
> python function from Builder to Builder.
> 
> In the last release I removed the core functionality for creating layers
> and delegated that to libraries through patches. Now TensorBuilder's focus
> is to enable Tensor-based libraries with nice syntax, branching
> capabilities, and the DSL.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2750#issuecomment-224975942,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_ci99ZcbgWdIdm4sLRbY59YkA6jEks5qKFQfgaJpZM4Ixorg
> .
 Closing the issue for now since it isn't likely to go in the core `tensorflow` repo, but we'd be happy to point to it from the community resources page once it's up and running.
  @tensorflow-jenkins test this please
 I had to add the pthread library on the Raspberry Pi too (though I'm not sure why some distros don't require it), so this additional change looks good to me. Thanks Gregory, I'll merge it once the tests pass.
  @yangmch @Anjum48 tf.learn and tensorflow in general are better used with named arguments.
For example in this case you are passing `ValidationMonitor` as number of steps.
If you should be `classifier.fit(x=X_train, y=y_train, steps=100, logdir='/path/', monitors=[ValidationMonitor])`.

Please let me know if this fixes your issue. Also can you point me where you took this usage from? [This example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/iris_val_based_early_stopping.py#L47) shows it correctly, so if there is another example somewhere we should fix it.
  Implementation of immediate mode execution in TensorFlow. The idea is to wrap original tensorflow API, but provide session/run management logic so that commands execute immediately, and graph caching to avoid modifying graph when same op is run repeatedly. Data is kept in TensorFlow runtime whenever possible using persistent tensors and transferred to Python runtime on demand when needed for printing/control flow.

```
import tensorflow as tf
from tensorflow.contrib import immediate

tfi = immediate.Env(tf).tf        # wraps "tf" namespace, saves it as "tfi"
val1 = tfi.ones((3,))             # creates tensor on GPU
val2 = val1 + val1                # runs into tf.add, keeps result on GPU
val3 = tfi.ones((2, 2, 2, 2))
val4 = tfi.nn.conv2d(val3, val3, [1, 1, 1, 1], "VALID")   # run CuDNN conv2d
if (tfi.reduce_sum(val3)>0.):     # runs reduce_sum on GPU, transfers bool to CPU
  print(val4)                     # transfers whole tensor to CPU for display

```

This is a single commit rebase of a previous pull request from https://github.com/tensorflow/tensorflow/pull/2346

@keveman @yuanbyu 
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 https://github.com/edx/edx-platform/wiki/How-to-Rebase-a-Pull-Request
 CLAs look good, thanks!

<!-- ok -->
 I tried the following :

```
import tensorflow as tf
from tensorflow.contrib import immediate
tfi = immediate.Env(tf).tf

val1 = tfi.ones((3,))
val2 = val1 + val1
```

and got :

```
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/tmp/imm_venv/local/lib/python2.7/site-packages/tensorflow/contrib/immediate/python/immediate/itensor.py", line 103, in __add__
    return self.env.tf.add(self, other)
  File "/tmp/imm_venv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py", line 70, in add
    result = _op_def_lib.apply_op("Add", x=x, y=y, name=name)
  File "/tmp/imm_venv/local/lib/python2.7/site-packages/tensorflow/contrib/immediate/python/immediate/op.py", line 158, in apply_op
    name=op_name)
TypeError: get_session_tensor() takes at least 2 arguments (2 given)
```

Do you think you forgot to commit and push some changes?
 It looks like https://github.com/tensorflow/tensorflow/commit/bf8084b71006123575c6416d69efdd00c13fe41c has changed the signature of `get_session_tensor`, let me update it with new sig
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 So I just sequence below which brought a bunch of extra commits. [hood-devel](https://github.com/yaroslavvb/tensorflow/tree/hood-devel) is my develoment branch which is one commit ahead of master, and hood is my pull request branch, and merging hood-devel into hood like below pulls new commits into pull request for some reason

```
git checkout hood-devel
git merge -s ours hood
git checkout hood
git merge hood-devel
git push
```

OK, seems like merge always needs to be followed by rebase, TLDR; to squash, rebase and overwrite all conflicting files by version in $mybranch

```
git checkout $mybranch
git remote add tfmain https://github.com/tensorflow/tensorflow.git
git fetch --all
export merge_root=`git merge-base $mybranch tfmain/master`
git rebase -i $merge_root
git rebase -Xtheirs tfmain/master
```
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 There's a failure in `//tensorflow/tools/test:check_futures_test` in `Linux CPU Tests (Python 3) — FAILURE`. I can run it locally with `bazel test //tensorflow/tools/test:check_futures_test` and it passes. Any suggestions how I can see the logs/reproduce the failure?
 I think I fixed test_util issue, can someone trigger tests again? I believe "test_util" failure is caused by "bazel test" not being hermetic. If you build pip package and do `python setup.py develop`, then future `bazel test` will prefer try to grab python files from pip-package path, rather than dependencies specified for the test in `BUILD` file . Work-around is to make sure all Python files make it into pip package. The `check_futures_test` ones I fixed by running "check_futures_test" as a binary, not as `bazel test` and editing source to specify new `BASE_DIR`
 Jenkins, test this please.
 Tweaked/disabled some failing tests caused by Jenkins environment being different from mine ([issue #2844](https://github.com/tensorflow/tensorflow/issues/2844) ). Didn't have any luck debugging 2to3 failure in Python3 compatibility test (bazel [issue #1393](https://github.com/bazelbuild/bazel/issues/1393)), but this is ready for another Jenkins run
 Jenkins, test this please.
 On this [ci.tensorflow.org](http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/731/consoleFull) I see failure in `tensor_handle_test`, but it doesn't give any details, is there a way to see the log file linked `/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-py3-opt/testlogs/tensorflow/contrib/immediate/tensor_handle_test/test.log` ?
 This kind of error is usually because you don't have srcs_version = "PY2AND3" on some python target. I see a bunch of cases in your BUILD files, so maybe that's it?
 @martinwicke maybe, although that particular test already had `PY2AND3` on its deps. I've added it on all targets anyway, could you trigger the tests again?
 Jenkins, test this please.
 There was [internal commit](https://github.com/tensorflow/tensorflow/commit/a558c6e3b38846727873b5afbbc3ba309ae5dff5) yesterday that moved some libraries around and broke immediate, just updated it with the fix. PS: I'm going traveling at the end of next week, let me know if there's something I can do to help get this in before it bit rots.
 Jenkins, test this please.
 @yaroslavvb Thanks for working on this and sending the PR. Imperative mode of execution is a really important feature that many users have asked for. While your implementation is impressive, it doesn't support a major feature that users ask for and we would like to get working, namely, automatic differentiation via `tf.gradients`. We believe the best way to implement the imperative mode that is efficient and would allow automatic differentiation to work out of the box would need some significant support in the backend. We are working on adding that support currently. I will keep you posted on its progress.
 So is the gradient support an alternative to this implementation, or is it something that can be built on top?
 It's an alternative implementation.
  I've seen this happen when the build failed for some reason. Do you have
any errors when you do "bazel build"?

Also you could try a sample test to see if anything works at all, ie:
bazel test -c opt //tensorflow/python:graph_util_test

On Wed, Jun 8, 2016 at 5:53 PM, ss32 notifications@github.com wrote:

> Environment info
> 
> Operating System: Linux Mint 17 (Ubuntu 14.04)
> 
> Installed version of CUDA and cuDNN:
> N/A - running CPU mode only
> 
> If installed from binary pip package, provide:
> 1. Which pip package you installed.
> 2. The output from python -c "import tensorflow;
>    print(tensorflow.**version**)".
> 
> $ python -c "import tensorflow; print(tensorflow.**version**)"
> Traceback (most recent call last):
>   File "<string>", line 1, in <module>
>   File "tensorflow/**init**.py", line 23, in <module>
>     from tensorflow.python import *
>   File "tensorflow/python/**init**.py", line 48, in <module>
>     from tensorflow.python import pywrap_tensorflow
> ImportError: cannot import name pywrap_tensorflow
> 
> If installed from sources, provide the commit hash:
> 
> $ git rev-parse HEAD
> b2a812b962f8088a8256d4e6271b9cd425936446
> 
> Steps to reproduce
> 1. In python: import tensorflow 1a. Alternatively in python: import
>    pywraper_tensorflow
> 
> What have you tried?
> 1. pip install, pip uninstall/reinstall, built from source
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2746, or mute the thread
> https://github.com/notifications/unsubscribe/AABaHFxHqyPAzkuS9Horc_GFHVLAOAWyks5qJ2P2gaJpZM4Ixi2T
> .
 @ss32 Did I read that right that your build failed?  What was the error?
 @ss32 Your traceback is missing the bottom part with the actual error.

Possibly related -- `pip install` interacts with `bazel test`. In particular, if you do "pip install" to setup TensorFlow for development in _python_build, then `bazel test` will look inside _python_build for dependencies. You can recover the proper behavior by moving _python_build to _python_build.disabled
 Sorry, the command should be
bazel test -c opt//tensorflow/python:graph_util_test --test_output=streamed

For reference, I just did installation from head, and it worked, here's the
full sequence of commands I used

# start new GPU TensorFlow build

mkdir /Users/yaroslavvb/tfimmediate_macbook
cd /Users/yaroslavvb/tfimmediate_macbook
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow

# Remove files that "./configure" touches from index

git update-index --skip-worktree
tensorflow/core/common_runtime/gpu/gpu_device.cc
git update-index --skip-worktree tensorflow/stream_executor/dso_loader.cc
git update-index --skip-worktree
third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc
git update-index --skip-worktree third_party/gpus/cuda/platform.bzl

./configure
host compiler: default
cuda sdk version: 7.5
location where tool is installed (/usr/local/cuda):
cudnn version: 5
where cudnn is installed:
compute capability: 3.0

bazel build -c opt --config=cuda tensorflow/...

On Thu, Jun 9, 2016 at 10:39 AM, ss32 notifications@github.com wrote:

> Full output of
> 
> bazel test -c opt//tensorflow/python:graph_util_test`
> 
> WARNING: /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/external/protobuf/WORKSPACE:1: Workspace name in /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.
> WARNING: /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/external/re2/WORKSPACE:1: Workspace name in /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.
> WARNING: /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/external/highwayhash/WORKSPACE:1: Workspace name in /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.
> WARNING: /home/user/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.
> INFO: Found 1 test target...
> FAIL: //tensorflow/python:graph_util_test (see /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/tensorflow/bazel-out/local-opt/testlogs/tensorflow/python/graph_util_test/test.log).
> Target //tensorflow/python:graph_util_test up-to-date:
>   bazel-bin/tensorflow/python/graph_util_test
> INFO: Elapsed time: 12.257s, Critical Path: 4.60s
> //tensorflow/python:graph_util_test                                      FAILED in 0.6s
>   /home/user/.cache/bazel/_bazel_user/f1b63f8b8ef5ddbd718e9d3afcdbe12a/tensorflow/bazel-out/local-opt/testlogs/tensorflow/python/graph_util_test/test.log
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2746#issuecomment-224970478,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHJgPOU2DB4KOP942x1v4yN2foV6Zks5qKE_OgaJpZM4Ixi2T
> .
 @ss32 what version of Python are you using? `is_abstract` seems like something that Python should provide
  What error message are you seeing?
 @jbohart Could you give me the unmangled traceback?  Somehow you've managed to strip away all the file information showing what code is involved.
 @ispirmustafa: Is `regressor.fit` supposed to take a steps argument?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Awesome, thanks for putting this together! One minor request - could you add a one-line comment in both places explaining why we're grabbing the hash from the file, something like "Pull the current Eigen version name from the Bazel build file"? Thanks again for doing this!
 unfortunately there are some conflicts :(
 Thanks Gregory, and apologies, I'm guessing my manual PR updating the Eigen hash caused the conflicts.
  It looks like Eigen doesn't exist.  Might that be the problem?
 That hash looks different than from your message above, which says 0c0b79ecd74c.
 @martinwicke Do we support the Makefile build actively, or is this contributions welcome?
 FYI @petewarden

It's contributions welcome. Definitely since we cannot guarantee that a
different version of eigen will work at all. In fact it's quite likely it
will not (or work slowly).
On Wed, Jun 8, 2016 at 16:36 Geoffrey Irving notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke Do we support the Makefile
> build actively, or is this contributions welcome?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2742#issuecomment-224762880,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_bBDHVepl5QBpiSamg8hinQtjQnuks5qJ1HxgaJpZM4IxdOc
> .
 Sorry about the problem! I have a pending PR at https://github.com/tensorflow/tensorflow/pull/2743 that fixes this particular issue. Longer-term, I want to automatically grab the checked-in Eigen version, maybe just by grepping the Bazel workspace file that holds the hash.
  Sorry, there are conflicts in this file now, can you rebase?  This looks good, though it would be a good idea one day to avoid having duplicate copies of this text -- maybe one should reference the other.
 @hsaputra I'm sorry for not looking at this sooner. My bad!

We will merge #2795 to make it into r0.9. Please take a look at it and rebase your PR on it. There is still the typo and the os_setup.md may deserve more words about gpu docker than #2795 contains.
  This is great. Can you add tests for this? I think having the ability to use static checks is useful (sometimes), and backwards compatibility is nice, so I don't think you should remove he static checks.
 That would mean that tensors of unknown size will pass all the static tests? That's a good idea. I think the danger is minimal here. The errors in the resize kernels are not too bad anyway.
 @gaohuazuo you'll have to ping us when this is ready for review
 Thanks! Added bunch of comments. Mainly: Some doc nits, some nit nits, and, could you add another test that assertRaises appropriately if you pass in illegal values, and with also asserts that if you pass in tensors which have illegal values it still fails (although not with ValueError, but probably with OpError).
  Did you run `configure`?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Assigning to tf learn people, since this seems like a bug they should fix instead?
 @ilblackdragon this will likely break some tests. At the very least this change would need some changes to the examples? Maybe it's better to adjust the mnist tutorial text to change the reference to the input.
 @tensorflow-jenkins test this please!

@martinwicke not sure if it will break tests, let's see. Indeed changing the tutorial text makes sense.
 Jenkins, test this please? For real?
 Indeed, some failures unfortunately.  For example:

FAIL: //tensorflow/examples/tutorials/mnist:fully_connected_feed_test (see /private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/testlogs/tensorflow/examples/tutorials/mnist/fully_connected_feed_test/test.log).
INFO: From Testing //tensorflow/examples/tutorials/mnist:fully_connected_feed_test:
==================== Test output for //tensorflow/examples/tutorials/mnist:fully_connected_feed_test:
Traceback (most recent call last):
  File "/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/examples/tutorials/mnist/fully_connected_feed_test.runfiles/tensorflow/examples/tutorials/mnist/fully_connected_feed.py", line 27, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File "/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/examples/tutorials/mnist/fully_connected_feed_test.runfiles/tensorflow/examples/tutorials/mnist/input_data.py", line 30, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
ImportError: cannot import name input_data
 Sounds good -- we'll close this request.  Feel free to send us a PR to correct our tutorials as you best see fit!
  Do you get deterministic results if you run this on CPU? Results of optimized GPU computations for NN ops are usually a little bit non-deterministic, I think it's a nature of how modern GPUs work, @zheng-xq may have more understanding of this
 On GPU, small amount of non-deterministic results is expected. TensorFlow uses the Eigen library, which uses Cuda atomics to implement reduction operations, such as tf.reduce_sum etc. Those operations are non-determnistical. Each operation can introduce a small difference. If your model is not stable, it could accumulate into large errors, after many steps. 

If you see a large difference, after one or two operations, it would be problematic. Otherwise, it is somewhat expected. Regularizers such as dropout helps the model tolerate that. 
 This is expected behavior: see #2652 for more discussion.
 Reopened: We'd be happy to accept a PR adding a note to this effect.
 @shiviser Let us know what you find out!
 @shiviser, that particular kernel was fixed to be determnistics. I believe TensorFlow picked up the same fix. However, other Cudnn conv algorithms are still non-deterministic, since they use atomics inherently. Most frameworks may encounter them depending on the input and kernel shapes. 

That being said, if your investigation reveals something else that is causing the problem, a PR to address the doc and the code is welcome. 
  Given limited resources, this is not something which we are currently planning to work on.
  Does `tf.Graph.get_operations` or `tf.Graph.get_operation_by_name` work for you?
 @danmane Care to comment?  Should we mark this contributions welcome? 
 I'm assigning nikhil and daniel to take a look, since it's a graph explorer question. But I also pre-emptively marked it contributions welcome...
  It will be very helpful if someone can post a small case to reproduce the problem. 
 @abezuglov, does the memory leak only happen with multi-GPU? 

It will be great to have a repro with single GPU. If not, it is okay to have a repro case with multi-GPU. 
 @osdf @neggert @abezuglov @jihunchoi @nsuke Just to gather some info here: What is your NVIDIA driver version (see nvidia-smi output)?
  @vrv: Any suggestions here?
 What version of tensorflow are you using?  Please fill out the issues template instead of erasing it, particularly when it comes to issues with GPUs.
 Actually, I'm not sure what's going on, even if you told me the version of TF you were using.  In one case you are running outof GPU memory, in the other case we're getting an error from cudnn that tells me nothing about what happened (INTERNAL ERROR).  I wish error messages were more informative :(
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Thanks for this -- though you should make the fix in the original documentation: https://github.com/tensorflow/tensorflow/blob/9078909131d5e91e8bd1878eac54136885656043/tensorflow/core/ops/array_ops.cc#L1718  since the others are automatically generated.
 No problem -- you can drop the change from ops.pbtxt -- we auto generate that afterwards.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please.
 @tensorflow-jenkins test this please. (Alas we lost the results from the last run....)
 It would be nice to mention that the name of the attribute has changed in RELEASE.md ... people who use named arguments are going to break (even though it doesn't make sense for them to for a one argument function).
 Or maybe it's not worth changing the name right now and just leave it as 'image' -- the documentation should hopefully be sufficient.
 Also, shouldn't there be a test of this behavior?
  Thanks! We are fixing this internally (upstream) since it requires updates to other source files as well. Will close this issue once the internal change surfaces as a commit here and will reference it.

Thanks for finding this and bringing it to our attention!
  Please could you provide more detail about what you are trying to do here, in particular could you provide the code you are trying to 'freeze' and the exact command lines used to build and execute both the 'fast' and 'slow' versions.

@petewarden  - could you please take a look at this once the above information is provided.
  The API docs will be updated soon; as we _just_ released the RNN docs in HEAD.  Not sure if this will end up in 0.9 or the next version.  @martinwicke?
 Can you check the 0.9 docs? If the docs are there they're there.
On Sun, Jun 12, 2016 at 20:12 ebrevdo notifications@github.com wrote:

> The API docs will be updated soon; as we _just_ released the RNN docs in
> HEAD. Not sure if this will end up in 0.9 or the next version.
> @martinwicke https://github.com/martinwicke?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2719#issuecomment-225481251,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_TyAVXLMbjsEEo2GA64KAcVKLZlBks5qLMqFgaJpZM4IwhYu
> .
 @martinwicke for some reason they don't show up.
  @ebrevdo: Do summaries work inside control flow? 
 I need to see the code where you add the summaries.
 can you paste more of the error trace?  need more context.
  Thank you for posting the question on StackOverflow!  See also #2075.
  @martinwicke: Do we support Pyinstaller?  I've never heard of it. 

@thematrixduo: Where did you get `tensorflow_script.py`? 
 I don't know what pyinstaller is, and consequently I'm certain that we don't support it. 
 (It is likely that whatever dependency resolution pyinstaller uses to determine which extensions to include in its package does not pick up the correct files. That would be an issue with pyinstaller. You can look at the build_pip_package target to find out what needs to be included for all of TensorFlow.)
  Sorry for the delay.  Looking over this now.
 Jenkins, test this please.
 @ibab Alas, re the eigen change.  Is there something I should do to help that process along?  If not, can you send me a mention here when we're ready to go?
 @benoitsteiner Does something need to happen to get those Eigen changes in?
  @mrry has an optimization that decouples this dependency, but maybe it has to be turned on manually?
 Why not just use the static shape, `myPlaceholder.get_shape()`?

There is a prototype optimization (@cwhipkey is working on it now) that uses inferred shapes in constant folding, but I'm not sure that it will make your use case work. I'm inclined to say that you must _always_ feed a placeholder if you want to fetch a tensor whose value is a function of that placeholder, because making the semantics depend on optimization level seems confusing....
 @RafaelCosman: I think the conclusion is that we don't want to support the independence you want to rely on, but we also don't want to guarantee that the exception will be thrown.  Thus, what you want to do might happen to start working at some point in the future. 
  Thanks for reporting this.  Would it be possible to create a simpler example to reproduce the problem?
 There are at least two loops involved.  Would it be possible to reduce it to contain only one?
 rnn.dynamic_rnn() is implemented using a while_loop, and you also have an explicit while_loop in your code. It would be simpler to debug if the problem could be reproduced with only one of them.
 FYI: I have a fix in the work. If my understanding is correct, the problem is that we don't support broadcasting for AddN, which is used to accumulate partial gradients.
 A possible workaround is to make:

def condition(mem_state_previous, hops):  
            z = tf.mul(stories, mem_state_previous)
            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name="e_reshaped")
            e_gate = tf.nn.softmax(e_reshaped)
            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))  
            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1
            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))

to be true at least once.  
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins, test this please.
 Thanks for doing this! I've been trying to resuscitate the CMake files, and this saves me a lot of effort :).
 @jendap @caisq: Does either of you know why the "Linux CPU Tests CMAKE" test might be failing with the following error?

```
[  0%] Running C++ protocol buffer compiler on tensorflow/core/framework/log_memory.proto
make[2]: execvp: /usr/local/bin/protoc: Permission denied
make[2]: *** [tensorflow/core/framework/log_memory.pb.cc] Error 127
make[1]: *** [CMakeFiles/tf_protos_cc.dir/all] Error 2
make: *** [all] Error 2
```

It looks like the `install_proto3.sh` script is running. Is there something else we need to do to chmod `protoc`?
 @lilac: I've forked this PR (#2839) and added the necessary fixes to make it build the tutorial example.
 I'm going to merge this (even though it is incomplete), and follow up with #2839, which makes the build work again.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Did you run configure?
 @jplu: I saw that, but one can never be too careful when triaging hundreds of issues. :)

Can you add a `pwd` line to `python_config.sh` to see what directory it's in?  I'm not sure how Bazel genrules work, so it's possible that the BUILD file should declare some dependencies.
 @jplu You might need a flag to bazel to get it to not suppress stdout.  Not sure which that is, though; `bazel help build` doesn't make it obvious.  The high tech option is `pwd > ~/blah`.
 @damienmg We have a `python_check` genrule that verifies that a symbolic link to a directory exists.  I think the check is failing since we don't depend on the symbolic link.  Can one put a symbolic link to a directory in a normal filegroup, or is something special required?

https://github.com/tensorflow/tensorflow/blob/master/util/python/BUILD#L14

On the other hand, @martinwicke, I suppose if we add `python_lib` as a dependency then `python_check` will fail with a less informative error message.  Thoughts?  I imagine this error message has a significant role in reducing filed issues, so it'd be bad to make it worse. 
 Can we just use all_files as a data depedency for python_check? That will not fail (since it's just a glob), and if nothing is there, we'll still see it's not there and get our nice message.
 @martinwicke Seems reasonable.  Want to assign it to someone?
 This used to work out of the box, no?  Sounds like a bug was recently introduced.
 Olivia, can you change the BUILD file in util/python to have a filegroup containing all files in the directory, like this:

filegroup(
    name = "configure_files",
    data = glob([
        "*",
    ])
)

(syntax may be screwy), and add that as a data dependency to the python_check target?

And then make sure it works... I hadn't seen this before.
 @jplu can you check whether #2958 fixes your problem?
  Duplicate of #2573.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Thanks, it may take a bit for the CLA to propagate.  will look at this again tomrorow
 Make sure to close #2420 once this is merged, since the commit message doesn't say "Fixes #2420".
 I updated the commit message.  Corp CLA still hasn't propagated, will check again soon.
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I guess this doesn't have a test for it, but I'll test and merge anyway.

@ilblackdragon @martinwicke need moar test coverage

@tensorflow-jenkins test this please
  It's a protocol buffer that is commonly used as the data format for training and evaluation examples in TensorFlow. The format is defined here:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto

The `tf.train.Example()` call instantiates a new protocol buffer, and fills in some of its fields.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Thanks!  Jenkins, test this please.
 Looks like you'll need to sign the CLA first.
 @lesniewski I believe Dropbox has signed the corporate CLA so this should be fine, but I'm not sure if we should expect @googlebot to detect this.

@willnorris for corporate CLAs, do we expect googlebot to say this has been signed? or do we just assume that if the user has @dropbox.com in their email that it is fine?
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 Sweet, thanks for fixing this Chris!
  This may be something we're working on.  Would you like to open a new bug to track it, explicitly describing the semantics of how you'd like to be able to pass tuples into feed_dict?  Closing this bug for now.  Thanks for answering @jihunchoi!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 The docs have gone back and forth so many times, I have no idea what the right answer is anymore ;).  Merging anyway.
  The PR looks ok to me, but it will need tests. Please add tests to rnn_cell_test and make sure to test both with and without state_is_tuple.
 I think you can just run rnn_cell_test to test locally, and then github/jenkins will test all other test for you, right? As for TODO: do you mean that MultiGFRNNCell needs to work with other cells than GFXXX cells? Maybe not, we have MultiRNNCell for that in the end. Or did I misunderstand you?
 Would it be possible to extend MultiRNNCell by an optional argument rather than making a separate MultiGFXXX Cell? Maybe that'll be cleaner?
 Initial comment:

Don't import _linear.  Use contrib.layers.linear with the appropriate initializer instead.

Lulasz: I prefer not making MultiRNNCell more complicated, so this is a good place for this code for now.
 Oh, and get rid of the state_is_tuple. That's for backwards compatibility. Assume it's true.
  Unfortunately, TensorFlow is not yet supported on Windows.
  Unfortunately this is too little information.  It sounds like you added a print statement or similar and it didn't show up when run.  This is probably a simple shell error, not an issue with TensorFlow itself.
 Unfortunately I still don't understand your issue, but I suspect it is closer to a StackOverflow question than a problem with TensorFlow.  You could try posting your code changes, or you could try posting them on StackOverflow with the `tensorflow` tag.
 Okay, I'll close for now.  Let us know if you have further issues, but we'll probably need more details for those too if so.
 @SunAriesCN: Questions about how to write new code on top of TensorFlow belong better on StackOverflow. 
  @samfux84: Did you point `./configure` to the right place?  Having `Python` in `$PATH` may be problematic for bazel, but I believe this should be solved by passing the absolute path to `configure`. 
 @martinwicke: Do you know if this is on our end or Bazel's? 
 It may be a recent thing. Can you cherry-pick edf3f943068445ab58ca7d14a3d3103cafc618d0, or build from r0.9 and see if that helps? This will disable the exporter, which uses a genrule using python.

You can also try adding `--host_force_python=py3` to your bazel commands (and not disable the exporter).
 It's only set for test, not for build. You need the line

```
build --host_force_python=py3
```

On Tue, Jun 7, 2016 at 12:44 AM samfux84 notifications@github.com wrote:

> --host_force_python=py3 is already set in my bazel.rc
> 
> Please see below my bazel.rc:
> 
> # Autogenerated by configure: DO NOT EDIT
> 
> build:cuda --crosstool_top=//third_party/gpus/crosstool
> build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true
> 
> build --force_python=py3
> build --python3_path=/cluster/apps/python/3.3.3/x86_64/bin/python
> build --define=use_fast_cpp_protos=true
> build --define=allow_oversize_protos=true
> 
> build --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python
> test --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python
> test --force_python=py3
> test --host_force_python=py3
> run --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python
> 
> build --spawn_strategy=standalone
> test --spawn_strategy=standalone
> run --spawn_strategy=standalone
> 
> build --genrule_strategy=standalone
> test --genrule_strategy=standalone
> run --genrule_strategy=standalone
> 
> Thank you for your suggestion. I will try to checkout the newest commit
> from the TensorFlow repository and try to build it again.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2687#issuecomment-224204683,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_YW6ae9aRpQmQqDzWY2btVH_xqllks5qJSFPgaJpZM4Iu-gD
> .
 Ok, now I'm all out of ideas. Can you try building r0.9 to see if that works for you? It won't have some exporter functionality, but you can replace that with a separately built tensorflow/serving.
 Have you run `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`? That's what creates `/tmp/tensorflow_pkg`.

As for other build systems: there are both contrib/cmake and contrib/makefile directories, but they're far from complete (only building the C++ parts or partial python), and we do not have the resources to maintain two build systems. If someone were to volunteer to maintain them, they'd have our enthusiastic support.
 Probably related to https://github.com/tensorflow/tensorflow/issues/2751 at this point -- let's de-dupe with that thread now.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @Undo1: Thank you for the fix!  Backticks are depressingly hard to see. :) 
  I install tensorflow on a new cluster and try to run program on it. 
But I get this error:
`ImportError("This module is deprecated.  Use tf.nn.rnn_* instead.")`, 

from this line: 
`from tensorflow.models.rnn import rnn, rnn_cell`. 

It seems the new version of TF has changed the way to import rnn, but I don't know how to fix it. (version of TF is 0.9)
 Try `tf.nn.rnn_cell` and similar.
  PRs welcome!
 @ajaybhat Let us know if you have questions or issues!
 @ajaybhat Search for classes which inherit from `Optimizer`.  However, note that for Hessian free methods the standard split into `compute_gradients` and `apply_gradients` won't work, since you need to compute partial second order gradients.  There are a few different ways one could handle that; the simplest would be to compute gradients as normal during `compute_gradients` and do the higher order stuff in `apply_gradients`.
  Jenkins, test this please.
 @siddharth-agrawal I took a quick look and left a few minor comments.  Let's wait for Eugene's thought on the returned dtype.
 Jenkins, test this please. 
 Merged into master. Thanks.
  Hi, do you still experience this error after syncing past https://github.com/tensorflow/tensorflow/commit/7ee839a3ecb68ea0eedb1193638c05c53f06e56d? It adds the "Inv" operation that was previously not in the Android kernel filegroups.
 There is a "Switch" op in control_flow_ops.cc which is also included by android_extended_ops_group1. Can you confirm that this file/filegroup are being used by your build?
 I think your problem node results from the following python:

```
    affn1 = control_flow_ops.cond(phase_train,
                                  lambda: tf.nn.dropout(affn1, keep_probability), lambda: affn1)
```

The node summary from the error: 
`Switch[T=DT_BOOL](phase_train, phase_train)]`
implies it's looking to instantiate a SwitchOp node taking phase_train as both inputs 1 and 2.

However looking at the actual SwitchOp definition, it seems the second input must be a scalar, not a tensor as phase_train is defined to be.

I may be misinterpreting something here, but does anything stand out given this?
 @petewarden Do you know how the cc code here could be formulated to satisfy the Switch op? Alternatively, is this a situation in which [strip_unused.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py) could be used to simply remove it?
 The first thing I would double-check is that the Switch kernel is actually being registered. I normally do this by adding a `LOG(INFO) << op_name;` call to the KernelDefBuilder constructor:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/kernel_def_builder.cc#L22

Once we know for sure the kernel is being linked in, then we can look at exactly what permutations are listed in the registry.
 Try editing the constructor Pete linked to in kernel_def_builder.cc to this:

```
#include "tensorflow/core/platform/logging.h"
...
KernelDefBuilder::KernelDefBuilder(const char* op_name) {
  kernel_def_ = new KernelDef;
  kernel_def_->set_op(op_name);
  LOG(INFO) << op_name;
}

```
 Thanks for trying that. Do you see any op names listed in the output?
 @TianweiXing The op name output should be the first native log output from the app. It should be above even "Loading Tensorflow" (since it happens as the library is loaded, not when the session is initialized).

If you're not seeing anything there at all, it's possible you're not linking in the kernel lib successfully. What is the command you're using to build?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  In your Python 3.x shell, can you please run the following:

``` python
import imp
print(dir(imp))
print(imp.__file__)
```

...and let us know what it prints. The `find_module()` function [should be available](https://docs.python.org/3/library/imp.html#imp.find_module) in Python 3.5, which you appear to be using, so I'm not sure why this would happen. (One possibility is that there's a file called `imp.py` in your PYTHONPATH, and that's causing it to get confused.)
 The problem is that there's a file called `imp.py` in your PYTHONPATH. In particular, it looks like it's in `/home/prayalankar/imp.py`.

There are two options for fixing this:
1. Rename `/home/prayalankar/imp.py` to something else, so that it doesn't clash with the name of a built-in Python module, or
2. Change your PYTHONPATH so that it no longer includes `/home/prayalankar`.
  Thanks for this! Added few comments.
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @ilblackdragon looks like comments were addressed?
 Also can you add this example to the README.md file.

Otherwise, @vrv looks good.
 @tensorflow-jenkins Jenkins, test this please
 FAIL: Found 2 non-whitelited pylint errors:
tensorflow/examples/skflow/lstm_regression.py:158: [E1123(unexpected-keyword-arg), ] Unexpected keyword argument 'batch_size' in method call

tensorflow/examples/skflow/lstm_regression.py:185: [E1123(unexpected-keyword-arg), ] Unexpected keyword argument 'batch_size' in method call
 @mouradmourafiq let's either use `Estimator` then or change `TensorFlowEstimator.fit` to take `batch_size` for consistency.
 @ilblackdragon Ping. Could you review this and the other PR @mouradmourafiq created? I am not too sure what the future refactoring plan is. @mouradmourafiq Thanks for your patience. 
  I ended up using @fayeshine fix from https://github.com/tensorflow/tensorflow/pull/2073 to get tensorflow building on Ubuntu 16.04, add a mention to this work-around under "Linux Issues"
 @yaroslavvb do you have a problem with it? I have just build on 16.04 without issues. CI build is coming.
 Yes, I couldn't build after upgrading my Ubuntu last week. The upgrade changed gcc to version 
gcc version 4.9.3 (Ubuntu 4.9.3-13ubuntu2) 
 @jendap: Assigning you since you commented; feel free to change. 
 As mentioned on the issue, we should probably recommend upgrading to cuda 8.0 instead.
  This is expected behavior.  TensorFlow ensures that names are unique within the same graph, so if you generate the same ops twice without changing graphs you'll get different names.
  I'm using this to remove incompatible shape functions from the registry to make "immediate" tensors work as a drop-in replacement for regular TensorFlow tensors.

Since "static shape inference" doesn't make sense for persistent tensors, I'm returning Dimension(None) for the shape for such tensors. But, there's at least one shape function which doesn't allow `Dimension(None)` -- (`_ReverseShape(op):`), so as a work-around, I'm using `unregister followed by`register(None)` 

@mrry 
 @tensorflow-jenkins test this please
 @jendap: Tests look good.  Merge if you've reviewed? 
 I don't think this is a good idea. We should just fix `_ReverseShape()` if there's a bug in it.
 @yaroslavvb: Are bugs in shape functions the only issues here? 
 Yes, I think it's a bug in shape inference, filed https://github.com/tensorflow/tensorflow/issues/2690, will close this then
 FYI: Yes, I have read it but I was not sure why to add it ... I have triggered the build because I needed some pull request build to test jenkins ;-)
  @petewarden: Can you take a look at this? 
 Unfortunately this is expected behavior. All of the images are being transformed, the percentage values are for the amount of cropping, etc, not how many images are being affected. This means the original images can't be cached at the bottleneck stage as they are without transformations applied, so running each training step is a lot more expensive. I recommend switching to GPUs once you have heavier duty training needs like this.

Closing since this is working as intended.
  This seems like a bazel issue, unfortunately.  @damienmg: Do you have any ideas?
 Thanks @damienmg!  Closing since it is not a TensorFlow issue. 
  I posted an [answer](http://stackoverflow.com/a/37671613/3574081) on Stack Overflow. Feel free to continue the discussion over there!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 This file is already big. It does not feel right to put there all those instructions...

Why to build tar.gz anyway?
 I think this would be great as a gist or something hosted elsewhere, and we'd be happy to link to it at the bottom of the install page -- our installation instructions are already way too long and it's already a pain to maintain right now :)
  `FLAGS.worker_hosts` is the string (type `str`) value of the `--worker_hosts` flag. It doesn't define a `__call__()` method, hence the error you are seeing.

I think instead you want to have:

``` python
worker_hosts = FLAGS.worker_hosts.split(",")
```
 It looks like the error is still present in the code snippet you posted:

![worker_hosts = FLAGS.worker_hosts(",")](http://i.imgur.com/7eH14HE.png)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Jenkins, test this please.
 I signed it.
 @martinwicke Can you figure out why the CLA bot thinks I'm not authorized? I've checked the commit and it seems to have my correct google.com address attached.
 Hmm. Jenkins ignored you too.
 The creator of the PR shows as @petewarden4prs, not @petewarden, maybe that's why? Have you successfully created PRs from that special account before?
  Can we see the full error message?
  @vrv: Can you take a look?  At best, the error message isn't very explanatory.  
 I talked to @zheng-xq: NCHW is supported only for GPU (because we call cudnn), we do not have an implementation for CPU.

I'm not sure how to correct the documentation though -- the data_format strings indicate the potentially supported formats, not the actually supported ones :(.

To reiterate:

MaxPool on GPU supports NCHW and NHWC
MaxPool on CPU only supports NHWC.

If you have a concrete suggestion for how I should change the documentation, please let me know and I'll be happy to make the improvement!
 @vrv: Maybe just add "(on device %s)" to the error message so that we know which device it isn't supported on?
  @siddharth-agrawal: looks good, could you rebase?  I will kick off the Jenkins run after that.
 Jenkins, test this please. 
 yeah, rebasing only needed on conflicts (most of the time).

@tensorflow-jenkins test this please
 You're right -- I saw the merge button being gray but I didn't look too closely.  It was not due to conflicts but due to the lack of Jenkins results.  Will merge after green.
 Merged into master.  Thanks @siddharth-agrawal!
  ping for @andrewharp 
 Hi, thanks for the PR. We had previously merged a commit to add a permissions check (https://github.com/tensorflow/tensorflow/pull/926), but reverted it because at the time it was causing some Google-internal build complications.

Instead it was easier to change the app to build at API level 21, where all required permissions are granted on install automatically (are you trying to build at API 23? If so, maybe adb install -g will work for you in the short term?)

My major concern here is bringing in the Android support libraries as a dependency, since that complicates the build. I can test the previous PR again and see if that builds internally now, if this works for you.
 I checked, and I can no longer reproduce the internal issue with the previous change.

Also, it does make more sense to put the authorization code in CameraActivity.java and to additionally request the storage permission, as you've done. Would you mind modifying your commit remove the use of the Android support libraries (you can use the previous solution for reference)?
 Thanks!

With the runtime checks on Android M for devices at API < 23, yes it should be safe (and required) to set the target level to 23.
 @tensorflow-jenkins: test this please
 @andrewharp ready to merge?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  Yes, we'd like to do that. Nobody is working on it right now though.
  @domluna: Assuming the meta file is a MetaGraphDef, can you check whether it's the `GraphDef` part that's growing?  One possibility is that you are adding new ops to the graph each step, causing the graph to grow linearly.
 FYI, if you aren't intending to use the `MetaGraphDef`, you can call `saver.save(..., write_meta_graph=False)` to avoid writing it with each checkpoint.
 @domluna: Try printing out the node names in the graph each step.  If you see it growing over time, it probably means you're adding new nodes to the graph over time. 
 You can do "tf.get_default_graph().finalize()" after you've done modifying
your graph, that'll throw an error on any new modifications

On Wed, Jun 8, 2016 at 8:56 AM, Geoffrey Irving notifications@github.com
wrote:

> @domluna https://github.com/domluna: Try printing out the node names in
> the graph each step. If you see it growing over time, it probably means
> your adding new nodes to the graph over time.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2654#issuecomment-224636494,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHFIHCpBF19MbaRxJc1ZZyBQzE3viks5qJuYygaJpZM4IuJSB
> .
  Unfortunately, the reduction ops on GPU use asynchronous atomic adds, and are therefore fundamentally nondeterministic for floating point.  Making them deterministic would require either tree-structured reductions or integer math, both significantly slower.

I can leave this open with contributions welcome if you'd like (with an adjusted title), but it'll be a lot of work if someone tries to take it on, and it's unclear how best to make it happen automatically.  Even if one added deterministic reductions as an option (either as a separate op or as an attr on the existing ops), we'd need an unpleasant global flag to turn this on when building the backward pass.
 @metap Do you have a link for that?  I don't quite follow, especially the bit about keep_dim since that doesn't change the computation structure.
 Cc @zheng-xq @benoitsteiner in case more GPU knowledgeable folk want to take a look.  Determinism would certainly be nice to have if we can get it.
 The shfl_down results are only useful within a single wrap. That technique itself would take a second pass to accumulate the results for each block. 

In general, there is no guarantee of determinism on GPU. Therefore, we are not sure how much effort we want to spend on it. Even if we can fix this particular kernel, we have other Cudnn kernels that do have non-determinism. 
  @tensorflow-jenkins test this please
  This is unexpected, and definitely a bug. When you run it in the single-machine mode, are you using the `tf.train.Supervisor`?

Also, which process segfaults? Is it the Python client that calls `saver.restore()` or the parameter server that runs the restore op?
 Can you try running the chief worker under `gdb` and reporting the stack trace for the failure? Also, are you running the 0.8 release or a nightly/from-source build of TensorFlow?
 Thanks, it looks like it is failing to deserialize a large proto. We just fixed a related bug, which will be part of https://github.com/tensorflow/tensorflow/pull/2649, so if you could try out your code against the PR, that would be great.

There's a higher level issue, which is that running the saver shouldn't be causing large amounts of data to be transferred across the network. Try passing `sharded=True` to the [`tf.train.Saver` constructor](https://www.tensorflow.org/versions/r0.8/api_docs/python/state_ops.html#Saver.__init__) and that might also solve the problem without rebuilding.
 That's strange, because I think `grpc::DeserializeProto()` (`#1` on the stack trace) no longer exists in the 0.14 release of gRPC, which we're now using.

#2649 is now merged, so can you try this again with either the latest nightly or the 0.9 release candidate?
 Thanks for persisting with this. I've been trying to reproduce this, but haven't managed to get the exact same failure.

One potential issue is that protobuf doesn't handle messages larger than 2GB. We haven't optimized this because we haven't seen any realistic models where it is necessary to transfer this much in a single step, but it obviously shouldn't crash like this. If I set up two servers, and try to transfer a tensor larger than 2GB from one to the other, it fails with `SIGABRT` on the _sending_ side. However, I haven't managed to get it to "successfully" send a tensor that the other side fails to retrieve.

Are there any very large (> ~2GB) variables in your model that could be brushing up against the protobuf limit?
 Ah, if the model is only 10M, then my hypothesis about protobuf overflow is almost certainly wrong. (There were certainly segfault- and `SIGTERM`-causing bugs in that path, which should be fixed now, or at least in the next push from the internal branch.)

I see two potential options for fixing this, but I'm afraid I'm going to need your help.
1. Can you package up a minimum failing example, which I'll try and run locally to track down the failure?
2. Can you try building from source with `bazel build -c dbg`, and report a stack trace with line numbers, so that we can see exactly where it's failing?

If you're already familiar with building from source, I suspect option 2 might be easier.
 Thanks! Only the process that you expect to crash needs to be in debug - it should be able to communicate with other workers that are running release binaries.
 The easiest way to get it is probably to run under `gdb` (otherwise you could turn on core file recording and load one of those in gdb).
 Thanks for following up. Unfortunately, we still haven't been able to reproduce the failure you are seeing, but it does sound like there's a bug.

Just to clarify: does the failure only occur when there is a mismatch in the directory structure on the master worker and the parameter servers?
  Yeah I believe sizeof(std::unordered_map) is different between the c++0x (tr1 version of std library) and --std=c++11. If protobuf and tensorflow are not communicating unordered_maps amongst each other and protobuf functions are not inlined into tensorflow translation units it can be perfectly safe. However, if some of the inlined functions are not inlined and instead turned into exported symbols implicitly, the dso loader will attempt to share one symbol with both libraries which will not work (it will assume the wrong structure layout).  You could attempt to work around this by adding 
-fvisibility-inlines-hidden to tensorflow which will force tensorflow to use its own copies of all inlines that are spilled to function calls. OTOH, if this is due to inlines using a std::unordered_map in a tensorflow translation unit that was allocated in a protobuf translation unit, this will not solve the problem.

-fvisibility-inlines-hidden
 @aselle: To pull an offline comment into the thread, is there a way we could detect the size mismatch (possibly at runtime) and bail with an explanatory error? 
 @llchan With respect to libpng and other libraries embedded in `_pywrap_tensorflow.so`, look [here](https://github.com/tensorflow/tensorflow/blob/9078909131d5e91e8bd1878eac54136885656043/tensorflow/tensorflow.bzl#L651). The linker scripts strictly control the symbols exported by `_pywrap_tensorflow.so`. Even though the library is loaded with `RTLD_GLOBAL`, only tensorflow related symbols are available in the global address space.
 Sorry for the late reply. It's difficult to do this in general, because you'd need to put something in a translation unit for every project that might have possibly been compiled differently that you can query later. I.e. something already has to be compiled that remembers the size or that it is c++11. I don't really know enough about the toolchain to know if the compiler dumps something we could reliably use to determine if a c++11 or c++0x standard library was used. We could roll our own, but it seems difficult to control. It is best to avoid std::unordered_map being part of a systems API to avoid most times you would hit this, at least until c++0x tr stuff is put to bed.
 @llchan Thanks for your comments. I am closing this issue as there seems to be no outstanding action items. Feel free to open the issue if you feel that it has not been sufficiently addressed.
  cudnn is not publicly available, you must register with nvidia to get them.

The notes say

"Uncompress and copy the cuDNN files into the toolkit directory. Assuming the toolkit is installed in /usr/local/cuda, run the following commands (edited to reflect the cuDNN version you downloaded):"

which should illustrate that you might need to edit the version.
  I have reproduced this.
 The gradients are computed by computing the full prod and then dividing, which is broken as you point out.  For example, for a length two produce `x *  y` the gradient w.r.t. `x` is `x * y / y`.  Indeed, there's an ancient TODO in the code that it is broken.

@ibab, @benoitsteiner: The easiest way to fix this would be to do two scan products to get the sequence of partial products from both directions, then multiply them together to get all products with one element removed.  Unfortunately, we don't yet have scan.
 Adding contributions welcome, but note that it'll have to wait until after #2711.
  I'm not sure what's going on there, but it looks like /bin/false is failing with no such file directory on execvp. That could be because Mac OS has /bin/false in /usr/bin/false instead. Could you try ln -s /usr/bin/false to /bin/false and see if it gets any further?
 Bazel 0.2.3 doesn't support NDK r11. Support was added in commit https://github.com/bazelbuild/bazel/commit/abdaff492440b373bacd016d772ef73611a27901, which will probably be in 0.2.4. Until then I'd suggest using r10e.

As far as libpthread.so in particular, this file is actually not even necessary for the android build. It's just that protobuf assumes it will be there, so I had to add a workaround to tensorflow/examples/android/BUILD. The newer version of the protobuf library is more intelligent about adding the option, but we hadn't upgraded TF to it until recently. I'm working on a commit right now to remove libpthread.so from the android/BUILD file.

You can try editing protobuf/BUILD and remove the pthreads entry from LINK_OPTS, and also remove all references to libpthread from tensorflow/examples/android/BUILD.

Hopefully switching to  NDK r10e and trying to excise libpthread.so from the build will solve your problem, but if it doesn't please update with the new error.

edit: updated protobuf info
 For reference you can find links to download NDK r10e here: https://github.com/tensorflow/tensorflow/issues/1468
  The problem is in this line:

``` python
grad_vars = [tf.Variable(tf.zeros_like(_)) for _ in tf.trainable_variables()]
```

If you initialize a variable from (something derived from) another variable, you need to use the [`Variable.initialized_value()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/state_ops.html#Variable.initialized_value) method to tell TensorFlow about the ordering dependency when running the initializers. So your line should look like:

``` python
grad_vars = [tf.Variable(tf.zeros_like(v.initialized_value())) for v in tf.trainable_variables()]
```

Note though that your `grad_vars` don't really depend on the _value_ of the trainable variables, and the trainable variables all have static shapes, so you could alternatively solve this with the following:

``` python
grad_vars = [tf.Variable(tf.zeros(v.get_shape())) for v in tf.trainable_variables()]
```
  Could you show uname -a on the machine and also let us know what PIP binary package you installed or what sha hash if you installed from source.
 @zjhzxhz: Can you check if doing `import numpy` before `import tensorflow` fixes the problem, as per #2034.  Alternatively, you could try upgrading to 0.9, which shouldn't have this problem. 
  This is a question for StackOverflow
  I think this might be related to the problems affecting "setting up for development" #2497 i.e. in 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md
section "Setting up TensorFlow for Development."

Specifically, The instructions have to be different on what you symlink into the python build directory dependent on the version of bazel (which ones to do when are outlined in #2497).  Are you able to get those instructions to work and sucessfully import tensorflow from python?
 For the generated file location see bazel-out...

For example [pywrap_tensorflow.cc](http://ci.tensorflow.org/job/tensorflow-master-cpu/ws/bazel-out/local_linux-opt/bin/tensorflow/python//pywrap_tensorflow.cc) on [ci.tensorflow.org](http://ci.tensorflow.org/job/tensorflow-master-cpu/ws/bazel-out/local_linux-opt/bin/tensorflow/python/).
  I posted an [answer](http://stackoverflow.com/a/37618688/3574081) to this same question on Stack Overflow. Feel free to continue the discussion over there!
  @tensorflow-jenkins test this please
  Looping in @ibab. Did you expect your CL would cover this case as well?
 Sounds awesome, thanks!
  Thank you for carefully narrowing this down to a commit SHA!

It looks like your local pre-sudo environment does not have access to libcupti.so but the default one in the sudo'd space does. The change that seems to have hit you is this

![image](https://cloud.githubusercontent.com/assets/326106/15761987/2f44e606-28d0-11e6-8bd2-1e877c3424dc.png)

So previously LD_LIBRARY_PATH was spelled wrong so it wasn't actually preserving the environment (see man sudo ) of that variable. Whereas now it is wiping out the default environment in sudo'd space. environment variables are usually wiped out for security (i.e. LD_PRELOAD and LD_LIBRARY_PATH are vulnerable to code injection that would run as root). 

So check the disparity with your environment between sudo'd and not sudo'd.
 I'm not sure that the change to 'with_the_same_user' is relevant.  (AFAIK, This is just used in our Jenkins test framework to ensure the LD_LIBRARY_PATH from the Docker container is used when running GPU tests as the user 'CI_BUILD_UID')

Note: the same GIT commit also changed the interpretation of the trace level enum.  Prior to this change, the TRACE_FULL option was only recording the host-side enqueueing of ops (i.e. what is now done if you specify 'SOFTWARE_TRACE') 

It is now the case the 'TRACE_FULL' will also try to enable the GPUTracer on CUDA builds.  
If you're running a python script, this requires libcupti to be on your LD_LIBRARY_PATH (since there's no other way to find the NVidia library.)

e.g. LD_LIBRARY_PATH=/usr/loca/cuda/lib64:/usrlocal/cuda/extras/lib64

Please can you check your path, and see if this fixes the problem?
 Closing as (assumed) fixed.
 > I think this might be another issue.

Yes, the warning messages are a separate (known) issue which we are currently working on.  (also reported in [this StackOverflow post](http://stackoverflow.com/questions/37849009/warnings-when-executing-tensorflow-sample-code-mnist-with-summaries-py))

They are most likely due to differeces between versions of libcupti, and should be harmless (other than the undesirable log spam)  

Issue tracked under #2959
  Correct me if I'm wrong, but you're saying that adadelta_test has the right equation, but that this test was passing before your change, and still passes after your change?
 doesn't that mean that the existing test is not really sufficient?  Can we modify the test so it would have failed before your fix?
 There's no really good default, since every test has a different number of operations being performed.  Ideally we would test based off of expected number of ULP differences for every test, but that's somewhat impractical.

If you can find a setting that passes with your change and fails before it, that would be awesome.
 Thanks for this!  mostly just lint / style issues, otherwise this looks great.
 We can squash now when we merge :)

@tensorflow-jenkins test this please
 Thanks!
  @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
  Could you please include exact instructions on how you are running this (command line usage) so I can reproduce the problem. If you click "Create New Issue" you can find the template of what information we need and how to gather it. It is impossible for us to help diagnose without that information. Thank you.
  I am unable to reproduce this on my machine using the tensorflow master branch branch. I am using CUDA 7.0 with cuDNN 4.0 however. So perhaps try an older cudnn and let me know if that works for you.

```
$ python cifar10_train.py --data_dir=./cifar-data
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Graphics Device
major: 5 minor: 2 memoryClockRate (GHz) 1.2155
pciBusID 0000:02:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28fc190
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 2.00GiB
Free memory: 1.04GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Graphics Device, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0)

2016-06-02 14:22:38.249729: step 0, loss = 4.68 (6.7 examples/sec; 19.180 sec/batch)
2016-06-02 14:22:39.837789: step 10, loss = 4.66 (1339.4 examples/sec; 0.096 sec/batch)
2016-06-02 14:22:40.867866: step 20, loss = 4.64 (1101.5 examples/sec; 0.116 sec/batch)
2016-06-02 14:22:41.923386: step 30, loss = 4.63 (1120.5 examples/sec; 0.114 sec/batch)
2016-06-02 14:22:42.924298: step 40, loss = 4.60 (1186.7 examples/sec; 0.108 sec/batch)
```
 Also, click "Create Issue" and look at the template of what information we need. Without that information it is very difficult for us to help you, because installation situation has many variables. Thank  you.
  What architecture are you running on (little endian or big endian)?
 @dave-andersen Could you take a look since you touched the png reading most recently? :)
 Urgh.  I'm concerned that we're not properly setting the IS_LITTLE_ENDIAN flag in the OSS build:
https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=IS_LITTLE_ENDIAN

I'll take this on.  @girving - do you know off the top of your head if we have an existing flag for little endian I can repurpose here?  (I'll dig, just trying to be lazy. :)
 @dave-andersen I don't remember, nor do I envy you this task. :)
 I'm, um, just going to use the existing flag and assume it's someone else's responsibility to make the flag work. :)
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/host_info.h#L25

(static const bool kLittleEndian = true; )
 Actually, I'm not sure that flag is the issue.  @beopst Are you on a little endian machine?
 @girving - the issue is that we don't use that flag in the PNG decoder, we use the IS_LITTLE_ENDIAN flag, which is only defined by some transitively included google-internal header.

The second part of the issue is that we don't have any tests for the png decode op that would catch this on jenkins.

Fixing.
 Fix submitted, should hit github at next sync.  Will let the fix close it.
@beopst - huge thanks for providing an easy way to reproduce with your bug report!
  This may be related to a recent change. I'll inquire about that, but it would be helpful to know if you were able to build with an older version successfully?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  If you're just asking to re-write some error messages with more information, I'm all for it. You should specify which existing error messages should be modified and how.

If you're suggesting to add more checking code to the mnist example, that's also a good idea, I would change the title of this issue if that was the case.

Checking the sizes in the general case is fairly hard (since sizes can only be checked once they are known, which is often when you run the graph, not when you define it). The ops do check size requirements, but because of their generality, often don't have a ton of information about the larger use case. 
 Closing for now since we already check dimensions pretty thoroughly.  @konts6102 Please comment if there's something specific that we're missing and I'm happy to reopen.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Can you log https://github.com/tensorflow/tensorflow/blob/ecc6c5ae3dcec7ea263a010ae2b6b1f7dc0da1a8/tensorflow/core/common_runtime/gpu/gpu_device.cc#L622 those two values there and let me know what they return in this case?  I'm a bit confused about how 'total' can ever be less than 'available'.
 Yeah, there's no way to change it without a recompile -- basically my expectation of what is being reported by the device (through the StreamExecutor->DeviceMemory() call) appears to be wrong, so I don't know what the right fix until I can see what behavior you're running into.
 Cc @zheng-xq in case he knows better 
  Hi there! I'm going to close this issue as it doesn't sound like there's a bug or missing feature in TensorFlow itself, so this kind of question is better suited to Stack Overflow:

> GitHub issues are for bugs / installation problems / feature requests.  
> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
> To make bugs and feature requests more easy to find and organize, we close issues that are deemed
> out of scope for GitHub Issues and point people to StackOverflow.

Feel free to repost this question on stackoverflow.com, with more details about exactly what is going wrong, and somebody will respond soon.
  @tensorflow-jenkins , test this please.
 Jenkins, test this please.
 Jenkins, test this please.
 @siddharth-agrawal thanks, just a few more minor comments, then we're good to go!
 Jenkins, test this please.
 Looks good!  Could you rebase?

On Thursday, June 2, 2016, Siddharth Agrawal notifications@github.com
wrote:

> @concretevitamin https://github.com/concretevitamin Updated!
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2608#issuecomment-223394523,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAkLHn69HstWKqMHAyxyDKYq8SqUcGvkks5qHy0MgaJpZM4Ir2qC
> .
 Jenkins, test this please.
 @siddharth-agrawal: thanks, I just merged this in.
  It's working for me on both Chrome and Firefox. Can you provide some screenshots of how it looks on your machine, along with console output?
 Also, please include info on what operating system + browser you are using.
 I see, I can repro that too now.
The issue is that Safari doesn't support ES6 fat arrow functions. 
In general we don't test Safari and don't guarantee Safari will work; you should use Chrome or Firefox.
However, this is quite annoying so I will make an exception and fix it for Safari :)
 OK, I've put in a fix upstream. Thanks for the report, otherwise the next release of TensorBoard would have been broken in Safari. 

Note, it may take several days before this change is reflected on the website. 
 What do I need to do to install a version with this fix. Is an updated `.whl` available?

(I've installed `mac/tensorflow-0.9.0rc0-py2-none-any.whl`.)
 We need https://github.com/tensorflow/tensorflow/pull/2786 to merge into 0.9 for 0.9 to be fixed.
Then, you can rebuild the .whl yourself using the build_pip_package script, or wait for us to publish an updated whl. 
  Hi SpencerC,

I'm currently working on a re-architecutre of the TensorBoard backend that will shift the underlying data pipe to be based on streams of data rather than async loading, with the option to connect TensorBoard directly to the TensorFlow process. Once this is setup, it will be feasible to have realtime updating in TensorBoard. However, it's a very involved change that involves changes to TensorFlow, the TensorBoard backend, and the TensorBoard frontend. So don't expect it to be released in the near future. 

You're welcome to hack a streaming system together, but unless we coordinate closely there's a pretty good chance that our changes will clobber each other.

As a workaround, you can change the frequency with which TensorBoard loads events from the backend by setting `reload_interval` to something much shorter, e.g. tensorboard --reload_interval=2. Then you can click the refresh button and get data much faster (note you should increase the flush frequency on your summary writer too). 

Right now the reload frequency on the frontend is hardcoded to 120secs, I would like to setup a route so that it reads from TensorBoard and reloads as quickly as TensorBoard is reloading. If you feel like contributing that, it's a good candidate for a pull request.
  Hi haraldschilly,

Can you give an example of exactly how TensorBoard misbehaves in your setup? I.e. what requests does the frontend make that 404?

Overall, this seems like a nice candidate for a pull request.
  @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
  @caisq: Do you know how much effort it would be to add this?
  Adding @yuanbyu@, @ebrevdo.
 We don't support taking gradients of nonscalars, so I wouldn't expect this to work.  However, this particular error message is pretty confusing.  @yuanbyu: Is there a way we could improve this error message if someone tries for a Hessian in the naive way and control flow is involved? 
 Yes, the error message should be better. Let me see what I can do.
 @dementrock: That's true in this case, but we don't want to do extra work on the control flow ops if all it provides is higher order derivatives w.r.t. scalars.  What is your intended use case? 
 @dementrock: We don't support higher order gradients even ignoring control flow. 
 @dementrock The problem is that the registered gradient routines would get significantly more complicated if both sides were nonscalar, and we don't want to support that kind of complexity.  As discussed in #675, it's possible one could implement registered gradients with some sort of automatic machinery to map scalar gradient routines to nonscalar gradient routines, but this is a lot of work and we don't have any plans to do it.

The other problem is that the applications I know of nonscalar gradients aren't that compelling as yet, since they tend to be impractically huge.  However, there are cases where higher order gradient information arises where you're differentiating a scalar, specifically Hessian-free Krylov-ish methods where one evaluates the gradient dotted with a suitably chosen vector.

If that last bit is what you're trying to do, or something similar, we'd be happy to accept pull requests to make control flow not interfere.  It might be pretty complicated, though.
  Hi,
The native classifier code returns the results as a String, which is parsed by TensorflowClassifier.java. It sounds like something has changed about the return format with your new model. Can you post the entire result string?

I don't think the RGBA_8888 warning is related. The bitmaps are created in 8888 format explicitly, so they don't rely on the system default.
 Thanks for the clarification, makes sense now.

It should be pretty easy to convert an rgb565 (or whatever) bitmap to rgba8888 prior to sending it to the classifier.

Your suggestion to raise an exception is a good idea, I'll see about adding that.
 Native error conditions have been changed to LOG(FATAL) messages  in https://github.com/tensorflow/tensorflow/commit/c32ef5a6b7bbcacfe39f20bc7bea9514a505c74f, so that errors can be caught as soon as they happen now.

I decided against adding 565 support for now because it's a little bit application specific how the resulting values are handled. Probably best to transform to RGBA8888 before passing to the native code, or make custom changes to tensorflow-jni.cc to handle the bitmap appropriately.
  Fair enough. I'll take a PR if you'd like.
  Implementation of immediate mode execution in TensorFlow. The idea is to wrap original tensorflow API, but provide session/run management logic so that commands execute immediately, and graph caching to avoid modifying graph when same op is run repeatedly. Data is kept in TensorFlow runtime whenever possible using persistent tensors and transferred to Python runtime on demand when needed for printing/control flow.

```
import tensorflow as tf
from tensorflow.contrib import immediate

tfi = immediate.Env(tf).tf        # wraps "tf" namespace, saves it as "tfi"
val1 = tfi.ones((3,))             # creates tensor on GPU
val2 = val1 + val1                # runs into tf.add, keeps result on GPU
val3 = tfi.ones((2, 2, 2, 2))
val4 = tfi.nn.conv2d(val3, val3, [1, 1, 1, 1], "VALID")   # run CuDNN conv2d
if (tfi.reduce_sum(val3)>0.):     # runs reduce_sum on GPU, transfers bool to CPU
  print(val4)                     # transfers whole tensor to CPU for display

```

This is a single commit rebase of a previous pull request from https://github.com/tensorflow/tensorflow/pull/2346

@keveman 
 Jenkins, test this please.
 The failures are due to https://github.com/tensorflow/tensorflow/issues/2586, @yuanbyu  are you able to reproduce?
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Hm, I may need to move this to a new pull request, every time I do rebase, it includes all the changes fast-forwarded by the rebase into the pull request.

I've asked question here with all the commands I've used to get into this weird state:
http://stackoverflow.com/questions/37707605/doing-rebase-adds-replays-master-branch-commits-into-my-pull-request
 Sorry, I'm opening new request since I don't know how to fix this:

https://github.com/tensorflow/tensorflow/pull/2747

In my current work-flow, when I rebase, every commit that's included during fast forwarding goes into the pull request. Here's the sequence of commands I've been using so that people know not to do this: http://stackoverflow.com/questions/37707605/doing-rebase-adds-replays-master-branch-commits-into-my-pull-request
  It looks like you might need `allow_soft_placement=True` in your Session configuration (`tf.Session(config=tf.ConfigProto(allow_soft_placement=True))`), but let's check with @ebrevdo to see if there's a more systematic solution.
 @markpwoodward: If you do soft placement, it will put anything it can on the GPU and use the CPU for the rest, which is exactly what you want here (since assert is CPU only).
 `dynamic_rnn` runs some dynamic assertions on the input, making sure things are within bounds, etc.  the error you're seeing is the static assertion message (a string) trying to get allocated on the GPU.  Since we don't mix strings with GPUs, we lack a const string op mapped to the GPU.  That's the error you're seeing.  I think it's fine for error checking and error strings to sit on the CPU, but perhaps we should consider a Const[string] that is declared for GPU but stores data on the host?  Either way, you should be fine with soft placement here.  @girving  @mrry  think host-memory const string GPU op is a reasonable thing?
 @ebrevdo: We should avoid adding more lies about which things are on the CPU vs. GPU (too many of these related to int32 already).  In any case, the actual problem here is that `tf.assert` itself is CPU only.  Maybe the ideal fix is to add a GPU version of `tf.assert`, specifying HostMemory for the string argument?
 @markpwoodward: No worries, thank you for the report!  We might close, but we might also rename it to "add tf.assert for gpu" in order to fix the other underlying issue. 
 Thanks!
 @girving can you assign to appropriate folks?
 I'll leave it as contributions welcome for now.  Happy to accept PRs if someone wants to make a GPU version of `tf.assert`!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (Reopen if you can sign the CLA)
  From: https://github.com/tensorflow/tensorflow/issues/2497#issuecomment-221631692

```
Bazel 0.2.1 and lower, any TF: the path will be build_pip_package.runfiles/*
Bazel 0.2.2 with TensorFlow before 32fa42a: build_pip_package.runfiles/*
Bazel 0.2.2, TF after 32fa42a: build_pip_package.runfiles/org_tensorflow/*.
Bazel 0.2.3, TF before 32fa42a: build_pip_package.runfiles/__main__/*
Bazel 0.2.3, TF after 32fa42a: build_pip_package.runfiles/org_tensorflow/*
```
  The `piecewise_constant()` function added in #2442 enables you to specify learning rate schedules like this one&mdash;the trick is to use `tf.case()` (or `tf.cond()` for simpler conditions).
  Seems reasonable to add another alias so that dialated_conv2d == atrous_conv2d.  You can add the alias into nn_ops.py like we do for other kinds of renames here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L1212
 @gpapan what do you think?
 @ry I prefer atrous_conv2d.

I will reserve the name dilation for the morphological operation https://en.wikipedia.org/wiki/Dilation_(morphology) which will be added very soon to TF.

We used the term "atrous convolution" in the CVPR 2015 paper:
http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Papandreou_Modeling_Local_and_2015_CVPR_paper.html
before the term "dilated convolution" was proposed in https://arxiv.org/abs/1511.07122.
  We don't currently support the output being a tuple, only the state.  This should wait until we support the output_shape being a (possibly nested) tuple.  Then we can add this change.
 For backwards compatibility reasons, we can't change bidirectional_rnn.  However we have an open PR implementing dynamic bidirectional rnn, and we can change the signature there to emit the final states.
 Indeed - your dynamic bidirectional rnn PR is the one i'm talking about ;)
  Test this please 
 @tensorflow-jenkins test this please
  ping for @ebrevdo 
 Will look tomorrow
On Jun 7, 2016 5:55 PM, "Vijay Vasudevan" notifications@github.com wrote:

> ping for @ebrevdo https://github.com/ebrevdo
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2581#issuecomment-224457640,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtimwGtHxN1ISsuG_2iujnK03rBfYErks5qJhMGgaJpZM4Ip5gv
> .
 ping :)
 Jenkins, test this please.
 @jihunchoi sorry for the late response.  some more comments.
 Jenkins, test this please.
  ping for @ebrevdo 
 ping for this too!  thanks
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 This is great. Could you add a test to make sure it keeps working?
 sorry, some conflicts have sprung up, can you rebase?
  We're not comfortable with the FreeImage dependency. Is there a gif reader library we can use instead?
  I [answered this on Stack Overflow](http://stackoverflow.com/a/37573208/3574081). Feel free to continue the discussion there if it does not fix your problem.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
  This looks to be the case, and it seems like MaxPool3D, MaxPoolGrad3D, AvgPoolGrad3D all have the same issue. Thanks.
  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py might be useful for you, potentially?
 As you seem to have been able to resolve your problem, I am closing the issue for now.
 @Shaikjalal Are you sure it's getting stripped out? You should be passing bypassing the DecodeJpeg node completely on Android.

@kuza55 Agreed that the code seems to imply we support jpeg on Android, which is inconsistent with the fact that we don't. I'll take a look at providing a better error message in the invalid configurations.
 @kuza55 jpeg.h and png.h have been updated to explicitly exclude mobile builds so that it's clear we don't support them there.
 @Shaikjalal What's the command line you're giving to strip_unused? It sounds like it's not reading from a valid text-format protobuf file.
 @maelp Sorry, I'm not sure what you mean. AFAICT libjpeg and libpng are not included in any mobile build. The makefile-based iOS build uses https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/gen_file_lists.sh to create the source file list, and this specifically excludes png and jpeg files.

You should be able to get them build if you start by adding them to the dependency tree for //tensorflow/core:android_tensorflow_lib_lite. We just aren't supporting that right now as there are built-in image manipulation options available on mobile platforms.
 @Shaikjalal Can you try running strip_unused with --input_binary=true? It may be trying to read a binary proto as if it were text format.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @sun9700 Could you provide a failing example that's reproducible? Thanks.
 Sorry I meant hdf5 example. BTW, you need to use the primary email of your Github to sign CLA. 
 @sun9700 Thanks for reporting. However, we should fix it in data_feeder instead. Some recent changes broke the example and [this test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/tests/test_data_feeder.py#L137) is not checked on Jenkins since we don't have it installed there. @ilblackdragon @martinwicke Moving forward, are we planning to test those on Jenkins? What's the status on data_feeder refactoring?
 CLAs look good, thanks!

<!-- ok -->
 Yes, we should be running those tests. You can add the h5py package to install_pip_packages.sh (and possibly the 3.5 version of that), then the tests should start running. @caisq FYI in case I forgot something important that would make that a bad idea. 
 @tensorflow-jenkins test this please!
  @tensorflow-jenkins test this please
  There are some known issues with protobuf and large files that we need to work on. Thank you for tracking it down and the report.
 Thanks for the report @kuza55! I'll work on getting a patch into the Android example.
 I've got your suggested fix checked in @kuza55, thanks for that. I'm closing this as fixed.
 Apologies, the code isn't in Github yet, so reopening!
 This should actually be fixed now.
  Looks good at first glance, assiging to @lukaszkaiser - do you know someone who can double check the math portions?
 George Dahl agreed to take a look, he's just joining the right groups on github.
 I took a very quick glance and I believe this PR does the "wrong" thing efficiently, however it is a pretty reasonable "wrong" thing to do. What I mean is that for sparse updates, I don't think this will behave the same was as treating them as dense would. So doing the same optimization, but treating the updates as sparse will produce different results than treating them as dense, even if all the gradients are the same. This property needs to be documented, because one might somehow assume that using the sparse version would do the same thing as the dense one, just more efficiently when you have sparse updates.

Essentially, RMSProp and similar algorithms generate an update for each weight at each step and the approach here discards the updates for weights that have a zero gradient on that step. This gives the rarely updated indices a lower learning rate than they would otherwise have in the dense version of the algorithm since "correct" RMSprop would divide by gradient statistics and amplify the updates for weights that had many zero gradients. However, that said, for sparse updates this might work ok sometimes and it isn't clear what the best thing to do is, so doing the easy and efficient thing could be worth implementing.

I am pretty sure Adam in TF for sparse updates has the same problem as exhibited in this PR, and since this PR was for a feature request spawned by the inconsistency between ADAM supporting sparse updates and RMSProp not supporting them, it makes sense to have an analogous behavior. Of course it would be good to add something to the ADAM docs explaining that for sparse updates it only updates weights with non-zero gradient that step and makes no attempt at correcting for this.

So to summarize, the basic approach of the PR seems OK, but I would like to see the exact algorithm documented better in the sparse case so users can be warned not to think it works the same way as the dense update algorithm. ADAM might also need a doc improvement.
 Sorry, you should comment when you've pushed new changes, otherwise we don't know when the code is ready.  You'll have to resolve conflicts and then update us when you've done so.
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 I think you rebased incorrectly, can you try again?

The only commits that should show up here are your own, even after a rebase.
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 I think it's a flaky test, i will try again  @tensorflow-jenkins test this please
  This change adds in an example of using the static library generated by the makefile in an iOS app, and allows optimization flags to be passed to the makefile on the command line.
 What version of Xcode are you running? I've seen that with versions before 7.3.
  @Nessphoro that answer is incorrect.  RNNs are a stable API.

We will add support for higher rank inputs and nested tuples of inputs and outputs in a backwards compatible way in the future (i can't give estimates, but imagine it'll happen this summer sometime)
 We just moved the code into tf.nn.rnn_cell.*, tf.nn.rnn, tf.nn.dynamic_rnn,
tf.nn.seq2seq, etc.

On Wed, Jun 1, 2016 at 3:45 PM, Pavlo Malynin notifications@github.com
wrote:

> @ebrevdo https://github.com/ebrevdo I've though that some work was
> being done to change how RNNs and LSTMs work because of
> https://github.com/tensorflow/tensorflow/blob/82ff4cd8b0d541ede107d34d8eecc769c91dda11/tensorflow/models/rnn/rnn_cell.py
> And the stale documentation in master with regards to that.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2560#issuecomment-223147293,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtim9pCTcs7xW6mp6aIzgU2UzsdRlCMks5qHguPgaJpZM4IpHF0
> .
 @ebrevdo: Feel free to reassign or mark this contributions welcome if that's best. 
 This is being worked on internally.  I'll see if we have an external github account for the contributor.
 @kosklain Here you go! 
 @kosklain Should we close the issue, or is there more to do?
  #2614 is merged on master. We will cherry pick it for 0.9 tomorrow and build the artifacts...
  Thanks, this is helpful! Will assign to @martinwicke since I think he's setting up the Mac OS X GPU test machine and this should be useful to validate the instructions :)
 BTW, I just tried these instructions after upgrading MacOS and TensorFlow to today's head and they don't seem to work. Something about libcudart not being loaded because I'm running a "restricted program". Sudo doesn't help

```
bash-3.2$ echo $DYLD_LIBRARY_PATH
/usr/local/cuda/lib

bash-3.2$ find /usr/local/cuda -name libcudart.7.5.dylib
/usr/local/cuda/lib/libcudart.7.5.dylib

bash-3.2$ bazel-bin/tensorflow/python/session_ops_test

dyld: warning, LC_RPATH $ORIGIN/../../_solib_darwin/_U_S_Sthird_Uparty_Sgpus_Scuda_Ccudart___Uthird_Uparty_Sgpus_Scuda_Slib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
dyld: warning, LC_RPATH third_party/gpus/cuda/lib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
dyld: warning, LC_RPATH third_party/gpus/cuda/extras/CUPTI/lib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
Traceback (most recent call last):
  File "<console>", line 1, in <module>
  File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/__init__.py", line 23, in <module>
    from tensorflow.python import *
  File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/__init__.py", line 48, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib
  Referenced from: /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found


```
 Yes, tried that, restarted, same issue.

On Wed, Jun 1, 2016 at 10:14 AM, Dr. Kashif Rasul notifications@github.com
wrote:

> @yaroslavvb https://github.com/yaroslavvb can you try xcode-select
> --install and then try again?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223061679,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHCqorwoJnQtIockTMRBWMr32Nqsfks5qHb3cgaJpZM4IpDEQ
> .
 Seems like it's possibly coming from this line in ImageLoader. Somehow
"context.processIsRestricted" is set when I'm running with config=cuda
http://opensource.apple.com//source/dyld/dyld-210.2.3/src/ImageLoaderMachO.cpp

if ( context.processIsRestricted  && (context.mainExecutable == this) ) {
                        dyld::warn("LC_RPATH %s in %s being ignored in restricted
program because of @loader_path\n", path, this->getPath());
                        break;
                    }

On Wed, Jun 1, 2016 at 11:54 AM, Yaroslav Bulatov yaroslavvb@gmail.com
wrote:

> Yes, tried that, restarted, same issue.
> 
> On Wed, Jun 1, 2016 at 10:14 AM, Dr. Kashif Rasul <
> notifications@github.com> wrote:
> 
> > @yaroslavvb https://github.com/yaroslavvb can you try xcode-select
> > --install and then try again?
> > 
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223061679,
> > or mute the thread
> > https://github.com/notifications/unsubscribe/AABaHCqorwoJnQtIockTMRBWMr32Nqsfks5qHb3cgaJpZM4IpDEQ
> > .
 Using brew. I'll try disabling this SIP feature and update in few hours
On Jun 1, 2016 12:35 PM, "Dr. Kashif Rasul" notifications@github.com
wrote:

> @yaroslavvb https://github.com/yaroslavvb how did you install the cuda
> toolkit?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223100791,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHJMg-DxumqCiQEA8aUk9QcTMav5bks5qHd7ygaJpZM4IpDEQ
> .
 Hm, after disabling SIP I'm seeing this [segfault](http://pastebin.com/bbzsTMPq) in dylib, looks something's broken in trying to load those libs from relative path
 @kashif btw, how old is your TensorFlow version where you got this to work? (I've only tested this on version from this morning). Stack trace looks as if it's trying to read past end of string for DSO filename location. Looks like this logic got touched about a month ago:
https://github.com/tensorflow/tensorflow/pull/664/files#diff-1e480f00c75a5f10cda6d37f7b6e3e3dR75
 @kashif 4 days old is pretty recent, seems likely problem is somewhere else

On Wed, Jun 1, 2016 at 2:49 PM, Dr. Kashif Rasul notifications@github.com
wrote:

> @yaroslavvb https://github.com/yaroslavvb my tensorflow is about 4 days
> old... i can merge in master and try to compile again...
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2557#issuecomment-223135531,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHKSzZAPkwO0RbcUmcOusJYt0Tg_Iks5qHf5xgaJpZM4IpDEQ
> .
 Update, I installed CUDA from official website, ran ./configure again, rebuilt and now it works.
 OK, I believe this is an issue of environment variables, probably the `DYLD_LIBRARY_PATH`. I was testing things by running `bazel test` which spawns it's own environment and probably doesn't set that path correctly

so this fails
`bazel test -c opt --config=cuda tensorflow/python:depthwise_conv_op_test
`
but then this passes
`bazel-bin/tensorflow/python/depthwise_conv_op_test
`
 We've had problems with environment variables in bazel test before. :/

It'll take me a while to actually get to installing the GPU test machine. I'll probably have an update then. For now, this looks good. Clarifications to the instructions are always welcome.
 OK, I just did SMC reset, which somehow reset my SIP and this stopped working with same `being ignored in restricted program because it is a relative path` message, and then after disabling SIP, it started working again. So disabling SIP seems to be a requirement for MacOS 10.11.5 (15F34)
  @tensorflow-jenkins test this please
 @vrv, @martinwicke, I've confirmed that with this patch, TensorFlow would build with Cuda 8. Feel free to merge this change. We can always improve it later. 
 test this please
 @martinwicke @caisq our Jenkins is broken   

Merging since this change was verified by @zheng-xq
 @kashif, thanks for getting to this quickly. 

Please name it SE_CUDA_DATA_HALF, since this is stream-executor code, which TensorFlow only hosts a fork temporarily. Other than that, it looks fine. 
 @tensorflow-jenkins test this please
 Thank you @kashif!
  @tensorflow-jenkins test this please
 Looks ready to merge, @vrv?
  It seems like you must have installed Python 3 on your own before installing tensorflow. On my El Capitan machine I do not have /Library/Frameworks/Python.framework at all. Could you let me know how you did that so I can attempt to reproduce this. 
 Closing for now, but please comment if more information is available and I'm happy to reopen.
  We haven't prioritized this since we always needed to run all of our internal tests anyways, but it definitely seems like it would be handy for contributors to get an earlier feedback.
 Any of the "CI as a service" would not work...

They can't run tests because compile alone takes more cpu and memory than limits on any of those services for open source projects - our jenkins nodes have far more cpu and memory and the builds take 15-30 minutes because they are cached. Does any of the free services persist workspace between builds? Even with cache it would not fit free tier limits (with some it would not fit even within paid limits).

Even our sanity job which takes 30 seconds to run in our current setup would take far longer. For example validation of BUILD files using "bazel --nobuild" would have to install bazel, fetch external dependencies - more than 1GB of source code from many git repositories and zip files...

There are a lot more more problems I know of. And we have not even tried it...

Given that said we might be able to enable some light build to be automatically trigger on ci.tensorflow.org. Not the full suite as it is for the time being too expensive.

@Mistobaan is there something in particular you would think should be triggered automatically?
 Closing as infeasible as @jendap discussed.  @Mistobaan: Please let us know if there are particular things we should fire off automatically.
  uh, good question.  @ebrevdo 
 Probably a bug. Will take a closer look on Monday.
 Working on this now.
 Fixed at head.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 make sure to sign the CLA with the email that you used for your git commit.  otherwise we can't accept this
 You need to say "I signed it!" (with exclamation mark at the end) and
respond on the github issue.

Thanks!

On Sat, May 28, 2016 at 12:39 AM, baoblackcoal notifications@github.com
wrote:

> I signed it
> 
> ---
> 
> baoblackcoal@hotmail.com
> 
> From: googlebotmailto:notifications@github.com
> Date: 2016-05-28 11:49
> To: tensorflow/tensorflowmailto:tensorflow@noreply.github.com
> CC: baoblackcoalmailto:baoblackcoal@hotmail.com; Author<mailto:
> author@noreply.github.com>
> Subject: Re: [tensorflow/tensorflow] Added Mnist example that uses RNN
> model. (#2548)
> 
> Thanks for your pull request. It looks like this may be your first
> contribution to a Google open source project. Before we can look at your
> pull request, you'll need to sign a Contributor License Agreement (CLA).
> 
> ???? Please visit https://cla.developers.google.com/ to sign.
> 
> Once you've signed, please reply here (e.g. I signed it!) and we'll
> verify. Thanks.
> 
> ---
> - If you've already signed a CLA, it's possible we don't have your GitHub
>   username or you're using a different email address. Check your existing CLA
>   datahttps://cla.developers.google.com/clas and verify that your email
>   is set on your git commits<
>   https://help.github.com/articles/setting-your-email-in-git/>.
> - If you signed the CLA as a corporation, please let us know the company's
>   name.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub<
> https://github.com/tensorflow/tensorflow/pull/2548#issuecomment-222288150>,
> or mute the thread<
> https://github.com/notifications/unsubscribe/ANYEMkHlNc6JWtTpoEQnHy2iHZuOW28Iks5qF7tYgaJpZM4Io_G2>.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2548#issuecomment-222295421,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAKtfjEjaK7YySE2gCxVLAQtokghiAWcks5qF_EmgaJpZM4Io_G2
> .

## 

Best regards,
Illia Polosukhin
 What email did you use to sign the CLA?  And what email is on the git commits of this PR?
 CLAs look good, thanks!

<!-- ok -->
 @ilblackdragon @terrytangyuan do skflow examples not generally come with tests?
 @vrv Not yet. Some examples require other libraries, e.g. h5py, dask, pandas, etc. Some examples take too long to run. I'll probably add some tests for them after refactoring. 
 @tensorflow-jenkins Test this please
 @vrv Ping. @ilblackdragon We'll probably need to update the examples if we are deprecating things soon. 
  It's an interesting point. It is true that the mathematical definition of a partition is as you say. On the other hand the idea that a partition is actually synonymous with "disjoint subset" is actually really common in CS. Consider disk partitions and memory partitions where a "disk partition" is considered the subset of the disk. So I'm not sure... someone who thinks about things from that common usage would find this totally clear.

The argument for changing it is that the documentation sort of uses both interpretations.  "This function is used to perform parallel lookups on the list of tensors in params. It is a generalization of tf.gather(), where params is interpreted as a partition of a larger embedding tensor."  Here we should probably say "interpreted as partitions of a larger embedding tensor" to be consistent with the CS common usage.
  For instance (today's head, Bazel 0.2.3 on Ubuntu)
`
bazel test -c opt --config=cuda tensorflow/contrib/distributions:chi2_test
I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcurand.so.7.5. LD_LIBRARY_PATH: 
`

I'm setting `LD_LIBRARY_PATH=/usr/local/cuda/lib64`, but `bazel test` starts own environment where `LD_LIBRARY_PATH` is empty. Strangely though, it finds libcublas/libcudnn/libcufft which are in the same directory as `libcurand.so.7.5`

A work-around it to set `LD_LIBRARY_PATH`, and run the test harness through Python stub rather than through blaze test

```
export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
bazel test -c opt --config=cuda tensorflow/contrib/distributions:chi2_test
bazel-bin/tensorflow/contrib/distributions/chi2_test

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally

```
 `ls -al /usr/local/cuda/lib64/libcu*` and report that here? Do you have any special bazelrc entries?
 ```
yaroslavvb@lenin:~$ cat ~/.bazelrc
startup --max_idle_secs=1000000

```

```
yaroslavvb@lenin:~$ ls -al /usr/local/cuda/lib64/libcu*
-rw-r--r-- 1 root root  28585480 May 24 18:12 /usr/local/cuda/lib64/libcublas_device.a
lrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcublas.so -> libcublas.so.7.5
lrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcublas.so.7.5 -> libcublas.so.7.5.18
-rwxr-xr-x 1 root root  23938736 May 24 18:12 /usr/local/cuda/lib64/libcublas.so.7.5.18
-rw-r--r-- 1 root root  28220076 May 24 18:12 /usr/local/cuda/lib64/libcublas_static.a
-rw-r--r-- 1 root root    322936 May 24 18:12 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root    383336 May 24 18:12 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root    720192 May 24 18:12 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root        13 May 24 20:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root        17 May 24 20:48 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxr-xr-x 1 root root  59909104 May 24 20:48 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root  58775484 May 24 20:48 /usr/local/cuda/lib64/libcudnn_static.a
lrwxrwxrwx 1 root root        15 May 24 18:12 /usr/local/cuda/lib64/libcufft.so -> libcufft.so.7.5
lrwxrwxrwx 1 root root        18 May 24 18:12 /usr/local/cuda/lib64/libcufft.so.7.5 -> libcufft.so.7.5.18
-rwxr-xr-x 1 root root 111231960 May 24 18:12 /usr/local/cuda/lib64/libcufft.so.7.5.18
-rw-r--r-- 1 root root 115104400 May 24 18:12 /usr/local/cuda/lib64/libcufft_static.a
lrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcufftw.so -> libcufftw.so.7.5
lrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcufftw.so.7.5 -> libcufftw.so.7.5.18
-rwxr-xr-x 1 root root    447664 May 24 18:12 /usr/local/cuda/lib64/libcufftw.so.7.5.18
-rw-r--r-- 1 root root     42206 May 24 18:12 /usr/local/cuda/lib64/libcufftw_static.a
lrwxrwxrwx 1 root root        17 May 24 18:12 /usr/local/cuda/lib64/libcuinj64.so -> libcuinj64.so.7.5
lrwxrwxrwx 1 root root        20 May 24 18:12 /usr/local/cuda/lib64/libcuinj64.so.7.5 -> libcuinj64.so.7.5.18
-rwxr-xr-x 1 root root   5751400 May 24 18:12 /usr/local/cuda/lib64/libcuinj64.so.7.5.18
-rw-r--r-- 1 root root   1649726 May 24 18:12 /usr/local/cuda/lib64/libculibos.a
lrwxrwxrwx 1 root root        16 May 24 18:12 /usr/local/cuda/lib64/libcurand.so -> libcurand.so.7.5
lrwxrwxrwx 1 root root        19 May 24 18:12 /usr/local/cuda/lib64/libcurand.so.7.5 -> libcurand.so.7.5.18
-rwxr-xr-x 1 root root  51765952 May 24 18:12 /usr/local/cuda/lib64/libcurand.so.7.5.18
-rw-r--r-- 1 root root  51992564 May 24 18:12 /usr/local/cuda/lib64/libcurand_static.a
lrwxrwxrwx 1 root root        18 May 24 18:12 /usr/local/cuda/lib64/libcusolver.so -> libcusolver.so.7.5
lrwxrwxrwx 1 root root        21 May 24 18:12 /usr/local/cuda/lib64/libcusolver.so.7.5 -> libcusolver.so.7.5.18
-rwxr-xr-x 1 root root  37034328 May 24 18:12 /usr/local/cuda/lib64/libcusolver.so.7.5.18
-rw-r--r-- 1 root root  16613348 May 24 18:12 /usr/local/cuda/lib64/libcusolver_static.a
lrwxrwxrwx 1 root root        18 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so -> libcusparse.so.7.5
lrwxrwxrwx 1 root root        21 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so.7.5 -> libcusparse.so.7.5.18
-rwxr-xr-x 1 root root  36816424 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so.7.5.18
-rw-r--r-- 1 root root  44445334 May 24 18:12 /usr/local/cuda/lib64/libcusparse_static.a

```
 Only suggestion I have left, before asking bazel folks is to make sure --spawn_strategy=standalone and --genrule_strategy=standalone are set
 OK, I'll try it and update. One thing that comes to mind, is that the other 4 libraries are loaded successfully, and I remember seeing some BUILD-warning about those libraries in earlier version of bazel, so perhaps there's some BUILD-logic for those 4 libraries

```
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally

```
 Doesn't help (at 07db1806460cbebbbf3abe9174a65eb52c8a63e0, 4 commits behind now)

```
yaroslavvb@lenin:~/tfimmediate_src.gpu/tensorflow$ bazel test -c opt --config=cuda  --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/python:batch_matrix_band_part_op_test --test_output=streamed


I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcurand.so.7.5. LD_LIBRARY_PATH: 
I tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.

```

Running stub directly works

```

yaroslavvb@lenin:~/tfimmediate_src.gpu/tensorflow$ bazel-bin/tensorflow/python/batch_matrix_band_part_op_test
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2155
pciBusID 0000:03:00.0
Total memory: 4.00GiB
Free memory: 3.53GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x1c34f10
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2155
pciBusID 0000:04:00.0
Total memory: 4.00GiB

```
 @yaroslavvb: Does @rayglover-ibm's tweak fix your problem? 
 His solution seems like it would address the issue. At the same time, all the tests pass, so it's not clear TensorFlow actually uses curand for anything.

Simplest way to reproduce the warning:
`bazel test -c opt --config=cuda --test_output=streamed tensorflow/contrib/distributions:chi2_test`
 I'll close it since it doesn't seem to affect anyone. The person who adds some cuRAND-using-op will discover this anyway since their bazel tests will fail
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  @tensorflow-jenkins test this please
  Thanks!  Yes, `tf.Assert` is the right tool.  I'm tempted to not require it since it mucks up the code quite a bit and doesn't affect the correct path, but I agree it's the right thing to do.
 Cool, two more small comments and then I'll fire off the tests.
 Jenkins, test this please.
 @martinwicke: Do you understand the rather unhelpful "invalid syntax" lint error?  The line looks perfectly reasonable to me.
 Currently unknown keyword args are silently dropped; we should report an error instead.  It's unfortunate that Python 2 doesn't have keyword-only arguments.
 Thanks!  Jenkins, test this please.
 Jenkins, test this please.
 @caisq, @gunan: Do you know what's going on with the ci.tensorflow.org failure?
 Jenkins, test this please.
 Jenkins, test this please.
 @ibab: Looks like you'll have to drop the 1 argument test since numpy doesn't support it for the version Jenkins uses.  Ours should, though.
 Jenkins, test this please.
 Ready to merge @girving  ?
 @vrv: Yes!  Thank you for yet another contribution @ibab! 
  Somewhat related, Select gradient is also failing the test when there's GPU

bazel test -j 1 -c opt --config=cuda tensorflow/core:ops_math_grad_test --test_output=streamed

```
[ RUN      ] MathGradTest.Select
I tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:04:00.0)
E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Not found: No registered '_Arg' OpKernel for GPU devices compatible with node n0 = _Arg[T=DT_BOOL, index=0]()

```
 This propagation is inevitable, actually.  If we introduce the extra variable

```
w = tf.exp(x)
```

then `tf.gradients` correctly computes `dy/dw = 0`.  It then computes

```
dy/dx = w * dy/dw = nan * 0 = nan
```

I.e., the issue is unrelated to select.

@yaroslavvb: That seems like an unrelated issue, so it should be filed separately if it's still a problem. 
  @tensorflow-jenkins test this please
  I'm not entirely sure -- since it's a ./configure problem for png (an external repo), pulling in @damienmg or @davidzchen in case they know what might be wrong.
 @damienmg Is there a version of that fix we could incorporate?
 I should have a PR for the cuda autoconf later this week.
 Here is the tracking bug for the cuda autoconf: #2873
  There was a code change not that long ago that added a new argument (embedding_size) to the call of embedding_attention_seq2seq. It looks a bit like you've mixed the versions, maybe copied a newer file while still using old TensorFlow PIP?
 Closing, since @lukaszkaiser seems to have explained the problem.  We don't support mixing models defined in the tensorflow repo with a different version of tensorflow.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Good catch!
  (Which link should we use?  gcr.io/tensorflow/tensorflow or b.gcr.io/tensorflow/tensorflow-full ?
 I will update the documentation. It has to be very old. I don't think "full" was even in the original release 0.5.0.

There are basically two images:
1) gcr.io/tensorflow/tensorflow - smaller but cpu only
2) gcr.io/tensorflow/tensorflow:0.8.0-gpu - bigger but supports nvidia cuda

If you have nvidia gpu use the second otherwise the first.

BTW: Do you really have ubuntu 16.10? :-)
  I'm not entirely sure -- it's possible there's a bug with freeze graph in not preserving the type
 I don't have an update on this yet. From the node names, it sounds like there's something that's expecting to update a variable, and it's getting a constant?
  Hi, how are you building the Tensorflow demo? Are you building the native library as well? Can you check in your .apk (it's a zip file) to see if the .so file is actually there? 
 Right, the .so file is the native library that contains the actual Tensorflow code. Eclipse can't do all the building by itself (unless you configure extra build steps to handle the native libs).

There are instructions for building in the Android example's [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md) file which will cover the native compilation as well.
 Closing, please reopen if you need specific help building the demo (though currently we don't officially support building on Windows).
  Thanks!  I think there's a few more in the codebase that can be removed, can we do that in the same PR?
 LGTM, thanks!
  @tensorflow-jenkins test this please  (not sure if our tests require docker)
 I'm optimistically merging this.  We can rollback if there are problems.
 wait, why? that would make the image large. we should change it back
  It does not return 4. It will return empty string. It should (probably) look for /usr/lib/x86_64-linux-gnu/libcudnn.so

@3XX0 what waas the story why the container doe not contain a symlink from `/usr/lib/x86_64-linux-gnu/libcudnn.so.4` to `/usr/lib/x86_64-linux-gnu/libcudnn.so`?

If we really have to have the number there we should modify the configure script to find the latest number by default. Or maybe even better - we could create a static constant set to CUDNN_MAJOR (macro in cudnn.h). That should work. But the link would be much better if cuda is somewhat backward compatible. We could compile for 4 and it would run even on 5. That will not happen if we look for exact filename containing the version.

@3XX0 what is the nvidia oppinion? We should finally fix this. It is getting back too often.
  - @petewarden 

retrain.py is, as far as I know, an illustrative example.  You would have to modify it to support multiple GPUs like is done with cifar10_multi_gpu_train.py, I believe.

For information about how to do this, StackOverflow might be a better resource -- issues are mostly for bug reports and installation issues.
  You erased all the information we ask in our issue template, can you please include all that information so we can provide more help?
 It looks like your symlinks aren't set up properly -- you have both v3 and v4 installed, libcudnn.so points to so.7.0, which is v3

Secondly, your LD_LIBRARY_PATH doesn't have /home/sal/cuda/lib64 in it
 Remove the existing libcudnn files and recopy libcudnn (just for v4).  You can Google for better instructions about how to set LD_LIBRARY_PATH.
  The real gradient of a complex analytic function is the conjugate of the complex derivative.  Derivation left as an exercise for the reader. :)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  cc @ebrevdo @mrry @josh11b 
 Why not build a separate graph with its own queue for eval?  You'll also get the flexibility of being able to add additional flexibility to append the eval graph for eval purposes.
 People usually use one method to create the core of the code, and a
separate method to take its output and generate the loss and gradients. A
third to generate the eval losses.  This way you can share code and
variables between the two graphs.  See tf.contrib.learn.
On May 26, 2016 3:05 PM, "Mark Woodward" notifications@github.com wrote:

> If this isn't how others are structuring their code, then there is no
> urgency/necessity. I have been pre-computing the number of enqueue()'s that
> will be performed in an epoch, and then running the consumer the
> appropriate number of times, with asserts that the producer is not alive
> and the queue is empty before starting the next epoch.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-222008578
  @wolffg has a solution for this upcoming I believe.
  The whitepaper is probably out of date / incorrect.  See https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#placeholder for up to date documentation.
  @danmane: Unless you're planning to work on this soon, let's mark as contributions welcome. 
  I think this is actually a duplicate of https://github.com/tensorflow/tensorflow/issues/2505  -- the error message is misleading and I'll try to fix that.  But the problem has been fixed at HEAD.

Unfortunately last night's pip binary didn't build properly, so it's still on the previous night's version, which doesn't have the fix.

@caisq do you know why the release binaries keep failing?
  There's no Java API for Tensorflow yet, so this is not possible to do directly.

However, in the Android example project we run inference from Java via JNI: see [tensorflow_jni.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/tensorflow_jni.cc) and [TensorflowClassifier.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorflowClassifier.java)

You'll likely want to use [freeze_graph](https://www.tensorflow.org/versions/r0.8/how_tos/tool_developers/index.html#freezing) on your GraphDef first to merge your checkpoint values in, letting you load it all as a self-contained file.
  @tensorflow-jenkins test this please
  the error message actually says there's no "mod" support for int32 on GPU.

Thankfully, this was fixed yesterday at https://github.com/tensorflow/tensorflow/commit/249106f652591afd0fc36ee4e560c3989287f699 -- can you sync to head and try again?
  There doesn't seem to be a more efficient way. Kernel registrations are
done through C++ registry mechanism, so some non-trivial piping would need
to be done to expose it in Python.
 BTW, when tracking capabilities through code, one thing to keep in mind is
that there are logical devices and "physical" devices that don't
necessarily coincide. For instance, the following
from core/kernels/cwise_op_add.cc shows that "Add" for int32 is registered
for GPU logical device, but the actual implementation will run on CPU.
(This was done to avoid crossing logical device boundary for efficiency
reasons)

REGISTER_KERNEL_BUILDER(Name("Add")
                            .Device(DEVICE_GPU)
                            .HostMemory("x")
                            .HostMemory("y")
                            .HostMemory("z")
                            .TypeConstraint<int32>("T"),
                        BinaryOp<CPUDevice, functor::add<int32>>);
 The fact that word2vec is bound to CPU is historical: `tf.gather` works fine on GPU now.  I believe `LogUniformCandidateSampler` is still CPU-only, though.

Unfortunately making the docs not depend on kernel registrations helps significantly, and we'd lose those advantages if we presented this information in the docs.  However, a runtime feature for checking which ops have which kernels would be great, so I'll mark this as contributions welcome.
  This issue tracker is for core TensorFlow issues. Please reopen this issue on the [TensorFlow Serving issues page](https://github.com/tensorflow/serving/issues).
  https://github.com/tensorflow/tensorflow/pull/2504  can you let me know if that solves the problem ?
  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md

TLDR;
instead of
`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .
`
I had to do
`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* .
`
Ubuntu 15.04, bazel-0.2.3, week-old TensorFlow from head

On MacOS, Bazel 0.2.1-homebrew, the command that works is `ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/* .`
 cc @kchodorow 
 Oops, I guess a week-old version is too old. I just synced to head and now the instructions are current
 @kchodorow btw, perhaps related, at HEAD I'm seeing following ominous-looking warnings

```

WARNING: /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/highwayhash/WORKSPACE:1: Workspace name in /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.
WARNING: /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/re2/WORKSPACE:1: Workspace name in /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.

```
 BTW, current instructions for latest bazel are a slightly broken:
`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow* .
`

It should say
`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .`
 Send a PR to fix?

On Sat, May 28, 2016, 2:16 PM Yaroslav Bulatov notifications@github.com
wrote:

> BTW, current instructions for latest bazel are a slightly broken:
> ln -s
> ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow*
> .
> 
> It should say
> ln -s
> ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/*
> .
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2497#issuecomment-222330071,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAcTeaOcxUT9H3Bb4XylXeUfsKRGksdEks5qGLDBgaJpZM4ImKHo
> .
  Cool, thanks for the report.  http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.pad.html is I think where we get these names from -- do you want to send us a PR to fix this?

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L922 would be where to add the documentation.
  @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 Looks like flakiness in matmul test, trying again.  @tensorflow-jenkins test this please
  @sherrym's out, so reassigning to @josh11b for now.
  @lukaszkaiser any ideas when or why this could happen?  This seems like a python-level issue, not a runtime one...
 Variables are stored with the graph, not session. So it looks like we're changing the session but the graph remains. I'm not 100% sure what's the intention, will try to construct a much smaller example tomorrow (it doesn't look like anything specific to seq2seq, but an interaction between session and variables).
 Indeed, I can confirm that variables are stored in the graph which is not reset with session. So this is behaving as intended. If you want to create a graph twice, separately, do it like this:

  with tf.Graph().as_default():
    train()  # or anything else...
  with tf.Graph().as_default():
    decode()  # or anything else...
  It looks like your embeddings have NaNs, most likely caused by your model training not being stable, which I suspect is not a TensorFlow bug.  You might get more help on StackOverflow for non-bugs like these (unless you can demonstrate to us that there's a bug here).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks for the contribution, can you add a test for these changes so that we don't regress in the future?
 Looks good!  Thanks.  @tensorflow-jenkins test this please
 Sure, will kick off a test:  @tensorflow-jenkins test this please

@ebrevdo should still look at this though 
 Some test failures -- you can test locally by running bazel test tensorflow/... to make sure you catch all failures.
 We have a change internally that should fix nested tuples.  It obsoletes
the rnn changes in this pr.  At our next public push you should rebase and
we can see about the ptb example updates.
On May 24, 2016 9:09 PM, "Vijay Vasudevan" notifications@github.com wrote:

> Sure, will kick off a test: @tensorflow-jenkins
> https://github.com/tensorflow-jenkins test this please
> 
> @ebrevdo https://github.com/ebrevdo should still look at this though
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2490#issuecomment-221468342
  When you create a `tf.train.Saver` with no arguments, it will implicitly use the current set of variables _at the time of Saver construction_ when it saves and restores. If you add a new variable (e.g. `v3` in your second code block), you have to create a new `tf.train.Saver` to save it.

``` python
import tensorflow as tf

sess = tf.InteractiveSession()
v1 = tf.Variable(1,name="v1")
v2 = tf.Variable(2,name="v2")
saver = tf.train.Saver()
saver.restore(sess,'v12.ckpt')  #works fine here

v3 = tf.Variable(3,name="v3")
sess.run(tf.initialize_variables([v3]))

saver_with_v3 = tf.train.Saver()
saver_with_v3.save(sess,'v123.ckpt')
```
  Let me look into this - we have refactored a large chunk of code, including monitoring code so may have missed some parts of functionality.
 FYI, got a fix, but after adding some more tests to check that `ValidationMonitor` actually works correctly got a problem with flaky test (result change slightly every time I run) - trying to debug what is happening there. 

We will cherry pick that fix into the 0.9 release though.
  This file is automatically generated (using `protoc`) from [`error_codes.proto`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/error_codes.proto). If you build TensorFlow using Bazel, the file will be generated for you. Otherwise, you will need to run `protoc` on that file to generate the C++ implementation of that type (and the other protocol buffers that TensorFlow uses).
  This problem should have been fixed in the latest nightly version of TensorFlow. Can you try downloading the nightly PIP package for your platform and see if the problem persists?
 (Closing this due to lack of activity. The problem should be fixed at HEAD, but let us know if it isn't.)
  I would step-by-step copy paste the instructions, since you've typed the installation instructions incorrectly.  Please comment further if you have evidence that our existing instructions are not enough.
  After running `docker run -it ...`, it says the Jupyter notebook server is running at http://[your ip]:8888

If you navigate to that address, you should be able to get things running.  Please comment if that's not the case.
 in your terminal, run

`docker run -it gcr.io/tensorflow/tensorflow`

Once that finishes, in your browser go to http://0.0.0.0:8888
 In addition, you might get more help on StackOverflow, this is only for bug reports and feature requests.
 Alright, then I'm not really sure -- try asking on StackOverflow and you should get some better advice.
  It may be helpful to look at the work I've been doing to get TensorFlow's C++ core building with a plain makefile:
https://github.com/tensorflow/tensorflow/pull/2440
I build the proto_text tool and use that to generate the needed files.
 @petewarden @timbrucks What's the status of this?
 We don't plan on supporting cmake ourselves, we're putting our effort behind the makefile approach for the core runtime instead. Closing this one, since we're not going to support this method.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 I merged this without CLA. We checked with all authors, and all are both ok with merging their commits and have CLAs in place.
 There is no PR to merge it into master right now. More infrastructure work
has to happen before we can merge it to master (from where it would
automatically be sucked into a lot of systems that are not ready for it).

On Tue, May 24, 2016 at 1:42 PM Alonso Vidales notifications@github.com
wrote:

> Hi @jmhodges https://github.com/jmhodges , yes, this PR comes from this
> other one: #1771 https://github.com/tensorflow/tensorflow/pull/1771
> I think that is because the code still has to be integrated on Jenkins and
> reviewed a bit more. By the moment, I think that there is no other PR to
> master.
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2479#issuecomment-221384030
 Yes. I am sorry, I did squash those commits. I should not have. You can
still look at #2479 for the history.

On Thu, Jun 9, 2016 at 4:18 PM Travis Cline notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke it looks like this got
> merged with 1429abd
> https://github.com/tensorflow/tensorflow/commit/1429abd4308b755d5c3a97f2788c55b5cd0f8230
> -- did that lose commit history? I was looking for how much of this still
> was made up of my initial contribution but it looks like that history may
> have been lost.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2479#issuecomment-225054846,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_cp8s1DQd2F1cDTFGDBuyLYLRrvbks5qKJ9VgaJpZM4IkyNX
> .
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  adding comment to see if it triggers the CLA bot
 @tensorflow-jenkins test this please

@zhongzyd is a prior committer and has signed the CLA, so this is fine.
  I use RMSPropOptimizer to optimize parameters, I get NotImplementedError. **But when I change to use AdamOptimizer, it works fine.**

So I try to fix the problem and I find some key point, may be it can help.
Here is my code:

```
self.x = tf.placeholder(tf.int32, [None, sequence_length])  
point = tf.get_variable([len(embedding_matrix)])
............
............
outputs, states = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)
index     =    self.x[:, 0]
index  = tf.reshape(index, [-1,1])
index_point  =    tf.gather(pointt, index)
output   =  tf.mul(outputs[-1] ,   index_point)

scores = tf.nn.xw_plus_b(output, self.W, b)
losses = tf.nn.softmax_cross_entropy_with_logits(scores, self.input_y)
self.loss = tf.reduce_mean(losses) 
optimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.9)
grads_and_vars = optimizer.compute_gradients(self.loss)
self.train_op = optimizer.apply_gradients(grads_and_vars)
```

When I try to change the line `output   =  tf.mul(outputs[-1] ,   index_point)` to some others such as `output   =  tf.mul(outputs[-1] ,   2)`, **the error disappear**. And I try to change to use `bidirectional_rnn`, **the error also disappear.**
 This is known: there is no implementation of RMSPropOptimizer for sparse gradients.  I'll retitle to a feature request.
  Thanks for raising this! We should have a fix coming out soon.
  It's a bug indeed.

On Mon, May 23, 2016 at 11:03 AM, Vijay Vasudevan notifications@github.com
wrote:

> Assigned #2473 https://github.com/tensorflow/tensorflow/issues/2473 to
> @touts https://github.com/touts.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2473#event-669223214
 Alas TensorFlow does not let you replace a shape function by another one,
so you cannot set one temporarily.

You'll have to edit the current shape function in python/ops/array_ops.py
and either run from the python source or rebuild tensorflow and run from
the compiled binary.

On Wed, May 25, 2016 at 12:26 AM, ThomasWollmann notifications@github.com
wrote:

> Any Idea how to fix it? If it's just the python shape function I can
> submit a patch. I want to test it first, so how to set a different shape
> function temporary?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2473#issuecomment-221493918
 I just sent a fix out for review.
  We'll need some more information to debug this. How are you invoking the programs when they hang? What logs do you see from each process, if any?
  @KyungJunAn Could you please clarify how it doesn't work?
 @zffchen78: Could you take a look at this since Sherry is out? 
  From the [implementation](https://github.com/tensorflow/tensorflow/blob/7b4d733593842361d066d7e33a03a07da5dca465/tensorflow/core/kernels/conv_grad_ops_3d.cc#L356) it looks like the `input` tensor is only used for shape information, so your dummy tensor approach should work fine. 
 We should aim for parity with conv2d then.  
 Cc @mjanusz
  Duplicate of #216 
 Agreed, I just wanted to have one issue that tracks them both, since they are closely related and we already have a billion issues.
  This is a question more appropriate to StackOverflow, unless there is a very concrete feature request here.
 @ebrevdo, @yuanbyu  to let me know if this is a feasible feature request
 What is the rank and size of the states that you plan to pass around?
 You're talking about the input shapes, not the state shapes. rnns accept inputs and emit outputs and state. I'm asking about the shape of the state.
  @girving is this just a doc bug?
 Yep, that's a doc bug.  It should be `tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases)` as suggested.  Fixing.
  This has been fixed internally and the next public push should have the fix.
On May 24, 2016 8:33 AM, "Jihun Choi" notifications@github.com wrote:

> After a bit more exploration, I found that it works in case that both
> flags are on, if I apply zero_state for each layer manually and then make
> a tuple of the results.
> However, I think zero_state should be modified, since it is specified in
> the abstract class RNNCell and MultipleRNNCell is a subclass of that
> abstract class.
> I made a PR modifying this issue. Since I started studying TensorFlow
> yesterday, my codes might be awkward!
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2463#issuecomment-221309662
 @ebrevdo: Is this fixed?
 Seems this is fixed.  @jihunchoi feel free to reopen if you're still having issues.
  We provide optimized cross-entropy implementations that are fused with the softmax/sigmoid implementations because their performance and numerical stability are critical to efficient training.

If however you are just interested in the cross entropy itself, you can compute it directly using code from the [beginners tutorial](https://www.tensorflow.org/versions/r0.8/tutorials/mnist/beginners/index.html):

``` python
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
```

**N.B.** DO NOT use this code for training. Use [`tf.nn.softmax_cross_entropy_with_logits()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#softmax_cross_entropy_with_logits) instead.
  We typically use a `tf.train.Saver` (which is implicitly created by the `tf.train.Supervisor`) to handle periodic checkpointing and restoring variables from a checkpoint on restart. Are you using those in your program?
 The checkpoints are written on the server side. The current code assumes that you have a shared filesystem between the client and the servers, such as a shared NFS mount (or gcsfuse, if you're running on GCE).

Note that you can set the supervisor to checkpoint more frequently by overriding the `save_model_secs` argument to the constructor.
 Closing due to inactivity. Feel free to reopen if you still have concerns.
  Thanks for noticing this and we'd welcome a PR!

@zheng-xq: Since you wrote this code, do you have a preference for the behavior here?
  Then truncation is the correct behavior. Thanks for noticing this!
  Not sure what the bug report is.
  @Mazecreator: The way to express this in TensorFlow is to make two different pieces of graph that share variables (either explicitly or using variable scopes).  You then run one part of the graph for training and one part for testing. 
  Before your changes, what were the full output logs from bazel ? (add `--verbose_failures`)
 You probably need to install the zlib development libraries, I guess.
  I think this is working as intended: variables are globally shared by default.

cc @mrry in case I'm misunderstanding something.  You probably want to reset the cluster and/or scope the variable names if you intend to reuse variable names across multiple runs.
  `tf.load_op_library` relies on the system provided mechanism to locate the `.so` file, especially if you don't provide the full path to the file. On linux, the directories present in the `LD_LIBRARY_PATH` environment variable are looked up. In your case, when compiling with `g++`, can you try providing the full path to `tf.load_op_library`, most probably `./zero_out.so`?
If you are compiling using bazel, there is an utility function to give you the full path to the `.so`. Look [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/adding_an_op/zero_out_1_test.py#L36) :

```
tf.load_op_library(os.path.join(
          tf.resource_loader.get_data_files_path(), 'zero_out_op_kernel_1.so'))
```
 Ping. @StrivingMax Does my suggestion work for you?
 The error seems to indicate that you have the `HostMemory` attribute set for an output arg , and a label called `Sergey` in your kernel specification. However, the code you provided doesn't seem to contain those. Can you please provide the exact code for your custom op?
 Closing the issue as there seems to be no outstanding action items. @StrivingMax feel free to open again if you feel that this has not been addressed sufficiently.
  @danmane: Can you comment on this?  It seems like a reasonable thing to mark "contributions welcome", but we should get your input first on how best to express it if so. 
 This is on my roadmap, actually. I want to make a new field in the event.proto that will let you record arbitrary info about the run, including key value pairs like "code version", "vx.x.x", hyperparameters, etc.
I'd like to get this in in-time for the next release, no promises.

It's probably not a great candidate for contributions welcome because I already have a plan to do it, and it requires some API changes.
  @jendap: Want to comment? 
 Switching to @caisq.
  I'm going to close this issue and encourage you to submit this on Stack Overflow instead:

> GitHub issues are for bugs / installation problems / feature requests.  
> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
> To make bugs and feature requests more easy to find and organize, we close issues that are deemed
> out of scope for GitHub Issues and point people to StackOverflow.
  The in-process session (i.e. `tf.Session()` with no arguments) is not designed to be `fork()`-safe. If you want to share a set of devices between multiple processes, create a [`tf.train.Server`](https://www.tensorflow.org/versions/r0.8/api_docs/python/train.html#Server) in one process, and create sessions that connect to that server (with `tf.Session("grpc://...")`) in the other processes.
  @tensorflow-jenkins test this please.
  @benoitsteiner @rmlarsen: Benoit, Rasmus, are there other configuration options for Eigen that could help improve the performance here? Would any of the post-0.8 performance improvements be likely to help here?
 @scroyston Haveyou tried building with the -mfma option as well ? The Mac Pro have shipped with Haswell cpus for quite some time, which support the FMA instructions. This can make a significant difference in performance.
I'll try to take a look at you testcase next week. There are a few things that we can do to improve the scalability of the code for relatively small matrix multiplication, and some of the options we're considering might help in your case.
 The threading issue is suspicious.  @rmlarsen, @benoitsteiner: Any thoughts as to why turning off threading would make Eigen 10x faster?  
  `tf.contrib.grid_rnn` is only available in the nightly builds of TensorFlow (and it will be in version 0.9).  We don't currently publish nightly docker images, but you can modify the [`Dockerfile`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile) by replacing these lines:

```
# Install TensorFlow CPU version.
ENV TENSORFLOW_VERSION 0.8.0
RUN pip --no-cache-dir install \
    http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl
```

...with:

```
# Install TensorFlow CPU version.
RUN pip --no-cache-dir install \
    http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl
```

Follow the Docker instructions for [building a new image on Windows](https://docs.docker.com/windows/step_four/) to complete the process.
 Here's a step-by-step tutorial for using Docker on Windows with a custom Dockerfile:

https://docs.docker.com/windows/step_four/
  I think we can defer adding the strictly-increasing assertions for now. Thanks for this!
 @tensorflow-jenkins test this please.
 @tensorflow-jenkins test this please.
 ok to merge ?
 learning_rate_decay_test.py appears to be a binary file...
 LGTM (modulo binary file concerns).
 I'm not sure -- if you look at the file diffs, I can't even see the contents of the test code
 Cool, looks good.  One more test and we can merge.

@tensorflow-jenkins test this please
  Did you change from having one `Optimizer` per tower to a single `Optimizer`? In that case, it's possible that all of the gradients are being computed on a single GPU (most likely `/gpu:0`), because the `colocate_gradients_with_ops` argument to `Optimizer.minimize()`/`Optimizer.compute_gradients()` defaults to `False`, and so the gradient ops will default to running on a single device.  You could try switching that option to `True` to see if it makes a difference.
 If you `tf.add_n` the losses together, you fuse the two different backward passes into a single backward pass which can no longer be parallelized across multiple GPUs (TensorFlow currently does not split single ops across multiple GPUs).  Thus, one of the GPUs is likely running idle during the backward pass.
 @cgel: Apologies for the brief hallucination: what I said above is of course wrong. 
 @vrv: Do you have any guesses as to the problem?
 No guesses, it would be useful to use the timeline.py and chrome trace viewer to see where there may be gaps in execution. 
  I'm working on automatically generating a makefile from our Bazel dependencies, to make it easier to cross-compile for mobile platforms, and build on systems that can't handle Bazel's resource requirements or dependencies. Supports OS X, Ubuntu, iOS, and Android currently.
 @cg31 No, it's focused on the C++ core needed to load and run pre-trained inference models, which doesn't include the generated cc ops and headers. In this it's similar to the Android libraries we build with Bazel. We haven't defined this core subset of files explicitly before, so I'm hoping to document what we're doing a bit more.

It would also be possible to add the cc ops to the makefile, but I'm trying to keep it as minimal as possible since my focus is porting to resource-limited platforms.
 @timbrucks thanks for the update on your progress! Unfortunately that's a problem I know well, and the summary is that the linker is stripping out global constructors it mistakenly thinks aren't needed.

The longer version is that most components in TF are registered using C++ macros that look like REGISTER_KERNEL("SomeOp", SomeKernelClass).

Under the hood, those create a global variable of a C++ class that registers the component with the main registry as part of its constructor. The problem is, that global is just an implementation detail, and is never referenced by any code. This leads many linkers to conclude that it's not needed and can be removed, even though its constructor has an important side-effect.

The only reliable solution we have is to force the linker to avoid stripping any code from the library, even if it thinks it's not used. There are a variety of different ways to specify that, depending on your platform. On OS X/iOS you use -force_load with the path to the static library. On Ubuntu, we use --whole-archive:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L124

In your case, you should be able to pick the correct linker flags for the platform and it will avoid stripping the global that does the factory registration that's causing your error. Hopefully the linker flags in the makefile should help there.
 @timbrucks you're not considering how slow we will be getting to 1.0. :)
 This looks useful. Does it work?

If so, should we create a build for it?
 We should. Its main purpose is to create an iOS library, for which we could
create a build on the Mac.

On Thu, Jun 2, 2016 at 10:53 PM Jan Prach notifications@github.com wrote:

> This looks useful. Does it work?
> 
> If so, should we create a build for it?
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2440#issuecomment-223496634,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_UFDiO5H3CEANTW5aR_H8luBttf_ks5qH8FPgaJpZM4IjXMh
> .
 I thought it is there for ios. More pain to maintain macs. Ok, I'll leave macs to caisq or gunan ;-)
  Thanks for the report. Because there's an easy workaround, and it seems to come up pretty rarely, I'm not going to prioritize this right now.
  If you grep for "CUDNN_PATCHLEVEL" in your "/usr/local/cuda/include/cudnn.h", what does it say? 
 If you grep third_party/gpus/cuda/include/cudnn.h, do you see the same thing? You could also try to set LD_LIBRARY_PATH to point to the right library. 
 Another possibility is that you are using an older TensorFlow binary. Please check around and make sure you didn't have those. Also look under /tmp/tensorflow_pkg as well. 
  Thanks for the fix!
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @hunkim: The commit author is currently "Your name" (you can see this if you do `git log`).  The CLA check won't pass until you fix the author to your email address. 
 I recently had the same issue, fixed it by following instructions at https://help.github.com/articles/changing-author-info/ to rewrite the commit history with correct author name/email
 Thanks for fixing this.  The file change looks good, but we have to edit the history before the test passes, right?  I suppose I can find out: Jenkins, test this please.
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.  I think it stopped since you rewrote the commit: it isn't smart enough to notice that the tree didn't change.
 @josh11b: Does it make sense to you that the backwards compatibility test is passing with this change?  It's specifically removing registered dtypes.  The change is good, but it's unnerving me that history surgery wasn't required. 
 @hunkim: There was a bug in the backwards compatibility check which Josh fixed.  We'll let you know when that's checked in and pushed.  It's probably easiest to wait until after the test is fixed to update your change for it.  Thank you for uncovering yet another bug!

@sjperkins: Yes, that should be a separate PR.  I think it's orthogonal to this one, so it could happen at any time. 
 @girving is this ready to test / merge?
 @vrv: No, a change which Josh submitted internally on Friday is still not external, so we can't make progress.  Is it possible to speed up the lag when externalizing internal changes?
 I have the external push out for testing.
 Ok, merged the push.  rebase and try again?
 @hunkim: The internal change fixing the test is finally out, so if you rebase the backwards compatibility test should fail again.  Can you verify that and then do the necessary history surgery to make it pass?
 Looks good!  You'll need to squash before submit to avoid a commit with failing tests.  Jenkins, test this please, however.
 @hunkim: Can you squash?
 We can squash for them when merging now.

On Tue, May 24, 2016, 9:20 AM Geoffrey Irving notifications@github.com
wrote:

> @hunkim https://github.com/hunkim: Can you squash?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2435#issuecomment-221324950
 @vrv: Ah, thanks Vijay!  I'll merge as soon as we do a quick check whether this will break an internal-only test.  @hunkim: No need to squash. 
 @hunkim: Thank for the fix! 
  This sounds like an potentially useful feature, but it won't be trivial to develop a good version of this. Assigning to @martinwicke for prioritization.
 This does look like a useful feature. There are a number of such configuration options exposed as Environment variables, with appropriate refactoring it would be straightforward to expose them as "check environment, if not set, read from config file, if not found, apply default" in a uniform manner. 
 That said, this is not going to be worked on right now, if someone wants to pick it up, I'd be delighted to get a PR.
 There are a bunch. The best (and maybe only) way to find out is to grep for `getenv`. I know this is a sad answer.
  This seems like it could be a useful contribution!
  Why does your `np.split(...)` approach not scale? This is an easy way to proceed if your dataset fits in host memory. For larger datasets, you can use the standard reader pipeline to read batches of input at a time: e.g. see the `batch_inputs()` function in the [Inception model](https://github.com/tensorflow/models/blob/dc7791d01c9a6b1fcc40e9e2c1ca107cbd982027/inception/inception/image_processing.py#L407). You could also try [`tf.train.batch()` and related functions](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline) to control how the inputs are batched.

As to why TensorFlow doesn't do this automatically, there are clearly [lots of different ways to do batching](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline), and TensorFlow can't reliably infer the user's intent. Therefore, we provide higher level libraries to allow users to build the appropriate input pipelines. 
 This is mostly questions and requests for help rather than an issue, so I'm going to close to keep the issue tracker focused.
 @girving: More of a feature request. What's the home for those?
 @orome: Unfortunately I think the feature request to have TensorFlow automatically shard ops is too broad to leave as a Github issue. 
 @girving: Not saying it should be here; but wondering where the home for general discussion of features requests is (where they can evolve to more specific issues).
 discuss@tensorflow.org is one option, but on second thought leaving this as an issue is fine.
 @girving: It's certainly within the scope of what's spelled out in the issue submission template.
  With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: 
https://github.com/tensorflow/tensorflow/

Linux Python 3.5 whl files will also be included in future releases.
  We're moving models over to their own repository. Feel free to contribute this to the https://github.com/tensorflow/models repository (or open an issue over there).
 As a brief update, I just noticed that someone has published an implementation of this model on GitHub: https://github.com/GradySimon/tensorflow-glove
  Which version of TensorFlow are you running? You will need to upgrade to version 0.8 for that example code to work.
  Thanks for the suggestion - this looks like it would be better to implement in the C++ code (with a corresponding relaxation to shape function. @keveman is cooking up a patch as we speak.
  The solution is to start the queue runners after you have finished building the graph (which includes the saver). Does that work for you?
 We've gone back and forth on disallowing it&mdash;for example, we could call `tf.Graph.finalize()` in the `tf.train.start_queue_runners()` method, but that would break a lot of code that _mostly_ works. However, it seems reasonable to log an ominous warning, to discourage this kind of code. At least it should help debugging. I'll prepare a change.
  Can you check and see if libtensorflow_demo.so is in the apk, and built for the correct target architecture? Just unzip -v to see the contents. It sounds like the native libraries (the .cc code) are either not getting built, or not getting packaged into the apk properly.

Gradle support is outside the scope of Tensorflow github issues, but I would suggest looking for differences between [this](https://github.com/miyosuda/TensorFlowAndroidDemo/tree/master/jni-build) known working Gradle/Android build of Tensorflow and yours to see if you can spot the difference.
 Hi, were you able to resolve the issue?
  Closing, please reopen if problem persists.
  Tensorflow currently doesn't support too many random operations. It would be nice to have things like bernoulli, batched normal (the existing normal only takes scalars), beta, gamma, and so on.
  The answer here is that it would require extra code to be written and tested, and&mdash;as you point out&mdash;it's not particularly useful :).
  The skflow examples in the master branch have changed to use a new API that isn't present in the 0.8 release. There are two options:
1. Download the examples from the `r0.8` branch: https://github.com/tensorflow/tensorflow/tree/r0.8/tensorflow/examples/skflow
2. Install a [nightly version of TensorFlow](https://github.com/tensorflow/tensorflow#installation) and use the examples from the master branch.
  Thanks @alrojo for the upcoming fix!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We have been seeing this type of misoperation PRs from a couple of GH users recently. Not sure what might be the cause of those.
 @caisq: In that case I'll close this request, because it seems spurious.
 @caisq @mrry Seems like some of theses changes are not on master
  Can you add a `complex128` version of the the binary op version of `testComplex64Basic` in `cwise_ops_test.py`?
 Looks like `complex128` `tf.conj` needs a test too (copy the `complex64` one).
 And `complex128` `tf.select` and `tf.pow` (see testOverload for the latter).  Finally `testOverloadComparisons` should test both `complex64` and `complex128`.
 One more: `sparse_tensor_dense_matmul` needs to test `complex128`.  That looks like it; otherwise the testing is good.
 Unfortunately, TensorFlow doesn't overload the `==` and `!=` operators: they are the same as `is` for `Tensor` objects in Python.  However, it's worth refactoring that routine so that we do test `equal` and `not_equal` for the two complex types.

Putting `complex64` and `complex128` next to each other is good as he suggests.
 Go ahead and add new commits resolving the comments to make it easier to review, and we can squash at the end once everything looks good.
 Yep, `testTensorCompareTensor` sounds good.
 Looks great!  Jenkins, test this please.
 Yep, looks like transpose complex128 needs to be registered on GPU.
 @ibab: What's the error message with `__ldg` on `complex128`?  We do want to preserve `__ldg` semantics here since it can improve performance, so we probably want to write a little wrapper around __ldb in `cuda_kernel_helper.h` that calls `__ldg` twice for `complex128`.

@zffchen78: You recently refactored transpose to be lower compile time; do you have suggestions for how to add `complex128`?  

@ibab: If you're unable to run GPU tests, it's going to be hard to iterate, so it may be worth splitting the changes out of this PR.  I could take over that bit of it if you want.
 @ibab: Cool, `float4` sounds perfect.  I'll take a look. 
 Jenkins, test this please.
 @ibab: Please push your rebased and squashed version (reduced to one commit).
 Woot.  Jenkins, test this please.
 Merged.  Thanks again for the contributions!  This will be great to have for applications of TensorFlow outside standard deep nets.
  This doesn't look like the error is related to the GPU: the `MemoryError` is raised while you are **building** your graph, and before anything will be transferred to the GPU.

It's difficult to tell what's going wrong here without seeing your `train.py` code. One possibility is that your server has a 32-bit version of Python installed for some reason. Can you try running the following snippet and reporting the result?

``` python
import sys
print("%x" % sys.maxsize, sys.maxsize > 2**32)
```
 @zheng-xq: Do you have any ideas what could be causing an exception/segfault in the GPU device construction code path?
 The original `MemoryError` is probably caused by trying to allocate more memory than the machine has available.  In order to debug the segfaults, we'd have to get more information, such as stack traces or information from an attached debugger.
 @erickrf, one possibility is that with a shared server, some other processes are using that GPU. Could you run nvidia-smi and make sure no other is using that GPU? 

A few experiments to try: 
1. Allocate the server to yourself, if only temporarily, and see if the problem goes away. 
2. Add stack trace or printf near here and see what is causing the GPU construction to fail. 
   https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L587
  @gpapan: Assigning you based on your contribution of the original op.
 @chris-scott, it is a bit tricky to write an efficient implementation for general strides and rate based on the current approach. Note however that if rate is a multiple of stride, i.e., `rate = k * stride` for some positive k, then:

```
atrous_conv(input, rate=rate, stride=stride)
```

is identical to

```
atrous_conv(subsample(input, stride), rate=k, stride=1)
```

Can you adopt something like that in your project?
  I'm closing this because it's a question that would be more appropriately handled on Stack Overflow:

> GitHub issues are for bugs / installation problems / feature requests.  
> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
> To make bugs and feature requests more easy to find and organize, we close issues that are deemed
> out of scope for GitHub Issues and point people to StackOverflow.
  Does this problem only occur when you build that target with `--config=cuda`?

@damienmg: You've helped out with a few of these errors before, so perhaps you'll have a quick answer. Is there an obvious cause for `/usr/include` not being in the builtin include paths? As far as I can tell it's explicitly defined in `CROSSTOOL` [here](https://github.com/tensorflow/tensorflow/blob/449d122b46d6e7fa2deba151356ffda200d1be7c/third_party/gpus/crosstool/CROSSTOOL#L62).
 Thanks for persevering @guopei and massive thanks to @damienmg for helping! Damien, is there something we could/should do in the TF/Bazel configuration to make this less likely to happen, or is this just a case of adding documentation?
 Thanks again. @keveman, sorry for picking on you, but would you be able to make these changes (feel free to reassign if someone else would be better)? It looks like there are some longstanding TODOs in our crosstool from the original OSS release, and we might be able to revisit them now.
 @damienmg Damien, are you suggesting I add `/home/.slash_usr/lib/gcc/` to the `CROSSTOOL`? Or is this something that should be done via the `configure` script by querying the `gcc` installed on the system?
  Hi, 
A few questions first:
- have you made any changes to the code?
- which SDK build tools version are you using?
- how did you build the APK?
- What Android API level is the device you're testing it on?

My initial guess after a little googling is that it has something to do with API levels, themes, or Android support libraries (which we don't use -- did your IDE try to add one?).
 Android 4.3 is API level 18. The demo requires 21 due to the camera2 api. I'm not sure why Gradle allowed you to compile it for API 18 when the AndroidManifest.xml specifies 21 (did you change that?), but your errors are almost definitely related to the appcompat dependencies shown at the bottom of the gradle settings you provided.

Appcompat libraries should not be required for tensorflow, as we don't do any special theming or use any new UI elements in the demo. Try getting rid of those, and you'll also need to convert the camera code from the camera2 api to use android.hardware.Camera. You can see https://github.com/hamidb/tensorflow/tree/api20 for an example of this.
 I take it you were able to get past the original error? If so, was the fix removing the dependencies or something else?
 Closing due to inactivity, please reopen if you're still experiencing the issue.
  The documentation is intentionally vague on this point. Currently, you can pass any array-like argument that is supported by `tf.constant()`, but this is an implementation detail, and it is unreliable since `constant_initializer()` can be used to create variables of different shapes. Only a scalar argument has a well-defined meaning (fill a tensor of the given shape with that scalar value) for all possible shapes.
  To sync @nsthorat

test this please
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 The most likely culprit for the failure is https://github.com/tensorflow/tensorflow/pull/2408/commits/d0fa481d59228e39ee4e76d5191633cc7e9d1c29 (since the other changes seem to be Python only, and the failing test itself is unmodified). There were 3 days' lag between the Jenkins presubmit test for that change and its submission, so that could explain the breakage. Might be worth trying to roll it back?
 Looks like https://github.com/tensorflow/tensorflow/pull/2408/commits/d0fa481d59228e39ee4e76d5191633cc7e9d1c29 works in isolation, so there goes that theory. Let's run the tests again, in case it was a flake.
 @tensorflow-jenkins test this please.
 @vrv, should be ready to merge. The cmake build failure is pre-existing.
 OK, looks like the test failure was a flake :). I'll go ahead and merge this one, thanks!
 Merged in https://github.com/tensorflow/tensorflow/commit/b54b59e0e65861a624ff5273308565487c8ac25e.
  @vinhqdang you'll just save your map somewhere
 This kind of question should be asked on StackOverflow, since it is a question about how to use TensorFlow rather than a bug in TensorFlow itself.
  This looks like a duplicate of #2109. Closing this so we can merge the discussion over there (but feel free to reopen if it looks like this is a different issue).
  refer to `https://github.com/tensorflow/tensorflow/issues/2040`, they have solved this problem.

`cp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/`
 Thanks for replying @zszhong! Closing this as a duplicate of #2040.
  Thanks for the update!
  This question comes up quite often on Stack Overflow, and @Remper has pointed out a good solution. You can also see a Stack Overflow answer about the same problem [here](http://stackoverflow.com/a/33713196/3574081).
  I think the most realistic code for generating TFRecord files of Example protos is here: https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py

However, it's not quite clear what you're trying to do. I'm closing this issue so we can move it to Stack Overflow.
  @tensorflow-jenkins, test this please!
 LGTM, thanks!
  It sounds like you have checked out the `master` branch of gRPC. Subsequent updates have added a new dependency on `nanopb`, which we haven't added to TensorFlow yet. The solution is to check out exactly the named commit `3d62fc6` in your local grpc repository.
  The SummaryWriter API doesn't let you append to an existing file, but you can use TensorBoard to visualize multiple files in the same log directory.

Assigning to @danmane in case there are any other tricks you could use.
 Correct, if you point TensorBoard to the directory containing event files, then creating a new event file within that directory will be fine, and tensorboard will combine the runs.

N.b. you should use a clean directory for each separate training job.
  It looks like the `pip` is trying to remove an old version of a system library. To avoid version clashes, we recommend that you follow the [virtualenv installation instructions](https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#virtualenv-installation).
 One [suggestion from the FAQ](https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#mac-os-x-oserror-errno-1-operation-not-permitted) is to use the `--ignore-installed` flag when running `pip install`. Depending on the error message, you may need to pass `--ignore-installed six` or `--ignore-installed numpy` to ignore the appropriate package. 
  Since you have a Xeon CPU, it is less likely your CPU is the bottleneck, however, you have 8 workers but only 1 PS which is also very busy processing the input for the GPUs, that PS is likely to get congested and become the bottle neck. Two things you can try here:
1. Run the TF multi-GPU version with 4 GPUs locally and see if and how much slower it will be compared to 1 GPU.
2. Use more hosts from the network as the PS. At least make 2 PS (one on each machine) and see if and how much improvement it could get.

LMK how it goes.
 This 4 GPUs is not the distributed setting, right? If so, that means the slow down is not related to the distributed setting then.

I thought Xeon should be good enough to process images for 4 GPUs. Can you try to increase the threads for the preprocessing? What is the current number of threads?
 It should be just slightly slower than 1 GPU version if the CPU is powerful
enough and the connection to the 4 GPUs are also fast enough. People have
reported that with one core i7 with 4 Titan X, the system cannot feed GPUs
fast enough. Since you have a Xeon, maybe you can try increasing the
threads and see if it helps.

On Wed, May 18, 2016 at 9:59 PM, ZhuFengdaaa notifications@github.com
wrote:

> Yes, 4 GPU is not the distributed setting. OK, I know the point. I will
> adjust the Multi-GPU version until it is close to the one-GPU version. _And,
> according to your experience, Mult-GPU version is just slightly slower than
> one-GPU version, right ?_
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2397#issuecomment-220227723
  Please install latest version, .e.g. one of the recent nightly builds
 @nguyendn: See the answer to #2421. You need to install a **nightly** build to run the examples on the `master` branch, or run the examples from the branch that corresponds to the release (https://github.com/tensorflow/tensorflow/tree/r0.8/tensorflow/examples/skflow).
  You can call `Tensor.get_shape().ndims` to get the (inferred) rank of a tensor, but it is allowed to be `None` if the rank is unknown. Or you can call `tf.rank(tensor)` and the result is a `tf.Tensor` that you have to evaluate (e.g. using `Session.run()`) in order to get an integer.

Neither of these is quite the same as the NumPy property, so I don't see the advantages of having an identical interface here.
 It's possible but rare :) (at least in reasonable-quality code...). For example you can create one with `tf.placeholder(tf.float32)`, but you're encouraged to use `tf.placeholder(tf.float32, shape=[...])`. Unfortunately, we have some highly dynamic custom and legacy op code that requires this behavior....
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @tensorflow-jenkins test this please.
 @tensorflow-jenkins test this please.
 common_runtime_gpu_gpu_debug_allocator_test failed. Testing this again to see if it is a flaky test.
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 The common_runtime_gpu_gpu_debug_allocator_test failure seems to be reproducible:

==================== Test output for //tensorflow/core:common_runtime_gpu_gpu_debug_allocator_test:
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Running main() from test_main.cc
[==========] Running 7 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 7 tests from GPUDebugAllocatorTest
[ RUN      ] GPUDebugAllocatorTest.OverwriteDetection_None
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
[       OK ] GPUDebugAllocatorTest.OverwriteDetection_None (275 ms)
[ RUN      ] GPUDebugAllocatorTest.OverwriteDetection_Header

# [WARNING] external/gmock_archive/gmock-1.7.0/gtest/src/gtest-death-test.cc:825:: Death tests use fork(), which is unsafe particularly in a threaded context. For this test, Google Test couldn't detect the number of threads.
 @caisq: That warning looks ominous, but it doesn't appear to be new in gmock, nor is the death test in that target new. Is there a good way to run that test in isolation on the test machine?

@keveman is trying to reproduce this locally. If he doesn't succeed, I propose we continue the push and disable the test for now.
 @tensorflow-jenkins test this please.
 @caisq: @keveman figured out the breakage (due to a subtle change in the behavior of our CUDA code across `fork()`). Can you please try running the tests again, in case @tensorflow-jenkins doesn't listen to me? :)
 Jenkins, test this please.
  Can you share the code for your modified file? In particular, it looks like there is a big `#ifdef GOOGLE_CUDA` in that file, so my first suspicion is that the registration might not be executing.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Is this a misoperation?
 I am pretty sure this is misoperation. Closing the PR.
  @tensorflow-jenkins test this please
 some other tests failed, please take a look
 Thanks, seems reasonable -- I guess we were testing that float64 wasn't supported explicitly...

@tensorflow-jenkins test this please
  With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: 
https://github.com/tensorflow/tensorflow/

Linux Python 3.5 whl files will also be included in future releases.
  This type of question is better asked on StackOverflow, since it isn't about an improvement or bug fix to TensorFlow itself.
  You also need to start the chief queue runner which will start the synchronization thread. Please follow the comments and example in the sync_replicas_optimizer.py. Another example is the inception_distributed_train.py in tensorflow/models/inception
 Actually the inception distributed train is using slim which has its own trick related to device setter. For you case, can you try follow the instructions for the replica_device_setter:
  # To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
  # jobs on hosts worker0, worker1 and worker2.
  cluster_spec = {
      "ps": ["ps0:2222", "ps1:2222"],
      "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
  with tf.device(tf.replica_device_setter(cluster=cluster_spec)):
    # Build your graph
    v1 = tf.Variable(...)  # assigned to /job:ps/task:0
    v2 = tf.Variable(...)  # assigned to /job:ps/task:1
    v3 = tf.Variable(...)  # assigned to /job:ps/task:0
 Can you remove all the checkpoint before starting the run?

If you want to have checkpoint, you need to make sure the path is
accessible from all PS and chief worker.

On Wed, May 18, 2016 at 12:52 AM, smartcat2010 notifications@github.com
wrote:

> @jmchen-g https://github.com/jmchen-g
> Thank you very much for your quick help!
> I updated my code following your guide. I run 1 ps and 2 workers like this:
> 
> cluster_spec = tf.train.ClusterSpec({ "ps":["10.141.33.61:2222"], "worker" : ["10.141.33.61:2223", "10.141.33.65:2223"] })
> with tf.device(tf.train.replica_device_setter(cluster = cluster_spec)) :
> 
> My code file:
> mnist_softmax.py.txt
> https://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt
> 
> Then worker0 runs smoothly, worker1 shows the following error:
> 
> I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.141.33.61:2222}
> I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {10.141.33.61:2223, localhost:2223}
> I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223
> Traceback (most recent call last):
>   File "./mnist_softmax.py", line 83, in <module>
>     tf.app.run()
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 30, in run
>     sys.exit(main(sys.argv))
>   File "./mnist_softmax.py", line 80, in main
>     run_training(server, cluster_spec)
>   File "./mnist_softmax.py", line 70, in run_training
>     _, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y_ : labels_one_hot })
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 340, in run
>     run_metadata_ptr)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 564, in _run
>     feed_dict_string, options, run_metadata)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 637, in _do_run
>     target_list, options, run_metadata)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 659, in _do_call
>     e.code)
> tensorflow.python.framework.errors.InvalidArgumentError: Expected size[0] in [0, 0], but got 1
>      [[Node: get_local_step = Slice[Index=DT_INT32, T=DT_FLOAT, _device="/job:worker/replica:0/task:0/gpu:0"](local_steps_S11, Reshape, get_local_step/size)]]
> Caused by op u'get_local_step', defined at:
>   File "./mnist_softmax.py", line 83, in <module>
>     tf.app.run()
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 30, in run
>     sys.exit(main(sys.argv))
>   File "./mnist_softmax.py", line 80, in main
>     run_training(server, cluster_spec)
>   File "./mnist_softmax.py", line 45, in run_training
>     train_step = opt.minimize(cross_entropy, global_step = global_step)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 192, in minimize
>     name=name)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py", line 334, in apply_gradients
>     name="get_local_step")
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py", line 217, in slice
>     return gen_array_ops._slice(input_, begin, size, name=name)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1318, in _slice
>     name=name)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 655, in apply_op
>     op_def=op_def)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2154, in create_op
>     original_op=self._default_original_op, op_def=op_def)
>   File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1154, in __init__
>     self._traceback = _extract_stack()
> 
> When I change it to run the 2 workers in the same machine as follows, then
> both of the workers runs smoothly.
> cluster_spec = tf.train.ClusterSpec({ "ps":["10.141.33.61:2222"],
> "worker" : ["10.141.33.61:2223", "10.141.33.61:2224"] })
> 
> So is there something wrong in my multi-machine code?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-219951942
 Looks like you only specified 1 worker? In your case, they should be 2 instead.

```
            **num_workers** = FLAGS.gpu_cards # Use only 1 worker for test.
            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate = num_workers,
                replica_id = FLAGS.task_index, total_num_replicas = num_workers)
```
 No problem. Glad it worked~:)

Ya, we might add an example at some point. Thanks for the suggestion.

On Wed, May 18, 2016 at 8:43 PM, smartcat2010 notifications@github.com
wrote:

> @jmchen-g https://github.com/jmchen-g
> Thank you very much! It's my stupid mistake of specifying 1 worker in
> previous code. Now it works on multi-nodes/multi-cards after fixing my bug.
> Really sorry for bring so much trouble to you.
> My fixed code which works:
> mnist_softmax.py.txt
> https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt
> 
> Maybe it's a good idea to put the usage of Sync-mode into tutorial
> https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html.
> The current tutorial only has a Async-mode code example. Although Inception
> example code uses Sync-mode, it also uses more complex things such as slim,
> queues, so it's not easy for junior user to start.
> 
> Thanks a lot again for your kindly help!
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-220220342
 @s0okiym: It sounds like you might be having a different problem. Please open a new issue and describe exactly how you are invoking the various binaries.
  This looks like a bad error message. Can you try printing the dtypes of all elements in `x`? My guess is that they have incompatible types....
 The most likely culprit is an invalid value in the list - e.g. one that cannot be converted to a tensor. I'm about to submit a change that will improve the error message here, so it should help to discover the real problem.
 Looking closer at the exception backtrace, it is failing on [this line](https://github.com/tensorflow/tensorflow/blob/449d122b46d6e7fa2deba151356ffda200d1be7c/tensorflow/python/ops/rnn.py#L128) in `rnn.py`:

``` python
array_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)
```

This suggests that either `batch_size` or `cell.output_size` has the wrong type. With the forthcoming change it will be easier to tell which one of these is incorrect, but perhaps you can add some debugging code to find out?
  Thanks for the report! It might take some time to figure this out, but we'll get to the bottom of it.

First of all, was it only the PS task that died? Do the worker replica tasks show an upward trend in memory consumption over time?
 @mrry: Any suggestions for further debugging?  I don't think we have enough information to diagnose just yet.  If it's an actual memory leak valgrind could help, but that would be difficult to set up.

@mmuneebs: The summaries should be written to disk soon after they are generated, so I don't think that's the issue.  The allocator warnings are likely a symptom, not a cause.
  Examples for tf.learn [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/skflow) should make things easier for you. 
 Questions like this should go on StackOverflow, not Github Issues.
  My guess is that your variable creation statements in `_make_encoder()` and `_make_decoder()` are implicitly creating large constants in the graph - but I'm not sure without knowing the implementation of `rboltz.fit(...)`. What is the return type from that function?
 @mrry: How does one set a variable without creating a `tf.constant`?
 @girving: You can initialize it from a `tf.placeholder()` and feed a value when you run the variable's initializer op. There's no sugar for it, but something like the following should work:

``` python
init_val = np.array(...)  # Construct a large numpy array.
init_placeholder = tf.placeholder(tf.float32, shape=init_val.shape)
v = tf.Variable(init_placeholder)
# ...
sess.run(v.initializer, feed_dict={init_placeholder: init_val})
```
 @mrry: Alas, that's a bit horrendous, but it would certainly work.  If you have a good idea for sugar we should file a more specific bug, but I'm going to close this one for now. 
  @wb14123: A Python loop doesn't support automatic gradient computation, which is the typical reason to use the fancier control flow ops.  I'm going to close this for now, since questions about how to use TensorFlow are better suited to StackOverflow (use the `tensorflow` tag). 
  ping for @ilblackdragon  (i hope you are getting these mentions)
 Ok, we plan to move / change a bit this function, so it would be good to have this PR in for informing use cases.
 @tensorflow-jenkins test this please!
  Hi @nmiculinic Thanks for contribution, but we actually refactored both code to run evaluation (via `evaluate` method which now is verbose by default) and monitoring (change a API as well).

I'll close this one for today, please try new APIs and see if there is missing functionality there.
Thanks!
  @tensorflow-jenkins test this please
 Merged. Thanks.
  NHWC will be supported by converting to NCHW internally back and forth. Our experience is that in most cases that is okay. If you absolutely don't want that overhead, you can switch your entirely model to NCHW through data_format, and that will be fine. If for some ops, NCHW support isn't available, either a conversion can be added, or add new support. 
 @zheng-xq: Should I leave this assigned to you, or is it already done? 
 We are still waiting for the BatchNorm kernel to be merged. Support for other formats will proceed afterwards. 
  It looks like you're invoking this from a script called `BuildConvNet.py`. Can you share the source of that file?
 OK, it looks like your GPU is running out of memory when you try to run the eval. You seem to have a GTX 750 with 2GiB of RAM. It's possible that copying all of the MNIST test images and labels to the GPU exhausts its memory. I'd suggest batching the test data (e.g. taking 50 images at a time) and aggregating the accuracy in your Python code.
  I'm closing this as it doesn't seem relevant to TensorFlow.
  @tensorflow-jenkins test this please
 @girving @vrv
 I don't follow your message.  Are you saying that your new value helps after you change the network to a different topology?  What are the evaluation accuracies before and after your change?
 @vincentvanhoucke: Can I get your eye here?  His change seems reasonable to me (I would think 1e-2 is a better std for both conv layers). 
 @keiji your solution doesn't exactly replicate Alex's model. You use convolutional layers for local3 and local4, whereas Alex used locally-connected layers that do not share their parameters across patches. My guess is that's possibly one reason why you see such a strong effect of initialization.
Come to think of it, I don't know how one would do that in TensorFlow. @girving do you know who wrote the tutorial and whether they had a specific trick in mind for locally connected, non-convolutional layers?
 @vincentvanhoucke: To make sure I understand: you mean a convolutional layer but using separate filters for each output point?  You could do it with batch matmul and a bunch of reshaping / tiling logic, but it would be quite slow.  I think we'd need a custom op to it fast.  And actually even the reshaping / tiling logic may be out of reach in a performant way.
 @girving yes, that's what the original Cifar10 model uses. We should remove that exercise if we don't know ourselves how to do it well at this point, unless the author of the tutorial has something up their sleeve I don't know.
 @shlens: Looks like you wrote parts of the deep_cnn tutorial.  Do you know a trick for untied convolutions?
 There is no trick. We envisioned the exercise to be difficult because a user would need to invoke several tiling, reshape and matmul operations. Most of the challenge would be in the construction of the tiling operation across the local3 and local4.
 I'm going to close this PR. Please reopen if you feel strongly that the defaults should match cudaconvnet. As it stands, I'm not convinced it's an issue.
  I think the sample code doesn't consider the issue because it doesn't affect neural network performance.  If you're seeing significant differences between zero padding an new-token padding, I would use a mask as @LittleYUYU suggests.  `embedding_lookup` doesn't handle this by itself since we try to keep the semantics of each op simple and focused.
  @tensorflow-jenkins test this please
  I think the problem here is that you have a file called `tensorflow.py` in your current directory. Python will interpret `import tensorflow as tf` as an instruction to load that script, rather than the installed TensorFlow module. Renaming that file to something else should fix the problem.
 No problem, it's happened to me a few times too! :)
  Thanks for this fix!
  Scatter add mutates one of its inputs - it's more like "scatter assign-add" - so I don't think it the current gradient system would support it. It certainly doesn't have a registered gradient function.

I think this is covered by #2358 ("scatter_add for non variable tensors"). If there were a functional scatter_add-like op that operated on regular tensors, the concern about mutation would go away.

I'm going to close this issue so that the discussion is in one place (on #2358).
  can you `import ssl` in python?
 This is a known issue with some `pip` installations. The workaround is documented [here](https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#sslerror_ssl_verify_failed): simply download the `.whl` file using `curl` or `wget`, then pass the name of the downloaded file to `pip install` in place of the URL.
  I am trying to compile tensorflow from source. I can build it **successfully** with CPU support only( i.e. not use `--config=cuda`) .

But when I try to build it with GPU support, I get error:

```
[chaowei@node07 tensorflow]$ export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'
[chaowei@node07 tensorflow]$ 
[chaowei@node07 tensorflow]$ /gpfs/home/chaowei/download/bazel-0.1.5/output/bazel  build -c opt --config=cuda --linkopt '-lrt' --copt="-DGPR_BACKWARDS_COMPATIBILITY_MODE" --conlyopt="-std=c99" //tensorflow/tools/pip_package:build_pip_package
...........
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
ERROR: /gpfs/home/chaowei/.cache/bazel/_bazel_chaowei/2ce35f089de902cec16e4a2c6a450834/external/grpc/BUILD:485:1: C++ compilation of rule '@grpc//:grpc_unsecure' failed: gcc failed: error executing command /gpfs/home/chaowei/software/gcc-6.1.0/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 ... (remaining 39 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
external/grpc/src/core/compression/message_compress.c:41:18: fatal error: zlib.h: No such file or directory
 #include <zlib.h>
                  ^
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 71.894s, Critical Path: 58.77s
```

**I also compile python3 from source in my computer. And when I `import zlib`, it works fine.**

Here is the information of my system:
`[chaowei@mgt ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 6.5 (Santiago)`

```
[chaowei@node07 gcc-6.1.0]$ gcc -v
built-in specs。
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/gpfs/home/chaowei/software/gcc-6.1.0/libexec/gcc/x86_64-pc-linux-gnu/6.1.0/lto-wrapper
Target：x86_64-pc-linux-gnu
Configured with：./configure --prefix=/gpfs/home/chaowei/software/gcc-6.1.0
Thread model：posix
gcc version 6.1.0 (GCC) 
```

I wonder  why I get `zlib.h` error  when I only build tensorflow with GPU support.
 @hholst80 I have tried `yum install zlib-delve` before and it showed `zlib-devel has been installed`
 @333caowei: Where is your `zlib.h`? 
 Closing as duplicate of #2536, which was filed later but seems to have a more useful thread.
  @ilblackdragon: Is this a duplicate of #2167?  If so please close as such. 
  This kind of question is better suited to StackOverflow or an email to discuss@tensorflow.org.  It is not an issue with the TensorFlow code.
  Use `tf.sparse_to_dense`.
 Actually, by "this operation" do you mean the whole thing, or do you want to allocate a `Z` and do a bunch of separate scatters into it before deallocating it?
 @altaetran: We could certainly make a deallocation op, but I don't think we have one at the moment?  However, using it would be somewhat awkward. 

@yuanbyu: Do you have an ideas?
 Non-Variable tensors are immutable, so supporting scatter into them would break an important part of the model.  The uninitialize op is much easier.  Would you be interested in submitting a patch? :)
 Judging by #2367, it appears that @altaetran also requires gradients for this operation. It sounds to me like a functional op would be preferable for this purpose.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @ilblackdragon @martinwicke This may have conflicts with my last PR with baseEstimator
  @tensorflow-jenkins, test this please.
 Change looks good to me, thanks!
  Have you tried one of the nightly builds? https://github.com/tensorflow/tensorflow#installation
 Great. Please close this issue and submit new one if you encounter new problems.
  Use `tf.train.shuffle_batch`.
 @ebrevdo: Is there a way to get both bits of functionality in the same routine currently?  If not, what would be the best way to support it? 
 Right now, batch and batch_join use a PaddingFIFOQueue if dynamic_pad=True otherwise a FIFOQueue.  shuffle_batch uses a ShuffleQueue which does not do batchwise padding.

Maybe by combining the PaddingFIFOQueue with the ShuffleQueue underneath if someone passes dynamic_pad=True to shuffle_batch (which right now doesn't have that parameter)?  Marking as contributions welcome.  I don't have bandwidth to work on this in the near future.
  Following on cgorman's comment: based on your `session.run` call, it look like you aren't actually running the summary op, or adding the output of the summary op to the SummaryWriter. 

Please take a look at the example code [here](https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py)
 @kangxin That's strange. Based on the output from --inspect it looks like TensorBoard is detecting the scalar and histogram data.

Can you upload the events file so I can inspect them?
 Hi @kangxin, 
I've loaded your event files on my machine, and TensorBoard is displaying them properly. So it seems like the problem is not with the data itself, but with something specific to your TensorBoard or frontend.

It looks suspiciously similar to #2607, where someone reported that the demo instance at https://www.tensorflow.org/tensorboard/index.html#events is not showing them events or histograms, so maybe it is a problem affecting a particular browser/OS.

Please check if that url is working for you. If it is broken, then please tell me which browser and operating system you are using. If it is not broken, then there's probably something wrong with your particular TensorBoard, so please give me the following info: 
- Which version of TensorBoard are you using
- What browser and operating system are you using
- Does the issue still re-occur if you use a pip-installed TensorBoard? 
 OK, I've fixed that at head, see: https://github.com/tensorflow/tensorflow/commit/53344688b8be0bb3e73431b1e5a0ad924c024732

That said, we only guarantee support for Chrome and Firefox, and performance and experience are likely to be best with Chrome.
  I think we do want to add a test for double -- that commit you linked was removing test cases, I think.
 Actually I'm not sure what's going on now -- if nn_test is passing right now without the GPU double kernel registered, then that kernel isn't being tested.
 What I'm asking is for a test of the L2Loss op -- you've added the registration but we have no idea if it works for double!  So you should be able to write a test that fails without the registration, and passes after it.
 I'm not sure, can you try passing 'log_device_placement=True' to the session options passed to test_session to see what's going on?
 You should still add a test for float64 though, otherwise it's just completely untested.

You can then use log_device_placement temporarily to verify it's running on the GPU.
 Nice, let's also add one https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_test.py#L119 as well.  something like

```
for dtype in [tf.float32, tf.float64]:
  with self.test_session():
    x = tf.constant(..., dtype=dtype)
    ... same as before ...
```
 @tensorflow-jenkins test this please
  Only organization members are allowed to run the tests.  I'll fire them off once you make the suggested simplification.
 Hmm, sorry I wasn't reading this carefully enough.  I think we should just throw a `ValueError` with a reasonable message if those two numbers are equal, since it is a useless queue.  @yaroslavvb: Since you filed the original bug, does that sound right to you? 
 ping for @girving and @yaroslavvb 
 @girving Correct, it's a useless queue. With my "convert_to_tensor" suggestion it will fail with "tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: min_after_dequeue 4 must be < capacity 4" which is more informative than original division by zero in Summary construction. Throwing explicit ArgumentError sounds good to me too
 Closing due to inactivity, comment and we'll reopen and retest, etc.
  fixed wrong indexing into dict; every picture is read into 3 channels even if input is grayscale; loaded jpeg is read as rb buffer.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I signed it!
 CLAs look good, thanks!

<!-- ok -->
 One fix of this PR seems to aid the following issue: #2304 .
 Seems fine but are there any tests that should have caused this to fail?  cc @petewarden 
 Good catch! It would have been good to catch this with a test, but the training apparently still produced good results even with black and white images, so it wasn't obvious.

Jenkins, test this please.
  I don't think cuda supports host gcc of >= 5 yet, so you'll have to install gcc 4.8 or 4.9 and use that for nvcc's host compiler
 I've built gpu-tensorflow with gcc 5.3.
 I think that I have fixed the problem in Eigen with these 2 commits: https://bitbucket.org/eigen/eigen/commits/1ba80c847ac312333e13f2274c6e69d4654966d9 and https://bitbucket.org/eigen/eigen/commits/0e3f9602b2aa787d851c6aa85930fa1dcac7f693

The problem will be fixed in tensorflow as soon as we update the build rules to pull a version of Eigen which contains the fixes. 
 I updated TensorFlow to pull the latest version of Eigen that contains the fixes for the compilation errors, so I'm closing this issue. Let me know if the fix doesn't work and I'll reopen this bug report.
  why this test only?
 also this doesn't select based on platform
 Do you understand why it fixes anything, and what the implications are for setting this globally?  Cause I don't :)
 Jenkins, test this please.
 We probably want to accept this anyway, and remove it once the fix has made its way into a bazel release.
 @tensorflow-jenkins test this please
  This version implements Immediate execution for native TensorFlow ops with graph caching:

IE, you can do

```
    tf = immediate.Env()
    val = np.ones(())
    tensor1 = immediate.Tensor.numpy_to_tensor(env, val)
    tensor2 = immediate.Tensor.numpy_to_tensor(env, val)
    tensor3 = tf.add(tensor1, tensor2)   # executes operation immediately
    tensor3 = tf.add(tensor3, tensor2)   # executes operation without modifying graph
    print tensor3

```

What's not implemented yet (rough implementation detail is in the design doc):
- support for "tf.nn" namespace
- support for manually created Python ops
- caching for get_session_tensor and get_session_handle ops

@yuanbyu @keveman 
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 I signed it!
 If I click on the link it gives me, it shows that I signed
![screen shot 2016-05-12 at 6 01 40 pm](https://cloud.githubusercontent.com/assets/23068/15234921/c1cb14a2-186b-11e6-8e6f-674305907bff.png)
 rats, I did all my commits as "yaroslavvb@Yaroslavs-MacBook-Pro.local"
 CLAs look good, thanks!

<!-- ok -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @adarob using `tf` or some other name is up to the user, the framework itself doesn't change anything in the original `tf` or `tensorflow` namespace. You could use `tfi` instead of `tf` for instance. I've been using `tf` in order to reuse existing tests, but run them in immediate mode, but when you want to mix things, I would use `tfi`. Also, Python's symbol resolution happens during module loading time, so when SyntaxNet is loaded it remembers what `tf` was during that time, so if you redefine it later, it won't be affected.
 You should do a rebase, I think something went wrong in the one you did. Try making a clean branch from tensorflow/tensorflow, cherry-picking your changes there, and then force-pushing it onto your master.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Something is wrong with your fork. I at this point, I would recommend to keep your changes someplace safe, delete the fork, and re-fork. Then add the new fork as a remote to your existing local copy, and do something like

```
git remote add new-fork git@github.com/yaroslavvb/tensorflow
git checkout new-fork/master
git cherry-pick all-your-actual-changes
git push new-fork
```

Or similar. In the end, you should have only a few commits on top of master when you make the PR.
 Thanks for the tip, I'll give that a try

On Tue, May 31, 2016 at 9:46 AM, Martin Wicke notifications@github.com
wrote:

> Something is wrong with your fork. I at this point, I would recommend to
> keep your changes someplace safe, delete the fork, and re-fork. Then add
> the new fork as a remote to your existing local copy, and do something like
> 
> git remote add new-fork git@github.com/yaroslavvb/tensorflow
> git checkout new-fork/master
> git cherry-pick all-your-actual-changes
> git push new-fork
> 
> Or similar. In the end, you should have only a few commits on top of
> master when you make the PR.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2346#issuecomment-222748552,
> or mute the thread
> https://github.com/notifications/unsubscribe/AABaHKJG2KnVZP1mKPgRabCoKHj_3frLks5qHGYDgaJpZM4IdnId
> .
 I accidentally committed debug changes to files outside of contrib/ which makes this pull request hard to merge, closing this request to replace it with a fresh one
  Did you test this out locally, by any chance?
 Cool, thanks!  This used to not work because we compiled an Eigen version of contraction which didn't support double, but cublas supports it, so we're all good.

@tensorflow-jenkins test this please
 Keeping it manageable is always good!  Doing classes of ops at a time is fine -- you can probably do a bunch of related ops at once so they're easy to test and verify together, but too many at once and it'll be hard to review.    This is a fine forum to ask about this, since it's about development / new code features.
 Going to try one more time, the test that failed wasn't the 'double' one...

test this please, @tensorflow-jenkins 
 Sorry, I was on vacation, can you rebase?
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 I'm not sure, I generally do (on the dev branch);

```
git fetch upstream
git rebase upstream/master
git push -u origin $branch
```

You might want to start again in a new PR if it's too messy to undo for you
  Can you post a tfevents file that is broken? I tried the original one attached to this thread and it renders just fine.
  Does the problem persist in TensorFlow 0.8? Can you share a complete program that exhibits the problem? In particular, how are `a` and `input_tensor` defined?
 If you simply fetch a constant containing that data using `Tensor.eval()` in a loop, does the time increase for each call?

I expect that the problem is somewhere in the other part of the 2000 lines, and it's just being accounted to `TF_Run()` since that's the main entry-point to the native runtime. It would be helpful to make a minimal example that still exhibits the problem. For example, are there queues, loops, or other more exotic features in your program?
 I just tried to reproduce this with a simple program, and while the distribution of step times is noisy, it doesn't seem to show an increasing trend:

![figure_1](https://cloud.githubusercontent.com/assets/192142/15228283/41845590-1840-11e6-9c1e-f8ad6a217909.png)

``` python
import time

import matplotlib.pyplot as plt
import tensorflow as tf

c = tf.constant(5)
times = []
with tf.Session():
  c.eval()  # Ignore first, slow step.
  for _ in range(100000):
    start = time.time()
    c.eval()
    times.append(time.time() - start)

plt.plot(times)
plt.ylim(0, 0.0010)
plt.show()
```
 When we've seen this problem in the past, it's usually because something in the training loop is creating new graph nodes, and the TF runtime has to do more work each time you call `eval()`, because it believes it's running a new graph (even if it's structurally identical to the old ones). If you're calling `tf.concat()` in the training loop, that would be adding new nodes to the graph, and would lead to bad performance. One quick way to test this is to call `tf.get_default_graph().finalize()` before your training loop, and you will get an exception if anything is added to the graph.

You can usually avoid these performance issues by creating `tf.placeholder()` nodes, defining the (e.g.) `tf.concat()` in terms of them outside the loop, and feeding in the appropriate values.

I'm going to close this issue for now, but let us know if you have other questions!
  LGTM, thanks!
  @tensorflow-jenkins test this please
  @tensorflow-jenkins test this please
 no idea, let me try again.

@tensorflow-jenkins test this please
  Does this require a specific version of bazel to be installed, btw?
 Given that 0.2.3 isn't officially released yet, I think we'd prefer to try to make it work with both versions, at least for a little transition period.
 @tensorflow-jenkins test this please
 We should probably upgrade our CI to use a newer version of bazel, but supporting older is good too, particularly since check_version requires 0.2.1 right now

@tensorflow-jenkins test this please
  I can't reproduce your compilation error so it'll be hard for me to diagnose the root cause. However, you should be able to work around the problem by editing the tensorflow.bzl file and deleting the  "-DTENSORFLOW_USE_EIGEN_THREADPOOL" string in the tf_copts() function.
  Try 

```
bazel test --linkopt=-headerpad_max_install_names
```

I have no idea why but this solved the problem for me in the past.
  Can you be more specific about the feature you're looking for?  Do you want an op that checks whether a file exists?
  It would be great to see a test on this one if you're able to put something in code. Thanks!
 I'll close for now since this isn't enough information to reproduce without some example code or perhaps the details of the error.  Please comment and we can reopen if there is more information available.
  would "git submodule update --init" be sufficient ?
 Tried "git submodule update --init --recursive" and it works. 
 Okay I think git submodule update --init --recursive might be simpler then.  Feel free to update the PR with that
 Now that we don't use submodules for protobuf, we can actually close this, but thanks for the doc suggestion!
  I'd like to do things such as the following:

``` python
>>> tf.convert_to_tensor([tf.constant(1), 2])
>>> tf.slice(..., [0, 1, some_tensor], ...)
```

Currently both of these raise exceptions. The alternative is to use a combination of `tf.expand_dims` and `tf.concat`, which is a bit unwieldy.
 Hi there,

Several people have requested this recently, and I think it's time we implemented this. I'll take on the feature request.

Derek.
 @rdadolf I don't see why type errors become logical errors - they'll just point to the nested object instead of the root. In any case, the current behavior allows both python objects and tensors, just not mixed - I assume this is done with an implicit call to `convert_to_tensor`, so any improvements to `convert_to_tensor` should carry over to all its use-sites.
  As stated in the documentation, we require a valid probability distribution.  Unfortunately, for speed reasons, we don't check this precondition.  Therefore, this is expected behavior.  Implementing your own loss in terms of other ops is the right fix.
  Looking at the `QueueBase` docs, they leave a lot to be desired. I've sent out a change internally, and it should appear soon....
  @ajaech is absolutely right. This is intended behavior: `tf.random_uniform()` and the other random ops produce new random numbers each time they execute, and each call to `{y,z}.eval()` will execute the random op another time.
  This looks like a duplicate of https://github.com/tensorflow/tensorflow/issues/2333
  If you want training to start sooner, you should reduce the value of `min_after_dequeue`. This parameter is here to ensure good mixing, so you may find that the loss of randomness leads to poorer accuracy, but this depends on your model.
  Is this issue related to [this Stack Overflow question](http://stackoverflow.com/q/37177992/3574081)? The command lines and error messages are similar but not identical. I posted an answer on Stack Overflow.
 Looking at your command lines, it's a little suspicious that the PS tasks (mentioned in the error message) have the arg specified as `-task_id=1` (and not `--task_id=1`). Is it possible that your PS task 1 (i.e. on `10.10.102.29:2220`) thinks that it is actually task 0, because this flag isn't being parsed?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Hi there - just so we can make the comment as clear as possible, what was it about the remark in threading_and_queues.md that confused you?
 Ping for @mrry, is the existing comment okay?
 closing due to inactivity, please comment and we'll reopen!
  That's true. Maybe what this discrepancy conveys is that it doesn't matter :) Happy to take a PR if you think it should be changed, but please make sure the results don't change in unexpected ways as a result.
 Closing pending PR since this is not a mandatory fix.
  This is intentional behavior. Many users write e.g. `tf.constant(4.0, shape=[100, 100])` to generate a tensor filled with a given value. Supporting this for non-scalars is more questionable, but might be required for backwards compatibility. 
  This should be easy to add, after my fix to #2328 goes in.

@rdadolf: Was there any reason for your thumbs down? If this change is easy to make, is there any reason not to do it?
 We'd be happy to accept a PR for `__array__` support.
  I am fixing this for the future. For now, you can simply clone the tensorflow repository (the r0.8 branch if you want to be consistent with your binaries), and copy the `alexnet` and `imagenet` directories from the cloned directory into the `anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/models` directory.
  Sorry you're hitting problems! The best way to tackle this is to update the Android Demo to handle retrained models, so I will work on that.
  Can you isolate an input to `tf.nn.softmax` that produces the problematic values?  It would be nice to have a simple test case that exhibits the problem.  Does adding a `tf.Print` to the inputs give you a reproducible bad input?
 If it's consistent, it seems like it's a deterministic problem in softmax.  If that's the case, you should be able to get a small reproducible instance via something like:

```
... lots of complicated graph construction code ...
logits = ...
probs = tf.nn.softmax(logits)
... more complicated graph construction code

# Instead of
np_output = sess.run(output)
# do
np_output, np_probs, np_logits = sess.run([output, probs, logits])

# np_logits should now be a numpy array giving the reproducible bad input
simple_prob = tf.nn.softmax(np_logits).eval()
assert np.all(simple_prob == np_probs)
```
 Hmm, I found _a_ bug, and it's certainly consistent with your excellent observation about `log_softmax`, but I'm not sure how you're triggering it.  Basically `tf.nn.softmax` and `tf.nn.log_softmax` check the op name to see if it starts with `"Log"`, but they should be checking the op type.
 `"Logistic"`!! :)
 Fix coming up.
  Apologies, this seems to have fallen through the cracks (@mrry).  We'd be very interested in GPU versions of these ops, but it's unfortunate if we have to sacrifice CPU performance to get them.

@vrv: Can you think of a clean way to use a fused C++ op on CPU but a combination of separate ops on GPU?  I'd be tempted to just accept the fix, but people will probably yell at me if I let a performance regression through.
 IIRC, the reason for implementing all of these as C++ ops is due to race conditions between read/compute/update.  If you implement these as separate ops, the time between computing the various compositions of the update rule is much higher than if it is implemented in a single op.  We found this caused problems for multi-replica models due to racy updates, which is why I think most of these ops are fused.  So I'd be a bit concerned about adding the composite op version without sufficient testing.

And no, I can't think of an easy way right now to do this, since we don't have a good rewrite infrastructure that's device dependent.
 @stephenroller: Cool, I'll leave this open as contributions welcome, but based on Vijay's comments it would have to be a unified op to reduce races.  The GPU scatter and gather kernels are reasonably clean, so it wouldn't be too hard to mimic them.
  Are you using the 0.8 release or a recent nightly build? `tf.nn.atrous_conv2d()` is only available in the nightly builds.
  The assign op is not consuming memory, but the problem is caused by the fact that each instance of `new_weights` is converted to a constant op, and added to the graph. Each constant op owns a buffer containing the value that it produces, and a constant op on the GPU device will allocate that buffer in GPU memory.

The fix is to rewrite your program somewhat. Instead of doing:

``` python
for i in range(3000):
    print "Assigning i:{}".format(i)
    sess.run(w1.assign(new_value_array))
```

... you should declare the assign op and a placeholder before the loop, and feed different values to the placeholder in each iteration:

``` python
assign_placeholder = tf.placeholder(tf.float32, shape=[1000, 1000])
assign_op = w1.assign(assign_placeholder)

for i in range(3000):
    print "Assigning i:{}".format(i)
    sess.run(assign_op, feed_dict={assign_placeholder: new_value_array})
```
 Indeed - it's a difficult error to disallow, because there are many totally valid patterns that involve adding nodes to the graph. One tip is to try calling `tf.get_default_graph().finalize()` before your training loop, so that an error will be thrown if you accidentally add a node. (However, we can't do that automatically - e.g. on the first `run()` call - because it would break a huge number of people :( ...)
 Glad to hear it!
  Which version of `ipdb` are you using? It looks like an issue regarding `sys.modules['__main__']` was recently closed: https://github.com/gotcha/ipdb/issues/85.
 OK, it looks like `ipdb` does something strange to `sys.modules`:

``` python
import sys
print sys.modules['__main__']
import ipdb
print sys.modules['__main__']
```

...yields the following output:

```
$ python script.py
<module '__main__' from 'script.py'>
<module '__main__' (built-in)>]
```

I'm not sure if it's feasible to fix this. Two alternatives present themselves:
1. Call `tf.app.run(main=main)`.
2. Place the `import ipdb` statement in the `main()` function itself.
 @mrry: Feel free to close if you still think fixing it is infeasible.
 Closing as infeasible. One of the two [workarounds](https://github.com/tensorflow/tensorflow/issues/2310#issuecomment-218927231) should suffice.
  Have you tried the following?

```
c1 = tf.constant(1)
c2 = tf.constant(2)
scan(fn=lambda state, el: state+c1+c2+el,
     elems=[10,20,30],
     initializer=[0])
```
 I don't quite understand your `GruCell` example. Could you provide a concrete example?
 Is it a python compile error or a TensorFlow graph build error?
 Would it be possible to create a simple example that could reproduce the problem? It looks like a bug.
  In theory it should be possible to reuse the TensorFlow Serving tutorial and just change the name of the output layer to "final_result", as shown in the label_image example:
https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html#using-the-retrained-model

Unfortunately the TensorFlow Serving code creates its own model from scratch, and then loads in the checkpoints to fill in the variables. This means that it's not obvious how to convert a plain GraphDef with the weights as constant ops into their format. I'm passing this over to @jharmsen to see if he can offer any advice?
  You should configure with `TF_UNOFFICIAL_SETTINGS=1` in order to allow compute capability 3.0.
 @hholst80 You're right, `git grep` didn't turn up anything for me either. This worked for me though on my GTX 660.

I got the solution from https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658. Note that the variable is actually `TF_UNOFFICIAL_SETTING` (no "s") - sorry!
  Sorry you're hitting problems. Are you able to reproduce the same error with the standard flowers set of photos? If you can, that will help me track down what's going wrong, thanks!
 Thanks for investigating! Can you upload one of the EXIF files that's causing the error and send me a link? I'll try to reproduce the problem here.
 Hey guys, 
I ran into the same problem and have a fix ready. You seem to have problems with grayscale pictures that have one channel only.
The fix is available in this PR
https://github.com/tensorflow/tensorflow/pull/2349#issuecomment-219077419 
and you can find the fix in the marked line:
https://github.com/tensorflow/tensorflow/pull/2349/files#diff-e27def9997dcd569e1f2412fd6f3ae76R619
(I load every image as a 3 channel image)
 Hey, my pull request was accepted. Please check if your problem remains. 
 Closing this as presumed fix, since it's been a while. Please reopen or let me know if it's still an issue.
  Are you synced to master?  Or are you on the 0.8 branch?  This is fixed at HEAD, so there should be no reason to downgrade to Bazel 0.2.2 at HEAD on master.
  @tensorflow-jenkins test this please
 @dsmilkov: look ok?  We'll merge if so.
 Thanks for trying to fix this!

I tested it and the minimap doesn't work on Firefox now. Can you get a solution that works on Chrome/Firefox/MacOS? Thanks!
 @qbx2 I did some testing and having the old way as a fallback makes it work on Firefox/Safari/Chrome. Can you change your commit to:

```
image.src = 'data:image/svg+xml;charset=utf-8,' + svgXml.replace(/\n/g,'');
image.onerror = () => {
  let blob = new Blob([svgXml], {type: 'image/svg+xml;charset=utf-8'});
  image.src = URL.createObjectURL(blob);
};
```
 Thanks! One last question: any reason why in the new commit, you removed the "replace" as in svgXml.replace(/\n/g,''); Is that not needed? Thanks!
 Great - was just curious! @vrv Good to merge!
 there are conflicts unfortunately -- can you rebase so we can test and merge?
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
  TensorFlow is an Apache licensed open source project, so there is no need for a specific permission grant (nor do we provide such).
  I'm not sure if it answers your question, but here is some documentation on the GraphDef fileformat we use to save models:
https://www.tensorflow.org/versions/master/how_tos/tool_developers/index.html
  I don't think you are using too many files, however using large amounts of data makes it more likely to expose underlying bugs. The error indicates that "bottleneck_string" contains some non-numeric characters. Not familiar with that part of code, maybe @petewarden  has an idea how non-digits could've snuck into `bottleneck_string`?
 Yes, it probably could be missing. @dave-andersen had fun dealing with various forms of corruption when trying to reprocess imagenet dataset. Some ".jpg" files in the set were actually ".png" files in disguise
 There shouldn't be a theoretical cap, but there could be practical caps. In my own experiments I saw overfitting happening when using <1M images. However, when increasing to 20M, I didn't see overfitting after hundreds of millions of steps, so I've been using it as personal rule of thumb for desired number of training images. I'm expecting GTX1080 to be 2x faster than TitanX so it's not a great panacea. It should go beyond 2x speedup for float16, but probably will require adjustments to training procedure. 
 If you had something like 1B images, then you could train for one epoch
since another epoch would be 2x longer but a only small improvement in accuracy
But for 600k you probably will need several passes to get convergence (a
few dozen epochs at batch size 32 maybe?). I don't remember the details off
the top of my head, but ImageNet papers should talk about how many epochs
they used

On Mon, May 9, 2016 at 2:06 PM, oren weingrod notifications@github.com
wrote:

> @yaroslavvb https://github.com/yaroslavvb Interesting. Another
> question—albeit a stupid one—should the number of training steps reflect
> the number of images 1-1? As in, 600k images, 600k training steps?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2296#issuecomment-217989795
 No problem, closing this for now since not sure what can be done on our side, but feel free to re-open if you are able to get more debugging info (ie, the contents of `bottleneck_string` for the failing example)
 I have a helper program I used to validate all of the imagenet files, but it's a little too Google-specific to be useful more broadly.  The heart of it, though, was just:

... do something to read in your JPEG into a string called image_buffer ...

  features = tf.parse_single_example(example_serialized, feature_map)
  image_buffer = features['image']
  filename = features['filename']

  image_saved = tf.Variable('')
  save_image = image_saved.assign(image_buffer)
  decode_saved_image = tf.image.decode_jpeg(image_saved)

  init = tf.initialize_all_variables()
  with tf.control_dependencies([decode_saved_image]):
    force_decode_no_copy = tf.no_op()

  with tf.Session() as sess:
    sess.run(init)
    tf.start_queue_runners()

```
try:
  while True:
    key, fn, _ = sess.run([exid, filename, save_image])
    try:
      _ = sess.run(force_decode_no_copy)
    except:
      print "Error decoding ", key, fn

except tf.errors.OutOfRangeError:
  print "OK"
```

You just need a wrapper mechanism to run a bunch of parallel threads for this and handle the input depending on how you're getting it all read in.
 @oweingrod: What's the status of this?  It seems to have fallen through the cracks.  Cc @petewarden.
  Hi,

I had a pipeline where I was first clamping to [0..1] and then doing x^2.4 (converting to/from gamma RGB to linear RGB):

```
y = tf.minimum(tf.maximum(y, 0.0), 1.0)
gamma = tf.constant(1.0/2.4, tf.float32)
y = tf.pow(y, gamma)
```

However, this instantly created inf values (and NaNs in the next step), seemingly due to the use of log() to compute the gradients. The gradient of pow() in 0.0 should be pretty well defined as long as the exponent is nonzero; at the very least, it is not infinity.

As a workaround, clamping to 1e-5 instead of 0.0 caused the NaNs to disappear.
 I have no idea, I never printed them out. But the graph trained just fine.
 The fast way to solve this is an add an extra op that does `x * log y` and does the right thing for `x = y = 0`.  The slow way is to do that in Python via `tf.select(tf.equal(x, 0), 0, x * tf.log(y))`.  Unfortunate that simplistic code doesn't work out of the box because `tf.select` doesn't broadcast.

The simple way to solve it is to use `x * tf.log(y + y.dtype.epsilon)` in the gradient.  Probably we should just do that.

@sesse: I'm happy to review if you want to make the change, but won't be able to work on this soon.
  You are welcome to contribute.  We will accept it if it passes our review process. 
 +ebrevdo@. One way to do it is to extend TensorArray a bit so we could convert between a (python) list of tensors and TensorArray.  It would also be nice to change session slightly so we could return a list of tensors for a single output. 
 @jihunchoi I don't know that anyone is working on it currently.
 We are working on more generalized nesting in general; then I'll pick this up in a couple of weeks once we have that in.
  As @smartcat2010 mentioned, the tutorial is to illustrate the use of allow_soft_placement.

Closing this as it's a not a bug.
  @martinwicke: Do you know of a reason we can't add `-lm` in all cases?  Would it bloat the binary? 
 @petewarden would be most sensitive to bloat. I'm always in favor of -lm.

On Tue, Jun 7, 2016 at 5:51 PM Geoffrey Irving notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke: Do you know of a reason we
> can't add -lm in all cases? Would it bloat the binary?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2291#issuecomment-224457136,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_baHZU43Sz4HYMkJsqTYQ2Bq-X_5ks5qJhH9gaJpZM4IaJBS
> .
 @edi-bice: Let us know if you want to send us a PR with `-lm`!
  Sorry you're hitting problems! There are various changes you need to make to successfully run the Inception v3 model in the Android demo, including altering the mean and std values that the input images are scaled by. We don't have an example of this yet, but we'll track it in this bug.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  I'm going to merge this thread into the extremely related #1029.  We'd be happy to accept PRs for either one.
  The high-level problem should be fixed by @vrv's ongoing work to improve device placement. (Making `tf.Variable` ignore `tf.device()` will not work, because many of our users, especially in distributed settings, use this to configure parameter servers.) In the short term,  try using soft placement in your session constructor:

``` python
config = tf.ConfigProto(allow_soft_placement=True)
with tf.Session(config=config) as sess:
    # ...
```
  If you have a batch of images in a `[batch, height, width, channels]`, can't you just use `tf.slice()` to crop the whole batch?
 Closing for now, since slice does seem to be a good solution.  @ziky90: Please comment if slice doesn't fit your use case. 
  @smartcat2010: Could you try to isolate where the problematic `IndexedSlices` object without a dense shape is coming from?  It's likely that whoever is creating that object needs to attach a shape at creation time. 
 @smartcat2010: We need more information than that it comes from somewhere inside `tf.gradients`, which contains something like half of TensorFlow.  The best way would to be either find the specific line that constructs the `IndexedSlices` without specifying a `dense_shape`, or to produce a minimized test case.  Here are some ways to go about the former:
1. Add an assert in the constructor of `IndexedSlices` that `dense_shape` is specified.  If you're lucky, this will break on the problematic line.
2. In the constructor of `IndexedSlices`, print `id(self)` if `dense_shape` is not specified, and compare it to the `id` of the problematic `IndexedSlices` you're getting out.
 Closing for now.  @smartcat2010: Please comment if you have more information and I'll reopen. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 Merged. Thanks, @itsarbit 
  Thanks for reporting.  It will be fixed in the next pull.
  `partial_run` should enable these kinds of computation reuse. Can you check your device placement to make sure you are not spending your time waiting for CPU->GPU memory transfers?

Also, is this issue fixed if you consolidate and fetch all your outputs in a single `run`?

You can enable device placement logging by creating your session with `config = tf.ConfigProto(log_device_placement=True)`.

Also more generally, you can see more details about CPU-GPU transfers by using following snippets (not sure if `partial_run` looks at run_options though)

```
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
sess.run(..., options=run_options, run_metadata=run_metadata)
run_summarize(run_metadata)

def run_summarize(run_metadata):
  global run_metadata
  print "***"
  for device in run_metadata.step_stats.dev_stats:
    print device.device
    for node_stats in device.node_stats:
      print "   ", node_stats.node_name

```
 @mrry: Looks like this fell through the cracks.  Do you know what's wrong?

@myme5261314: I'd guess that the reason `[b, c]` is slower than just `c` is that all you're timing is the overhead of converting things back to numpy.  I don't think it's related to your original motivating problem.
  Tried to fix #2198 but I guess I'll fire a separate PR when I get a chance
 Test this please, Jenkins.
 @petewarden @vrv Could any of you run the Jenkins again? I just found a huge portion of repeated code - maybe from conflicts not being resolved correctly during internal branch merge. Also cc @ilblackdragon 
 @ilblackdragon Good to know. I'll take a look at graph_actions later when I get a chance. I've reverted the changes. Let's just keep this PR to be removing those repeated lines (tests passed since the repeated code are inside a function block). Could you take a look and merge this first or people will have some issues? 
 @tensorflow-jenkins test this please.
  From looking at the warnings, it does seem like there's a numpy installation problem. Are you able to run a simple test program that imports numpy?
  I will attempt to reproduce on Monday. However, this specific error is typically TensorFlow's oh-so-cryptic way of saying it's run out of memory. Can that be the case here? Can you also tell me what tf.**version** this is?
 I can't reproduce this at the head of the tree. It seems to strongly point at a memory issue.
If you reduce the size of the test and validation sets (which are kept in memory), does it start working?
  Test this please, Jenkins.
 We have some changes incoming to the signature of the various RNN Cells.  This change has a few lines conflicting with it, so will hold off till Wednesday or so to review.
 Yes - please rebase, resolve any conflicts, and ping when ready.
 Jenkins, test this please.
 Jenkins, test this please.
 Thanks!
  Test this please, Jenkins.
  See issue #2089
cc: @ilblackdragon 
 @ilblackdragon Not so much special handling is needed. I have added an example and a test case. I think this is ready to merge. 
 Test this please, Jenkins.
 I overlooked something. Will fix it soon. @petewarden 
 @petewarden Could you trigger test again (should be fixed)? Sorry I wasn't able to test it locally for some reason. Errors like this happen recently even after I re-clone the whole repo. It worked fine before.

```
exec ${PAGER:-/usr/bin/less} "$0" || exit 1
-----------------------------------------------------------------------------
Traceback (most recent call last):
  File "tensorflow/contrib/learn/test_base", line 110, in <module>
    Main()
  File "tensorflow/contrib/learn/test_base", line 93, in Main
    program = python_program = FindPythonBinary()
  File "tensorflow/contrib/learn/test_base", line 25, in FindPythonBinary
    'Bazel does not support execution of Python interpreters via labels yet')
AssertionError: Bazel does not support execution of Python interpreters via labels yet
```
 Test this please, Jenkins.
 Tests passed. Ping @petewarden 
 Thanks @terrytangyuan, I've reviewed the changes and they look good, merging! I appreciate your work on this.
  @tensorflow-jenkins test this please
 Test this please, Jenkins.
  Test this please, Jenkins.
 vocabulary is a very generic term in this context, just like alphabet for coding theory. I don't think we'd want to deviate from standard nomenclature for character level models, but @vincentvanhoucke should decide.
 Yes, 'vocabulary' in the context of NLP defines the set of entities that are being predicted. They're often words but can be sub-word units or characters. This is very standard practice, so I'll close this PR.
  @rmlarsen: Any guesses as to what's going on? 
  Hi there. I'm going to close this issue because, as it says in the issue template:

> GitHub issues are for bugs / installation problems / feature requests.  
> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
> To make bugs and feature requests more easy to find and organize, we close issues that are deemed
> out of scope for GitHub Issues and point people to StackOverflow.

You might find the [`tf.tile()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#tile) and [`tf.pad()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pad) ops useful, however. If you have further questions, please ask the question on Stack Overflow instead.
  This example has been updated to use features only available in TensorFlow 0.8. I'd strongly recommend upgrading to the newest version. Otherwise, you should check out the [version of this example from the 0.6.0 branch](https://github.com/tensorflow/tensorflow/blob/0.6.0/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py).
  Could you pass the absolute path to your python, such as? 

/usr/bin/python
 The error message is from the first line here: 

#!/usr/bin/env python2.7
https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc

Could you confirm that you are running bash? And if you start from your bash shell, if you run the displayed command line from "bazel", you would pass the first line? 

If so, it could be possible that the new bazel is running a more restricted bash and however you put python2.7 on env didn't get picked up. If that is indeed the case, it would be better solved if you file a bazel issue directly. 

https://github.com/bazelbuild/bazel
  Seems as parameter keep_prob is now called dropout. Because of the comment above I'm pretty sure droupout is meant.
 @tensorflow-jenkins : test this please
 Merged. Thanks, @codingyourlife 
  For reference, here's the error output from StackOverflow:

```
E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.96GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
F tensorflow/core/common_runtime/gpu/process_state.cc:155] Check failed: numa_node >= 0 (-1 vs. 0)
Aborted
```

Assigning to @poxvoculi 
 For reference, cuda_gpu_executor [here](https://github.com/tensorflow/tensorflow/blob/7b4d733593842361d066d7e33a03a07da5dca465/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L905) should've convereted all -1 to 0 (I've gotten -1 Numa node on machines with 1 or 2 nodes, but it turns into 0 in the end
 Thanks @yaroslavvb, looks like it's failing to get the numa node on one of the other logic paths through TryToReadNumaNode.  @petamem: Fix coming.  Meanwhile, it's probably safe for you to hack common_runtime/gpu/gpu_device to set the num_node parameter to 0, where it is used.  This value is only important if you have multiple GPUs on different PCI buses.  If that's your case, then we need to fix the stream executor to find the numa node correctly.  
 Unfortunately, the fix is not in the latest (pending) merge up from Google: https://github.com/tensorflow/tensorflow/pull/2299
It should be in the next one.  The testing process makes the code pushes slow.

If you'd like to help debug the underlying problem, look at tensorflow/stream_executor/cuda/cuda_gpu_executor.cc
In there is a function TryToReadNumaNode() which examines the /sys/ directory to try to find the socket/bus affinity of a GPU device.  Evidently this doesn't work on your system.  It would be nice to know why: is the data not present, or stored somewhere else?

If you just want to get on with your other task, override the kUnknownNumaNode value (-1) being returned with 0, and the rest of the system should work ok...assuming that TF is actually accessing the correct GPU.  (Notice that for OS/X that's exactly what happens in TryToReadNumaNode().)
 Thanks, glad you found a solution.  I'll look into the error messages.
  Please squash into one commit.  It looks like a lot of the intermediate commits don't even compile, much less work, which makes them harder to review.
 Please wrap the 81/82 column lines.  It's a hard limit, except for Python import statements.
 Looks good, thanks!  I'll fire off the tests once you fix the `complex6128` bit and the > 80 column lines.
 Woot!  Jenkins, test this please.
 Jenkins, test this please.
 Is there any test excercising the new type complex128's operations?
 The code base is fast changing. The only way to ensure it works for everyone is to maintain a high standard of the code quality. One important aspect is to make sure testing is done well. In your case, can you add proper test cases covering the operation this PR touches? It should not be too complicated. Most relevant tests should be in python/kernel_tests/cwise_ops_test.py. You can grep test cases for complex64, most of which is just matter of extending the type list w/ the new type.
 @hunkim: Apologies, I should have clarified.  I agree with @zffchen78 that you should add tests, and was waiting on those.  It should be as simple as searching for `complex64` in `cwise_ops_test.py` and making sure `complex128` does the same thing.
  @colah did you make this one?
 From previous discussions, I believe this was manually created with After Effects, so there isn't an easy way to do something similar without using motion graphics tools.
 I've made great animations using Keynote. This one would actually be pretty easy, just tedious.
 Closing, since this is a question not an issue.
  Sorry you're hitting problems! I'm a bit confused though - the title indicates a problem reading float64 values, but the text talks about hitting memory usage problems.

Do you have an example of the error message you're hitting? That might help.
 You can checkout some examples in skflow using dask if it doesn't fit in memory.  You can also use pandas to load csv files. 
 Closing this issue due to inactivity. If you revisit it, @liumilan, I think this might be a better question for Stack Overflow....
  Test this please, Jenkins.
 I'm still not happy about just blanket adding sudo. We already have a sentence there saying the command (whl file name) may depend on the platform. I suggest we add there that "on some platforms (some versions of MacOS in particular), you may have to use sudo", without actually putting that into the code.

If you add sudo on a linux machine, it will actually mess with your pip install and prevent updates without sudo and all kinds of other nasty stuff. 
 Never mind. I got confused where in the doc this was. Sorry about that.
  Which versions of bazel are you using? 
 I hope that 2.2 would work. We'll have to make changes to make 2.3 work
when it comes out.
On Sun, May 8, 2016 at 13:53 Valentine Svensson notifications@github.com
wrote:

> Ah sorry, actually I was using HEAD of master of the basel github
> repository. I had to build it from source.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2256#issuecomment-217745439
 Thanks, it's good to know we'll have to fix TensorFlow for the next bazel
release.
On Sun, May 8, 2016 at 14:23 Valentine Svensson notifications@github.com
wrote:

> I rolled back to 0.2.2 by git checkout 759bbfe in the bazel repository,
> then rebuilt. After this I could successfully build tenssorflow!
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2256#issuecomment-217746981
  Re-assigning this to @yuanbyu, who knows the current gradients code best, and would be able to estimate the changes needed. (I suspect that this would require a considerable amount of work to update the existing gradient functions, which may have latent assumptions about expecting `tf.float32` or `tf.float64` values.)
  This seems similar to #2034. Can you try nightly build with the fix, if it still has this issue?
 Closing as duplicate of #2034, which is fixed in 0.9.
  The problem is that file should not be sent to gcc. It could be a bazel misconfiguration somewhere. 

The correct sequence is as follows. You can follow it and see where things went wrong. 
- bazel.rc has this line "build:cuda --crosstool_top=@tf//third_party/gpus/crosstool". So under "--config=cuda" third_party/gpus/crosstool should be used instead of the default. 
- third_party/gpus/crosstool should have this line "tool_path { name: "gcc" path: "clang/bin/crosstool_wrapper_driver_is_not_gcc" }", which instructs the bazel to use that as the compiler instead of gcc. It understands the "-x cuda" option and knows which compiler to use. 
 Unfortunately we don't officially support CentOS. We'll do our best to support installation on unsupported platforms, but in this case, it's unclear what we can do. If it turns out there is a problem that can be fixed with changes to TensorFlow, we're happy to accept a PR.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I will close this PR. It appears it's outdated. Please comment if that's not the case.
  @tensorflow-jenkins test this please
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  A small code segment to reproduce the error and an error message log will be very helpful. 
 This seems similar to #2034. Can you try nightly build with the fix, if it still has this issue?
  I am not sure that I understand the question here. Could you elaborate with a small code example? 
 Closing due to lack of activity.
  Merge internal changes.
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @tensorflow-jenkins, test this please. 
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @martinwicke @vrv : ready to merge!
 Thanks for resolving the conflicts and fixing the tests, @zheng-xq 
  One way is to change the default graph, and not putting all the nodes on the same graph. 

with tf.Graph().as_default():

Adding @mrry to point out if there are better ways. 
 If a graph node is unused, it will not be executed, and hence it won't contribute to the step time (apart from possibly the first step, when the graph is first built and pruned). There's no API for deleting nodes in Python, but you can try using the [`extract_sub_graph()`](https://github.com/tensorflow/tensorflow/blob/5f4524928af5ba5727fa97a35ddd5da72be0c476/tensorflow/python/client/graph_util.py#L125) function to prune the graph to only the nodes that are needed. (Note that doing this is unlikely to improve performance though.)
  @girving Will this be a backward-compatibilty issue?
 It will break the backwards compatibility test, since it reduces the list of datatypes the ops are registered for.  However, it's not otherwise a backwards compatibility issue if the removed ops never ran, and it's a good change to make since it stops us from lying.  We should arguably make an exception and do the necessary surgery to fix the test.

I believe the backwards compatibility test is open source, so @hunkim: am I correct that you haven't tried running the unit tests yet with your change? 
 @josh11b: If you agree, what's the best way to perform the necessary surgery to clarify that `tf.sqrt` has never worked on `int32`? 
 This is not a doc file. This is the file that actually defined which
kernels are compiled.

On Fri, May 6, 2016 at 8:53 AM Sung Kim notifications@github.com wrote:

> @girving https://github.com/girving, @martinwicke
> https://github.com/martinwicke Not at all. Now this API doc is synced
> with actual implementation.
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2244#issuecomment-217482152
 @hunkim: I understand that your change is good, but by "Not at all" do you mean that you definitely haven't run the tests?  I believe they will fail, since we have a backwards compatibility check that registered ops only get more permissive (ignoring the kernels).
 Adding @girving to give guidance on how to deal with the backward compatibility test. 

In general, it is a good idea to run the TensorFlow tests locally. 
 > > I wonder why it runs all tests again and again for just one-line-change? Can it only run affected test cases by the change?

The public bazel does not have the capability to achieve that automatically. 

When you get to the point that you know the test structure, you can just run the tests that are most likely to fail. 
 Verifying with @josh11b, but I think we should do the surgery to make the backwards compatibility test pass.  However, you seem to say above that the backwards compatibility test doesn't fail with your change, which is suspicious.  To confirm, can you do

```
bazel test core/ops/compat:backwards_compatibility_test
```

That should fail.  Assuming it does, it should be reasonably easy to fix by manually editing

```
 core/ops/compat/ops_history.v0.pbtxt
```

to remove the types that are invalid.  I will then look at the changes to that file very carefully. :)
 @hunkim: Go for stripping the bad dtypes out of `ops_history.v0.pbtxt`.  Let me know if you have questions or want help.
  Could you post your environment, and a small code segment to reproduce the problem? Also please upload the log file so we can check if there is something meaningful there. 
 @mxrguspxrt: Exploding to nans with high learning rate is standard behavior for neural networks.  If you're seeing a rapidly exploding network with a very small learning rate, there might be an issue, but as in I'm going to close this as (unfortunately) intended behavior.  Please comment if you think it should be reopened.  
  @martinwicke: Is there a convenient way to report error message in this case instead of crashing? 
 I don't know. @zheng-xq? 

This is almost certainly due to the version conflict. Our binaries are built against cudnn4, Cuda 7.5. Any other version will have some problem like this.
  Please run with --verbose_failures and post the results. 

I am not sure whether Centos 7.1 is supported by bazel. You can also file a bazel bug to get more help on bazel issues. 
 Closing for now due to insufficient information.  @liumilan: I am happy to reopen if there are more details. 
  The error says no GPU can be found on this machine. 

Could you run "nvidia-smi" and post the log? 
 @martinwicke: It looks like our docker instructions are out of sync.  One of these says to use nvidia-docker; the other does not.

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker#running-the-container
https://www.tensorflow.org/versions/master/get_started/os_setup.html#docker-installation

Should they be synchronized?
 @caisq, which one are the "correct" ones? There should definitely only be one version.
 Cc @jendap who said he was planning to synchronize the two places.
  I can't reproduce the issue in title, ie `sess.run(tf.equal(1, 2))` produces `bool` rather than `int32`
 It seems the problem was in the inputs to cond being wrong shape, keveman gave solution here http://stackoverflow.com/a/37053896/419116
 @dtracers: Like numpy, `tf.equal` returns a broadcasted tensor of whatever shapes you gave it as input.  You can only use it as a conditional if the shape is scalar (0-dimensional).  `tf.reduce_all` or `tf.reduce_any` can be used to collapse to a scalar, or `tf.squeeze_dims` in this case. 
 We follow numpy convention on that. IE the following returns an array of [False] and it can't be used directly as a conditional

`np.array([1])==np.array([2])`
  Adding @yuanbyu to comment on how to use the control flow the implement recursive network.

My guess is that the control flow plus stack might be helpful. 
 @yuanbyu: Do you have any suggestions?  I'll mark this contributions welcome for now.  If it's added, we should make sure it supports efficient batching. 
  Adding @a-dai to comment. 
 @a-dai can confirm, but I don't think anyone is working on this at the moment.  PRs are welcome, though! 
 However, since this is about tensorflow/models, not tensorflow/tensorflow, it should be refiled at the right repo.
  Can you check the output of `which ipython3` to see if it is running the version inside your virtualenv? You may need to install it inside your virtualenv (e.g. using `pip install`) to get it to work.
  Can't reproduce, tried on Z420 and Z840 machines and TF at head.

Are you using GPU?

```
import tensorflow as tf
with tf.device("/cpu:0"):
  a = tf.ones([1, 3], tf.float32)
  b = tf.ones([3, 5], tf.float32)
sess = tf.Session()
print(sess.run(tf.tanh(tf.matmul(a, b))))

[[ 0.99505478  0.99505478  0.99505478  0.99505478  0.99505478]]
```
 Can you try and see if it's still non-deterministic when using with less threads

```
import tensorflow as tf
with tf.device("/cpu:0"):
  a = tf.ones([1, 3], tf.float32)
  b = tf.ones([3, 5], tf.float32)
  c = tf.tanh(tf.matmul(a, b))
config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)
sess = tf.Session(config=config)
print(sess.run(c))
```
 Hm, could it be numpy display issue? What if you do

```
import tensorflow as tf
with tf.device("/cpu:0"):
  a = tf.ones([1, 3], tf.float32)
  b = tf.ones([3, 5], tf.float32)
  c = tf.log(tf.tanh(tf.matmul(a, b))-0.99505478)/tf.log(2.)
config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)
sess = tf.Session(config=config)
print(sess.run(c))
```

I see
`[[-inf -inf -inf -inf -inf]]`
 I suspect eigen packet/vector vs non vector difference. Inputs mod 4 will
be the same, the remainder will be different because it uses a different
code path?

On Thu, May 5, 2016, 3:37 PM Yaroslav Bulatov notifications@github.com
wrote:

> Hm, could it be numpy display issue? What if you do
> 
> import tensorflow as tf
> with tf.device("/cpu:0"):
> 
> a = tf.ones([1, 3], tf.float32)
> b = tf.ones([3, 5], tf.float32)
> 
> c = tf.log(tf.tanh(tf.matmul(a, b))-0.99505478)/tf.log(2.)
> 
> config = tf.ConfigProto(inter_op_parallelism_threads=1,
> intra_op_parallelism_threads=1)
> sess = tf.Session(config=config)
> print(sess.run(c))
> 
> —
> 
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2234#issuecomment-217287498
 @benoitsteiner @rmlarsen for comment?

On Thu, May 5, 2016, 3:40 PM Vijay Vasudevan vrv@google.com wrote:

> I suspect eigen packet/vector vs non vector difference. Inputs mod 4 will
> be the same, the remainder will be different because it uses a different
> code path?
> 
> On Thu, May 5, 2016, 3:37 PM Yaroslav Bulatov notifications@github.com
> wrote:
> 
> > Hm, could it be numpy display issue? What if you do
> > 
> > import tensorflow as tf
> > with tf.device("/cpu:0"):
> > 
> > a = tf.ones([1, 3], tf.float32)
> > b = tf.ones([3, 5], tf.float32)
> > 
> > c = tf.log(tf.tanh(tf.matmul(a, b))-0.99505478)/tf.log(2.)
> > 
> > config = tf.ConfigProto(inter_op_parallelism_threads=1,
> > intra_op_parallelism_threads=1)
> > sess = tf.Session(config=config)
> > print(sess.run(c))
> > 
> > —
> > 
> > You are receiving this because you are subscribed to this thread.
> > Reply to this email directly or view it on GitHub
> > https://github.com/tensorflow/tensorflow/issues/2234#issuecomment-217287498
 We have an optimized version of the tanh function that processes coefficients 4 by 4. In a tensor of 10 elements we call that function twice to process the 8 first element, and we process the last 2 elements one by one by calling std::tanh. std::tanh and our optimized versions don't fully agree on the final result, which is why the last 2 coefficients are slightly different from the rest.

When the input contains 100 elements, we simply call the optimized version of tanh 25 times, so all the results match.
 Contributions welcome to fix this to be consistent!
  Thanks for letting us know about this - it sounds like a bug in the tensor serialization code, but to figure out what's going wrong, we're going to need more information:
1. Can you pinpoint the line of Python on which it hangs? (Does it hang on the `sv.prepare_or_wait_for_session()` or does it hang when you try to run something?)
2. Is there an obviously-large tensor in your program? Does it exist as a `tf.constant()`, a numpy array that is implicitly converted to a `tf.constant()`, a value that is fed into the graph, or could it be a result that you're fetching? (Looking at the expected size, 72000800 bytes, I'm guessing that it's a tensor with size 90001 in one dimension, maybe a 200 x 90001 matrix of floats?) All of these cases should be handled, but this will help to construct a minimal failing test case.
3. Can you try running the code in the following tests from [`server_lib_test.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/server_lib_test.py) in your environment: `testLargeConstant()`, `testLargeFetch()`, and `testLargeFeed()`.
 Hi folks, sorry for the delay on this one. We've tracked down the issue: the generated code for gRPC uses a [protobuf parsing routine](https://github.com/grpc/grpc/blob/305b0f4e2f99d2326bf1aaa881f857bb5fe1a817/include/grpc%2B%2B/impl/codegen/proto_utils.h#L209) that doesn't override the default 64MB limit.

Generally speaking, if your trainer is transferring large tensors in every step, there might be a more efficient way to implement it at the Python level. For example, instead of fetching a large fully connected layer to compute the logits, you might use a [sampled loss function](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#sampled-loss-functions), and you can use [`tf.nn.embedding_lookup()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#embedding_lookup) instead of [`tf.gather()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#gather) to more efficiently access sparse rows in an embedding matrix. Alternatively, you can shard your variables manually to avoid the limit.

Clearly, this isn't ideal, and we're working on a more robust fix.
  Closing since the conversation is happening over in #739.
  There's a hard bound on the number of elements in a Tensor (about a trillion), do you really need to use a 9 trillion element tensor?
  Can reproduce at head with
(remove "from tensorflow.python.ops import image_ops")

```
im_bi = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BILINEAR)
im_nn = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
im_bic = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BICUBIC)
im_ar = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.AREA)

```
 ![bad-sun](https://cloud.githubusercontent.com/assets/23068/15034991/3019a0e0-1231-11e6-9697-7f26bfd28253.jpg)
 Closing as duplicate of #1029.  Though this version does have prettier pictures.
  @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 Thanks! Looks good generally, waiting for tests to pass....could you squash the commits?
 OK, so apparently this was enabled before and it generated nvcc warnings, because the macro creates a kernel that can automatically downgrade floating point inputs to integers. Also it's a weird kernel to enable because trig functions use floating point math internally anyway, so you don't gain anything (as opposed to, say, defining integer valued matrix multiplication). To deal with user-side crash, the more principled way might be automatic promotion of input arguments (int->float). Sorry this was a bad suggestion on my part :/
 I think promoting int32->float or double will have the same effect as in the current implementation. IE, int32 may have different precision than float, but it doesn't matter if internal implementation converts it to float anyway since output is floating point even if input is integer. The difference with `Pow` function is in that case integer inputs also produce integer outputs
 I think that's a slightly different issue -- someone has data of type `DT_DOUBLE`, but only convolution with `DT_FLOAT` implementation is available, so it fails. The idea is that it's better to fail and request explicit conversion, than silently lose precision.

Another example of original problem is that the following fails. Automatic type promotion would help with the problem but it needs someone familiar with the internals to implement and I don't think anyone is working/planning to work on it at the moment.

`sess.run(tf.constant(1)+tf.constant(np.array(1)))`
 Yes
On May 5, 2016 4:42 PM, "Sung Kim" notifications@github.com wrote:

> @yaroslavvb https://github.com/yaroslavvb I see. Then, just API doc
> update at this point?
> 
> "A Tensor. Must be one of the following types: float32, float64, int32,
> complex64, int64"
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2227#issuecomment-217311256
  The GPU kernel for reduce_sum is known to be non-deterministic since it
uses the atomic operations. When the number of elements is large, the
difference could be quite large.

If your model is trained with dropout in it, it tends to be less likely to
be affected by the noise.

Otherwise, it is a good idea to have a small example where we can observe
the 98% -> 10% difference.

On Wed, May 4, 2016 at 5:03 PM, alexlee-gk notifications@github.com wrote:

> The evaluated value of the l1 or l2 loss of variables differ sometimes
> when using the GPU. This seems to happen when the variables are "large".
> 
> Even though the difference is not that much in the example below, these
> differences have resulted in a 98% test accuracy on a network trained on
> the CPU, but a 10% test accuracy on the same network trained on the GPU.
> Environment info
> 
> Operating System: Ubuntu 14.04
> 
> Installed version of CUDA and cuDNN: 7.5 and 7.0 cuda_info.txt
> https://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt
> GPU: Titan X
> 
> If installed from binary pip package, provide:
> 1. Which pip package you installed.
> pip install --upgrade
> https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl
> 2. The output from python -c "import tensorflow; print(tensorflow.
> _version_)".
> python -c "import tensorflow; print(tensorflow._version_)"I
> tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA
> library libcublas.so locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA
> library libcudnn.so locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA
> library libcufft.so locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA
> library libcuda.so.1 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA
> library libcurand.so locally
> 0.8.0
> Steps to reproduce
> 
> import tensorflow as tf
> W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
> W2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
> sess = tf.InteractiveSession()
> sess.run(tf.initialize_all_variables())
> l2 = tf.reduce_sum(W1 \* W1) + tf.reduce_sum(W2 \* W2)
> for i in range(100):
>     print('%.10f' % sess.run(l2))
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2226
 @zheng-xq: Any thoughts?  @alexlee-gk: If not, we may have to close this as expected behavior, unfortunately.  It sounds like your 97.7% vs. 9.8% difference occurs for a model where different CPU systems can also produce 10%, and thus that you may have found an impressively unreliable set of hyperparameters.  If using a smaller learning rate helps, it may just mean that the GPU version blows up slightly earlier than the CPU version.
 @alexlee-gk, the problem could be attributed to small differences introduced in the non-deterministic behavior on GPU. Or some numerical instability due to the precision difference between CPU and GPU. 

The best way to tell whether this is actually a bug is to narrow them into a small script that shows a big difference in one or two steps. If that is not possible, then the model might be unstable, and this might be an expected behavior. 
 @alexlee-gk, thanks for bringing this issue to our attention. 

Closing as expected behavior. Feel free to reopen if you can reproduce with large difference within one or two steps. 
 At some point I was debugging tests failing when running with -copt=avx2 instead of avx. We couldn't get rid of the numeric difference and ended up just raising tolerances on tests. What happens is that there's some small operation that produces small difference in the range of ulp or two and then it blows up. In particular, I noticed forward pass during MNIST training to be relatively stable, but during backward pass, tiny difference blow up to billions of ulps
  The kernels were removed as part of the internal clean-up, since there is no good reason to have integer version of those functions. Apparently the doc was not updated.

You are welcome to make a contribution to update the doc before the TensorFlow team can get to it. 
  @tensorflow-jenkins test this please.
 PR merged. Thanks!
  @lukaszkaiser: Can you comment?  Should we mark this contributions welcome? 
 Yes!
  @tensorflow-jenkins test this please.

Need to pass checks even for documentation change.
 Can you also add the flags to the docker_run script? And maybe add a sentence about what it's for?
 That would be ok. Can you add a sentence explaining why the -p argument is
necessary?

On Sun, May 8, 2016 at 11:01 AM Henry Saputra notifications@github.com
wrote:

> This PR is for updating documentation.
> Will send another PR to update the script.
> 
> Would that be ok?
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2222#issuecomment-217736643
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Test this please, Jenkins.
   @tensorflow-jenkins test this please.
  Is this error fatal in your log? Depends on which part of TF triggers this error, it might or might not be a problem. 
 @neurotenguin Are you running skflow digits.py example? Try change the `RunConfig` and pass it to your estimator before training. See example [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/iris_run_config.py).
  I'm working on this.  The testing is a little tricky to integrate but it should be done soon :)
  A good way to start with tf.contrib. Once it matures enough the TF team would move it to the core. 
 Here are a few examples in tf.contrib.

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/README.md

Once you created your own directory, you need to create your own BUILD file to set up your target and dependencies. Then bazel can build it like any other target. 
 @c0g: Thanks, we look forward to PRs!  @rmlarsen: Do you have any comments?
  @mrry, any idea here? 
 Hi, did you try the alwayslinks=1 and linkopt workarounds suggested in https://github.com/tensorflow/tensorflow/issues/2047?

Another potential solution is to add an explicit reference to DirectSession in the code to make sure it's not getting stripped out.

It'd be nice to know the root cause of why this is happening to people occasionally, of course, but I've never been able to reproduce.
 Another thing worth checking: how much free disk space and memory does your system have? I've seen cases where Bazel "forgets" that it has successfully built entire .so libraries when running low on resources -- I wouldn't be terribly surprised if the same thing happened for individual .o files.

Can you do a bazel clean, then try building with `--jobs=1 -s` and attach the entire build log here please?
 I'm not a linker expert, but it looks like the problem might be discussed in this SO question: [Static initialization and destruction of a static library's globals not happening with g++](http://stackoverflow.com/q/1804606/3574081). Perhaps there are some linker options that you need (e.g. `-Wl,--whole-archive` as suggested in [this comment](http://stackoverflow.com/questions/1804606/static-initialization-and-destruction-of-a-static-librarys-globals-not-happenin#comment1692937_1804618))?
 A colleague suggests adding the following option to your build command for the code that uses `libandroid_tensorflow_lib.a`: `linkopts=["-Wl,-no-as-needed"]`.
  The network expects a batch of images, so add `tf.expand_dims(image, -1)`.
  @tensorflow-jenkins test this please.
 @tensorflow-jenkins : test this please.
 Test this please, Jenkins.
 @tensorflow-jenkins, test this please.
  A 3D convolution is almost done internally. I'll add the internal author to comment on the progress. 
 Yeah it's unfortunate that you went through all this effort since it's probably arriving with the next push (including GPU support).  I would have suggested mentioning you were working on it earlier! :(
 Also, our fault for not indicating on that bug that it was being worked on internally. It came together pretty quickly in the last week or so and we forgot to update this
 Sure!  conv1d and conv4d would be nice!  Once the conv3d implementation is available you can generally see how to do it for both CPU and GPU -- there's opportunities for lots of code sharing across all of the ops then.
 3d convolutions added in https://github.com/tensorflow/tensorflow/commit/6a187ccddaebb741ea77fc3201c6e36625f0aadb, you can take a look to see how it was done and maybe look for opportunities for code sharing with something like conv1d if you think it's worth doing!
  This is a pip problem.  Running 
`sudo pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl` works for me.  You can read more [here](https://github.com/pypa/pip/issues/3165).
  I would not use tf.add_check_numerics_ops in this case, since it doesn't know what are the hidden nodes that shouldn't be checked. 

Add tf.check_numerics to the node that you want to check it yourself. 
 You should be able to use tf.check_numerics within the loop. It is like an
identity op.

On Tue, May 3, 2016 at 4:13 PM, Daniel Johnson notifications@github.com
wrote:

> I was attempting to use add_check_numerics_ops for debugging purposes. One
> of the models I was building was spitting out NaNs and I wanted to figure
> out where they were coming from. Ideally I'd like it to check within the
> while loop too, but that doesn't seem to work.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2211#issuecomment-216692313
 @yuanbyu: Should I mark this contributions welcome?  It seems like it would be nice if `tf.add_check_numerics_ops` just did the right thing in this case. 
 Yes, sounds like a good idea.
  Adding @zheng-xq.  As noted in https://github.com/tensorflow/tensorflow/issues/916, sharing a GPU between TensorFlow and other code is not generally supported.  However, you mentioned that you modified your other code to use stream executor, which seems like it has a chance.  Are you saying you got the same error after switching to stream executor, or did the error change?
 @sjperkins is correct. If you want to share the same GPU between TensorFlow and other Cuda library, you have to be very careful. No Cuda Runtime API is safe. Either you call stream-executor, or call Cuda Driver API, and manage the context yourself. Every time when you are done with your Cuda work, restore the context back to the previous state, because that is what stream-executor implicitly expects. 
  @snakecharmer1024, int64 is better than uint64 for size. 

Adding @zffchen78 to comment whether it is okay to int64. It is possible that you need to change several places to make this work. 
 In general, TensorFlow prefers int64 when large indices or size are needed. One reason is that the following code would work naturally. 

for (auto i = v_size - 1; i>=0; i--) {...}
  @rmlarsen, I'm assigning this to you because you mentioned there was a plan to add SVD this quarter. Feel free to reassign if someone else is doing the work....
  @ry would you be open to reviewing this? 
 Thanks!

On Tue, May 3, 2016 at 4:10 PM ry notifications@github.com wrote:

> @danmane https://github.com/danmane yea sure. I'll check it out tomorrow
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-216691907
 @danmane how different is this from google style? I'm happy to use a standard if there is one.
 You can make a PR, add [WIP] to the title and reference this thread in the
description.
On Thu, May 19, 2016 at 00:00 Peter Braden notifications@github.com wrote:

> I'm not so interested in specific code lint styles, I can defer to people
> there.
> 
> I've been working in this branch master...peterbraden:nodejs-wip
> https://github.com/tensorflow/tensorflow/compare/master...peterbraden:nodejs-wip
> - I'm not sure the best way to have transparency here, should I add to this
>   pull-request, even though it's not ready to be merged, and have a
>   long-running PR, or should I wait until I have something more complete? I
>   don't have a huge amount of time to work on this, and would love to attract
>   some collaborators.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-220243286
 @martinwicke @billautomata
There are a few notable diffs, e.g. [googlejs](https://google.github.io/styleguide/javascriptguide.xml) calls for semicolons always, standard calls for no semicolons at all. But given that node is basically unused within Google, I see no benefit in tying the node frontend to Google's style guide. And it looks like standard has pretty good adoption based on the download stats.  So let's go with package/standard.
 Wow. No semicolons ever? That's insane. I would have expected semicolons to
be part of any style guide for JS given how brittle JS is around line
breaks.

Anyway. package/standard it is.
On Thu, May 19, 2016 at 10:56 Daniel W Mane notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke @billautomata
> https://github.com/billautomata
> There are a few notable diffs, e.g. googlejs
> https://google.github.io/styleguide/javascriptguide.xml calls for
> semicolons always, standard calls for no semicolons at all. But given that
> node is basically unused within Google, I see no benefit in tying the node
> frontend to Google's style guide. And it looks like standard has pretty
> good adoption based on the download stats. So let's go with
> package/standard.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-220402876
  This just bit me today. I would definitely appreciate a python 3.5 wheel, as I'm working on a system with only python 3.5.
 With PR https://github.com/tensorflow/tensorflow/pull/2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: 
https://github.com/tensorflow/tensorflow/
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Most likely we don't work properly for cudnn6.5, since it's now two generations old.

https://github.com/tensorflow/tensorflow/commit/57b4b9b401f5600d58e040664471a2cdcbcc818f probably broke it.  Can you try to sync before that change, and/or upgrade to a newer version of cudnn?  I doubt we want to support all the various versions, the code is already getting too complex for that, given the breaking API changes cudnn makes.
 Closing since the issue seems to be resolved by using a more recent cuDNN.  Please comment if that's inaccurate and I'll reopen.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please

(Thanks!  This was the intention of the field, so hopefully people aren't expecting the current behavior).
 Looks like infrastructure failure, you can ignore. I will merge tomorrow!

Cc @jendap @caisq
  Did you update the state_size property of your gru cell?
  Have you tried changing/accessing `estimator.learning_rate` after you restored the model? 
 Yeah it's currently not possible. I'll submit a fix soon. cc: @ilblackdragon 
 @mirikle See #2279. Once it's merged, you'll be able to specify e.g. `new_params={'learning_rate': 0.2}`
 @mirikle It requires different mechanism to make it work. I'll try fix it later when I get a chance. 
 @ilblackdragon Could you re-open this and assign to me? I'll try work on this soon. 
  Adding @martinwicke. 

Could you post the log to gist? 
 @keveman, do you know what could be going on?
 Hard to say, but I am guessing the binary protobuf package needs updating.
 @fcole90 Do you mind trying this wheel package for protobuf?
https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b3-cp34-cp34m-linux_x86_64.whl

(Here is the one for Python 2 : https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b3-cp27-none-linux_x86_64.whl)
 @fcole90 Gentle ping. Please let me know if the new protobuf wheel package solves this issue.
  rnn() assumes your cell is not stateful.  What you want to do is probably concatenate your activations into the output and then split them off after reading all the outputs off the rnn call.
 I would need to see all your code and a longer stack trace to answer your question fully. For now, avoid creating stateful RNNCells. Treat them like functions.
  Hi there!

> GitHub issues are for bugs / installation problems / feature requests.  
> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
> To make bugs and feature requests more easy to find and organize, we close issues that are deemed
> out of scope for GitHub Issues and point people to StackOverflow.

I'm going to close this issue because it's a question rather than a bug, but if you'd like to ask it on Stack Overflow then one of the team will respond with an answer (and it will be easier for other people to find if they hit the same problem).
  Use `labels.set_shape(())`

`batch` is a kind of FIFOQueue and it needs to know sizes of inputs for memory allocation reasons. It can either get it from `shapes` argument, or use shape inference to infer it automatically. Since `py_func` can call arbitrary Python code and give different-shaped argument between invocations, `py_func` doesn't set static shape and you need to either use `shapes` argument to `batch` or `set_shape` on `labels` tensor.

So you either need to 

`labels.set_shape(())`

or do

`labels_batch = tf.batch(..., shapes=[()])`

Here's an end-to-end example to test this

```
# Create queue, initialize it with 0,1,2,3,4,5,6,7,8,9
queue_size = 10
tf.reset_default_graph()
queue = tf.FIFOQueue(capacity=queue_size, dtypes=tf.int64)
enqueue_placeholder = tf.placeholder(dtype=tf.int64)
enqueue_op = queue.enqueue(enqueue_placeholder)
dequeue_op = queue.dequeue()

sess = tf.InteractiveSession()
for f in range(queue_size):
  sess.run([enqueue_op], feed_dict={enqueue_placeholder: f})
sess.run(queue.close())

def unpackbits(arr):
    return arr.astype(np.int64)

labels = dequeue_op
labels2, = tf.py_func(unpackbits, [labels], [tf.int64])

labels2.set_shape(())
labels2_batch = tf.batch([labels2], 2, capacity=10)

tf.start_queue_runners()

for i in range(5):
  print labels2_batch.eval()
```
  This code looks reasonable for the prototype. In general, a few ways to make it faster when you get serious about performance: 
1. Use Tensor form and avoid the loop as much as possible. Give each node more things to compute tend to improve performance. 
2. Look at the timeline / profile of the nodes, and see which one is more expensive that it should. And optimize the implementation of that one. 
3. Write a custom op or kernel and manually fuse together the operations. There are research efforts on how to automatically facilitate that. But before that becomes main stream, manual fusing is the way to go. 
 You could also compare your speed against @dave-andersen 's K-means implementation here https://gist.github.com/dave-andersen/265e68a5e879b5540ebc
  As a step to debug this problem, could you try to run without modify the classifier? 

In Cudnn documentation about this particular error: 

CUDNN_STATUS_MAPPING_ERROR
An access to GPU memory space failed, which is usually caused by a failure to bind a texture.
To correct: prior to the function call, unbind any previously bound textures.
Otherwise, this may indicate an internal error/bug in the library.

TensorFlow doesn't bind any texture with its Cudnn calls. So the last part indicated that it could be a problem in the library itself. 
 @zheng-xq: Any thoughts as to how to better diagnose?

@agupta83: What platform are you on?  I'd like to know if it's Ubuntu version specific, since @waTeim says it goes away at a later version. 
 Since the bug is fixed by upgrading Ubuntu and thus is likely to be a bug outside TensorFlow, I'm going to close this issue.  If upgrading is not an option and more information is available we can reopen, and of course we're happy to accept PRs fixing TensorFlow for older systems.
  Can you please tell us a bit more about the TensorFlow program that you were running. In particular, what `tf.scalar_summary()` ops did you expect to see in the TensorBoard output, and how did you write them to `summary_writer`? Also, when you say "Checked their sizes / Everything looked correct", does this mean that there were events in the file, or just a graph?
 Looking at the code of that notebook, I think the "No scalar data was found" error message is expected behavior: the notebook doesn't explicitly log anything to the `summary_writer`, apart from `sess.graph_def` (which probably accounts for the 8kb of data in the event log file). If you click on the "Graph" link at the top of the TensorBoard page, you should be able to see the graph visualization, as described in the notebook.
 I'm closing this due to inactivity. Let us know if you still have problems!
  @caisq: Can you take a look as someone more familiar with pip-land? 
 We are in the process of getting python3.5 build to work for linux. Stay tuned. Renaming is the temporary solution.
 With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: 
https://github.com/tensorflow/tensorflow/

Linux Python 3.5 whl files will also be included in future releases.
  This is not a good change, since it makes type errors silently work with strange behavior.  What are you trying to do?
 For example, if you pass some floats in as indices, it is a type error.  Floor is not what the user expects.
 I believe this should be fixed by making `one_hot` natively work for `int32` tensors and then removing the `int64` default.
 Thanks!  Let me know if you have questions.
 @girving assigning to you for now.  i can pick up after may 10th if it's still open.
 @ebrevdo: Assigning to you since you seem to be looking over it already.  Let me know if you want me to take over, but one reviewer seems sufficient. 
 The test failure with the string test: truth should not be a tf.constant but a np.array([[b"1.0", b"0.0", ...]], ...)
 Jenkins, test this please.
 Uh-oh.  We can't break the backwards compatibility.  What this means is folks who have created a graph with a previous version of this op will not be able to run it on a more recently compiled version of TF.  That's a big no-no.  Sorry for suggesting it.  Can we revert the change on dtype of depth?
 Jenkins, test this please.
 Jenkins, test this please.
 Thank you for bearing with me!
 @ebrevdo next time squash before merging 
  That question was a bit broad -- I think you were asking how to use skflow to compute confusion matrix on CIFAR input pipeline which possibly nobody has done before, hence no answers. I think if you break this task into smaller tasks, maybe only using TensorFlow, you'd have better luck. Closing this for now since it's not a bug or feature request.
 Posted [an answer on SO](http://stackoverflow.com/questions/36960457/tensorflow-evaluate-with-fusion-matrix/36987469#36987469) - leaving here link in case people stumble this issue.
  @tensorflow-jenkins test this please.
 Merged. Thanks.
  Can you combine this with 2184 please?
  There's no built-in op for doing this, but you could do something like the following:

``` python
input_st = ...  # object of type `tf.SparseTensor`, with 2-D indices

indices = input_st.indices
# Reverse the columns of `indices`
transposed_indices = tf.concat(1, [indices[:, 1:2], indices[:, 0:1]])

dense_shape = input_st.dense_shape
# Reverse the elements of `dense_shape`
transposed_dense_shape = tf.pack([dense_shape[1], dense_shape[0]])

transposed_st = tf.SparseTensor(transposed_indices, input_st.values, transposed_dense_shape)
```
 (Assigning this to @concretevitamin, since he is looking at the sparse ops right now.)
 Don't forget to call Reorder Sparse on the result of Derek's code.
 @concretevitamin: Are you planning to work on this?  If not, we should mark it as contributions welcome.

@mrry: `tf.reverse` is a better way to reverse a Tensor that extracting each component and manually reassembling them in the opposite order. 
 I see Eugene has marked it as contributions welcome.  I am happy to review any PRs!
 @maniteja123 please feel free to take it.  Besides all of the above suggestions, I'd imagine adding an additional bool flag (`reorder`?) to the Python function can be useful, with default to True.
 AFAICT, adding a wrapper function to sparse_ops.py sounds good.

On Wednesday, June 15, 2016, Maniteja Nandana notifications@github.com
wrote:

> Thanks for the response. Could you please clarify some questions ? Would
> adding a wrapper suffice ? I was thinking sparse_ops.py where similar
> functions are there ? Or should it be added to the core code ?
> 
> Also the implementation AFAICU would be something like
> 
> transposed_indices  = tf.reverse(sp_input.indices), [False, True])
> transposed_values = sp_input.values
> transposed_shape = tf.reverse(sp_input.shape), [True])
> sp_output = tf.SparseTensor(transosed_indices, transposed_values, transposed_shape)
> sp_output = tf.sparse_reorder(sp_output)
> 
> Please let me know if this is the right approach here ? Thanks.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2180#issuecomment-226123114,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAkLHs3DOY2wBLyLt7yCCqyVP4sa2LlXks5qL7kqgaJpZM4ITiLc
> .
  @tensorflow-jenkins test this please
 Merged. Thanks.
  Gradient is defined almost everywhere, so it could be defined in practice. It's not very useful though, so it's not registered for this op in TensorFlow.

```
x = tf.Variable([1., 1])
z = tf.argmax(x, 0)
sess = create_session()
xgrad = tf.gradients(z, x)
sess.run(tf.initialize_all_variables())
sess.run(xgrad)

LookupError: No gradient defined for operation 'ArgMax' (op type: ArgMax)
```
  The current logic is implemented as a fraction of the remaining free memory. 

Adding @vrv to decide whether it is okay to switch to a fraction of total memory. 

You are also welcome to make a contribution. The logic is here:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L599
 Seems fine to switch to fraction of total memory.  Please send a PR!
  Just as a note, pinning the ops to cpu:0 may not prevent tensorflow from initializing the GPU device. 

Is there still an issue here? Closing for now.
 Could do something like this to see placement, I bet your ops are still on CPU.

Also, to remove GPU from consideration completely, run `export CUDA_VISIBLE_DEVICES=`

```
  config = tf.ConfigProto(log_device_placement=True)
  config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM
  config.operation_timeout_in_ms=50000   # terminate on long hangs
  sess = tf.InteractiveSession("", config=config)

```
  I think zhengxq@ was looking at the auto-selection algorithm in CuDNN, any ideas?
 @vrv: Presumably more information is required, but I'm not sure what to ask for.
 @taylorhughmorgan, could you try the latest TensorFlow again, and see if you still have this problem? Thanks. 
  Nice investigation...how did you find it/any idea why this happens? Adding @keveman who added this part of code (although he may be slow to respond due to ICLR)
 Jenkins, test this please.

Nice catch. Very obscure, much wow.
 Hm... timeout, probably infrastructure failure. Will try again later.
 @tensorflow-jenkins test this please
  Hey guys, indeed we are changing a number of things - this problem should
be fixed though.

That said, currently master doesn't support save / restore for Estimators
the way it was done before. We are working on a bit revised functionality
for this.

On Wed, May 25, 2016 at 12:23 AM, Dong-Hyun Kwak notifications@github.com
wrote:

> @abronte https://github.com/abronte No I didn't try yet. I will test
> the master branch few days later.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2167#issuecomment-221493741

## 

Best regards,
Illia Polosukhin
 @ilblackdragon: If this is fixed at master, can you close it? 
  A fix for this has been checked in internally and will be available shortly.
  @ry recently added support for strides > filter, so if you use our nightly pip binaries or build from sources you should be able to do what you want!  It will be part of our next official release if you want to just wait.
  @vincentvanhoucke: Do you know what the issue is since you deprecated the old batch norm? 
 @girving someone just needs to retrain the model. @shlens FYI
 Oh, right, these are just warnings, and the GraphDef versioning mechanism is working fine in terms of not generating errors.  Yep, it'd be good to retrain the model since the old ones will vanish eventually.
 We might be able to get away without retraining by pointing the Image Recognition tutorial to the newer model provided here:
https://github.com/tensorflow/models/blob/master/inception/README.md#getting-started-1

This would require a little bit of plumbing and identifying the appropriate input layer Tensor name.
 Ironically, I did the inverse to make classification work in the deepdream
tutorial. Now I wish I had gone the other way around.
On Fri, Apr 29, 2016 at 18:46 Jon Shlens notifications@github.com wrote:

> We might be able to get away without retraining by pointing the Image
> Recognition tutorial to the newer model provided here:
> 
> https://github.com/tensorflow/models/blob/master/inception/README.md#getting-started-1
> 
> This would require a little bit of plumbing and identifying the
> appropriate input layer Tensor name.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2164#issuecomment-215923062
 The warnings are safe to ignore. We have to update the saved graph and checkpoint to not use these deprecated ops. I will leave this issue open to remind us we have to do that.
  Thanks for the bug report.

@girving tf.gradiensts() is generating an unguarded div by 0.  Internal bug opened.
 Thanks for reporting!  We're going to fix TensorFlow to produce nice exceptions for integer zero divisions.
 Ah, there are two bugs here: one bug that we crash the process on integer division by zero, and another that we generate a division by zero.
 @altaetran: It's conceptually straightforward: one has to go through and sanitize all the occurrences of `//` in `math_grad.py`.  Unfortunately there are a bunch of them for a variety of different ops, and the fixes look different in the different cases.

However, there's no way to get a `None` gradient, since the variables _are_ connected, just through an empty tensor bottleneck.  Since we don't necessarily know that the shapes are empty until runtime, we can't produce `None`. 
 @altaetran: To make my initial comment make sense: we want to produce nice error messages even for malformed ops, so we want to fix the underlying divisions by zero regardless of whether we fix the gradient code. 
 As I said: the fixes are straightforward but there are a number of them.  Here is an example of a bad line: https://github.com/tensorflow/tensorflow/blob/5e22e3a3187e6729488f4a6da59b4cfbedc40946/tensorflow/python/ops/math_grad.py#L36
 I'm looking at it now, so not too long except for the upcoming weekend. :)

I'll probably fix the gradients first since making integer division safe is slightly more awkward (to do in a reasonably performant manner).
 CL submitted, so it should be on Github within 24 hours.
 To clarify: I fixed the gradients not to divide by zero, which should solve your issue.  Integer divide by zero on its own will still crash the process.
 @rbharath: Do you want source or binaries?  The source version should have the fix tomorrow.  Not sure when the nightly appears (tomorrow or the next day), but the links for those are here: https://github.com/tensorflow/tensorflow/blob/master/README.md
 @rbharath: Ah, yes: the fix I just made is Python only.  However, we don't have any setup for using different Python alongside C++, and in any case you'd have to carefully cherry-pick just my change on top of 0.8.0 to have a hope of succeeding.
  @tensorflow-jenkins test this please
  Yes, currently it only support dense numpy matrix / pandas DataFrame and Series.
If you are interested in adding a data feeder for sparse matricies - that would be great contribution!
  Docs only change, merging.
  @cssndrx: Did this fix make it into the master branch?
 @mrry I think so. Looks like you are adding that commit internally. https://github.com/tensorflow/tensorflow/commit/2c3c49bb6a1a2f3a9ebcf0b9f48cf8988197f718
 Great, thanks!
  in `seq2seq.sequence_loss_by_example` the averaging happens across timesteps but only if the timesteps are specified as a list of elements.

In PTB case we concatenate all the steps before calling that function and `log_perps` ([batch_size \* num_steps]) is divided by tf.ones([batch_size \* num_steps]) which is esentially a no-op.

Instead of calling sequence_loss_by_example we could directly call sparse_cross_entropy_with_logits because that's the only thing of interest here. In the past, that function was more complicated and we didn't want to copy its logic into ptb_word_lm.py. Feel free to send a PR.

Hope that makes sense!
 @giancds: Sorry for the delay; Rafal just left Google.  @ludimagister: Could you answer @giancds's question?  
  does this change cause any tests to fail?  if not, shouldn't it have? :)

cc @Mistobaan 
 Yes please!
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Unfortunately because you rebased I've lost the history of the tests, and also you probably rebased in the incorrect way, since a bunch of other commits are showing up in this PR.  Try restarting the PR from scratch?
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 test this please
 Wahh @caisq what happened
 Oh @poxvoculi introduced a problem during the merge conflict, nevermind.
 @tensorflow-jenkins test this please
  No good way really, besides running and seeing if you run out of memory.
You may get more help on stackoverflow, we are trying to keep issues for bugs:

related SO questions
http://stackoverflow.com/questions/36514994/how-to-estimate-the-amount-of-memory-needed-for-rnn
http://stackoverflow.com/questions/36331419/tensorflow-how-to-measure-how-much-gpu-memory-each-tensor-takes
 have you seen https://github.com/tensorflow/tensorflow/blob/fe454464681b036ff7fed3e42c6bb541fa52dd7c/tensorflow/python/tools/graph_metrics.py ?  It might not provide everything you want, and 'how much memory is needed' is a little hard to quantify, because you can trade off speed for memory.
 Oh, didn't know about graph_metrics....from first glance that seems to only calculate memory needed for parameters and not activations
  Did you build with -c opt ?
 Hm, not entirely sure :(

1) Did you build the pip package additionally with  --copt=-mavx ?  I think our pip wheels do that.
2) Is there a previous commit where this didn't happen, or has this always been happening for you?

@damienmg for help 
 AVX can still help since not all operations are on GPU, and some code can still be accelerated by avx.  Worth a shot anyway :)

I think if you have to tweak include paths and all of that jazz, something is really wrong -- most of our users don't have to do that.  Did the instructions to add --spawn-strategy and genrule-strategy=standalone not help?
 Closing for now due to lack of response to @damienmg's question.  @Dapid: Please comment if you have more information and I'm happy to reopen. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @ilblackdragon Should transform or fit_transform be preferred here?
 @willnorris if users sign the corp CLA, is there any way to get the label to be set to yes?
 @poxvoculi transform should be used usually - as in serving you don't actually know what would be mean / std.dev of the distribution.
 @tensorflow-jenkins test this please.
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
  Our docs are automatically generated from source: can you modify the version here instead: https://github.com/tensorflow/tensorflow/blob/d8e8b872eae63188c75046d5bb068e03a81b3f85/tensorflow/python/ops/array_ops.py#L1760
 @tensorflow-jenkins test this please

(our markdown generator I guess isn't perfect in terms of spaces and tabs I guess?)
  Two comments:
1. Gate seems like an overkill class.
   If you want to allow setting the nonlinearity and initialization of a particular gate, why not just accept and store these parameters directly?
2. We're deprecating linear() in favor of the layers functions, like fully_connected. In fact, linear can and will disappear as soon as can make this happen.
 We currently use single fused weight and bias variables in order to speed up computation.  Having separate initializers would lead to a significant slowdown.  You are welcome to write your own RNNCell which creates separate weight variables, each with their own initializers.
 Closing this for now; as it sounds straightforward for you to create this custom LSTMCell in your own repository.
  Thanks for the fix!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins test this please
 CLAs look good, thanks!

<!-- ok -->
  As Eugene and Vijay commented, it is currently on the contrib directory. And once it matures enough, it will be moved to the core directory. 

As always, contributions are welcome. For any significant code changes, it is optional to announce it first, so people would notice and avoid replicated work. 
  The NIVIDIA driver function cuDeviceCanAccessPeer is returning false for all pairs of GPUs in your environment.   Perhaps your installation does not have correct access to the proper or full CUDA libraries?   @zheng-xq  Any clue how this should be diagnosed?
 Today, you are not losing much if peer-to-peer access is not enabled. But it might have performance implication for future TensorFlow releases. 

If you want to debug this issue locally, you can run the Cuda SDK sample simpleP2P and see if there is a problem. 
 Maybe your GPU0 and GPU2 are you different Pcie Root Complexes? That would force communication to go through QPI link, so maybe that prevents peering? FB supposedly open-sourced it's Big Sur design which shows to configure all 8 GPUs to be on the same root complex, although I'm having trouble finding the actual specs
 The results from `simpleP2P` seem like the problem is a CUDA configuration issue and not a TensorFlow-specific issue. I'm closing this for now.
  Hi,

Thanks for filing and apologies for the delay in response. This was fixed with f12b843d78b0119b3f8d0ffc99a9e417d9232e7c. You should be able to see correct edge shapes (not only the first output shape) in the 0.9 release, or if you don't want to wait, we just published a release candidate https://github.com/tensorflow/tensorflow/releases/tag/v0.9.0rc0
  Try running bazel with `--verbose_failures`, otherwise we have no information to help.
 Closing for now, since it seems like an NVIDIA bug.  I'm happy to reopen if there's something we can do about this on our end.
 @Froskekongen The problem is that we're not sure if that cxx_flag will break other people.  Do you have a better sense for what it does, and whether other breakage is likely?
  Hi @Russell91. This request sounds quite reasonable to me. If you send in a pull request that enables this, I would happily accept it. I think the best thing to do would be to have gfile.Walk https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_gfile.py#L400 follow symlinks by default (we can just pass follow_links=True to the os.walk call)
  This question is better asked in the discussion forum.   This context is for bug-like issues, thanks!

https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/discuss
  Sorry, I couldn't replicate it yet (didn't try on exactly the same settings though). Could you try with batch_size=2 and/or memory_dim=2? Does it happen as well, or is it specific to both being 1?
 Hmm, it generally works, e.g. the translation model from the seq2seq tutorial. Can you share your full code so I can reproduce the problem?
 Hmm, this repo is quite a lot of indirection to go through. Could you try to minimize the bug example in some way, provide, e.g., a single file?

One thing to note: there is a known outstanding bug that causes segfaults like the one you reported when scipy is imported after tensorflow. I think someone is working on that, could it be that some of your imports is causing the same problem?
 Hi harpibot, any update on this? Let me know if you still see the problem!
  Issue 2066: Fix the conv for stride > ksize case. Passed all the tests and locally verified it fixed the problem. 
 @tensorflow-jenkins test this please
 I think this is a known flaky test.

@zheng-xq: shouldn't we add a test to conv_ops_test.py for this?
 @tensorflow-jenkins test this please
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 Looks like this is ready to merge?
  Yes, it would be very useful to have better tools to analyze and predict TensorFlow program costs, in terms of time, memory and other resource needs.

TensorFlow programs can be analyzed both statically and dynamically.  Your questions seem to me to suggest that maybe the properties in which you're interested can be discovered through static analysis.  

By and large, through static analysis one can determine the Tensor shapes and hence sizes of Variables.  Whether or not a given Variable functions as a model parameter is another question.    

The maximum memory required to execute a program is harder to get through static analysis because it depends on the extent to which memory intensive operations not otherwise constrained may overlap in time.  This may be non-deterministic, or determined by the runtime by rules that go beyond the denotational semantics of TensorFlow.  For example, there is the possibility of leaving values in memory for a long time, or recomputing them on demand, when the cost of doing so is less than the cost of leaving the memory occupied.  A version of this strategy is swapping values between CPU and GPU memory to free GPU memory for a period.

Determining the number of operations required to execute a node is also difficult for static analysis, so long as the Op implementing the node is opaque to the analysis, i.e. if you're not able to examine the actual machine instructions generated by the compiler.  In this case it seems more practical to dynamically measure the time or cycles consumed by an Op and use that to estimate its cost.

Better runtime profiling tools would be a welcome addition.
  @benoitsteiner: Eigen's ExtractGlimpse code does seem broken: it builds its own internal RNG without any seed input from outside. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 closing due to inactivity and lack of CLA
  1. By doing the following:
   with tf.control_dependencies([apply_gradient_op]):
     train_op = variable_averages.apply(tf.trainable_variables())

The parameters are updated before the variable_averages.
1. This is multi-gpu run. We are trying to return a snapshot of total_loss in case it's being updated by the other thread.
2. I don't fully understand your question. Are you asking whether the implementation you quoted is correct, or you are trying to do something else different, and wonder how you should do it?

Thanks,
Sherry 
 This fits better as a question on StackOverflow, since it doesn't seem to be about a fixable issue in TensorFlow.
  I can reproduce. Thanks for giving self-contained repro, this is super helpful. Looking
 Oh...you are calling "queue.close()", but that actually returns an op which needs to be run to do anything. You need to do `sess.run(q.close())`. Since your first queue is not closed, your "batch" queue is waiting forever for something to be added to the first queue.

Furthermore, this wait is happening in C++ mutex, so `stop_grace_period_secs` is useless -- the queue runner thread checks for "stop_requested" between session `run` calls, but because dequeue op never returns, it's stuck inside `session.run` forever.

Here's the simpler example I made for myself to reproduce this

```
def create_session():
  """Resets local session, returns new InteractiveSession"""

  config = tf.ConfigProto(log_device_placement=True)
  config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM
  config.operation_timeout_in_ms=5000   # terminate on long hangs
  sess = tf.InteractiveSession("", config=config)
  return sess

tf.reset_default_graph()
q = tf.FIFOQueue(4, tf.string)
enqueue_val = tf.placeholder(dtype=tf.string)
enqueue_op = q.enqueue(enqueue_val)
size_op = q.size()
dequeue_op = q.dequeue()
sess = create_session()
def enqueueit(val):
  sess.run([enqueue_op], feed_dict={enqueue_val:val})
  print "queue1 size: ", sess.run(size_op)
enqueueit("1")
enqueueit("2")
enqueueit("3")
#sess.run(q.close())

dequeue_op.set_shape([])
queue2 = tf.train.batch([dequeue_op], batch_size=1, num_threads=1, capacity=1)
threads = tf.train.start_queue_runners()

def dequeueit():
  print "queue1 size: ", sess.run(size_op)
  print "queue2 size before: ", sess.run("batch/fifo_queue_Size:0")
  print "result: ", sess.run(queue2)
  print "queue2 size after: ", sess.run("batch/fifo_queue_Size:0")

dequeueit()
dequeueit()
dequeueit()
coord.request_stop()
coord.join(threads, stop_grace_period_secs=5)
```
 So I guess the problem is that there's no way to stop a thread from Python if a thread is stuck inside of a `session.run` call. One work-around is to create session with `config.operation_timeout_in_ms` as above
 closing since it doesn't seem to be possible to fix this besides the work-around given
 You are saying we have documentation that tells people to train/validate on separate GPUs? Which one is it? Meanwhile, you could also post your implementation as a comment here, and I'll reference as it comes up in issues/stackoverflow questions, etc
  Your stack trace shows a failure in numpy, yet it sounds like your source program is just the one line.  What if you import numpy first, ie.  place
import numpy
above 
import tensorflow
?
 possibly related: https://github.com/tensorflow/tensorflow/issues/2034
  @ziky90: Does @callicles's fix solve your issue?  
  Thanks for the detailed report.   
 tensorflow/core/kernels/tensor_array_ops.cc:753 is the key line from the stack trace.  @ebrevdo is the author, but is on vacation at the moment.  Other calls to functor::Split() can be found at tensorflow/core/kernels/split_op.cc:172 and tensorflow/core/kernels/unpack_op.cc:86.  Looks like @girving might have some familiarity.
 The Split kernel assumes its inputs are nonempty, and `TensorArrayUnpackOp` doesn't check this.  It should.
 @ebrevdo: Would you have time to look at this, now you're back? (Or did it get fixed in the mean time?)
 Looks like it was fixed in https://github.com/tensorflow/tensorflow/commit/b7f7fe26fd2e15978d84a1f02e983faf7324a131.
  @vrv is working on improving the device placement so that policies will be more declarative, rather than binding to specific devices at graph construction time. This should address some of the limitations of `tf.train.replica_device_setter()`. If you had specific requirements, it would be good to collect them here, so that we could take them into consideration in the redesign.
 See the discussion on that issue. The problem is most likely caused by the fact that `tf.train.SyncReplicasOptimizer` bakes the number of workers into the checkpoint, which is another issue that should probably be addressed.
 Indeed, this is a much bigger project that we'll chip away at over time (optimizing for runtime across a distributed cluster is kind of a hard problem, and we're not good enough at it that manual placement typically produces better results).  We'd love to do a better job of this, so we'll be looking at this over the next little while.  Happy to brainstorm ideas here.
  @yuanbyu: Could you take a look?
  https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#mac-os-x-typeerror-init-got-an-unexpected-keyword-argument-syntax
  This kind of problem is very hard to diagnose.  Given that we have a long track record of running inception variants on TensorFlow, it's unlikely to be a simple logic error in the TensorFlow source.  Are you running any Ops you've defined yourself, or custom kernels?  If not, I would suspect either a mutual problem between TF and CUDA 7.5 or something in your hardware environment.  To diagnose that, you probably need to use the NVIDIA tools and also anything else that might indicate problems with the PCIe bus(es) to which your GPUs are attached.

How many GPUs is your model using?
When a GPU freezes like this, is it always the same one?  If so, try running the model without that one, just using the others.
Take a look at the full
  nvidia-smi -q
output.  It might be that the memory utilization number is bogus.  
If you can try using CUDA 7.0, that might be worth a shot.

The first thread that you've traced is just the python thread that's issued a session run request.  The model executes within the C++/CUDA binary.  However, if the GPU has gotten into some kind of infinite loop, you won't see much indication of why in any of the thread stacks: you'll likely be able to see that an executor thread is waiting for some Op to terminate, but not which one.  Although it's unlikely to be of much help, if you're willing to modify the source and recompile you can log the names of ops before/after execution by modifying common_runtime/executor.cc.
 @zheng-xq: Any ideas here, including ideas for further experiments to get more information? 
 @ppwwyyxx, @liuyipei, @yosinski, in order to debug this problem, it is very useful to list all threads information, even better with call stacks, at least for the unique ones. Also please run the same experiment with environmental variable "CUDA_LAUNCH_BLOCKING=1" set. 

That would help us pin-point where the hang is happening. 

There are a number of problems described here. Not sure all of them are the same one. But on the surface, "query event: CUDA_ERROR_LAUNCH_TIMEOUT" implies a "cuEventQuery" call fails with a "CUDA_ERROR_LAUNCH_TIMEOUT" status. Since "cuEventQuery" itself is non-blocking, it is quite strange that it could fail with timeout. So another possibility is some other kernel is failing, and cuEventQuery is the one that detects that. The "CUDA_LAUNCH_BLOCKING=1" should tell us which Cuda calls is making the trouble. 

One possibility is that we are deadlock within the Cuda library itself. Since we don't have source code, debugging that would be more difficult. 
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Test this please
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins test this please
  `output_shape` is needed because the shape of the output can't necessarily be computed from the shape of the input, specifically if the output is smaller than the filter and we're using `VALID` padding so the input is an empty image.  However, this degenerate case is unimportant most of the time, so it'd be reasonable to make the Python wrapper compute `output_shape` automatically if it isn't set.
 Ah, also the shape is ambiguous in cases with `stride > 1`.
 `conv2d_transpose` is also used as the gradient of `conv2d`, so it needs to be able to spit out any shape that can be an input to `conv2d` for a given output shape of `conv2d`.  The formulas for the shape of the output of `conv2d` are

```
output = (input - filter + stride) // stride  # VALID
output = (input + stride - 1) // stride  # SAME
```

If `stride > 1`, neither of these are invertible.  However, I think it's an excellent idea to give `output_shape` a default value for `conv2d_transpose` in Python.
 @amolchanov86 Questions like that should go on StackOverflow; Github issues are for bugs and feature requests.
 I'm going to mark this as contributions welcome since there are some subtleties that I don't have time to investigate at the moment.  I'd be happy to review PRs if anyone wants to tackle this.  The subtlety is that  I'm not sure what the ideal default should be.
  Thanks for the detailed report.  It sounds like the pip installation didn't work for you for reasons that we can't reproduce.  Would you consider the virtualenv install?  Under that install you should end up with a small executable 'tensorboard' in the bin directory under the virtualenv directory.    Its contents should be

#!<path to virtualenv directory>/bin/python

# -_\- coding: utf-8 -_-

import re
import sys

from tensorflow.tensorboard.tensorboard import main

if **name** == '**main**':
    sys.argv[0] = re.sub(r'(-script.pyw|.exe)?$', '', sys.argv[0])
    sys.exit(main())
  @tensorflow-jenkins test this please
 there are conflicts :(
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Jenkins, test this please!
 Come on Jenkins, test this, please!
 Jenkins, test this again, please.
 Jenkins? Test this please!
  This might be a good issue to take up on the discussion board:

https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/discuss
  It would be great to get these instructions into the documentation somewhere.  @martinwicke : is there a place for build help specific to linux distributions?
 https://github.com/tensorflow/tensorflow/pull/2094 is fixing the bazel parse version issue.

As for the other four issues, would it be better to file these bugs at buildbazel/bazel ?  @damienmg is there anything else we should be doing on the TF side to make this better?
 @vrv, @akors: Should we split the conclusions of this up into separate bugs, appropriately distributed between TensorFlow and Bazel?  At least one of the issues is fixed by #2504, and it's hard from this thread to know what's left. 
 Yes, it would be good to file separate bugs for the individual issues, since catch all bugs like these tend to get filled with lots of hard to parse info.
 @martinwicke @damienmg Can you advise on how to split this bug across Bazel and Github?
 This is the tracking bug for cuda autoconf: #2873
  Assuming you are using the Android demo code [here](https://github.com/tensorflow/tensorflow/tree/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/examples/android/src/org/tensorflow/demo), yes, that looks approximately right.
  This is because you have cudnn v5, which requires installing from source.

See: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#requirements for info
  Closing as a duplicate of https://github.com/tensorflow/models/issues/54.
  Sorry you're having trouble.  Did you previously install another version of tensorflow?  If so, you must use pip uninstall to remove the first installation before trying to upgrade.  If you have an unusual linux distribution it may be necessary to install from sources by configuring and compiling on your machine.
 @liumilan: It does sound like you'll need to install from source.  I'm going to close this bug for now: if you still have issues please reopen a separate bug about the problems installing from source. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please.
 The false branch is `lambda: (ema_mean, ema_var)`.  Both ema_mean and ema_var must not be None. Otherwise the computation is meaningless.  The error message should have been better.
 @tensorflow-jenkins test this please.
 @mikowals can you please resolve conflicts?
 @mikowals There's still conflict
 status?
 @tensorflow-jenkins test this please.
 status?
 not sure, might be test infra failure.  if you rebase to resolve the conflict we can try again
 Thanks!

@tensorflow-jenkins test this please
 Since this has yet to be merged I wonder if I should amend to it use the newly added  `tf.contrib.layers.batch_norm` to avoid duplication.

I think this could become:

```
def batch_normalize(tensor_in,
       epsilon=1e-5,
       convnet=False, # this becomes redundant with tensor_in shape
       decay=0.9,
       scale_after_normalization=True):

    with vs.variable_scope("batch_norm"):
        batch_norm = functools.partial(layers.batch_norm, tensor_in,
              decay=decay,
              center=True,
              scale=scale_after_normalization,
              epsilon=epsilon,
              updates_collections=None, 
              scope='learn_bn')

        batch_norm_from_moving_averages = functools.partial(batch_norm, 
              is_training=False, reuse=True)

        is_training = array_ops_.squeeze(ops.get_collection("IS_TRAINING"))
        return control_flow_ops.cond(is_training, batch_norm, batch_norm_from_moving_averages)
```
 Sorry @mikowals, this PR dropped off the attention list.  @ilblackdragon what do you think about his suggestion?
 I have rebased to current master.  Sorry for close / reopen activity.  I am learning the limits of my git understanding.
 @tensorflow-jenkins test this please!

@mikowals Let's submit what you have (sorry for late responses on my side) and we can refactor to use `layers.batch_norm` in subsequent changes. Note, we ideally want to move `learn/ops/` into `layers/`, so it will be a bit more work.
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 test this please
 test this please
 Jenkins, test this please!

the RE isn't perfect. We want to move this to a proper chatbot. ;)
 i added benoit to the trigger list (I used the wrong username, sigh).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I believe these are actually floats. Sure, floats that each carry only one bit of information, but partly for historical reasons, partly for convenience, they are floats. 
  Can you try importing tensorflow after PIL? There are some shared library issues we've been seeing where the version of libpng and libjpeg tensorflow is compiled with conflicts with other libraries that are linked against a different version (we've seen problems with OpenCV).
 I'm closing this issue as a duplicate of #1924, it seems to be the same problem.
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @tensorflow-jenkins test this please
 Regarding the quantization related errors, please make another push from internal. There is a recent CL (120733385) that fixes it.

We'll also have to look into the remaining errors.
  Thanks for the bug report.  Fix in process.
 @zheng-xq, @poxvoculi: Is a fix really in progress?  It's only 10 or so lines of code, and @mwalton already wrote most of it here: https://github.com/tensorflow/tensorflow/issues/1793  
 @girving, a PR is welcome. 
 Closing as duplicate of #1793.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please.
  I'll close this for inactivity. If you update the PR, I'm happy to reopen.
  There are probably better ways to do this, but in the interest of unbreaking people, this seems fine.  Going to test and merge if successful.
 @tensorflow-jenkins test this please
  status of this PR?  you also need to add license headers to all of these new files
 This looks good overall. Remember to add the required license headers as vrv pointed out.
 Look at just about any other checked-in file :)
 @tensorflow-jenkins test this please !!
 may want to add sample_ops to gen_docs ?
 is this ready to review again?
 Ah, gotcha.  We can reopen later if needed.  Thanks!
  See: https://github.com/tensorflow/tensorflow/issues/1961
 This bug (#1961) has been fixed at HEAD and in the 0.8 release.
  Merge summaries after optimizer adds loss and gradients summaries.

Re: #2063 
  HDF5 is a popular format to store complicated datasets (alternative to proto files).
Also has different random seeking functionality and is widely used in scientific community.

This is FR for:
- Adding HDF5 reader.
- Support of HDF5 in tf.learn.

(originally from https://github.com/tensorflow/skflow/issues/52)
 This can be closed. See an example in examples/skflow. 
 Thanks @terrytangyuan!
 @sun9700 Yes, the out-of-core doesn't work yet. Feel free to file a separate Feature Request about it (support streaming from hdf5) - and of cause PR will be highly welcome :)
  I am planning to do some refactoring: 
1. Move `rnn_estimator` in  `get_rnn_model` into an `rnn_op`
2. Replace `get_dnn_model` and `get_rnn_model` with a more generalized `get_supervised_model`.
3. Make `get_autoencoder_model` more general so other ops, like `rnn_op`, can be used as encoder/decoder. 

Any thoughts? Should I do that in a following new PR? @ilblackdragon 
 @ilblackdragon Conflicts fixed, example added, delaying refactoring after internal changes. 
 @tensorflow-jenkins test this please.
  What if you switch the order and do

```
import numpy as np
import tensorflow
import matplotlib.pyplot as plt
```

Seems related to https://github.com/tensorflow/tensorflow/issues/2034
 Does it still happen at head? (a similar-sounding segfault issue was fixed in https://github.com/tensorflow/tensorflow/commit/be092f47e0072645116a64496f7fa654ed708624)
 Closing as a duplicate of #2034.  This is fixed in 0.9.
  Thanks for the suggestions!

Our documentation is open source too, so feel free to send us a PR for these!

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md is the file to edit.
 For 3., it would be best to note that you can use -j X to throttle bazel. It's easier than local_resources and gets the job done. 
  Thanks for the report: we do not yet support GPUs on OS X, but https://github.com/tensorflow/tensorflow/pull/664 is adding support for it, stay tuned.  Follow on that thread, will close for now!
  @tensorflow-jenkins please test this!
 @tensorflow-jenkins test this please.
  I agree that it would be good to add masking to the attention model and API. But I think adding an op and a separate cuda kernel just for that is a huge overkill. Why not just multiply pointwise by the mask in python before the softmax? Without a good reason, I find a PR like this not acceptable, the code complexity and maintenance cost looks much larger than the simple problem it solves.
 It is possible that I don't understand the problem, so please, clarify. But why would it need a special op to make the mask a function? If it's a tensor of 0s and 1s, it can be changed at every step, why can't this be done the simple way in python? Is there a very big speed advantage to having an extra op?
 I was thinking about it a bit and I still don't see the problem. Here is how I'd do it:
- add a parameter to attention_decoder called mask_function
- this parameter is a python function that takes the step number and the previous attention mask tensor
  and outputs a new attention mask tensor
- every attention is multiplied by mask_function(prev_attention, step_number) (one-liner in python)

Would that work? If you want to feed-in attention, you'd just provide mask_function(_, i) = length-tensor[i] so you can feed the example lengths. And if you want to average by previous (or move it with a step), you just need to write a lambda that does it.

Or am I missing something crucial here?
 To answer your question, let me be a little bit more precise. What I suggest is to modify the [seq2seq.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py) in two ways.

(1) on lines 546, 565, 567 instead of `attns = attention(state)` let's do `attns = attention(state, attns, inp, i)` so the `attention()` function now takes the arguments `attention(query, previous_attention, decoder_input, step_number)`

(2) on line 530 instead of `a = nn_ops.softmax(s)` let's do `a = nn_ops.softmax(s + mask_f(previous_attention, decoder_input, step_number) * -LARGE_NUMBER` where `mask_f` is an extra argument to the decoder.

In this way the mask_f function gets all the current information in the step. Its task is to generate 1.0 (or anything positive) if we want to mask something and 0.0 otherwise. The 1.0 will be turned into a -LARGE_NUMBER bias in the softmax and effectively disable that field for attention, right?

The tensor s has shape [batch_size x attn_length], where attn_length is the size of the attention_states (length of the encoder in a seq2seq model). Let me focus on the basic case when we want mask_f to mask an input iff the decoder input symbol is a padding, which means that it is all zeros. We can do this in two ways, I think. If we're sure the decoder input is all 0s only if it's padded and has a reasonably large norm otherwise, then we can do `mask_f(_, dec_inp, _) = tf.minimum(1.0, tf.nn.l2_loss(dec_inp) * LARGE)`. If we're not sure of that, then we can do `mask_f(_, _, step) = self.decoder_mask[i]` when declaring the `mask_f` function somewhere in our model code. In this case `decoder_mask` are placeholders of the same kind as `decoder_inputs` that are 1 in case of padding and 0 otherwise -- and we need to make sure to feed the appropriate values.

I'm sure I missed some things. In particular, `previous_attention` coming to the `attention` function should probably be the previous attention mask, not the previous attention vector as I suggested above (so the new mask needs to be returned in the function and the pointer saved in the loop, a few more lines). But does the general idea sound reasonable and flexible enough?
  The `array` in

```
array = np.random.rand(32, 100, 100)
```

in the context 

```
import numpy as np
array = np.random.rand(32, 100, 100)

def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.float32)
  return tf.matmul(arg, arg) + arg

# The following calls are equivalent.
value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))
value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])
value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))
```

is never used. I suggest removing it for clarity.
 The markdown file is auto-generated from a docstring in `tensorflow/python/framework/ops.py`. Can you please make the change there?
 @mrry Ah, sorry about that ... that was quite stupid; reverted the previous change and applied the fix to the ops.py file now
 @tensorflow-jenkins, test this please.
 Thanks for the fix! It looks like Jenkins isn't listening to me, so I'm going to go ahead and merge it directly.
  @yuanbyu Could that be related to the recent gradients code in while/loop?
 I will have to take a look.  My guess is that the gradient code for one of the TensorArray ops missed a dependency, resulting in reading an unwritten slot.
 I see you are performing an unpack to write into one of the TensorArrays.  Though I haven't taken a careful look at the code, make sure all the elements you unpack have a corresponding read or you have a pack(); otherwise there may be no gradient written to the corresponding slot at runtime.

It's also easier to debug if you can write a very simple failing case.
 When reading an unwritten slot from a gradient TensorArray, could we return zeros instead of an error?
 They should all be read from also.
On Apr 27, 2016 11:32 AM, "ofirnachum" notifications@github.com wrote:

> Is it actually reading an unwritten slot? At least in the forward stage,
> all read slots should be previously written to.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2077#issuecomment-215184037
 Are you using a separate tensor array inside each while loop iteration?
On Apr 28, 2016 7:29 PM, "ofirnachum" notifications@github.com wrote:

> This should also be the case. In fact, when I replace the 'while' loop
> with a written out single iteration (equivalent in the case that I test),
> the code works fine forwards and backwards. So I would assume there is
> something to do with using the TensorArray's inside the while loop that is
> causing issues.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2077#issuecomment-215613376
 We haven't tested your use case very carefully.  I'll have to review your
code.. May be a few weeks unless Yuan has time to do it before I get to it.
On Apr 28, 2016 7:49 PM, "ofirnachum" notifications@github.com wrote:

> The inner handle of the tensor array should be the same through all
> iterations. The style is similar to _dynamic_rnn_loop in
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py
> (the TensorArray is written-to to yield a new TensorArray which is passed
> to the next iteration - the difference in my code is that the array is also
> read from).
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2077#issuecomment-215615001
 @ebrevdo, @yuanbyu: What's the status of this?  
 We have a bug in gradient support for unpack and split of dynamic TensorArray. I have a pending CL fixing this problem.
  That won't work, I'm afraid: `asctime()` will be evaluated once when you build the graph in Python, and all log entries will include the same timestamp. The `tf.Print()` op uses the [standard logging implementation in TensorFlow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.h), which does not have a particularly rich feature set.

It might be useful to add a timestamp to all log entries, but that would be a fairly invasive change. Alternatively, one could add some ops (perhaps in `tf.contrib`) that returned/formatted the current time, so that you could build whatever log you wanted.

I'll mark this one contributions welcome.
  I think it'd be better to handle this the way numpy does it, with `np.unravel_index`: https://bytes.com/topic/python/answers/509074-numpy-argmin-multidimensional-arrays.  Making `tf.argmin` do this directly would either require a new op or an unpleasant boolean flag that indexes over everything, and it still wouldn't be what you want since most of time you want to minimize over some but not all of the dimensions (e.g., the last three dims of a 4D batched image tensor).

Cc @aselle since `unravel_index` sounds vaguely index related, but I'll mark this contributions welcome for now.
 Not sure which document you mean, but `tf.unravel_index` doesn't exist yet.  `np.unravel_index` is described in that thread I linked to.
 Someone would have to write a TensorFlow version of `np.unravel_index`, which could be called `tf.unravel_index`.  We might not do that soon, so PRs adding it would be welcome.  `tf.unravel_index` could either be a new C++ op or something written in Python.
  @tensorflow-jenkins test this please
(and as a test for yourself along the way)
  This change has more impact that this issue alone. So I would like to find out whether there is another way. 
1. Is aggregate_ops_gpu.cu.cc the only *.cu.cc translation unit that has this problem? You can continue to build with "-k". 
2. Could you list the header file inclusion order from aggregate_ops_gpu.cu.cc to the offending header file on your system? We should check whether all of them are necessary.
 @zheng-xq @fayeshine any updates? 
 friendly ping for updates?
 Have we changed this recently? I have just build "bazel test //tensrflow/..." without any error on 16.04...

I will setup CI build for it tomorrow then we will know for sure :)
 @jendap what if you do`bazel test --config=cuda` 
 @zheng-xq: apparently discussion on https://github.com/torch/torch7/issues/670 suggests that others have

1) added this flag FORCE_INLINES

2) Suggested people move to cuda 8.0 RC which apparently fixes the problem. 
 #2 seems to be good path right now. 
 ok, closing, since it sounds like a cuda / nvidia problem that has a workaround (upgrade), and we'd rather not add the large hammer of FORCE_INLINES in our entire build
  I think I [answered your question](http://stackoverflow.com/a/36817586/3574081) on Stack Overflow.

(To save other readers a click, the `inputs` argument to `tensorflow::Session::Run()` is equivalent to the `feed_dict` in `tf.Session.run()`.)
  I'm not aware that this issue has been raised before.  Could you clarify what you think the problems are?
 There is no supported Java API for TensorFlow at present, but you could look at the source of the [Android JNI wrapper](https://github.com/tensorflow/tensorflow/tree/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/examples/android/jni) for guidance in how to build your own interface.
  I've been able to replicate this behavior by constructing n+1 padded queues and using tf.case (default a noop).  n queues, one for each bucket, deququeing batch_size elements and enqueueing the result in a single enqueue to the final queue you will read from.
 You'll also need Queue.from_list
 update on this?  @ebrevdo @wchan 
 We'll work together to see if we can build a simple implementation using
queues.  That said, this approach has a certain appeal to me.
On May 24, 2016 2:13 PM, "Vijay Vasudevan" notifications@github.com wrote:

> update on this? @ebrevdo https://github.com/ebrevdo @wchan
> https://github.com/wchan
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2069#issuecomment-221394836
 At the very least I'd like this to work more like PaddingFIFOQueue, if we go with this approach.
 Closing due to inactivity
  This was an intentional decision: the existing "tensor conversion functions" (perhaps an ambiguous name...) are mostly used to add nodes to the current graph, so we decided that these shouldn't be run on the arguments to `Session.run()`. In addition, the tensor conversion functions, which return a single tensor, aren't compatible with the mechanism used to flatten objects (e.g. `tf.SparseTensor` or `tf.SparseTensorValue`) into the component tensors, so that they can be passed to the underlying API.

It might be useful to add a mechanism for registering fetch and feed conversion functions, so I'll mark this as "contributions welcome".
  @zheng-xq any ideas?
 Sorry, yes, I can reproduce it with the latest nightly.  We'll try to take a look at this.
 I've created a PR [2137](https://github.com/tensorflow/tensorflow/pull/2137) to fix this issue. 
 Fixed in #2137
 http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/

It looks like the nightly gpu wheels have been failing to build recently.  It likely doesn't have the above fix in it -- :(
 @caisq  -- seems like more platform python issues (can't import default/ dir).  I assume that's fixed internally and just needs to be pushed?
  I am unable to reproduce this on my installation...

Could you please provide more information. e.g. did you have a previous version of TensorFlow installed, and if so did you remove it before installing 0.8?   (see note on https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#pip-installation)

What is your LD_LIBRARY_PATH?

Thanks, Paul
 Are you still having a problem?  When you say it "gets stuck", what do you mean?  What are the symptoms?  Have you identified exactly where it gets stuck?
  @laouer I'll send PR for validation loss later today.
 Cool, let me know if this fixes the issue fully.

I'm going to add more flexible validation evaluation API in next few weeks - so there will be easier to log bunch of metrics.
 With the fix of #2487 I'll add a property to the `ValidationMonitor` to get best value. Currently you need to rerun `evaluate` again to get the value.
  @yuanbyu - any ideas?
 You need to move the count_up_to op inside the conditional branch you want it to be executed.

```
a = tf.Variable(0)

def todo_if_true():
  incr = a.count_up_to(1)
  with tf.control_dependencies([incr]):
    return tf.identity(a)
def todo_if_false():
  return tf.identity(a)

g = tf.cond(tf.constant(False), todo_if_true, todo_if_false)
```

I have added the following paragraph to the doc:

 Note that the conditional execution applies only to the operations defined in
  fn1 and fn2. Consider the following simple program:

``` python
  z = tf.mul(a, b)
  result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
```

  If x < y, the tf.add operation will be executed and tf.square
  operation will not be executed. Since z is needed for at least one
  branch of the cond, the tf.mul operation is always executed, unconditionally.
  Although this behavior is consistent with the dataflow model of TensorFlow,
  it has occasionally surprised some users who expected a lazier semantics.
  It seems to have been a deliberate stylistic choice not to define **repr** within tensorflow.  Not being a python enthusiast myself, I can't comment on the virtues of that decision.  If you feel differently, perhaps the tensorflow discussion group would be a good place to raise the issue.  [https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/discuss] 
  @leary-google any ideas why you did it the other way?
 @tensorflow-jenkins: test this please
  Thanks - this does look suspicious and I see the same behavior on my machine.  
We are investigating.
 Thanks to @prb12 for debugging this with me.
The TensorFlow runtime optimizes the graph before it runs it the first time. One particular optimization of interest here is constant folding, that replaces a node whose input is a constant, with the output of the node after executing it. The 'execution' of the node during constant folding always happens on the CPU. Note that this is functionally correct, as both the CPU and GPU kernel implementations for an op are supposed to produce the same result. To make sure your op runs on the GPU in tests (where you presumably feed in constant values as inputs), use `self.test_session` to run your test. That [disables](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L246) optimization when running the graph. Note that when you use your custom op in a larger program that has real inputs instead of constants, it will run on the GPU as expected. Closing this issue because this is the expected behavior. Please feel free to add follow up comments in case you need more info.
 Yes, test_session is just a thin wrapper around creating a normal TF session, so you can see how the code disables this optimization here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L246
  gcc 4.9.2 is what I currently use and it has worked for me.  @keveman in case he has any ideas, or maybe we have to rope in bazel devs.
 I also just tried from another machine and it still works for me (just synced to HEAD today).

On this machine, I'm using ubuntu 14.04, gcc 4.8.4 provided by distro.

If you manually run bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc, do you still see the failure?

Do you see a bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc.runfiles/third_party/gpus/cuda/lib64/libcudart.so symlink?
 Btw the reason we don't use gcc 5+ is that nvcc currently isn't compatible with it, so we're all stuck on 4.8 or 4.9 :(
 Hmm, this is probably a bazel-related problem, since the binary itself does seem to have the right linkages.  @damienmg for some help, if he has ideas.

(No, it works for me at 7.5 too, I did not have to patch my CROSSTOOL file).  To be completely honest, I'm not sure why some configurations work and others don't.
  It scans every minute (although it may take longer to finish loading the data). In master you can configure this via [a flag on the TensorBoard process](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard.py#L60) although that didn't make it into 0.8.

It would be nice to wire up the reload button to also trigger an immediate reload on the backend. But I don't have any near-term plans to implement that.

In practice, if TensorBoard seems to be scanning the log infrequently, it is more likely that this is because you are writing to multiple summary files simultaneously (ie. you have more than one SummaryWriter active at a time). TensorBoard expects that only one file is written to at a time, so if you have an early file A and a later file B, once B is ever written to, it assumes file A is finalized and will stop reading events from file A. So if most of your actvitiy is in file A it will look like TensorBoard has just stopped updating. When you kill and restart the server, it reads everything from file A before going on to file B, so the result is it loads data that was hidden to it before.

We're currently working on code to have TensorBoard detect this case and warn you in the UI, so it should become less confusing. 
 I see; that makes sense then. Your help with adding this feature would be extremely welcome :) here's an overview of how we could do it.

Overall, TensorBoard consists of a few major pieces. On the backend, there are the [`EventMultiplexer`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/python/summary/event_multiplexer.py) and [`EventAccumulator`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/python/summary/event_accumulator.py), a pair of classes which provide an API for accessing summary data from TensorFlow. 

The [`Server`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/tensorboard/backend/server.py) instantiates these classes and creates a `MultiplexerReloadingThread` which sets a timer to automatically reload the `Multiplexer`. If the `Multiplexer` is already caught-up to historical data, it will generally load very quickly, but if you just turned TensorBoard on and it has 5 gigs of historical data to chew through, it may take a while.

[`handler.py`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/tensorboard/backend/handler.py#L460) contains all the handlers for the server's routes. The API is documented in the file [http_api.md](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/tensorboard/http_api.md). Please take a moment to read that (the next paragraph will assume some familiarity).

The way reloading works is as follows: the user clicks the reload button, which calls the `reload` method that is implemented by `TF.Backend.Behavior`. The behavior first reloads the run-tag mapping, and then triggers every existing data display element (e.g. a chart or image loader) to reload.

Thus, if we want to have the reload trigger a backend reload, we should have the first method that is called on the backend (the `runs` route) delay while the backend reloads. I think a good way to implement this would be to have an optional query parameter passed to the route (?reload), and if the query is present, then rather than respond immediately, the server reloads the backend and then responds.

However, as I mentioned above, the reload may take a long time if the server just turned on and there is a lot of historical data to explore. So we'll need to have a timeout, say 5s - if the backend reload takes more than 5 seconds, then we resolve early even though we aren't done loading.

It would be really nice to further return some info on whether the backend is done loading or not so we can put a little "data still loading!" type indicator, but this change is complicated enough without doing that :)

Does that seem reasonable?
 @danmane: Any chance this is obviated by improvements since then? 
 This is still a valid request. I changed the issue name to better reflect where the conversation went, though.
  I am not famiilar with Caffe but:
1. yes, conv2d_transpose is what a lot of people call 'Deconvolution': https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#conv2d_transpose
2. Also not sure what Crop does (can't find documentation for it), but check here: https://www.tensorflow.org/versions/r0.8/api_docs/python/image.html#cropping
3. https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#softmax_cross_entropy_with_logits is our documentation for the math behind the op -- I cannot find similar mathematical description of SoftmaxWithLoss here: http://caffe.berkeleyvision.org/tutorial/loss.html, so I can only assume that they are the same.

Lastly, these are questions better asked on StackOverflow, only people looking at bugs / issues are really looking here.
 (closing -- further discussion should be on StackOverflow)
  @neverPick, the size of the "image" needs to be the same as what you are reshaping it to.
 (in other words, "reshape" is not "resize".  There are operations for resizing available in TensorFlow though.)
  There are problems with Bazel and NDK 11 currently; it's suggested that you use NDK r10e until this is resolved. See #1468 for details and links to relevant r10e downloads.

If the problem persists after trying r10e please let me know the bazel version and command you're using to build.
 I just tried with api_level=11, but was unable to reproduce the issue.

If you update tensorflow/examples/android:libtensorflow_demo.so to have the following linkopts:

```
    linkopts = [
        "-landroid",
        "-ljnigraphics",
        "-llog",
        "-lm",
        "-z defs",
    ],
```

And then run:

```
bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures
unzip bazel-bin/tensorflow/examples/android/tensorflow_demo.apk -d apk
nm --radix=d --print-size -C --size-sort apk/lib/armeabi-v7a/libtensorflow_demo.so  | grep DirectSession
```

Do you see anything?

As a potential workaround, you can try adding alwayslink=1 to tensorflow/core:android_tensorflow_lib and see if that does anything.

How are you installing it on your device?
  There is a note on the installation documentation page which says: 

NOTE: If you are upgrading from a previous installation of TensorFlow < 0.7.1, you should uninstall the previous TensorFlow and protobuf using pip uninstall first to make sure you get a clean installation of the updated protobuf dependency.
(https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#pip-installation)

When I followed these instructions, I end up with protobuf-3.0.0b2 installed...

(tensorflow.virtualenv)pbar@pbar:/tmp/addone$ pip list | grep proto
protobuf (3.0.0b2)

Is there some reason you needed to do this manually?

Could you please try this again with the final release version r0.8, (removing the release candidate first) and let me know if it fixes your problem?

Thanks,
Paul
 @martinwicke: Is there any way we can shield people against these torments, or at least generate nicer errors? 
 @aselle is there a way to find a mismatch at runtime? Maybe by comparing the size of an unordered_map returned from a function in each version of the library? If we can identify the problem we could at least die with informative last words.
 @lichan, do you think stricter symbol visibility solves this problem as well (similar to #2646)?
  ping @ilblackdragon @terrytangyuan 
  @tensorflow-jenkins test this please
 Merged. Thanks.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (assuming this is an accident).
  https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#pow ?  Comment if you think this isn't sufficient
  Did you following the steps listed here? https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#create-the-pip-package-and-install
 We are unable to reproduce this -- can you send us instructions about how to reproduce this from a fresh build / source tree?  We'll reopen if this is still a problem.
 @damienmg @kchodorow will you help us move towards the new runfiles location before you decide to release the next version of bazel officially?
  @jendap: Do you know if this is still an issue? 
 @chan1 Thanks.  Switching assignment to @caisq who knows more about the docker setup.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  I don't believe that this is the expected behavior, but I have been unable to reproduce this on my installation (admittedly using a single K40 GPU).   Since the example should only be using one of the GPUs I can't think why this might be happening.

Could you please provide me some more information about how you installed TensorFlow?
e.g. you mention 'from sources' and upgrading.  Is this a clean tree, built from source, or one of the binary distros?
 PS.  Your error when trying to import tensorflow looks like the error message I once got when I tried to run an interactive python session in the root of a tensorflow source tree.  Is this possibly what's happening?
 @ernest-tg: Does this problem persist, or did you manage to get any further information?
 So far, I have been unable to reproduce the NaN behavior building source from the current head, or using the binary distros.
 I have the exact same software versions here, but don't get the NaNs.  Perhaps this is a hardware related issue, e.g. placement of ops on multiple GPUs?  
@zheng-xq Are there any known multi-gpu issues with TF0.9/Cuda7.0/cuDNN4  ?

@weiliu620, @ethereon   Are you also running a multi-gpu config?  

@ernest-tg  Could you please try running this with the environment variable CUDA_VISIBLE_DEVICES=0 

Does the same problem happen with newer CUDA SDKs and cuDNN?

Might also be worth checking where ops are running as follows:

```
diff --git a/tensorflow/cc/tutorials/example_trainer.cc b/tensorflow/cc/tutorials/example_trainer.cc
index a465d98..02b42ab 100644
--- a/tensorflow/cc/tutorials/example_trainer.cc
+++ b/tensorflow/cc/tutorials/example_trainer.cc
@@ -93,6 +93,7 @@ string DebugString(const Tensor& x, const Tensor& y) {
 void ConcurrentSteps(const Options* opts, int session_index) {
   // Creates a session.
   SessionOptions options;
+  options.config.set_log_device_placement(true);
   std::unique_ptr<Session> session(NewSession(options));
   GraphDef def = CreateGraphDef();
   if (options.target.empty()) {
```
 > > Are there any known multi-gpu issues with TF0.9/Cuda7.0/cuDNN4 ?

Not to my knowledge yet. 
  I've never used Gradle before, but the project you linked to seems to be configured to only produce armeabi-v7a builds. I assume you'd edit the occurrences here: https://github.com/miyosuda/TensorFlowAndroidDemo/search?utf8=%E2%9C%93&q=armeabi-v7a
and add x86 and x86_64 to each.
 It looks like you'll also need to remove the ARM-only flags being passed to the compiler. There are flags that only work for building x86, and likewise some that only work for arm.

You can take a look at tensorflow.bzl and try using the argument list from there with the exception of the mfpu one.
 The only thing preventing the demo from running on API 19 is that it uses the camera2 api. If you rewrite the relevant code to use android.hardware.Camera it will run fine on older devices.

In general I haven't found emulators and cameras to get along, unfortunately. If possible I'd suggest running on a real device or altering the code to take canned images (if you're just trying to test a model).
 Closing this now as your original issue has been addressed (thanks 
@miyosuda!) and there are complete instructions for adapting the TF demo to api level < 21 in https://github.com/tensorflow/tensorflow/issues/419 now.

Emulator support for cameras is outside the scope of Tensorflow support (I'd try stackoverflow for that), but feel free to open any additional TF-specific issues you have with the demo.
  Hi,
Could you please provide a little more information:

What is the hardware configuration of your machine and what versions of the NVIDIA software do you have installed?

Thanks,
Paul
 The error sounds like "import_array" is not getting run. That's a function that sets up some global state and must be run before using numpy C API.

We used to have the following in tf_session.i

```
%include "tensorflow/python/platform/numpy.i"
%init %{
import_array();
%}
```

It looks like it's got enhanced with some logic which I don't fully understand. @girving -- do you see any scenarios where "import_array()" isn't going to run?
 @tmsimont: I'm confused by your stacktrace.  Why is scipy involved?  Is it possible to get a reproduction case that doesn't involve scipy being the culprit?
 Ah, looks like there a few tests that do touch scipy, including one broken one that requires it or fails.  I will get the culprit to fix that one, but it's unrelated to this issue.
 @vrv or @martinwicke: Do you know what version of numpy we're building against?
@chengdianxuezi, @mouendless, @fxia22: What versions of numpy do you have?

If these versions don't match, numpy might decide to crash.
 @caisq for pip build info
 It does look like an old numpy is creeping in from somewhere.
On Tue, Apr 26, 2016 at 06:48 Trevor Simonton notifications@github.com
wrote:

> I noticed something potentially relevant here:
> 
> INFO: From Compiling tensorflow/python/lib/core/py_func.cc:
> In file included from third_party/py/numpy/numpy_include/numpy/ndarraytypes.h:17
> 77:0,
>                  from third_party/py/numpy/numpy_include/numpy/ndarrayobject.h:1
> 8,
>                  from third_party/py/numpy/numpy_include/numpy/arrayobject.h:4,
>                  from tensorflow/python/lib/core/py_func.cc:19:
> third_party/py/numpy/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning:
>  #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECA
> TED_API NPY_1_7_API_VERSION" [-Wcpp]
>  #warning "Using deprecated NumPy API, disable it by " \
>   ^
> 
> There's a warning about the numpy include using deprecated NumPy API.
> 
> I tried to use #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION as it is
> used in other TensorFlow headers, but then the project fails to compile:
> 
> tensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ConvertNdarrayToTensor(PyObject_, tensorflow::Tensor_)':
> tensorflow/python/lib/core/py_func.cc:235:50: error: 'PyArrayObject' has no member named 'data'
>        memcpy(const_cast<char*>(p.data()), input->data, p.size());
>                                                   ^
> tensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::ConvertTensorToNdarray(const tensorflow::Tensor&, PyObject**)':
> tensorflow/python/lib/core/py_func.cc:330:61: error: 'PyArrayObject' has no member named 'data'
>      PyObject** out = reinterpret_cast<PyObject**>(np_array->data);
>                                                              ^
> tensorflow/python/lib/core/py_func.cc:345:22: error: 'PyArrayObject' has no member named 'data'
>      memcpy(np_array->data, p.data(), p.size());
>                       ^
> 
> Is TensorFlow trying to use two different versions of the NumPy API?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-214750702
 @tmsimont: Can you remove `py_func.cc` entirely (or comment out the whole file) and see if that fixes the problem?  `py_func` should definitely be fixed, but I'm skeptical that it's the problem here (hopefully I'm wrong!).
 I'm fixing `py_func` to not roll it's own numpy import logic now in case that is the problem.
 That's "tensorflow::ConvertTensorToNdarray(tensorflow::Tensor const&, _object**)" (used c++ filt)

So the library is being included through "py_func_lib", could you remove "py_func_lib" dependency here? "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L1010"
 https://github.com/tensorflow/tensorflow/pull/2114 fixes the `py_func` weirdness, but I still don't think it's related to the original issue.
 @caisq: Do you know what numpy version we're building against for the pip packages? 
 @kanwar2preet: Is it possible to get a stack trace from that crash by running Python inside gdb?  Also, unfortunately I didn't quite follow the set of commands that work and do not work from the above description.  Can you say which commands you mean again?
 @girving @vrv The numpy version we use for current nightly builds and 0.8 release builds are:
On Mac: 1.11.0
ON Linux (in ubuntu:14.04 Docker images): 1.8.2

1.8.2 is the version that comes with apt-get on ubuntu:14.04. See:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_deb_packages.sh#L41
 @stephenroller "blaze build -c dbg <tensorflowtarget>" to build with debug symbols
 Thanks @stephenroller: I don't like the fact that importing tf automatically imports scikit-learn.  @martinwicke: Has that ship sailed?  For a long time we were explicitly working to keep tensorflow's scientific dependencies to just numpy.

@stephenroller: Am I correct that the problem would go away if `tf.contrib.learn` explicitly imported numpy before scikit learn?   
 +Illia Polosukhin ipolosukhin@google.com

Let's disable that. It should automatically load the mock base class for
estimator, I think that's the only one we need. We already have conditional
loading there anyway (it only loads scipy if available).

That does seem like a pretty terrible bug in scipy though?

On Fri, Apr 29, 2016 at 11:45 AM Stephen Roller notifications@github.com
wrote:

> Yeah, it seems wrong that skflow is being automatically loaded. If nothing
> else, it's adding a whole bunch of unnecessary startup time to a submodule
> that may not ever be used...
> 
> However, it's explicitly not a dependency: it's wrapped in an ImportError
> that has smart fallback behavior. The fallback behavior doesn't trigger the
> segfault, as it doesn't have these cascading imports to this obscure module.
> 
> But explicitly importing numpy in tf.contrib.learn wouldn't help; numpy is
> imported nearly a dozen times before then, by scikit-learn, scipy _and_
> tf.contrib.learn.
> 
> Looking at the specfun module, it calls import_array and doesn't properly
> define a PY_ARRAY_UNIQUE_SYMBOL like all c modules are supposed to do. And
> I _think_ TensorFlow is doing things in the wrong order, either in numpy.c
> or numpy.h. The result is that you have both modules slightly misbehaving
> about import_array and overwriting some function pointers somewhere, hence
> the segfault. I'm still confirming this though.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-215845406
 Ok, I'll remove importing `sklearn` at all. And probably we can incorporate some of the functionality then into our own BaseEstimator later.
The only issue is with people checking stuff for things like `isinstance(obj, sklearn.BaseEstimator)` will stop working. I guess we are not very interested in supporting this anyway. I'll send a PR later today.
 @stephenroller: Can you expand on the fact that we're doing things in the wrong order?  I'd love to fix it if we're doing it wrong.  If it's just scikit-learn, we should probably file a bug for them as well. 
 The `setdlopenflags` is necessary to get imports of multiple Python packages using the same underlying C++ libraries to share copies of the libraries (in particular their global variables).

It's not optimal, but given that we don't want to depend on a fix to scipy: should we add an `import numpy` before we do our the pywrap import as a workaround?  If you add `import numpy` to the top of `tensorflow/python/__init__.py`, does it fix the problem?
 Also, @stephenroller: thank you so much for the detailed investigation! 
 https://github.com/tensorflow/tensorflow/pull/2173 
 @fivejjs: Are you using the version of the code from git after this bug was fixed?
 Please file a separate bug and give us more information.  Does importing numpy before tensorflow fix your problem?  If it doesn't, your issue is unrelated to this bug.  A stack trace would be ideal.
 This bug is fixed.  If you have the same problem, please try with a version of TensorFlow after the fix.  If it is a different problem, please file a separate bug.
 @gladys0313: I believe we haven't released a version since this fix.  @martinwicke: Is that correct?  
 Once before I have also encountered this problem. In my case it's related to the readline package. I reinstalled it from the readline-0.6.0.tar.gz. Then recompile the Python-2.7, then the error is gone. Maybe you can have a try.
 We have not released since so unless you're using a nightly or building
from source it's not surprising it's still an issue.
On Sun, May 22, 2016 at 09:36 zszhong notifications@github.com wrote:

> Once before I have also encountered this problem. In my case it's related
> to the readline package. I reinstalled it from the readline-0.6.0.tar.gz.
> Then recompile the Python-2.7, then the error is gone.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220842034
 @gladys0313 I'm not sure if it is because of readline. But when I reinstall the readline-6.0.0 and use it to recompile the python. The error is gone. I didn't do anything else. Thus I doubt it is related to the readline. In my case, the error is when I run `python` in a terminal, it outputs `Segment fault`. And when I recompiled it, it can run OK. I use readline-6.0, did you try readline-6.0? not readline-6.3. In my case, reahat 6.5, gcc-4.8.4, python 2.7, readline-6.0
 @gladys0313 , I can't remember the details. Maybe after some googling, and got some hints. And then I tried, then it worked. 
 Probably because that updated scipy or numpy as a side effect.
On Sat, May 28, 2016 at 20:59 davebs notifications@github.com wrote:

> In my case, _pip install scikits.ndimage_ fixed it.
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-222341508,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_Y81C43-2fWqec_YERAKYwrI2jPvks5qGQ8agaJpZM4ILdja
> .
 For anyone who finds this thread: TensorFlow 0.9 has the workaround.
 @amineHorseman If importing numpy doesn't fix it, please file a different issue.  It is probably unrelated, unless you have reason to believe otherwise.
  To use cudnn 5 you have to build from sources -- cudnn5 is not binary compatible with cudnn4, and our binaries use the latest official cudnn release (which is 4).
 https://github.com/tensorflow/tensorflow/pull/2065 ?
  "Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version" from your logs sounds like a problem -- try to upgrade your cuda driver?
 (Or consider compiling from sources if you want to use an earlier CUDA runtime)
  It's on our TODO list. Just trying to figure out how to do it nicely to have a general way to pass hyper-parameters into the models.
  @tensorflow-jenkins : test this please
 googlebot where are you?
 @googlebot, wakey wakey

@willnorris in case you know why CLA check is not triggering anymore.
 Actually I'm an admin so I can merge it forcefully -- I'd be happy to wait to get the official seal if you think it's for some reason better.
 We're already merging things we know are kosher (PRs auto-created from internal CLs, cherry-picks from another branch, ...), so I think there's no reason to wait if clabot has trouble re-checking this particular PR.
 Done.
  You can do equality constraints by adding up gradients from 2 variables.
IE, initialize them the same and do a+=(agrad+bgrad); b+=(agrad+bgrad)
 Closing due to lack of details/activity
  @tensorflow-jenkins: test this please
 @tensorflow-jenkins test this please
  @tensorflow-jenkins: test this please
  @davidzchen in case he knows what's going on here (or knows someone who might)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 We probably put it in because on managed corporate machines the sudo can be necessary. Let me test on a clean machine. I'm all for removing it if it works for people in general. 
 Actually we left `sudo` off in our initial docs, and many of our users who weren't installing in virtualenvs kept running into permissions problems, so we added `sudo` and the ones who knew what they were doing left it off in virtualenvs ;).
 We already have no sudo in the virtualenv section of the docs. Sadly, as @vrv has said, in the non-virtualenv case, it is terribly common to require either a targeted chmod/chown or sudo. 

I'd rather not go back to fixing people's permissions. :(
 Well "personally" is a little harsh -- nobody has yelled at me on the street about this yet. But it is a support burden we'd rather not go back to. I'm sad about it too.

I'll close this sad PR. 
  We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 We kept the images out of the repo, they are instead served from tensorflow.org. We keep the markdown in github so you can fix things, but it's not meant to be viewed on github.
  @gilberthendry 
  This looks like the same issue as #1965, so I'm closing this issue as a duplicate. (On the other thread, somebody suggested upgrading the `six` library to fix this problem.)
 That's a pity - can you comment on the other issue with the version of Six you're using, to help narrow down the cause (since the underlying error is the same).
  Thankyou for the detailed bug report and repro instructions!  

We are looking at a similar bug report internally, and will update this post once we know more.
Thanks,
Paul 
 @prb12, @benoitsteiner: Any updates?  
  @jcyk, @arlejeun: Does updating to a more recent version fix this for you?
 @kadrach: I'm going to close since the original problem seems to have been fixed, but please open a separate issue if you have a different extant problem. 
  Different parts of TensorFlow treat them differently. Float computations (usually?)
propagate them. Int conversion treats them as 0. Int computations fail with Python parts of TensorFlow often raise an error on "NaN", ie, trying to add a NaN summary to histogram will fail with Python
exception.

a=tf.constant(1.0)
b = tf.constant(0.0)
c = a/b \* 0          # nan
d = c+1             # nan+1 = nan
e = tf.to_int32(d)  # int(nan) = 0
f = c - c           # nan - nan = nan
sess = create_session()
print sess.run(c)
print sess.run(d)
print sess.run(e)
print sess.run(f)

On Mon, Apr 18, 2016 at 8:20 PM, zhang8473 notifications@github.com wrote:

> 1. raise error
> 2. treat them as empty: I would say this is the proper way but the
>    developers have to some math works to let the codes work
> 3. treat them as zero
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/2013
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 conflicts need to be resolved, I think the failures are infra failures, we can ignore them.
 friendly ping to update conflicts 
  Hi, this is probably a better question for either the discussion mailing list or StackOverflow -- can you please repost at one of those [venues](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community-)?
  Shape and type are different things.
 The wording isn't great. However, this is an autogenerated file. You'd have to modify the source of this doc which is the docstring of the function in question.
  You can get the sources from github (check out the code at the r0.8 branch if you're using the r0.8 wheel).  The models code aren't really core libraries, just examples.  I'd like to remove mnist/cifar10 from the pip wheel too.  Unless someone can convince me otherwise, it's weird to me to have models be part of the pip installed package.
 +1 to @vrv. I think part of the reason it was "convenient" to have this is to paper over import issues for the various `input_data` modules. We could consider moving these to `tf.contrib` (or shoehorn them into skflow's dataset support).
  @ebrevdo has worked on benchmarking tools in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/benchmark.py

We use this internally to catch regressions because we log them to internal databases.  We'd like to have this also in the OSS world too, but we haven't figured out all the tooling yet.
 The main blocker I see now is a way to write the [TestResults](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/test_log.proto) protos we generate into a BigQuery table; which means being able to automatically generate and update the table's structure when the TestResults proto is updated.

This tool could be a start, but I'm unaware of who is maintaining it and it doesn't support proto3 format (which TestReport is):

https://github.com/GoogleCloudPlatform/protoc-gen-bq-schema

Anyone interested in getting this working?  I don't have the bandwidth...  marking as contributions welcome.
 @caisq  feel free to remove self; just wanted you on the participants list so you see relevant updates here.
  @tensorflow-jenkins test this please test
  Very cool!  I'll let Eugene look at this in detail but:

1) Tests need to be written.  See how the other LSTM composition ops are tested and you'll get a sense of what we require

2) Documentation: none of your code or methods have any documentation.

3) I don't see any .cu.cc files, so I'm not sure how anything is running on the GPU -- are you sure you haven't missed including a bunch of files?
 4) It would be nice if this interface works well with the cudnn LSTM interface in CuDNN r5.  What would be needed to get that working? 
 Also, let us know how we can help you to get bazel to work -- it's going to be a painful process if you're unable to test  locally.
 Yes, they can and should :)
 You should just be able to say `bazel test //tensorflow/...` anywhere inside the code tree and have the tests run. What's the errors you're seeing?
 That would be nice, although that particular interface may not be eternally unchanging, @ebrevdo, what's the last status on the layers/RNN work?

I'd also like to understand what @vrv asked -- if we wanted to use cuDNN5's LSTM functionality, how hard would that be from where you are now?
 I'm tempted to replace the current BasicLSTMCell with this implementation; so long as you can get unit tests that show that the results are numerically identical w/in machine epsilon for fw and bprop.

One thing to keep in mind: we are considering adding back support for split states, where state_size() is allowed to return a tuple; and in this case, each **call** is allowed to accept and return a tuple of states.
 If we replace the implementation with a monolithic op, it'll be under the covers, not exposed directly.
 That would not be appropriate to implement in the Blocks C++ ops; as this
contains enough complexity in one fused op that it's already a stretch to
get it merged into core TF.

On Wed, Apr 20, 2016 at 1:14 AM, Dr. Kashif Rasul notifications@github.com
wrote:

> @wchan https://github.com/wchan would it make sense to add the batch
> normalization option to your blocks see: #1736
> https://github.com/tensorflow/tensorflow/issues/1736 ? Would love to
> try it out together!
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-212317863
 We'd like to make sure this is compatible with cudnn's LSTM implementation and API, but that will take some time to test and validate, and this is an awesome op to have.  So in the meantime, can you move this code to tensorflow/contrib/rnn/ ?

See some of the other examples of custom C++ ops in contrib/ for how to pattern your files and tests and imports.   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/linear_optimizer is one example.

Thanks!
 From an interface point of view, it's not totally independent.  Like I said, we'd be happy to get this in first via contrib/.
 You can provide patches!  Yes, you can write a custom op that does this and add it to tensorflow/contrib like I mentioned above, and we'd be happy to add it.
 Sorry, I'll be away for a while.  Assigning @vrv to overlook the rest of the review (or triage reassignment).
 I'd like all the code to be in contrib for now and we'll later do the work of moving into the core.
 You may be missing a step from the [instructions here](https://www.tensorflow.org/versions/r0.8/how_tos/adding_an_op/index.html).
 @keveman load_library strikes again, help?
 @wchan I noticed that `nvcc (7.5)` was taking a really long time to compile `lstm_ops_gpu.cu.cc`. I don't have enough cycles today to do a detailed breakdown, but do you mind reporting the compilation time for that file?
 Yep, that's good.

On Thu, Apr 28, 2016, 10:57 PM William Chan notifications@github.com
wrote:

> @keveman https://github.com/keveman , FYI, had to add
> 
> copts = if_cuda(["-DGOOGLE_CUDA=1"]),
> 
> to the cc_binary in tensorflow.bzl or else the GPU kernels wouldn't
> register. is that correct, or should i move the register code into the
> .cu.cc file?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-215632593
 Perhaps it's worthwhile to not have the parallel_dw option?  Is it much
higher memory consumption?

On Fri, May 6, 2016 at 2:43 PM, William Chan notifications@github.com
wrote:

> OK... New tests+benchmarks added! Rewrote the op to allow more
> parallelism. Compared earlier version, it now uses more memory but is
> faster. Simplified the kernel code quite a bit too.
> 
> When "parallel_dw == True" (option in the op), its pretty much faster
> across the board on an variety of minibatches / cell_size configurations.
> On certain small (but common) minibatch sizes (i.e., 32/64), we can get
> double the speed. WOOT!
> 
> See below for benchmarks (benchmarks shamelessly mirrored from existing
> python/kernel_tests/rnn_test.py).
> 
> Calculation: Static Unroll with Basic LSTM vs. Block LSTM
> batch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)
> 512 50 512 False True 1.608925 1.595009 0.991351
> 512 50 512 False False 1.622124 1.652539 1.018750
> 512 50 512 True True 0.105598 0.100758 0.954168
> 512 50 512 True False 0.105723 0.110407 1.044307
> 512 50 256 False True 0.570323 0.557588 0.977670
> 512 50 256 False False 0.548141 0.565626 1.031899
> 512 50 256 True True 0.051169 0.041841 0.817712
> 512 50 256 True False 0.049043 0.049948 1.018455
> 512 50 128 False True 0.255528 0.223692 0.875412
> 512 50 128 False False 0.265746 0.230062 0.865724
> 512 50 128 True True 0.035591 0.025530 0.717325
> 512 50 128 True False 0.039220 0.030153 0.768819
> 256 50 512 False True 0.914745 0.912568 0.997620
> 256 50 512 False False 0.898728 0.957362 1.065242
> 256 50 512 True True 0.065084 0.061613 0.946667
> 256 50 512 True False 0.065379 0.066195 1.012489
> 256 50 256 False True 0.345520 0.321437 0.930302
> 256 50 256 False False 0.339179 0.331827 0.978325
> 256 50 256 True True 0.045273 0.030890 0.682309
> 256 50 256 True False 0.043981 0.035227 0.800952
> 256 50 128 False True 0.189186 0.148082 0.782732
> 256 50 128 False False 0.193971 0.150826 0.777567
> 256 50 128 True True 0.035081 0.022858 0.651583
> 256 50 128 True False 0.039035 0.024608 0.630408
> 128 50 512 False True 0.571939 0.545305 0.953431
> 128 50 512 False False 0.554707 0.597802 1.077690
> 128 50 512 True True 0.052550 0.046426 0.883464
> 128 50 512 True False 0.053724 0.049175 0.915331
> 128 50 256 False True 0.235361 0.207240 0.880520
> 128 50 256 False False 0.231161 0.224550 0.971402
> 128 50 256 True True 0.037826 0.027088 0.716106
> 128 50 256 True False 0.034500 0.028032 0.812499
> 128 50 128 False True 0.153420 0.112691 0.734525
> 128 50 128 False False 0.139047 0.134652 0.968391
> 128 50 128 True True 0.042392 0.023966 0.565348
> 128 50 128 True False 0.041555 0.021308 0.512771
> 64 50 512 False True 0.404943 0.384559 0.949663
> 64 50 512 False False 0.403185 0.450789 1.118070
> 64 50 512 True True 0.048875 0.037370 0.764596
> 64 50 512 True False 0.048246 0.038403 0.795976
> 64 50 256 False True 0.179140 0.155737 0.869363
> 64 50 256 False False 0.172691 0.176327 1.021057
> 64 50 256 True True 0.036864 0.024936 0.676441
> 64 50 256 True False 0.036052 0.024993 0.693251
> 64 50 128 False True 0.147388 0.122999 0.834523
> 64 50 128 False False 0.123700 0.122942 0.993868
> 64 50 128 True True 0.039172 0.020994 0.535938
> 64 50 128 True False 0.037731 0.018184 0.481955
> 32 50 512 False True 0.321763 0.303199 0.942308
> 32 50 512 False False 0.313588 0.384228 1.225262
> 32 50 512 True True 0.039348 0.032741 0.832085
> 32 50 512 True False 0.041097 0.033916 0.825274
> 32 50 256 False True 0.156073 0.135813 0.870190
> 32 50 256 False False 0.155621 0.171671 1.103136
> 32 50 256 True True 0.040236 0.025395 0.631153
> 32 50 256 True False 0.049391 0.024777 0.501641
> 32 50 128 False True 0.144160 0.111829 0.775730
> 32 50 128 False False 0.136285 0.118303 0.868056
> 32 50 128 True True 0.045720 0.024474 0.535300
> 32 50 128 True False 0.046344 0.023173 0.500020
> 16 50 512 False True 0.264227 0.244503 0.925353
> 16 50 512 False False 0.267688 0.349579 1.305921
> 16 50 512 True True 0.045465 0.031925 0.702196
> 16 50 512 True False 0.041305 0.032435 0.785265
> 16 50 256 False True 0.155288 0.120286 0.774599
> 16 50 256 False False 0.142004 0.181369 1.277210
> 16 50 256 True True 0.035678 0.025734 0.721294
> 16 50 256 True False 0.041158 0.024548 0.596429
> 16 50 128 False True 0.146916 0.117762 0.801561
> 16 50 128 False False 0.108035 0.122011 1.129364
> 16 50 128 True True 0.040217 0.024973 0.620957
> 16 50 128 True False 0.036523 0.022020 0.602924
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217565864
 (i.e., always have it "True") via the split ops

On Fri, May 6, 2016 at 2:47 PM, Eugene Brevdo ebrevdo@gmail.com wrote:

> Perhaps it's worthwhile to not have the parallel_dw option?  Is it much
> higher memory consumption?
> 
> On Fri, May 6, 2016 at 2:43 PM, William Chan notifications@github.com
> wrote:
> 
> > OK... New tests+benchmarks added! Rewrote the op to allow more
> > parallelism. Compared earlier version, it now uses more memory but is
> > faster. Simplified the kernel code quite a bit too.
> > 
> > When "parallel_dw == True" (option in the op), its pretty much faster
> > across the board on an variety of minibatches / cell_size configurations.
> > On certain small (but common) minibatch sizes (i.e., 32/64), we can get
> > double the speed. WOOT!
> > 
> > See below for benchmarks (benchmarks shamelessly mirrored from existing
> > python/kernel_tests/rnn_test.py).
> > 
> > Calculation: Static Unroll with Basic LSTM vs. Block LSTM
> > batch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)
> > 512 50 512 False True 1.608925 1.595009 0.991351
> > 512 50 512 False False 1.622124 1.652539 1.018750
> > 512 50 512 True True 0.105598 0.100758 0.954168
> > 512 50 512 True False 0.105723 0.110407 1.044307
> > 512 50 256 False True 0.570323 0.557588 0.977670
> > 512 50 256 False False 0.548141 0.565626 1.031899
> > 512 50 256 True True 0.051169 0.041841 0.817712
> > 512 50 256 True False 0.049043 0.049948 1.018455
> > 512 50 128 False True 0.255528 0.223692 0.875412
> > 512 50 128 False False 0.265746 0.230062 0.865724
> > 512 50 128 True True 0.035591 0.025530 0.717325
> > 512 50 128 True False 0.039220 0.030153 0.768819
> > 256 50 512 False True 0.914745 0.912568 0.997620
> > 256 50 512 False False 0.898728 0.957362 1.065242
> > 256 50 512 True True 0.065084 0.061613 0.946667
> > 256 50 512 True False 0.065379 0.066195 1.012489
> > 256 50 256 False True 0.345520 0.321437 0.930302
> > 256 50 256 False False 0.339179 0.331827 0.978325
> > 256 50 256 True True 0.045273 0.030890 0.682309
> > 256 50 256 True False 0.043981 0.035227 0.800952
> > 256 50 128 False True 0.189186 0.148082 0.782732
> > 256 50 128 False False 0.193971 0.150826 0.777567
> > 256 50 128 True True 0.035081 0.022858 0.651583
> > 256 50 128 True False 0.039035 0.024608 0.630408
> > 128 50 512 False True 0.571939 0.545305 0.953431
> > 128 50 512 False False 0.554707 0.597802 1.077690
> > 128 50 512 True True 0.052550 0.046426 0.883464
> > 128 50 512 True False 0.053724 0.049175 0.915331
> > 128 50 256 False True 0.235361 0.207240 0.880520
> > 128 50 256 False False 0.231161 0.224550 0.971402
> > 128 50 256 True True 0.037826 0.027088 0.716106
> > 128 50 256 True False 0.034500 0.028032 0.812499
> > 128 50 128 False True 0.153420 0.112691 0.734525
> > 128 50 128 False False 0.139047 0.134652 0.968391
> > 128 50 128 True True 0.042392 0.023966 0.565348
> > 128 50 128 True False 0.041555 0.021308 0.512771
> > 64 50 512 False True 0.404943 0.384559 0.949663
> > 64 50 512 False False 0.403185 0.450789 1.118070
> > 64 50 512 True True 0.048875 0.037370 0.764596
> > 64 50 512 True False 0.048246 0.038403 0.795976
> > 64 50 256 False True 0.179140 0.155737 0.869363
> > 64 50 256 False False 0.172691 0.176327 1.021057
> > 64 50 256 True True 0.036864 0.024936 0.676441
> > 64 50 256 True False 0.036052 0.024993 0.693251
> > 64 50 128 False True 0.147388 0.122999 0.834523
> > 64 50 128 False False 0.123700 0.122942 0.993868
> > 64 50 128 True True 0.039172 0.020994 0.535938
> > 64 50 128 True False 0.037731 0.018184 0.481955
> > 32 50 512 False True 0.321763 0.303199 0.942308
> > 32 50 512 False False 0.313588 0.384228 1.225262
> > 32 50 512 True True 0.039348 0.032741 0.832085
> > 32 50 512 True False 0.041097 0.033916 0.825274
> > 32 50 256 False True 0.156073 0.135813 0.870190
> > 32 50 256 False False 0.155621 0.171671 1.103136
> > 32 50 256 True True 0.040236 0.025395 0.631153
> > 32 50 256 True False 0.049391 0.024777 0.501641
> > 32 50 128 False True 0.144160 0.111829 0.775730
> > 32 50 128 False False 0.136285 0.118303 0.868056
> > 32 50 128 True True 0.045720 0.024474 0.535300
> > 32 50 128 True False 0.046344 0.023173 0.500020
> > 16 50 512 False True 0.264227 0.244503 0.925353
> > 16 50 512 False False 0.267688 0.349579 1.305921
> > 16 50 512 True True 0.045465 0.031925 0.702196
> > 16 50 512 True False 0.041305 0.032435 0.785265
> > 16 50 256 False True 0.155288 0.120286 0.774599
> > 16 50 256 False False 0.142004 0.181369 1.277210
> > 16 50 256 True True 0.035678 0.025734 0.721294
> > 16 50 256 True False 0.041158 0.024548 0.596429
> > 16 50 128 False True 0.146916 0.117762 0.801561
> > 16 50 128 False False 0.108035 0.122011 1.129364
> > 16 50 128 True True 0.040217 0.024973 0.620957
> > 16 50 128 True False 0.036523 0.022020 0.602924
> > 
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly or view it on GitHub
> > https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217565864
 Should be fine.  Thought you were using cublas for contractions?  @benoitsteiner @rmlarsen 
 @LeavesBreathe even if we do it, i'd like it in a separate pull request, not this one.
 What's the current status of this?  ready to review and test and merge?  
 Okay, once Eugene gives the LGTM we'll test and merge.
 A few more comments.
 Jenkins, test this please.
 Looks like tests are failing..
 Jenkins, test this please.
 FYI we are also working on cudnn rnn API. It's a bit more tricky than it
looks.

On Thu, May 12, 2016, 12:41 AM Fabrizio Milo notifications@github.com
wrote:

> @wchan https://github.com/wchan
> - yes that is actually true, I thought that you could create the Back
>   Operator from the Forward Operator and thus passing some reference from it.
>   My work in progress code is in my branch feature/cudnn-rnn-lstm
>   https://github.com/Mistobaan/tensorflow/tree/feature/cudnn-rnn-lstm
> - yes you need to be whitelisted to actually run the testing.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-218683138
 @martinwicke we could use some help with the issue @wchan is experiencing above.
 @Mistobaan i don't think so - @zheng-xq for context.
 @caisq any ideas why there would be any difference here?
 What's the status of this?  What is blocking this  cc @wchan @ebrevdo @caisq 
 marking this as 'awaiting response' since I don't know what's holding up progress on this
 Closing due to inactivity / apparent lack of interest
 William is working on this internally.
On Jun 7, 2016 5:59 PM, "Vijay Vasudevan" notifications@github.com wrote:

> Closed #2002 https://github.com/tensorflow/tensorflow/pull/2002.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/2002#event-685164337, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABtimzX8zGVJPDNsPQZIkqXBN6njngvjks5qJhPggaJpZM4IJMkM
> .
  All of those benchmark numbers are quite stale -- things should be much faster, though there's still room for improvement.  If you have a specific benchmark with up to date numbers, we can definitely take a look!
  @kashif: no longer, we update the ops docs ourselves now.
 @concretevitamin: Are you working on a out-of-place version of scatter?  I'd prefer that confusion matrix was implemented as a Python wrapper on top of something more fundamental like that.
 @girving: not sure if these are exactly what you want, but we have a [ScatterNdAdd functor](https://github.com/tensorflow/tensorflow/blob/cc9bfbf8ef4a3dea6514ad939d238f7442188247/tensorflow/core/kernels/sparse_tensor_dense_add_op.h) and a [SparseTensorDenseAddOp](https://github.com/tensorflow/tensorflow/blob/cc9bfbf8ef4a3dea6514ad939d238f7442188247/tensorflow/core/ops/sparse_ops.cc#L393) now. Will they work for this purpose?
 The functor looks good; `SparseTensorDenseAdd` isn't quite the right since you have to pass zeros but otherwise works.  For now, let's implement the confusion matrix on top of `SparseTensorDenseAdd` in Python, maybe with a TODO to use something better if it comes around.
 Would you be interested in modifying your PR instead?  I think it's just a few lines of Python.  I would call it `confusion_matrix`, but otherwise the interface is good.
 @lucasmoura: Your original interpretation was correct: it's better to remove the C++ implementation and just have the few lines of Python. 
 Here's an alternative implementation (details such as name skipped):

```
def confusion_matrix(predictions, target, num_classes=None):
  if num_classes is None:
    num_classes = tf.max(tf.reduce_max(predictions), tf.reduce_max(target)) + 1
  shape = tf.pack([num_classes, num_classes])
  sparse = tf.SparseTensor(values=1, indices=tf.transpose(tf.pack([predictions, target])), shape=shape)
  return tf.sparse_add(tf.zeros(shape, dtype=tf.int32), sparse)
```

It probably has a few bugs, but hopefully it's close and conveys the general idea.
 @lucasmoura could you paste the errors somewhere?  `tf.sparse_add()` should support both S+S and S+D.
 Yeah, in your snippet both operands are passed as dense Tensors.  Geoffrey's snippet actually does Dense + Sparse. 
 Apologies: I had a broken version first and then edited it to be more
correct.

On Thu, Apr 21, 2016, 5:50 PM Zongheng Yang notifications@github.com
wrote:

> Yeah, in your snippet both operands are passed as dense Tensors.
> Geoffrey's snippet actually does Dense + Sparse.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1999#issuecomment-213184911
 Thanks!  Can you merge the two commits into one?  Tests for new functionality should be added in the same commit as the functionality.
 Excellent!  The only code comment is the names, but you may want to rerun and update the tests.  Then I'll test and merge.
 Jenkins, test this please.
 Ug, build timed out.  @martinwicke: Should I just retry?
 Yes, try again.

On Fri, Apr 22, 2016 at 1:57 PM Geoffrey Irving notifications@github.com
wrote:

> Ug, build timed out. @martinwicke https://github.com/martinwicke:
> Should I just retry?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1999#issuecomment-213586047
 Jenkins, test this please yet again.
 Sorry, slow start today.  Unfortunately it looks like the extra rebase destroyed the link to the tests, so I'll rerun them again. :/
 Jenkins, test this please.
 Thanks!  I don't think that failure is related.
  Can you try reducing your learning rate? If your original optimization was close to being numerically unstable, then a small numerical change in the upgrade could cause training to diverge consistently.
 I guess the issue is whether this is due to a bug, or part of normal training procedure. Accuracy suddenly falling to zero is a common occurrence in neural network training. Typically it happens when your neural network weights overflow or underflow. Can you see what the norm of your parameters looks like over time? Also, another suggestion is to try retuning your hyper-parameters. (ie, trying different learning rate, training algorithm, etc) There might be some numeric difference between 0.7 and 0.8, but it's not clear which one is the "correct" one. 
 It's common to get `NaN` during training. For instance you have an expression `y_*tf.log(y2)`. If `y2` is 0 and `y_` is 0, then `tf_log(y2)` is -infinity, and 0*(-infinity) is NaN.
 One workaround is to do `tf.log(y2+1e-8)` instead of `tf.log(y2)`
  I'm agnostic as to whether this would be better as a standalone repository, or integrated into somewhere like `tf.contrib`. One of the concerns is that there might be version skew between TensorFlow `HEAD` and an external repository: although we're trying our best to keep the API stable, the distributed runtime libraries are pretty new, and we might want want to change them between releases, so having it local might be better. On the other hand, I'm not sure how easy it would be to add Mesos to our test matrix, and I don't want to put our testing team on the hook for that.

Hopefully the integration can be relatively simple, and exist as a set of Python scripts somewhere (though I don't have enough experience with Mesos to say). There might be some changes required in the core, so I'll be watching this thread, and prepared to respond to feature requests.
 @bhack @windreamer: We'd be delighted to see TensorFlow working on Mesos (and YARN, and other cluster managers). I don't know anything about how `tfmesos` is structured, but if it can exist as external code, that would be best. It shouldn't be necessary to add a dependency from the core `tensorflow` module to `pymesos` or any other cluster manager, unless I'm missing something. We could investigate putting things in `tensorflow.contrib` if the dependency were optional.
  Which TensorFlow version/Scipy version? Is there more debugging information? (ie, which line of scipy fails). I tried on last month's Anaconda install of scipy/tensorflow and it seems to work for me
 @keveman: I'm almost certain this has the same root cause as #1924.
 @keveman: Assigning this to you, but feel free to combine with the related bugs.
 Fixed with 5405394
  @zheng-xq: thoughts?  I know we don't do padding exactly the same as other frameworks, so I'm not entirely sure what to do here.
 To me, the right thing to do seems to be:

```
// keep pad_needed_height the unchanged.  
     int pad_needed_height =
          (*new_height - 1) * row_stride + filter_height - in_height;

// if the padding is negative, truncate it to zero. 
      if (pad_needed_height < 0) {
        pad_needed_height = 0;
      }

// The same goes to "pad_needed_width"
```
 I've read the example more carefully, and I still like a clipping-based approach. 

If R=C=5, K=2, S=3. R'=C'=2
The clipping method:
pad_needed_width = (R' - 1)S + K - R = 1*3 + 2 - 5 = 0

The proposed method:
pad_needed_width = (R' - 1)S + max(S,K) - R = 1*3 + 3 - 5 = 1

I would say the clipping method is better. 

The weird thing TF does with R=C=4 can also be attributed to the fact TF prefers adding an extra padding on the right. If it used the other way around, a few corner cases would be slightly easier to handle and matches Theano in that case. But at this point, too many models have already been trained this way. And this benefit seems small to make a global switch. 
 Awesome, thanks for helping us work through this!.  @tensorflow-jenkins: test this please
  The pip wheel contains the python version in its name (cp34-cp34m). If you download the whl file and rename it to say py3-none instead, it should work. Can you try that?
 We want to add more platforms to our build, so we have a properly named whl for everyone. For now, this is going to be it. I'll close this issue, sorry for the trouble.
 Ah, thank you. I did indeed not understand properly. Can you take a look at #2032, would that fix the problem (at least temporarily, until we have whl files for 3.5)?
 You can rename the wheel file as pointed out in the installation docs. Sorry for the inconvenience.
 We do not currently support a Windows binary and I am not sure whether that
would work. I heard rumors that it is possible to build TensorFlow on
Windows 10, but I haven't done it myself.
On Wed, May 11, 2016 at 20:39 stationedabroad notifications@github.com
wrote:

> Hi - does anyone know what the path is to download using anaconda for a
> windows machine? is it the same one as for linux?
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1990#issuecomment-218651487
 There is currently no way to install TensorFlow from a binary package for
Windows.

On Wed, May 11, 2016 at 10:53 PM stationedabroad notifications@github.com
wrote:

> thanks for reply. I meant I am using anaconda on windows - i followed all
> advice above, saving the fie locally even - but thought since the set up
> files have 'linux' in the name they would not work regardless. So is there
> no way to install tensorflow on windows through anaconda? the message i get
> when trying the above when using pip and the copied file as
> tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl is:
> 
> "tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl is not a supported wheel on
> this platform."
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1990#issuecomment-218666518
 With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: 
https://github.com/tensorflow/tensorflow/
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Thanks for the contribution: We can't look at or accept this until the CLA is signed -- please close if you are unable to sign!
 CLAs look good, thanks!

<!-- ok -->
  You replaced x with Print(x,...), and then you fed x in your eval, replacing the result of Print and going directly into reshape.

Try:

```
x = tf.placeholder(...)
x_print = tf.Print(x, [x], ...)
i = tf.reshape(x_print, ...)
```

So that when you feed 'x', x_print is still run.
  scan is defined in terms of while_loop.  So I will try to answer your question using while_loop. 

while_loop implements non-strict semantics.  An iteration can start as soon as one of the ops for this iteration is ready (i.e., all its inputs are available.) for execution.  So a while_loop can easily have multiple iterations running in parallel.  For example, for scan, even if the accumulated value is not available in a step, the step can still start and execute any ops that don't depend on the accumulated value. One problem to allow multiple iterations to run in parallel is resource management. parallel_iterations is introduced to give users some control of memory consumption and execution order. 

For _correct_ programs, a while_loop should compute the same value for any value of parallel_iterations >= 1. 
 @yuanbyu: can you document this somewhere in control_flow_ops.py ?
 I will. But let us first see if it clears up the question(s) by lahwran@.
 Thanks. Added some more documentations.
  Yes, we don't yet support GPU on Mac.  
 Closing as dup of #491
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please.
(for auto-merge happiness even though it's just a doc change)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 These are already in master  (#1926)
  One way we could do this is to host a static graph visualizer on `tensorflow.org/tensorboard/graph-visualizer` (or similar) and by navigating to that url, you could upload a pbtxt file from your computer and visualize it.

I imagine though that you would probably like a cleaner workflow so you don't need to move these pbtxt files around yourself. What should the API look like? Maybe something like:

tensorboard.visualize_graph(graph) <- this will launch a TensorBoard server that visualizes your graph?
 Having tensorboard read pbtxts directly from the logdir is possible, although it's a material departure from the way TensorBoard is currently organized (right now it always tries to read from events files). I'm not sure if that's important enough / common enough to change TensorBoard to read the pbtxts - if you had a tensorboard.export_graph(graph) api why would you want to bother with the pbtxt files?

Although, if what you want right now is to visualize your pbtxts without needing to turn on TensorBoard, you can already do that: navigate to [https://www.tensorflow.org/tensorboard/index.html#graphs](https://www.tensorflow.org/tensorboard/index.html#graphs) and click on `Upload: Choose File` in the left sidebar.
 @ultrons, There's a way to show graph visualization in IPython notebook,  see the function "show_graph" in Alexander's deep dream [notebook](http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)
  It's still 0.8.0rc0, we changed the install instructions before we uploaded the new packages.  Sigh.

Example:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl
 Fixed in github, pushed to website.
  LGTM
 @tensorflow-jenkins test this please.
(for auto-merge happiness even though it's just a doc change)
 Jenkins, this this PLEASE!
 Jenkins? test this please?
 Man, sometimes he's really stuck up.
 It's a Friday - he probably left early to go to Yosemite.  In fairness, the falls are absolutely gorgeous this time of year.
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 test this please
 @tensorflow-jenkins test this please
  @Vlsarro You are right, it was Rst (to put it in pip easily). Though now after move inside tensorflow, to easily display on website it should be Markdown. I'll address links to actually be markdown this weekend.
  See https://github.com/tensorflow/tensorflow/issues/1787#issuecomment-210560825
  It should be easy to extend/modify `tf.rank()`, `tf.shape()`, and `tf.size()` to have them work on `SparseTensor`s.

Happy to review PRs!
 What are the errors?  I trust that the latest 0.8 docs on the website did
not help?

On Sunday, April 17, 2016, Anish Shah notifications@github.com wrote:

> @concretevitamin https://github.com/concretevitamin I'm having some
> problem with bazel-build. Not able to build from source. Is there any other
> way I can build from source?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-211090564
 @siddharth-agrawal Essentially, SparseTensor in Python can be thought of as a 3-tuple of dense tensors, `(indices, values, shape)`.  You're right that most underlying Ops take these three dense Tensors separately, but there's no "conversion" going on. 

As for `sparse_matmul()`: due to historical reasons it's actually a misnomer.  It does not operate on `SparseTensor`s, but instead uses an optimized algorithm to operate on two dense Tensors depending on the two flags.  So that op is irrelevant. 
 @siddharth-agrawal I think this can be implemented entirely in Python.  In [array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py), you could add a `def rank(...)` function with the correct signature and convention.  Inside that, if `isinstance(t, ops.SparseTensor)`, you can return `size(t.shape)`, because the number of elements in `t.shape` is the rank of `t`. 

(More explanation: right now, when `tf.rank()` is called, what really happens is [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L89). So once we override it properly, the Python frontend can handle the dispatch. )
 @siddharth-agrawal One recent example is the introduction of `tf.sparse_softmax()` [here](https://github.com/tensorflow/tensorflow/commit/7e5261b7f35871282010f655de5d72e2d2edc29f).  The Python wrapper in `sparse_ops.py` -- named `sparse_softmax()` and which accepts one input -- overrides the function `gen_sparse_ops.sparse_softmax()`, which accepts three inputs.  This is what I meant by overriding.  Does this make sense?  
 I am leaning towards making it consistent with dense Tensors, i.e. it
returns the size of the tensor.  Consistency here might be the less
surprising choice.

(Also, quick heads up I will be out this coming week.  @ebrevdo and
@rmlarsen might be good persons to look at SparseTensor-related changes!)

On Sat, Jun 4, 2016 at 11:08 PM, Siddharth Agrawal <notifications@github.com

> wrote:
> 
> @concretevitamin https://github.com/concretevitamin For tf.size(), what
> should the output be for SparseTensor? The size of the tensor or the
> number of non-zero values?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-223795234,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAkLHld0QxdDB5qSAFGmU2MU_9MeyezPks5qImfqgaJpZM4IIer2
> .
 +1 for having it return reduce_prod(sparse_tensor.shape)

On Sat, Jun 4, 2016 at 11:16 PM, Zongheng Yang notifications@github.com
wrote:

> I am leaning towards making it consistent with dense Tensors, i.e. it
> returns the size of the tensor. Consistency here might be the less
> surprising choice.
> 
> (Also, quick heads up I will be out this coming week. @ebrevdo and
> @rmlarsen might be good persons to look at SparseTensor-related changes!)
> 
> On Sat, Jun 4, 2016 at 11:08 PM, Siddharth Agrawal <
> notifications@github.com
> 
> > wrote:
> > 
> > @concretevitamin https://github.com/concretevitamin For tf.size(),
> > what
> > should the output be for SparseTensor? The size of the tensor or the
> > number of non-zero values?
> > 
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <
> > https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-223795234
> > ,
> > or mute the thread
> > <
> > https://github.com/notifications/unsubscribe/AAkLHld0QxdDB5qSAFGmU2MU_9MeyezPks5qImfqgaJpZM4IIer2
> > 
> > .
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-223795458,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtim3ryad0V6lkPzSXxdLwEdKN_K_ecks5qImm3gaJpZM4IIer2
> .
 Drive by comment to say thank you @siddharth-agrawal for all the PRs!
 @siddharth-agrawal Thanks for your contributions!
  Not with the current op interface. You would have to write a new op that takes the kernel size as an input tensor. The kernel implementation can be refactored to use most of the current max_pool implementation. 
 Closing. Feel free to reopen if this is a feature request worth tracking.
  Creating a `tf.train.ClusterSpec` from another ClusterSpec was broken,
which in turn broke creating a `tf.train.Server` from a ClusterSpec.

Fixes #1961.
Change: 119954117
 @tensorflow-jenkins test this please.
 LGTM. All PR tests passed. Will merge and build the docker images for the GRPC TF server and run the tests in dist_tests.
  FYI, saw similar issue when installing 0.8rc0 in the Travis: https://travis-ci.org/tensorflow/skflow/jobs/123599570
 cuda issue: our pip installs require libcuda 7.5  I've updated the documentation and the website to clear this up.  If you want to use a lower libcuda version, you need to install from sources.
 the NewBase one looks related to benchmark.py, assigning to @ebrevdo 
 Looking specifically at the NewBase issue: please tell me the exact version of python and six that you are using.
 specifically, run:

```
import six
print(six.__version__)
```
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L41

We require > 1.10  for six.  I'm wondering why setup.py didn't require upgrading your 'six' :(  Can you try updating?
 That won't work in the long run if your package manager / os manages six in
that directory.
On May 10, 2016 2:32 AM, "Zhang Wang" notifications@github.com wrote:

> Aha
> I have another solution:
> $ cd /usr/lib/python2.7/dist-packages
> $ sudo rm six*
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1965#issuecomment-218106367
 Seems like the issue is fixed by updating six correctly. Closing.
  btw, feel free to batch all of these changes into one commit.
  Could you please try calling save with this option:

saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)

Sherry
 @jmugan, I agree with you that the model is probably too big for your system.

@pkmital, could you please file a bug if you believe the memory leak is in TensorFlow, and not in the way your model is constructed?

Closing this bug as the fundamental issue is memory, not the saver itself.
  Oops, this is an embarrassing bug - thanks for catching it! I've got a fix in the pipeline. For now, you can make it work by changing line 35 of your `mlp_mnist_dist.py` to:

``` python
server = tf.train.Server(cluster.as_cluster_def(), job_name=FLAGS.job_name, task_index=FLAGS.task_index)
```
 @mrry Let's put the fix into 0.8.0 final release.
 @caisq: Agreed! Shall I send an individual PR with the cherrypicked commit?
 @mrry: Yes, please send the PR to branch r0.8 and we'll merge it back to master.
  I left a few comments. Thanks for writing this up! 
  batch_size may be a dynamic tensor, and we don't check to see if it's a constant of the graph when performing static shape inference.
 Closing assuming there is no real action item here.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 The important fixes were already cherry-picked into this branch, but thanks.
  Thanks!  @tensorflow-jenkins test this please
 somehow we killed jenkins, going to wait for it to restart to try again.
 test this please
  Thanks for reporting this @Remper.  Could you clarify a bit on your use cases:

1) Do you want to gather based on indices to the _first_ dimension only?  Or do you want to gather by all coordinates (i.e., similar to `gather_nd()`)?

2) How do you intend to use the gathered results?  Do you want them to form a new dense Tensor, or remain a SparseTensor with the original shape?
 @Remper: What would the implementation look like?  I'm having trouble seeing how to make this not terrifyingly slow.  I suppose you could do binary search to find the matching sparse indices? 
 @ysuematsu This seems hashtable related.  Can we do this with existing int -> int hashtable ops?
  No; that's not necessary.  It'll be done automatically on a regular basis.
 @tensorflow-jenkins test this please.
  Sorry - I missed this issue. I don't have a good idea of why this might be happening, so I'm reassigning to @zheng-xq as our best GPU expert.
 The first step is to find out where the hanging is happening. Could someone help print out the call stack of all threads when the hanging is happen, at least the unique ones? 
  If you are installing from pip packages, try installing cudnn v4 -- our pip packages require cudnn4.
  Possibly duplicate of https://github.com/tensorflow/tensorflow/issues/1923
 Fix to this issue in 0.8.0 RC0 is on the way and will be incorporate in the final 0.8.0 release. See #1926
  A similar syntactic sugar with current implementation is to do this:
`loss, accuracy = sess.run(fetches, feed_dict)`

Current approach is more familiar for people coming to TensorFlow from Theano since Theano object also returns a list rather than dictionary.

The biggest problem though is that this change is going to break thousands of people who have code using `session.run` and treating the result as a list.

It might be better to create a different interface on top of TensorFlow, like Keras, PrettyTensor, tf.train rather than altering the core interface. Ultimately most people may be using TensorFlow through one of those higher level interface anyway, rather than calling `session.run` directly
 OK, good point, it does seem mostly backward compatible. Could this be done as an object on top of TF? As you mention, one may want to extend it to tuples, nested dictionaries, or dictionaries containing lists of elements, so that's additional changes to the core TF. Also, maybe asking for "isinstance(dict)" is too restrictive, instead you might want to follow the spirit of duck-typing and accept any object that is dictionary-like

Maybe a way to go is to have a thin wrapper like "EasySession" or something like this on top, and if it gets traction, then people can switch over to using it rather than the old run interface. In general, changes to core interface may break tests in unexpected ways an require some work to get integrated. For instance, there may be some code which feeds in a `dict` into session.run already and expects it to fail (it may seem like a stretch, but I have seen similar scenarios)
 The dictionary thing sounds like a good plan.  I don't think we need to add support for all possible types.  Dict support would have prevented a crazy workaround in my current code.

@Styrke: I'd welcome a PR if you're still interested (I'll mark this as contributions welcome). 
  Can you clarify on what "dimension variable inputs" means?
 I am not aware of anyone doing exactly what you are trying. Maybe the following. Say your batch has N sentences, each sentence has up to M words, each word's embedding dimensions is K. For sentence shorter than M, pads it with a special word with a fixed embedding vector of 0s. Then, you can do batch 1D convolution on [N, M*K](per-row). Multiple the result w/ a mask of [ [1,..., 1, 1, ...0, ..., 0, 0], ...[...]], the total number of 1s and 0s on each row can be computed for each batch. 
 I'm going to close, since I don't think this is feasible in TensorFlow in a performant manner.  Everything down to cuDNN assumes rectangular tensors, and uses this assumption to arrange for high arithmetic intensity.  Fancy approaches which do manual tiling of irregularly sizes images are possible, but those would be a layer on top of tensorflow (or possibly something in `tf.contrib`).
  Can you try the pip with with `--ignore-installed`?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins , test this please.
 LGTM. All PR tests passed.
  Can you please try installing a virtualenv as described [here](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#virtualenv-installation) and let us know if you see the same problem?
  @keveman: Did you get a chance to look at this?
 @shiuan5566 What platform are you trying this on?
 I am not the best person to debug this. Throwing it over to @martinwicke so he can find someone with access to different platforms.
 I don't know what spyder is, and the fact that it runs ok in plain python makes me think this is not actually our error.

These types of errors are caused by the flag initialization code such as flags.DEFINE_string('batch_size', ...) running twice, or two different parts of the program defining flags with the same name (for self_test that would be a pretty likely candidate, batch_size not so much.
  On q1. 

Because A - B is an automatic broadcasting binary operation. A - B does not require B to be expanded to use 2.5GB. Only the result needs to be allocated (2.5GB). So, the peak memory usage in your code snippet should be 2.5GB + 500KB + 2.5GB. 

On q2.

It's been worked on. There can be multiple approaches to this issue: a) extend assign_sub() to support broadcasting; b) support inplace operationos like -=, +=, etc.
 If you replace write

sub_op = tf.assign_sub(A, B)

then the broadcasting happens implicitly in the underlying Eigen
expression, and the memory overhead should be minimal.

(Low level code is here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/dense_update_ops.h#L43
)

Rasmus

On Thu, Apr 14, 2016 at 9:44 AM, zffchen78 notifications@github.com wrote:

> On q1.
> 
> Because A - B is an automatic broadcasting binary operation. A - B does
> not require B to be expanded to use 2.5GB. Only the result needs to be
> allocated (2.5GB). So, the peak memory usage in your code snippet should be
> 2.5GB + 500KB + 2.5GB.
> 
> On q2.
> 
> It's been worked on. There can be multiple approaches to this issue: a)
> extend assign_sub() to support broadcasting; b) support inplace operationos
> like -=, +=, etc.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1934#issuecomment-210042344
 Both Rasmus and I meant your code do need > 2.5GB \* 2 memory. I was trying to correct your understanding that A - B would expand B to 2.5GB first. If your understand was that way, the peak memory would be 2.5GB \* 3.

Rasmus was saying, if your original code is changed to sub_op = tf.assign_sub(A, B) _and_ extend the code he points out, the peak memory usage in your program would be 2.5GB \* 1.
 temp2 -= A is not doing any inplace update.
 `temp2 -= A` tries to call `__isub__`, if that's not defined (it's not defined for TF tensors), Python interpreter rewrites it into `temp2 = temp2 - A` which calls `__sub__` which calls `tf.sub`
 @myme5261314 : Did this answer your questions?
  I think by context vector you mean just the state after the encoder. For example, in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/seq2seq_test.py#L34](this test) it would be the `enc_state` tensor.
  @forfish Do you still have problems compiling example_trainer?
  Looks great!  Please smash once you fix the notes above to avoid the merge conflicted commit.
 ping for update?
 @vrv, @ibab: I was waiting for the smash.  Unfortunately now we have an overlapping PR here: https://github.com/tensorflow/tensorflow/pull/2263. 
 Apologies, @ibab and @hunkim: I should have remembered the original PR when I started reviewing the new one.  Apologies also for forgetting about the precision issue.  @hunkim: Would you be okay with @ibab finishing up his PR since it handles the precision correctly? 
 Looks good!  Jenkins, test this please.
 Sweet, tests pass.  @ibab: Shall I go ahead and merge?
 I went ahead and merged.  Further changes in further PRs.  Thank you for the contribution!
  I assume you're running via bazel?  What if you just run the test manually / in isolation like

bazel-bin/tensorflow/python/math_ops_test ?
 Hm, when you set ./configure, did you specify cuda compute capability to 5.0?

I wonder if this is related to ptx-sass compilation somewhere.
  Since this looks like the same issue as #1924, let's close this issue for now and continue the discussion over there.
   Can you make another one against r0.8?
 Should happen tomorrow.
  @keveman: Could this be related to the build configuration or the `dlopen()` flags that we're using to support `tf.load_op_library()` and friends? Perhaps there's some way we could structure the libraries so that `import cv2` doesn't link against TensorFlow's internal `libjpeg` or `libpng`?
 @beniz, thanks for offering to help. What is the command line you are using for compiling your opencv_tensor binary? I doubt that the problem involving `dlopen` and the one you are facing are the same, but you never know.
 Do you mind switching around the order of the libraries? i.e., have the -ltensorflow at the beginning as follows : 

```
/usr/bin/c++   -g -Wall -Wextra -fopenmp -fPIC -I/path/to/include \
-I/path/to/tensorflow/dbuild/external/eigen_archive/eigen-eigen-4c94692de3e5 \
-I/path/to/tensorflow/dbuild/external/eigen_archive -I/path/to/tensorflow/dbuild/tensorflow \
-I/path/to/tensorflow/bazel-out/local_linux-fastbuild/genfiles/ -std=c++11 -O2 opencv_tensor.cc  \
-o opencv_tensor  \
-ltensorflow \
-Wl,-rpath,/path/to/tensorflow/dbuild/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow \
-L/path/to/lib -L/path/to/tensorflow/dbuild/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow \
-rdynamic -lopencv_core -lopencv_highgui -lopencv_imgproc -lboost_thread -lboost_system \
-lcrypto -lssl
```
  ### Environment info

Operating System: Mac OS X
Using virtualenv

Installed version of CUDA and cuDNN: None
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:
1. Which pip package you installed.
   sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0rc0-py2-none-any.whl
2. The output from python -c "import tensorflow; print(tensorflow.**version**)".
   0.8.0rc0

If installed from sources, provide the commit hash:
### Steps to reproduce
1. git clone --recurse-submodule https://github.com/tensorflow/tensorflow.git
2. cd tensorflow
3. git checkout r0.8
4. python tensorflow/examples/tutorials/mnist/mnist_with_summaries.py --summaries_dir=/tmp/summaries_1
5. tensorboard --logdir /tmp/summaries_1
6. Open http://127.0.0.1:6006 in Chrome

Notice that the CSS don't load properly. The graphs and charts don't show either. 
### What have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
If accessed from non-localhost, tensorboard prints messages like the following in console:

WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/Users/cais/venv1/lib/python2.7/site-packages/tensorflow/tensorboard/**_rPc_sWiTcH_**' on path /Users/cais/venv1/lib/python2.7/site-packages/tensorflow/tensorboard/**_rPc_sWiTcH_**
10.1.2.3 - - [13/Apr/2016 12:28:20] code 404, message Not Found
 Should be fixed by https://github.com/tensorflow/tensorflow/pull/1926 (ty for confirmation from cavedweller)
  What if the light-sources are asymmetrical, and the backplane is slanted?
 Maybe the light source doesn't have parallel rays.
 It's a very complex lighting setup, that's for sure. I think it can only be explained by participating media.
 I think this is best summarized as 'by design'. ;-)
Feel free to discuss further in one of the community forums, but I'd like to close this issue.
  Also fixes a couple of incorrect names in the code examples.

Change: 119605636
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins test this please
 we need the CLA to be signed before we can accept.  are you able to sign?
 (leave a comment if you're able to sign and we'll reopen, thanks!)
  This error is a sign that the code used to run the graph is (much) newer than the code that generated the graph.

It seems to me you must have an older TensorFlow somewhere on the system, could you check that it's clean?
 I'm not sure I'll be a lot of help in this kind of situation. 

I suggest you stud your code with assertions about the tensorflow version to make sure you're on 0.7.1 when you create the graph, and then do the same when you actually run it. If you write out a graph, you can check its supported graphdef versions too, it's the versions field in `core/framework/graph.proto`, filled by `core/public/version.h`.

You should probably also consider running one of our docker containers instead of using the installed binaries (if that is possible), or at least install into a virtualenv to get at least some isolation from the rest of the versions.

I'll close this bug, I don't believe there's an indication of a bug, but please feel free to continue commenting, I will try to help out if I can.
  What install command did you use? Try installing it in a virtualenv as described [here](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#virtualenv-installation)
 This is a pip problem.  Check out #2212 
  A quick workaround for this problem is to declare your embedding matrix (`target_embeddings`) on CPU:

``` python
with tf.device("/cpu:0"):
  target_embeddings = tf.Variable(..., name="target_embeddings")
```
 @mrry: what is the exact problem?
 The problem is (or seems to be) that the variable+initializer get assigned to GPU when the initializer runs, but the training op is CPU-only, leading to this error. Your upcoming placer changes should fix this, so hopefully it'll be no extra work! :)
  Duplicate of https://github.com/tensorflow/tensorflow/issues/1923
Fixed by #1926 
   Do we want to switch the "latest" tags to point to release candidate versions? I think it is better pointing to old stable release. But that means we should during the release candidate period edit the documentation to contain explicit tags - i.e. gcr.io/tensorflow/tensorflow:0.8.0rc0 instead of gcr.io/tensorflow/tensorflow. Then for final release we should edit the links again. Too complicated?

Anyway, this time we can switch "latest" to the RC because people reading old documentation will se b.gcr.io where the latest will still point to 0.7.1 ;-)
 @vincentvanhoucke you should probably also upload the next version of udacity container to gcr.io and update the readme accordingly.
 ``` python
import tensorflow as tf
tf.__version__
```
  Jenkins, test this please
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  This is currently by design. You can probably do something like below. 

```
with tf.control_dependencies([train_step]):
  dummy = tf.constant(0)
```

And
`final = sess.partial_run(h, [dummy])`
  `np.random.random_integers()` is deprecated and recently started returning non-`int` values. This PR fixes this so that the `tf.Tensor` indexing operator can handle non-`int` (but convertable-to-`int`) values, and removes the deprecated method from the test.
 @tensorflow-jenkins, test this please.
  It looks like there are some missing types in tensorflow/core/framework/types.h, such as uint32 and uint64. This means I can't make tensors of these types in C++.
  Simpler example

shape = tf.placeholder(tf.int32, shape=(2))
a = tf.ones(shape)
sess = tf.Session()
sess.run(tf.diag_part(a+1), feed_dict={shape:(1,1)})

ValueError: Invalid shape, shape[:mid](?,) and shape[mid:](?,) do not match
 A fix has been submitted.
  It looks like all of the links from https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html are broken. For example, https://www.tensorflow.org/versions/r0.7/api_docs/cc/classTensor.html.
 It appears that they just need to be capitalized: https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassTensor.html.
  Hi,

Thanks for submitting the PR. I just submitted similar code with gradients
for {batch_}matrix_solve, {batch_}triangular_solve, and
batch_matrix_determinant. So this is probably not necessary anymore.

Cheers,
  Rasmus

On Mon, Apr 18, 2016 at 8:58 AM, Alexander G. de G. Matthews <
notifications@github.com> wrote:

> @rmlarsen https://github.com/rmlarsen What do you think of this one?
> Would you like me to update it? I notice the linalg_grad unit test has had
> some work recently.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1891#issuecomment-211443746
 I actually didn't see your PR until after I submitted, but as you know "all great minds think alike", or the math only has one solution... or something... ;-)
  `tensorflow/core/framework/graph.pb.h` is a generated file: it should be produced in the `bazel-genfiles/tensorflow/core/framework` directory when you build TensorFlow using Bazel.

Building from source without using Bazel is not fully supported right now, but you could try adding `-I/home/sander/tensorflow/bazel-genfiles` to your `g++` arguments (assuming you have previously build TF from source).
 That should work, although the recommend workflow is still to use Bazel for building C++ targets (since it takes care of these issues).
 I mean that it would be easier to build your own program `hello_world.cpp` using Bazel as well. From your error message it looks like you are using `make all` to build it, which is unsupported, but you might be able to set up your include path appropriately.
 Ah, that makes sense! The [Bazel C++ tutorial](http://bazel.io/docs/cpp.html) covers all of the concepts that you need to know, although for simple projects you might get just as far by copying the rule for one of the TensorFlow C++ binaries ([e.g.](https://github.com/tensorflow/tensorflow/blob/cf1659d1c233f8ddbee13fd298464d76e58bdccb/tensorflow/cc/BUILD#L61)).
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I'm a googler, so I don't need to sign the CLA.
 You need to use your @google.com email address in your commit. Otherwise you need to sign the CLA.
 I signed it! (with my github email address)
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
  Hi @oweingrod,

you probably find the explanation for the slow training [here](https://github.com/tensorflow/tensorflow/blob/d4bf5e072478c92cae2bd71b96e10e77e229dfba/tensorflow/examples/image_retraining/retrain.py#L497).
It's said that bottleneck caching cannot be applied if distortions are enabled. That means the bottlenecks are calculated by the inception model for each image and each step.

Even it makes sense that the randomized distortion is applied ad-hoc when an images is processed, I wonder if it would be possible to apply the distortions before the training is executed to create more training examples. Then we are able to precompute and cache the bottlenecks again. I see one tricky problem arising: The handling of train and validation splits will be trickier.

Cheers,
Max
 @petewarden: Assigning you since it's about retraining.
  I received this on twitter:
"Can you please help update TF website to state TF-GPU needs nvidia compute cap. >= 3.5 and to install cudNN v4 (not 5). Thanks"
https://twitter.com/AlliseeSolution/status/719789250082766848

I'm not using Cuda myself, so I'm not sure of the details here, but wanted to get it logged so somebody more knowledgeable can take a look.
 @alliseesolutions: which documentation was incorrect?  https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#optional-install-cuda-gpus-on-linux appears accurate but we may have stale docs somewhere.
 Oh, our pip install for 0.7.1 was only built for cuda 3.5+, but I believe our installs for 0.8.0+ will probably support 3.0 natively.
 If you build from sources you can use both cudnnv5 and cuda compute 3.0.  We only build pips for the latest official cudnn (v4), and the nightly pip packages now are built with 3.0 support.
  It's exactly as you say! I think it's a bit too strong to call it "not real" -- the reported perplexities come from teacher-forced examples, and so are indeed not fully representative for what you'll get during decoding. But some people report this kind of perplexities in their papers and they are often a good measure to judge how the training process is progressing. In particular, I think they are enough to determine if the model is overfitting or not and whether it is converging or not. You could create a second model with the same parameters and `forward_only=True` and use that for decoding, but it'd use a lot of memory, that's why it's not done by default.

To get a final measure of the model quality, it's best to take a separate test set, decode it using the `decode` function [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L208](which uses `forward_only=True` of course) and then use any measure on the result, e.g. BLEU. I'd also recommend to use a test set that's disjoint from the dev set for that, as you might be tuning hyper-parameters on the dev set.
  @tensorflow-jenkins: test this please
 no, jenkins broke, we restarted it.  let me try again:  @tensorflow-jenkins test this please
 Jenkins keeps breaking but python3/mac tests passed so this is syntactically fine.  Merging.
  What numerical issues were you seeing, and what matrix were you differentiating around?  If the inverted matrix has bad condition number the gradients will be very unstable.
 We're happy to look at a PR.  Everything you've written sounds reasonable, so the next steps are all details. :)

The main difference between lower triangular and full here is that the determinant of a triangular matrix is the product of the diagonal entries, so you have `n` entries where a zero makes the entire matrix singular regardless of the other entries.  I'd try picking the diagonal to have random values bounded well away from zero.
 As mentioned on the PR, I actually just submitted support for backprop for  {batch_}matrix_solve, {batch_}triangular_solve, and batch_matrix_determinant.
  @tensorflow-jenkins: test this please
  To get top-N results in a sequential network you need to run a beam search. While it's not implemented in the tutorial, there were already some suggestions and code on github -- please see #654 .
  @tensorflow-jenkins test this please
 I'm killing the test jobs for now to make space for release tests (sorry!).
 I'll restart them later.
 Linux GPU PIP passed, and this was just doc changes, so I'll merge.
  @kmuriki What location did you enter for python while running `./configure`?
 @kmuriki Gentle ping. Please let me know if this is still an issue.
  Sorry, I'm away for a while.  Assigning @girving to finish the review.
 Happy to finish the review, but can you squash the commits into one?
 Thanks!  Taking a look now.
 Looks good!  Squash and I'll off the tests.  Unfortunately I have to do an errand soon, so there may be a delay.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
  Very nice, and good catch there!  Just a few small comments, and then we can test / merge.
 (@ry: tell me when I'm supposed to look at this again)
 alright then!  LGTM.  @tensorflow-jenkins: test this please
 Doh, pooling_ops_test also has a similar function for checking whether stride > ksize that you'd have to disable, though I'm now wondering whether pooling also supports stride > ksize the same way conv does.
 If pooling doesn't natively support it, it's still probably worth getting this CL in, but will require a bit more work, in that you'd have to retain the stride > ksize checking for pooling (essentially forking the python shape code that's currently shared between conv2d and pooling ops).  Let's hope Eigen / Cudnn already support this:)
 This is great!  @tensorflow-jenkins: test this please
  Scan uses TensorArray internally.  We currently don't support shape inference for TensorArray operations.  
 Fixed in HEAD.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks!
  Does http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-cp27-none-linux_x86_64.whl work? 

(difference seems to be gpu-working vs. gpu-slave)
 I think we keep moving the URL as we add more jenkins instances and forgetting to change the README :(  sorry about that
  Thanks!
 Will go  out with the next website push.
  You picked the worst file to modify. It's included a lot and building it is pretty hard on the compiler. There's not all that much we can do about this, sorry. I would guess that those 204s do not include any unnecessary steps.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Just to be clear, these are commits to 0.7 that you're trying to forward port to master?
 Ah. Well we should fix some of these properly then.
  @sherrym skflow currently uses Saver to save checkpoints - is there anyway to disable saving global path into `checkpoint` file and save local path instread?
 @nemo Actually looking [at this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L558) code, it seems like if you just specify relative path when you are saving (e.g. `est.save('my_model/')`) - it will save with relative path and should work. Can you tell if this fixes your problem?
 @nemo , could you please cut and paste your code showing how you set the save_path? I have unit tests demonstrating that relative paths work correctly.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver_test.py#L659
 @ilblackdragon, Saver.save() takes a save_path as an argument. You just need to change TensorFlowEstimator's save() in estimators/base.py to pass in save_path.
  Jenkins, test this please.
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins test this please (trying to get them all green)
  Hi Valentin :).

So it sounds like you have three processes: a client (on machine A or B? or a third machine?), and two TensorFlow servers (one on machine A and one on machine B). If you connect to machine A and all of the ops are on machine A then all of the master<->worker traffic will be local; whereas if you connect to machine B and all of the ops are on machine A then all of the master<->worker traffic will cross the network. (I'm not sure about the client<->master traffic, because you didn't mention where the client is running. You might want to try putting the client in the same process as the machine A master, as I would expect that to give the best performance. That's the configuration we typically use for distributed TF.)

In the latter case, it's possible that 30% of machine B is being consumed by proxying RPCs between the client and machine A. You might be seeing latency because the "queue runners" that prefetch input data run sequentially and now have two network round trips per record, and you're now "I/O bound" (not actually bound by the disk, but by the steps that have to run to populate the input queues). You could try adding prefetching threads to the input pipeline to see if that improves things. (I'm assuming you're using a version of [`cifar10_multi_gpu_train.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py), so no data is being fed or fetched in each training step.)

The input pipeline "protocol" is very chatty on the network, and we've been relying on most of the interactions happening in-process. Thanks for digging into this: it might give us good motivation to improve the batching etc.!
 That's surprising. Are you fetching anything in the call to `sess.run()` that you're measuring?
 Hmm, how are you determining that the operations are actually running on the GPU? The `cifar10_multi_gpu_train.py` example has `allow_soft_placement=True`, so there's a possibility that the code is being silently placed on the CPU instead of the GPU (not sure if that would account for the slowdown, as I'm not 100% familiar with the performance of that model, but it seems plausible). If you pass the flag `--log_device_placement=True`, what device does it show for the ops?
 Have you installed the GPU version on both machines? I think the placer uses information about the locally registered kernels to make placement decisions, so if you connect your session to a process that only has the CPU kernels, it won't use a GPU anywhere. (This is a bit embarrassing, and we should fix it....)
 I believe it should... if not, let me know and we'll have to build a workaround!
  Jenkins, test this please?
 @tensorflow-jenkins test this please.
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 Jenkins, test this please.
 Jenkins, test this please, again.
 Jenkins, test this please
 @tensorflow-jenkins test this please
  Is it possible to uninstall tensorflow and then reinstall?
  Jenkins, test this please.
 @tensorflow-jenkins test this please.
  Updated it just now.
 We're looking into it.
  This documentation is auto-generated from [this line in `array_ops.py`](https://github.com/tensorflow/tensorflow/blob/fe9dafd1583da5ccc205eab776f86afcb00411d2/tensorflow/python/ops/array_ops.py#L564). Can you please make the change there instead? Thanks!
 @tensorflow-jenkins, test this please.
 Jenkins, test this please.
 That looks like a tool failure. I'm pretty sure those timeouts are not your fault.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (we can't look at this until the CLA comes back as signed, can you double check)
 Not sure, you can give it a try!  I think you can add multiple emails to your github account (as non-primary) as well, if you felt comfortable doing that.  Basically:
- email of commit must match CLA email
- CLA email must be associated with your github account (somehow)
 google cla bot doesn't seem to have picked it up, sadly.  what's the commit email you used on the commit?  (why github makes this very hard to find is beyond me)
  TensorFlow does not support 3.5 as of today. Try using 3.4 which is supported.
  @tensorflow-jenkins: test this please

(nice!)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  "didn't work" - What do you mean? What does TensorBoard display?
I think this may be a duplicate of https://github.com/tensorflow/tensorflow/issues/1421 so I'm closing it. Please re-open it if it's not the same issue.

Also, can you try with the 0.8 release instead of 0.7 and let me know if you still have the issue?
  Almost all of the content on tensorflow.org is also on github. Can you see it here? Also, if you check our the source, it is in the g3doc folder locally. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Overall looks good, though I am worried about the integer first argument.  It is mainly an issue because of the proto serialization and working across multiple machines; it's not always gonna be the case that if i create a float 2.0, and then it undergoes proto serialization, then gets passed to the op on a different architecture, that floor(n) == n anymore.

Would you consider making a functor that adds a small epsilon (e.g. 0.01) , then floors the integer input, just before calling the Eigen tensor method?
 i think the abs(round(n) - n) < epsilon, where epsilon is e.g. 10*machine
epsilon is a good solution.

On Thu, Apr 14, 2016 at 4:14 PM, Till Hoffmann notifications@github.com
wrote:

> What would be your opinion on demanding abs(round(n) - n) < epsilon in
> the eigen implementation or do you think this is TensorFlow specific
> because of the serialisation?
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1834#issuecomment-210199478
 Perhaps I jumped the gun.  Looks like float and double can represent
integers up to and including 2^{mantissa bits + 1}.  So there seems to be
no need to worry in this case.  Even with the vagaries of
serializing/deserializing, I don't believe that a  number with exact FP
representation will be modified.

On Fri, Apr 15, 2016 at 2:50 AM, Till Hoffmann notifications@github.com
wrote:

> I'm having second thoughts about the suggested implementation. I'm sure it
> works fine but I'm a bit worried about consistency: a number of functions
> in SpecialFunctions.h in eigen use equality tests. For example, igamma
> tests for equality with zero and I also test for equality with one in the
> implementation of the zeta function. Is there something special about the
> representation of one and zero in memory that we don't have to worry about
> serialisation issues? Would it be worthwhile adding a helper function
> integer_equal_impl for all such comparisons?
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1834#issuecomment-210393620
 I think we're ready to import this, but I'll be away for a while.  Assigning to @benoitsteiner for final merge.
 @tensorflow-jenkins test this please.
 I was looking at the eigen implementation and saw that there may be a missing registration for the CUDA (float4 or double2) packet ops.  Can you revisit and make sure you added all the correct items?
 @tensorflow-jenkins : test this please.
 @martinwicke jenkins doesn't like me? :(
 @tillahoffmann ignore my comment about Eigen.  I re-checked and it looks good.  Waiting on jenkins to review.
 I'll try one running tests more time.  It's possible you have to remerge and fix any merge conflicts
 @tensorflow-jenkins : test this please.
 Jenkins, test this please. And stop ignoring @ebrevdo.
  Builds on top of #1832 
 Can you rebase?
 I merged #1829 and #1832, but conflicts.
 Done.
  @tensorflow-jenkins: test this please
  @ebrevdo: Should we mark this contributions welcome, or are you working on it? 
 This requires advanced slicing.  I started work on extending `gather_nd` to support this type of usecase this week.  No ETA, but expect 1-2 weeks at the absolute minimum.
 @ebrevdo: Thanks!  It'd be good to coordinate further slicing work with @aselle to make sure there's no duplication. 
 After discussing w/ @girving and @aselle, looks like gather_nd extensions will be coming in soon.  **Keep in mind** as of now, gather_nd has no gradient equivalent, so you still won't be able to use this to train :(  We'll need a scatter_nd op for that.
  Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors ([ref](https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#basic-math-functions)):

```
tf.pow()
tf.exp()
tf.log()

# lower priority?
tf.abs()
tf.neg()
tf.sign()
tf.inv()
tf.square()
tf.round()
tf.sqrt()
tf.ceil()
tf.floor()
```

and so on.

We'd like these ops to work on SparseTensor. These do not change the indices nor shape of `SparseTensor`s, so all that's needed is transform the `.values` field on Python side in O(1) line.
 Are `log()` and `inv()` well-defined for sparse tensors? Also, doesn't `exp()` make the whole tensor dense? (Or do other frameworks deal with the implicitly-zero elements differently in those cases?)
 As a first cut, I think having these ops transform only the non-empty entries makes sense and is simple to start with.  (This is what Theano calls "structured", as opposed to "regular" where the implicitly-zero elements are worked on as well.)  

So, `log(), inv(), exp()` should all work on the provided elements and thus will not make the tensor dense.
 pow(0, x) is undefined for x == 0, so that requires subtle work.

exp(0) == 1, so that densifies the SparseTensor (therefore it's not very useful)
log(0) = -inf, and i see you just pushed that; we will have to revert it because the correct version densifies the sparse tensor (and is not very useful) while the pushed version assumes log(0) == 0.
inv(0) is not defined, and so is not implementable.
 @vrv have to assign you for now, to revert the log() change.  feel free to unassign yourself after.
 No sparse matrix library I know of supports these operations, and SparseTensor was definitely not designed to support structured unary operations like this. It's meant to represent tensors with lots of zeros. Any functions that modify the sparsity structure of a dense version of the matrix have to be implemented similarly. One can always call tf.exp(st.values) themselves. Then it's very clear what's going on.
 Binary operations like tf.pow are even more tricky. See the various implementations of sparse_add.
 There are no sparse libraries that implement log or exp or inv.  Unary operations that preserve the sparsity pattern are reasonable and easy to implement... Think sign, neg, square, round, floor.
 Basically, any injective function for which f(0)=0 is OK.  floor and ceil require care because they move values near 0 to 0, so those are probably also not great candidates here.
 After more thoughts, I agree with @ebrevdo about leaving out the tricky ones for now.  Biggest reason being the purposed semantics is indeed weird, and if users do want to do something like that, it'd be easy for them to make a new SparseTensor with transformed values.  

Apologies for my earlier comment in this thread.  This issue probably still should exist for other unary ops that are well-behaved.
  Hi Alex,

I'm actually working on adding the equivalent of np.tril and np.triu, or rather an op that can copy a specified set of diagonals from a given matrix or batch of matrices. I expect it to be done sometime next week. 

The other pack/unpack of triangular to/from vector are probably lower priority. Feel free to submit a PR :-)

Rasmus
 Actually, regarding the packing/unpacking, you might want to look at the code in tensorflow/core/kernels/batch_matrix_diag_op.h which should get pushed to OSS shortly. It could be extended to extract multiple diagonals or you could at least use it as a template for a packing/unpacking op.
 Some of the functionality of 2) and 3) is covered by the SparseTensor class and the functions
tf.sparse_merge, and tf.sparse_to_dense. 

The op pair tf.diag / tf.diag_part  (and batch_ extensions) do perform the unpacking/packing similar to what you suggest in 3), but here the potential space savings are much larger. I suppose batch_matrix_band_part could have been written that way (with a companion unpacking op), but I thought the current form, which simply zeros out the remaining part of the matrix, more convenient and much faster than having to write a packing followed by an unpacking op to achieve the same.

As always, feel free to send a PR if you think it would be useful.
 @rmlarsen What's left to do in this issue? 
  Could you take a look at the [recently added timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) and its associated test to see if it's helpful?
 @concretevitamin: A how-to for TensorFlow profiling is probably a good idea.  If you wrote the timeline, would you want to write a tutorial highlighting it?
 @girving Paul is probably working on something along this line (profiling tools for OSS land).  (Although I wrote the RunOptions/RunMetadata support, I'm not the original author of the timeline.)
 @concretevitamin Which Paul?
 @geoffreyi pbar.

On Tue, Jun 14, 2016 at 12:36 AM, Geoffrey Irving notifications@github.com
wrote:

> @concretevitamin https://github.com/concretevitamin Which Paul?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225636669,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAkLHlA4FU1FAmpGsoL4brz0m7WKKB-Uks5qLYcjgaJpZM4IDGnH
> .
 @prb12 What's the story for OSS profiling?  If we have tools in place, could we make a tutorial explaining them?  Assigning to you for now, but feel free to triage further.
 I'm unlikely to have much time to write a tutorial in the near future, but the current status of the open source tools are as follows:

There is now a basic CUPTI GPU tracer integrated in the runtime.  You can run a step with tracing enabled and it records both the ops which are executed and the GPU kernels which are launched.  Here is an example:

```
run_metadata = tf.RunMetadata()
_, l, lr, predictions = sess.run(
            [optimizer, loss, learning_rate, train_prediction],
            feed_dict=feed_dict,
            options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
            run_metadata=run_metadata)
```

After the step completes, the run_metadata should contain a StepStats protobuf with lots of timing information, grouped by tensorflow device.  The CUPTI GPU tracing appears as some additional devices with names like  /gpu:0/stream:56 and /gpu:0/memcpy

Note: to get GPU tracing you will need to ensure that libcupti.so is on you LD_LIBRARY_PATH.  It is usually found in /usr/local/cuda/extras/lib64.

The simplest way to use this information is to load the stats into a 'Timeline' as follows:

```
from tensorflow.python.client import timeline
trace = timeline.Timeline(step_stats=run_metadata.step_stats)
```

The Timeline class can then be used to emit a JSON trace file in the Chrome Tracing Format, as follows:

```
trace_file = open('timeline.ctf.json', 'w')
trace_file.write(trace.generate_chrome_trace_format())
```

To view this trace, navigate to the URL  'chrome://tracing' in a Chrome web browser, click the 'Load' button and locate the timeline file.

It would be fairly simple to write a small python web server which served up these traces from a running TensorFlow program like [this](https://github.com/catapult-project/catapult/blob/master/tracing/docs/embedding-trace-viewer.md)
 @ericox  There are a bunch of hooks for distributed trace collection, but they are currently unimplemented in the open source gRPC code, see [here](https://github.com/tensorflow/tensorflow/blob/a7b7b26051989e1af40e657adef84c5550b838fe/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc#L445) and [here](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/protobuf/worker.proto#L180)

It might not be _too_ hard to plumb these through...  @mrry do you know of any showstoppers? 
 It wouldn't be too hard to get a basic version of this up and running:
1. Plumb the [`ExecutorOpts.record_timeline`](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/protobuf/worker.proto#L147) through the `RunGraphRequest` sent from the master to the worker.
2. Detect when this is set at the worker and create a `StepStatsCollector` to get the profiling information for the step.
3. Pass the stats back in the `RunGraphResponse`.
4. Aggregate the stats at the master and pass the aggregated stats back in the `RunStepResponse`.
 It would also be necessary to pass in a worker-specific device prefix to the Collect() method of the GPUTracer for use [here](https://github.com/tensorflow/tensorflow/blob/545df470168f52a369b5f1510f26ad001f48c650/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L550).
(At the moment this code assumes a single worker.  When running with multiple workers we wouldn't want to merge all of the gpu activity together)
  The `tf.train.NewCheckpointReader()` API lets you inspect a checkpoint to find out what variables it contains (it's in the nightlies at the moment, will be in 0.8).
 I'm closing this because the `tf.train.NewCheckpointReader()` API is now in the latest version, and enables users to build this functionality in code that uses the existing Saver API.
  @tensorflow-jenkins , test this please.
  To deal w/ a large text corpus while still using a single process, mainly we need to change the code to avoid loading the text file in memory in whole and instead use LineReader to maintain a sliding window over the corpus.
  Did you pass in --checkpoint_dir=your-checkpoint-dir?

Sherry
 No user response. Closing as not reproducible. Please open a new bug if the issue persists.
  @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 @caisq: we probably don't need to test doc changes :)  (awaiting for one more URL update and then I'll merge).
  Unfortunately it looks like this fell through the cracks.  @Yunlong-He: Do you know if it's still an issue?  @martinwicke: Is this a known problem? 
 If the connectivity to github is patchy, these things happen. Those are
usually transient issues though, sadly, bazel does not retry anything.

On Mon, Jun 6, 2016 at 5:21 PM Geoffrey Irving notifications@github.com
wrote:

> Unfortunately it looks like this fell through the cracks. @Yunlong-He
> https://github.com/Yunlong-He: Do you know if it's still an issue?
> @martinwicke https://github.com/martinwicke: Is this a known problem?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1817#issuecomment-224127572,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_cp12zLYLXhVar3BSorIP-5Pobupks5qJLmMgaJpZM4ICoAr
> .
  From [Stack Overflow](http://stackoverflow.com/questions/36480456/dynamic-rnn-and-array-ops-reverse-sequence-problems):

> I am trying to reverse my inputs with array_ops.reverse_sequence() before sending it to dynamic_rnn(), the inference graph can be build with no problem, but when building the training graph, I got the following error:
> 
> ```
>   Traceback (most recent call last):
>   File "bin/trainer.py", line 158, in <module>
>     kmer_len=args.kmer_len)
>   File "/home/ubuntu/GIT/IvyMike/ivymike/base_model.py", line 193, in run_training
>     train_op = model.training(loss, learning_rate)
>   File "/home/ubuntu/GIT/IvyMike/ivymike/base_model.py", line 100, in training
>     train_op = optimizer.minimize(loss, global_step=global_step)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 190, in minimize
>     colocate_gradients_with_ops=colocate_gradients_with_ops)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 241, in compute_gradients
>     colocate_gradients_with_ops=colocate_gradients_with_ops)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 481, in gradients
>     in_grads = _AsList(grad_fn(op, *out_grads))
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py", line 307, in _ReverseSequenceGrad
>     seq_lengths=seq_lengths),
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 1143, in reverse_sequence
>     batch_dim=batch_dim, name=name)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py", line 655, in apply_op
>     op_def=op_def)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2119, in create_op
>     set_shapes_for_outputs(ret)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1586, in set_shapes_for_outputs
>     shapes = shape_func(op)
>   File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py", line 1257, in _ReverseSequenceShape
>     (batch_dim, input_shape.ndims))
> TypeError: %d format: a number is required, not NoneType
> ```

The problem appears to arise when `input_shape.ndims` is `None` (which is a valid possibility).
 I'm not certain that fixing this issue will solve #1779, but I have a simple fix to this issue in review so hopefully you'll be able to check soon!
  I believe this is a duplicate of #889, which contains a workaround. I'll close this one, but we still have to fix it.
 It's not a duplicate, atrous convolution is adding holes in the input patch, whereas #889 is about dense patches but large strides.
 @gpapan I assigned you based on your email to @yaroslavvb. Thanks!
 Thanks. This will be ready in the next few days. 
 The change is in the process of being committed to github's Tensorflow.
 You are very welcome @fyu! Hope that people find this feature useful.
 @laurentk67 
I have not yet come across a use case that needs both stride > 1 and rate > 1, so I decided to omit the stride parameter from the API:
You typically use atrous_conv2d() in order to preserve the spatial resolution of your input feature map, whereas conv2d() with stride greater than one lowers the resolution. Not exposing conv2d's stride parameter thus prevents the user from using atrous convolution incorrectly. If some legitimate use case of atrous convolution comes up where both stride and rate may be greater than one, we can easily add a stride parameter and pass it along to conv2d().
 You should file a new issue with that feature request, I doubt it will be seen hidden in this thread.
  You should make merge_pairs return its result, and call it in the call to matmul.

This type of question is generally better answered on StackOverflow. I will close this issue, we're trying to keep issues for bug reports.
  We're working on a benchmarking framework and some infrastructure. I'll let @ebrevdo comment.
 Gchat me and we can discuss.
 @dominichamon, @ebrevdo: What was the result of the offline discussion?  
 No concrete plans to merge code, but backward compatible commits are welcome.
  This type of question is better handled at StackOverflow, please ask it there. We're trying to keep github issues for bug reports. Thanks.
 Yes, please post some code to reproduce. Getting NaNs during training is quite common and not usually a sign of a bug. A small code snippets that shows an actual bug will help a lot. I'll reopen this once you post code to reproduce.
 Please use the check_numerics op to find out where the NaNs are first introduced. In particular, split the .optimize() call on the optizer into compute_gradients and apply_gradients, so you can look at the computed gradients. If you see any variable get really large or very small, that's a sign of trouble. 

The distance function you wrote may be the cause, you could try making sure the value inside the square root is greater then 0 before you take the square root. 
 I think it's a bug in your code rather than in TensorFlow. Your confidences converge to 0's and 1's, then you take a log of 0 which gives -Inf, and multiply it by 0, which gives NaN
 Thanks! I got too hypnotized by the sqrt to see the log. :)
 You are taking a tf.log(y), and you get a 0 as an element of y as you train
 Mathematically, softmax can never be zero, but because we use floating point numbers, a number close enough to zero is represented as zero. Anyway, this is a common issue with neural networks, search for NaN and softmax
  @josh11b: Did you get a chance to look at this?  @cuiguoxin: Could you try running valgrind on TensorFlow to see where the problem begins?    
 Could this be related to #2646 ?
 @cuiguoxin Closing this due to inactivity and it seems to most likely be the same issue as #2646. Feel free to open it again if you can indeed reproduce with a recent version of TensorFlow.
  It should work, we don't do anything special for 3.5 -- the only difference would be the name (in fact, our Mac wheels are built with 3.5 without any changes).

We may in future provide more binaries, ideally directly in pypi, but I will close this issue for now.
 With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: 
https://github.com/tensorflow/tensorflow/
  It sounds like you're using github sources from master/HEAD, rather than from r0.7 branch.  Try installing one of our pip nightlies if you want to use source code from HEAD.
 If you created the model with HEAD, it's going to have ops have been added since r0.7, so that's not going to work :(.  
  @tensorflow-jenkins test this please.
  @tensorflow-jenkins , test this please.
  Can you please try running your script under `gdb`, to get a stack trace?
 Thanks, that's super helpful! So now we know that the issue is in one of the [random ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op.cc) and it's probably `tf.truncated_normal()`.

To help us track this down, could you please share your Python code for where you invoke `tf.truncated_normal()`? This will make it obvious if there's a corner case that's not being handled.
 Thanks for sending in these stack traces. It does look like an issue (perhaps due to an unexpected input?) in the `tensorflow::functor::FillPhiloxRandom` class that's used to compute random tensors (in e.g. `tf.truncated_normal()`.

I'm not particularly familiar with this code, so I'll hand off to @zheng-xq, who wrote it.
 @mrry, @zheng-xq: This whole "hand off" thing doesn't seem to work very well. :)

@anuragkr90: Is your issue different from @avati's?
 @anuragkr90: A floating point exception typically means integer division by zero.  Could you recompile the code in debug (pass `-c dbg` to Bazel) and rerun gdb to see where it's dividing by zero?  Otherwise we'll probably need a minimized test case for reproduction purposes.
 @anuragkr90, I am not able to reproduce this problem locally. 

Could you try to upgrade your Cuda driver, and use the Cuda SDK, and see if the problem still persists? 
  Easiest thing to do is to tf.split() your tensor into batches of 10, call tf.reduce_max() across each of those tensors along the batch dimension, and then concat them back together.  It sounds like you're not really looking to slide a window over the batch dimension since your kernel size is equal to your stride.
  This is necessary to remove the need for libpthread workarounds when building binaries for Android (as seen in tensorflow/examples/android/BUILD).
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @googlebot I signed it!
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please
 @martinwicke @vrv Is there a way to view the failing test.log files for this PR? I'm also getting 233 test failures when I try to test a clean copy of master locally:

```
File "/usr/local/google/home/andrewharp/.cache/bazel/_bazel_andrewharp/111e4735042cad380f0b87cff4bce3f6/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/user_ops/ackermann_test.runfiles/google/protobuf/symbol_database.py", line 180, in <module>
    _DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())
AttributeError: 'module' object has no attribute 'Default'
```

And 233 test failures locally with this branch, but with a different error:

```
File "/usr/local/google/home/andrewharp/.cache/bazel/_bazel_andrewharp/7d6b411acf30faa7da8a8cd3f21cf053/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/user_ops/ackermann_test.runfiles/tensorflow/core/framework/graph_pb2.py", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
ImportError: cannot import name descriptor


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```
 Also, just switching branches gives me issues where it thinks I have a pending commit to google/protobuf. 

If I'm on my local master branch and do "git checkout update_pb_lib" and it tells me I have a pending commit to google/protobuf (it wants to make it go back to using the old version).

Then I do "git checkout update_pb_lib -- google/protobuf" because I want to revert this unwanted commit and use the version I selected in "update_pb_lib", but it just ignores it and the pending commit remains active. I assume git is treating it differently from a regular file because it's actually a submodule, but I'm not sure how I'm supposed to switch local branches cleanly with this behavior.
 It seems like `git submodule update --init --recursive` does the trick to wipe the incorrect pending local commit.
 @tensorflow-jenkins: test this please
 @davidzchen 
I've updated my commit to get past the initial build errors related to https://github.com/google/protobuf/commit/985c968443e5124327fb600a91856192df4476ac, but now I get this:

> ImportError: Traceback (most recent call last):
>   File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/string_to_number_op_test.runfiles/tensorflow/python/__init__.py", line 49, in <module>
>     from tensorflow.core.framework.graph_pb2 import *
>   File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/string_to_number_op_test.runfiles/tensorflow/core/framework/graph_pb2.py", line 6, in <module>
>     from google.protobuf import descriptor as _descriptor
> ImportError: cannot import name descriptor

Does anything stand out to you that I might be missing here? I'm not sure how it's finding google.protobuf and not descriptor google.protobuf.descriptor, given that your PR didn't do anything to descriptor specifically.
 Thanks! Adding the extra parameters to our skylark macro calls to point it to protoc and protobuf_library fixed the original errors I mentioned in your PR. Though I suppose it's possible I did that incorrectly, leading to the current descriptor error.

@vrv What do you think about getting #1289 in?
 Sure, making protobuf a workspace dep looks good.  Enough bazel improvements have occurred to force users to upgrade at this point (and we can now detect the version)
  I'd like to embed integer labels into a fixed dimension space via the classic one-hot embedding. Here is an example:

```
onehot(inputs=[0, 2], num_labels=4) -> [[1, 0, 0, 0], [0, 0, 1, 0]]
```

It seems that sparse_to_dense does something similar, but not quite what I want. I've found some solutions online but they are rather convoluted. Is there a straightforward way to do this in tensorflow?
 Is there any way to do this with variable-length inputs (shape=[None])?
 I did check StackOverflow but found only convoluted answers, and I wanted to ask if there wasn't a more idiomatic way to perform this fairly ubiquitous operation. Since that doesn't appear to exist, perhaps this could be a feature request for it?

~~I've also tried tf.range but have encountered some errors when trying to reshape it #1801~~ Ignore that! Even so, I don't see how this would allow for variable length inputs, since tf.range needs to take a concrete limit.
 @samjabrahams That's great, thanks for the tip!
 Not a huge deal, but for some reason tf.one_hot insists on taking int64 as input - it should probably also work with the other int types. I'll just add in the cast myself for now.
 You should clarify what you mean by the improvements you'd like to make.  Regarding on/off value:

on_value and off_value must be provided to the kernel (because they're inputs, not attributes, and as such can be tensors).  It _is_ reasonable to add defaults of 1 and 0 for the python wrapper.  You'll have to hide the auto-generated one_hot function in the BUILD rule for array_ops and then write your own python wrapper that calls gen_array_ops._one_hot().  to make sure that the default type is float32, you should use np.float32(0) for the off_value and np.float32(1) for the on value.

PRs welcome; happy to review.
 It's fine to use math_ops.to_int??()  as a stop gap.  ops.pbtxt is auto
updated and only depends on the c++ side, you don't have to update it.
On Apr 9, 2016 11:06 AM, "Sam Abrahams" notifications@github.com wrote:

> Also- with this changes, should I make tweaks to the OneHot definition in
> ops.pbtxt
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/ops.pbtxt
> ?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1799#issuecomment-207823500
 @samjabrahams I've been using `one_hot` for a few weeks without problems. I recently reinstalled the master branch of tensorflow with `sudo pip3 install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl` but there doesn't seem to be any update to the signature of `one_hot` - it still takes only int64 tensors and required manual on_value and off_value.
 You need to use a nightly build to see the change.
On Apr 29, 2016 10:03 AM, "Vlad Firoiu" notifications@github.com wrote:

> @samjabrahams https://github.com/samjabrahams I've been using one_hot
> for a few weeks without problems. I recently reinstalled the master branch
> of tensorflow with sudo pip3 install
> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl
> but there doesn't seem to be any update to the signature of one_hot - it
> still takes only int64 tensors and required manual on_value and off_value.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1799#issuecomment-215812405
 Ok, I installed the nightly build, which provides default on_value and off_value. However `one_hot` still demands `int64` for the input. If I pass in `int32` I get an error:

```
>>> tf.one_hot(tf.constant(1, dtype=tf.int32), 10)
Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor("Const:0", shape=(), dtype=int32)'
```
 Fixed, so closing.
  You can use import_meta_graph and export_meta_graph.
 oooh, that's no bueno, @sherrym may know a workaround
 You should be to do this:

``` Python
  with tf.Session() as sess:
    new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')
    new_saver.restore(sess, 'my-save-dir/my-model-10000')
    # tf.get_collection() returns a list. In this example we only want the
    # first one.
    train_op = tf.get_collection('train_op')[0]
    for step in xrange(1000000):
      sess.run(train_op)
```
 @meereeum Any updates?
 What did you set to outfiles['meta'] to? Could you pass in an absolute path to your meta file, something like this:

tf.train.import_meta_graph("/tmp/model.meta")

if your model.meta file is indeed in /tmp?

Sherry
 The instructions and examples for using meta_graph can be found here:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/meta_graph/index.md
 @meereeum: Closing for now since it seems like it might have been caused by a malformed use of `import_meta_graph`.  I'm happy to reopen if it's a persistent issue. 
  @ebrevdo: @meereeum's example with `y.name == 'foo_1/Variable:0'` looks like a bug.  Could you take a look?
 I'll assign to lukasz, he may have code fixing this.
 I'm working on a CL/PR that at least adds the name scope to variable scope. Will report back here once that's in.
 A recent CL correcting this is now live on master. It includes a unit test that re-enters a variable scope and the original name scope this variable scope was created in (by using the new .original_name_scope property). See here: [variable_scope_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/variable_scope_test.py#L297). Closing this now, please re-open or make a new issue if you find more problems!
  retrain.py is to take a trained inception model, reuse all layers up to bottleneck layer, and train a new top layer for new image classifications. Therefore, from an inference perspective, the input is the same as the inception model being used, and the output is the output of the new top layer.

input_tensor=jpeg_data_tensor
scores_tensor=final_tensor
 Closing since this seems to be a (hopefully answered?) question rather than an issue that needs fixing.  Please ask to reopen if you think a doc-fix is warranted, or if there's a real bug here. 
  @mwalton: `some_other_arg` should be called `unused_argmax_grad`, but otherwise that's basically it.  Would you be interesting it submitting your change as a PR?  We'd have to add tests if so (mirroring those for the gradient of the normal max_pool.
 @mwalton: The tests should go in the same PR.
  @martinwicke: Is there a way we can fix this kind of thing on our end, or is it just a bazel issue? 
 @damienmg is there a solution to this that does not involve compiling bazel from source? Otherwise, that sounds like the right thing to do.
 Thanks @damienmg!  I'll close the issue on this end. 
  Check your bin path to see if there's a stale / old tensorboard.py ?
 Yes, we don't release anaconda wheels (yet) :(.  You might want to try building from sources.
  @tensorflow-jenkins test this please.
  @tensorflow-jenkins test this please.
  If you are installing the pip package for 0.7.1, check that you are using cudnn r4 (make sure the libraries being loaded are the correct version). 
 No, it's not backwards compatible from the pip installation, the APIs are often different.

https://github.com/tensorflow/tensorflow/pull/1794 is a change to allow compiling from source to support cudnn r5.  Since it's RC, we're unlikely to package it in our official PIPs until it is officially released (as they work out bugs), but that commit should add basic support for it.
 for 0.8.0 pips, cuda 7.5 and cudnn v4.

If you build from sources, you can use anything from cuda 6.5-cuda 7.5 and cudnn v2 - cudnn v5
  I looked into this just now briefly -- there are a lot of API changes that are somewhat annoying to #ifdef around but I made progress. 

edit: (Looking into how to plumb some more APIs, not sure how to expose all of the various algorithms for convolution like in the Theano example).
 https://github.com/vrv/tensorflow/commit/a76449737fed046b7e9f9871dd2d1d5c2f51ead0

Built for me with R5, I was able to run a few benchmarks, though not a huge difference, presumably because I'm not choosing the 'optimal algorithms'.
 #1794 added basic support for v5 RC, so you can at least install the library on your system and it will work.  We haven't performance optimized it or added the RNN/LSTM API yet.
 I'm not sure stream executor works for OS X yet (hence #664).  I did test cudnn v5 on my local machine and it seemed to work, so I'm not entirely sure why it's still failing for you -- might double check that the symlinks and cudnn header file and ./configure options have been set properly.  
 Thanks @benbarsdell: want to send a PR to fix either one?  Also when I wrote that up, it wasn't clear to me how a 'filter' could have a format of NCHW, which is why I used the data layout.  Filters don't talk about batch, just input and output channel / height / width, so I wasn't really sure what to do there.
 @benbarsdell okay I think with the commit above and dfcad66c5ecc6e49aff0fa379832982809b2b447 we addressed your issues -- let me know if there's still something left there.

Will leave this open until we figure out what we're doing about the LSTM/RNN API.
 We're sort of working on this now.
 @vrv: Assigning to you for now. 
  @tensorflow-jenkins test this please
 All tests passed. The sanity failure can be ignored for now. Admins, feel free to merge as you see appropriate.
  Agreed with @alexatknit, seems like we have the primitives to implement this already -- though if you have a more concrete feature request, we'll take a look.
  Hi @eriophora,

To make sure I understand your question: Are you asking for "weights" to be left of "BatchNorm" inside the conv0 namespace? Left-to-right ordering is meaningless in the graph visualizer. What matters is the bottom-to-top ordering. We leave left-to-right ordering to the layout optimizer to figure it out, which sometimes can swap nodes to reduce edge crossing and we can't control it (even when no edges are shared).
  The nightly binaries are linked from the top-level readme: https://github.com/tensorflow/tensorflow
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I'm conflicted -- the optimizer interface is too narrow as it is, and learning rate is not necessarily universal (although arguably it is for the optimizers we actually support). @rmlarsen, an opinion?
 We might want  to implement other optimizers where the concept of "learning rate" does not make sense (such as second order methods), so I find moving it to the base class is not the right design.
  @ebrevdo: Should I leave this one assigned to you? 
 Sure, I'll review the PR.
  Are you running a 32 or 64 bit OS?

Here are some suggestions for a user who experienced a similar error: http://stackoverflow.com/questions/17020298/android-sdks-build-tools-17-0-0-aapt-error-while-loading-shared-libraries-libz
 Closing due to inactivity. Please reopen if issue persists.
  @tensorflow-jenkins test this please.
 Nice, looks good!
 Thanks for the catch and the fix @fayeshine!
  @tensorflow-jenkins: test this please
  Is protobuf installed already, and which version? 
 No reply in a month. Close?
 Yup, closing due to inactivity,  re-open if more information is available.
  You seem to still have 0.6 installed somewhere (output of tensorflow.version), can you make sure you have 0.7.1?
 It is possible that TensorFlow is choosing a bad placement of operators across your two GPUs, and slowing things down by copying data between the GPUs. Would you try restricting to a single GPU:

```
config = tf.ConfigProto(device_count={'GPU': 1})
with tf.Session(config=config) ...
```

and see if things get faster?
 FYI 0.7.1 requires cudnn r4, not v3, so I'm not sure how your system is working at all.
 Unfortunately, I don't think we have enough information to diagnose this issue, so I'm going to close.  Please comment if better information appears, or file a separate issue.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 @tmc, @dave-andersen, please take a look. 

I think high level documentation should live in contrib/go/g3doc, we're working on glue to integrate such docs into the website.

We can look into generating markdown from godoc, maybe? I don't know the first thing about go, or godoc, so stop me if this is stupid. Our publishing to website pipeline goes via md, so that would be easiest, probably, assuming we want to have documentation for the go part on tensorflow.org.
 I'd like to get @tmc approval first before we go too far.
 @tmc can you chime in and OK this?
 Because @tmc has signed the CLA, I approve this import of his patch with modification.  Could you go ahead and sign the CLA also, Alonso?

I'm fine leaving the docs in godoc format (confirmed with @martinwicke ) or as .md, since this will be in contrib/ for the time being.

Reviewing now.
 Cool re CLA.  @googlebot signed.  (let's see if that works to get the status updated. :)
 A quick global pass through comments to make sure they're all terminated with a period would be good in  example_test.go.  It's a bit inconsistent.
 @martinwicke - can you bring your expertise to bear on this CLA problem? :)
 Done with comments - thanks for putting up with the flood of them combined with the fairly painful github comment mechanism. :)
 (btw, I agree with tmc about being careful with the API.  From the TensorFlow team perspective, this is in "contrib", so we're allowed to change it at will, but if people start using it seriously, it'll become implicitly harder to change.  Once the nits are addressed I'll take a final peek from a bigger picture API perspective and then we should be good to go.

I'm more concerned _for now_ about the serving API, because I think that's the most obviously clear match for Go+TF, but your image recognition example shows a very nice reason for having some graph construction integrated with the Go code as well -- though that could also be baked into the Inception graph.)
 Re questions about adding more to the API:  I'd strongly suggest against it.  There's a lot in here already, and I think it would be useful to get it in to contrib/ and let people play around with it to get experience with what's really wanted.  If everyone starts screaming that they need saver, it's easy to add later - but harder to remove if we add it now.
 (btw, thanks and congrats for fixing all of those things so quickly.  I'm taking another look now.)
 I think we should pull this in as soon as these comments are done and we know there's a way to build & run the tests.  The one thing I'd ask is that you put a comment in the example imagenet program that says this isn't really the right way to do it - pending a way to modify the graphdef, it's much more efficient to directly couple the input and recognition path so that the image doesn't get copied back to python/CPU between the steps. :)
 Hah!  So it is - thanks.  It turns out that I don't like our API example program, then, but that's not your problem in the slightest.  _grins_  Fine to leave it as-is.
 Curious to see what this does to Jenkins - will it break the build or be happy?
@tensorflow-jenkins test this please.
 Last major thing that needs to be addressed:
  (a)  Is the build / install solution satisfactory and can it be improved?  (I'll look @ this next);
  (b)  We need to get the tests integrated to the point where jenkins auto-runs them as part of the overall test framework.  I'm going to need to dig into how this works with bazel/blaze unless @rakyll is willing to help clue me in on integrating bazel's stuff with go's testing.
 @tmc - try @ googlebot I'm okay with these changes
 +1 to committing the generated files.  I checked with some of the other go folk here, and they're of the very strong opinion that if it's going to go in our repo, it needs to be entirely buildable with `go get`.

Which shared libraries do you need to compile?  We could make those part of the tensorflow C++ build and then you just need to find them.  cgo supports some ways to do this;  I'm not sure with the way you've set it up.
 I know very little about go, so that sounds fine, but a go expert should probably chime in here.  In some future world we'll hopefully have shared library distributions of the C++ core that doesn't require building during go get, and then this can probably be cleaned up.
 I concur - go gettable is critical, and making someone manually install tensorflow in a special way first is probably a reasonable cost.  LMK once that's there and I'll test out the workflow also.
 So what's the status of this? Is reviewing done and ready?
 @vrv @martinwicke - I'm OK accepting this into contrib within a branch.  It's going to need build changes to get it to work with Jenkins and internally, so I think this is the best approach to continue the momentum -- it'll be available to the community to play with, and we can then start poking at what's going to be required within both tensorflow and this PR to simplify the build and install process.

(I don't know the mechanism for getting that working, so I'll let one of you take it from here. :)

Alonso, thanks for your patience with this!
 Ok. I made an appropriately named brach (go branch, go!), can you redo the PR against said branch? I will then merge it onto there.

Thanks again for all the effort.
 Thanks! #2479 is merged. I updated go before merging. 
  Unfortunately it's not very obvious how to build standalone Android in Bazel. There's an example command in the comments for //tensorflow/core:android_tensorflow_lib.  Here it is for reference (adapted for libtensorflow_demo.so):

bazel build -c opt tensorflow/examples/android/libtensorflow_demo.so \
 --crosstool_top=//external:android/crosstool \
 --cpu=armeabi-v7a \
 --host_crosstool_top=@bazel_tools//tools/cpp:toolchain

Keep in mind that the link flags for libtensorflow_demo.so will by default strip out anything not explicitly exported by the library.

edit: fixed crosstool_top so as not to confuse future visitors
 Sorry, I mixed up my build commands. Here's one I just verified to work with a fixed crosstool_top param.

bazel build -c opt tensorflow/examples/android:libtensorflow_demo.so \
--crosstool_top=//external:android/crosstool \
--cpu=armeabi-v7a \
--host_crosstool_top=@bazel_tools//tools/cpp:toolchain
1. You can certainly have other targets depend on Tensorflow, or have Tensorflow depend on other targets. Or do you mean depending on TF from outside of Bazel? That's also possible, if you have the precompiled static library and the header paths configured appropriately.
2. You can build a static library by removing the alwayslink=1 attributes from the //tensorflow/core:android_tensorflow_lib(_lite) targets and building those. If you want the examples/android/jni code as well I think you could experiment with a "-static" linkopt, or alternatively turn that lib into a cc_library.
 Closing due to lack of activity; please reopen if still seeing issues.
  I looked into the code base for when that error is raised and the only place is in the optimizer.py code that was referenced in #1758 which does show the nodes responsible.  I'm closing this because the uninformative error message is not reproducible.  Feel free to reopen if you can provide more details.
  A limited version of this is called hyperparameter optimization. You can check out projects like spearmint (https://github.com/JasperSnoek/spearmint). In general, it's a far off research problem. 

I'm closing this issue since it's not a well-defined feature request. Better support for hyperparameter tuning is planned.
  This should be possible using reshape, argmax, one_hot, reshape, cwise_mul. If you need help with the exact invocation, stackoverflow is probably a better place.
  Thanks!
  @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
  Yes, this is a bug. Either a Python wrapper or the op kernel for the different resize methods should perform conversion if necessary.
 jhspaybar, I believe that you should fix this on the python side of things.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please

(Thanks!)
  Cool, thanks!  Do you want to send us a PR?
 Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Awesome!  At a high level these numbers sound great to me, but we can't really take too much of a look until the CLA is signed.  If this makes sense, we'd be happy to work with you to get these changes in.

@vincentvanhoucke @zheng-xq @benoitsteiner as an early heads up.
 CLAs look good, thanks!

<!-- ok -->
 This looks very good from my POV. @vrv how do we handle stream executor changes like this?
 I think we now just accept them and we'll figure out how to backport them to the open source stream executor.  Unfortunately the best person to review this change is @zheng-xq who is out of the office this week and next.  I could offer style / tests suggestions, but I'd rather do that only after the high level review has been done.
 @zheng-xq did you have a chance to look at this?
 @lukemetz, thank you for the contribution!

I've put in my first round of comments. We can go through more details later. 
 Hi @lukemetz, any updates?
 @lukemetz, thank you very much for the code improvement! It is awesome to hear about the performance gain. 

There are a few comments at the bottom. Some of them are regarding the interface of this op. Because TensorFlow promises backward compatibility on TF core, it is difficult to change the op interface once the code is in TF core. 

On the other hand, we would like to merge in this code as soon as possible so more people can benefit and help improve. How about checking the same code into TF contrib for now? I think we would be able to merge with very little modifications. And after we iron out the details, the TF team will help integrate the change back into the TF core. 

Detailed comments: 
1. Lack of CPU implementation. 
2. We do not want a separate training and inference op. The same op should check whether "save_mean" and "saved_inv_var" are actually used before picking the "training" and "inference" version. 
3. We don't want stream-executor to allocate any memory. TensorFlow has a much more efficient memory management system. So this should be done through ScratchAllocator. 
4. Ongoing discussion whether to expose the running mean and average, and measure the performance impact. 
5. We need to test more common shapes to verify the results. 
 @lukemetz, unfortunately, there is no contrib equivalent of stream_executor. However, since your change is mostly addition, so that should be fine. 

Also there are a lot of people still using V4, so we should make sure both are functionally correct. TensorFlow in general makes sure at least two versions are supported. 
 @lukemetz any updates?
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Not sure what the status of this is, but presumably it's almost ready now?  We don't get notifications when you push commits, unfortunately.
  The [`tf.import_graph_def()`](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#import_graph_def) function provides the only (supported) way to perform this surgery, via the optional `input_map` argument.

Let's say you want to replace the tensor `"DecodeJpeg:0"` with your new variable. You would do something like the following:

``` python
graph_def = ...
tf_new_image = tf.constant(...)
_ = tf.import_graph_def(graph_def, input_map={"DecodeJpeg:0": tf_new_image})
```
 @TianweiXing: Can you post this as a question on Stack Overflow? We're using it to track problems that are not bugs or feature requests.
  1) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/graph_metrics.py might help you figure out some stats about your model

2) For training you're also going to need to account for the memory used by gradient update tensors -- graph_metrics.py should help you a little.

It's definitely possible we're using more memory than we ideally could, but we're going to need more data to understand where we can improve.
 You are probably encountering an all to common problem with Tensors secretly conspiring to broadcast. I don't know where 10000 comes from, but the fact that you have a sudden large matrix where you expected only long vectors is typical. 

I will close this issue for now. Please comment if you have more evidence for an actual bug. Or create a new issue if you uncover something else.
  Sorry for the late response. But yes, you need libprotobuf installed on your machine for the C++ extension in the python package to work. I will update the instructions to reflect that.
 Closed due to inactivity.
  Please verify that this doesn't break nvcc 7.0 and we'll re-open, thanks!
  @tensorflow-jenkins test this please.
 Thanks, good catch!
  It seems to work in our tests. Can you provide more information? Is this still an issue?
 Closing for now, please comment with more information if it's still an issue.
  Duplicate of #1741 
  Does pinning the variables to CPU make this work? i.e.

``` python
with tf.device("/cpu:0"):
  nce_W = …
  nce_b = …
```
 @mrry I assume this is fixed now, in that we don't do improper placement, but I'm not sure all sparse optimizers are supported on GPU, which is perhaps another bug which I believe we already have an issue for.
  Most of our in-op parallel currently comes through Eigen, which includes SIMD optimizations. We're not planning on using OpenMP right now, see also a discussion at #22. 

I'll close this issue for now. We may revisit it depending on how the OpenCL work goes.
  Try https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#pip-installation-issues and leave a comment if that doesn't solve that issue.
  retrain.py at HEAD now uses code that is only available after 0.7.1.  Try installing the pip nightly or use retrain.py at the r0.7 branch.
  This is method is simple to write using the sparse_tensor_to_dense op.  Contributions are welcome.
 Sorry right. I added gather_nd which does what you need.
 Create 3 tensors (indices, values, shape), wrap them in a SparseTensor
object, and feed the 3 tensors together.
On May 3, 2016 11:37 PM, "Yongliang Wang" notifications@github.com wrote:

> @ebrevdo https://github.com/ebrevdo
> 
> Hi ebrevdo ,
> recently I am trying to use ctc in a OCR project, but I found that the
> "label" parameter in ctc_loss function is sparse tensor, but it seams that
> I can't feed a sparse tensor to placeholder. Is there any way to work
> around it?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-216758508
 No, for CTC you have a different number of output time steps than input
time steps.  That's the point.

On Fri, May 13, 2016 at 7:13 AM, Erik Rehn notifications@github.com wrote:

> Maybe a stupid question, but why use a SparseTensor at all since it is
> never sparse? You have a label for each timestep right?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-219054493
 Yes - the target sequences almost always have different length, from one
batch entry to the next.

On Fri, May 13, 2016 at 9:38 AM, Erik Rehn notifications@github.com wrote:

> Ah, now i get it, you want to use a SparseTensor since the target
> sequences might have different length?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-219095155
 @ebrevdo @raindeer What's the status of this issue?
 Marked as resolved.  It's easy to create a SparseTensor if you have fixed length target sequences.
  Let's get that code through review on bitbucket, then we can update the references and I'll push my changes adding igamma/igammac to TensorFlow.  You'll be able to use those as reference for the TF implementations of polygamma/zeta.
 Great!  I need a couple of days to import your changes on our end.  In the meantime look at my recent push of igamma/igammac as a reference for wrapping zeta/polygamma.
 (note the unit tests I added: they require scipy and cause a soft failure if it's not found.  this is fine - just make sure you have scipy installed when testing locally)
 The Eigen implementations of the polygamma and zeta functions are now available in TensorFlow
  This is probably a question better suited to StackOverflow, since there doesn't appear to be a bug.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 Merged. Thanks.
  @tensorflow-jenkins test this please.
 There is still a failure in the cmake PR build: http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-cmake/9/console
 @tensorflow-jenkins test this please
 The renaming "tensorflow/core/ops/compat/update_ops.cc → ...orflow/core/ops/compat/update_ops_main.cc" has broken CPU builds. 
 Will cmake work for gpu builds? If so, the install_proto3.sh needs to be added to tensorflow/tools/ci_build/Dockerfile.gpu as well.
 @tensorflow-jenkins test this please
 @clsung Let me help you test it later. We have GPU machines available. How much change do you expect to be needed? If the test passes, we'll just merge it right away. BTW, the commits need to be squashed before the merge. 
 @clsung if cmake 3.0 would be enough then please put it into your next PR.

@vrv I will make PR to move the ppa repositories.

Both... 48 lines in 17 commits? Should we squash it next time? :-)
 https://github.com/tensorflow/tensorflow/commit/2c2f422a628d83a591fb5d20d46deecc47655ead it was squashed.
 Right! That's why I had to look for the PR and could not just click through :-)

Github should really make squash+rebase a feature! (even a default option)
 It is a feature -- it's what we used here.  I guess it doesn't modify the original pull request, that's all.
  You may need to run sdk_path/tools/android and ensure that you have the 23.0.1 Android build tools installed. Alternatively, if you already have a version >= 21, you can try changing the setting in WORKSPACE and point to that.
 Is your username popo or poporo? I notice it changes in the two printouts you've provided.
 If you can't create a bazel-genfiles link I'd suspect it's a basic Bazel installation problem. You could try doublechecking the permissions on the relevant directories and the users you installed Bazel as/are building as.

@damienmg Does this seem familiar to you? thanks
 @qemb1 What target are you trying to build, and what version of Bazel are you using? Can you rerun with --verbose_failures, and then check that there are no ownership/permission/existence issues with the Android SDK binary in question?
 @qemb01 Do you have the sdk build tools 23.0.1 installed? If not, you'll need to update your WORKSPACE file to point to a version you do have. If it is already installed, check your SDK directory for ownership conflicts.
  @alquraishi, that definitely seems like would be a great addition to the existing RNN library in TF. Marking as 'contributions welcome', feel free to send a PR.
  I cannot reproduce the problem at commit 3c8780451007c27b69f10925d43d1f3501d94106, Cuda 7.5. 

For both CPU and GPU, this is what I got

[[ 11.89173698  11.91889286]
 [ 11.91601562  11.93181419]
 [ 11.83213997  12.00035954]]
 Closing this issue for now. We will reopen it if more people hit into the same issue. 
  This has been fixed internally and is waiting for the next push to nightly.
 It means it'll show up on github in a day or two; and in our pip nightly builds the same night.
 Should be fixed at head. Can you confirm?
  @tensorflow-jenkins: test this please
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  @martinwicke Should we move all skflow documentation into g3doc? Or should we update the generation file and keep it in the contrib?

Optionally we can call it `learn` documentation to consolidate all things there.
 We should keep it in contrib and update the website generation. It's not the only project with this issue now and separating the documentation seems like a bad idea.
 @dansbecker Closing this one, as we moved documentation to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/contrib/learn 

Feel free to add more documentation there - it will be showing up on tensorflow.org website over time.
  This is a bug.
 A negative side-effect of this is that you can't run all the tests with 
`bazel test -c opt --config=cuda tensorflow/...`

A fraction of the tests (10-30%) fail with `CUDA_ERROR_OUT_OF_MEMORY` on my 4GB GTX 980.
But then if I rerun any of the failing tests using separate `blaze test`, it works.
 Each bazel test invocation is a separate process, so when the process exits, it does release the memory.  In this case, bazel is running multiple tests in parallel.  Use bazel test -j 1 to only run one at a time.
 Thanks @vrv, that fixed all the out-of-memory errors I had
 As for the original problem, currently the Allocator in the GPUDevice belongs to the ProcessState, which is essentially a global singleton. The first session using GPU initializes it, and frees itself when the process shuts down. Even if a second session chooses a different GPUOptions, it would not take effect. 

Changing this would be a fairly large change. We need to rethink how the device is initialized and how it interacts with the session, and therefore modify the current API. It is unlikely the TensorFlow team can get to this in the short term. 

Marking it as contribution welcome. If anyone is interested, a design proposal could be discussed here, before proceeding to implementations. 
  @tensorflow-jenkins: test this please
  Try:
x = tf.Variable(...)
x = tf.identity(x)

does the code run then?
 Perhaps it's fine to always require that the input is not a ref. I'll double check to see why it's not being converted for you. In the meantime use variable.value instead of variable directly.
 See tf.nn.dynamic_rnn.
On May 9, 2016 1:39 AM, "李炜" notifications@github.com wrote:

> I am trying to use scan in tensorflow 0.8 version to implement a theano
> like rnn, while getting an error like this
> [image: screenshot from 2016-05-09 16 35 28]
> https://cloud.githubusercontent.com/assets/3108838/15107733/19526442-1604-11e6-9eb7-5fbbc6ae0fc4.png
> my code is like this
> [image: screenshot from 2016-05-09 16 36 24]
> https://cloud.githubusercontent.com/assets/3108838/15107751/32d2b7f0-1604-11e6-8527-0195a95e5aad.png
> can anyone tell me what's going on? and is there any possibility that
> tensorflow will make rnn easier to write with a symbolic loop like scan in
> theano with multiple parameters in the fn function and maybe multiple
> return values?
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1725#issuecomment-217807428
 it's "undocumented" as a still-changing API, but it's pretty stable now;
should be "officially released" into the documentation soon.  for now, you
can use ipython and type:

import tf
tf.nn.dynamic_rnn?
tf.nn.rnn_cell.LSTMCell?
tf.nn.rnn_cell.GRUCell?

to get some documentation

On Mon, May 9, 2016 at 7:43 AM, 李炜 notifications@github.com wrote:

> @ebrevdo https://github.com/ebrevdo sorry to bother you, I just
> transferred to tensorflow from theano, I am having trouble finding
> tf.nn.dynamic_rnn and its api, where can I find it? Because I can't find it
> in the api page.
> thanks for your kind reply.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1725#issuecomment-217884239
 Anyone with pending questions - please open new issues for your individual questions - the initial question was answered.
  Thanks for adding a reference to issue #2062!  I meant but forgot to do that. 
  @martinwicke @vrv Any plan to get this fixed soon?
 To me, that's gating the next release, so probably :)
 I filed bugs for these and assigned people, and some were fixed. It looks
like the failures are still the same though :(
On Thu, Mar 31, 2016 at 09:15 Vijay Vasudevan notifications@github.com
wrote:

> To me, that's gating the next release, so probably :)
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1722#issuecomment-204004713
 I'm fixing some of them.
  @tensorflow-jenkins: test this please
 Hm, looks like a cmake failure -- not sure how to resolve this.  Is this working for you, and maybe  our cmake test suite isn't working?
 @jendap, @caisq  to help validate, if possible
 I believe 2c2f422a628d83a591fb5d20d46deecc47655ead fixed this, right?  Let me know if I'm wrong and I'll reopen.
  Fixed, I believe.
 (with 2c2f422a628d83a591fb5d20d46deecc47655ead)
  Thanks!
  I think that the model in translate.py should work fine with variable batch size, it looks like it's changed to 1 in the decode function.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L212

Did you try setting model.batch_size=260 before decoding, and then setting it back again? On the other hand, if you want a truly proper testing, it would probably be better to have a parallel process reading the latest checkpoint and running in decode-mode, e.g. to avoid using sampled softmax.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Thanks! (I know it sucks to have to sign a CLA for a doc change).
  Currently `from tensorflow.contrib import skflow` doesn't import all the things. This adds `__init__.py` files to the BUILD file and correct absolute imports.
 squash commits?
 @vrv done
  Can you run `uname -a` in your terminal and report what gets printed?
  Sorry for the delay. I will get a change review back to you in two days
(Monday PST). Thank you for the contribution!

On Sat, Apr 2, 2016, 09:13 Wenjian Huang notifications@github.com wrote:

> @Sohl-Dickstein https://github.com/Sohl-Dickstein ,what's your opinion?
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1710#issuecomment-204747240
 I have some concerns about this pull request. This modification requires training for 10 times more learning steps, with twice the batch size, and with a model that has several hundred times the number of parameters. We want the models in these tutorials to run as quickly as possible, and this seems like a big step backwards in that respect. Even with that size increase, the performance change due to dropout is very small. There are also two images in the tutorial (graph visualization, and tensorboard screen shot) that will no longer be accurate after this change.

Let me suggest a much more light weight change. How about placing a footnote link at the end of the first sentence in the dropout section in the original MNIST for Experts tutorial, then adding the following footnote text to the end of the tutorial:

"For this small convolutional network, performance is actually nearly identical with and without dropout. Dropout is often very effective at reducing overfitting, but it is most useful when training very large neural networks."

Instructions for adding footnotes in markdown are here:

http://stackoverflow.com/questions/25579868/how-do-i-add-footnotes-to-github-flavoured-markdown
  Update build_pip_package.sh for generating a tarball

One can generate a single wheel file using:

```
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```

One can also generate an extra tarball file using:

```
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg tarball
```
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Hello, I signed it!
 CLAs look good, thanks!

<!-- ok -->
 Close this as it is a mistake. The tarball only contains the generated dynamic libraries that might be not suitable for other environment, e.g., the `GLIBC_2.14 not found` error.
  Thanks, you can you try to squash the commits to get rid of all of those merge commits?
 The merge commits are distracting -- maybe clone a new copy of your local repo, rebase to head, and then apply your changes?

```
git clone ... your fork
git remote add upstream https://github.com/tensorflow/tensorflow.git
git fetch upstream
git rebase upstream/master
git checkout -b new_branch
... make edits ...
git push -u origin new_branch
```
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Great, thanks!  @tensorflow-jenkins: test this please
  @Dominator008, thanks for your comments. We did start out with maintaining both gerrit and github and accepting changes via both systems. However, the maintenance burden became too much, and with the limited human resources we had (have), we decided to focus on making the  github process as robust as possible. Gerrit indeed has superior code review interface, but unfortunately, we are unable to dedicate resources to maintain multiple sources of truth for TensorFlow code.
 For what it's worth, github now allows squash and merge, creating no merge commits. Yay!
  This would be nice to figure out with "just use PYTHONPATH" way.  This way one could start `ipython notebook` to play around with their tensorflow changes in Jupyter, but current way is limiting you to `simple_console.py`
 These set of commands work for me :

```
mkdir tensorflow
export TF_INSTALL_DIR=/tmp/tensorflow
pip install --prefix=$TF_INSTALL_DIR https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl
Collecting tensorflow==0.9.0rc0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl
Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl
Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)
Requirement already satisfied (use --upgrade to upgrade): protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)
Requirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)
Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)
Requirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf==3.0.0b2->tensorflow==0.9.0rc0)
Installing collected packages: tensorflow
Successfully installed tensorflow

PYTHONPATH=$TF_INSTALL_DIR/lib/python2.7/site-packages:$PYTHONPATH python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import tensorflow as tf
tf>>> tf.__file__
'/tmp/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.pyc'

mkdir /tmp/tensorflow
export TF_INSTALL_DIR=/tmp/tensorflow
```

I am on a Ubuntu 14.04, though. Can you give this a shot? Having said that, `virtualenv` is our tested installation methodology. We need lot more people asking for this way on installation before we can start thinking about supporting and testing this. Feel free to keep this open in case you really want to see this supported, otherwise please close the issue.
 @rdadolf Closing this one out due to inactivity. I don't see anything fundamentally broken about using `pip` with `--prefix` and using `$PYTHONPATH` later, at least in the platforms I tried. Please feel free to reopen if you think this is still an issue worth solving.
  Thanks for filing this issue. `tf.Print` is primarily as a tool for debugging during graph execution. Making the output nicely formatted is not going to be high priority for us in the near future. But the code to print out the tensor is here [1]. I am marking this issue as 'Contributions welcome' :)

[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.cc#L628
  I was unable to reproduce with a fresh Bazel 0.2.0 install and tensorflow download. This is on OSX 10.11.3, with Java 1.8.0_72-b15 and ndk r10e.

For reference I installed bazel from [here](https://github-cloud.s3.amazonaws.com/releases/20773773/dc48f278-da3a-11e5-9243-85bbaa6e4f62.sh?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAISTNZFOVBIJMK3TQ%2F20160330%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160330T225519Z&X-Amz-Expires=300&X-Amz-Signature=824c159effdd075803132a589805fedbcd4813908d24fe983b86c7e4f3d7b8bb&X-Amz-SignedHeaders=host&actor_id=3376817&response-content-disposition=attachment%3B%20filename%3Dbazel-0.2.0-installer-darwin-x86_64.sh&response-content-type=application%2Foctet-stream) and installed with:

```
chmod +x bazel-0.2.0-installer-darwin-x86_64.sh; 
sudo ./bazel-0.2.0-installer-darwin-x86_64.sh
```

I downloaded TF with:
`git clone --recursive https://github.com/tensorflow/tensorflow/`

And appended the following to my WORKSPACE:

```
android_sdk_repository(
    name = "androidsdk",
    api_level = 23,
    build_tools_version = "23.0.2",
    # Replace with path to Android SDK on your system
    path = "/Users/andrewharp/Library/Android/sdk",
)

android_ndk_repository(
    name="androidndk",
    path="/Users/andrewharp/Downloads/android-ndk-r10e",
    api_level=21)
```

I doubt it's related, but what happens if you strip the trailing slash off your android ndk path? 
  Reverts tensorflow/tensorflow#1648 -- it's breaking the mac build
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (this was a mistake, right?  the tests passed on the mac build)
 (Closing, feel free to reopen if you disagree)
  @tensorflow-jenkins: test this please
  There is an easier fix - `__init__` files on the way weren't in the BUILD file.
Added a PR #1712 for that. Please take a look, and I'll close this one.
  It looks like you have non-ascii quotes around the paths. Did you use a rich text editor to make the edits?
  @tensorflow-jenkins: test this please
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Pretty old unattended bug. Was the issue resolved?
 I can confirm this is no longer an issue, so closing this issue. 

@elanmart, feel free to reopen if you still see the problem with the latest TensorFlow, Cuda SDK and Cudnn library. 
  @tensorflow-jenkins: test this please (I'm assuming this is correct ahead of time).

@ebrevdo: is it worth adding C++ tests that would catch this memory leak?  (It's interesting that the python one doesn't expose it).
 Can you do a favor and run the tests with blaze test -c dbg ?  This should check that you don't unref once too many times.
 Great!  Can you rebase on HEAD so we can merge?
 @tensorflow-jenkins: test this please
 Thanks!!
  Check out the github repo at the r0.7 branch if you want to run code using 0.7.1 -- github master may introduce incompatibilities with the pip installs until we reach 1.0
 I would try upgrading to 0.8 now, there might have been some fixes since then.

If not, can you paste the entire stack trace, so we can see where this might be coming from?
  What do you mean by "time cost"? Do you mean the time to run the operation? If so, how are you measuring it? Can you please provide some more details?
 Thanks for the details. Your methodology looks good. You are running on the CPU, yes? TensorFlow uses Eigen for CPU convolutions and cuDNN for GPU convolutions. It's quite possible that the implementation used for 1,1,3,64 convolution is not as efficient as the larger one, hence your observation. Can you try it on as GPU?
 Unless I'm missing something, your timing loop appears to fetch the result of the convolution each time and discard it? Depending on the image sizes you may just be measuring serialization overheads. (though I may be missing something since the function inference() in your  code snippet appears to have some typos?)

To benchmark the cost of just the convolution, do session.run(target.op)  
 @MisayaZ, it is true that the filter size [5, 5, 3, 64] entails five times more computation than [1, 5, 3, 64]. However, the actual run times depend heavily on the implementation, memory access to computation ratio etc. So there is no way to guarantee any correlation between actual computation and run times. Also, we make use of external implementation such as cuDNN for the kernels. Hopefully those libraries will keep getting better. I am closing this issue as we don't plan to do anything special in TensorFlow for this.
  The link from the website actually points to the r0.7 version of the branch: https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py, so I'm not sure what else we can do here -- it is generally recommended (for all software), to match the binary install you are using with the branch that matches it.
  @martinwicke: why don't we just host this ourselves on tensorflow.org instead?
 No reason apart from fear of accumulating things requiring maintenance.
Gmock is stable enough. We can add this along with our hooks.
On Mon, Mar 28, 2016 at 20:17 Vijay Vasudevan notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke: why don't we just host
> this ourselves on tensorflow.org instead?
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1687#issuecomment-202686300
 @fayeshine: can you try accessing https://storage.googleapis.com/download.tensorflow.org/deps/gmock-1.7.0.zip or is that also blocked?
 Well that's unfortunate.  I think openswitch is okay for now, but we should probably find a better long-term solution.  One option to use: https://github.com/google/googletest, if that works, since that's the more-or-less official github one now
 (Specifically, using git_repository instead of http_archive, and changing the build_file to something that works for that github repo, rather than the hardcoded 1.7 paths)
 @tensorflow-jenkins: test this please
  From a quick look at the Slurm docs, it looks like it could well support running TensorFlow, since (i) there's [decent support for running Python jobs](https://support.nesi.org.nz/hc/en-gb/articles/207782537-Python). I don't have access to a Slurm cluster, so I'm going to tag this "Contributions welcome", but I'd be happy to work with you on this Issue to get your cluster running on Slurm.

Here's how you could get started:
1. Create a Slurm job that sets up a Python virtualenv, installs a recent nightly PIP package, and runs a Python script. (Version 0.8 should include distributed cluster support.)
2. In your Python script, parse the `$SLURM_NODELIST` environment variable to get a list of the hosts involved in the computation.
3. Similarly, extract the [`$SLURM_STEP_RESV_PORTS` environment variable](https://computing.llnl.gov/linux/slurm/mpi_guide.html#open_mpi) and somehow decide what port to use in that range.
4. Create a `tf.ClusterSpec` based on the information from the environment variables, and use that to create a `tf.GrpcServer` (documentation coming soon; see [`server_lib.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/server_lib.py)) in each process.
5. Define a TensorFlow graph that is distributed across the nodes in your Slurm job.

Let me know if you have any questions!
  Rolled back in #1683 -- the version string was not what I expected it to be.  I'll roll it forward with the correct change soon.
 #1685 for the improved version that works
  @tensorflow-jenkins: test this please
 Thanks, I'll merge after testing.  Unclear to me why the existing commit passed our tests :(
    There seems to be 2 issues, one is directory permissions, and the other is that the whl file is not compatible on your platform. Do you mind trying the pip install under a virtualenv to eliminate the first problem?

```
$ virtualenv -p `which python3` py3env
$ source py3env/bin/activate
# No sudo
$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl
```
  @tensorflow-jenkins test this please

(no, sadly only we can kick off tests)
 well, that didn't work :(
  This is intended behaviour. The shape input you are passing to `tf.random_normal`, namely, the output of `tf.shape` function, is only available at runtime. Hence the static analysis is unable to infer the shape. You can consider setting the shape of `y` explicitly using the [`set_shape`](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Tensor.set_shape) function.
 `x.get_shape()[1]` ?
  Just a quick suggestion: have you tried adding `writer.flush()` or `writer.close()` after `writer.add_summary(...)`? The `tf.train.SummaryWriter` class writes events to a file asynchronously, so if you exist immediately after writing the event, it may not appear in the log.
 You could try using a [`tf.train.summary_iterator(filename)`] in a separate Python program to scan over the contents of your event files, and check that the expected events are in the file.
 When you run the `tf.train.summary_iterator()` does it only show those two records, or do you see any records that contain `wall_time:` and `summary:`? (The log you showed doesn't contain summaries, and so TensorBoard would not generate any plots, but it should show the graph visualization.)
 @jaycode: If you think the logdir isn't getting through properly, you can test that by launching TensorBoard with the --debug flag. [See details here.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#my-tensorboard-isnt-showing-any-data-whats-wrong)

I think this might be a duplicate of [#1421](https://github.com/tensorflow/tensorflow/issues/1421). It seems that there's an issue with open-source TensorBoard sometimes not loading any data from events files. The trouble is, I've never been able to reproduce this on any of my machines. A co-worker had it reproducing once, but it disappeared when he re-installed the package. I now think it might be consistently repro'ing on docker, so I plan to look into that next.
  TensorFlow doesn't have this operation currently, and doing it with existing ops is probably going to be clunky. Do you mind describing the operation slightly more formally, so that it can act as a specification for someone who can pick it up to implement it? Also, please post it to StackOverflow, in case someone in the larger community has an idea to implement it using existing TensorFlow operations.
 I think you probably need to write a custom op for this, if you want it to be at all fast.

In any case, stackoverflow is probably the right venue for this question. Thanks!
  2966fcd adds support for most of what you want. Assigning to @sherrym for further updates.
 286c5a4 added support to print the content of a tensor.
 c85b942 added a python-based tool for inspecting checkpoint.

To print shape information:
  inspect_checkpoint --file_name=model.ckpt-4971

To print contents of a particular tensor:
  inspect_checkpoint --file_name=model.ckpt-4971 \
    --tensor_name=mixed_17x17x768e/branch7x7dbl/Conv_2/weights/ExponentialMovingAverage

NOTE: If you see this message, it's likely that your checkpoint file
has been compressed with SNAPPY:

  "Data loss: Unable to open table file /tmp/model.ckpt-4971: Data
  loss: corrupted compressed block contents: perhaps your file is in a
  different file format and you need to use a different restore operator?
  It's likely that your checkpoint file has been compressed with SNAPPY."

To work around this issue without regenerating the
checkpoint files for now, you can rebuild by defining SNAPPY in
tensorflow/core/platform/posix/port.cc and link "lib_internal" with snappy
(-lsnappy) in tensorflow/core/BUILD.
  The attention mask is available as a tensor here : 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L522

It should be easy to fetch it out during a run call and visualize it. You can try posting this to StackOverflow  to see if someone in the general community has done this visualization. I am closing this issue, since we have the required functionality in TensorFlow.
  This is a useful change, but can you make it to the master branch instead of r0.7 ? 
  `log10` can be constructed by composing existing TensorFlow operations, as below : 

```
def log10(x):
  numerator = tf.log(x)
  denominator = tf.log(tf.constant(10, dtype=numerator.dtype))
  return numerator / denominator
```

Closing this issue as we don't plan to support this natively.
  Will take a look tomorrow.
 Thanks for the hard work!  Some comments.
 Without a unit test that calls tf.nn.rnn or tf.nn.dynamic_rnn, it's not clear that these cells interact correctly with those methods.  Can you add such a test for your classes?
 Will look tomorrow-thanks!
On Apr 6, 2016 11:36 AM, "Vu Pham" notifications@github.com wrote:

> I added the tests for Grid1LSTM, Grid2LSTM, Grid3LSTM (with ReLU),
> trained with tf.nn.rnn. Can you take a look?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1665#issuecomment-206506910
 Jenkins, test this please?
 Sorry for not reviewing earlier.  Let's retest this and we can get it merged.
 @phvu can you rebase on HEAD and run the tests?
 @ebrevdo: why does he have to rebase to HEAD?  Do you think there will be a conflict in doing so?

Also, only we can trigger tests.
 @tensorflow-jenkins: test this please
 Excellent, we're good to go.  Thanks for the contribution!  If you have any example code using this we can consider adding it elsewhere in the repo (e.g. under models).
  Did you try git submodule update --recursive?
 Yes, you need an updated version of the protobuf submodule to support grpc at head.
  @jerabaul29, generalizing convolutions to higher dimensions would be a great addition to TensorFlow, especially the GPU implementation. It will be great if someone in the community can take this up.
 https://github.com/tensorflow/tensorflow/commit/6a187ccddaebb741ea77fc3201c6e36625f0aadb for an example of adding 3d convolutions, which uses cudnn Nd tensor interface and eigen for CPU.  Something similar could be done for conv4d, conv1d, etc.

If someone wants to tackle this, please let us know (cc @rewonc), since we now have conv2d and conv3d to provide a template for generalization.  We may still be missing something though, so no guarantees it will be straightforward.
  This does look correct.  Send a PR.  Make sure the rnn tests continue to pass.
 I'm assuming this can be closed since #1691 is merged now.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks for the contribution!  IANAL, but I don't think this is needed -- you have to maintain at least the copyright year of the first distribution (2015).  Some resources online say it is okay to update on major updates / releases though, but I'd rather just keep the original year unless lawyercats suggest otherwise.   git history is a more accurate source of truth than what we put in this file anyway ;)

@martinwicke might know some lawyercats who say otherwise, so we can re-open if needed.
  Hm, that is indeed weird.  Not sure why CUDA_VISIBLE_DEVICES isn't helping you.

Can you try the following in the meantime?

```
config=tf.ConfigProto(device_count={"GPU": 0, "CPU": 1})
tf.Session(config=config) 
```
 I think it would be nice to figure out what's causing the actual failure at the cuda level, but it's surprising to me that this code is being reached at all -- I think that's the bug.  Basically, we need to figure out why "GPUMachineManager()" is ever being called when no GPU devices are requested -- that's the entry point that tries to do GPU initilization.  I'll try to find time to debug this.
 Okay I partially take this back.  It is weird that the stream executor code is called at all, but I don't think that's the cause of the problem.  I tried reproducing your problem at HEAD, spawning 48 processes with CUDA_VISIBLE_DEVICES="" and I didn't get any std::system_error.

My guess from looking up the error is that each session is creating a number of std::threads, and at some point one of them blows up.

Another thing to try for debugging is to set the intra_op_parallelism_threads and intra_op_parallelism_threads in the ConfigProto to smaller values (they default to the number of cores on the machine).

As for why this causes problems only with the GPU pip, I'm not entirely sure yet :)
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please.
 @tensorflow-jenkins: test this please
  Is the C++ API described here [1] not sufficient to you for some reason?

[1] https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html
 This kind of question probably belongs on StackOverflow, unless it is a concrete feature request.
  https://github.com/AKSHAYUBHAT/VisualSearchServer might be a good example to look at. (StackOverflow would be a better location for this question, in the future).
  @girving: glennrp maintains libpng so this seems like a trustworthy github dependency -- what do you think?
 Seems fine to me.  Note that the only risk of attack occurs when we change the hash.
 Jenkins, test this please.
  @fchollet, can you please take this?
  The existing implementation focuses equally on being readable/understandable/hackable, and being performant. Please see the documentation [here](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph.device) for how to place variables/operations on specific devices. Closing this issue as the functionality to do what you want is already present in TensorFlow.
 That's correct.
  @andrewharp since he's been dutifully maintaining this target

@damienmg because I don't understand why our Android test target is fine but according to Jenkins but users still run into this problem :)
 @darrengarvey Hi, are you building tensorflow/core:tensorflow_android_lib directly (as opposed to //tensorflow/examples/android:tensorflow_demo)? If so, you need to tell Bazel to actually build for Android, e.g. with the flag --fat_apk_cpu=armeabi-v7a

@vrv There's a select in android_tensorflow_lib_lite that nullifies attempts to build for non-Android, but not for android_tensorflow_lib. This is probably what's causing the misleading errors people are seeing. Jenkins always builds in an Android configuration so it's not catching this. I'll send a CL to fix this.
 @darrengarvey You'll need a couple more flags in addition to fat_apk_cpu if you want to build the library by itself. Here's what worked for me:
`bazel build -c opt tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain`
 Note that if you are building this for an app you'll probably want to wrap the cc_library in a cc_binary (see tensorflow/examples/android/BUILD for an example) so that unused code can be stripped out. Otherwise you'll end up with over 200mb of unnecessary size. Just make sure you export the symbols you want to reference so they don't get stripped as well.
 I've submitted an internal patch that will nullify the src for android_tensorflow_lib in the same way as android_tensorflow_lib_lite. I agree that an informative error message might be ideal, but I'm worried about erroneous failures in automated builds and tests (not everything we run TF through seems to respect manual and notap at this point). As a compromise I added a comment on the targets specifying the targets must be built for Android, and how to do so.
  Thanks for the fix!  Would it be possible to introduce a test that exercises this codepath, so we can prevent it from regressing?
 @tensorflow-jenkins: test this please

(well, add a test later if you can, this is better than nothing)
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins: test this please.
 @tensorflow-jenkins: test this please
  This sounds like a version error with the `six` library: can you please run the following code and reply with the result?

``` python
import six
print(six.__version__)
```
 Can you also try the following?

``` python
print(dir(six.moves.queue))
print(six.moves.queue.__file__)
```

I have tried `six` versions `1.4.1`, `1.5.2` and `1.10.0` (in Python 2.7 and 3.4) and these all have `six.moves.queue.Queue`.
 I believe this is related to Python's confusing import semantics, which make problems likely if a local module shadows the name of a standard library one. The easiest fix is probably to rename your local `queue.py`, or move it into a submodule (I can't tell from the path if your `queue` module would be imported as `queue` or `Hand.queue` in your program; if the former then I think you'll need to move it).
 (Closing this since the issue seems to be related to local configuration.)
  This seems like a reasonable feature to add in `contrib/...`.
  I've done some tests of this, and it seems like the Nexus 5X is capable of Nexus 5-comparable performance when the 2 A57 cores help out with the processing. Which, unfortunately, is rarely. The majority of the time the bulk of the processing is done by the slower (but more energy-efficient) A53 cores.

You can see this behavior if you run a systrace while the demo is running and look for the "Recognize" calls. e.g. `python $SDK_ROOT/platform-tools/systrace/systrace.py --time=5 -o nexus5x.html sched gfx view wm --app=org.tensorflow.demo`

Over 25 runs I saw a min demo inference time of 285ms, a max of 642ms, with an average of 491ms.

I also looked at the per-operation performance (by compiling with -DLOG_DETAILED_STATS), and found the worst offenders for lower performance by far were the convolutions. For example,                    conv2d2_pre_relu/conv took an average of 40ms on the Nexus 5, but a whopping 106ms on the Nexus 5x.
 If you can figure out how to force all the cores to activate it should be possible to increase the speed. Though I expect thermal throttling will kick in fairly quickly and it will be impossible to maintain the performance over a sustained duration.
 @maciekcc may have more insight into this.
 When I ran a standalone benchmark of the same model (coming soon), both devices improved their performance considerably.
5 standalone benchmark: 238ms
5x standalone benchmark: 315ms

For reference, here's the performance I saw in my last run of the standard TF Android camera demo:
5: 330ms
5x: 565ms

The standalone runs perform much better than the app version in both cases, but considerably moreso on the 5x. This implies the overhead of running the user-facing Android app differentially affects the 5x, in addition to or magnifying the speed discrepancy caused by the CPU setup. 
 I just tried setting kDefaultCores to 6 on the 5x, and got 501ms/run in the TF demo app. Surprisingly there was no improvement at all in the standalone benchmark, as it ran in 316ms/run.
 Further testing using the standalone benchmark tool reveals that adding a delay between inference passes of 1 seconds gives an per-inference average of 427ms over 50 runs.

This is compared to a 321ms result on an otherwise identical test with the inter-inference delay set to 0.

I'd speculate that in the Android camera demo, the delay between receiving frames to process is significant enough that the processor scales down, and needs to ramp back up once inference starts again. A busy-loop in the inference thread between frames might help with that behavior.
 @andrewharp @petewarden What's the status of this? 
 The standalone benchmark target tensorflow/tools/benchmark:benchmark_model has been added which will allow users to repeat their own experiments.

We're still evaluating compilation flags to get the best performance out of mobile builds. As far as the 5x in particular, I think we have viable explanations for the performance difference (asymmetric cores, etc) but no simple solution to improve it right now.
 @andrewharp Should I leave it assigned to you or Pete, or mark it contributions welcome if there is something specific we could ask others to contribute?  We're trying to avoid leaving nonspecific bugs open. 
 I don't think we have any specific plans at the moment to improve 5X (relative) performance, so this should probably be closed. Happy to test out any contributions, though.
  So that I understand, how does the old description imply the gradient is a scalar?
 Jenkins, test this please
 @tensorflow-jenkins: test this please
 The line `x_with_bias = np.array([(1., a) for a in x]).astype(np.float32)` makes the simpler formula correct. Your new description is inconsistent with the simplified formulas above.  If you want to clarify that `x` contains a 1 entry corresponding to the bias, just add a parenthetical comment like "(Note that the first entry of `x` is 1 corresponding to the bias)`.
 Very true!  Let's use `x_with_bias` in the formula; that even avoids the need for an extra comment.
  Can you squash these commits to get rid of the merge commit?  Then I'll test and merge.  Thank you for the fix!
 @girving: merge commits don't show up in our history, so you can ignore those.  Only real commits show up in the final tree.

@tensorflow-jenkins: test this please
  Looks like you left a couple of `mu` references in the file.  Can you get rid of those too?  Also, please rebase and squash to remove the merge commit from the pull request.
 Yeah, we should be uniform throughout the page.  `learning_rate` is more verbose but obviously less confusing as a name.
 Thanks!  Jenkins, test this please.
 I should have double checked the merge commit - it was bogus.  You can send this PR in again with the right content!
  It's a bug in _GetGradSource.  we have a fix incoming, probably a few days till it shows on github.
 Can you verify this is fixed for you at HEAD?
  Thanks!  Jenkins, test this please.
 The failure looks like a flaky test on our end.  Investigating.
 Jenkins, test this please.
 @keveman: Do you know might be causing this protobuf pip test failure? 
 @vrv apparently knows the issue.
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please  (PLEASE)
  This isn't enough information.  From what you've written so far, maybe you typed ^C to kill a process?
  Thanks! Just 1) a tiny comment and 2) make sure your branch is up-to-date with master and you are good to go.
 @tensorflow-jenkins: test this please
  I'd suggest `tf.pack` or `tf.concat`.  If you want more details, please ask questions like this on stackoverflow; issues are for bugs in tensorflow.
  Compiling this will require the top-of-tree version of Bazel, until it makes it into the released version in about a week.

To compile the iOS example, run the following command:
`bazel build -c opt -s //tensorflow/examples/ios:ios-app-binary-cc --ios_sdk_version=9.2 --ios_cpu="arm64"`

You will need to put your mobile provisioning certificate in tools/objc/default_provisioning_profile.mobileprovision, and the Inception v3 model file and labels in a data directory inside tensorflow/examples/ios/data, e.g.
`ls tensorflow/examples/ios/data/`
`grace_hopper.jpg  imagenet_comp_graph_label_strings.txt tensorflow_inception_graph.pb`
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 @petewarden The failures still show bazel 0.2, are the new docker files not pushed out yet? Or did something go wrong with the bazel update? Or is this just a too old test?
 @tensorflow-jenkins: test this please
 This was a recent test, I need to debug what's going wrong on the Jenkins server. The install script does appear to be updated:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_bazel.sh
 @tensorflow-jenkins: test this please
 I just checked, and at least on the Mac, bazel 0.2.1 is definitely installed
 You have the same error in linux build with 0.2.1 as well as on mac where is 0.2.0.

@martinwicke you have checked only mac0-slave, right?

@caisq can you upgrade bazel on mac1-slave, please?
 Oh, right, I have only updated mac0-slave to 0.2.1.
 I've installed the 0.2.1 binaries locally, and it seems like the fix we need didn't make it in, despite being in top-of-tree ahead of time! :( I'm following up with the Bazel team to find out what happened, and what we can do to fix this.
 We're currently blocked because the top-of-tree protobuf is broken, and we need that for iOS support. I'm working on fixing that right now.
 @petewarden is this PR still active or can it be closed?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 @vrv: Looks like @concretevitamin doesn't have test permissions?
 (he is working on that, I think ;)  He's the test case!)
 @tensorflow-jenkins : test this please
  Jenkins, test this please.
  How did you ensure that "Save two sessions independently from each other"?

Sherry
 When restoring, could you please create a new graph for each of your sessions instead of sharing the same one? Something like this:

self.graph = tf.Graph()  # Creates a new graph.
tf.Session(graph=self.graph, config=self.tf_config)

Sherry
  @mrry is probably the right person for grpc questions.
 @tensorflow-jenkins: test this please
 Should be fine as long as (i) it builds, and (ii) the tests pass. I'm not aware of any breaking API changes in gRPC since we first pinned to a commit.
 The timeout look suspicious but is unrelated to this change (and will be fixed when one of @vrv's patches gets in). I'm going to merge this now.
  Looks good.  Please rebase and squash to avoid all the merge commits.  We don't like them in PRs.  Once you do that, I can test and merge.
 @tensorflow-jenkins: test this please

(all the extra commits are merges).
 No need to rebase / squash :)

@tensorflow-jenkins: test this please
  Already merged as #1617
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  @vincentvanhoucke: feel free to re-assign to reassess
 @fayeshine I agree that it makes more sense to introduce dropout in a context where it actually helps performance. Good catch! Adding it to a later tutorial sounds like a good idea. Another (possibly simpler) approach would be to modify the model architecture in the MNIST for Experts tutorial in such a way that dropout actually provides a benefit.

I don't think we should devote an entire tutorial to dropout. The goal of the tutorials is to teach people how to use TensorFlow rather than how to do ML research. We definitely want our examples to be scientifically correct and relevant, but we want to focus on the code. In the case of using dropout the code consists of the one function tf.nn.dropout(...), so an entire tutorial would be overkill.

Feel free to bounce ideas or an outline of your planned changes off of me!
 Closed since you've opened up other PRS
  @cuiguoxin, you might have misunderstood the code. The code is initializing the backprop state starting from y(s). So it needs to look at y's inputs. If you think it's wrong,  do you mind constructing a test case to show it?
  Could you please elaborate on your usage model and expectations of these operations? Thanks.

Sherry
 @fayeshine: Since we have a confusion matrix op as of #1999, is this obsolete?
 @fayeshine: Note that `confusion_matrix` was implemented on top of existing TensorFlow ops.  Does that mechanism suffices for your use cases?  If not, can you propose a specific semantics of what you want?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Jenkins, please test this!
 @dongjoon-hyun Thanks for your commit, can you rebase master and I'll merge?

Thanks!
 @ilblackdragon: I can merge if you'd like (no testing needed it seems)
   We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 @tensorflow-jenkins: test this please.
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
  I've already fixed it.
  Already fixed.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 Already fixed. Thanks!
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
  There are Bazel compatibility issues with r11 currently, so we suggest you stick with r10e for the time being. See https://github.com/tensorflow/tensorflow/issues/1468 for details and links to relevant r10e downloads.
 @andrewharp: is this a bug in bazel or a bug in TensorFlow?  If the former, have we filed a bug with them?
 Due to the nature of the errors people are seeing (like missing RELEASE.TXT files) I'm assuming Bazel. I'll see if I can reproduce with the latest Bazel and NDK to make sure.
 Ok thanks!  @damienmg in case he's curious :)
 @Muaazbinsaeed 
It would be possible to make Tensorflow build with ndk-build/Gradle, this just isn't something we can maintain right now.

If you look at https://github.com/miyosuda/TensorFlowAndroidMNIST/find/master, this is an example of linking Tensorflow into a Gradle/ndk-build environment. So you might have luck by compiling tensorflow/core:android_tensorflow_lib into a static .a library, then dropping it into your Gradle project.
 Closing, @ahumesky added NDK 11 support in https://github.com/bazelbuild/bazel/commit/abdaff492440b373bacd016d772ef73611a27901
  Fixed by #1999.
  Yes, this sounds like it requires custom kernels to have reasonable performance, though I don't see the word "pointer" in the paper so I can't confirm.

In terms of collaboration, do you want to post on the discuss@tensorflow list asking if anyone wants to collaborate?  It's a good use of the list, and most people outside the team are unlikely to notice a Github issue.

Note that we are doing some work on better sparse ops (e.g., @ebrevdo's https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc), but I don't think sparse convolutions is on our radar just yet.  It would be a good thing to have, and I'd be happy to answer questions if you do end up working on it. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  @awni Thanks for the fix. As it happens, I have the same fix in #1608 that is farther along in the tests. Do you mind if I drop this one? #1608 has other commits and extricating changes to the one file would be painful.
  Fixes #1595.
 Jenkins, test this please.
 @tensorflow-jenkins: test this please
 Jenkins, test this please.
  This is a huge diff: +782 −754. Can you make it such that the only change is that one line?
 Merged. Thank you!
  Andrew, does this look familiar?
 @Kuntal-G Which version of Bazel are you using?
 WORKSPACE parsing should be just about the first thing that happens when building, so make sure there are no other changes introduced there that could be causing an issue.

Also, other users have had problems with a trailing slash on paths, so I'd try removing the one on your ndk path and see what happens.
 Glad you were able to find a workaround. Closing this now as I can't reproduce myself. Please reopen if you try again and still experience issues.
  My own setup doesn't require it, so I can't easily test it. Would you mind sending me a pull request with the change? I'll be happy to merge it.
  Just to check: have you installed `swig` on your machine? It has to be installed manually (via `apt-get`) and is reponsible for producing that file. (I would have expected a different error message though.)
  @hassanabidpk What version of Bazel are you using?
 Where did you get Bazel homebrew from? I'd suggest sticking with a vanilla default Bazel install if you're experiencing problems.

This is the complete output I see when I type bazel version:
`$ bazel version
Build label: 0.2.0
Build target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Feb 23 13:11:56 2016 (1456233116)
Build timestamp: 1456233116
Build timestamp as int: 1456233116
`

Also, what did you set your ndk path to in the WORKSPACE file?
When you run `ls -la $NDK_PATH/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-gcc`, what do you see?
 @hassanabidpk have you tried @kmader's solution?

@ahumesky Do you have any guess why it's trying to use the non-existent darwin-x86 toolchain here? Any system running El Capitan should be 64bit from what I understand. I've not been able to reproduce on my macbook running OSX 10.11.4 -- it uses the darwin-x86_64 toolchain automatically.
 Just noticed that you built with --config=android_arm. This should not be necessary (or do anything, but it's worth checking to see if it's what's causing the issue). It was included in some previous directions by mistake.
 @ahumesky Did we ever figure out the root cause to this? If not, is this still relevant now that Bazel supports NDK 11?
 Seems likely, we can reinvestigate if it ever happens again.
  I think this would be awesome to have, and contributions are definitely welcome here :).  
 Or we could also implement them as individual OpKernels if it is too difficult to get it into Eigen.
 @bhack we have a set of Eigen extensions to better support quantized operations on tensors in https://github.com/tensorflow/tensorflow/tree/master/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint. It's definitely possible to use the same approach to package a XnorGemm operation.
I can also talk to the other maintainers of Eigen to check if it makes sense to add the code into the core Eigen and make it more widely available.
 I have been looking at 'popcount' for binary networks, as bitcount is often known, since that seems to be the trickiest part to map to processor instructions. There is some BSD-licensed work here:
https://github.com/WojciechMula/sse-popcount
Interestingly the x86 CPU instruction seems to be competitive with SSE implementations. It looks like ARM requires a multi-instruction macro though:
http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0081b/CHDJJGAJ.html
  We don't currently have any plans to implement a slice op on SparseTensor.  My suggestion would be to first implement a sparse->sparse slice, and then use sparse_to_dense to densify it.

The closest thing we have is a sparse split; the meat of which is implemented here:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/sparse/sparse_tensor.h#L437

If you're interested in implementing a SparseSlice, contributions are welcome.  The signature should probably be:

```
SparseTensor SparseTensor::Slice(
  const SparseTensor& input_tensor,
  const gtl::ArraySlice<int64>& start,
  const gtl::ArraySlice<int64>& size)
```
 @concretevitamin: Is slice one of the ops for which you're implementing SparseTensor support?
 mdan is implementing SparseTensor slice this week.  I think there's some
code already done.

On Mon, May 16, 2016 at 11:25 AM, Derek Murray notifications@github.com
wrote:

> @concretevitamin https://github.com/concretevitamin: Is slice one of
> the ops for which you're implementing SparseTensor support?
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-219504264
 I am not actively working on this op.  Happy to review any PRs ;)

On Monday, May 16, 2016, ebrevdo notifications@github.com wrote:

> mdan is implementing SparseTensor slice this week. I think there's some
> code already done.
> 
> On Mon, May 16, 2016 at 11:25 AM, Derek Murray <notifications@github.com
> <javascript:_e(%7B%7D,'cvml','notifications@github.com');>>
> wrote:
> 
> > @concretevitamin https://github.com/concretevitamin: Is slice one of
> > the ops for which you're implementing SparseTensor support?
> > 
> > —
> > You are receiving this because you were assigned.
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-219504264
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-219516188
  Added a note about compiling user op library with gcc 5
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I signed it!
  Could you try passing in the following argument:

bazel-bin/tensorflow/examples/image_retraining/retrain --train_batch_size=1 --image_dir ~/flower_photos

Sherry
 I looked into this, and it appears to have broken when the new shape strictness checking went into effect:
https://github.com/tensorflow/tensorflow/commit/9a8c5ad18c61cb0695d31e2ce969008c82999c7c

Unfortunately, the input we're feeding in is not a placeholder, but is an override of a regular node that's loaded from the graph def, so it's not clear from the examples how to fix the error in this case. Passing over to @mrry for advice on the best way to fix this.
 Right, you need to replace the node `"pool_3/_reshape:0"` with a `tf.placeholder_with_default(tf.reshape(...), shape=[None, 2048])`.

Would one of you be a dear and show me where this graph is created? Then I can send a PR or make the appropriate changes. If as I fear it's using a binary graph downloaded the website, we might need to mint a new graph.
 > If as I fear it's using a binary graph downloaded the website, we might need to mint a new graph.

Your fear is correct, and it's actually even worse, because we also need to save the result out to a binary file, so we'll have to do any swapouts twice. Here's the place where the loading is done:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L291

Here's where we save it out again at the end:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L820
 Oh, maybe it's not so bad then: in `create_inception_graph()` you can specify `{BOTTLENECK_TENSOR_NAME: some_placeholder_tensor}` as the `input_map` argument. That way, you can define the placeholder to be whatever shape you want it to be in `retrain.py`, and you won't have to modify the original binary graph!
 Thanks Derek! I'll see if I can get your help on this tomorrow, since I'm still a bit lost. I expected that changing input_map would mean that the original Reshape node would be inaccessible when I ran the full graph?
 Absolutely, let's talk tomorrow! (Using `input_map` would modify all consumers of the reshape node so that they took the placeholder instead. In the present implementation, the reshape node would actually still be in the graph, but nothing would be connected to it, and it probably wouldn't ever be executed.)
 We have just got a fix completed and approved, I should be able to check it in this evening. Apologies for the breakage!
 Sorry for the delay, I have this working on our internal repo, but there are some hiccups on exporting it to Github. I should be able to get this released on Monday morning.
 The fixed version should now be in the top-of-tree code. Please let me know if you're still having issues:

https://github.com/tensorflow/tensorflow/commit/3ca08f75896d733d12d7106f76abf51e782f50da
  I think this is "intended behavior" for calling `sess.run([])`: although its use is not recommended, it runs all of the ops in the graph. This would include the enqueue, dequeue, and close ops for the queue created in `tf.train.shuffle_batch_join()`, leading (non-deterministically) to the error you saw).

I don't think any well-formed programs rely on the behavior of `sess.run([])`, and it can't possibly be used in a program with a complex input pipeline, so we should probably make it an error.
  Jenkins, test this please.
  Try passing "https://github.com/tensorflow/tensorflow/blob/30b52579f6d66071ac7cdc7179e2c4aae3c9cb88/tensorflow/core/protobuf/config.proto#L35" set to true as an argument to your Session's config arguments, it might help a little, though it won't release memory, it just allows growth at the cost of some memory efficiency.

Alternatively, you could delete your session objects (which should release the memory associated with them) when you don't need them.
 ```
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.Session(config=config)
```
 Not sure, you could try invoking python's garbage collector?
 Well, I'm out of ideas :(.  Hopefully allow_growth is good enough for you. 
  @danmane: Do you have enough information to look at this? 
 I'm planning to replace the histograms with entirely separate code soon(tm), so considering this obsolete/won't fix.
  mistake?
  The gen_docs_combined script should build markdown, but I'm not sure about HTML.  @martinwicke?
 You cannot currently build html, but you can build the complete set of markdown, using [gen_docs.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/gen_docs.sh).
 Closing, please ping if you have trouble with gen_docs.sh.
  @tensorflow-jenkins: test this please
 Looks like one small test failure for tan -- can you fix and verify?  Otherwise looking good!
 @Mistobaan -- I see other people requesting it, this would be a useful addition
 Can you fix cwise_op_test failures?

```
INFO: From Testing //tensorflow/python:cwise_ops_test (shard 2 of 2):
==================== Test output for //tensorflow/python:cwise_ops_test (shard 2 of 2):
.............................................................F...
======================================================================
FAIL: testComplex64Basic (__main__.UnaryOpTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/cwise_ops_test.runfiles/tensorflow/python/kernel_tests/cwise_ops_test.py", line 250, in testComplex64Basic
    self._compareCpu(x, np.tan, tf.tan)
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/cwise_ops_test.runfiles/tensorflow/python/kernel_tests/cwise_ops_test.py", line 75, in _compareCpu
    self.assertAllClose(jacob_t, jacob_n, rtol=1e-3, atol=1e-3)
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/cwise_ops_test.runfiles/tensorflow/python/framework/test_util.py", line 431, in assertAllClose
    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)
  File "/usr/lib/python2.7/dist-packages/numpy/testing/utils.py", line 1183, in assert_allclose
    verbose=verbose, header=header)
  File "/usr/lib/python2.7/dist-packages/numpy/testing/utils.py", line 644, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.001, atol=0.001

(mismatch 100.0%)
 x: array([[ 0.00947874,  0.00274417,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],...
 y: array([[ 0.00947807, -0.00274181,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],...

----------------------------------------------------------------------
Ran 65 tests in 197.312s
```
 It's a strange failure, let's retry the test.

@tensorflow-jenkins can you test this please?
 the branch currently has conflicts, and yes, removing complex64 support for now is fine -- just leave a comment saying something was broken when you tried it, so others can pick up from where you left off.
 @tensorflow-jenkins, test this please.
 @Mistobaan: ...apparently not. @caisq, @ebrevdo, @martinwicke: can one of you please kick off the tests?
 @tensorflow-jenkins test this please.
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 Still seeing errors for complex64, and also for float16.
 Search the [console log](http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/890/console) for the string `ERROR:`
 If you can, please fix the tests and make sure you verify they are fixed via bazel test tensorflow/python:cwise_ops_test
 Probably becaues you aren't compiling any GPU code.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_gpu_abs.cu.cc -- no equivalent file exists
 @tensorflow-jenkins test this please
  @zheng-xq: Any chance this is fixed?  If not, I'm going to mark this as contributions welcome in case someone else gets to it before we do.  PRs welcome! 
 @alexeygrigorev, @ChuyuHsu, are you running the following example? 

https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py

It seems that it was using GradientDescentOptimizer, not AdaGradOptimizer. 
  @keveman  to verify
 Sorry about closing without a comment. I did verify before closing that I could successfully load the op, I don't see the undefined symbol error. I am on Ubuntu 14.04. What platform are you seeing this error?
 I tried the following :

```
$ pip install -U http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl
$ python2
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
>>> tf.load_op_library('/home/keveman/zero_out.so')
<module '18c058680842296ccc50a4eac3d8b763' (built-in)>
```

The one reason I can think of why the undefined symbol error could be happening for you is, pywrap_tensorflow.so is not dlopened with `RTLD_GLOBAL` for some reason. Look at `tensorflow/python/__init__.py` for these lines : 

```
_default_dlopen_flags = sys.getdlopenflags()
sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
from tensorflow.python import pywrap_tensorflow
sys.setdlopenflags(_default_dlopen_flags)
```

Try this on your machine : 

```
$ python2 -c "import ctypes; print ctypes.RTLD_GLOBAL"
256
```

Although I don't see why that would be different on ArchLinux.
 Can you try building with gcc 5, but with `-D_GLIBCXX_USE_CXX11_ABI=0` , like so : 

```
g++ -std=c++11 -shared zero_out.cc -o zero_out.so -I $TF_INC -fPIC -D_GLIBCXX_USE_CXX11_ABI=0
```
 Glad it worked. Added a note to the documentation. #1584 
  @prb12: Did you get a chance to look at this? 
 Could you please clarify what you mean by 
"Just run translate.py with 10M pairs training data." ?
Are you saying that you're trying to run with a batch_size argument of 10 million?

Could you please provide the exact command line you used, along with any changes to translate.py, and it would also be helpful to know what GPU configuration you have (e.g. the output of nvidia-smi)
  Is it possible to delete the image entirely from the repo, and just have it be downloaded by the tutorial?  large images in git are bad :(
 Yikes, we shouldn't have had the notebook in here in the first place either :(.
 @tensorflow-jenkins: test this please
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Merged. Thanks!
  This is a question better suited for stackoverflow -- please ask there.
  The performance is completely dependent on the implementation of your custom op.
  I think this is a benign error, we'll try to maybe remove it.

@leary-google
  Assigning to @ludimagister for general API 
 Yes, this should be fine. Thanks for adding. 
 Can you squash the commits?  Then we'll test and merge
 Thanks!

@tensorflow-jenkins: test this please
 I think it's a 'flaky test' due to timeouts -- I think it's fine, so I'm going to merge.
  Thanks for filing this with a clear example and description of the issue! :+1:  If you feel like taking this on yourself and submitting a pull request, that would be very welcome
  @tensorflow-jenkins test this please
 Merged. Thanks. 
  Closing, since @cg31 seems to have found the fix.  Thanks!
  Yes, that's not a typo. :) just looks like it in source.
On Sat, Mar 19, 2016 at 14:22 Abhinav Upadhyay notifications@github.com
wrote:

> On second thought it seems it wasn't a typo. In which case I'm sorry for
> the noise. Just let me know and I will close the PR. :)
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1557#issuecomment-198786836
  Please squash the commits to one so we can test it. Thanks.
 @caisq: merge commits are ignored, so this is technically already squashed.

However, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/summary_io.py#L119 is the right place to make this change, since our metadata documentation is generated from the original source.  Can you make that change @davidkretch ?
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
  Jenkins, test this please.
 @tensorflow-jenkins: test this please
  Merged. Thanks!
  @martinwicke: Have you seen this one?  @YonatanSimson: Do you know if this is still an issue?  Apologies for letting it fall through the cracks.  
 Thanks @YonatanSimson!  Closing for now since it seems fixed. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Looks like an accident, closing.
  Closing issue as it sounds like it has been resolved.
  thanks @ecobost for the correct info :)
i've added information on this to [the TensorBoard documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#runs-comparing-different-executions-of-your-model)
  @concretevitamin, do you feel comfortable taking a look? I can re-assign otherwise!
 (@vrv: late apologies for missing this.  It must be that I hadn't properly setup Github notifications when you pinged me.)
 closing due to inactivity -- feel free to comment when some more work has been done on this PR.
  Assigning @zheng-xq as the most expert on GPU performance issues.

It would help to know which version of the code you're using, because @zheng-xq recently made some substantial performance improvements, so these numbers might be different at HEAD.
 @brantbzhang, could you try the latest TensorFlow, which enables auto-tune on conv and see if that is faster? 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 This should be put into the models repo: https://github.com/tensorflow/models
  Hi, what exact command are you using to build? What versions of Bazel and the Android NDK are you using?

It appears you're having an issue linking in the dummy pthread target. Does it work if you comment out the libpthread.so dependencies in tensorflow/examples/android/BUILD, and also remove "-lpthread" from LINK_OPTS in google/protobuf/BUILD?
 Are you saying that using NDK r9 with the lines commented out, you get the same error as with r11 without having commented them out?

It seems like a general environment problem. Do you get any more info when you build with --verbose_failures? 

@lberki, do you have any ideas? It seems similar to https://github.com/bazelbuild/bazel/issues/698, but this is happening with Bazel 0.2.0
 Hi, are you still having this issue? Have you tried building with Android NDK r10e?
 The include warnings are expected and can be safely ignored, btw.
 Closing due to lack of activity; please reopen if you're still experiencing this issue.
  This sounds like an issue with your `pillow` (or possible `PIL`) installation. See the answer to [this Stack Overflow question](http://stackoverflow.com/questions/16612293/scipy-misc-imread-creates-an-image-with-no-size-or-shape) for a suggestion on how to fix it.

My guess is that a different version of some common library like `numpy` is being loaded when you load the modules in the two different orders, which leads to the confusion. This would seem especially likely if you had installed TensorFlow in a virtualenv, since TensorFlow installs its own copies of some libraries (including `numpy`), which might not be compatible with the system-installed version of, e.g., `scipy`.
 Which version of TensorFlow are you running? The 0.9 release candidate should have the fix.
  I believe that test should not be run on GPUs.  @zffchen78.

I believe those tests are specifically not in the gpu-enabled test set: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L1143

Do you know why it's trying to load the GPU for you?
 use --platform_suffix=clang_cuda for one build and --platform_suffix=somethingelse so bazel chooses different output directories for temp files, and then you can switch between them without problems :)

Building unconditionally with -DGOOGLE_CUDA=1 on every file is likely to be a problem -- you should see some of the tensorflow.bzl rules like tf_cuda_library to see how we conditionally build some targets with -DGOOGLE_CUDA
 Interesting. I don't have to set LD_LIBRARY_PATH and bazel test runs for me. However @vrv had the same problem you are facing yesterday and we were unable to figure it out :( Nothing is obvious or easy with bazel.
 @damienmg is there something we're missing?
 @zheng-xq Any idea about this one? We are having a similar issue running on AWS instances (inside a docker container).
 @martinwicke, yes, "bazel test" by default strips out LD_LIBRARY_PATH. What I recall from before is that you have to specify --spawn_strategy=standalone or --spawn_strategy=local should fix that. If @jlebar can confirm "--spawn_strategy=standalone" does not fix the problem, this might be a different issue.  
 Unless you've had bad luck with the particular commit you're at, all the tests should pass (see https://github.com/tensorflow/tensorflow/blob/master/README.md, build status at head is in there, or look at ci.tensorflow.org for details). The tensorflow-master-\* builds are the ones you want to look at, unless you're using a release. All tests should pass in a release.
 1) Are you building with -c opt too?
2) Pointers to logs would be helpful.
 So indeed, I can reproduce this locally too:  bazel test -c opt --config=cuda tensorflow/... results in those errors.  However, running the test directly:

```
bazel-bin/tensorflow/core/kernels/cast_op_test
```

works.  It used to be the case that standalone mode preserved LD_LIBRARY_PATH, but I guess no longer.  We'll look into it, but you do have direct-execution of the tests as a way to make progress.
 @jlebar: This issue has been quiet for a while. Did you eventually find a workaround, or is there still work ongoing to fix it?
  Is the difference in duration consistent when you perform multiple calls to `sess.run()` in a loop? As I mentioned in #1532, the first execution of a subgraph does not have a representative execution time, because it must perform various setup tasks.
 Closing this due to inactivity. Feel free to reopen if my previous answer didn't fix things for you!
  Yes, either use a virtualenv, or have the necessary version (currently 3.0.0b2) of protobuf installed system wide (via sudo pip install protobuf==3.0.0b2)
 FWIW, I think we need to find a better solution to our protobuf issues, in light of deeper issues like #1415
 Leaving this open until some solution emerges.
 @jlebar Can you please close this if you think that this has been addressed sufficiently in the latest TensorFlow?
  Hmm, this is probably wrong (we at one point had GPU devices created statically once per process).  I'll try to get a fix for this at some point soon.
 Ahh, good to know!  I had seen a report of this before and thought we were doing something wrong.  Glad to hear it's supposed to work in this fashion too :)
  Can you check whether the downloaded files are decompressible with `gunzip`?
 I'm assuming that [this question on Stack Overflow](http://stackoverflow.com/questions/36055090/zlib-error-error-3-while-decompressing-invalid-distance-code) is related. It does sound like something is corrupting your files as you download them (since `gunzip` isn't working). Can you try downloading the files using `wget` or `curl` and ensuring that they work this way?
  This looks like a very small layer, and you're only running it once. I suspect the cost is dominated by session setup costs and transferring the constant data to the GPU.

Try running `predictions = sess.run(local4)` in a loop to understand the average step time in the limit. This will give you a better understanding of whether some code is more efficient on CPU or GPU.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Is this still an issue since we moved from Yaroslav's domain to:
url = 'http://commondatastorage.googleapis.com/books1000/'
?
 If that's all right, I marginally prefer the simpler original code. It's more readable to novice coders. I might be proven wrong in case we hit other DOS protection issues, in which case we should definitely revive this patch. Thanks for your help!
  Bryan Hynds reported this problem using one of our tutorials, in these YouTube comments:
https://www.youtube.com/watch?v=h7xuEiZjqqo

---

I'm running into some problems with compiling.  I'm getting a compile error that's failing to build the target:

```
root@d121482d25c8:/tensorflow# bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain
INFO: Reading 'startup' options from /root/.bazelrc: --batch
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
INFO: From Compiling tensorflow/core/kernels/conv_ops.cc:
In file included from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Core:35:0,
                 from external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from ./tensorflow/core/framework/type_traits.h:22,
                 from ./tensorflow/core/framework/allocator.h:25,
                 from ./tensorflow/core/framework/op_kernel.h:22,
                 from ./tensorflow/core/framework/numeric_op.h:19,
                 from tensorflow/core/kernels/conv_ops.cc:22:
external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16> > >; bool Vectorizable = true]':
external/eigen_archive/eigen-eigen-0b9ab889fac2/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]
   EIGEN_STRONG_INLINE T& operator[] (size_t index) { return values[index]; }
                                                                   ^
INFO: From Compiling tensorflow/core/kernels/argmax_op.cc:
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
ERROR: /tensorflow/tensorflow/core/BUILD:358:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 79 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
Target //tensorflow/examples/image_retraining:retrain failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 424.939s, Critical Path: 412.00s﻿
```

I've wiped the container and rebuilt it several times, and I can reproduce it every time I reach this point.  I'm compiling with --verbose_failures now to get more details.  I'll post the log once I have it.﻿
 INFO: From Compiling tensorflow/core/kernels/argmax_op.cc:
gcc: internal compiler error: Killed (program cc1plus)

typically means gcc ran out of memory I believe -- there are ways to give more 'resources' to bazel, but I don't know the incantation offhand.
 Thanks Vijay! Bryan, how much memory were you able to allocate to the VM in VirtualBox when you did that step?
 I think you can pass -j 1 to prevent parallelism in the build that might help, or --ram_utilization_factor=50 or something lower than the default (67).  I don't know -- I'm grasping at straws at this point :(

(From http://bazel.io/docs/bazel-user-manual.html).  
 Great to hear you're finally running, and sorry it took so long. I will update the YouTube comments, make a note in the next version of the tutorial, and close this for now.
  Hi @Mistobaan, unfortunately there are conflicts that need to be resolved.
  This sounds like a versioning issue. Does upgrading to a nightly version of TensorFlow solve it? Assigning to @shlens in case he knows of a change to the model that could have caused this.
 I am not aware of any changes in this code base. @petewarden but might be able to comment as well.
 I think this change might be more likely to have caused the error you're seeing: https://github.com/tensorflow/tensorflow/commit/01a6f5e504d9299395888a786e52c589c16af529

If the graph for the model that you're retraining was regenerated _after_ that change, and you're using TensorFlow version 0.7.1, then I'd expect to see that message. I'm not sure of the provenance of the trained model graphs though. @oweingrod: Can you confirm how you obtained the graph (i.e. which download link)? @shlens or @petewarden: Would you know if the graph had changed since the 0.7.1 release?

Here are links to the Mac OS X nightly PIP packages: [Python 2](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) / [Python 3](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py3-none-any.whl)
 @shlens: did you regenerate the graph at the same time that you regenerated the checkpoint?  If so, you'll probably need to replace the graph with the old version.
 > I'm assuming that it is the case that there is a discrepancy between the version of tensorflow used by bazel to build the inception graph for retraining, and my (likely slightly more recent) build that I am using to classify?

I believe from @mrry and @shlens comments that you should only see the message if the version used for retraining was more recent than the version that you're calling in classify_image.py. Looking at the code, the data_format attribute was added to Conv2d back on February 19th:
https://github.com/tensorflow/tensorflow/commit/898d26a1cef46c0848188e22ba4b435846649b6d

I believe the other commit mentioned is a red herring, because it only affects pooling ops. So you'd have to be running code from before February 19th in classify_image.py to run into this error I believe. Can you try printing tf.version when you run your modified classify_image.py script?
 @martinwicke can you help on the version printing?
 I have a feeling the example was regenerated by accident using a newer version of TF that introduced new attributes.  If we used the old graph but a new checkpoint, I think things would work.
 I bet the new graph was built on a version _newer_ than 0.7.1.  My guess if you installed one of the pip nightlies off the main github page, you might have better luck, but we should really fix the graph.
 @shlens/@petewarden: deferring to you at this point, I've exhausted my knowledge of retrain.py :)
 @oweingrod What's the exact error that you see when you run classify_image.py?
 Great to hear, let us know what you end up building!
 @petewarden or @shlens Is there some way one of you could generate a graph that's compatible with the current release and upload it? I predict other people will run into the same issue, and telling people that they have to use the nightly isn't great....
 The problem was that the image_retraining example (which loads in an old graph and writes out a new one) was run with a nightly build, and then later classify_image.py was run on that new graph with an older build of TensorFlow. So I don't believe this requires any changes to the graph. I'll stop by your desk to make sure I'm making sense!
 It does make sense. Thanks Pete!
  This change looks good. I'm passing it to @vrv to merge.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Jenkins, test this please.
 @mheilman : what did you sign as, so I can check the status of the corp CLA?
 Try https://cla.developers.google.com/clas/new?domain=DOMAIN_GOOGLE&kind=KIND_CORPORATE ?
 @mheilman Hey, just wanted to update - corp CLAs take few days to process and I still don't see Civis Analytics in the list. Will keep checking on Monday. And thanks for contribution, hope next time will be smoother!
 Jenkins, please test this!

CLA looks good.
 @tensorflow-jenkins: test this please

Our bot doesn't yet understand natural language ;)
  We've fixed those internally, they should come out shortly. Sorry we keep stepping on your feet. Everything will get better once we have the python3 lint running :)
  At present this doesn't work, unless you write some complicated code that uses `tf.transpose()` and `tf.gather()` (euch). @ebrevdo is working on generalizing the slicing code as part of #206, so I'll merge this issue with that one.
  Jenkins, test this please.
 Jenkins, test this, PLEASE
 It looks like you branch might be taken from when the build was in a bad state. Can you please rebase to master and we'll rerun the tests? Thanks!
 FYI, you have to tell us when you rebase, github doesn't tell us :/

@tensorflow-jenkins: test this please.
 It only responds to admins right now, sorry :(.  Let me kick it off one more time.  And no worries about any ignorance, we're honored to be your first code contribution!

@tensorflow-jenkins: test this please.
 @tensorflow-jenkins: test this please (our tests are now passing so this will actually tell us something)
 every time we merge any other PR, this happens.  It is weird github behavior they recently introduced, because it means everyone would have to rebase after any merge, which is insane for projects that have 30 outstanding PRs.  

since i have admin powers, I can merge though :)
  @tensorflow-jenkins: test this please
  I think we've limited the number of dimensions to keep binary size down, rather than because it's any more complicated to broadcast on more dimensions. @benoitsteiner, is there some way we could handle higher dimensions in Eigen without another specialization, even if it leads to a slower codepath?
 @mrry If I'm not mistaken, the broadcasts we need in the examples above are [30, 1, 1, 20, 1] for the first tensor, and [1, 20, 1, 1, 1] for the second one. For the first tensor, we could collapse the 2nd and 3rd dimensions together, and apply a  [30, 1, 20, 1] broadcast. For the second one, we could similarly reshape the tensor into a 3d tensor by collapsing the last 3 dimensions, and apply a [1, 20, 1] broadcast.
  I'm not sure what semantics you're suggesting for `sess.status(...)` or `tf.train.status(...)`. If you're within the same Python process, you can treat a `tf.Session` like any other object.

A `tf.Session` is not serializable, but you can use the `tf.MetaGraphDef` protocol buffer, along with a checkpoint file, to represent an entire model.
 I think the answer is "probably no," but I'm still unclear on what you want the behavior of `????` to be. If you describe a high-level use case, we would consider a feature request.
 Ah, ok. Right now there is no way to copy an entire session, and the best way to do what you're asking about is to use the `tf.train.Saver` to write out a `MetaGraphDef` proto, and import that into another session using `tf.train.import_meta_graph()`. This is still quite a new feature, but you can look at [the relevant tests](https://github.com/tensorflow/tensorflow/blob/df2ea2c8fb6fc2a653843ec521adb52305f1679f/tensorflow/python/training/saver_test.py#L762) to see an example of how it's used. It _does_ give you the ability to restore an entire graph and resume training etc.

If there is any improved tooling you'd like to see around this format, please let us know in a feature request!
  This sounds like it might have consequences for backwards compatibility. @girving and @josh11b are our resident experts on that.
 Checking with Josh.  I think we may need an extra feature.
 @josh11b: In a perfect world the op spec would say you can't take the real part of a `complex128` and get a `float32`.  Reasonable to just ban that by not registering the kernels?

@ibab: If so, your solution is fine.  I'd change it to `Tout=in.dtype.real_dtype`, but otherwise good.  There will be no backwards compatibility issues since both defaults are the old values.
 I recommend writing a Python wrapper that sets Tout based on the input type.  We don't have a better mechanism than that at the moment.
 +1 to @josh11b.  @sjperkins: Since this can easily be handled at the Python wrapper level as @ibab described, we'd like to avoid complicated the low level GraphDef structure. 
  Are you asking about Windows support? (If I understand correctly, `.pyd` files are Windows DLLs.)

We don't currently support Windows. See issue #17 for a tracking bug for Windows support.
 Closing this as a duplicate of #17. Hopefully it won't be too much effort to support that usage model as well!
  Can you include a full stack trace? When I try to run your code I get a different error (also shape-related).
 (Assigning this to @ebrevdo, since he's most familiar with this code. I suspect he'll also need a stack trace.)
 Yes - stack trace is helpful.  But from the get-go looks like you're feeding a sequence_length scalar but should feed a [batch_size] length vector; perhaps [5, 5, 5] ?
 Great!  Closing this issue.
  - Now soft exit if cpuinfo/psutils not found.
- Uncomment test again.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Should I resubmit from my corporate email address?
 I signed it!
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 Jenkins, test this please.
 Jenkins, test this please.
 Jenkins, test this please?
 @ebrevdo can you please resolve the conflicts? Ideally rebase to new master?
 Bug fixed internally.  Closed.
  This seems like the intended behavior. You have a completely misclassified set of logits, and the cross entropy loss is very large.

What were you expecting to see instead?
  It's been over a month since the last activity, so I'm closing this -- feel free to re-open when you have something to update!
  Thanks! Just did the same without seeing yours. I'll merge yours (tests are in #1508). We'll need more fixed though.
 No problem. Always nice to have backup. :)
  I don't think this is the same as #1199, and I'm not sure why pip upgrade doesn't overwrite /usr/local/bin/tensorboard -- it might be a permissions issue.  Uninstalling the pip, removing /usr/local/bin/tensorboard and then reinstalling should solve the problem.
  @tensorflow-jenkins test this please
  I imagine we'd welcome contributions to https://github.com/tensorflow/models with graph embedding models. If there are any specific features missing from core TensorFlow that would enable these models to be expressed efficiently, please let us know!
 Adding contributions welcome in case someone feels like contributing a graph model to tensorflow/models.
  It looks like two commits were inadvertently squashed. Here's the intended description for the batch norm parts of that commit:

> General batchnorm op, suitable to both global normalization for convolutions and per-depth normalization of fully-connected layers. It also supports arbitrary layouts and orderings of the axes.
> 
> Nothing deep in these essentially 4 lines of code, but note:
> - The ops are ordered in a way that makes the computation much more efficient (thx Sergey). None of the existing wrappers that didn't use the fused kernel did that, and as a result were almost 2x slower according to the benchmark.
> - The benchmark confirms that this implementation is competitive with the handcrafted kernel.
> - I confirmed on inception that the step time or accuracy are not affected.
> - Added documentation and tests showing how the op can be used for arbitrary layouts.
> - The change in API compared with batch_norm_with_global_normalization makes it possible to not have a scale tensor when it's not used, which caused some confusion and unnecessary additional saved variables in the past.

@vincentvanhoucke is working on the related issue https://github.com/tensorflow/tensorflow/issues/1122 and might have more to add.
 If you're seeing a significant regression from the non-fused kernel, I'd be interested in hearing about it. Note that there were two changes:
- removing the fused kernel, which did not affect performance on any benchmark I could run. In particular, this one:
  https://github.com/tensorflow/tensorflow/blob/0249729a26b2cd3bdbbc5880f8d634a56860b0fd/tensorflow/python/ops/batch_norm_benchmark.py
- replacing the two-pass moment accumulation algorithm with a one-pass algorithm. This was a tad slower on GPU, and faster on CPU.

On balance, those two changes gave us a lot more flexibility:
- the one-pass moment accumulation removes a long-lived doubling of the memory footprint of the activations.
- it makes it possible to compute moments over long sequences more efficiently.
- the removal of the fused kernel generalized the computation to many more data shapes and layouts, and will enable for example batch-major and depth-major for convolutions.

We could recreate a fused op that implements the more flexible shapes if it significantly improved performance. It's also possible that a fused kernel for the one-pass moment computation wold be much more efficient. We need to know which is causing regressions first if there are any.
 @benoitsteiner, a few reports of very poor reduction performance for some layouts (see benchmark above). Can you triage or investigate?
 @ppwwyyxx @lukemetz if you could confirm which version you're using as well. I know that a lot of work has gone into improving reductions recently, so if you're not using the head of the tree I'd love to get confirmation that this is still an issue.
 In the first case (reduction axes = [0, 1, 2]) we have an optimized implementation leverages the gpu fairly well. In the second case (reduction axes = [0, 2, 3]), we use a basic approach that currently leverages few cuda threads to perform the reduction. This is why the second case is much slower. I'll see what can be done to improve the second case.
  Does the [`tf.contrib.ctc.ctc_beam_search_decoder()`](https://github.com/tensorflow/tensorflow/blob/b9e9c9a12dbd2946d44c76abdd603eedf6eedb9a/tensorflow/contrib/ctc/ctc_ops.py#L183) help here?
 Nope - ctc_beam_search_decoer is only meant to be used with the ctc_loss and not more general seq2seq models.  Sorry!  You'll have to write your own beam search for now.
  It looks like the encoded apostrophes are causing problems for rendering on OS X. Perhaps replacing them with `^\prime` would work?

Assigning to @martinwicke since he knows the website generator better than I do.
 Closing this as a duplicate of https://github.com/tensorflow/tensorflow/issues/1377.
We're working on a fix and it should be in soon - as a workaround for now, you can right click on the equation and select the SVG renderer.
  Thanks to @zheng-xq, we figured out that `compute_21` is not a valid architecture (instead it is a virtual architecture). We need to provide a way to pass a compute capability of **2.0** to `nvcc`. Currently our build scripts do not support that, but you can hack in support by editing the [crosstool wrapper](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc). After you rung `./configure`, replace the loop on [L253--256](https://github.com/tensorflow/tensorflow/blob/e4add493f1020c0eb986aba21c266d9e6e6f4182/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc#L253) with the following:

``` python
nvccopts += r'-gencode=arch=compute_20,\"code=sm_21,compute_20\" '
```
 @rajarsheem, you'll have to post the errors here. In general, TF is tested on compute_35. While compute_20 may work for now, which I am not sure, there is no guarantee that it will keep working in the long term.
 > > unsupported GNU version! gcc versions later than 4.9 are not supported!

This problem is that your GCC and NVCC is not compatible. Which Cuda SDK
are you using? Could you try the latest Cuda 7.5 and GCC 4.8, which are
heavily tested.

Thanks.
-XQ

On Mon, Mar 14, 2016 at 1:04 PM, Rajarshee Mitra notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq
> INFO: Found 1 target...
> ERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:212:1:
> error while parsing .d file:
> /home/rajarshee/.cache/bazel/_bazel_rajarshee/0d043bf46cad9f31127eb8d06453610d/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/gather_op_gpu/tensorflow/core/kernels/gather_op_gpu.cu.d
> (No such file or directory).
> nvcc warning : option '--relaxed-constexpr' has been deprecated and
> replaced by option '--expt-relaxed-constexpr'.
> In file included from third_party/gpus/cuda/include/cuda_runtime.h:76:0,
> from <command-line>:0:
> third_party/gpus/cuda/include/host_config.h:115:2: error: #error --
> unsupported GNU version! gcc versions later than 4.9 are not supported!
> #error -- unsupported GNU version! gcc versions later than 4.9 are not
> supported!
> ^
> Target //tensorflow/cc:tutorials_example_trainer failed to build
> INFO: Elapsed time: 1.447s, Critical Path: 1.19s
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196498691
> .
 I don't think GCC 4.9 is officially supported by Cuda 7.5. Look at
include/host_config.h

#if defined(**GNUC**)
#if **GNUC** > 4 || (**GNUC** == 4 && **GNUC_MINOR** > 9)
#error -- unsupported GNU version! gcc versions later than 4.9 are not
supported!
#endif /\* **GNUC** > 4 || (**GNUC** == 4 && **GNUC_MINOR** > 9) */

You are better off with GCC 4.8 and use that instead.

Thanks.
-XQ

On Mon, Mar 14, 2016 at 1:19 PM, Rajarshee Mitra notifications@github.com
wrote:

> I am using Cuda 7.5 and two gcc versions installled - 4.9 and 5.2. the
> cuda located in /usr/local/cuda has it's gcc _linked_ to gcc-4.9. (the
> gcc incompatibility issue earlier occured with theano and was solved after
> the linking)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196502623
> .
 I stand corrected. It is a good idea to check which version of your GCC was
actually used. A few things you can try:
- You can add "-s" to your bazel to print out the crosstools command line.
- Add "--copt=--cuda_log", which is passed to crosstools to print out the
  nvcc command line.

That should be enough to see which version of the gcc was actually used.

On Mon, Mar 14, 2016 at 1:42 PM, Rajarshee Mitra notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq upto 4.9 is supported.
> `#if _GNUC_ > 4 || (_GNUC_ == 4 && _GNUC_MINOR_ > 9)
> 
> #error -- unsupported GNU version! gcc versions later than 4.9 are not
> supported!`
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196512886
> .
 I think that might be the problem. The default gcc 5.2 is used instead. You
have a few options:
- Symlink your default /usr/bin/gcc to gcc 4.9.
- Modify
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc to
  point to your gcc-4.9 location.

On Mon, Mar 14, 2016 at 2:18 PM, Rajarshee Mitra notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq May this be of help.
> 
> http://paste.ubuntu.com/15387128/
> (note that default /usr/bin/gcc is actually the 5.2 one!)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196524722
> .
 Unfortunately, Eigen starts to pick up warp shuffle capabilities recently,
which requires compute_30 support.

On Mon, Mar 14, 2016 at 3:09 PM, Rajarshee Mitra notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq
> I changed all the variables below
> CPU_COMPILER,GCC_HOST_COMPILER_PATH and LLVM_HOST_COMPILER_PATH by
> replacing gcc with gcc-4.9. While the gcc issue seems to be vanished, this
> came:
> 5 errors detected in the compilation of
> "/tmp/tmpxft_00005de0_00000000-7_sparse_tensor_dense_matmul_op_gpu.cu.cpp1.ii".
> Full output:
> http://paste.ubuntu.com/15387425/
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196542169
> .
 @benoitsteiner to comment on the Eigen request.

@rajarsheem in this thread wants to support compute_20 device. However, since Eigen starts to use warp shuffling instructions, which is only available for compute_30 devices and later, it won't work.

Benoit can tell you whether Eigen will support older devices without the warp shuffling or not. 
 Eigen only supports compute 3.0 and above. Old GPUs tend to run slower than a decent CPU these days, so there is little incentive to remove this restriction at the moment.

@rajarsheem @marcdumon: What are your use cases for GPUs that only support compute capabilities 2.1 ?
  I would ask this question on StackOverflow, since this isn't a bug or feature request, thanks!
  @tensorflow-jenkins: test this please
  At present, TensorFlow only supports primitive NumPy datatypes. If you need to represent a tensor of a tuple of values, you can instead use a (Python) tuple of tensors (see e.g. the [`tf.QueueBase`](https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#QueueBase) documentation for an example).
  Yes, you cannot import tensorflow from anywhere inside tensorflow, except for inside tests.
 Jenkins, test this please.
 Any luck?
 Hm... I usually use virtualenv which makes this pretty straightforward. For maximum isolation from problems, you can try the ci_build scripts. They set up a docker container and install all the prerequisites inside. Then you can work inside there.

You do need the latest bazel, and the key is to have no incompatible protobuf and/or tensorflow installed. That's surpsingly hard to enforce, but virtualenv takes away most of the pain according to my experience.
 Do you want to update this PR? It seems it's a simple python syntax thing in test_util_test.
 @caisq: This fails only GPU_PIP, because tf.test is not available there, is that expected? It's possible we simply have to blacklist this test, but I may be missing something.
 Ping @caisq 
 ping @caisq @martinwicke 
 @caisq should we blacklist this test in the PIP build? Or should it be able to run?
 Closing due to inactivity, we can re-open if there's any more traction or debugging on this.  I suspect our testing team will at some point try to address these issues for load reasons.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (If you sign the CLA, comment here and we'll reopen)
  Nice!  Can you squash the three commits into one?  I'll merge right after.
  @tensorflow-jenkins: test this please.
 sounds good, thanks!

as for remote testing -- unfortunately only admins can trigger tests right now.
  Looks like this fix is now live, thanks!
  We're moving the data to a better host, stay tuned.
 It sounds like somebody enabled the filtering for a reason. And downloads from tensorflow could be that reason.

:+1: for a different host
 The new URL:
`url = 'http://commondatastorage.googleapis.com/books1000/'`
Let me know if that one is still an issue. I'll update the container soon.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  The only way to get the dynamic value of a tensor as a Python integer is to pass it to `sess.run()`. However, that doesn't really solve your problem, as you want to have data-dependent control flow in your graph. One way to do this is to upgrade to the latest nightly version of TensorFlow, and try the `tf.foldl()` or `tf.foldr()` functions, which operate like `scan()` in Theano to iterate over one dimension of a tensor.
 Indeed, for this case it would be more appropriate to use `tf.reduce_sum()`. However, if you need to define a recurrent network, where the input to an op depends on its output in a previous iteration in some non-trivial way, then the support for iteration will be useful.
 Assigning @yuanbyu, who might have more information about the current status of the control flow ops.
 You could use fold to achieve what you wanted to do:

```
x = tf.placeholder(tf.float32, shape=[None])
init = tf.initialize_all_variables()
sum = tf.foldl(lambda a, e: a + e, x, initializer=0.0)
with tf.Session() as sess:
  sess.run(init)
  for i in range(100):
    length = np.random.randint(0,10)
    a = np.random.randint(0, 10, length)
  print sess.run(sum,feed_dict={x:a})
```

If you want to split/unpack on a tensor with dynamic shape and do some computation on each subtensor, the combination of TensorArray and While() is the way to go.
 Closing this, since it looks like `tf.foldl()` is the way to go here.
  Unfortunately it's not trivial to change these names, because some code might have used them in explicit named arguments. I agree it would be nicer to have them consistent, so perhaps we could consider a breaking change at some point in the future.
 @rdipietro: Sorry this fell through the cracks.  I agree that `params` is an odd name, but it doesn't really indicate any special purpose: it's hard to think of a less specific name for a parameter than "params".

I'm going to close this for now since this bug is too general (it asks for all the parameters to be renamed).  I also don't think it's worth making "data", "params", "values", etc. be consistent, since they are all essentially meaningless names and no one should really be using them as keyword args.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 The Linux CPU test has failed //tensorflow/tools/docs:gen_docs_test. @iblis17 can you fix it please?
 @tensorflow-jenkins test this please
 LGTM, thank you @iblis17 
  Did you install from sources?  If so, which commit hash?

If not, which pip package do you have installed?
 I recently made a change to the gpu allocator, so I might have introduced a bug.  Can you sync to
f916ae47698c19c0fb3116e471abc39e9b788279 and try again?  I'd like to rule out that recent change from last night.
 FYI I just built from HEAD, built with 7.5/cudnnr4, and have so far run cifar10_train.py with no problems for 10000 steps -- I'll keep running it for a while to see if I can reproduce it.
 Interesting -- we should support r3 just fine (assuming you installed it from sources and said cudnn r3 during ./configure).  Thanks for the report, hopefully this is useful for others that run into this.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I already committed this, thanks.
  I don't understand, http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz is there, where does "403 Forbidden" error come from?
 Thanks for the report, looks like my ISP is finicky about User-Agent strings, rehosting it on Google Cloud, should get upstreamed soon
 Just checked http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz
and it works for me.
But future version will refer to alternative location at
http://commondatastorage.googleapis.com/books1000/notMNIST_small.tar.gz

On Sun, Mar 13, 2016 at 5:45 PM, Scott Lett notifications@github.com
wrote:

> I can't access the files on the either yarslavvb or google.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-196090146
> .
 I just tried accessing http://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz and it works for me
 What is the error?
On Apr 8, 2016 5:38 PM, "KSTseng" notifications@github.com wrote:

> But I tried to accessing by 1_notmnist.ipynb. It still give me the error.
> 
> `url = 'http://commondatastorage.googleapis.com/books1000/'
> 
> def maybe_download(filename, expected_bytes, force=False):
> """Download a file if not present, and make sure it's the right size."""
> if force or not os.path.exists(filename):
> filename, _ = urlretrieve(url + filename, filename)
> statinfo = os.stat(filename)
> if statinfo.st_size == expected_bytes:
> print('Found and verified', filename)
> else:
> raise Exception(
> 'Failed to verify ' + filename + '. Can you get to it with a browser?')
> return filename
> 
> train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)
> test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)`
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-207663388
 Looks like there's size mismatch. Can you see what size it is and also try
with force=true
On Apr 8, 2016 5:56 PM, "KSTseng" notifications@github.com wrote:

> ---
> 
> Exception Traceback (most recent call last)
> in ()
> 13 return filename
> 14
> ---> 15 train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)
> 16 test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)
> 
> in maybe_download(filename, expected_bytes, force)
> 10 else:
> 11 raise Exception(
> ---> 12 'Failed to verify ' + filename + '. Can you get to it with a
> browser?')
> 13 return filename
> 14
> 
> Exception: Failed to verify notMNIST_large.tar.gz. Can you get to it with
> a browser?
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-207672738
 Are you in mainland China?
On Apr 8, 2016 6:04 PM, "KSTseng" notifications@github.com wrote:

> I just re-run the code without modifying with force = true, and it works
> right now!
> I don't know why.
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-207675081
 Can you download it manually? I just clicked, and the download seems to be happening fine
  This problem doesn't seem to be happening with the [nightly build for Python 3](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py3-none-any.whl), and it looks like you're using an old snapshot of the tree from February 22nd. Can you try again with the latest HEAD?
 I'm going to close this for now, since we can't reproduce. Let us know if you still have a problem with the up-to-date code.
  Try updating your version of TensorFlow: 0.5 was the original release and there have been a ton of bug fixes and improvements since then.
 Try reading through: https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#download-and-setup
 (Feel free to comment if, after upgrading, you still have the problem, and we can debug further!)
  You may need to run sdk_path/tools/android and ensure that you have the 23.0.1 Android build tools installed. Alternatively, if you already have a version >= 21, you can try changing the setting in WORKSPACE and point to that.
 What does it say if you try building with --verbose_failures?
 Here's the contents of my aapt_binary script:
#!/bin/bash -eu
SDK=${0}.runfiles/external/androidsdk
exec ${SDK}/build-tools/23.0.1/aapt

I'm not sure exactly what's going on, but it appears that on your system, "${0}" (the path to the current bash executable) is defined to be: `/root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary`

You don't appear to be running as root given the path to your build file (`/home/islamoc/tensorflow/tensorflow/examples/android/BUILD`), so it's not surprising that the aapt file can't be found due to not existing or not having permissions. Are you running bazel with su/sudo, or have you given the bazel any special permissions or anything like that could cause such a mixup?
 Status 4 is an OOM issue. Try increasing your swapspace, giving bazel more memory/decreasing memory usage, or running fewer simultaneous jobs.

See http://stackoverflow.com/questions/34382360/decrease-bazel-memory-usage?rq=1 for more info.
 Awesome! Glad that did the trick :)
 We use the camera2 API in the official repo, which requires API 21+.

However, @hamidb has a fork of TF using the old android.hardware.Camera that should run on devices api 15+. See https://github.com/hamidb/tensorflow/blob/api20/tensorflow/examples/android/src/org/tensorflow/demo/CameraConnectionFragment.java for the relevant changes.
  Are you passing `grads` directly to `Session.run()`? I believe that will contain a list of (gradient, variable) tuples, which needs to converted to a list a tensors in order to pass it to `Session.run()`. For example, you could do:

``` python
grads_and_vars = rmsprop.compute_gradients(cost, [w1, b1])
grads = [g for g, _ in grads_and_vars]
vars = [v for _, v in grads_and_vars]
grad_vals = sess.run(grads)
var_to_grad_val_map = {v.name: val for (v, val) in zip(vars, grad_vals)}
```
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @rmlarsen: Want to take a look? 
 CLAs look good, thanks!

<!-- ok -->
 Thank Alex, I'm reviewing the code now.

On Mon, Mar 14, 2016 at 1:09 AM, Alexander G. de G. Matthews <
notifications@github.com> wrote:

> I signed the CLA. All authors have now signed.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-196192589
> .
 Hi Alex & James,

Thanks for your contribution, this will be really nice to have in
TensorFlow. I left a number of specific comments on the pull request.

Best,
   Rasmus

On Mon, Mar 14, 2016 at 8:49 AM, Rasmus Larsen rmlarsen@google.com wrote:

> Thank Alex, I'm reviewing the code now.
> 
> On Mon, Mar 14, 2016 at 1:09 AM, Alexander G. de G. Matthews <
> notifications@github.com> wrote:
> 
> > I signed the CLA. All authors have now signed.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-196192589
> > .
 Sounds great. I'm back from a short vacation, so please let me know when the code is ready for another look.

Rasmus
 Would be great if you could make this batch cholesky grad, for which cholesky grad is a simple special case requiring a reshape.
 Hi Alex, I think the code looks good now. I had a few nits about the formatting of the python code. I agree that implementing batching can be done in a followup PR.
 On Tue, Apr 5, 2016 at 1:19 AM, Alexander G. de G. Matthews <
notifications@github.com> wrote:

> Hi @rmlarsen https://github.com/rmlarsen,
> I have added the longer comment about _bar you asked for.
> 
> @jameshensman https://github.com/jameshensman has spent some time using
> flake8 on the unit test to ensure pep8 compliance which also covers your
> points on this. It turns out some of the rest of the script wasn't pep8
> compliant so he fixed that too.
> 
> I you agree, I think that therefore means we are good to go. What happens
> next?

Thanks for all your work - it looks nice. I will test your PR internally
and then we can to merge it. But first, it would be preferable if you could
squash your work into a single commit. You can either use the script
at http://rebaseandsqua.sh/
or follow these instructions: https://davidwalsh.name/squash-commits-git

Best,
   Rasmus

Thanks for all your help,

> @alexggmatthews https://github.com/alexggmatthews
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-205716282
 Jenkins, test this please.
 Hi Alex,

I suspect you have to merge manually for now. Perhaps @martinwicke knows?
 Now that the squash/merge is done, we can test it once more and then merge.
@tensorflow-jenkins: test this please.

(alternatively, we could have squashed/merged ourselves).
 Looks like some of the tests are timing out -- can you figure out which test is taking so long and try to reduce the entire test suite to run in < 30 seconds?
 Also, the mac test failed with:

INFO: From Testing //tensorflow/python:cholesky_op_test:
==================== Test output for //tensorflow/python:cholesky_op_test:

# .F..........

## FAIL: testSmallMatrices (**main**.CholeskyGradTest)

Traceback (most recent call last):
  File "/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/python/cholesky_op_test.runfiles/tensorflow/python/kernel_tests/cholesky_op_test.py", line 100, in testSmallMatrices
    self.runFiniteDifferences(shapes)
  File "/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/python/cholesky_op_test.runfiles/tensorflow/python/kernel_tests/cholesky_op_test.py", line 138, in runFiniteDifferences
    self.assertLess(error, 1e-3)
AssertionError: 0.0011898875 not less than 0.001
 Hi Alex, Given that the OS-X test fails due to a tiny numerical discrepancy, I'd say that increasing the test tolerance by a factor of 2, say, is an acceptable fix.
 @tensorflow-jenkins: test this please
 Woohoo!
 Hi Alex,

Would you have time & interest in extending the Cholesky gradient code to
the batch case?

Thanks,
   Rasmus

On Fri, Apr 8, 2016 at 1:01 AM, Alexander G. de G. Matthews <
notifications@github.com> wrote:

> Excellent!
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-207289948
  @danmane: Want to comment?  I think we'd be happy to accept patches for this. 
 This is something I'd like to fix, but not super high priority. Patches would, indeed, be welcome. 
  This is a stackoverflow question, not a Github issue.
  @mrry: Was there some plan to change `get_shape()` to be less verbose as well?
 Yes, but it depends on fixing all of the legacy code that uses the vestigial `Tensor.shape` property internally. Assuming this eventually happens, I'd be inclined to add `Tensor.shape` as an alias for `Tensor.get_shape()`, instead of accepting this PR.
 @mrry: What does the vestigial thing do?  I can't find it in the code.
 It does seem like once we rename `get_shape()` to `shape`, these aliases fit naturally into the picture.
 Ok, looks like we have some internal work to do instead.  Thanks!
 (We can revive this later when it becomes possible to get to our end goal)
  This is a Stackoverflow question, not a Github issue.  `template` is indeed a feature of C++.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @petewarden: These scores do look pretty different.  Is there a potential issue here?
 Those new scores look correct. The difference comes because I was originally using Inception v1 (that we include with the Android example) to generate the scores, and we switched to v3, but I forgot to update this README, though the tutorial has the correct values:
https://www.tensorflow.org/versions/master/tutorials/image_recognition/index.html
 Thanks for this change @shekkizh! We appreciate the fix.
  Checking with the main partial run person (I'm not sure I know his Github username).
 Just assign it to me.
 I don't seem to be able to reproduce the reported issue.  Here is the code I use:

```
import tensorflow as tf
sess = tf.Session()
a = tf.placeholder(tf.float32, shape=[])
b = tf.placeholder(tf.float32, shape=[])
r1 = tf.add(a, b)

h = sess.partial_run_setup([r1], [a, b])
res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})
print("111", res)

h = sess.partial_run_setup([r1], [a, b])
res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})
print("222", res)

h = sess.partial_run_setup([r1], [a, b])
res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})
print("333", res)
```

Running it produced:

```
('111', 3.0)
('222', 3.0)
('333', 3.0)
```
 I am using HEAD. Could you try HEAD?
 Excellent!  Closing.  @yuanbyu: We should probably add a note to `RELEASE.md` saying some bugs were fixed in partial_run.
  @shlens: Want to comment here? 
 Agreed. The image preprocessing is confused and superfluous.

For the most part, the distortions provided by random_brightness combined with random_contrast would be largely removed by per_image_whitening performed in the subsequent operation. If you removed the per_image_whitening, I suspect that the model should train nearly identically.

Would you be able to perform a side-by-side comparison where you retain and then remove per_image_whitening? If the results are identical, we would remove per_image_whitening from the preprocessing of this network.

(In an earlier incarnation of this network we employed random_hue prior to per_image_whitening, however this operation we removed for other reasons.)
 @stefan-w: Pull requests welcome! 
  This looks good, just checking with @benoitsteiner to make sure nothing strange is going on.
 Jenkins, test this please.
 Uh oh, it looks like this PR has uncovered a bug in the previously untested GPU reduce_mean code:

http://ci.tensorflow.org/job/tensorflow-pull-requests-gpu_pip/308/console

@benoitsteiner: Can you investigate? 
 @benoitsteiner just submitted the registration along with a fix to the untested kernel, so it should show up in open source soon. 
  The broken link is unfortunate, but this PR is bad because it would make all versions of the docs link to the r.07 version.  Cc @martinwicke.  
 Yes, the absolute path is bad, can you just add a .md after framework? That should fix it as well.
 (feel free to comment if you've updated this as above, otherwise closing due to inactivity)
  @danmane: Looks reasonable to me, is there any website subtlety here? 
  Unfortunately numpy as the same issue and for the same reason: our comparison operators return tensors of comparisons rather than single comparisons, and such tensors can't be combined with Python's `and` routine.
 Is there a particular library or language on which you're basing your expectation (other than mathematics, where obviously the expression is well defined :)...)?

Generally speaking, we've been trying to maintain consistency between the TensorFlow Python API and NumPy. If I try to compute `multiple_inequality` using NumPy, I get the following:

``` python
>>> import numpy as np
>>> lower_bound = np.array(
...         [[ 0.,   0.,   0. ],
...          [ 0.,   0.,   0. ],
...          [ 0.5,  0.5,  0.5],
...          [ 0.5,  0.5,  0.5],])
>>> upper_bound = np.array(
...         [[ 0.5,  0.5,  0.5],
...          [ 0.5,  0.5,  0.5],
...          [ 1.,   1.,   1. ],
...          [ 1.,   1.,   1. ],])
>>> test = np.array(
...         [[ 0.30000001,  0.2,  0.82],
...          [ 0.30000001,  0.2,  0.82],
...          [ 0.30000001,  0.2,  0.82],
...          [ 0.30000001,  0.2,  0.82],])
>>> lower_bound <= test < upper_bound
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```

My intuition would be that this expression would be evaluated as one of the following (depending on the associativity of Python's comparison operators... I think it's the first of the two):

``` python
x = lower_bound <= test  # a bool array
multiple_inequality = x < upper_bound  # bool vs. float
# or
x = test < upper_bound  # a bool array
multiple_inequality = lower_bound <= x  # bool vs. float
```

...however, either of these examples seems to work in NumPy (albeit with potentially wacky results, since NumPy converts `False` to `0.0` and `True` to `1.0`), so I'm not quite sure of the types.

---

**EDIT:** TIL that Python has [special syntax for chained comparisons](https://docs.python.org/2/reference/expressions.html#not-in)! However, its functionality seems to be predicated on interpreting the result of a comparison as a boolean. Unfortunately, the result of TensorFlow comparisons&mdash;like NumPy comparisons&mdash;is _not_ a simple `bool` but rather a `tf.Tensor` object. We don't currently override `Tensor.__nonzero__()` (Python 2) or `Tensor.__bool__()` (Python 3), so a `tf.Tensor` is always considered to be "`True`" in comparisons etc.

Since this is a common error, I'm going to submit a quick fix that prevents this comparison. Unfortunately it isn't feasible to make the chained comparison syntax work, because Python's [bytecode translation for such comparisons](http://stackoverflow.com/a/12675160/3574081) relies on the value of the `Tensor` which isn't available when you construct the graph. However, we can at least provide a better error message.
 @mrry: Thank you for the more detailed explanation of my admittedly terse reply. :)  Good idea to produce a reasonable error!
 Hmm, it looks like I may have spoken too soon... raising an error in `__nonzero__()`/`__bool__()` breaks a lot of previously legitimate code where we have:

``` python
if tensor:
  # ...
```

...where really it should be:

``` python
if tensor is not None:
  # ...
```

We can fix all of the instances in the TensorFlow library, but it might take longer to fix up all of the existing code that relies on the first version working as expected. Let's keep this issue open though, because the improved error messages are probably worth the pain.
  @ebrevdo: Want to comment on this?  Fancier variants of LSTMs would be great to have, the only question is whether they should go in core tensorflow, the contrib directory, or the models repo.  If you can make the interface similar to Eugene's existing RNN cell setup contrib might be the best place at first.
 Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN.  etc.  Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.  And maybe feed the result into another RNN.

This allows one to implement both separable and non-separable multidimensional RNNs.  Not sure we need the sugar until people show it's superior to convolutional networks for something, and there's lots of use cases.

**That said**, it would be really nice to implement GridLSTM (also by Alex Graves, and according to their paper showing a lot of promise with fewer parameters).  GridLSTM uses standard LSTM cells, so it would be a function that uses RNNCell objects.  It connects them in interesting, convolutional, ways.

GridLSTM would probably go well in contrib, until enough people use it and it shows to be a very successful technique.
  @mrry: Have you seen this before?
 Urgh, this sounds like `protobuf.bzl` might have changed... did we recently move to a newer version of protobuf? @keveman knows the mystic runes that make it work with gRPC.
 I would also suggest maybe refreshing your submodules -- someone more git-competent than I can explain why git fetch/pull doesn't also update your submodules.
 @vrv: If it did that there's a risk of stomping on someone's changes inside a submodule.  Submodules are a pretty flaky part of git, unfortunately.
 I thought that would be no different than resolving conflicts from the main repo :)
 Maybe if submodules weren't a bolted on hack.
 Try:

```
git pull --recurse-submodules
git submodule update --recursive
```

(I haven't tried this, but this is what seems like might work -- essentially the tree of each of tensorflow's submodules)
 @davidzchen is this related to your change to remove python protobuf as a submodule?
 The following error message seems to indicate that the version of Bazel being run does not support the `imports` attribute for the Python rules:

```
ERROR: /mnt/tmp/tensorflow/tensorflow/core/BUILD:87:1: //tensorflow/core:protos_all_py: no such attribute 'imports' in 'py_library' rule.
```

@hrajanie Which version of Bazel are you running? The `imports` attribute was added in 0.2.0.
  Yes, it's called `partial_run`: see #672.  It's experimental at this stage so the API might fluctuate, but it should be what you're looking for.
 Closing as duplicate.
  Yikes.  Good to know @aymericdamien, thanks!
 Are you building from source, or did you install the pip package?  What's your environment?  E.g., all the information the template had but you removed :).  If you built from source, what command line did you use? 
 @zffchen78: Can you take a look?  Is there any relationship between this and  #2471?
 @vrv: Reassigning to you per @zffchen78's request.
 Pretty sure this is going to be hard for us to debug without being able to reproduce this.  

I would suggest:
1) Upgrading your nvidia drivers 
2a) Updating cuda to 7.5 and cudnn v4 and installing TensorFlow r0.9 or
2b) Updating cuda to 8.0 and cudnn v5 and installing TensorFlow from sources

and then try again.
  The permission denied bit might mean that the permissions for that file are weird.
 Unfortunately no, it's a guess without knowledge of details.  What happens if you do `ls -ld` on that file and all the directories above it?
  Yes, you have to build from sources and you can specify the toolkits you want to use via ./configure
 Yes, I believe so.
 Closing since the pip packages are built against specific CUDA versions.  @vrv: We can reopen if we have plans to make pip packages that work everywhere, but I'm not sure if that's practical.
   We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 Hi @terrytangyuan @makseq @dansbecker @elqursh @dgboy2000 @cbonnett @ivallesp @frol @liyongsea,

We decided to move skflow into tensorflow - for better convergence of APIs and generally better support.
Can you please comment in this thread with "I signed it! message for CLA to propagate for you to tensorflow project (this will make you contributor of tensorflow and keep your stats).

Thanks!
 Jenkins, test this please
 Jenkins, test this please
 Jenkins, test this please
 Jenkins, test this please
 Sorry about the change in suggestions, but PR 1472, which is already merged after discussion, ended up including all the deb and pip packages required for the builds and tests here. As such, please remove all changes to the following files: 
tensorflow/tools/ci_build/Dockerfile.cpu
tensorflow/tools/ci_build/Dockerfile.gpu
tensorflow/tools/ci_build/install/install_deb_packages.sh
tensorflow/tools/ci_build/builds/test_installation.sh
tensorflow/tools/ci_build/builds/pip.sh
Thanks.
 Everybody has signed the Google CLA when contributing to skflow.
  Constants are part of the `GraphDef`, so unfortunately this is intended behavior.  To keep them out of the `GraphDef`, change from

```
random_a = tf.Variable(tf.random_normal([1000000]))
```

to

```
random_a = tf.Variable(shape=[1000000])
random_a.assign(np.random.randn(*random_a.shape)).eval()
```

Ug, actually that'll implicitly create a constant too.  @petewarden: What's the right solution here?
 The same question came up on StackOverflow today: http://stackoverflow.com/a/35904439/3574081

(FWIW: `random_a = tf.Variable(tf.random_normal([1000000]))` won't add a large constant to the graph, but `random_a = tf.Variable(np.random.normal(size=[1000000]))` will.)
 To answer what I think is the original question, it's definitely possible to strip out Constant ops from a graph def. We do the opposite in the freeze_graph script, so we could create a 'strip_graph' script or similar to slim down files by replacing large constants with something smaller (though we'd have to do some resize tricks to do it properly).

I think the bigger question is 'Why are there these big weights in my graph def file?'. We're working on answering that here by looking at different ways that we can save external weights, without using the Variable/Restore checkpoints since those are fairly specialized for training. We don't have a good design for that yet though.

Does that help at all? 
 @mrry: That solves the code shown, but I believe the original code loads a big numpy array from a file.
 Closing this as intended behavior.  @toma5692 and others: If you want to keep graphs small, use `tf.Variable` instead of `tf.constant`.
  It'd be good to have this as part of `configure`, but I don't know how awkward it is.  @ebrevdo: Care to comment?
 https://github.com/tensorflow/tensorflow/pull/1220 might be relevant
 +1 if we can configure via bazelrc.  Alternatively it would be great if we could set up a symlink or something similar to the way we do python
 So what's the status of this.  Should we accept #1220 since that's going the ./configure route, or wait for bazel auto config?
 @vrv: This is fixed by #1220, right?
 Yes.
  It's impossible for us to debug this without more information.  First, though, a wild guess: what happens if you restore and test on Linux but using only CPU?  That might at least isolate it to a GPU bug.  If it does turn out to be CPU vs. GPU, can you isolate which op is problematic?
 To satisfy my curiosity: what was the other package?
 @shoc2005: As with the other issue, we're going to need more information.  What version of tensorflow are you using?  Can you localize the differences to a particular op?
  @martinwicke: Looks like the wonderful world of wheel naming conventions? 
 Can you try with the 0.9 wheel? I have no idea what the problem can be and the message is useless, but maybe we get lucky.
  @zheng-xq: Should this be changed to 3.0?
  For future issues, please use Markdown to format the code.  For this one, I edited it to indent by four spaces to preserve code formatting.
 Your code snippet is incomplete, since you haven't shown us the definition of `local3_value` and `centroids`.  There are also some strange bits, such as allocating `image_result_table` but never using it.
 @girving: the problem here is that the program is constantly adding more nodes to the graph because it is mixing graph construction with graph execution.
 Ah, right, forgot about our quadratic time behavior.  @MisayaZ: Move the `mat_result` and `reduce_sum` definitions outside the loops and you should be fine. 
  @keveman: Do you any ideas here?  Feel free to comment and reassign.
 No ideas. Passing the baton to @zheng-xq 
 To clarify: was the `LLVM_HOST_COMPILER_PATH` bit the part that fixed the `NewString` error?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Looks good!  Can you sign the contributor agreement (see above)?
 Jenkins, test this please.
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 @martinwicke: Did I invoke Jenkins correctly?  I don't see anything happening, but maybe that's just a lack of patience?
 Hmm... Jenkins, test this please?
 Yours seemed to work better.
 Yup. There was a problem. Fixed it. Should work for you as well now.
 @OlavHN: Can you rebase this on top of master to get rid of the merge commit?  I'll merge after that.  Thank you for the fix! 
 Jenkins, test this please.
 @tensorflow-jenkins: Test this please
 Looks good -- just one comment and then can you squash?  We'll merge right after.
 @tensorflow-jenkins: test this please
  @martinwicke: Are these files autogenerated? 
 I need to redeploy the website. Otherwise this should be fixed.
  I think this is intentional: it's better to use `tf.matmul(x, w) + b` directly.  We are working to lock down our symbol exports so that only documented symbols are exported, but we're a bit lax at the moment.
  "accidental" may be a better word than "negligence". :)

@benoitsteiner: Want to take a look at this?
 That was indeed a mistake on my part. Somehow I didn't checkin the kernel registration part. We should merge pull request #1457 that fixes my oversight. 
  Can you confirm that you're using python 3?  You say `pip3` and `python` above; is just `python` Python 3?  If so, what version?
 Next question: what exception are you seeing?  It may just be my eyes glazing over, but it looks like the initial comment has a traceback but not the actual exception.
 Does changing `with gfile.GFile(data_path, mode="r") as f:` in `models/rnn/translate/data_utils.py` to have `mode="rb"` fix the problem?  If so, that's a better fix than teaching `gfile.py` about latin.
 What error do you get if you only make the line 134 change?  The reason I ask is that it should work the same way in Python 2 and Python 3, and Python 2 doesn't automatically use unicode.
 So the right fix for that is to put `b` in front of the regular expression definitions to get `bytes`-compatible regular expressions.  A few more fixes along those lines may be required.
 If you want to keep trying to fix it I'm happy to keep helping, but I could also try to find someone to reproduce on our end.
 I believe this was fixed by #1436.
  @martinwicke: Whose the right person for pip issues? 
 @amirkhango Is this still an issue with 0.7.1? Or are you deliberately trying to install 0.5.0?
  Please ask non-issue questions on stackexchange or the discussion list, not as Github issues.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Looks good!  @martinwicke: Can you launch the tests? 
 You can, too!

Jenkins, test this please.
 @martinwicke: How does one look at the test output?  http://ci.tensorflow.org/job/tensorflow-pull-requests-mac-python3/99/console shows which tests fail, but I don't know how to see how they fail.
 http://ci.tensorflow.org/job/tensorflow-pull-requests-mac-python3/99/consoleFull shows the full log. Also, theres's a little button on top of the log to show the full output.

This looks fine, these three python3 tests are unrelated.
 If I have the power to run tests, does that also mean I have the power to merge?  What's the process for that these days?
 Merged!
 https://github.com/tensorflow/tensorflow/commit/b4b2575ec4bf7e6da2686505f61b5f16cb9273ab
  This is caused by changes in the behavior of zeros_like (See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L641). 

As a temporary workaround, please replace
`ns = tf.Variable(tf.zeros_like(xs, tf.float32))`
with
`ns = tf.Variable(tf.zeros(Z.shape))`

We will provide a more permanent solution to this soon.
 Fix in review.
  You'll need to provide more information if you want help.  If the build failed, what was the error message, and which compiler are you using?  You'll definitely need a C++11 capable compiler.
 I'm going to close this since it appears to be a bazel issue.  Please reopen if you think there's a fix on the TensorFlow side.
  @ibab: Taking a look now.  As for `TF_COMPLEX`, let's have both `TF_COMPLEX` and `TF_COMPLEX64` with the same enum value (https://stackoverflow.com/questions/11412516/enum-why-can-two-different-enumeration-constants-have-the-same-integer-value).
 Looks great, thank you for the contribution!  Just those couple style comments on the Python code.  This is by far the least number of comments I've wanted to make on a TensorFlow PR so far. :)

It's good to keep the commits separate while you're working on it, and once everything is in place we can squash them into a single test-passing commit.
 @ibab: Actually, let's hold the kernel updates to a separate PR.  Adding just the dtype and associated framework support is a nice standalone step. 
 Awesome, looks good to me.  @martinwicke: Can we run this through the tests?  Is it better to smash into a single commit before or after testing? 
 Jenkins, test this please.
 @josh11b: Looks like this PR requires the backwards compatibility test to run to make it work (unsurprising since there's a new numeric type).  Is the update script in open source yet?
 Jenkins, test this please.
 The backwards compatibility test has recently been moved to open-source, along with the update_ops script.
 @tensorflow-jenkins: test this please.
 Excellent!  I'll take another quick look and then merge.
 @ibab: Unfortunately the backwards compatibility test files have been changed since your update, so the test fails again for me when I try to merge.  Can you merge, regenerate those files, and then squash into one commit?  There's a bit of an unfortunate race condition here.

Incidentally, in a future commit we should add a note to `RELEASE.md` saying TensorFlow has complex64 support now. :)
 I'll relaunch tests ASAP when you do that and try to get it in as quickly as I can.  Large refactoring changes can be tricky to sneak into an atomic system.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
 Trying merge now...
 Jenkins, test this please.
 Jenkins, test this please.  (The first time failed for some infrastructure reason)
  @martinwicke: The infrastructure failure persists.  Ideas?

@ibab: Do you mind if I pull this in and commit it myself (crediting you explicitly of course)?  Looks like our Jenkins setup is experiencing some issues, and I'd like to get this in before the compatibility test bit rots again. 
 Excellent, looks like the infrastructure failure has passed.  Back to the normal merging process.
 Thank for the contribution!
  @kashif: Yep, though the names of tests unfortunately don't correspond exactly to the directory structure.  Usually it's easiest to write op tests in Python, so I'd either put something `python/kernel_tests/softmax_op_test.py` or make `log_softmax_op_test.py`.  You can run these as

```
blaze test python/softmax_op_test python/log_softmax_op_test
```

I.e., the `kernel_tests` part goes away.  You can also run all the tests by doing `blaze test ...`. 
 Please squash this into one commit.  It's hard to see the differences from master as is, since you introduced a bunch of code in one commit and then removed it in another.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 @kashif: Actually the backwards compatibility test is failing, which does require an update on your end.  Please do this:

```
bazel run core/ops/compat/update_ops
```

It updates some files that keep track of the history of op changes to ensure that everything is backwards compatible.  In this case, `update_ops` will add a new entry for `LogSoftmax`.
 Note that you can reproduce this problem locally by running `bazel test ...`.  You should see the same failure.
 Apologies for not responding to this for a few days.  What's the status?  It looks like the squash hasn't happened yet.
 I still see two commits.  Did you squash?
 Looks good!  Jenkins: Test this please.
 Tests all look good.  Unfortunately Github recently changed the "Merge pull request" button to not work if you're out of date.  Can you rebase?  Apologies for the trouble; we haven't yet figured out to hack around this unfortunate behavior change.
 @tensorflow-jenkins: test this please
  - Can you include the stack of the entire error message?
- Have you tried installing in a virtualenv from scratch , using version 0.7.1 ?
 https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation-issues might help :)
 Sounds from the discussion like this issue can be closed - please re-open if you're still experiencing problems.
  I just ran this locally on my machine and it worked. Could you upload the events file so I can see if I can reproduce it? Also, how large is the events file? it should be around 1MB.
 OK, I've looked into the event file you've provided - it's perfectly fine, which means the issue is in TensorBoard. Specifically, I think TensorBoard is never detecting that the file exists, and so isn't loading anything. However, I can't reproduce this behavior on my Mac or Linux.

Can you please help me explore this issue by doing the following: 
1. Let's just verify that the bug still reproduces if you build from source. 
- Clone the TensorFlow repository
- Checkout the branch r0.7
- run ./configure (no need to put in GPU support)
- bazel build `//tensorflow/tensorboard:tensorboard`
- `./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/mnist_logs`
  Note: make sure the logdir is an absolute path, not a relative path. I think there is an issue with relative paths, let's disambiguate between them.

If TensorBoard is still broken, let's add some debugging statements into `tensorflow/python/platform/default/_gfile.py`, which is the file that contains the interface between TensorBoard and the filesystem. We can test my hypothesis that TensorBoard isn't loading the right paths by adding some print statements in the following functions:

ListDirectory (line 353): print the directory, and the files
Exists (line 250): print the path, and whether it exists

IsDirectory (line 255): print the path, and whether it is a directory

Let's also check the subdirectories generated by TensorBoard:
go to line 194 of tensorflow/tensorboard/tensorboard.py and print out the subdirectories

Once you've added these statements, rebuild tensorboard (as above) and rerun it.

Can you please try this and let me know what gets printed? 

Also, feel free to ping me via Google Hangouts at danmane@google.com or danmane@gmail.com and I'll help debug this synchronously.

Thanks for your help! Let's fix TensorBoard :)
 @llevar - I'm surprised that you are getting a valid response for the graph if you issue it from the browser, but the dashboard is showing nothing. (Just to be clear, this is also the "graph" dashboard that is showing nothing, i.e. if you click "graph" in the top right of the screen.)

Can you try with TensorBoard from rc0.8 and see if this still happens? Also, are you getting any 404s on the console in the TensorBoard frontend? 
 Re: the missing TAG file - that is not particularly important, and should be fixed in 0.8.
 OK, this was a known issue in the original 0.8 release candidate, but it was fixed by #1926 two days ago. Is it possible that your code is 2 days out of date? 
 @llevar Thank you very much for that report, that's super important that we don't release a broken python package with r0.8. I'll make sure this gets fixed shortly.
 @vrv @martinwicke Can you guys shed any light on why this file would be missing from the pip package? I'm not very familiar with that part of the codebase. 
It looks like the pip_package BUILD file just references tensorflow/tensorboard as a directory, so I'm confused how it would skip the css file. 
 globs only go down to the directory that contains the next BUILD file, and
then they stop descending into the tree. You can blame me, because I added
many more BUILD files to the opensource tree, including one in lib/ which
now prevents the frontend rule to include anything in lib/ via its glob.
It's still a bit surprising to me because lib/BUILD does have all_files,
but it may be that you have to depend on tensorboard/lib as well now that
it has its own BUILD file.

On Sat, Apr 16, 2016 at 2:23 AM Daniel W Mane notifications@github.com
wrote:

> @vrv https://github.com/vrv @martinwicke
> https://github.com/martinwicke Can you guys shed any light on why this
> file would be missing from the pip package? I'm not very familiar with that
> part of the codebase.
> It looks like the pip_package BUILD file just references
> tensorflow/tensorboard as a directory, so I'm confused how it would skip
> the css file.
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1421#issuecomment-210780370
 The bug should now be fixed at r0.8 head. This will move to master as well
soon.
On Tue, Apr 19, 2016 at 05:14 Sergei Iakhnin notifications@github.com
wrote:

> Thanks @romanows https://github.com/romanows! That makes sense. This
> should be part of the official installation instructions until they fix the
> bug. :-)
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1421#issuecomment-211888781
 @llevar: On current head I've built & used pip packages on both MacOSX and linux and found that the css file was included properly. 

Going to close this issue, please re-open it if you continue to have issues (or open a new issue, this one has become very cluttered)
  Yes, `complex128` would be good to have, but I don't think anyone's working on it at the moment.
 If someone wants to add it, I'm happy to answer questions.  It should mostly involve duplicating any mention of `complex64` into `complex64` and `complex128`.
 @ibab: Yep, it'd be `DT_COMPLEX128 =  18` and `DT_COMPLEX128_REF = 118`.
  @keveman: What's the situation with user_ops these days?  Was it going through some flux? 
 Not that I know of. There is #1378 for which I'll have a fix soon.

On Mon, Mar 7, 2016 at 9:04 AM, Geoffrey Irving notifications@github.com
wrote:

> @keveman https://github.com/keveman: What's the situation with user_ops
> these days? Was it going through some flux?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1419#issuecomment-193345938
> .
 @sbyma Can you please verify and close this issue?
  @Fhrozen did @sbyma's solution fix your problem?
  It'd be better to use a pure Python test for this, since then you can use tf.test.compute_gradient_error.
 Also, can you implement and test the gradient of diag_part in the same commit (which is diag)?  Should just be a few more lines, and it's always delightful to add gradients of transpose operators in pairs. :)
 Looks good once the C++ changes are removed!
 No, normally we just register gradients in Python.
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
  It's just protobuf, not google.protobuf. Does something in tensorflow not work?
 There seems to have been an issue with the protobuf pypi package, which should have been fixed about 30 minutes ago, please try again.
 pip uninstall tensorflow
pip uninstall protobuf
pip install path/to/tensorflow.whl

has also worked for a lot of people.  0.7.0's install broke a lot of things that 0.7.1 fixed.
 Can you take a look at our install guide on tensorflow.org for "common problems" and let us know if that helps ?

https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation-issues
 Did you try:

pip uninstall tensorflow
pip uninstall protobuf
pip install path/to/tensorflow.whl ?
 @vrv: What's the status of this?
 I think we fixed this in 0.8 and it hasn't regressed.
  @mrry: Looks like a distributed-related issue.
 Are you able to run `git clone https://boringssl.googlesource.com/boringssl` locally?
 #1387 might be relevant...
 Can you please try upgrading to the latest HEAD of Bazel? It looks like a fix was added for proxy support in https://github.com/bazelbuild/bazel/commit/d4c00d42b7ac3939f7041ba3bcb920088857adbd.
 Hi, I also have this problem. 

I'm trying to use `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` to build the package from source. And I met an error on `boringssl`. The error message is as follows:
![1](https://cloud.githubusercontent.com/assets/8293993/14003987/dc05a9b4-f191-11e5-88aa-1022013d087f.png)
After blocking with several minutes... the error is shown:
![2](https://cloud.githubusercontent.com/assets/8293993/14003988/dc067cb8-f191-11e5-8171-698bb03f4e29.png)
After checking the `grpc` repository [https://github.com/grpc/grpc.git], I found the `boringssl` submodule has been updated with the new mirror in [https://github.com/google/boringssl.git]. 
After cloning the `grpc` repository into my local disk, I try to find where the `boringssl` is needed. 
![3](https://cloud.githubusercontent.com/assets/8293993/14004056/4caa29b0-f192-11e5-9fb2-9e7324464bcc.png)
It seems nowhere for [https://boringssl.googlesource.com/boringssl] is needed. 
So,could you please change the dependency of `grpc` for [https://boringssl.googlesource.com/boringssl] into [https://github.com/google/boringssl.git], completely?
 @shengjt Thank you very much! Problem solved!
 @shengjt Yes, I have modified the `tensorflow/workspace.bzl` as follows:

```
    native.git_repository(
      name = "grpc",
      # commit = "73979f4",
      commit = "403cd6c",
      init_submodules = True,
      # remote = "https://github.com/grpc/grpc.git",
      remote = "https://github.com/melody-rain/grpc.git",
     )
```

Then, use `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` to build the pip package, and install it into python packages. Then, the `tensorflow/models/image/mnist/convolutional.py` can be run correctly.  
 Another possible solution is:

```
    native.git_repository(
        name = "grpc", 
        commit = "3d62fc6",
        init_submodules = True,
        remote = "https://github.com/grpc/grpc.git", 
     )
```

Why `3d62fc6`? Because after this commit, the `boringssl` from the original googlesouce was changed into `https://github.com/google/boringssl.git`. See `https://github.com/grpc/grpc/commit/3d62fc68349a5ef31b9c2b0250818343bf9cca68#diff-8903239df476d7401cf9e76af0252622`.

Why not the latest commit? I have tried to set the value of `commit` as the latest commit, but it seems to still have some problems. 

I think the error in the main post is from the commit `73979f4`. In commit `73979f4`, the submodule of `boringssl` was still from googlesource. 

Maybe somebody could try this solution and submit a pull request into the master branch. 
 Hi, @fayeshine @myme5261314 @shengjt , I cloned the latest master branch [b1aeb4495bd10b3f1f0d4aed64880f8896fe990b] and modified the `tensorflow/workspace.bzl` with #1626, along with the following lines:

```
    # url = "https://storage.googleapis.com/libpng-public-archive/libpng-1.2.53.tar.gz",
    url = "http://denemo.org/~jjbenham/gub/downloads/libpng/libpng-1.2.53.tar.gz",
```

Now the following binaries can be built successfully.

```
    bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
    bazel build -c opt --verbose_failures //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server
    bazel build -c opt --config=cuda //tensorflow/core/distributed_runtime/rpc:grpc_testlib_server
    bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
    bazel build -c opt --config=cuda //tensorflow/libtensorflow.so
    bazel build -c opt --config=cuda //tensorflow/core/ops/compat:update_ops
    bazel build -c opt --config=cuda //tensorflow/examples/label_image:label_image
```
  @shlens: `resize_image_with_crop_or_pad` calls `Check3DImage` with `required_static=True`?  Is there a reason for that?  Fixing it would require changing a few routines to compute height and width via `tf.shape` at runtime rather that at graph construction time.
 @shlens: Feel free to remove your name and put contributions welcome if you want.

@zplizzi: I think the only reason for this limitation is that all the existing uses appear as part of a larger image pipeline where the size is known.
 This looks like a duplicate of #521.
 Yep, closing.
  If you want something to happen conditionally, it has to go _inside_ the `tf.cond`, so this is expected behavior.  The analogy is

```
alpha += 1
beta += 1
if pred:
  ...
```

Both `alpha` and `beta` will be incremented regardless of `pred`.
  I think an optimized (blocked, etc.) implementation of semiring matmul is out of scope for tensorflow at the moment, since it would require either code generation or a massive amount of template instantiation.  Of course, for small sizes it can be implemented by writing down all the terms via broadcasting (producing a cubic size tensor) and then reducing, but that's not very interesting.

It's possible that TensorFlow may be better at this kind of flexibility at some point, so I'll leave this bug open.
 If a couple particular semirings capture most of the value, this seems quite reasonable.  @benoitsteiner: Can Eigen be convinced to give optimized semiring matrix multiply? 
 Computing log Z can be rewritten in terms of regular matrix multiplies, but Viterbi is an interesting example. You need efficient way to  compute matrix vector min-plus product r=Av as  r(i) = min_j A(i,j) + v(j)
  @danmane: Do you know what the issue is here? 
  1. is done: https://github.com/tensorflow/tensorflow/blob/3871973b302b3c8357a4984fb37d934cf4633f21/tensorflow/python/ops/nn.py#L287 -- will be in our next binary release.
 Nothing is impossible, it would just take a lot of work to plumb that through everywhere, to set the option globally, and for everything to work as well without broadcasting.  For example, when computing gradients, automatic broadcasting is quite useful to simplify gradient expressions, particularly when the shapes are unknown at construction time.  I've seen similar requests made for numpy, and it sounds like higher level libraries typically provide the 'safety' for those users who want it.  However, I'm open to the idea if it can be done in a nice way.
  That would be a really nice enhancement. We're planning to improve all the indications around when TensorBoard is loading, have it automatically refresh data, etc. I don't have a timeline, though. 
  closing for now, this is a useful reference for the future when we decide to use this mechanism for configuring.
 For my reference, here is the tracking bug for CUDA autoconf: #2873
  `force_gpu` will fail the test if there is no GPU available (e.g. if you compile without cuda, or if you don't have a supported GPU). It's really for debugging, not particularly useful in tests. The absence of something in between of `force_gpu` and `use_gpu` is a bit of a bug: we do need something that will fail only if there is a GPU, and TF is compiled with --config=cuda, but it's not used.
 I made #1423 to track this.
 Jenkins, test this please.
 hmm... not sure what happened with the tests. Checking.
 Yeah, it was just waiting in the queue.
 The mac/python3 tests had been failing already, so I merged anyway. 
  Good catch. Clearly, this wasn't properly tested (that line never executed), could you add a unit test for the failure behavior?
 Not coverity, but we do measure coverage. It was around 96% or so last time I checked.

I'll merge this.
  The function requires its input to already be a tensor. You can call `convert_to_tensor` to make a tensor from a numpy array, or you can put the numpy data into a `tf.constant` first. 

Or you could fix `flip_left_to_right` to call `convert_to_tensor`, which would be the right thing to do.
 It looks like we could use a cleaning pass through some of the image ops.  A lot of them unnecessarily require known shape tensors as well.
 On the `random_` ones now, which only work on a single image anyway. I think.
 PR #1428 didn't include something like "Fixes #1399" in the commit message, so it didn't autoclose.
  There isn't going to be any updates to the course from the instructors beyond fixes at this point.
  Closing since the discussion indicates it may not be a TensorFlow specific issue.  Please comment if more information appears or file a separate issue.
  Can you explain what currently happens?  Our python code is supposed to return C++ status errors, which can then be handled by users.
 Closing since questions about how nan works are better suited to StackOverflow.  Generally, nans occur when either a specific singularity is hit (division by zero) or an iterative process explodes to infinity.
  Merged. Thanks!
  Jenkins, test this please.
 Just some docs and naming nits, otherwise looks good! 
 One more run of tests, then we're good.

Jenkins, test this please.
 Merged.
  np.float is an alias for float64, not float32. Surprised me too. http://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html
 Thanks for moving SetZero, that's good. Can you add a blank line after it? Otherwise looks good to me. You can amend your last commit with that, then I'll merge.
 Jenkins, test this please.
 Jenkins, test this please
  Fixed by #1395.
  Can you upgrade pip or tell me which version of pip is installed?
 Closing due to lack of information.  I'm happy to reopen if more information arises.
  It sounds like the variables were not initialized, but that's strange if tf.initialize_all_variables is being run. Can you check that you initialized the variables? Also - could you make a repo with just the problematic model and test it against the head TensorFlow?
 I tried to replicate your problem, but I can't -- I don't have a multi-gpu machine and, even though I tried, I cannot replicate the problem in a single-gpu setting in any way. In fact I even managed to run "python cifar10_multi_gpu_train.py --num_gpus=2" with your change and it was all ok, but I guess it just soft-placed everything on GPU0. One problem I can think of is that in your code your LSTM variables will not be placed on CPU, like the other ones. Can you try to put the whole LSTM on cpu first, just for a test (using tf.device around it)? If that works, then you can use pin_variables_to_cpu in your tf.device to just put the variables there, I hope. (It did not work once, I guess that's why cifar10 has _variable_on_cpu, but I think it's ok now.)
 I don't think there are any bugs in LSTM. Variable placement in a multi-device or multi-machine setting is hard, but it's solved by tf.device. We do not want to have parameters for this in LSTM or any other function, as we'd basically need to add the same code to every function we write. It is solved differently using tf.device -- you can use a tf.device context around anything you want, and it will put variables (or any other ops you wish) on any device you specify. Here is an example.
  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/graph_util_test.py#L57
  The Bazel build process helps set up the dependency paths for things like graph_util, so that's the supported path for this. Looking at your error, I'm guessing you have an older pip-installed version of TensorFlow, where the graph_util namespace didn't include that function, which was introduced after 0.7.1.
https://github.com/tensorflow/tensorflow/commit/2179890199c3561ff3a1297c5e9c073471473a77#diff-33f2d39e607bc1b6234f0972973159cb
I hope this explanation helps. I'm closing this since I believe it's working as intended.
  This is intended behavior and unlikely to change: tf.gradients adds new nodes to the graph via symbolic differentiation.  The underlying TensorFlow runtime doesn't have any idea these nodes are gradients, so it won't treat them specially based on feeding.

For your use case: ask `tf.gradients` to give you the gradient w.r.t. y as well, then feed zeros to `dzdy` to tell the backward pass that you want nothing to pass through there.  It won't be as fast as possible, though.
 @rdipietro: Yep, that's actually better than my suggestion, since the graphs don't be doing any unnecessary computations on zeros. 
  I think this is fixed thanks to the underlying fixes in gRPC.
  @tensorflow-jenkins: test this please
 Merged
  This is probably more a question for the bazel team, but bazel clean is supposed to clean up all state to try to provide hermeticity in builds.
  Unless you think this is a bug in TensorFlow, this question is better suited to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow)
  Closing as duplicate of #1377 -- thanks for the report!
  This isn't a bug  / feature request, so StackOverflow is a better venue for this question.
  1) Breaking pip.sh into the install step (pip.sh) and the test step
(newly added test_installation.sh)

pip.sh now performs only the building and install of the pip package,
with the install happening inside virtualenv. (vritualenv address the
reinstallation issue on Mac pip tests). Then it calls
test_installation.sh and test_tutorials.sh to test the virtualenv pip
install of TensorFlow.

2) Adding a new file, docker_test.sh, as a first step toward automation
of TensorFlow docker image build and test process.

This script builds and tests docker images with TensorFlow pip whl
installed. The tests include the Python unit tests and tutorial tests
against the (non-virtualenv) install. This is not fully functional yet, because
we need to further automate the pip package uploading process, so the
Dockerfiles in tensorflow/tools/docker can point to the latest pip
files.
 Nice!

But let's make the line wrapping good. I see it was broken on many places even before this commit (at least in parametrized_build). We normally indent by two spaces. Hence when the line is too long and continues on next one it should have the same indentation PLUS 4 spaces. And all the || and && should be at the end of the lines not beginnings. Right?
 Addressed @jendap's latest comments. Tested on Jenkins. This PR is ready to be merged. 
 Thank you!

Nit: There are still a few multiline statements without intending the next line by two indents. For example

```
  die "FAILED: Unable to find the base directory where the dockerfile "\
"${DOCKERFFILE} resides"
```

is missing 6 spaces on the second line. And there are others.
 @tensorflow-jenkins test this please.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
  This isn't a bug or a feature request (as far as I can tell), so this is better suited to Stack Overflow
  This is fixed now.
  what version of tensorflow do you have?  We renamed tf.image.random_crop to tf.random_crop between 0.6 to 0.7 -- upgrading should solve the problem, or checking out the code at the 0.6.0 branch if you don't want to upgrade.
  Duplicate of #1362 -- can you see the suggestion I added there?
  What version of python3 do you have installed?

Have you tried installing tensorflow within a virtualenv, to isolate the environment?
 Can I close this bug as a duplicate of #2034?
 Closing as duplicate of #2034. 
 @cruvadom: That sounds like an unrelated issue, since what you describe is not a segfault.  If your issue still occurs with tensorflow at master, please file a separate issue.
 @kalleknast That does seem like it could be a similar problem, but please file a separate issue anyway since we're trying to keep the issue tracker organized and this one has already been fixed.  Apologies for the bureaucracy, but keeping issues focused helps us not let fall through the cracks (and not get fixed).
  Are you able to use bazel to compile the re2 library from https://github.com/google/re2 by itself?  If not, you might want to file the issue with either re2 / bazel, since this looks like it's not Tensorflow related from a first glance.
  @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 Merged
  @ebrevdo Is this still an issue? 
 Should be fixed now.
  This doesn't look like an issue with TensorFlow -- please comment if you disagree.
  I think this is intended behavior, unfortunately.  A typical resolution to this is to pick `kernel_weights` to have a nice name and use

```
`tf.histogram_summary(kernel_weights.name, kernel_weights)`
```
  Merged
  I don't have gcc 4.8 installed, and I think the problem is that gcc 4.8 can't understand the 'using typename' declaration there.

In https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matrix_solve_ls_op.cc#L66, can you try changing the
'using' to just 'typedef' ?
 you should do typedef typename BinaryLinear......::Matrix Matrix;

(Need to give it a name)
 Let me submit a fix to this
  Yeah, I don't think TensorFlow's or Eigen's GPU kernels are written for pre 3.0 compute capability :(
  Duplicate of #1341, essentially.

I think this is superfluous logging.  I tried to get rid of this but apparently these logs are useful for debugging deadlocks. @jmchen-g 

Maybe we should just write to log files instead of spamming users.
  What version of cudnn do you have?  The pip package at 0.7.1 is built assuming you have cudnn r4.  You have to build from sources if you want to support earlier versions.
 It sounds like this issue is caused by system irregularities which I'm not sure we'll be able to work around on our end, so closing for now.
  Perhaps one reason that reduce_sum was used is that reduce_mean is not yet implemented on GPU.
 Just pushed b3dfff2a23b435f14cca7377f8e0e5ad7583b45e which added reduce_mean support for GPU, so that will no longer be the reason.  @vincentvanhoucke does this look good?
 Does this converge to the same result though? Our learning rate is not scale invariant.
 @colah: friendly ping?
 @basveeling I agree that switching from a sum to a mean across datapoints is better. Can you:
- Make the corresponding changes to the tutorial text https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/tutorials/mnist
- Confirm that the model converges to the same result after making this change. (You will probably need to multiply the learning rate by the batch size of 100.)
 Thank you for the submission @basveeling.

See the inline requests for minor text changes. Also, both tutorials reference a performance of the naive model of 91%. Thanks to your cross validation, this is now 92% -- so you should make that change too.

Otherwise it looks good. Thanks again.
 Jenkins, test this please.
 Thanks!
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @kmader: I like this, but right now we don't have a good way to test ipython notebooks, which means it's very hard to keep updated if we make changes.  The udacity notebooks already often break, and I'm worried about adding more notebooks at this point, but I could be convinced otherwise.  @martinwicke 
 Flags is just a thin wrapped around argparse, in what way would it have to
be reworked?

I'm quite excited by the idea of tested ipython notebooks, it's been
something we've been thinking about for our tutorials (although we've gone
with regular tests for now).
On Sat, Mar 5, 2016 at 12:00 Kevin Mader notifications@github.com wrote:

> @vrv https://github.com/vrv not sure how @googlebot
> https://github.com/googlebot works but we have jenkins setup to extract
> the python code
> https://ipython.org/ipython-doc/1/interactive/nbconvert.html from the
> notebooks and make sure it executes without throwing any errors.
> Alternatively it is fairly easy to construct the notebooks from annotated
> python code, so maybe that is a more viable option. The biggest issue there
> is the FLAGS in TF needs to be reworked.
> 
> ipython nbconvert --to python notebook.ipynb notebook.py
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1357#issuecomment-192720472
> .
 friendly ping -- what needs to be reworked?

Also, @craigcitro for general advice on migrating our tutorials to testable notebooks rather than having copies that will inevitably diverge.
 Is this PR still active? 
 We want to move our tutorials to notebooks, but I guess we're not ready now.  Thanks for this PR -- we'll try to get to this for all of our tutorials eventually.
  It sounds like you are running out of memory
 Yes, this sounds like an out-of-memory issue.  Try reducing your model size / number of concurrent active models.  Note that we will continue to try to improve our GPU memory utilization, so the situation may get better over time.
 I think we need a small representative example to reproduce the problem -- we shouldn't have any cumulative allocations since we reference count everything, and the only state that persists across run calls (in the non-partial run case) is stored in Variables
 Maybe worth trying cuda tools but:

1) We should dump out of memory information to LOG(INFO) when we OOM: https://github.com/tensorflow/tensorflow/blob/cc9bfbf8ef4a3dea6514ad939d238f7442188247/tensorflow/core/common_runtime/bfc_allocator.cc#L639

2) Try setting https://github.com/tensorflow/tensorflow/blob/16d395e5dea687ab3aece0a462e631de25c8d77d/tensorflow/core/framework/log_memory.cc#L25 to true and see if you get more information ?

I haven't looked at Keras code, perhaps it is using a single session and adding/initializing new variables to the session for each new model?  Variables are persisted through the lifetime of a session, so it would be wise to use a different session for each "model", if creating that model requires creating variables.
  http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory
  That's really strange -- can you try installing in a virtualenv from scratch, to isolate the installation?
 Also, if `import tensorflow` works, can you report the output of `dir(tensorflow)`?
 Glad to hear it's working, and sorry for the installation trouble.
  Thanks for the report, we'll get in a change tomorrow!
 Fixed in c3c40b7a8950bba876b92456824e5b6a9505c0a7.  Still one more error to fix.
  https://www.tensorflow.org/versions/r0.7/tutorials/deep_cnn/index.html

the image is there -- don't rely on github's .md files to look perfect :(
  It's possible this is not a bug -- changing the batch size often requires changing the learning rate.
 I think we would need real code to reproduce this, and very strong evidence to suggest there is a bug to make it worth our time to investigate (we do have 276 open bugs and growing...)
 @keskival Not sure if this is the issue, but the 970 has a strange architecture with 3.5GB of fast memory and 0.5GB of slow memory - from what I've read it isn't recommended to go over the 3.5GB when running cuda programs. This might interact poorly with tensorflow's default allocation of all available GPU memory, although I don't see why you would get NaNs.
  Please don't cross post stack overflow questions as Github issues.
  Yeah, I think this is the only workaround for now -- it looks like during gradient construction, we have to aggregate the gradients to 'W' from the two different sources with different shapes, and it's not easy to know that one of them needs to be zero-padded.  Slicing provides this extra bit of information.

I'm not sure of a straightforward way to make this easy, so I think your workaround is probably the right thing to do.
  I am not sure either.  It looks like maybe the failure is during nvcc compiling aggregate_ops_gpu.cu.cc, but I don't understand why it works for us at CUDA 7.0 and not for you.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  op.device reflects what the user specified, because it is part of the GraphDef that is sent to TensorFlow.

If you use log_device_placement in the ConfigProto, you will get a log of what the assigned device is (after applying all soft_placement rules, etc).  I don't think op.device ever could reflect what the runtime actually did.
 An update here: you can call `Session.run()` passing a `RunOptions` protobuf that requests a trace, and fills in a `RunMetadata` protobuf with details of all the ops that ran, on which device they ran, and how long they took. I think this gives you the programmatic access that you're looking for. Here's an example:

https://github.com/tensorflow/tensorflow/blob/6f90ede2496134777a948fea872b17a67b4f6ef2/tensorflow/python/client/session_test.py#L913
  That is indeed a bug.  We can add the proper shape validation code in that function to prevent the unexpected broadcasting.
  I don't think we have any plans to do this. In the absence of much interest, I'm going to close this.
  @vrv: Was this supposed to be closed by your commit?
 Yes, sort of.  We no longer log those warnings, because they were just superfluous warnings.
  Unfortuately we don't have a pip package for 32-bit linux right now.  You'd have to build from sources, and I'm not even sure it's supported.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 LG, can you rebase to master?
 Nevermind, there's no conflict
 Merged
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Merged
  Can you try:

pip uninstall tensorflow
pip uninstall protobuf

then just reinstall tensorflow ?

See: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#cannot-import-name-descriptor
  I think I like this one more than #1345.  can you rebase?
 Nm, the icon they switched to looks like the conflict icon.  @tensorflow-jenkins: test this please
 Merged
  I'm fixing the first issue, will try to figure out why the second one is happening.
 You seem to be building the target `//tensorflow/python:zero_out_op_test`. `bazel query tensorflow/python/...` doesn't show that it exists. Is that something that you added? What does your rule look like?
 The above commit probably fixes some issues, and maybe some others, but leaving it open until we fix any other issues that have cropped up recently
 I pushed c3c40b7a8950bba876b92456824e5b6a9505c0a7 to fix the first problem, but now there's one more linker problem.  I just fixed that internally and will be pushing it out momentarily.
 Okay, I think the builds are going to be green (or at least, not broken).
 @keveman, @mrry, any ideas?
 I don't see these errors when building `//tensorflow/tools/pip_package:build_pip_package` on a Ubuntu 14.04 machine. What Bazel version are you using? What OS version?
  Can you try building with --verbose_failures and repaste your logs?
 Closing since it looks like the problem is not having installed zlib.
   Thanks, can you update to master?
 Merged
  Thanks, can you update to master?
 @tensorflow-jenkins: test this please
 Flaky test failed.  Merging.
  Okay I updated the website and pushed the fix.
  Bazel likes to strip things it doesn't like out of `PATH`, as discussed at https://github.com/bazelbuild/bazel/issues/957.  For TensorFlow, this is a problem since we have a shell wrapper around swig that just says `swig "$@"`.  In my case, `swig` is in `~/homebrew/bin`.

Since Bazel considers this a bug in TensorFlow rather than in Bazel, it may be more useful to have the discussion here than on the Bazel site.
 Here is the problematic file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/swig/swig.sh
 Perhaps we can use some kind of crosstool wrapper-type functionality?
 Alternatively set it up during the configure script, the way we do python: as a parameter in bazelrc.
 @ebrevdo: I think configure is the way to go.
 `configure` hack complete.  I'll send it for review soon.
  https://github.com/tensorflow/tensorflow/blob/795f35da2d458cbae477ac2fe2bff80c1427a771/tensorflow/python/ops/rnn.py#L92 -- I think the initialization of the state in the rnn cells is zero.  Is that what you were wondering?
 https://github.com/tensorflow/tensorflow/blob/0e9240b72613aa084728dcba55f7fae3591e86ad/tensorflow/python/ops/variable_scope.py#L65

Does that help?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks, can you update to master?
 Apparently github changed their icons for when the changes are out of date, and they chose a color that matched the indication for 'this has a merge conflict'.  Sorry for the noise.
 Merged
  May not get to this till next week.
 @ebrevdo: Are you still working on this, or should I mark it contributions welcome? 
  Indeed -- variables created in other ways than with tf.get_variable(...), esp. by the lower-level tf.Variable, are not added or recognized by variable_scope. This is partly intentional (as some special variables may need to be treated specially) and partly a result of how the variable sharing process developed.

My suggestion for you would be to always use tf.get_variable if you're relying on it later -- why not?
 Definitely! Re-opening so we don't forget it, thanks!
 Indeed - any sub-scope of a reusing scope is also reusing. So just setting things with .reuse_variables() can be a bit dangerous. We could forbid it and have only context, but on the other hand it's convenient from time to time.
 @lukaszkaiser: Are you planning to make the documentation fix, or should I mark this contributions welcome or close? 
 I think there is a true bug here, not a documentation problem. Variables loaded with saver should be imported to with with get_variable. It is being addressed in tf.learn (and also in the context to make partitioned variables work ok). I think Illia was working on that, but I'm not sure how far it got.
 @ilblackdragon: Reassigning to you; feel free to adjust if that's wrong.
  Just to clarify: the code works, it's just that shape inference does not infer the batch size, right?

I think it's harder to infer sizes of 1 because they have different behaviour in terms of broadcasting. Assigning to Derek who know the shape inference code much better.
 Yes, this seems to happen because, internally, we compute `tf.mul(x, y)` where `x` has shape `[None, 2000]` and `y` has shape `[batch_size, 1000]`. If `batch_size` is 1, then it can't tell whether `None` is actually `1` (i.e. there's no broadcasting), or a number greater than `1` (i.e. broadcasting happens).

The typical way to deal with issues like this - when shape inference is hard to do - is to have the higher-level library function (e.g. `BasicLSTMCell()`) call `Tensor.set_shape()` with the statically known shape.
 I was thinking about this, and I'd like to know more about your use-case. While we could add the extra shape inference code to BasicLSTMCell, the question arises where else should we do it. In every LSTM cell? In every layer in contrib, in every layer we write anywhere? I mean -- that's the reason we have automatic shape inference, so we don't write (and maintain) this code at every point.

The above is the reason why I'm hesitating to add extra shape code, and is did not seem to be needed until this point. But of course we need to make your code work, so it would be good to see if it's just BasicLSTMCell or more places or an instance of a more general problem. Could you share more about your code and how the lack of the shape on batch dimension hurts you?
 @lukaszkaiser: Closing for now since the original issue is not fixable given broadcasting. 
  Thanks, can you update to master?
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 Merged
  The directions for a development build say to make it, so .gitignore
should ignore it.
 Can you update to master?
 Merged
  @ebrevdo: Could you take a look or reassign?  I'm not sure if there's a problem we can fix here or not.
 Sounds like a bad download. We can auto retry the download, or catch the error and ask the user to delete the gzipped files before rerunning. Not sure who wrote the original mnist fetching code.
 @ebrevdo: I don't think we should do too much work if the issue is a server outside of our control.  PRs welcome if someone wants to make this more robust: if so please comment and I can reopen the issue. 
  We wouldn't be able to actively support it, so a Lua layer would be best as a separate repo.  If you want to work on it, there is some discussion on https://github.com/tensorflow/tensorflow/issues/388 that may be relevant.
  In some sense the reuse checks in tf.get_variable are indeed superfluous -- one could, as you say, just always create non-existent variables regardless of the reuse-check setting (and get rid of reuse).

When designing variable_scope we discussed this issue at length. The decision to enforce reuse-checks was motivated by a history of research model development. It happened a few times that people trained models that had more variables than they thought, exactly because these parameters were silently created in the way you suggest. The models still gave reasonable results, so the problem was hard to debug -- but the results with proper reuse were much better. Taking this into account we decided to enforce reuse checks quite strictly. But it's python -- it's easy to disable the checks if you want to, e.g., like this.

```
def unsafe_get_variable(name, ...):
  try tf.get_variable(name, ...)
  except _:
    with tf.variable_scope(tf.get_variable_scope(), reuse=True):
      tf.get_variable(name, ...)
```

But it's a dangerous road, the checks can save you some debugging later, so consider not disabling them.
  Merged
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Already did: https://github.com/tensorflow/tensorflow/commit/55be24524167c1d1879dba64b492c33c82244d1f
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please

@martinwicke @jendap FYI, not sure what the implications of this was
 @tensorflow-jenkins: test this please
 Merged
  Merged
  Working on a bugfix now.
 Should be fixed at HEAD.
  Several users have reported an issue that occurs with the following steps (e.g. Issue #1297):
1. Start TensorFlow in a setting with a GPU.
2. Define some variables with no explicit `tf.device()` set. This most often happens with embedding variables (used as arguments to `tf.gather()` or `tf.embedding_lookup()`).
3. Initialize them. (They will be placed on the GPU, because it is the "best available device".)
4. On running the first training step, the following error (or similar) is raised:

```
InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'
```

The `SparseApplyAdagrad` op (or in general most `SparseApplyFoo` ops) is only defined on CPU. The variable has already been placed on GPU, so the graph is not placeable. Attempting to run the same program with `use_soft_placement=True` also fails, although with a stranger error.

**TL;DR:** If this affects you, create your variables in a `with tf.device("/cpu:0"):` block, until this issue is resolved.

The issue arises because (i) TensorFlow places variables on the first device where they run, (ii) it always prefers GPU over CPU when it is availabe, (iii) initialization ops are available on GPU, and (iv) it applies the placement algorith to the _pruned subgraph_ (not the entire client graph).

One workaround would be to apply the placement algorithm to the entire client graph. (This is the approach used in the separate `master_session.cc`/`simple_graph_execution_state.cc` codepath, used in the distributed runtime.) However, this has the effect of leaving the session in a broken state as soon as an unplaceable node is encountered. Switching to this behavior might cause issues for people doing exploratory graph construction in a REPL (IPython etc.) because the only remedy would be to recreate the entire graph on seeing such an error. Therefore, while failing fast in a non-interactive setting would be fine, a different solution for interactive use might be required.
 (The root bug was fixed a few weeks ago by placing the entire graph on first run, not just the pruned ones).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.

Testing for core/lib/io change.
 Our tests are down for the weekend, so we'll have to wait a bit. Sorry about that.
 @rbharath can you please squash the commits? It would be great to have cmake working! I will try to set up jenkins build for cmake later this week.
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Yeah, we are constantly messing with the location of header files etc.
Sorry. It should stabilize shortly.

On Thu, Mar 3, 2016 at 2:04 PM Bharath Ramsundar notifications@github.com
wrote:

> As a quick update, it appears that the cmake no longer builds tensorflow
> head due to recent changes in tensorflow :-(. Fixing this looks tricky, and
> I probably won't have time till next week or so. I'll update this thread
> then.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1309#issuecomment-191986089
> .
 I forget that cmake build. I will try to get it done by tomorrrow.
 There is very simple https://github.com/tensorflow/tensorflow/pull/1488. That should be enough for linux cmake cpu build.

I'm have set it up to run on pull request but only when pull request touch cmake. (Bash condition `[ -z "$(git log $master_sha1..HEAD tensorflow/contrib/cmake)" ]` is used for triggering).

That should allow to run cmake tests while not showing errors on non-cmake tests.

Once we get cmake working well we can enable it for all the builds. Then we would love to create windows build as well.
 @rbharath can you please modify this PR to contain the intended changes only?
 Ok, thanks! I hope somebody will help.
  @danmane can you take a look?
 Ah, that makes sense.  In general, if you have installed the pip package, we recommend checking out the github repo at that branch, since we make improvements to master which often updates the code that goes into the binary pip package.  Thanks for the help!
  @mrry: Is there any easy to get `DGPR_BACKWARDS_COMPATIBILITY_MODE` into our grpc build in a suitable version dependent way?
 @girving: Turns out it's quite easy: #2697.
  The shape bug should be fixed at HEAD
 We have another commit in the works to fix the second problem.
On Feb 26, 2016 4:40 PM, "nryant" notifications@github.com wrote:

> Regarding scope issues, it appears that the TensorArray instances created
> in dynamic_rnn always have the same value for tensor_array_name, which
> causes issues if you try to use dynamic_rnn multiple times in the same
> graph; for instance, when building a bidirectional RNN.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1306#issuecomment-189539201
> .
 @ebrevdo: Assigning to you if something is in the works.
 alquraishi: can you provide a list of things still broken at head of master branch?
  Also added auto version update for README.md in update_version.sh. 
 @vrv can you merge this? My workstation is packed up and I don't  have script access.
 Merged.  

Our tests machines are down for the weekend so we'll probably hold off merging anything else until Monday.
  It certainly can! Have a look at the [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/README.md) for how to get started. You can place nodes on GPUs on different machines using `tf.device()` statements:

``` python
with tf.device("/job:worker/task:0/gpu:0"):
  # ...
with tf.device("/job:worker/task:0/gpu:1"):
  # ...
with tf.device("/job:worker/task:1/gpu:0"):
  # ...
# ...
```
  @chemelnucfin: Is this bug fixed now? 
  I think I merged this a few days back.
  Can you resolve the conflict?  Then we'll merge
 merged
  63fd88065526c6301569743b1cec12fd2561589f enables inf/NaN debugging (if requested) for fp16, like you already can with fp32 and fp64.
 Some changes from @benoitsteiner: d526527a7f2bb3d29fed58b9f759f59e9a49c890 enables fp16 support for the tiling ops, 7ae67a14e273a000e5b306d335dc0f1d7c0650c1 enables fp16 support for the l2loss kernel.
 523055469c8a61425e3b8f104be67787c2933ccb adds support for fp16 GEMM (matrix multiplications) to StreamExecutor. 61b12f567ad167556bf55d4375112ed262d37975 does the same for convolution operations via cuDNN. (Both require CUDA 7.5.) Unfortunately there were some late changes to how convolutions are done in TensorFlow proper, so we'll need a followup commit for updating the cuDNN use.

Note that neither of these actually add support to the TensorFlow MatMul and Conv2D ops, but they're necessary building blocks for that.
 6350f17524f0f6f3761e84589846191a90fef061 does the required fixups to StreamExecutor.
 4d55cb5c55fcc85009171b6a4657cbd966fd85e5 adds fp16 support to the MatMul op (gated on CUDA 7.5 or higher). This is a major milestone for fp16 support; it means that many real graphs can probably be  trained and run in fp16.

Convolutions are missing still.
 @kunal-bajpai: It will almost certainly happen next week.
 @kunal-bajpai I'm hoping to land a set of improvements necessary to make the convolution code work on fp16 ready in a couple of days. We should be able to support convolutions on fp16 shortly thereafter. 
 Some image ops while we're waiting (I'm trying to make Inception train with fp16 end-to-end): 48b52d88a419d26beb9ee5cb3c19dba61c68cc3b makes SummarizeImageOp support fp16, c3465a857fcef34fcf2894e90af882f11007762e makes DrawBoundingBox support fp16 (although the bounding box is still fp32).
 https://github.com/tensorflow/tensorflow/commit/36357e7e1127873165694a38e3a989df4e0b6ffe adds support for fp16 to the batch normalization operations.
 99671f3a705789ef217c7bca92409add3cf529e5 fp16-enables ReverseOp. 6164d02144239c58a8f19cd12ff2a3ff7b7605d4 (by @benoitsteiner) fp16-enables AddN. bb0190f6c26bf11f601102dfe2166a68a7833020 (also by @benoitsteiner) fp16-enables the softmax ops. 80da0a63200cb7c9c449188620992c7a8d18c8b9 fp16-enables the resize ops, although you should note that some of them always output float no matter the input (this was preserving existing behavior, although I'm not sure it makes sense for half or double).

And… 4f257a2427ba0414bd7513c9b61fb835870bd3cf fp16-enables convolutions on GPU, assuming you have CUDA 7.5 or newer.

You still can't MaxPool/AvgPool, but that's up for next week. After that, I will declare this feature complete, since you can actually do useful stuff with it. There's always more performance to be had, though (currently it's pretty much performance-neutral on Maxwell, so it's mostly about the memory savings), more ops that need conversion and so on, but that should probably live outside this bug. And that work will be continued by Benoit alone, as I'm leaving Google and thus TensorFlow. :-)
 @kunal-bajpai Most if not all the coefficient-wise operations should work on half floats. I have checked that this is the case for ReLU.
 I have added support for pooling on fp16 in https://github.com/tensorflow/tensorflow/commit/b7c416926e4d31f7d7a924bfdc97580bf7d44c04. I have also added a --use_fp16 option the convolutional mnist model at tensorflow/models/image/mnist/convolutional.py to make it train using fp16 instead of fp32.
 I don't think the pooling operations made the 0.9 cut, but they'll be available in the release that comes after 0.9. In the meantime, you can always [install from source].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#installing-from-sources)
  @zheng-xq: I vaguely recall this coming up in another context.  Have we hit anything like this?
 I've go through the repro instructions, and all the test cases are passing
for me locally.

test(batch_size=3, set_others=0)
test(batch_size=4, set_others=0)
test(batch_size=3, set_others=100)
test(batch_size=4, set_others=100)
test(batch_size=2, set_others=0)
test(batch_size=1, set_others=0)

My environment: Titan-X GPU, Cuda 7.5 SDK, and Cudnn R4.

Which Cudnn version are you using? I would highly recommend to try Cudnn R4
and see if this problem would go away.

On Tue, Mar 8, 2016 at 10:54 AM, Geoffrey Irving notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq: I vaguely recall this coming up
> in another context. Have we hit anything like this?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1299#issuecomment-193913374
> .
 Closing for now.  @cesarsalgado: Please reopen if the issue persists with cuDNN R4. 
  That other problem you reference should be fixed:
https://github.com/tensorflow/tensorflow/issues/1117
So that's likely a different issue. I have never tested the class notebooks on GPU.
Just to make sure: your setup works fine on CPU?
 Does using:
`with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session:` fix it?
 @mrry here is a tiny colab notebook which works fine on CPU, but fails on GPU even with soft placement in r0.7. Any idea?
 I think this is a known issue (at least offline), where running the initializer will cause all variables to be assigned to the GPU, then subsequently trying to run SparseApply<FOO> on one of those variables will fail because SparseApply<FOO> doesn't have a GPU implementation. (@vrv has an idea for how to fix it, but it might break other users, so we haven't pushed it yet :(.) The workaround is to pin just the offending variable to CPU using `with tf.device("/cpu:0"):`.

I think for the [notebook in question](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb), this involves changing the line:

``` python
embeddings = tf.Variable(
  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
```

...and replacing it with:

``` python
with tf.device("/cpu:0"):
  embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
```
 Please leave it open. I need to add the workaround to the notebook. (PRs gladly accepted)
 Sorry for the confusion... it looks like you might also need to pin the `softmax_weights` and `sotmax_biases` to `"/cpu:0"`. I hadn't read the following code closely: these are both used in `tf.nn.sampled_softmax_loss()`, which uses `tf.nn.embedding_lookup()` internally, and thus results in sparse updates in the training step.
 Merged a workaround that pins everything to CPU. Please follow #1310 for the actual fix to make it work on GPU as well.
  Thanks, can you update to master?
 Merged
  Can you explain what you want to do with these files?

It's possible this is a duplicate of https://github.com/tensorflow/tensorflow/issues/720: we don't yet have a devel package that installs all of the headers -- the pip package just distributes the python interface and C++ runtime, not the code to do development on it.  You currently have to integrate with bazel and our github tree to do development, but we're hoping to get #720 resolved eventually to make it easier.
 Gotcha -- yes I think this is related to https://github.com/tensorflow/tensorflow/issues/1270 -- we have a fix to the pip package installation to make these available.  Our test nodes are down for the weekend so we'll push on Monday.  Let's take discussion to #1270.
  1) Breaking up the pip.sh into the install and test-on-install

This is aimed at automating testing of docker images.
Also, a step is added in pip.sh to uninstall existing versions of
protobuf and tensorflow if they exist. This addresses pip install-test
issues on non-Docker environments (e.g., Mac)

2) Using virtualenv to perform pip test-on-install

The built pip-package is installed in a virtualenv, then the pip install test and the tutorial
test are carried out inside the virutal env. This is done inside the same docker container as the one used for bazel build, if docker is available. 

3) Automating Docker image build and test

ci_parameterized_build now has a new env-var parameter:
TF_BUILD_DOCKER_TEST, that can be used to trigger Docker image build
and test following the PIP build (i.e., TF_BUILD_IS_PIP=PIP or BOTH).

The image is automatically tagged as ${USER}/tensorflow:${VERSION},
wherein the version is extracted from version.h.

The main script for Docker build and test is the newly added:
docker_test.sh

It uses Dockerfiles in tools/ci_build/docker and installs the whl files
inside the Docker image. After the pip installation step, the Python
unit tests and tutorial tests are inside the Docker container.
 Splitting the test out of pip.sh is good. Using virtualenv is good. Testing the docker images is, imho, nice to have but not critical for now. We should not change the docker files. Also putting docker files testing into parametrized build is, imho, an overkill.
 Addressed @jendap's comments. Closing this PR and opening a new fresh one with all changes squashed. https://github.com/tensorflow/tensorflow/pull/1380
  Once https://github.com/tensorflow/tensorflow/pull/1265 is in place this becomes very easy to fix.
  Duplicate of #464 (contributions welcome!)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Can you go to https://cla.developers.google.com/clas and check that the corp CLA for Arterys shows up as "CLAs I am covered by"?
 CLAs look good, thanks!

<!-- ok -->
 Thanks, can you update to master?  Then we can test.
 @tensorflow-jenkins: test this please.
 Our mac tests are broken, but this otherwise lookss good. merging.
  So this require the entire world to upgrade to bazel 0.2, right?

@tensorflow-jenkins: test this please, but note that this probably will fail if the answer to the above question is "yes", since we haven't updated Jenkins to bazel 0.2 yet.
 We are going to upgrade bazel to 0.2 today anyway in https://github.com/tensorflow/tensorflow/pull/1206. We will update mac as well and this will work :)
 @tensorflow-jenkins : test this please
 Can you bump the version of required bazel https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE#L21 here and then we can re-test?
 @tensorflow-jenkins test this please
 Some target in protobuf is missing a `srcs_version = "PY2AND3"` annotation. See 
http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/262/consoleFull
 Also looks like build_pip_package needs to be updated.
 The exact command is in the log, but the main point is bazel test
//tensorflow/...

On Fri, Apr 8, 2016 at 14:13 David Z. Chen notifications@github.com wrote:

> Opened google/protobuf#1402 https://github.com/google/protobuf/pull/1402.
> 
> What is the Bazel command run for
> http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/262/consoleFull?
> I tried bazel test //... but I got an error about the BUILD file for
> gmock not being found:
> 
> ❯❯❯ bazel test //...
> ERROR: /usr/local/google/home/dzc/Projects/tensorflow/tensorflow/tensorflow/workspace.bzl:21:3: no such package '@gmock_archive//': In new_http_archive rule //external:gmock_archive the 'build_file' attribute does not specify an existing file (/usr/local/google/home/dzc/Projects/tensorflow/tensorflow/google/protobuf/gmock.BUILD does not exist) and referenced by '//external:gtest'.
> ERROR: Loading failed; build aborted.
> INFO: Elapsed time: 0.190s
> ERROR: Couldn't start the build. Unable to run tests.
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1289#issuecomment-207608299
 Can you update the commit hash for protobuf?

@vrv, @keveman: Just to make sure -- this shouldn't interact with our binary protobuf distribution, right?
 @tensorflow-jenkins test this please  (to see what's broken)
 master tests are failing right now, so don't expect a clean build.
 or not. Never mind.
 Jenkins, test this please.
 @tensorflow-jenkins test this please
 ready to test?
 @tensorflow-jenkins test this please
 that failed quickly: ERROR: /workspace/tensorflow/workspace.bzl:84:3: no such package '@gmock_archive//': In new_http_archive rule //external:gmock_archive the 'build_file' attribute does not specify an existing file (/workspace/@protobuf/:gmock.BUILD does not exist) and referenced by '//external:gtest'.
 test this please
 Almost, looks like the pip install doesn't work

Python binary path to be used in PIP install: /usr/bin/python (Major.Minor version: 2.7)
Thu May 26 05:26:43 UTC 2016 : === Using tmpdir: /tmp/tmp.PJlk9Vtu7r
rsync: link_stat "/workspace/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google" failed: No such file or directory (2)
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.0]
build_pip_package FAILED
 Yeah see the section called Create the pip package and install on https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html
 @tensorflow-jenkins test this please
 Sweet, this is awesome.  @martinwicke @caisq okay to merge?  (I'll have to do some work internally to bring this change in, but I'll sign up for that this week).
 Woot

On Thu, May 26, 2016 at 3:25 PM David Z. Chen notifications@github.com
wrote:

> \o/
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1289#issuecomment-222012562,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_VGti6nugRU9vmAFc_GOhwHn_B-1ks5qFh3NgaJpZM4HjOh8
> .
 Yay!
 We are sorry for the inconvenience @StephenOman!

It is much better to download git repos using bazel. Also for all the builds there is a lot of them, not just protobuf.

What build tool are you using? We have also have (incomplete):
1) [contrib/cmake](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake) - it may not work today but could be made work with some effort
2) [contrib/makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) - this one is super new. It is there to build the ios example. We will create CI build for it next week.

Please take a look at those two. Please contribute if one of them would suite you. Once we get them into reasonable state we can enable builds for them in ci.tensorflow.org. That way we make sure no change break that. No more breakage for you :)
 Take a look at [contrib/makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile). We have added it exactly for this reason. It will be recommended way of working in xcode. We will soon have automated build this so it should work in future. But it is very new (may be even broken right now).
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Yeah, can you remove the change to stream_executor? If it still works after that change (which I expect it would -- my change was almost identical except for that), I'll merge it.
 Jenkins, test this please.
 Ah. backwards_compatibility_test. Check the build log -- it's a simple thing to fix, the console log contains instructions. Add the changed files to this PR.
 You should be able to run 

bazel-bin/tensorflow/core/ops/compat/update_ops tensorflow/core/ops

and things will work
 Can you rebase? Sorry I sat on this.
 Jenkins, test this please.
 yes, please update, I can't merge without (we discontinued our merge script and now we're stuck with github's button)
 if you update from github it'll make an ugly merge commit. It's really bad
actually. and they don't allow merges any more if the PR is not rebased to
head. Silly.

On Tue, Mar 15, 2016 at 1:30 PM chemelnucfin notifications@github.com
wrote:

> ok. Let me see if I can update from github and merge it or if I need to
> rebase and push from my end.
> 
> On Tue, Mar 15, 2016 at 1:15 PM, Martin Wicke notifications@github.com
> wrote:
> 
> > yes, please update, I can't merge without (we discontinued our merge
> > script and now we're stuck with github's button)
> > 
> > —
> > You are receiving this because you authored the thread.
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/tensorflow/tensorflow/pull/1288#issuecomment-197002878>
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1288#issuecomment-197007761
 @martinwicke: we can still merge when the branch is out of date, as long as there is no conflict.  It's just that they recently changed the icon to match the same one that indicates conflict, so it's hard to tell :)
 I guess the reasoning is that if you merge, you never know whether the merge is any good.
 @vrv You rolled this back, right? It's not at head -- do you remember what the problem was?
  Hi @dgolden1,

Thanks for reporting and taking the time to include a clean repro. Would you mind trying this setup against the master branch? We've been doing some work in improving the pipeline for large graphs, so it might be that this is already fixed at head.
 In general, using gulp vulcanize then bazel build will get you the latest and greatest TensorBoard..
Although now that I [released a new compiled TensorBoard](https://github.com/tensorflow/tensorflow/commit/65c9124086240616747e26e0aa5ef3412a3be55d) that is more recent than the improvements that @jameswex are describing, it should be enough to just use bazel. (on master)
 Was able to reproduce the issue using python3. The problem comes down to `str` and `bytes` being equivalent types in python2, but not in python3. Moreover python3 bytes requires an encoding to be specified when converting a string to bytes (protobuf uses utf-8 for encoding strings).

Fix is on the way. The commit should appear tomorrow. If you don't want to wait, a small fix that makes it work just for python3 is to replace line 66 in `process_graph.py` from
`node.attr[large_attrs_key].list.s.append(str(key))`
to
`node.attr[large_attrs_key].list.s.append(bytes(key, 'utf-8'))`
 And also replace line 58 in `process_graph.py` from
`keys = node.attr.keys()`
to
`keys = list(node.attr.keys())`
 So we have python 3 tests, but they are not fully integrated requiring us to run them manually. A change yesterday broke TensorFlow on python 3 and fixes are on the way.
  This will happen once we stop using linear() in the rnn cells and use layers directly.  Lukasz is working on that.
 @lukaszkaiser: @ebrevdo says this will happen automatically as a consequence of other work you're already doing.  Is that right?
 Eugene is right that it'll happen when we replace linear with layers, which should happen (as part of the general move to layer/tf.learn, right?). I'm not working on it right now though.
 We need an ETA on the move of contrib.layers to core.
 Indeed. Should we mark this in some way to indicate it's blocked?
 @lukaszkaiser I don't know any way to do that, unfortunately.
  The error message should be pretty helpful, I think.
 Closing.  The issue is that "you should not try to import tensorflow from its source directory", as the error message states.
  Can you explain what you mean by "when building the op library"?  Are you trying to build new C++ code (perhaps user defined ops) that depend on TensorFlow?
 To confirm: You can build tensorflow just fine without the `ZeroOut` op, and that error shows up when you add `ZeroOut`?  @keveman: Have you seen that kind of error before?
 Can I see a branch with your change?
 That's a strange setup.  If you're going to build from source, can you grab tensorflow via git normally rather than checking pip into git?

@keveman: Are we close to replacing the `user_ops` directory with something that doesn't require modifying the tensorflow tree?
 @jackpaparian, you are right, you should be able to build a new op with Tensorflow binary pip installed.  Can you try installing the pip package from [here](http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) and then try compiling your user op? The pip package for r0.7 had some issues related to user ops. See #1569 for some related discussion.
 Looks like a bug that creeped in recently. Fix on the way. Sorry about that.
 1a25c92 adds selective_registration.h to the pip package. Please verify with the most recent pip package [here](http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) and close the issue if successful.
 I ran the following and things worked for me : 

```
$ pip install -U http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastStableBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl
$ TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
$ g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I $TF_INC
```

I ran this inside a virtualenv, on a Ubuntu 14.04. I can definitely see that the file is available

```
$ ls local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/selective_registration.h 
local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/selective_registration.h
```

Perhaps you are still somehow installing an older whl file.
  We would strongly suggest upgrading to 0.7.1 (sync to the 0.7.1 tag if needed, or HEAD if you are daring). tensorboard had a few bugs in 0.6.0 which should mostly be fixed now.
 Closing due to lack of activity.  Please reopen if it's not fixed in 0.7.1.
  We'd love to have it in the models repo if anyone writes it: https://github.com/tensorflow/models!
 @aidangomez Can you link to the pull request?
  - Some people said that renaming cp34 to cp35 doesn't work for them, but you are welcome to try it before installing.
- If you want to install for python3, use the url the python3 wheel which is listed on our download page, and it should work.
  @josh11b: Any ideas here? 
 Does the CIFAR example set an epoch limit?  Looks like this might be
poor handling of running to the end in the CIFAR code?  That is the
normal end-of-input exception.
 It has `FLAGS.max_steps`, so looks like yes.
   This change needs to be made so that tutorial tests can work on Jenkins. I guess the file permission was somehow not set correctly when it is pushed from internal. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks!
  As the now-deleted-comment suggested, this type of detailed debugging is likely out of scope for GitHub -- you may get better help on StackOverflow or such. 
  @keveman, @josh11b 
 Sorry everyone about the breakage. Some changes got inadvertently pulled in that removed Eigen and third_party header files from the pip package. I added them back, and also clarified the documentation a bit. The 0.7.1 pip package still has the problem, but if you create the pip package from HEAD and install it, that should work.
 The website is lagging behind the documentation from HEAD. Can you please follow the instructions here : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/adding_an_op/index.md
  batch_norm_op.cc is not included in the default Android operator sets. You can add it in to one of the filegroups in the kernels BUILD file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L987
 @andrewharp: shouldn't we do this?
 Sure, I don't see any reason it shouldn't be included, just assumed it was left out on purpose.
 Thanks for the update @syed-ahmed -- will add check_numeric_op.cc to the extended kernel filegroup as well.
 @mattpoggi @entttom Try running [strip_unused](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py) on the graph to remove the unsupported jpeg decoding layers. For inception v3 I believe the input you'll actually want to use is "Mul:0".

You may need to also pass in --input_binary=true depending on the format of your graph.
 @dmirk I'm not sure that label_image will work if the decodejpeg op has been stripped out of the graph. Similarly it won't work on android unless you have stripped it out, so the symptoms point to it not having been removed. Can you double check that you're using the right output graph in both places please?

@natanielruiz You should be able to use strip_unused like so to remove the DecodeJpeg node:

```
bazel build tensorflow/python/tools:strip_unused
bazel-bin/tensorflow/python/tools/strip_unused --input_graph=inception.pb --output_graph=/tmp/stripped_inception.pb --input_node_names="Mul" --output_node_names="final_result" --input_binary=true
```
  We'll need more information to help: what is the error you're getting "from MatMul"?
 Oh no!  I appear to have missed a notification here.  Fortunately since this bug was filed we've added `tf.gather` for GPUs, so part (1) is fixed.

@zheng-xq: Do you want to comment on the multiple GPU bit?  
 @Palang2014, a typical set up is to have all the variables on CPU, and all the computation on GPU. If you have multiple GPUs, it is a good idea to merge the gradients on them before sending the delta back to CPU. 

It is not a good idea to place some ops on one device, and others on another, with fine granularity. The problem with that is the data transfer. Each time your data flow across device boundary, the device needs to synchronize and copy the data over. This can easily defeats whatever computation gain you are getting. 
  We don't currently test on K80s, so we can't guarantee it will work, but it should work.  We just won't currently be able to pinpoint a problem if there is a problem and it's specific to K80.

Maybe in the future we'll have a larger testbed of devices to validate against though.
  Thank you for the contribution!  I added a bunch of comments.  Let me know when I should take another look.
 A force push is good.
 Thanks, getting much closer!  I guess add commits for now if forcing removes old comments, and once everything is good we can squash into one.
 @Mistobaan: I don't much like it either, but I haven't thought of something good yet.  Do you have suggestions?
 Ah, got it!  `diag_part`.
 diag_part is mathematically unambiguous and matches common usage ("the diagonal part of A").  It's unfortunately that numpy is different, but `get_diag` is just as bad there.
 Looks good, thank you!  @vrv: How does one run tests and get this merged?
 Yep, an offset would be a reasonable addition for a future CL, but it has the unfortunate issue that it would ideally be an `input` rather than an `attr`, and adding new inputs is awkward to do in a backwards compatible way.

Also, before we merge (but perhaps after the tests?) we should squash this into one commit.  Thanks for keeping it separated for now to make it easier to incrementally review.
 only admins can trigger it :)

@tensorflow-jenkins: test this please
 Let's leave generalizations to a separate CL.  The symmetry of your existing change is good, since it doesn't change the existing op and adds it's one sided inverse.
 @tensorflow-jenkins: test this please
 Merged
  I would suggest uninstalling protobuf and installing the protobuf version you want to make Caffe work.  I would then suggest installing TensorFlow in a virtualenv, so that you don't have to worry about having incompatible copies of protobuf in your system libraries.  
  this would be a better question for StackOverflow, GitHub issues are for bugs / feature requests.
  Can you try pip uninstalling tensorflow _and_ protobuf, then reinstalling tensorflow?
   The main script for tutorial tests has arrived from internal: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/test_tutorials.sh
 Jenkins, test this please
 merged.
  Can you try uninstalling and re-installing TensorFlow?  That looks like a stale file (we no longer have AutoloadingMultiplexer).
 pip uninstall tensorflow
pip uninstall protobuf
pip install url/to/the/tensorflow-...whl
 Most likely you have a stale tensorboard.py in your /usr/local/bin dir (or some other bin path)
  What do you get when you run

```
pip --version
```

?
 Try installing the python3 wheel, not the python2 one.
 (https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl)
 Sounds like an issue with permissions -- consider installing in a virtualenv, as described on the install guide on tensorflow.org
  You probably don't want to run bazel with 'sudo'. 
  This is due to a deep technical issue with being able to access the values on a GPU.  For now, use tf.to_int64 to convert your values.
 Closed because the deeper issue has an open internal bug case.
 int32 should work fine on GPUs now.  I'll fix this.
  @lukaszkaiser, @ludimagister can you take a look (once this is updated to master)?
 This change looks good, just needs a merge.
 Let's get it in, what's missing?
 I still see "This branch is out-of-date with the base branch. Merge the latest changes from master into this branch." just above this comment. Are you sure it's all merged?
 Thanks for doing this -- just a few comments from my side, and then if everything looks good, please squash the commits and we'll test and merge.
 Don't worry about that, it just means we pushed a commit since the last
update.

On Wed, Mar 2, 2016, 11:07 PM Lukasz Kaiser notifications@github.com
wrote:

> I still see "This branch is out-of-date with the base branch. Merge the
> latest changes from master into this branch." just above this comment. Are
> you sure it's all merged?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1249#issuecomment-191619663
> .
 @tensorflow-jenkins: test this please
 No need, as long as there are no conflicts we rebase for you.

@tensorflow-jenkins: test this please.  PLEASE.
 Merged
  How do you build auc.so?
Like it says in the documentation [here](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#building-the-op-library), make sure you have `-rpath $TF_LIB` in the command line arguments to `g++`. If you indeed pass that flag and it still doesn't work, try adding `$TF_LIB` to your `$LD_LIBRARY_PATH`. I verified that I can indeed load an op with `load_op_library` and didn't see the errors you are seeing. Closing the issue. Please reopen with more information, in case it still doesn't work.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  @theaverageguy said he was working on it, but no movement in a while, so go ahead.  As always, good documentation and tests are going to be needed!
  I believe the reshape option only reshapes if the shapes have matching sizes.  Here the lhs would have 1 element and the rhs 60.  I'll post an answer on Stackoverflow to that effect.
  Feel free to reopen if @samjabrahams's suggestions did not work.
  Yes, earlier wheels were doing bad things to the protobuf installation, and the only way to recover is to uninstall to 'start from scratch.'
  This was recently added.  See sparse_ops.sparse_tensor_dense_matmul.  The first input should be a 2-D SparseTensor and the second a regular dense tensor (matrix) of the same dtype.

Backprop coming soon (at least into the dense tensor, no backprop into the SparseTensor is currently planned).
 I'm closing this issue unless there's missing functionality in the op I mentioned.
 Use sparse_ops.sparse_tensor_dense_matmul.  Does this do what you need?
 Feel free to open a new issue asking specifically for (SparseTensor, SparseTensor) matmul.
  Thanks so much for contributing this, Travis!  I've put some code comments above;  most are nitpicky, but some should be given a bit of thought (such as the Right Way(tm) to represent sizes, and adding more tests -- we depend heavily upon having good test coverage to not accidentally break things during refactoring.)
 Final final question (i hope):  What's the story for memory management w.r.t. calling Data on a tensor, and did you verify that the C allocations are getting properly deleted when the Go objects are destroyed?  (This may be obvious to someone with more recent swig experience than I!)
 Also, can you move the whole tree to contrib/go please? We'd like for fast-moving code to live in contrib.
 I don't actually know much about how that would look like from the go side. We're giving guarantees about the API stability for things not in contrib, so we need to be confident things will stay as they are, and for large chunks like this one, I'd rather have them live in contrib for a while until things have settled with some actual use.

Another alternative would be a tensorflow/tensorflow-go repo which could point to tensorflow/tensorflow as a submodule. 
 Agreed.  I think a final question for this - perhaps one that's deferrable, particularly if we put it in contrib - is:  What to do with Tensors once they're in Go?  A quick check doesn't suggest that there are immediately obvious libraries to use beyond 2D matrices, but I might be missing something.

Serving in many cases can probably get by with extracting arrays of basic types, but it's worth thinking a bit about what the better general solution is.  I don't have a great idea yet.

In chewing on this more, I'd probably go with something like:
UnmarshalTensor(t Tensor *, v interface{})

in an analogous way to Go marshaling for other 'foreign' types.  It should be relatively straightforward to reflect upon the tensor and unmarshal it into [][]floats, etc.

This seems like something that could be added later - this binding is enough to at least let one do that by hand for now.  That said, _having_ the unmarshal interface available might make testing a lot easier. :)
 Hi, tmc - just wanted to check on this, and note that if you'd like, I'm happy to help with some of the additional test code.  LMK.  I'd love to see this happen. :)
 (Indeed, GitHub is quite annoying.  We'll try to get to this soon!)
 Agh - I fail github 101 and didn't have my notifications configured properly.  I'll get on this towards the end of this week - I'm OOO.  Very sorry about that.
 Reviewing and checking now!
 Looks good aside from a few very minor typos, Martin's suggestion of moving to contrib/, and probably needing a few more tests.  Could you add some of the tests noted earlier, and we can get it merged?
 @alonsovidales - got it, moving my attention over there.  thank you!
  This is fixed in 0.7.1, which we will release soon.
 We just released 0.7.1 -- let us know if it still happens there.
  Are you using tf compiled from git? If so, how recent?
 That version may not have the function. Try building the package from HEAD.
 Did that solve the problem?
 @danmane This looks like a tensorboard dependency, can you verify?
 Yes, that is a TensorBoard dependency. It looks like something went wrong in the connection to GitHub while Bazel was downloading plottable (unexpected end of file from server).

Can you try re-building and see if it works? If it still doesn't work try `bazel clean` and try again?
 Closing since the original issue is fixed.
  This seems like reasonable behavior to preserve, since we're unlikely to ever add any kind of checking to these ops (doing so is problematic due to numerical error) and we're unlikely to normalize due to speed.  Documentation contributions welcome!
  The root of this is a protobuf library version mismatch, you appear to be using an older version that 3.0. There's more information, and some possible solutions, in #11.
 Try updating to 0.7.1 (cleanly -- uninstall tensorflow, probably even protobuf, and then reinstall 0.7.1).  Let us know if you still have problems.
  This is a question best suited for stackoverflow. Please ask it there to get a good answer. Regular softmax isn't a good idea for very large vocabularies.
  Thanks for this contribution @rekhajoshm, but we don't want to introduce command line flags to our library like this -- most options should be passed via ConfigProto or another proto.  We could maybe add a LogOptions structure to hold options related to logging.

Feel free to file an issue asking for a way to control log message output and we'll try to get it done.
  This is a question better suited to StackOverflow, since it is not a bug / feature request.
  My recollection is that when I put logging statements into the C++, I observed that the file was being closed properly.  This may have been a result of the Python object being unreferenced, though, and I suspect this is a better approach.  However, given that [the commit you mention](https://github.com/tensorflow/tensorflow/commit/67b2abcd673f466d32c4017ad8bab0d0b9ac86c5) does not appear to have been included in 0.7, I am a little worried about causing trouble for people who get the example from master but installed 0.7 via PiP or something.
 Can you update to master and ping this thread when ready?
 Actually looks like there's no conflict.

@tensorflow-jenkins: test this please.
 Merged
  Duplicate of #521.
  Looks like a bug.  Will take a look.
 We pushed, but I don't know which commit fixed this.  Assuming it was fixed :)
  I'm adding a section to the website in #1232, and pointing to it from the README. There's a number of them I still have to add, and I'd like to keep the README for resources contained inside the repo.
  Jeff, it looks like this check was recently added. Can you add some background on why it might be triggered? Thanks!
 Thanks for the update! I'll close this.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks, you'll also need to make the same modification to https://github.com/tensorflow/tensorflow/blob/f2bd0fc399606d14b55f3f7d732d013f32b33dd5/tensorflow/python/training/learning_rate_decay.py#L53 -- our MD files are generated from the original source documentation.
 Thanks, can you address any merge conflicts and squash the changes?  (Sorry, github doesn't notify me when you push changes without commenting on this thread).
 @tensorflow-jenkins: test this please
 Merged
  @tensorflow-jenkins: test this please
 I think this looks good, someone with more bash-fu want to double check before merging?
 Assigning to @girving since he is also touching ./configure :)
 Can you squash your commits?  We'll then test and merge
 Apologies for the delay, @vrv.  I can do the rest.
 The rest not including squashing, that is.
 Still waiting for the squash.
 Merged
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Merged
  When running the instructions to build the Android demo with the latest open-source repo, a protobuf visibility error causes the build to fail.

Steps to reproduce:
- On an Ubuntu system, git clone the repo following the install instructions.
- Run `bazel build //tensorflow/examples/android:tensorflow_demo`.

Results:

```
ERROR: /usr/local/google/home/petewarden/projects/tensorflow/tensorflow/core/BUILD:77:1: Target '//google/protobuf:cc_wkt_protos' is not visible from target '//tensorflow/core:protos_all_cc'. Check the visibility declaration of the former target if you think the dependency is legitimate.
```

Notes:
I think this might be a protobuf Bazel rule problem. I was able to fix it by patching google/protobuf/protobuf.bzl like so:

```
+++ b/protobuf.bzl
@@ -132,6 +132,11 @@ def cc_proto_library(
   if include != None:
     includes = [include]

+  if "visibility" in kargs:
+    visibility = kargs["visibility"]
+  else:
+    visibility = None
+
   if internal_bootstrap_hack:
     # For pre-checked-in generated files, we add the internal_bootstrap_hack
     # which will skip the codegen action.
@@ -141,6 +146,7 @@ def cc_proto_library(
         deps=[s + "_genproto" for s in deps],
         includes=includes,
         protoc=protoc,
+        visibility=visibility,
     )
     # An empty cc_library to make rule dependency consistent.
     native.cc_library(
@@ -157,6 +163,7 @@ def cc_proto_library(
       protoc=protoc,
       gen_cc=1,
       outs=outs,
+      visibility=visibility,
   )

   if default_runtime and not default_runtime in cc_libs:
```
 @davidzchen, @damienmg 
 https://github.com/google/protobuf/blob/master/protobuf.bzl hasn't changed in a month -- did a change in bazel cause this?
 Thanks - from inspection it looks like it's been fixed in a newer protobuf than we're pinned to in TensorFlow. I'll talk to Vijay about how we upgrade the submodule reference.
 Our submodule is pinned to https://github.com/google/protobuf/tree/c40f8c1f54f028b1ca73f3fb2dfdde500f94918f from 3 days ago, so I suspect it's something with your local config .
 For posterity I've confirmed that this does work at head when I do a clean git clone. It turns out the problem was due to doing a 'git pull origin master' from the main tensorflow directory that was synced to an older build. The protobuf submodule didn't update, hence the compilation problems.

From searching, it seems like I should be using more than just a `git pull` to update the submodules:
http://stackoverflow.com/questions/1030169/easy-way-pull-latest-of-all-submodules
 I'm extremely excited about #1069.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLA check still fails :(  Closing for now but comment here and fix the CLA issue and we can take a look.
   Merged
  Consolidating the two steps into one.
  Merged, thanks!
  Merged, thanks!
  Jenkins, test this please.
 Jenkins, test this please (again). @dongjoon-hyun I'm just using this PR to test some Jenkins stuff. Hope you don't mind.
 Merged. Thanks. I'm pretty sure this PR is not responsible for the coordinator test failure.
  I am not sure expanding the API of `make_tensor_proto` for this purpose is a good idea, but that's my opinion. Adding @josh11b and @mrry for opinions.
 I'm fine with this, but the code needs to be wrapped at 80 columns.
 Agreed, I prefer try/catching rather than adding args to the API.  Otherwise looks great.  Probably worth adding a test of the core functionality (without necessarily creating a 2GiB array)
  The memory optimizations between the CPU and GPU approaches are very different. In particular the GPU version will often allocate more memory to take advantage of batching and other approaches that speed up operations on CUDA devices. Is this causing an issue for you, or are you just interested in the inconsistency?
 We'll have better tools to understand memory utilization soon, I hope.  Closing for now, thanks!
  The inequality broadcasts, and the resulting variable is a vector. The vector decides for each batch entry whether to copy through or not.

It's a feature, not a bug!
  I've heard it's a problem with cuda 7.5 and ubuntu 14 -- downgrading cuda to 7.0 also apparently works.  Hopefully cuda 8.0 doesn't have this problem, but it's still in RC, so our pip installations still require 7.5.
 @vrv: Should we close, or is a fix possible on our end? 
 I have a way to reproduce or debug, so we either need a fix from the community, or a repro
 *I don't 
 @flow-ryan: Thank you for the instructions, but unfortunately this sounds like a bug below the level of TensorFlow, either in Linux or the GPU driver.  Since it's already fixed in more recent Ubuntu, I'm going to close.  We'll happily accept a PR if you find a reasonable workaround for the problem, though.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Can you squash the commits and make sure the email you have used for your CLA is the author of the remaining commit?
  This is a question about how to use PHP to invoke processes, not a TensorFlow issues.
  Do you have more than one installations of TensorFlow on your system? It could that the new tensorboard binary is trying to import from an older version of TensorFlow installation. 
 Can you try creating a new virtualenv and installing from scratch?  I just checked in a fresh install and that module exists.
 Hmm, our new wheels should get that function -- not sure how it didn't make it into the 0.7.0 ones.

As for the tensorboard bug, now I'm seeing it when building from HEAD.  Will look into it and make sure it's fixed in 0.7.1
 https://github.com/tensorflow/tensorflow/commit/716b7713e4c8b2d8f093f639ca41816cf4e1c696

I think that should fix it -- I'm cherry-picking that into 0.7.1 right now.  Feel free to let me know if it's still not working.
  @ludimagister: Is this problematic to fix? 
 Sorry for the delay, this is fixed at head now.

Mike
 @ludimagister: For future use: if you include "Fixes #<issue>" in the commit description the bug will be automatically closed on push.
  Mysterious. I don't know what cv2 does, but since you are building it with Cuda, it could be preallocating GPU memory (similar to what tensorflow does). The symptom is decidedly odd though. @zheng-xq, do you have an idea?
 TF takes most of the GPU memory upon its initialization. You can set the following session config to use smaller amount of memory:

config.gpu_options.per_process_gpu_memory_fraction

If that doesn't work, then it is useful to look at some profiling results and see where the cycles are spent. 
 Closing for now.  For better or worse, TensorFlow assumes it owns the whole GPU, so other libraries trying to use the GPU at the same time will cause issues.
  1) yes, Tensorflow uses all available GPUs and CPUs, although not necessarily in an optimal fashion.
2) I cannot comment on what keras exports, so it's entirely possible that it restricts to run only on some devices. I would take this up with people who know about keras first, or dig into the code that keras generated and ask about that.

From your description, it is very hard to tell what, if anything, is wrong with TensorFlow, so this is probably a better question for the mailing list, or, if you have a more specific question about using multiple devices, for stackoverflow. 

I will close this bug for now. Please reopen this if you find a more specific problem.
  Gradients for `tf.dynamic_partition()` should be supported as of https://github.com/tensorflow/tensorflow/commit/2b672c4a2f6aeaea8457fd4941f48f5a9e80d283 (and certainly in the 0.7 release). Does you custom build pre-date that change?
 I bring good news! You can just paste the implementation from that commit into your program somewhere (with the appropriate changes as below):

``` python
@tf.RegisterGradient("DynamicPartition")
def _DynamicPartitionGrads(op, *grads):
  """Gradients for DynamicPartition."""
  data = op.inputs[0]
  indices = op.inputs[1]
  num_partitions = op.get_attr("num_partitions")

  prefix_shape = tf.shape(indices)
  original_indices = tf.reshape(tf.range(tf.reduce_prod(prefix_shape)), prefix_shape)
  partitioned_indices = tf.dynamic_partition(original_indices, indices, num_partitions)
  reconstructed = tf.dynamic_stitch(partitioned_indices, grads)
  reconstructed = tf.reshape(reconstructed, tf.shape(data))
  return [reconstructed, None]
```

...then the gradient will be available from your code.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLA needs to be signed for us to look at it :(
  Thanks for the report, this was fixed yesterday, so either build from source at current HEAD or wait for our next patch release (soon)
  We generally use the default of clang for OS X compilation, which explains your error. When I run `gcc --version` I get `Apple LLVM version 7.0.2 (clang-700.1.81)`. You should try switching to clang instead of gcc, since that's the supported approach.
 Assuming issue has been addressed. Please reopen if not.
  Version of bazel? Our jenkins instances are totally fine...
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I suspect this was an accident.
  Hi, this is a question better suited to StackOverflow -- github issues are for bugs and feature requests.   Thanks!
  I think it would be nice to get these changes into the patch release too, at least the python3 bug fix one.  Thoughts?
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 @danijar: I assume this is okay since you contributed this already to master.

(This is a weird edge case @willnorris)
 Would it be safe to ignore the label, or should I still wait for consent?
 Ok, great, thanks :)
 @tensorflow-jenkins: test this please

added a whl package change as well.
 @tensorflow-jenkins: test this please
 Merged
  I think for illustration purposes, the current code might be useful to show that these functions do return objects that _could_ be used, even though at the moment the side effects are that they add to the graph. What do you think?
 @danmane: what do you think?  Originally they were given names (which led to unused_var lint errors).   Do you still think it's worth capturing the return values?
 The summary ops are magic in that the canonical summary use depends on the collection. They are ops just like all the others, but their return values are very rarely used (I guess you could depend on them, but typically you're really happy that they don't run, and what are you going to do with the serialized proto anyway except for write it to disk?). I would omit the return types in the example code, I don't think it's all that enlightening.
 Ok, merged
  This was fixed earlier today in: https://github.com/tensorflow/tensorflow/commit/5df9ff89a58a17e6efca7f186b57f22430130768

We'll probably amend our docs to fix this eventually.  Going to leave this open until then.
 This is fixed for docs versions later than 0.7. Do we have a policy on backporting documentation fixes to older versions?
@martinwicke want to chime in?
 FWIW I would be fine just closing this because I don't think many people will find the 0.7 docs.
 @robwell (or anyone else), if you are motivated enough to send a fixing PR against 0.7 we'll happily accept it. But I will close this issue, it's not enough of a problem since it's fixed in 0.8 and master.
  This was fixed in 555e73da8f171992085c68614f74b23b8180292c, which we will hopefully cherry-pick to 0.7.1 soon.
 We have some new candidate wheels that have the fix here: https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/g3doc/get_started/os_setup.md -- let us know if those are good for you.
  can you try pip install protobuf==3.0.0b2 ?
 A similar error at https://github.com/psi4/psi4/issues/94 seems to suggest an issue due to mixed versions of python.  You might want to install from sources rather than using our wheels, at least until we can figure this out more generally.
 Ok try our new 0.7.1 wheel (cleanly uninstall tensorflow 0.7.0 and protobuf, then install 0.7.1).  The pip installation requires cudnn v4 though.  Otherwise you'll need to install from sources.
  Thanks -- I pushed a bad commit there.

Sync past d0a822fbcb04d95a643d8efe65699a8d1cdce98b and it should work
  Duplicate?
 thanks -- we'll cherry-pick it for the r0.7 branch once we mint the binaries (the binaries and the docs are somewhat independent, and we want to keep the number of changes minimal)
  Ahh, new lines.

LGTM
 Merged
  (Try upgrading to 0.7.1 -- there have been many bug fixes since that release).
 Closing this issue, since the upgrade seems to have worked. Let us know if it didn't!
   @tensorflow-jenkins test this please
 @jendap's comment addressed. Squashed.
 Merged
  (You may get more help again from the bazel team on this one ...)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins test this please
 @tensorflow-jenkins test this please
 Merged
 Because we rebase your commits before merging and GitHub apparently is not smart enough to detect that.
  Thanks for the report! I'd accidentally added new-lines to some URLs when I was formatting the markdown to an 80-column width, but I've just pushed a fix internally. This bug should be updated automatically once the change goes in, and then the website should get a push soon after that.
 Actually I rolled back my internal change since it looks like #1174 / #1175 fix this and we don't want merging conflicts. Closing this bug.
  My guess is that more ops than before are now supported on GPU, so there may be more copying of data between CPU and GPU if you are not careful (e.g., you set allow_soft_placement=True, which means surprising placement decisions could be made :).  Try turning on log_device_placement in the ConfigProto to see what might have changed.
 Closing since we'd need more information to debug.  Please comment or open a new issue if more information arises.
  Thanks @HellMood, we'll take further discussion to that bug and we'll try to fix it in the next patch release.
  Duplicate of #23 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @danmane: is this correct? 

https://github.com/tensorflow/tensorflow/commit/880011919c7b113802eb23c54ed5b8e2218e4416 specifically changed this a few days ago.
 Hm, with the reorganization of tensorboard, I think this is right.  Currently  tensorboard install is broken at HEAD, and I think this fixes it.
 (Merged a few days ago)
  Merged
  The reason is that moments produces a slightly different graph when the input shape is known (it can be pre-computed).  When it's unknown, the code introduces a "squeeze" operator that isn't normally there, in the same op scope as the final mean, so there's a name collision and we increment by 1.

@vincentvanhoucke: can we/ should we make moments() return a consistent name for the returned tuples?
 These are the names of the ops, not the variables, so it won't be an issue for save and restore.
We should try to provide stable names for endpoints of composite ops though, that seems sane, even if users should definitely not rely on that naming to be stable over time. I'm working on updates to `moments()` for v0.8, I'll put that on my list of requirements. 
 A fix for this has been checked in and should make its way to github shortly.
  1) Adding test-on-intall for
tensorflow/python/{contrib,examples,models,tensorboard}

2) Adding option for TF_BUILD_IS_PIP=BOTH, which performs both the
"bazel test" step and the "pip.sh" test-on-install step in one single
build

3) Adding "bazel clean" switch to pip.sh

This is the mirror of #1154 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Merged, thanks for the surgery!
  ```
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.63
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module 352.55 Thu Oct 8 15:18:00 PDT 2015
GCC version: gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.55
E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:229] kernel version 352.55 does not match DSO version 352.63 -- cannot find working devices in this configuration
```

My guess is that you have to update your cuda driver to 352.63, is that right @zheng-xq  ?
  @damienmg, any more ideas? :)
 @martinwicke You could do further triage on this?  Taking advantage of @damienmg's new feature seems worth doing if it reduces further bug reports.
 Do I understand correcly that this issue will be resolved once current bazel head lands in a release?
 I have started working on the CUDA autoconf. With this and Python detection, I think the only thing left that the configure script manages is whether to build with GCP support.
 @davidzchen @damienmg that is exciting.
 Here is the tracking bug for cuda autoconf: #2873
  @Mistobaan can you also squash the commits please?
 Thank you!

LGTM, @vrv can you merge this please?
 Merged
  1) Adding test-on-intall for
tensorflow/python/{contrib,examples,models,tensorboard}

2) Adding option for TF_BUILD_IS_PIP=BOTH, which performs both the
"bazel test" step and the "pip.sh" test-on-install step in one single
build

3) Adding "bazel clean" switch to pip.sh

(This is a squashed version of https://github.com/tensorflow/tensorflow/pull/1151, with the same changes)
 @jendap has LGTM'ed this in #1151. Ready to merge.
 Merged
  1) Adding test-on-intall for tensorflow/python/{contrib,examples,models,tensorboard}

2) Adding option for TF_BUILD_IS_PIP=BOTH, which performs both the "bazel test" step and the "pip.sh" test-on-install step in one single build

3) Adding "bazel clean" switch to pip.sh
 @tensorflow-jenkins test this please
 @jendap: I know some tests will faiure (Mac+Python3). Those are unrelated to this change set. Also, the commits are messed up and difficult to rebase. I'll create another PR with only one single commit (with the same changes, of course).
 @tensorflow-jenkins test this please
 I think the linux build was killed by mac py3 failing. I have fixed that.

Can you squash and rebase it please?

Otherwise it looks good. (although I'm getting a bit lost in those scripts, but I may have idea about that)
  Indeed, I have a pending change to add this (that we can iterate on)
 Okay added one here: https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md 
  Martin's unassignment of @jendap was an accident.
  @tensorflow-jenkins: test this please
  When you build the pip package from sources, what do your wheel package names look like?  I have a suspicion that the wheels we provided were too specific, but were named too generally.
 @jjhelmus: I removed the copy of the protobuf from the wheel package in 76989f9839f4824aa9f5c1c1907fba3a02c1c83a, we now just rely on the whl dependency on 3.0.0b2.  I'm hoping this resolves most of the problems -- I'll try to push this out as part of our next 0.7.1 patch release.  Thanks for the help debugging!

In the meantime, my guess is you could probably get around this by pip uninstalling protobuf and then pip install protobuf==3.0.0b2, to remove the version we accidentally clobber and then reinstall the right version.
 Agreed, we'll try to fix this for 0.7.1.

I guess we have to create 3-4 different wheels for each version of python.
 Ok, updated wheels for 0.7.1 available.

We haven't yet built for python 3.5 on linux since it looks like only conda supports that.  @jjhelmus: since you seem to be maintaining the conda packages for tensorflow, do you want to take a stab for now at building from sources for conda?
 @vrv Is this still an issue?
 I mean I suspect it's always going to be an issue due to bazel, and all the information available is here, so we'll close this.
  @terrybroad do you have 64 bit ubuntu or 32? (run `uname -a`)

@HellMood thanks for a workaround. I have to try it myself. We have intentionally used this name so that people with py35 don't have to rename it. It installs fine on 14.04 with py3 but we have not tried 15.10 yet.
 (We will build the proper wheels for 0.7.1)
 Ok, we built a few new wheels for 0.7.1 (see https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/g3doc/get_started/os_setup.md) -- before we announce more widely we'd like to make sure it's working well for the people here who have had issues -- let us know if you still have problems with these new wheels.

We have not yet built a version for all python, just the ones in standard installations.
 We do support python 3.3 and 3.5 in that TensorFlow does work with them when compiled from sources.

However, since I suspect you mean provide pre-built pip wheels for 3.3 and 3.5 -- that is indeed what we would like to do, but we have not gotten around to yet.
 you need python 3.4. v3.5 is not yet supported. read 1 post above what did the trick for me.
 Closing, I think we fixed (some) of these issues in 0.8.0
 @tobegit3hub yes, that does not work. Python 3.5 will be part of net release. In the meanwhile you can try nightly build - http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/
  I'm a bit confused by your request. Can you explain what you mean by "If I set the epoch for output_[0,0]"? What is an epoch in this context, and how does it relate to the data that TensorBoard is recieving from TensorFlow?
  Looks like #1089 was already merged, so we'll need more PRs.
 @smcantab: Many people have requested better double support and it's critical for a lot of numerical applications, so we should do it even it increases binary size.  complex128 is rarer but the same principle applies.
 Here are the ops missing double support.  I believe most of these would be straightforward to add, except for max pooling as described in #547, and we'd be happy to accept PRs.  Also max pooling is doable, it would just take a bit of coordination to make the stream executor changes.

```
AdjustContrastv2
AllCandidateSampler
AudioSummary
CTCBeamSearchDecoder
CTCGreedyDecoder
CTCLoss
ComputeAccidentalHits
DrawBoundingBoxes
EditDistance
ExtractGlimpse
FixedUnigramCandidateSampler
HSVToRGB
ImageSummary
InTopK
LRN
LRNGrad
LearnedUnigramCandidateSampler
LogUniformCandidateSampler
MaxPool
MaxPoolGrad
MaxPoolGradWithArgmax
MaxPoolWithArgmax
NegTrain
RGBToHSV
SampleDistortedBoundingBox
SparseMatMul
StringToNumber
TensorArrayConcat
TensorArrayGrad
TensorArrayPack
TensorArrayRead
TensorArraySize
TensorArraySplit
TensorArrayUnpack
TensorArrayWrite
ThreadUnsafeUnigramCandidateSampler
UniformCandidateSampler
```
 @siddharth-agrawal Here's the script I used, but it would have to be modified a bit to work outside of Google: https://gist.github.com/girving/fbe861e6df1ce9d5add8e57bf32a247d
 BTW, you can also get op_def which has type-attrs for an op like this `tf.add.__globals__["_op_def_lib"]._ops["Add"].op_def`
There is one _op_def_lib per each gen.*ops.py file, so 19 "_op_def_lib" objects. If only we had some easy method like this to find which ops have GPU kernels....
  Nice catch. Looks good to me. Can you merge the latest changes from master into this branch. Branch is out-of-date.
 @dsmilkov: does this need to be cherry-picked into r0.7 too?
 No need to cherry-pick. No visual impact or perf impact by this change.

@wlsc I'm curious how did you notice this bug? Did you see something visually different in the graph, or just by looking at code?
 Merged
  Merged
  This already works since 0.6 -- do you have evidence this is not the case?
 No problem, always good to double check that we didn't mess something up :)
  Yes, conv2d can work with h=1, though it's possible one could write a faster kernel if you knew it was 1d.  If someone was willing to write a kernel (CPU and GPU) and benchmarks for this, we'd be happy to accept it.  Same with convxd.  #150 for a similar request, but for 3D, which we'd also love contributions for.
 @jramapuram Fractional striding is hard to implement efficiently using anything like direct convolution; normally I'd just write it using FFTs (which already works).
  Thanks for reporting this bug! You can safely ignore the warnings, which are related to the new `MetaGraphDef` support. If you want to be able to serialize your collections and avoid this warning, you can call the [`tf.register_proto_function()` function](https://github.com/tensorflow/tensorflow/blob/03bff43060229357cbe2cc1659e7d129c2799b06/tensorflow/python/framework/ops.py#L3447) for your collection.

The `OSError` is a bigger problem, and is caused by an incompatibility between our internal and external `tf.gfile.MakeDirs()` functions. As a temporary workaround, prepend "./" to the `save_path` argument when calling `Saver.save()`. I have a change in the pipeline that will fix this error.
 If someone can validate that the above commit fixes the problem with Saver, that would be appreciated! 
 (I was able to run cifar10_train.py with no problems after the above commit).
  Looks like another instance of #1046.  Going to leave this one open since I suspect a lot of people are going to be running into this one.
 Could you try updating polymer to at least 1.2? That has fixed it for at least some people.
 Hi guys,

If you open the console while your browser is pointed to 0.0.0.0:6006 and type `Polymer.version` do you get `1.1.5` or `1.2.4` or something else?

If you get `1.1.5` when you need to update to `1.2.4`. To do this, [download polymer 1.2.4](https://github.com/Polymer/polymer/archive/v1.2.4.zip). Unzip and copy the 3 html files (polymer*.html) into the external polymer directory under site-packages dir: /usr/lib/python2.7/site-packages/external/polymer (overwrite the previous files).

Let me know how it goes. We will also be releasing a patch for 0.7.0 where we ship Polymer 1.2.4 with it.
 Okay, we're going to cherry-pick the 1.2.4 version change into r0.7 branch and will include this fix in the next patch release.
 Ok, we have candidate wheels for 0.7.1 here: https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/g3doc/get_started/os_setup.md -- let us know if there are any problems.  If all looks good we'll update our docs to officially recommend 0.7.1 late this week.
  @tensorflow-jenkins: test this please
 (It's actually not a typo according to the spec, but it is probably unnecessary.)
 Merged
  Should be fixed. Sorry.
  This is already being merged from 0.7 branch
  Merged, thanks!
  Thanks -- we've fixed this in the source already, we'll push out a new version of the website hopefully today.
  @tensorflow-jenkins: test this please
 Merged.
  Cool! Jenkins, test this please.
 I don't think that's correct.  The failure here is legitimate:

//tensorflow/python:cwise_ops_test                                       FAILED in 2 out of 2 in 243.8s
  Stats over 2 runs: max = 243.8s, min = 133.8s, avg = 188.8s, dev = 55.0s
  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/testlogs/tensorflow/python/cwise_ops_test/test_shard_2_of_2.log
  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/testlogs/tensorflow/python/cwise_ops_test/test_shard_1_of_2.log

Executed 275 out of 317 tests: 316 tests pass and 1 fails locally.
 I click on the 'details' link and go to "Console output" in Jenkins :)
 If you go to the full log, the test failure output is embedded somewhere in the log file
 @tensorflow-jenkins : test this please
 @Mistobaan I think we are missing the GPU files like https://github.com/tensorflow/tensorflow/blob/f2bd0fc399606d14b55f3f7d732d013f32b33dd5/tensorflow/core/kernels/cwise_op_gpu_cos.cu.cc for these new ops.
 Sorry, conflicts again.  Can you rebase/squash one more time?  Sorry about this :( -- we just have such a huge backlog of issues and PRs. 

Please respond to the thread when you've done this, since GitHub doesn't notify me when you've done this.  Thanks!
 Can you reopen?  GitHub for some reason doesn't let me re-open this. asofjdsfsdfaldsfds 
  Also @shlens in case he knows.
 Thanks for pointing out this warning! It's benign (for the training process), but unfortunate to have this in an example model. I have a patch in the pipeline that will fix the root cause for the warning.
  I'm working on some parts of that.
 There is now a `batch_norm` layer:
https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100
 @sguada FYI
  My guess is that you are running source code from HEAD with a pip install from 0.6.0.  Either check out the code at the same branch / tag as the pip install, or compile from sources at HEAD to use code at head.
 Try upgrading to 0.7.1
pip uninstall tensorflow
pip uninstall protobuf
pip install path/to/0.7.1.wheel
  Jenkins, test this please.
 @tensorflow-jenkins test this please
 (the branch has conflicts, can you address the conflict? we can take a look after that)
 @vrv The conflicts need to be resolved first before the tests can pass on Jenkins. The base is a few weeks old.
 My guess is we'll get to this as part of our Windows support.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 The range does not matter, as long as it's O(1.0).
  You can still do a single lookup, and `tf.reduce_sum(embed, [1])` instead of += to aggregate.
 Thanks for the pointers. Let me take a look. Having different behaviors for different optimizers seems very fishy.
 @shaileshahuja @Smerity I can run the code fine at the head of the TensorFlow tree. Are you using a specific version of TensorFlow? The Docker container?
 Ok, thanks. I'll close this now. Let me know if it affects the Docker container that was built for the assignments. I'm pretty sure it does not, but I can rebuild it if it does.
   Odd problem with urllib.

On Wed, Feb 17, 2016 at 8:02 AM Alex Rothberg notifications@github.com
wrote:

> Why change the URLs to http from https?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1114#issuecomment-185271733
> .
 @jendap, what was the problem there?
On Sat, Feb 20, 2016 at 11:49 Alex Rothberg notifications@github.com
wrote:

> Care to elaborate?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1114#issuecomment-186670176
> .
   LGTM
 merged.
  Commit
https://github.com/tensorflow/tensorflow/commit/00b5ea730ddf63e0cb49a9200f3b610cdbbdc13c#diff-6ea208942bad0a8c2194f5540b5dd618

broke the exit code. This changeset fixes it.
  Jenkins, test this please.
 Thank you!
  And, of course, tanh. :)

That's a good idea.
 Also all the hyperbolic trig functions.
 #483 is probably separate since it involves a dtype change, and can't participate in the same pile of macros. 
  If you have installed 0.6.0 from pip, always check out the code repository at the same branch/tag -- they should be compatible.

If you want the latest changes from master, you'll have to build the pip package from sources.
  Thanks!
  I fixed the latter one, I'm not sure why the former is happening.  You shouldn't be running cifar10.py -- it's a library called by the other executables.
 (Note that you have to build from sources to get the fix.  Our first post-0.7 binary release will likely get this fix.)
 Yeah, the binaries are being built right now, we can maybe make a patch-release with these fixes (I expect we're going to want a protobuf fix in the patch release too).
 If you build from sources at HEAD, yes.
  Duplicate of https://github.com/tensorflow/tensorflow/issues/504
  This looks like a bug, a missing function that we need to implement.   Thanks for reporting.
 Yep, I have another fix for that coming.
  Duplicate of https://github.com/tensorflow/tensorflow/issues/582
  What version of bazel are you running?  We'd like to help you test locally, since only admins can trigger tests.
 You might be better off specifically testing "tensorflow/..." instead of "..."
 (I think our Mac build is broken, let us fix it first)
 Okay I pushed a change to fix, you can try again now
 That error is usually due to an older protobuf version installed somewhere on your system.
 Jenkins, test this please.
 @tensorflow-jenkins: test this please
 Merged
  For some reason, pip3 seems to be using python2.7 pip internally (see the
stacktrace). It appears there's something amiss in your pip installation.

On Sun, Feb 14, 2016 at 12:28 PM vladalonso notifications@github.com
wrote:

> INSTALLATION ERROR:
> $ sudo pip3 install --upgrade
> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl
> sudo: pip3: command not found
> 
> CONDITIONS
> I am running iPython using Python 3.4.3 | Anaconda 2.2.0 (64-bit), running
> on Ubuntu/Linux 14.04 LTS (64-bit) OS.
> 
> DESCRIPTION:
> The Tensorflow installation fails using PIP for Python 3, as suggested
> here: (
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md
> ).
> Ubuntu/Linux 64-bit
> 
> $ sudo apt-get install python-pip python-dev
> Ubuntu/Linux 64-bit, CPU only:
> 
> $ sudo pip3 install --upgrade
> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl
> 
> DETAILS:
> tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on
> this platform.
> Exception information:
> Traceback (most recent call last):
> File "/usr/lib/python2.7/dist-packages/pip/basecommand.py", line 122, in
> main
> status = self.run(options, args)
> File "/usr/lib/python2.7/dist-packages/pip/commands/install.py", line 257,
> in run
> InstallRequirement.from_line(name, None))
> File "/usr/lib/python2.7/dist-packages/pip/req.py", line 168, in from_line
> raise UnsupportedWheel("%s is not a supported wheel on this platform." %
> wheel.filename)
> UnsupportedWheel: tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a
> supported wheel on this platform.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1097.
 sudo apt-get install python3-dev and python3-pip ?
 pip3 --version?

On Sun, Feb 14, 2016 at 12:44 PM vladalonso notifications@github.com
wrote:

> $ pip --version
> $ pip 8.0.2 from ... anaconda3/lib/python3.4/site-packages (python 3.4)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1097#issuecomment-183973249
> .
 The sudo command probably messes with your environment. Your backtrace
shows that you are using files from /usr/lib/python2.7/dist-packages/pip.

Since you're using anaconda, you should be able to install without sudo,
can you try that?

Or, run `sudo pip3 --version` to verify my hunch.

On Sun, Feb 14, 2016 at 12:49 PM vladalonso notifications@github.com
wrote:

> $ pip3 --version
> pip 8.0.2 from /home/vladimir/anaconda3/lib/python3.4/site-packages
> (python 3.4)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1097#issuecomment-183973520
> .
 Glad you were able to make progress!
  1) Inculding https://github.com/tensorflow/tensorflow/pull/1072
2) Minor tweaks to ci_parameterzied_build.sh (a missing-space bug fix, env var display and timestamps)
3) Version bump from 0.6.0 to 0.7.0
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 @vrv version.h updated.
 CLAs look good, thanks!

<!-- ok -->
 @vrv's comments are addressed. This PR is squashed and ready to be merged. 
  Here is a sketch you can you extend the current code to implement the gradient for py_func and all changes will be restricted to script_ops.py:
1. 
 oops. sorry that I were distracted. But yes, you are welcome to extend the code to support gradient of a pyfunc. What I have in mind may be slightly different from your sketch. I'm thinking you can change
tf.py_func(lambda, ...) to take an extra argument grad_func=lambda. In the py_func.py's registry, keep lambda and grad_lambda keyed by the unique token. In the PyFuncGrad(op), dispatch to grad_lambda based on op.attr['token'].
 I don't think `py_func` should have any special functionality here.  You can already do this with a combination of `RegisterGradient` and `Graph.gradient_override_map`.  It's ugly, but if we want to make it less ugly we should clean up gradient overriding for all ops, not just `py_func`.
 Agreed, `gradient_override_map` is pretty ugly.  Separate issue from py_func, but so far I don't think we have either an internal or external bug tracking that ugliness. :)
  My high level comment is that, while this is nice, I think the original intent of the tutorials was to keep the examples minimal and self-contained, rather than factored out -- the longer term intent is to convert these into IPython Notebooks that have most of the relevant content in one.

If you feel strongly this should be factored out, can you explain what the goal of your refactoring is?
  I believe this is 'working as intended': tf.set_random_seed only sets the graph level randomization seed, not the op level randomization seed.  You have to set both if you want deterministic results.

(I believe tf.set_random_seed documentation explains the details).
 To clarify, you need to set both if you want deterministic results when the _graphs_ are different.  If the graphs were the same and you set the graph level seed, you wouldn't need to set the op level seed, but here your graphs are different, so you would need to set both.

We could probably add this to our documentation, since this example is a useful one to show.
  @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 merged.
 Thanks!
 I squashed them into one.
  @tensorflow-jenkins: test this please
 I think this is great, thanks for doing this.

We could use some tests to validate this though:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/reduction_ops_test.py#L371 is only testing for float, we should add one for double.

(I'm hoping we'll have our GPU machine to test this by the time you get this done :)  @martinwicke )
 I hope that we have our GPU machine replaced with a cloud thingy by then. :)
 cool, once we have some tests for these, we can merge
 It's difficult because then the test won't run when the client doesn't have a GPU.  This is basically a way to say 'if you have a GPU, test it.'  Since we now run GPU tests on Jenkins, we should get coverage.

@tensorflow-jenkins: test this please
 Don't worry about updating the branch as long as there's no conflict :)

@tensorflow-jenkins: test this please
 well, I guess that's not technically safe, but in this case it would have likely been fine.
 Merged
  De-duping with https://github.com/tensorflow/tensorflow/issues/526 -- I believe this happens when there is another process running that is also setting the cuda context, but @zheng-xq and @leary-google will know more (let's follow up on that #526).
  Checked in the change to our docs for version 0.6.0 (it was fixed in master).  We'll push the website this upcoming week hopefully as part of our next release.  Thanks for the report.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  https://groups.google.com/forum/#!topic/theano-users/-ZT-QObECXQ may be relevant -- probably has something to do with the cuda driver, not TensorFlow specifically.  If you find that TensorFlow is doing something specifically to trigger this problem, please feel free to leave a comment and we'll reopen.
   @tensorflow-jenkins: test this please
  This is a quick fix to have TensorBoard correctly load external assets, after we have renamed the directories for compatibility with new Bazel.
 @tensorflow-jenkins: test this please
  @danmane fixed it on our release branch here: https://github.com/tensorflow/tensorflow/commit/68e8b0f1e02f1e0e10f4fdd689ede80f973e2756

I'm upstreaming the change to master soon, but in case you want to get up and running, that commit should help.
 Please file a new issue for this separate problem to avoid confusion. Thanks.
   @tensorflow-jenkins test this please
 Fixed python import order. Rebased and squashed. Should I double-submit to master? 
 I'm thinking we'll likely merge r0.7 into master once we've release, so no
harm done either way.

On Fri, Feb 12, 2016 at 6:35 PM caisq notifications@github.com wrote:

> Fixed python import order. Rebased and squashed. Should I double-submit to
> master?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1075#issuecomment-183567765
> .
 @martinwicke OK. Please merge it to r0.7 so we can get this release issue out of the way. 
 @tensorflow-jenkins: test this please
 Merged
  From searching the web, this might be a bug in jpeg-9a for some environments.

What if you change our workspace file to fetch the v9b version instead?

http://www.ijg.org/files/jpegsrc.v9b.tar.gz
 Any updates, or should we close this?
  That's just an info log (which we have removed in 0.6.0 because it was spammy) -- there isn't an error here.
  Hopefully someone else knows more about the literature, but is "weight noise" a generalized stochastic operation applied specifically to weights?  For example, is dropout a form of "weight noise" if applied to weights?

You can implement dropout as a composition of existing ops here: https://github.com/tensorflow/tensorflow/blob/8048088bfb82846078f8023bc6199e6424926624/tensorflow/python/ops/nn_ops.py#L581, so if you can express the specific noise algorithm you want as a composition of existing ops, then you can do that today.  If there's something more specific you are asking for, let us know, and feel free to contribute it to the library.
 OK great, I've renamed the title to reflect the more general feature request.

I can think of various ways to do this, but none of them are the same as theano.clone. They involve duplicating the entire graph as mentioned in the SO post, or decomposing your graph into "functions" (a currently experimental and evolving feature) and using tf.cond to select which branch to run at runtime, etc, or using sess.partial_run or tf.py_func (also both evolving and experimental) to embed the logic in the user program rather than in the graph.

Thanks for the feature request!
  This is exciting and if it all works I would love that change.
On Thu, Feb 11, 2016 at 19:34 David Z. Chen notifications@github.com
wrote:

> Currently, TensorFlow vendors its protobuf dependency using a Git
> submodule due to the hack
> https://github.com/google/protobuf/blob/master/BUILD#L487 that is
> required for protobuf Python support.
> 
> I have made a change (bazelbuild/bazel#702
> https://github.com/bazelbuild/bazel/issues/702) to add an imports
> attribute to Bazel's Python rules, which allows us to add directories to
> the PYTHONPATH and thus allows us to remove the hack in protobuf.
> 
> I have a patch to remove the hack for protobuf Python support (see
> google/protobuf#1230 https://github.com/google/protobuf/issues/1230). I
> am testing this change using a corresponding change to TensorFlow which
> removes the git submodule and includes the protobuf dependency using a
> Bazel external repository instead. Once my patch for google/protobuf#1230
> https://github.com/google/protobuf/issues/1230 has been submitted, I
> will send out my corresponding patch to TensorFlow for review.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1069.
  Upgraded to the latest version of Eigen that introduces a way to conjugate a tensor as well as improved LU decompositions.
 Jenkins, test this please.
 this looks like the same commit now
 I upgraded to an even newer version.  Thanks  :)
  Hm, it looks like we recently updated our google/protobuf commit hash to what's currently at HEAD, I wonder if that's somehow to blame, though it's weird because our Jenkins builds are fine.
 Did you clone with --recurse-submodules?
 Looked around a bit and saw http://stackoverflow.com/questions/6622454/cuda-incompatible-with-my-gcc-version -- what version of nvcc do you have installed on your machine?  It might be incompatible with your system-installed gcc.
 Closing due to lack of response.  I'm happy to reopen if more details are available.
  We'll try to find someone at nvidia to ask what the best practice is here -- this might work but I don't think it's something we'd want to check in unless it's the recommended practice by nvidia.  We'll leave this open so others can find it in the meantime.
 Is this still an issue?
 I'm worried that either or both of `_FORCE_INLINE` and `_MWAITXINTRIN_H_INCLUDED` could cause strange regressions on other platforms.  What do they do?
  Using Python 2, Linux. This failure doesn't occur under the CPU config
`bazel test -c opt --config=cuda //tensorflow/core:ops_array_grad_test`

Error log:

> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 lo
> cally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 loc
> ally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally
> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally
> I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcurand.so.7.0. LD_LIBRARY_PATH: 
> I tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.
> Running main() from test_main.cc
> [==========] Running 10 tests from 1 test case.
> [----------] Global test environment set-up.
> [----------] 10 tests from ArrayGradTest
> [ RUN      ] ArrayGradTest.PackGrad
> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
> name: Quadro K620
> major: 5 minor: 0 memoryClockRate (GHz) 1.124
> pciBusID 0000:02:00.0
> Total memory: 2.00GiB
> Free memory: 1.57GiB
> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
> I tensorflow/core/common_runtime/gpu/gpu_device.cc:713] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0)
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 1.37GiB bytes.
> I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0xb011c0000 extends to 0xb58f68000
> [       OK ] ArrayGradTest.PackGrad (65832 ms)
> [ RUN      ] ArrayGradTest.UnpackGrad
> I tensorflow/core/common_runtime/gpu/gpu_device.cc:713] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0)
> [       OK ] ArrayGradTest.UnpackGrad (4 ms)
> [ RUN      ] ArrayGradTest.ConcatGrad
> I tensorflow/core/common_runtime/gpu/gpu_device.cc:713] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0)
> external/bazel_tools/tools/test/test-setup.sh: line 51: 112821 Segmentation fault      (core dumped) "$@"
 Known -- test is currently not expected to run on GPU.  Will keep it open until we fix.
 @vrv, @zffchen78: Is this still an issue? 
 Should have been closed.
  Try the instructions mentioned here: https://github.com/tensorflow/tensorflow/issues/349
 Can you file a bug with the bazel team to see if you can get more help?
  Our python3 binaries should work with python3.5 (although currently the wheels require renaming), and adding **matmul** seems innocuous enough. However nobody is working on that now, it's likely a feature we'll accept if someone writes it.
 I think we'd want to do this at the same time as supporting broadcasting for matmul in https://github.com/tensorflow/tensorflow/issues/216, otherwise the semantics might be confusing.
 Yes, we need to merge `matmul` and `batch_matmul` before doing this, and someone decided to give them different sets of attributes (I can't imagine why). 
 @ibab: I mean unifying the two ops.  The Python wrapper is problematic since we don't necessarily know which to use at graph construction time, so `tf.cond` would be necessary.  The difference in attributes is pretty frustrating; @rmlarsen: did we have a nice conclusion as to which attributes we want?
 @ibab I believe all of those semantics are backwards compatible with what we currently have, but @rmlarsen can correct me if I'm wrong.
  I suspect it's historical -- in the beginning few ops had working float64
implementations. Someone should try adding dtypes.float64 to that line and
seeing if it works.

On Sat, May 14, 2016 at 6:26 AM, Siddharth Agrawal <notifications@github.com

> wrote:
> 
> I looked into this issue, and as far as I can tell, the issue comes from this
> line
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L379
> in the class Optimizer() implementation, and not the clip_by_value op.
> Also none of the optimizers override the _valid_dtypes() method. I don't
> have background information as to why the dtypes did not include float64.
> Can one of the admins comment on this?
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1061#issuecomment-219220319
 Nice, good to know the fix is so simple. A PR seems like a generally good thing to do, although I don't know how overwhelmed the people in charge of merging are right now, I'll let someone else comment
  It looks like the 940M is compute capability 5.0 -- we default to only building for 3.5 and 5.2.

You might want to try building from sources.  See the section [here](https://www.tensorflow.org/versions/v0.6.0/get_started/os_setup.html#installation-for-linux) on installing for Cuda 3.0 to see how you can compile for Cuda 5.0.

@zheng-xq: Is https://github.com/tensorflow/tensorflow/blob/8048088bfb82846078f8023bc6199e6424926624/tensorflow/core/common_runtime/gpu/gpu_device.cc#L683 properly capturing this?  We probably want to just iterate over all compute capabilities and check for equality, unless I'm misunderstanding how compute capabilities work :)
 Every compute capability you compile for increases the size of the binary significantly, so it is unlikely that we'll support every compute capability Nvidia has ever produced in the default binaries.  But everything >= 3.0 should work if build from sources, although we only test for a small subset.
 Well, we'll think about it, maybe some people don't care about binary size and want something that works everywhere.  Once we get our release process a bit more solid we could consider this.
 @vrv: What's the status of this?
  @tensorflow-jenkins test this please
 INFO: From Compiling tensorflow/core/framework/op_segment_test.cc:
tensorflow/core/framework/op_segment_test.cc:35:14: error: cannot declare field 'tensorflow::OpSegmentTest::device_' to be of abstract type 'tensorflow::DeviceBase'
   DeviceBase device_;
              ^
In file included from ./tensorflow/core/framework/op_kernel.h:25:0,
                 from ./tensorflow/core/framework/op_segment.h:22,
                 from tensorflow/core/framework/op_segment_test.cc:16:
./tensorflow/core/framework/device_base.h:90:7: note:   because the following virtual functions are pure within 'tensorflow::DeviceBase':
 class DeviceBase {
       ^
./tensorflow/core/framework/device_base.h:178:35: note:     virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const
   virtual const DeviceAttributes& attributes() const = 0;

Interesting -- does this not happen when you compile in visual studio?
  Merged, thanks!
  I think a general issue here is that very often new versions of the papers are uploaded to arxiv, and hardcoding a link to a specific version will often go stale (at least, that's what appears to be the case from the URLs).  Does arxiv have a 'latest' pdf link so that we don't have to hardcode a specific version?
 In other words, some of your changes are great -- do you think we should just always link to the arxiv front page for a paper instead of pdfs, consistently?
 I too prefer the arxiv page over pdf.  Want to make that change to this PR?  If you want to have the pdf link too, include the version-less link and we're good :)
 Merged, thanks!
  It could be any number of things -- different amount/type of memory, onboard graphics card competing, different BLAS libraries... We'd need a lot more information to diagnose this. If those were the same computer with the same OS installed, it may be mysterious.
 Closed for now due to lack of activity.  Feel free to reopen if more information is available.
  This version provides better support for embedded platforms and optimized implementations of the tanh function amongst many enhancements
 Jenkins, test this please.
 Jenkins, test this please.

(@benoitsteiner, did you rebase before the tests were done? It looks like Jenkins stopped the running tests or something)
 @martinwicke The branch was out of date so I rebased it.
   Jenkins, test this please.
 Merged. Does this have to be synced with a CL internally?
  Need more info:
- What git commit did you try to build at
- What version of bazel are you running
- Did you run ./configure before building (what other steps did you run) ?
 1) We need to figure out why the bazel genrules aren't producing the right outputs.  I wonder if the ./configure script is not modifying the CUDA_VERSION string in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L7 ?  

2) @kbrems: where does the ./configure script say 7.0 is the default?
 @kbrems: that says 'e.g.,', which means 'for example', not the default.
 I disagree that 'e.g.' means 'default is', but we can certainly change that to be 7.5 :)

I can also reproduce the problem above, it's a dumb error, will fix.
 Fixed in 97f6b6fb66cd8e52af4750bd183dfc555d08ef4d
  1) Perform pip install twice, first time with --upgrade and second time without, to fix sporadic test failures related to protobuf version
2) Correctly determine the local pip install directory based on Python version being used
3) Always perform "bazel test" on non-Docker build environments, such as Mac
 I added some further changes to fix a bug related to ci_build.sh arguments containing double ampersand (&&). 
 Let me know if there are any further comments. If there is none, please merge it to master and r0.7 branches for the release. 
 I removed "bazel clean" as a compulsory step for non-Docker builds and added "--genrule_strategy=standalone" for such builds.  In addition, I added TF_BUILD_BAZEL_CLEAN as an optional environment variable. 
 Rebased and squashed
 merged.
 And cherry-picked into r0.7
  Locally, you should run `bazel test ...` to run all the tests, and if you have something affecting GPU, you should additionally run `bazel test -c opt --config=cuda ...`. Or, you can make us do it. Like this.

Jenkins, test this please.
 Thanks! Merged.
  Docs only, not testing, merging. Thanks! I am very impressed. You must have a spell checker in your IDE.
  Yes, don't build TF with bazel 0.1.4+ before d21c2d6653a3d9bc3376bcb190ba0ac31f52195b :)
 (I can't find the commit right now, but there was a commit a week or so ago that essentially broke backwards compat.)
 This one: d6f732205870e87b495e1c01d7b79c6512339021
  @mebersole Can you clarify "built from TOT"? Did you built it using `bazel` or using `gulp`? The right way is to run:

```
bazel build tensorflow/tensorboard
bazel-bin/tensorflow/tensorboard/tensorboard --logdir path/to/log/dir
```

If you do this, these is an unrelated issue #1076 , which makes whole tensorboard not to work, but applying patch https://github.com/tensorflow/tensorflow/commit/68e8b0f1e02f1e0e10f4fdd689ede80f973e2756 should make it work. I did this today and tensorboard is working.

In case you are building your own front-end (typescript, html), in which case you are running `bower install`, make sure you choose polymer 1.2.4 if bower asks you for which version of polymer. We have a fix that we will push soon, that will stop bower asking you which version of polymer (should be 1.2.4 by default). I actually tried running `bower install` from scratch, and purposely chose polymer 1.1.5 which resulted in the same error (Uncaught TypeError: Polymer.dom(...).unobserveNodes is not a function) that you have.

Hope this helps!
 Hi guys,

If you open the console while your browser is pointed to 0.0.0.0:6006 and type Polymer.version do you get 1.1.5 or 1.2.4 or something else?

If you get 1.1.5 when you need to update to 1.2.4. To do this, download polymer 1.2.4. Unzip and copy the 3 html files (polymer*.html) into the external polymer directory under site-packages dir: /usr/lib/python2.7/site-packages/external/polymer (overwrite the previous files).

Let me know how it goes. We will also be releasing a patch for 0.7.0 where we ship Polymer 1.2.4 with it.
 Closed in #1134 
  This doesn't seem to be a bug in TensorFlow (are you running initialize_all_variables?), this kind of question is better for StackOverflow
  iteritems() --> items() please
 @tensorflow-jenkins test this please
  This would a better question to ask on StackOverflow, github is for bugs / feature requests.
  merged.
  @damienmg: any ideas here?  Did something change about visibility targets?
 If you are building from HEAD, you need to look at the 'master' version of the docs, not 0.6.0.

Which says:

"Follow instructions here to install the dependencies for bazel. Then download the latest stable bazel version using the installer for your system and run the installer as mentioned there"

And currently bazel stable is 0.1.5.

(Bazel is evolving very quickly, along with us, so there's some unavoidable churn at the moment).
 What version of protobuf do you have? Did you accidentally move the
submodule ref in the tensorflow repo to something too new or too old?

On Wed, Feb 10, 2016 at 11:56 PM mondatu notifications@github.com wrote:

> Does anyone have this problem and manage to fix it? I used both
> bazel-0.1.5-installer-linux-x86_64.sh and
> bazel-0.1.4-installer-linux-x86_64.sh with no luck. The version of
> Tensorflow I tried is c420709
> https://github.com/tensorflow/tensorflow/commit/c4207090d68151fb967672a90ea6d574e62cfba0
> (the latest master version at the moment)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1041#issuecomment-182755053
> .
 I think the submodule pointer to protobuf may have been confused. Closing.
  @petewarden is hitting the same thing.  Looking at it now.
 Diagnosed: we deprecated the C++ `TopK` op in favor of `TopKV2` but that didn't affect the C++ graph builder API.  Fix for label_image coming up, and I'll file a more detailed bug to add a deprecation feature when registering ops.
 Fix in review.
 @vrv: Will there be a push to github soon?  Looks like the last one was Tuesday.
 Probably some time today

On Thu, Feb 11, 2016 at 8:31 AM, Geoffrey Irving notifications@github.com
wrote:

> @vrv https://github.com/vrv: Will there be a push to github soon? Looks
> like the last one was Tuesday.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/1040#issuecomment-182944819
> .
 @ziiw: master should work for you now, let me know if not.
  Merged, thanks!
  If you installed the 0.6.0 pip, make sure to also clone the repo at the 0.6.0 tag -- if you fetch from master, the pip binary may not currently work with master, since we're still in a rapid development phase.
 It's as VIjay says, it works ok on head.
  Assigned to @lukaszkaiser because his name is so close to yours.
 @ebrevdo @ludimagister too
 Sorry for the late reply. I think it is exactly as you said: the original reason was probably that there are 2 states, so it's not immediately clear whether to return them separately or concatenated or how, and so it was just not done until needed -- but it's clear we need this for long unrolls.

Eugene worked a lot recently on dynamic RNNs and it should probably also come to bidirectional RNNs. So I'm re-assinging to Eugene to make sure it's in line with the recent work. But if you have a small PR that just adds states on output from the forward and backward lines, then send it in, of course!
 re: long sequences, what lukasz said.  dynamic RNN is coming.

re: outputting the final state.  PRs that return (outputs, final_state_fw, final_state_bw) are welcome.  please add unit tests.

since bidirectional_rnn is not yet part of the official API (not documented) we can still break the function signature.
 Yes - I think it suffices if you modify the existing BidirectionalRNNTest there to test for the returned state. Thanks!
 @ebrevdo: Do we have dynamic RNNs yet? 
 Do you need the final state?  The forward or backward?  How would you combine them?
 And yes, tf.nn.dynamic_rnn.
 See the PR coming in now that has a dynamic_bidirectional_rnn.  That will return the split final states.  We can't modify the output signature of bidirectional_rnn now for backwards compatibility reasons.
 Marking "works as intended" for now, since the dynamic birnn should solve your problem.
  `saver_test` is still broken.
 This PR contains a lot edits in the same spots as https://github.com/tensorflow/tensorflow/pull/1034 We can look and decide which PR to merge. 
 It's much better to open the file correctly as binary or not than to decode manually, so this PR seems better.
 @girving I agree that this PR is better with one exception: In scatter_ops_test.py, use 
`from six.moves import xrange`

so that we can get both 2-3 compatibility and the performance of xrange. 
 I'm closing https://github.com/tensorflow/tensorflow/pull/1034 in favor of this one. 
 The performance difference is negligible for small loops and this is just a test, so I went for simplicity.
 Jenkins, test this please.
 merged.
  This is in response to @jendap's earlier comments. Using the $() subscript syntax.

Also, making --upgrade default option for pip

This is addressing the issue that if --upgrade is not used, preexisting installation of TensorFlow PIP on the system will make the install-test results obsolete (e.g., on Mac, where we don't use Docker to run the tests)
 merged.
  @shlens: Do you know the correct way to fix this?
 resize_images is light wrapper around several resizing kernels, including resize_bilinear, resize_bicubic, etc. The light wrapper functionality is intended to "assist" with the shape information and 3d and 4d images seemlessly. If you do not need all of these "assists" for shape information and 3d/4d images, could you call the underlying functions directly, e.g. resize_bilinear?
 I renamed the issue.  Ideally `resize_image` wouldn't require shapes to be known statically; certainly there's nothing fundamental about the underlying ops that requires it.
 @fayeshine: Thank you!  Let me know if you have questions. 
 Aside: When you paste code format it by indenting four spaces; otherwise the code's indentation will go away.

In the code you've shown the sizes are statically known since `images` comes from `convert_to_tensor`.  To exhibit the bug you'll have to make a `tf.placeholder` with no shape so that shape inference doesn't kick in.
 Please reformat the code as I mentioned so that it's easier to read.
 @fayeshine: So close!  It looks like `resize_images` goes out of its way to break only if the _rank_ of the input tensor is unknown.  Try using `shape=None` in your placeholder instead of `shape=[None, None, None, None]` and you should see a problem.  However, I'm not sure if this is the same problem that  @mackcmillion was experiencing.
 @fayeshine: Yep, the cleanest way is probably to make all the C++ resize ops accept any number of dimensions >= 3 and treat the initial dims-3 dimensions as batch.  There are a lot of other ops that behave this way: mixing together the last few dimensions and broadcasting over the first few. 
 As noted in #2286, `resize_image_with_crop_or_pad` has the same problem.  It should be fixed in the same way.
  Yup, we set --spawn_strategy=standalone by default now, so it's not necessary.  Glad you could fix it!
  Jenkins, test this please.
  LGTM. I was about to make the same change to fix a bunch of test failures under Python3.
 Jenkins, test this please.
 Test failure is not this PRs fault.
 merged.
   Is this file already used in our tests, or just in the experimental ones?
 @martinwicke It is used in only some of the experimental builds right now. 
 Then I'll save ourselves the testing. Merged.
  Looks good to me. :+1: Needs rebase though.
 @tensorflow-jenkins, test this please
 Tool failure.
On Tue, Feb 9, 2016 at 23:26 Panmari notifications@github.com wrote:

> @vrv https://github.com/vrv I don't think it's my PR that broke Linux
> GPU PIP... Or am I missing something?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/1020#issuecomment-182236312
> .
 @panmari @martinwicke I'm working on fixing that sporadic pip install-test failure. I'll submit a PR for that today. 
 merged. Thanks!
  Hi, does it work if you remove the mfpu flag entirely? It significantly optimizes things on ARM, but since x86 doesn't support NEON I'm not surprised it breaks there.
 Ok, thanks for the info. I'll look into selecting build flags dynamically by platform for x86 and x86_64, but in the meantime it's fine to just remove the flags locally.

Tensorflow doesn't support MIPs currently. It looks like you're running into this: http://stackoverflow.com/questions/23065501/stdatomicunsiged-long-long-undefined-reference-to-atomic-fetch-add-8, where an 8-byte atomic add is simply not in the instruction set.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Great, I'll squash an merge.
 Merged. Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Documentation should be fixed in the next sync
  @ebrevdo: Do you have any ideas here?
 Just saw this!  Without seeing the code that you wrote it's hard to see what the problem is.  Perhaps when you call .run or .eval you forget to pass the feed_dict to it?  Or perhaps when you call initialize_all_variables, sometimes that needs a feed_dict as well.  Are you sure the values you're feeding it are the correct numpy dtype?
 Closing for now due to lack of response.  Please reopen if it's still an issue!
  Answered this question on StackOverflow: http://stackoverflow.com/questions/35277339/cannot-import-tensorflow-in-python/35277461#35277461

The problem is that you are trying to install the Mac version of TensorFlow on Windows, and Windows is not supported. Follow #17 for progress in that regard. 
  In ci_parameterized_build.sh, allow the option of parallel bazel build
followed by serial (--jobs=1) bazel test.

Potentially useful for limiting the build time for GPU builds, where the
tests cannot run in parallel due to contention of GPU memory.
 @jendap I addressed your suggestion of keeping tensorflow/tools/ci_build/builds/configured untouched. The commits are squashed.
 merged
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Merged. Thanks!
  Fixing more tests in Python3

```
This CL fixes a number of (but not all) remaining Python3 test errors,
including:
//tensorflow/python:cwise_ops_test
//tensorflow/python:gradients_test
//tensorflow/python:topk_op_test
//tensorflow/python:parsing_ops_test
//tensorflow/python:learn_test

It also fixes a subset of the failing tests in:
//tensorflow/python:function_test
```
 Jenkins, test this please.
  Jenkins, test this please.
 @caisq, this looks like a test failure (this is an error you can apparently fix by reinstalling protobuf, according to #487). Is there an old protobuf version in the container?
 Merged.
 @jendap @martinwicke @vrv Regarding the test failure, I've seen this a number of times in the past couple of weeks. It looks sporadic. Oftentimes it goes away if you run exactly the same build again. The error message suggest that it has to do with protobuf version (see below). We should figure out the cause and solution to it because it'll most likely show up again and slow us down during release builds. 

> Traceback (most recent call last):
>   File "/tensorflow/pip_test/tests/in_topk_op_test.py", line 21, in <module>
>     import tensorflow.python.platform
>   File "/root/.local/lib/python2.7/site-packages/tensorflow/**init**.py", line 23, in <module>
>     from tensorflow.python import *
>   File "/root/.local/lib/python2.7/site-packages/tensorflow/python/**init**.py", line 37, in <module>
>     from tensorflow.core.framework.graph_pb2 import *
>   File "/root/.local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py", line 10, in <module>
>     from google.protobuf import descriptor_pb2
>   File "/root/.local/lib/python2.7/site-packages/google/protobuf/descriptor_pb2.py", line 1533, in <module>
>     **module** = 'google.protobuf.descriptor_pb2'
>   File "/root/.local/lib/python2.7/site-packages/google/protobuf/reflection.py", line 123, in **new**
>     new_class = superclass.**new**(cls, name, bases, dictionary)
> TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases
  Jenkins, test this please.
 merged.
  Yes, please point the logo to https://www.tensorflow.org/images/tf_logo.png or https://www.tensorflow.org/images/tf_logo_transp.png
 Pretty though. Can the build status images be centered? Or is that too much to ask from Markdown?
 I'd have to see the md to see which one is better. Some HTML annotations in MD are ok, but not all. Can I see how you did Version 2?
 Did you as a centering div to the build status icons as well to make the image you sent? Actually, I would center the whole table (maybe just put it into the same div as the logo), and also center the build icon as you (I think) did when you made image version 2. 

Then it's good, and I'll merge it.
 Have you tried putting the align="center", or even a style attr on the td itself? It's quite possible that github's CSS sets the table to left-aligned. If that doesn't work, don't worry too much, I like the image and the centered table.
 merged, thank you!
  Thanks!
  Confirmed. Thanks!
 Fix should propagate shortly.
  The script takes a few environment variables as input and invoke
ci_build.sh in the same directory in the proper way. It takes
information about the build type (cpu | gpu | android), Python version,
whether "-c opt" is used and whether PIP install-test is to be
performed. It automatically examines the parameters to determine the
correct build script to call and the argument flags to use with it. It
also examines whether Docker is available on the system to determine
whether the build will be done inside Docker or not. If GPU Docker build
is detectd, it'll map the CUDA devices and libraries automatically.
In addition, it looks at some optional environment variables to set
additional command flags for Docker and the build command.

See experimental Jenkins matrix builds at: 
For CPU (Linux and Mac): http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-cpu-test/8/
For GPU (Linux): http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-gpu-test/
For Android: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-android-test/
 I like this. @jendap?
 I addressed comments from @jendap. In addition, I realized that there was an issue with BUILD_TAG values from matrix builds. Those tags have special characters such as "," and "=", which make the Docker image names illegal. I modified ci_build.sh a little to solve that problem. 
 @martinwicke Please hold on merging this one. There are some issues related to Docker runs for parameterized builds. I'm looking into it. I'll let you know when it's okay to merge. 
 Ok
On Thu, Feb 4, 2016 at 20:29 caisq notifications@github.com wrote:

> @martinwicke https://github.com/martinwicke Please hold on merging this
> one. There are some issues related to Docker runs for parameterized builds.
> I'm looking into it. I'll let you know when it's okay to merge.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/998#issuecomment-180188710
> .
 @martinwicke @jendap Feel free to merge now. The issue on Jenkins matrix build is now resolved. The change set is now squashed and ready. 
 I merged this, apparently five hours ago.
  Thanks!  Minor comments.
 Jenkins, test this please.
 Can you rebase on master & squash commits?
 @ebrevdo: there's only one commit, and the branch has no conflicts with master.  We can probably just accept as is.
 Don't worry, our script can do that.
 merged.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I'm adding a section to the website in #1232, and pointing to it from the README. There's a number of them I still have to add, and I'd like to keep the README for resources contained inside the repo.
  We are in the process of publishing the trainable version of this, but we need some time to untangle it from our infrastructure.
  Does this help? https://github.com/tensorflow/tensorflow/issues/808#issuecomment-178854998
   Jenkins, test this please.
 your set of commits seems to have a protobuf change in it -- you might want to remove that since it looks like it's causing build failures?
 @vrv: I don't see a protobuf change in the diffs.In fact the files that I changed don't have any protobufs.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please.
 @benoitsteiner can you resolve the conflict and rebase?  Looks good otherwise.
 When you resolve the conflict, please omit the "external/eigen_archive/" part of the paths, a change to bazel makes this the preferred way to referencing those files, so they can be used from targets outside tensorflow.
 I have resolved the merge conflicts and rebased. 
 @peterbraden, are you asking about the include paths? What are you trying to do?

These are changes made necessary in order to allow tensorflow to be included in another bazel project (for example, as a submodule). 
 If you built tensorflow with bazel (just to get all the downloads), the
downloads will be symlinked in `tensorflow/bazel-tensorflow/external...`,
and the built libraries will be symlinked in
`tensorflow/bazel-out/external/...`.

If you need to fiddle with what bazel links in, you should probably ask
over at bazel, they're more likely to know the internals.

There's an effort in PR728 to make a cmake build, which I'd love to accept
to contrib. That may also solve your problem, at least partially.

On Wed, Feb 10, 2016 at 12:10 AM Peter Braden notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke I'm trying to reference
> tensorflow from a non-bazel project (I'm trying to create nodejs bindings
> to tensorflow). Because of the references to the bazel-downloaded files, I
> can't successfully link.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/991#issuecomment-182247599
> .
  See my comments at: https://github.com/tensorflow/tensorflow/issues/808#issuecomment-178854998
The comment is for GPU + Docker + TensorFlow. But the first three steps should apply to GPU + TensorFlow as well.
 @kibtes Did you try "sudo ./deviceQuery"? 
 @kibtes  As the error message says "-> CUDA driver version is insufficient for CUDA runtime version", you need to check the version of your NVIDIA driver.

sudo apt --installed list | grep nvidia

Most likely you'll need to uninstall the current one, and install a later version. I ran int a similar situation before. I uninstalled nvidia-340 and installed nvidia-352. 
 Closing since it looks like it's a system environment issue.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 You have to sign the CLA (see the instructions above to make sure our bot knows about it) before we can look at this.  Closing for now but we'll reopen once the CLA is sorted out.
  CLA didn't make it through somehow?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 can you rebase / squash the commits? it seems to have become conflated with another commit
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I think all these commits are in HEAD -- are you sure this is still needed?
 See #981 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Sounds like a good idea. Can you:
- replace "lets" with "let's"
- move the call to np.random.seed(133) to the top of the file, so that the seed is fixed for everything
- add a:
  valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) where the other calls are

I think that would take care of all randomization and reproducibility issues.
 We don't appear to be re-randomizing the validation set here:
train_dataset, train_labels = randomize(train_dataset, train_labels)
test_dataset, test_labels = randomize(test_dataset, test_labels)
 Merged. Thanks!
   @tensorflow-jenkins: test this please
 Merged
  LGTM, please rebase :+1: 
 Thank you too! :) 
 Actually, I've taken it upon myself to re-write this script today so it will work a bit more cleanly, and serialize some new endpoints we're adding to the backend. Since this is such a small change, do you mind if I just add this directly into the new version of the script? Lower friction than going through the pull request. 
 Thanks @dongjoon-hyun. Closing.
 I'm sorry, I got all excited and went on a issue/pr closing spree. It should get here soon.
  This is addressing issue https://github.com/tensorflow/tensorflow/issues/736

The print-out JSON object line can be used by Jenkins Description Setter to extract and display information about the build, including source version, platform and build tools. See example at: 

http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/29/
 Can you add the bazel --local_resources setting? I'm not sure how to get it out of bazel, but since we set it, we should know anyway. Actually I'm not sure whether we do set it. Only add it if we set it.
 Added container type and command, which covers @martinwicke's comment about local_resources.
 Jenkins, test this please. 
 Oops. Test this please.
 Can you squash?
 Squashed and merged.
  This seems to have fallen through the cracks.  @a-dai: Did you get a chance to glance at this?  @bugulnoz: Is it still an issue?
  If you then want to pass this data through, e.g., a FIFOQueue, so you can then perform minibatch processing, you will have different shaped tensors going in and as a result will not be able to dequeue_many.

Instead, you want to serialize these 1D sparse tensors via sparse_ops.serialize_sparse, and feed the resulting string into the FIFOQueue.  Then after a dequeue_many, you'll use sparse_ops.deserialize_many_sparse to get back a 2D SparseTensor with the dimensions [batch index, variable depth].
 In other words, works as intended.
 Aside from the bad naming, tf.FixedLenSequenceFeature is exactly what you want.  The "FixedLen" in your case is 13, but the first dimension of SequenceFeatures is considered unknown.  Store your data in SequenceExample proto (which is meant for data like speech), use parse_single_sequence_example, and pull out FixedLenSequenceFeature(float32, FEA_DIM),  and FixedLenSequenceFeature(int64, 1).  This will immediately return to you matrices of the shape [-1, FEA_DIM] and [-1, 1].

Alternatively you can just use tf.reshape(x.values, [-1, FEA_DIM]) and tf.reshape(y.values, [-1, 1])  since you can access the values vectors directly from a SparseTensor via property values.  But this won't scale if you start having more complicated features (e.g. 2D per time step, variable-length per time step).

The former solution
 ... the former solution will allow you to declare the more complex input formats and is meant for speech and other sequential data of unknown # of frames.
  This seems a more appropriate issue for tensorflow-discuss...

The most likely cause of this is that you are maybe calling session.run()  on an op and not fetching any tensor results?

This has an interesting behavior where all the computation is enqueued on the GPU device, but if you don't ask to copy any results back from the GPU the call to run() can return immediately. 

The second time you call session.run(), enqueueing the GPU ops ends up waiting for the previous work to finish (for some reason).  The steady-state throughput is limited to 7.7/second, but the first measurement is misleading.

You can verify this hypothesis by either fetching back a small result tensor at the end of each step, or by adding a check_numerics op at the end of the step (which is always executed synchronously).
 See https://github.com/tensorflow/tensorflow/issues/838 for some debugging techniques. If GPU is found, it'll place everything it can on GPU. However some operations don't have GPU implementation and this greedy placement technique can be suboptimal due to data transfers (making GPU run 20 times slower than manual placement in the issue I linked)
  Thanks for pointing it out. I've actually fixed it upstream and the fix should propagate soon. (The tree wasn't in a state I could simply pull a PR at the time.)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 Merged
  From the email thread: what we have so far is `tf.__version__` and the various GraphDef version numbers (`tf.GRAPH_DEF_VERSION` and such).  GPU support and exact commit would be useful.
 Editing title to reflect the focus.
 @martinwicke: Would you prefer contributions welcome or assigned to you? 
  I agree that getting the container started with GPU support still isn't the smoothest experience, especially when it comes to matching library versions. That said, I feel like "now go read the nvidia wiki and install this extra package" has its own tradeoffs as a getting started experience.

Probably the thing to do here is clear up the instructions:
1. make sure to clearly route people to either `docker_run_gpu.sh` or the NVidia docker plugin, and
2. add a few diagnostic steps (eg "here's how you run `deviceQuery` ...") for troubleshooting.

@ebrevdo do you have time to take a look at this?
 I would not rush it. I have spend a couple of hours with nvidia-docker two weeks ago. I have tried to use it for ci.tensorflow.org. It was not fun. I would not recommend that at this time. It would, imho, make more harm than good.

Simply try to install latest stable drivers. Check you have /dev/nvidia0, /dev/nvidiactl and /dev/nvidia-uvm outside docker and that you have /usr/lib/x86_64-linux-gnu/libcudart.so and /usr/lib/x86_64-linux-gnu/libcuda.so. If so the docker_run_gpu.sh script should work.

BTW: I believe docker and cuda together will get better. But it is not going to be a great experience any time soon if ever.
 I think @3XX0 is on the right track here -- in fact, at some point long ago @ebrevdo and I hit something like this and worked around it by creating that same symlink. Eugene, did we work around that at some point, or just leave the symlink intact and forget?

In any event, it seems like an easy thing to add in the container?
 nvidia-docker is a good idea but not working too well. I was trying it today again. It fails with no error message. Going without it there are at least errors to solve and get to a working state. I will report some bugs to nvidia-docker once I have time for it.

The 0.7 release had some issues so there will soon be a bugfix 0.7.1. It will go also with upgraded cuda and cudnn https://github.com/tensorflow/tensorflow/pull/1166.

@cancan101 How does it work with the symlink? nvidia-docker is working for some people but it require to create that symlink? Inside of the container? If so we can just add it to the container tomorrow :-)

BTW: Security? No big deal for something like this image. If anybody care about security they run apt-get update & upgrade any now and again anyway.
 Interesting. For whatever reason yet another try the same way and this time nvidia-docker worked! Nice. Sort of... The example nnvidia-smi run well, but tensorflow tests have failed!!!

It did not seem te require any link inside nor outside the container.
 That's not official container...

We will make sure the 0.7.1 will run on gpu. I'm happy to put the symlink inside the container. On the other hand the next development container (after https://github.com/tensorflow/tensorflow/pull/1166) seems to be fine without the link. But I have tried that after upgrading the base containers and stuff.
 Yep, an oversight. I'll send a fix.
 Maybe. I can try it for open source build as well as internally.

I'm not sure if the link is needed anymore but I'm going to put it into 0.7.1 release just to avoid problems.
 Perfect! Thanks @vrv!
 .1 was added in https://github.com/tensorflow/tensorflow/commit/d0a822fbcb04d95a643d8efe65699a8d1cdce98b
 That makes sense. Thank you @3XX0!

Note: If you want to work on tensorflow you may take a look at [ci_build](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build).
 @craigcitro, @jendap: If nvidia-docker is not ideal, should we close this issue?
 @flx42: Even better!  Does that mean this bug is fixed? 
 yeah, it is recommended only in that one file, because I was just updating that file a few days back. We should update all the places. I'm thinking about doing it tomorrow - so that it makes it into 0.9.
  Jenkins, please try to test this?
 And by that I mean, test this, please?
 Alright, Jenkins, test this please.
 Can you squash these commits?
 Thanks!
  @benoitsteiner, can you answer this question or reassign?
 @benoitsteiner: Can you comment on this? 
 As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this PR. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.
   @vrv: Can you review this?  Two commits, each a one line change.
 @tensorflow-jenkins: test this please
 Merged
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I don't want that behavior. It will silently cause irrecoverable issues if the user aborts the tarball extraction in the middle. If you don't want to re-extract, you can simply skip this cell. Note that this is different from the maybe_download() case since it's easy to check for download size.
 Aha, not having these variables defined is indeed a problem.
There are some changes to this notebook that are about to be pushed to the public github, let me think about it in the meantime, it seems to be a problem for several people. Thanks for clarifying.
 A fix similar to yours should be pushed shortly (sorry, my tree was not in a state where I could accept PRs easily). Thanks!
  We fixed this recently in https://github.com/tensorflow/tensorflow/commit/5ff6d34a05b2eb49f7a79a4d0b78ada9a6842b6c
  We fixed this just recently in https://github.com/tensorflow/tensorflow/commit/5ff6d34a05b2eb49f7a79a4d0b78ada9a6842b6c
  We would need to see the output when run with --verbose_failures to know what's going wrong.
 (Closing issue to help triage more active bugs.  Feel free to comment again and we'll reopen).
  Stack overflow is perhaps better for these sort of questions. Have you tried
https://www.google.com/search?q=ImportError%3A+No+module+named+examples.tutorials.mnist.input_data

BTW: It is something is wrong with the python environment. Have the pip finished without error? Are you using virtualenv?
  Note that, for experimental purposes, all of these can be implemented by doing the padding as a separate layer and using VALID padding.
 @cesarsalgado: You can pad with a constant other than zero by subtracting a constant from the input, padding with zero, and compensating in the output.
  @tensorflow-jenkins: test this please
 Merged
  @tensorflow-jenkins test this please
 LGTM if tests pass.
 Merged
  This sounds like an issue with your model rather than a bug in TensorFlow, you may get more help from stackoverflow community on an issue like this
 Numerical stability means the result of computation is more accurate. But when you have exploding gradients, the growth in magnitudes is a feature of your training procedure rather than numerical error. It's like putting i=i**2 in a loop, you are going to run out of range even if you use numerically stable implementation of square operator
  Closing due to inactivity.
  F tensorflow/stream_executor/cuda/cuda_dnn.cc:207] could not find cudnnCreate in cudnn DSO

suggests that you didn't point to the right cudnn path.

1) Double check that you pass the right paths when running ./configure before building the pip package
2) Try 

```
TF_UNOFFICIAL_SETTING=1 ./configure
```

to specify the cudnn versions / library paths.
  The issue is that start_queue_runners() starts (one or more) threads that are running portions of the graph.  Since graph modification is not thread-safe, you will see problems if the graph is modified after start_queue_runners().  @mrry do we have a way of making this an error?
 We could use [`tf.Graph.finalize()`](https://www.tensorflow.org/versions/0.6.0/api_docs/python/framework.html#Graph.finalize), which is intended for such purposes. To the original point: the `tf.Graph` methods for adding ops are definitely not thread-safe, and it would require a big redesign to achieve that (with only a small upside).
 Sounds like `start_queue_runners` should call `finalize` (possibly with a flag to turn it off if you really know what you're doing)?
 @josh11b: Should I reassign? 
 I think this is fixed since acac487ac4ebaa6edb3e3f866d41cbd12546a107.
  @tensorflow-jenkins: test this please
 Merged
  Merged. Thanks!
  Our GPU build is broken  -- probably sync back to Thursday and you should be alright until we fix it.
 Fix is incoming.
 Just pushed the fix
 Most likely the flags for you are passing for STRICT_ANSI etc aren't making it to nvcc.  You might need to edit the crosstool file in third_party/gpus/ to pass the flags there.
 I would argue this belongs in a new issue, the existing bug was for a separate problem.
  The eigen code is downloaded from bitbucket and stored by bazel under bazel-tensorflow/external/eigen_archive.

Can you tell me the exact version of gcc you're using ? I am able to compile on ubuntu without any problem, but I might be using a slightly different revision of the compiler.
 Maybe you can download Eigen directly from bitbucket and see if you can successfully compile the tests:
cd /some/dir/
hg clone HTTPS://bitbucket.org/eigen/eigen
mkdir build
cd build
cmake /some/dir/eigen DEIGEN_TEST_CXX11=ON
make check
 Is this still a problem? 
 When you try again, can you make sure that you create the VM with a lot of memory? gcc may simply have been running out of memory. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Cool, can you wrap the couple of lines that go past the edge and squash all the commits? I'll run a test in my local env this weekend or Monday and pull it. Thanks again!
 Merged, thanks!
  PIP package is built and installed in a Docker container. Then the Python unit tests in the TF source are moved to a separate folder and ran with the plain Python environment without Blaze.

Four out of 151 tests were skipped for now due to dependency on modules in TF source that are not publicly exported.

Tests performed: Jenkins experimental builds for CPU and GPU Python test-on-install, see passing results at: 
http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/1/consoleFull
http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-gpu-python27-copt_pip_install-test/11/consoleFull
 nit: you should drop the "(wicke's comments addressed)" from PR title
 FYI, I made a few additional small changes today to get the PIP test-on-install working on Mac (w/o Docker).
 @jendap @martinwicke @vrv Please let me know if you have any further comments on this PR. 
 I'm good with it.
 Looking at
http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/ws/

How many directories it creates? _python_build, pip_install_tests, pip_whl? Can it live inside one - maybe ".test_installation" or ".pip_test"? And it should probably be in .gitignore as well.
 It looks good. Just drop the one script, cat the log output on FAILURE and unify the directories into one.

Then squash the commits and merge it.

(Other things like potential separation test_installation and pip.sh can come later)
 Regarding the question of what directories are created. It currently creates "pip_whl" and "pip_install_tests". The "_python_build" was created by a previous version of the file and never got deleted because nothing deletes it. I'll consolidate the directories into one common parent folder:
pip_test/whl (for the .whl file to live) 
pip_test/tests (for the test files and logs to live)

I'll add "pip_test" to .gitignore as well. 
 Comments from @jendap have been addressed. Commits squashed. @martinwicke or @vrv, could you please merge it? 
 Merged
  Lots of people are having trouble downloading and pickling the data. need to look into ways to make that easier.
 @bhack under which conditions do you 'lose' the pickle? Is it because you're using an ephemeral docker container?
 @rthouvenin your approach is a net improvement over the status quo.
Would you be ok cleaning it up (in a way that can be easily diffed) and sending me a PR?
https://github.com/rthouvenin/tensorflow/blob/notmnist-lowmem/tensorflow/examples/udacity/1_notmnist.ipynb
  PIP package is built and installed in a Docker container. Then the Python unit tests in the TF source are moved to a separate folder and ran with the plain Python environment without Blaze.

Four out of 151 tests were skipped for now due to dependency on modules in TF source that are not publicly exported.

Tests performed: Jenkins experimental builds for CPU and GPU Python test-on-install, see passing results at: 
http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/1/consoleFull
http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-gpu-python27-copt_pip_install-test/1/consoleFull
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Can you either sign the CLA for your other email address or amend the
commits (or squash them) to use only your CLA-ok email?
On Fri, Jan 29, 2016 at 08:30 caisq notifications@github.com wrote:

> PIP package is built and installed in a Docker container. Then the Python
> unit tests in the TF source are moved to a separate folder and ran with the
> plain Python environment without Blaze.
> 
> Four out of 151 tests were skipped for now due to dependency on modules in
> TF source that are not publicly exported.
> 
> Tests performed: Jenkins experimental builds for CPU and GPU Python
> test-on-install, see passing results at:
> 
> http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/1/consoleFull
> 
> ## http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-gpu-python27-copt_pip_install-test/1/consoleFull
> 
> You can view, comment on, or merge this pull request online at:
> 
>   https://github.com/tensorflow/tensorflow/pull/935
> Commit Summary
> - Python PIP test on install
> - Improving test_on_install.sh
> - Minor topy fix
> 
> File Changes
> - _M_ tensorflow/tools/ci_build/Dockerfile.cpu
>   https://github.com/tensorflow/tensorflow/pull/935/files#diff-0 (8)
> - _M_ tensorflow/tools/ci_build/Dockerfile.gpu
>   https://github.com/tensorflow/tensorflow/pull/935/files#diff-1 (9)
> - _A_ tensorflow/tools/ci_build/builds/test_on_install.sh
>   https://github.com/tensorflow/tensorflow/pull/935/files#diff-2 (120)
> - _A_ tensorflow/tools/ci_build/ci_test_on_install.sh
>   https://github.com/tensorflow/tensorflow/pull/935/files#diff-3 (81)
> 
> Patch Links:
> - https://github.com/tensorflow/tensorflow/pull/935.patch
> - https://github.com/tensorflow/tensorflow/pull/935.diff
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/935.
 One of the commits is created using "cais cais@ci.tensorflow.org", maybe just squash the commits, then you should be fine.
 I'm closing this pull request and creating a new one to address wicke's comments and the cla issue. See https://github.com/tensorflow/tensorflow/pull/939
  This simple doc fix is related to issue: https://github.com/tensorflow/tensorflow/issues/896
 merged.
  @dvyukov, @rmlarsen: Is this resolved?  
  @zheng-xq: Do you have thoughts on this?  The non-blocking thread pool PR is problematic since it breaks Eigen's FIFO requirement, but the deadlock issue seems like something we should fix. 
 @dvyukov, @rmlarsen: Did the recent thread pool changes resolve this?  
 @rmlarsen: Assigning you since @dvyukov isn't part of the Github org.  Should we add him? 
  It works for me. What browser are you using?
 Closing. Please reopen with more information if it persists.
  @dvyukov, @rmlarsen: Did the recent thread pool changes resolve this?
  Did you upgrade to 0.1.4 or bazel HEAD?  0.1.4 should work.  bazel HEAD probably doesn't right now, because they changed the legal names of entries in WORKSPACE files.  I suspect they'll update our WORKSPACE file before their next official release.
  LGTM

Thank you!
 @vrv can we merge this?
 @thagikura can you squash your commits? 
 merged.
 Glad to see this go in. FYI there's also "adb install -r -g tensorflow_demo.apk" which will automatically grant permissions upon install (so you don't need to grant manually on every install). Unfortunately there's no way to do this currently directly through bazel, but they are working on it.
  GitHub was having trouble. Is this still a problem?
On Thu, Jan 28, 2016 at 23:54 LiLi notifications@github.com wrote:

> $bazel build -c opt --config=cuda --spawn_strategy=standalone
> --verbose_failures //tensorflow/tools/pip_package:build_pip_package
> 
> [4 / 4] Cloning https://githubcom/Polymer/polymergit: Resolving deltas
> (13948 / 15071)
> 
> it hangs here for a few hours
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/924.
 I'll close this. If you still have trouble, reopen it.
  This is a warning, not an error.  It might be an unhelpful warning though.

Did the test end up failing?
 This is a message that's printed when you reach the end of the queue, so it's part of normal operation
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  can you try sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))
If it still hangs, maybe you could compile in debug mode, attach GDB to hanging process and do "bt" to see where it's stuck
 Oops, brain freeze, updated to use the proper ConfigProto object rather than unsupported flags
   Jenkins, would you be so kind and test this please?
 Merged.
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 Closing, we need the CLA to be signed to look at it :(.  Feel free to sign it and we'll reopen.
  Hey TF,

Recently, for deep RNN's, sequence wise batch normalization has proven to be very helpful. [Deep Speech 2](http://arxiv.org/pdf/1512.02595v1.pdf) in section 3.2 explains this in more detail. Sequence-wise batch normalization is described in section 4.1 in [Batch Normalized RNNs](http://arxiv.org/pdf/1510.01378v1.pdf).

`tf.nn.moments` is very useful for batch normalization because it gives you the mean and variance. However, in seq2seq setups, we need to do a batch normalization across all timesteps (the entire sequence). Unfortunately, the way `seq2seq.py` is written, each timestep is within a separate 2D matrix. This means that there is a list of 2d tensors for the entire sequence.

If somehow, `tf.nn.moments` could be modified to accept lists of tensors (all of the same dimensions), it would be incredibly helpful. This way, we could input the entire list of tensors, and compute the resulting mean and variance for that sequence. 

Thanks!
 This is a good idea, we put this on the list of things to do.
 Thanks really appreciate it!
 @LeavesBreathe I'm looking into some of these issues right now. Is there any specific reason why a `tf.nn.moments([] ...)` type of API would be preferable to simply doing `tf.nn.moments(tf.pack([]) ...)` ?
 If I understand things correctly, tf.pack would copy all the tensors, and require more memory. Please correct me if I'm wrong. 

Otherwise, your right: this is a much better solution. Thanks for the help. 
 @LeavesBreathe in the grand scheme of things, it will not affect memory consumption much: as soon as the copy is performed, the input buffers will no longer be needed for the backward pass, and be releasable to the pool. Packing things into one tensor also potentially enables more efficient contractions to take place. What matters most to memory consumption is long-lived tensors, e.g. ones that have to be preserved between the forward and the backward pass. One could still consider a different API if there is evidence that this is a problem.
I'm also looking into separating the collection of sufficient statistics (which can be performed online) from the actual aggregation, which would give users the freedom to make the reductions more granular if they wanted. I'll be closing this bug now, and treat the other items as opportunities for optimization.
 Thank you vincent, I will try this approach and apologize for not thinking of it sooner!
 I have just committed a change to `nn.moments()` which allows you to decouple the collection of the sufficient statistics `nn.sufficient_statistics()` from the computation of the means and variances `nn.normalize_moments()`. This allows you to build compact statistics at each time step, aggregate them by summing them whichever way you want, and normalize them afterwards. I hope this helps.
  @zheng-xq: Can you take a look at this?  Seems like it fell through the cracks. 
 Using TensorFlow with other frameworks is a bit tricky, since there are many places assuming it has exclusive access to the GPU. So disables GPU functionalities in one of them sounds reasonable. 

@kbrems, with your use case, it is not recommended to use TensorFlow operators in the same thread as other Cuda runtime calls. TensorFlow uses stream-executor a faster Cuda runtime alternative. If standard Cuda runtime APIs is called on the same context, it confuses stream-executor with what context is bound to what thread, and many of its internal data structure. 

If you have to make Cuda calls, either: 
1. Call the stream-executor alternative. 
2. Call the Cuda driver API similar to what stream-executor does, and make sure you restore the context binding after you are done. 
 Closing for now since making TensorFlow share the GPU seems out of reach.
  Also we might be able to clean up most of these warnings by properly declaring Node::num_inputs and num_outputs as a size_t (and fix all the callers).  That seems like a much more correct change than masking these type errors.
 +1 for fixing the underlying problem.
  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/types.h

Are you properly synced?
  This is not the best place to ask questions about the class. In this case, delete the file and try to download it again. You can also try to download it from the website manually and see if that succeeds.
  If you have more memory than that, can you try raising the ceiling and see if it fails later?
Someone else commented they had needed more.
 @timothyjlaurent was successful with the Docker container. Any gotcha there? I haven't tried recently. Maybe @craigcitro has some advice too.
 ah, this one is interesting: the problem here is that while `docker run` happily allocates more memory to the container, the problem is that the underlying **vm** (owned by `docker-machine`) doesn't have any more memory to give.

i followed the second option in [this SO post](http://stackoverflow.com/questions/32834082/how-to-increase-docker-machine-memory-mac) to bump the underlying vm's memory to 8GB, and things seem to be humming along.

so:
- we probably want to add a note to the course instructions for OSX users about ensuring their machine has enough RAM.
- `docker stats` probably wasn't lying about the total memory _used_, just about total amount _available_. :grin: 
- it's also possible that we could do a bit more cleanup of leftover objects in the python code itself (i think sometimes numpy is bad about cleaning up after itself, but i could be wrong), but that's really a secondary concern (since we definitely need more memory in the base VM regardless).
 Would someone who understands this well mind adding those tips to the README.md?
 Sending a CL out now.
 Gracias
  It's fixed at Head https://github.com/tensorflow/tensorflow/blob/411f57e291839094108afdaa9c43094f44979eaa/tensorflow/g3doc/tutorials/mnist/beginners/index.md
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I'm wondering whether we should leave the code as is and write text to tell people to rename the whl file before installing it. The code gets very bifurcated the way it's written now.
 Closing due to inactivity.  We'll try to get better python install instructions if we can, without creating too much duplicate text
  @RobRomijnders @jeandut If you are on a quest for nice tutorials, I encourage you to help improve the TensorFlow tutorials. It's an open source project - you can modify everything yourself! For example, @RobRomijnders you could modify our tutorial code for CNNs to use TensorBoard like in the page you posted.

@martin-gorner nice job with the convolution filter visualization. If you want to submit a proposal for how the API should look for adding that to TensorBoard, that would be quite welcome.

Closing this issue since @jeandut says his question was answered (:
  Thanks for the fix, but it turns out this is actually required in our codebase for platform-specific reasons. 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/__init__.py#L16

We'll try to better document this somewhere.
  Thanks for the contribution!  I'm a bit confused though -- you've made the change to the crosstool when compiling with GPUs.  Did you intend to want it to work without --config=cuda too?
 Assigning to @andrewharp, who has more experience with arm builds :)
 Seems sensible, but it looks like you need to merge master into your branch.
 @tensorflow-jenkins: test this please

(sorry for the huge delay)
 every time you merge, it invalidates the tests, so it would be good to avoid merging after i've kicked off tests :)
 @tensorflow-jenkins: test this please
  Here's a recent discussion of how to read variable sized images: http://stackoverflow.com/questions/35028173/how-to-read-images-with-different-size-in-a-tfrecord-file
  If I remember correctly, the existing code is actually a correct reparameterization of momentum, even though it looks subtly different.

The momentum vector stores the unscaled gradients, and we multiply the entire momentum vector by the learning rate when applying the update.

(This has come up several times internally -- we should add documentation to the ApplyMomentum op: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/training_ops.cc#L98  Feel free to do so there if you think it would help others).
  Two forms produce equivalent descent path. Note that you can factor out lr() out of momentum in your formulation, and then write accum = lr()_(unscaled_accum); var-=lr()_unscaled_accum. So in our case the difference between our accum and your accum is an extra factor of lr(), but it doesn't affect the update. There are several slightly different variations of momentum equation out there which are equivalent for practical purposes. For instance, Polyak's original paper on momentum wrote update as x_{k+1}=x_k - \alpha_g grad + beta_k(x_k-x_{k-1})
 Closing this. Reopen if the scaling of the stored value is important.
  @tensorflow-jenkins: test this please
 Merged
  Jenkins, go test this please.
 Thanks. This looks good. Running tests for completeness.
  Looks like a transpose gone wrong here: tensorflow/python/ops/math_ops.py:967
 Thanks for the catch Bob! I've just submitted a fix, and adapted versions of your tests, hopefully it should show up in the public repo soon. I had a braino when I was coding up the FLOPs calculations, because I forgot that the y dimension in the shape [x, y] would be at index 1, not 0 in TF's conventions.

Anyway, I appreciate the well-documented bug and test code, thank you!
  Yes, the gradient of floor is always zero.
 Well, almost always.
 Yes, changing it to return `[None]` would be the fix.
 `None` will be treated by the gradient code as "no connection", which is mathematically equivalent to zero but faster since it will never construct large zero matrices.  There's some related discussion here: #783.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I think this may be superceded by a change @vincentvanhoucke recently made ...
 @vincentvanhoucke already fixed this internally (we'll be pushing it out soon).
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @keveman @josh11b for more thoughts.

At a first glance, it might be better to create a separate target for this.  libtensorflow.so is, I think, meant to hold the core TF library.  cc_ops are just an API for defining the graph, which isn't really part of the core, so I'd be worried about conflating the two.  
 The library for the graph building C++ API should be distinct and separate from libtensorflow.so, IMO. Perhaps make a separate libtensorflow_cc.so target for it?
 ping for @keveman 
 I was recommending separate library for building C++ ops, i.e., libtensorflow_cc.so containing only cc (not core + cc). But that's my opinion. Perhaps @josh11b can chime in?
 My recollection is that the library for building C++ ops depends on things like core/graph/graph_def_builder, which has other dependencies in core.  Were you thinking that libtensorflow_cc.so would have a dependency on the core so?
 Yes, libtensorflow_cc.so will have a dependency on libtensorflow.so.
 Hi @memo: is this still being worked on / ready to look at?
 Okay, sorry for the delay (our test machines were down).  can you re-merge one more time and squash the commits for one last look?
 @tensorflow-jenkins: test this please
 Merged
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 LGTM @vrv 
 @tensorflow-jenkins: test this please
 Merged. Thanks!
  Ha. You found a real problem here. You are correct that this is the right thing. I think I will change the installation instructions to say you'll be installing master, and fix the bazel version issue you noticed in 0.6.
 Take a look at #900. I change it to make clear that the installation instructions given will install the current master branch, and added what to do to install a specific release. I'll make another PR to make the same change in 0.6.0. It's not perfect, but if we add a bazel version number, it will get stale. I also don't want to encourage people to install the latest stable release from source -- I'd much prefer people installing from source work off master.
  There is not nearly enough information for anybody to help.  If you add more information about your setup, what you've tried, etc., we might be able to offer help, and I'll reopen the issue.
  I think this has been called 'atrous convolution', at least in the Eigen code.  Does anybody know whether cudnn supports this?  @zheng-xq ?  
 I don't think Cudnn supports convolutions for stride larger than 1. Not
sure the FFT algorithm they were using would still be correct.

On Tue, Jan 26, 2016 at 10:56 AM, Vijay Vasudevan notifications@github.com
wrote:

> I think this has been called 'atrous convolution', at least in the Eigen
> code. Does anybody know whether cudnn supports this? @zheng-xq
> https://github.com/zheng-xq ?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/889#issuecomment-175176178
> .
 I stand corrected. The stride larger than 1 should be supported by Cudnn. I was looking at upscalex instead. 
 it's not just stride > 1, it's stride > the input patch.  E.g., skipping regions of the input.

@vincentvanhoucke just corrected me: atrous is introducing holes in the input patch, so the input has 'holes'.  So it's different.

I don't know if our eigen (CPU) implementation supports stride > input_patch.  If it does, and cudnn does too, we can remove this assertion.
 FYI last week I did try to look at this and cudnn appeared to give different results than I expected (eigen was correct for both CPU and GPU).  Not yet sure whether this is a bug in cudnn or how we were calling it.
 I'm marking this a bug since it really should work. Also contributions welcome. See also the recent duplicate #1815.
 @ry: I wish I could still find my code.  I remember Eigen doing something consistent across GPU and CPU and expected and cudnn doing something different (and seemingly wrong), but I might have introduced a bug.

The best thing to start with is via a test: add a simple test to conv_ops_test.py that exercises say, a 1x1 convolution with input depth 1 and output depth 1 for a 1x3x3x1 input with stride 2 (so you get four values: the corners multiplied by a filter).  

If you try to run that, you'll first run into a problem doing conv2d shape inference in python (because it checks for stride < filter) -- remove that and make sure that the shape calculation is correct.

Then you'll probably have to make the same fix in the C++ code (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ops_util.cc#L41) to remove that check.

Other than that, I don't think there was anything else I had to do to get the calls to run, but that's when I started seeing different output and didn't have time to debug.  It's possible it has something to do with padding or the shape inference difference between us and cudnn.  Happy to help how I can.
  That message is not an "out of memory" error: it's just information about the status of the host-pinned memory allocator (e.g., it's useful for diagnosing poor performance) and is printed (hopefully) infrequently.

In general, messages that start with "I" are info logs.  Errors start with "E".
  @tensorflow-jenkins: test this please
 test failure is due to flakiness, so this is probably fine.  assigning to @zheng-xq in case he knows of any performance implications of switching to std::search over memmem
 ping for @zheng-xq 
 @tensorflow-jenkins: test this please

For the common case of simple short matches, std::search should be faster than memmem -- for the more pathlogical cases, memmem can be faster, but for most cases where the substrings don't overlap much, std::search should be preferred, so we'll take this change.
 Merged
  Merged
  LGTM @vrv 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 LGTM @vrv 
 LG, squash the commits into one and we'll merge!
  FWIW, our builds on Jenkins are still working...
 I might revert 19b3606: can anyone confirm that reverting just that CL fixes the problem for them?
 Reverted the change that updated Eigen.  Let us know if this was not enough and we'll investigate further.
  ping for @ebrevdo 
 Lukasz knows this code better.
 I'm not sure it's always better to propagate down to the embeddings, but indeed - it would be great to at least have the option. Before accepting the code I think it's necessary to correct the embedding_seq2seq models that define loop functions extract_argmax_and_embed -- these functions will not work at all without stop_gradients (as there is no gradient for argmax). I hope our tests will catch this.
 In all cases, this patch needs a unit test.
 This change looks very good to me, let's merge and submit. Thanks for correcting this error!
 Jenkins, test this please.
 I see a conflict in this PRs future. A modified seq2seq is already on staging. I'd like to wait until we push this, and then resolve that conflict before we merge.
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
 Merged
  ``````
$ git branch -a -v
* master                a27d844 Merge commit for internal changes
  remotes/origin/0.6.0  8242b4d TensorFlow: some more python3 compatibility test fixes
  remotes/origin/HEAD   -> origin/master
  remotes/origin/master a27d844 Merge commit for internal changes```

$ git checkout 0.6.0 -b 0.6.0

$ git log -n 1
commit 8242b4dd1b36440e191fef8a07b6f37d8bcee60d
Author: Vijay Vasudevan <vrv@google.com>
Date:   Wed Dec 9 17:00:29 2015 -0800

    TensorFlow: some more python3 compatibility test fixes

    Change-Id: I5678cbdfb45757e2218494f542c9b0a0d5cc16a4
``````

Seems to works for me...
 ```
$ git clone https://github.com/tensorflow/tensorflow.git  --recurse-submodules 
$ cd tensorflow
$ git branch -r
  origin/0.6.0
  origin/HEAD -> origin/master
  origin/master
  origin/r0.7
  origin/release-notes
$ git branch 0.6.0
$ git checkout 0.6.0
```
  The server hosting MNIST is currently down.
 https://www.reddit.com/r/MachineLearning/comments/42ic2y/does_anyone_know_where_to_download_the_mnist/
might help.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 LGTM, awaiting CLA. Thanks!
 CLAs look good, thanks!

<!-- ok -->
 @seanpavlov looks like you need to rebase, I'm getting conflicts.
 Fix a-coming from upstream.
  I like the idea of having a more or less platform independent script. Here are a few concerns I have with this version:
- I'm skeptical that the package names are all the same for yum, apt-get, zypper, yast. 
- I'm not even sure they're consistent across different distributions that use the same package manager.
- I don't know whether all of these take the same arguments (e.g. `install` vs. `-i`)
- variable evaluations should almost always be quoted. Otherwise expressions like `[[ $env -eq 0 ]]` fail with a syntax error if `$env` is empty.
- Why are you using the 0.5.0 wheel for CPU only python 2.7?
- If you get into a "bad choice" branch, the script still continues
- If you install without virtualenv, the script will still output instructions for virtualenv
 @martinwicke: Looks like this fell through the cracks.  
 Ok, I think after we know a little more about the environments out there, I doubt we want to maintain such a script. It's a great idea, felled by the messiness of contemporary information technology.

I'll close this issue. 
  You should try upgrading to 0.6.0 -- I think that file was broken in our initial release except when via bazel.
 You'll probably get a better answer for these types of questions on StackOverflow. 
  This seems like a question better suited for StackOverflow, since this isn't a bug / feature request.
  De-duping with #491
  Jenkins test this please.
 @tensorflow-jenkins: test this please
 This is merged. Thanks for noticing. 
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @craigcitro any gotcha going down that route?
Should I also be advertising not using --rm for users of the pre-built Docker container?
 No, these both seem pretty reasonable. Two thoughts:
1. The `--rm` thing is definitely a smart move -- I tend to spend more time _debugging_ docker images than _using_ them right now. For this case, though, it does make it a bit too easy to lose notebooks.
2. Mounting the notebooks dir as a volume also makes sense. This is one where mounting it ("I want to make changes that get saved") and not mounting it ("I'm playing around with something and it's a throwaway") both make sense, so maybe some text explaining when you would or wouldn't want the option? As a minor thing, no need to `cd` -- just pass the subdir path instead of `$(pwd)`?
 @timothyjlaurent thanks for the fix. Would you mind changing the doc for the hosted container as well and removing the 'cd'?
 LGTM @vrv 
 Thanks!  Squash the commits and we'll merge.
 We aren't running any `docker build` commands from the top of the tree, though, are we? (Just the one `docker run`?)
 Merged. Thanks!
 I'm not sure why you, as a class user, would ever want to build the container?
These instructions are for us, when we want to update the hosted containers.
 Arpan, there should be no need for anyone to build a container. Was the doc updated?
 @timothyjlaurent Access to the hosted version is described on the first line in the document you edited:
docker run -p 8888:8888 -it --rm b.gcr.io/tensorflow-udacity/assignments
Maybe I should separate the rest of this readme so that it stands out more.
 @timothyjlaurent we pushed some changes this weekend that should help with memory.
  Can you remove the commits which we've already merged?
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Hi @lzlarryli, thanks for your help with this. Is there any chance you could make the 'diff' minimal, with only the code changes that you really needed to do, but not mutating the state of the cells? It would make it a lot easier to approve quickly.
Also, I don't believe any of my uses of xrange are performance-critical. If it helps readability to just turn them into 'range' for bot python 2.7 and 3.0, then I'd marginally prefer that.
 I am going to re-export all the notebooks as v4 tomorrow. It should make things a lot easier after your rebase. Thanks again!
 @lzlarryli re-export should show up on the github repo soon. Thanks for your patience.
 Darn, I was hoping it would be enough. I have received a few patches such as https://github.com/tensorflow/tensorflow/pull/884/files that have clean diffs.
Is it the case that most of the change is along the lines of:
   "import sys\n",
-    "if (sys.version_info > (3, 0)):\n",
-    "    from urllib.request import urlretrieve\n",
-    "    import pickle\n",
-    "    xrange = range\n",
-    "else:\n",
-    "    from urllib import urlretrieve\n",
-    "    import cPickle as pickle"
  if so I can take a crack at it.
 @lzlarryli I have made updates to all notebooks. 4/5/6 should be pushed shortly.
I didn't quite understand what you were doing with char2id, it seems to work fine as-is, and exclude non-ascii lowercase characters, but maybe I missed something?
Would you mind giving it a whirl?
 Thanks for the report. I'll be happy to take a pull request, or I'll fix it myself tomorrow.
  As I [answered](http://stackoverflow.com/a/34996139/3574081) on StackOverflow, this is intended behavior when a dimension cannot be statically inferred. In fact, a dimension size of `None` is compatible with _every_ concrete dimension size (it acts as a wildcard), and any constraints will be checked at runtime.
  This api is about to change. From now on rnn() and its cousins will return just the very last state instead of a list of intermediate states.
  This was just rolled back.
 Fixed with pr #1611
  We could also avoid using auto there, I'm not sure it adds much to readability ...
 Pull requests welcome if anyone wants to fix this!  @lesniewski: Would you interested in wrapping your fixes up as a PR?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @shreyasva thanks! I would slightly prefer no line wrapping:
data_folders = [
  os.path.join(root, d) for d in sorted(os.listdir(root))
  if os.path.isdir(os.path.join(root, d))]

Please sign the CLA so that we can merge!
@vrv FYI
 "commit f13cf6a2b5609a6ed0f8c09c77661fca7ad95467
Author: Shreyas VA shreyasva@Shreyass-MacBook-Pro.local
"

You need to recommit with a proper email
 @shreyasva hold tight a little longer: I've just committed a change that upgrades all the notebooks to v4 to make such diffs easier down the road. Sorry about that :/
 Merged, so you can probably re-apply now.
  No, it looks like you have to use single quotes, I'll revert that commit since I can't really read the diff anyway.
 Reverted in https://github.com/tensorflow/tensorflow/commit/be1f68e254d38a7545145731ded5873cb914e509
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 LGTM @vrv
  @tensorflow-jenkins: test this please
 Merged
  Please use StackOverflow for these types of questions.
  This looks more like a StackOverflow question rather than a bug report.  You might try using a control dependency to make sure your assigns are happening after the subtractions.
  LGTM @vrv 
 Merged
  I'm not sure about one thing: is your intention to clear the variables in the scope, but preserve them in the graph, or to clear the whole graph? In the first case, I think it's dangerous -- why call it "x" if you're re-doing x? I understand if you want to clear the whole graph, but that can be done with tf.graph().as_default(), no? Please, let me know more of the context so we can solve it!
 [`tf.reset_default_graph()`](https://www.tensorflow.org/versions/master/api_docs/python/framework.html#reset_default_graph) is probably the closest thing we have to this functionality, although it blows away all ops and tensors, as well as all variables.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks, though this is a duplicate of #850, which I'm about to merge.
  Thanks for pointing it out!

The first part of Assignment 2 (code provided) is relevant immediately after SGD in L1, but the stated problem requires knowledge of ReLUs and the ability to create a multi-layer network. So Assignment 2 has been moved to right after Backprop in L2, and just before discussing deeper networks.
  Looks like this is an unclear error message.  You are asking for the output of the "init_all_vars_op", but it doesn't have any output.  You just want to run that, which you do by supplying it to the third parameter to Run(), see https://www.tensorflow.org/versions/master/api_docs/cc/ClassSession.html#virtual_Status_tensorflow_Session_Run

status = session->Run(inputs, {}, {"init_all_vars_op"}, &outputs);
   Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 signed it
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Merged
  @tensorflow-jenkins ok to test
 @tensorflow-jenkins: test this please
 (I would prefer squashing these two commits, since they are small)
 (yeah, squash the commits into one.  See [here](https://github.com/ginatrapani/todo.txt-android/wiki/Squash-All-Commits-Related-to-a-Single-Issue-into-a-Single-Commit))
 Merged, thanks!
  PR #853 merged
  @Yangqing FYI
 I think there is a TX1 that I could use to take a look. I'll see what I can do.
 @robagar It all depends on how large your network is and whether you intend to train the model on TK1 or just run inference. Two GB of memory is plenty to run inference on almost any model. 
 I have worked around an issue that prevented nvcc from compiling the Eigen codebase on Tegra X1 (https://bitbucket.org/eigen/eigen/commits/d0950ac79c0404047379eb5a927a176dbb9d12a5).
However, so far I haven't succeeded in setting up bazel on the Tegra X1, so I haven't been able to start working on the other issues reported in http://cudamusing.blogspot.de/2015/11/building-tensorflow-for-jetson-tk1.html
 I still haven't been able to install bazel. That said, the assertion you're facing seems to be triggered by the variadic template at line 195 of ./tensorflow/core/lib/strings/strcat.h. I would just comment this code and see how it goes.
 @damienmg FYI
 @jmtatsch At the moment, the version of cuda that is shipped with the tegra x1 has problems with variadic templates. Nvidia is aware of this and working on a fix. I updated Eigen a few weeks ago to disable the use of variadic templates when compiling on tegra x1, and that seems to fix the bulk of the problem. However, StrCat and StrAppend still rely on variadic templates. Until nvidia releases a fix, the best solution is to comment out the variadic versions of StrCat and StrAppend, and create non variadic versions of StrCat and StrAppend with up to 11 arguments (since that's what TensorFlow currently needs).
There are a couple of ways to avoid the STL issues: a brittle solution is to only compile optimized kernels. The compiler then inlines the STL code at which point the lack of host device annotation doesn't matter since there is no function call to resolve. A better solution is to replace all the STL functionality with custom code. We've started to do this in Eigen by reimplementing most of the STL functions we need in the Eigen::numext namespace. This is tedious by much more reliable than relying on inlining to bypass the problem.
 @maxcuda Where can I download the new cuda compiler? I'd like to make sure that I don't introduce new problems when I enable variadic templates again.
  LGTM, thanks.
@vrv feel free to pull (let me know if there is a different protocol to signify changes are ready, I'll just reassign to you)
 Merged
  Merged
  This fixes issue https://github.com/tensorflow/tensorflow/issues/580
 @tensorflow-jenkins: test this please
  Moved some checks out of inner loops
Split the mapper in 2: a base mapper, and a sub-mapper. This reduces the number of variables that are contained in the base mapper and helps reduce register spills
 @tensorflow-jenkins: test this please
   can you squash your commits?
 @tensorflow-jenkins: test this please.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Can you also fix it here: https://github.com/tensorflow/tensorflow/blob/2e7ea3d21be1284ae6470e2815b2664ef22bb884/tensorflow/python/framework/ops.py#L146

and then squash the commits?
 Merged
  As of current HEAD, you now have to upgrade to bazel 0.1.4.  Let us know if you still have problems after upgrading.
 Yeah, we updated here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#install-bazel 

but we haven't updated the website from that.  @martinwicke 
 Done.

On Sun, Jan 24, 2016 at 10:10 PM Vijay Vasudevan notifications@github.com
wrote:

> Yeah, we updated here:
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#install-bazel
> 
> but we haven't updated the website from that. @martinwicke
> https://github.com/martinwicke
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/843#issuecomment-174413099
> .
  This would be really cool to add! It's not on my roadmap for this quarter, but if anyone wants to implement it I'd be happy to discuss design and guide through the process.
 @shendiaomo - sorry, still not on my near or medium term agenda :)
  py_func isn't public yet, which is why it is not available.  Once it's ready, we'll import it, add documentation, etc.  We may additionally initially import it into an 'unsupported' module while it is being worked on.
 Actually, it looks like it is public (according to our documentation), but only at HEAD, not 0.6.0.
 Hi, aadih,

I just verified the code from HEAD should work. I did the following:
$ git clone --recurse-submodules https://github.com/tensorflow/tensorflow
$ bazel test -c opt //tensorflow/python:py_func_test
the test passes.

So, it's most likely your test code or your build environment is somehow different from ours. If you paste the exact error you see, we may be able to help you diagnose.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks for catching this.
You've commented some things out in that cell that shouldn't be.
Also, your commit results in a huge diff that's hard to verify (I know that iPython Notebooks are a pain in that way). Can you try to make the diff minimal? I'm also happy to commit it on my end if the diff is only 'if not d.startswith(".")]'
 This patch is made obsolete by:
https://github.com/tensorflow/tensorflow/pull/860
Thanks!
  ping for @dsmilkov or @danmane 
 Looks good to me :+1: Rebase please
 I'll rebase and merge.
 I take that back, someone more qualified please do the rebase.
 merged. Thanks!
  It's common for a non-optimized network to run slower on GPU than on CPU. For instance, CPU has 5x smaller kernel launch overhead, so models with lots of small operations will run faster on CPU. Stack-overflow is a good place for these kinds of questions (ie, I could elaborate on debugging/profiling technique there). In the particular case, it seems that the data is stored at a 600MB constant node on CPU, so you have a 600MB CPU->GPU transfer at every step
 Actually, this has an easy fix, put "input_images", "input_labels" block into "with tf.device("/cpu:0"):" block. The problem was that constant nodes were placed on GPU, but the ops that come after it are only supported on CPU, hence the data transfers
 I have this fix in a pending CL, this issue should get closed when it's pushed
 Fixed by  https://github.com/tensorflow/tensorflow/commit/ebe109b76d3a8fbdfadee8fa1e6e642563c88034
 After the patch I get 30ms per step on my CPU (HP Z420) and 140ms per step on GPU (K40c). Note that the patch only pins 2 constant ops to the CPU. It looks like there's a "Cast" op which is part of input pipeline that ends up on GPU, so that slows things down a lot. In general I found it best to pin the whole input pipeline to CPU manually.

However, even if you fixed this, this network is too small to benefit much from GPU. Using CPU-only version, the "QueueDeqeueMany" op takes 25ms out of 30ms total, so you have 5 ms spent on "actual computation", so even if you made the computation run 5x faster, it would have negligible effect on step size.

third_party/tensorflow/models/image/mnist/convolutional is much better example for utilizing GPU, I get 8x speed-up when using GPU. Also, cifar10_train example has a better tuned input pipeline

Regarding other questions:
if you turn on verbose logging in bazel, you'll see some messages every time tensors get copied between devices here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L41

One way to find if an op has GPU implementation is to search for "REGISTER_KERNEL_BUILDER" macros, like here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L217
  I was able to get 68x68 images reading/decoding from TF-examples fast enough to saturate my K40 with input pipeline using 6 threads/1 CPU. These tutorials have not been tuned to work efficiently on GPU, so there could be some small ops that are placed on GPU suboptimally and are causing unnecessary data transfers -- try pinning your input pipeline to CPU manually. See #838 for an example of suboptimal placement making GPU version run 100x slower
 Re: "Pinning the pipeline to the CPU":

here's how I would optimize it -- pin everything to CPU (export
TF_MIN_GPU_MULTIPROCESSOR_COUNT=800), and remove all the non-reading ops.
Tweak your pipeline design/number of threads until you get maximum
throughput. Then re-enable GPU, and use manual pinning to make sure that
your input pipeline throughput is unchanged. Then attach your processing
ops (on GPU)

On Fri, Jan 22, 2016 at 2:59 PM, nryant notifications@github.com wrote:

> Pinning the pipeline to the CPU helps somewhat, but still is worse than I
> would expect. For num_threads=2 the time reduces to 12.6 seconds with 13%
> GPU utilization and for num_threads=16 to 9.4 seconds with 18% utilization
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/837#issuecomment-174081084
> .
 oops, off-by-1 error on my part.
When you are using a reader, there's more work done at the beginning because of prefetching, could it be that the extra time is due to it filling up a queue of examples?
 @ebrevdo: Could you take a look since it's queue related? 
  Do you know why your protobuf is so big? Is it storing a very large GraphDef? Also, can you share the events file that is causing this issue?
 I'm not sure about RNN usage, sorry. (Maybe @martinwicke can advise or point in the right direction.)

It would be helpful to find out what message is so large that it's violating the protobuf message size limit - that would let us determine whether we should increase the message size limit, or if we should find a way to prevent such large messages from getting generated in the first place. Can you dig into the protobuf and find what it is, or send us the event file?
  `optimizer.minimize(loss, var_list=[your variables])` does this if I understand correctly.

Also, take a look at: http://stackoverflow.com/questions/34477889/holding-variables-constant-during-optimizer/34478044#34478044
 @zackchase: in the future I would use StackOverflow for these types of questions.
  For reshape, you can have one unknown dimension if you set it to -1.  For conv2d_transpose, I'm going to defer to Geoffrey.
 conv2d_transpose does need to know the exact shape, but that shape can be computed with tensorflow ops at runtime.  Presumably you have some variable whose first dimension is batch size, so you can do something like

```
batch_size = tf.shape(something_or_other)[0]
deconv_shape = tf.pack([batch_size, 40, 40, 32])
conv2d_transpose(..., output_shape=deconv_shape, ...)
```
 @bsautermeister Yes, that's expected behavior.  We currently don't do constant folding through partially known tensors such as the `tf.pack` here.  Hopefully that will be fixed at some point in the future.
 @amolchanov86 The proposed solution works fine for stacked filters, since it has no dependence on static shapes.  If you want assistance getting your model working, please ask questions on StackOverflow.
  Geoffrey is our Python 3 expert, perhaps he can help.
 Please split this issue into two: the GPU issue is not related to the Python 3 issue if it also happens on Python 2.
 For the Python 3 error, I'm trying to find the relevant function now, but I may need some way of reproducing the problem unless something obvious arises.
 Yeah, I think this is some sort of horrible swig issue, so I'll need a way to reproduce.  Can you make a minimal test case that causes the exception?
 Thanks!  I'll take a look.
 Apologies for taking forever to get to this.  Unfortunately, I am unable to reproduce the problem: even on Python 3, I hit the exception every time (out of 1101 test runs).
 I can reproduce it. In fact, it's a pervasive problem whenever you run with python3.5. See #1033.
 The culprit seems to be this: https://bugs.python.org/issue23571
 I think we're suffering from this: https://github.com/swig/swig/issues/567, and the solution may be this: https://github.com/swig/swig/pull/560
 I'll try installing a fresh swig.
 @martinwicke: Was this fixed? 
 Yes. For posterity: Upgrade to swig >=3.0.8.
  Hm....can't reproduce this at head. Closing/reopening the session doesn't seem to affect the list of trainable variables. Can you check at which point you are losing the trainable variables? That's the list that optimizer uses to identify the variables implicitly --
[v.name for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]
 I think we fixed this some time in December but after our last 0.6.0 release.  We're hoping to release a newer version soon, but it should also be fixed at HEAD if you have the ability / patience to install from sources.

Feel free to re-open if it's still broken at HEAD.
  Jenkins, test this please.
 Our CI is currently broken, will pick this back up when we fix it. Looks good to me though. Can you squash the commits?
 @martinwicke: the merge commits are dropped when we rebase, so this is actually fine as is.
 @tensorflow-jenkins, test this please

@cesarsalgado: shouldn't matter as long as there are no conflicts with the base branch :)
 Merged.
  Which version are you running? I would have expected this to be fixed by https://github.com/tensorflow/tensorflow/commit/bc624aa8d9460dca794fde6d5534f1d3e8054016, which avoids adding control dependencies to optimizers' slots. However that didn't make it into the 0.6.0 release, so you may need to build from source (or patch the appropriate lines in `optimizer.py`).
  np.diag can

1) take 2d array and produce 1d vector of diagonal entries
2) take 1d array and produce diagonal 2d array (inverse of case 1)

tf.diag only supports 2) and the behavior for case 1) is undocumented

Currently to do 1) you need to do something like this:

```
def identity_matrix(n):
  """Returns nxn identity matrix."""
  # note, if n is a constant node, this assert node won't be executed,
  # this error will be caught during shape analysis 
  assert_op = tf.Assert(tf.greater(n, 0), ["Matrix size must be positive"])
  with tf.control_dependencies([assert_op]):
    ones = tf.fill(n, 1)
    diag = tf.diag(ones)
  return diag

def extract_diagonal(tensor):
  """Extract diagonal of a square matrix."""

  shape = tf.shape(tensor)
  n = shape[0]
  assert_op = tf.Assert(tf.equal(shape[0], shape[1]), ["Can't get diagonal of "
                                                       "a non-square matrix"])

  with tf.control_dependencies([assert_op]):
  return tf.reduce_sum(tf.mul(tensor, identity_matrix(n)), [0])
```
 I believe tf.diag is only intended to support 2).  It would be reasonable to create a new op for 1).
 Yeah, having the same op arbitrarily be its own one-sided inverse seems pretty strange.  Two ops sound better to me.
 Besides numpy, this also how it's done in Theano, Torch, Matlab, ViennaCL
 In my defense, that's four different things copying Matlab's arguably sketchy choice, but I see your point.  I'm okay if we bow to peer pressure.
 Yep, closing.
 @chemelnucfin: For future use, you can include a line "Fixes #824" in the commit message in order to automatically close the issue when it's merged. 
  That could very well be a bug.  Sherry, could you look at this?
 HI there,

Sorry about the delay.

What did you mean by "they didn't save/restore correctly for me"?

We have introduced variables_to_restore() (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L327) to return a map from the moving_average_names to the variables mappings, such that you can restore the moving_average values into the corresponding variables. Would that do what you want? If not, would you please elaborate what you would like to restore?

Thanks,
Sherry
 @martinwicke, @aselle: I think we may need backup czars, since Sherry is out at the moment. 
  We're using a command line, not the green button, and it inconsistently reports some as merged and some as closed, using the same set of command lines on different PRs.  This seems like a github bug to me, unless someone can tell me what github uses to tell whether something is merged.
 It's really infuriating -- any advice would be appreciated. I'd much prefer
if we could have reliable statistics on merged vs. closed, but I also want
the power our scripts give us.

On Wed, Jan 20, 2016 at 1:20 PM Vijay Vasudevan notifications@github.com
wrote:

> We're using a command line, not the green button, and it inconsistently
> reports some as merged and some as closed, using the same set of command
> lines on different PRs. This seems like a github bug to me, unless someone
> can tell me what github uses to tell whether something is merged.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173362529
> .
 Interestingly, all the PRs merged via script have that message (Closes #XXX). Sometimes they show up closes and sometimes merged. May be a race condition what process first does the close/merge. I'll remove the "Closes ..." from the message to see if that fixes it.
 Then the PR sometimes remains open. Give it a shot though
 I just merged #826 without the "Closes #826" part and that seemed to work.
I hope we don't get PRs staying open this way again.

On Wed, Jan 20, 2016 at 4:14 PM bhack notifications@github.com wrote:

> I repeat I don't know you exact git command sequence execution but @vrv
> https://github.com/vrv declared that the sequence was always the same.
> So I suspected the race condition.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173409660
> .
 Thanks! That looks useful.

On Thu, Jan 21, 2016 at 2:46 AM bhack notifications@github.com wrote:

> @martinwicke https://github.com/martinwicke If you are interested this
> is an opensource customizable little Kanban style project board for Github
> Tensorflow https://waffle.io/tensorflow/tensorflow
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173534271
> .
 Yeah, I was using a stale script, I've updated since then.  I'm about to
merge another change so we'll see if that removes the race.

On Thu, Jan 21, 2016 at 11:36 AM, bhack notifications@github.com wrote:

> This has worked but have the "Closes" syntax in the commit message: #832
> https://github.com/tensorflow/tensorflow/pull/832
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173684773
> .
 https://github.com/tensorflow/tensorflow/commit/b11337ea5719a8799f279a41c2cd5c9e75a8acf7

Didn't merge the PR.  Sigh.
 My guess is that because we rebase, it doesn't think we merged the PR.  When the existing PR is already synced to current HEAD, it will say "Merged".  I don't see a way around this -- basically github expects you to click their merge button.  I'd rather have the PRs automatically closed: at least their closing statement mentions the commit it was in.
 Agreed. We'll have this script do more things in the future (like
squashing, upon request, and testing), so the merge button just won't cut
it. Sad for record-keeping, but otherwise shouldn't be a big problem.

On Thu, Jan 21, 2016 at 11:46 AM Vijay Vasudevan notifications@github.com
wrote:

> My guess is that because we rebase, it doesn't think we merged the PR.
> When the existing PR is already synced to current HEAD, it will say
> "Merged". I don't see a way around this -- basically github expects you to
> click their merge button. I'd rather have the PRs automatically closed: at
> least their closing statement mentions the commit it was in.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/822#issuecomment-173687973
> .
 We do ask contributors to squash, but if we require our clients to rebase, every time we accept another PR we'll have to ask the client to re-rebase :).  Since we own the merging process, it makes more sense for us to do the rebasing.
 Yup, the section under "Catch Feature Up with Master by Rebasing, then merge --no-ff" is exactly what our script is doing.  The point is that we're the ones doing that, rather than the author.

It seems the best solution would be for github to have special magic like "Closes #PR" but for "Merges #PR".
 Thanks for finding that!  Doesn't lend confidence in that getting implemented any time soon though. :/
  Is it something that was fixed recently? I only see "limit_epochs" as a variable made by string_input_producer, and it's regular, not trainable variable -- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input.py#L76
  That's mysterious. The file is clearly in the repo ([here](https://github.com/tensorflow/tensorflow/blob/master/util/python/python_config.sh)). Can you make sure that you checked everything out properly?
 Closing due to staleness / lack of reproducibility.  Please request to re-open if it's still a problem.
  LGTM
  Can you try 

```
bazel build -c opt config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/cc:tutorials_example_trainer
```

and if that doesn't work, post the entire logs?
 @lberki @davidxchen @damienmg
 Try adding to https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L89
  For step by step instructions, stackoverflow is probably the better forum.
But if the board runs Ubuntu, you should be able to follow the regular
installation instructions.
On Tue, Jan 19, 2016 at 20:41 david wang wei notifications@github.com
wrote:

> I have one board called udoo neo (from www.udoo.org) ,which run ubuntu
> system .
> I want to port tensorflow on it. How to do it step by step?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/816.
  merged.
  Improved the performance of the contrast adjustment code by enabling nvcc to optimize the broadcast of scalars
 @tensorflow-jenkins, test this please
 I think @martinwicke was playing around with jenkins, giving it another shot:

@tensorflow-jenkins, test this please
 @tensorflow-jenkins: test this please
 LG, merging.
  You can achieve what you're looking for by padding your input with zeros and then convolving the padded tensor with ones. For example, if you have a vector of N elements, you can start by padding it with N-1 elements on the left, and the launch a 1D convolution with a kernel of size N.

Of course that won't be very efficient, but that can get you going for the time being.
 If we do this, it should probably have a name pattern similar to `reduce_sum`, `reduce_prod`, etc.  Maybe `accumulate_sum`, `accumulate_prod` ([somewhat following numpy](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ufunc.accumulate.html))?  @dave-andersen: Do you have opinions here?

Also, I'm not sure if it should be inclusive or exclusive.

Once we figure that out: yes, the op definitions would go in `core/ops/math_ops.cc`, and the kernel files would have a similar pattern to the reduction ops in `core/kernels`.
 @benoitsteiner: Does eigen have accumulate (scan) capability? 
 @ibab @girving Eigen currently doesn't provide the functionality needed to efficiently implement an  accumulate op. You're more than welcome to contribute the code to Eigen, that would be a very useful extension.
 @ibab The TensorReduction.h code is very optimized and therefore pretty complex. A simpler operation that you could use as a template is TensorReverse.h (which reverse the order of the coefficients along one or more dimensions) or TensorShuffling.h (which reorders the dimension - this is a transposition in the 2d case)
 @benoitsteiner: How does the merging / syncing process work with external Eigen changes?  Do we have to wait for any period of time before using them in TensorFlow?
 @girving We can update the bazel build and workspace files to pull the latest version of Eigen any time. I typically wait for about one day before doing so though, since that gives me a change to check that the Eigen nightly regressions tests are clean before updating TensorFlow.
 The product version of this would be useful to fix https://github.com/tensorflow/tensorflow/issues/2641.
  You can already retrieve the weights easily with variable scope, e.g.:

```
cell = RNNCell(...)
with tf.variable_scope("RNN") as vs:
  # use cell here.
  for var in tf.trainable_variables():
    if var.name.startswith(vs.name):
      print var.name, var.get_shape()
      tf.histogram_summary(var.name, var)
```
 If you just need histogram summaries for all the variables, then you don't need to use it in any specific variable scopes:

```
for var in tf.trainable_variables():
  tf.histogram_summary(var.name, var)
```
 Yes, I understand. For common use cases this doesn't seem to be needed .. and you can always use variable scope to get specific variables, like: `rnn_matrix = tf.get_variable(vs.name + "/Matrix")` but you need to be in the proper scope and know the names.

With the current design, the rnn cells are not associated with any specific variables (and are stateless) unless in a specific variable scope so having properties to keep references to them sounds hacky as well.
  I'm not sure whether it is supposed to be public or not.
 It is not supposed to be public, and it will likely be deprecated (and
removed) in favor of layers.fully_connected_relu once that's matured.

I agree using unstable functions in the tutorials is suboptimal, and since
relu_layer is so simple, a simplified version of it should probably be
inlined in cifar.

On Tue, Jan 19, 2016 at 11:44 AM josh11b notifications@github.com wrote:

> I'm not sure whether it is supposed to be public or not.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/811#issuecomment-172964027
> .
 Nobody is working on it, if you wrote a pull request, we'd be grateful for it.
 Looks like it was fixed in #828 
  1) You might go back to #802 and checking out the code at the 0.6.0 branch if you have installed the pip package at 0.6.0.

2) As the instructions say, you can't import tensorflow when running in the root of the source tree.  Go to another directory and you should be able to import it.

Let us know if you have any problems and we'll reopen this ticket.
  Jenkins, test this please.
 Geoffrey, assigning this to you since you last touched this line.  It looks good to me, though.
 @tensorflow-jenkins: test this please.

( @josh11b, you don't have access to kick off jenkins.  @martinwicke).
 I don't have a reference in mind.  By the standard way, I mean that if we want to have default value `x`, you do something like

```
def foo(arg=x):
  ...
```

If you want to have the default be immutable, and can choose `x` to be immutable (`()` in this case), the above works fine.  It seems clearly superior to your code, which adds extra lines and doesn't document the behavior in the function signature.  Mine even documents that `x` is not written to, since it's immutable.

In contrast, the example you're pulling from is only for when you really do want a `list` instead of a `tuple`, perhaps because you want mutability.  Here, as far as I know, a `tuple` would be fine.
 Jenkins, test this please.
(Probably still won't work, but I can hope that I've been given permission by now.)
 @tensorflow-jenkins, test this please
  @mcuadros, TF docker has trouble finding your Cuda driver, not the Cuda libraries. Could you check? 
- /usr/lib/x86_64-linux-gnu/libcuda.so does exist on your machine, and in the docerk container? 
- Please make sure that path is on your LD_LIBRARY_PATH as well
- If still having trouble with the docerk container, could you also try without the docker container just to see if that works? 

Thanks. 
 Here are the debugging steps that I recently went through to get TensorFlow working (kind of) with GPU and Docker. (I say "kind of" because there are still some GPU-related bugs in Tensorflow, which caused some test failures and will likely cause some user-code errors in that regard as well). 

See: 
https://github.com/tensorflow/tensorflow/issues/952
https://github.com/tensorflow/tensorflow/issues/953

That said, here are the things you want to check on: 

1) On the host, outside Docker, you have NVIDIA driver installed. If you have it installed, the binaries "nvidia-smi" and "nvidia-debugdump" ought to be available on the host. Make sure that the following two commands list your GPU: 
`nvidia-smi`
`nvidia-debugdump -l`

2) On the host, the output of nvidia-smi tells you the version of the NVIDIA driver installed. It needs to be recent enough for your GPU. For example, on my machine, version 340 doesn't work, but version 352 does. 

3) On the host, get the CUDA sample code and compile the deviceQuery binary:
http://docs.nvidia.com/cuda/cuda-samples/#axzz3z3C3lhk1

For this you'll need to install the CUDA toolkit, which includes the nvcc compiler and supporting libraries. 
https://developer.nvidia.com/cuda-downloads

Once the deviceQuery binary is compiled, try to run it
`./deviceQuery`

If it fails, don't panic, just try
`sudo ./deviceQuery`

There are some file permissions issues related to NVIDIA devices in /dev that requires you to do sudo like the above for each boot cycle, for detailed dicussion, see: 
https://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/

4) After step 3, CUDA GPU is all set on the host. Let's now look inside Docker. Make sure that you install the same CUDA Toolkit as listed in step 3. TensorFlow additionally requires CUDA DNN (cudnn) libraries which you can also get from the CUDA website, after a somewhat time consuming user approval process. 

After this make sure that the following files are present in your docker, as they will be used by (the current version of) TensorFlow:

```
/usr/local/cuda/include/cudnn.h 
/usr/local/cuda/lib64/libcudnn_static.a
/usr/local/cuda/lib64/libcudnn.so.6.5.48 
/usr/local/cuda/lib64/libcudnn.so.6.5/libcudnn.so.6.5.48 
/usr/local/cuda/lib64/libcudnn.so/libcudnn.so.6.5
```

It is okay for NVIDIA driver to be unavailable inside the Docker, even though the CUDA Toolkit is required inside it.

5) Before you can try to start your Docker container, make sure you use docker flags to map a few devices so the the NVIDIA devices are available under /dev, inside the container:

```
"--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl"
```

Also, map a bunch of lib files, to make sure that cuda library files are visible inside the container

```
"-v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a" "-v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so" "-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5" "-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22" "-v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a" "-v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so" "-v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1" "-v /usr/lib/x86_64-linux-gnu/libcuda.so.352.63:/usr/lib/x86_64-linux-gnu/libcuda.so.352.63"
```

Now the NVIDIA docker container should be ready to run TensorFlow with GPU. 
 @zheng-xq, any ideas here?
 also pinging @ebrevdo who may have hit this more recently.
 You have cuda_path set, maybe it should be CUDA_HOME?
 Also add this to ld library path:

/usr/lib/x86_64-linux-gnu
 For the open-source bulid, I don't think the third_party/gpus path matters
at all. libcuda.so needs to be found at the system path.

Looking at
"tensorflow/tools/docker/docker_run_gpu.sh","/usr/lib/x86_64-linux-gnu/libcuda"
needs to be changed to the libcuda.so on the local system.

On Thu, Feb 4, 2016 at 9:38 PM, Craig Citro notifications@github.com
wrote:

> also pinging @ebrevdo https://github.com/ebrevdo who may have hit this
> more recently.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/808#issuecomment-180207303
> .
 @zheng-xq, @ebrevdo: What's the status of this?  
 If you have a problem try the [nvidia-docker](https://github.com/NVIDIA/nvidia-docker). It is in much better shape than i used to be half a year ago.

I believe we can close this. I'm going to update the documentation to use nvidia-docker.
 Per comments from @jendap, closing this issue. 
  Jenkins, test this please.
 @tensorflow-jenkins: test this please
 @ebrevdo: I assume adding 'scope' is the right change to the API, can you confirm?
 Can you justify the need for passing scope to the **call** interface?
 Ah sorry, I see now.  Changes LGTM.
 Sometimes it appears as merged, sometimes closed.  I'm not sure what I have to do to tell github it's been merged.  Help would be appreciated
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (We are waiting for the CLA before proceeding.)
 CLAs look good, thanks!

<!-- ok -->
 There are 2 diffs in that file, but I can't see what has changed in the second.  Is it a whitespace-only change?
 LGTM
 merged.
  As currently written, the op expects inputs of 4 dimensions.  https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/batch_norm_op.h

But it doesn't seem to need to -- it just flattens everything but the last dimension anyway.  We would need to update the kernels and then the shape function, but it seems possible.

@sherrym, @shlens in case they want to comment.
 Many ops still have historical and/or arbitrary rank restrictions. Wherever knowledge of the rank is not needed, they should be generalized. 
 Note that batch normalization is a different operation depending on whether the layer is convolutional or not: when using it on a convolutional layer, you can aggregate sufficient statistics across patches. Not so when the layer is fully connected. Reshaping to [-1, 1, 1, depth] is the correct thing to do though, and we should probably just do that.
 We'd be happy to accept PRs if anyone wants to generalize batch normalization to arbitrary rank.
 This is obsolete. `nn.batch_norm_with_global_normalization` has been replaced by `nn.batch_normalization` which accepts arbitrary geometries.
  Dupe of https://github.com/tensorflow/tensorflow/issues/802
  Sherry, this looks related to 110808120 or 110781462.
 If you're using the 0.6.0 pip installation, you should check out the code at the 0.6.0 branch.  There's no guarantee that the scripts at HEAD will work with the old binaries, since we add features to HEAD and then update our examples.
 This should have long been fixed now that both the code and pip package containing this change has been available for several months.
  @tensorflow-jenkins: test this please
 Merged
  (this type of question belongs on StackOverflow, github is for bugs / feature requests, etc).
  - I believe many of the functions in tensor_util are actually supposed to be private, so I probably would try to avoid using any function that's not currently publicly documented.
- In your code snippet, 'a' is really a symbolic tensor indicating that it will produce a random 3x3 matrix when evaluated in the context of a session, so to actually get its content, you need to 'eval' it, which will return a numpy array:

```
with tf.Session():
  a = tf.random_uniform((3, 3))
  b = a.eval()  # Runs to get the output of 'a' and converts it to a numpy array
```
 Also, this type of question is better suited for StackOverflow -- you'll get better feedback / advice there.  Try to use github issues for bug reports / installation issues / feature requests.
  @tensorflow-jenkins: test this please
  You don't have permission to create a directory in your current working directory. We won't attempt to change that programmatically for you since it might have unintended consequences.
 For now, I recommend cloning the git repository into another directory and running that version of the file.  I think we'd have to rewrite our tutorials and examples to not assume relative path otherwise.
  http://bazel.io/docs/install.html for installing bazel
  Not that I know of, but we haven't done any heavy testing. (Unless @ebrevdo has done some?) 

In principle, the GPU device is directly exposed to the docker container, so if there is a difference, it should be negligible. If you try it and find otherwise, @roschler, we'd definitely like to know!
  @tensorflow-jenkins: test this please
 @tensorflow-jenkins, test this please.
 (apparently our CI is down -- we'll look at this again when it's back up, sorry).
 @tensorflow-jenkins, test this please
 Thanks, this is great!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (looks like it hasn't registered -- did you verify that the commit email matches?)
 Options
- You can sign the CLA using the email address that you use in your git commits
- You can git commit --amend your change to use the email address you already registered the CLA with and then repush / resend the pull request.
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please
 ```
==================== Test output for //tensorflow/models/rnn/ptb:reader_test:
.E.
======================================================================
ERROR: testPtbRawData (__main__.PtbReaderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader_test.py", line 49, in testPtbRawData
    output = reader.ptb_raw_data(tmpdir)
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py", line 83, in ptb_raw_data
    word_to_id = _build_vocab(train_path)
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py", line 47, in _build_vocab
    key=lambda word, count: (-count, word))
TypeError: <lambda>() takes exactly 2 arguments (1 given)
```

Looks like this doesn't work for python 2 ...
 Wrap word, count in parens.  It should work.
On Jan 18, 2016 8:23 PM, "Vijay Vasudevan" notifications@github.com wrote:

> ==================== Test output for //tensorflow/models/rnn/ptb:reader_test:
> 
> # .E.
> 
> ## ERROR: testPtbRawData (**main**.PtbReaderTest)
> 
> Traceback (most recent call last):
>   File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader_test.py", line 49, in testPtbRawData
>     output = reader.ptb_raw_data(tmpdir)
>   File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py", line 83, in ptb_raw_data
>     word_to_id = _build_vocab(train_path)
>   File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/models/rnn/ptb/reader_test.runfiles/tensorflow/models/rnn/ptb/reader.py", line 47, in _build_vocab
>     key=lambda word, count: (-count, word))
> TypeError: <lambda>() takes exactly 2 arguments (1 given)
> 
> Looks like this doesn't work for python 2 ...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/792#issuecomment-172733176
> .
 That's what we had before, so not sure why it's not python 3 compatible, would have to look into it.
 Jenkins, test this please.
 Yeah it's known and unrelated, let me disable that for now.
 Thanks, can you squash your commits?  I'll merge once that's done.
 @brchiu: only admins can kick off tests right now.  I'll test and merge tomorrow morning.
  Some python targets are missing the srcs_version="PY2AND3" annotation in the BUILD file. We're working on it. In the meantime, any python target in python/BUILD without this annotation will fail when compiled with python 3.
  Looks good to me. Thanks!
 @tensorflow-jenkins: test this please.
 merged.
  @tensorflow-jenkins, test this please.
 Merged
  I feel like a memory leak would likely occur much earlier than 100,000 iterations -- the logs should produce a dump of the OOM information, which could help quickly identify whether there's an obvious memory leak.

Most likely it's just that you're already operating pretty close to the memory capacity of the device and that the order of node execution caused the system to allocate memory in a suboptimal order.

As mentioned on the [roadmap](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/roadmap.md), we're working on improving the memory consumption and reporting, so stay tuned.
 I don't think there's enough information to diagnose here.  Feel free to reopen if more details emerge.
  I'm not sure this isn't the desired behavior.  Returning `None` makes it explicit that there is no graph connection between the two.
 Yep, I agree that this is a bug; my comment was poorly worded.  @mrry: Do we use the `None` feature anywhere that changing this would break?  It could certainly cause some code to get slower, which is a potential concern.
 I'm pretty sure this is intended behavior. [`Optimizer.apply_gradients()`](https://github.com/tensorflow/tensorflow/blob/b159e211860e26e3dbae26989163b9da55e3f430/tensorflow/python/training/optimizer.py#L239) uses the fact that individual gradients may be `None` to remove assignment ops. While this would be an easy case for a graph rewriter, it's not something we currently do, so I'd be a little worried about regressions if various variables suddenly started being updated every time.

Since `tf.gradients()` is mostly hidden behind the optimizers, and the behavior isn't currently documented, we could consider a breaking change to the API, but I think we'd want to retain the existing functionality. Perhaps it could be controlled by an optional arg?
 It looks like there are hundreds of direct uses of `tf.gradients` within Google, so I don't think a silent performance breaking change is okay.  If we're going to change the default behavior, I think the only way would be to make the special `Zeros` class and give it suitable arithmetic overloads.  That way, anyone who doesn't realize and treats it as a normal tensor in a way that doesn't take advantage of zeros will throw an exception.

For now, how about a `return_zeros` argument that defaults to `False`?
 Writing this now.
 Actually, there's a wrinkle: `None` is used to indicate a variety of different things:
1. There is no connection from input to output.
2. There is a connection, but it's through a discrete variable with meaningless gradients.
3. There is a connection, but it's through an op that doesn't have an implemented gradient.

(3) in particular would be very bad to replace with zeros.
 That's sounds reasonable, but unfortunately it would require adjusting the gradient functions, since individual gradient functions also use `None` for both (1) and (2).

@mrry: What do the think about the `Zeros` solution? 
  There are a lot of unrelated commits in this PR -- can you separate them out into individual PRs?  Specifically, fixing the typos and such can be done as a separate PR, and then the main GPU impl + test can be in a single commit for reviewing.
 Quite possibly adding this adds a bunch of transfers back and forth, which
are slow. Optimizing the example for GPU would fix that.

Martin
On Fri, Jan 15, 2016 at 18:52 Frank Chu notifications@github.com wrote:

> Btw, with this change, I could re-enable word2vec example GPU support:
> fpmchu@121784b
> https://github.com/fpmchu/tensorflow/commit/121784b62da64f3a799cc0eb6776ca4f35594a9f
> 
> However, on my machine, this made the example _slower_. Either it's
> because my card is weak, or perhaps the example needs to be optimized for
> using GPU? I'm not sure if it's my code that's slow...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/782#issuecomment-172149174
> .
 @fpmchu, I apologize, I should have checked Thrust's documentation before sending you on a wild goose chase. But you are right, since we have committed to this in the documentation, it'll be nice to support repeated indices. My belief is, the performance may not be terrible if you turned the `+=` to `atomicAdd`. Do you mind giving it a shot and running the benchmarks?
 fpmchu@, thanks for doing this, this is awesome! Almost there, we'll merge this soon.
 - Changing the documentation for `scatter_update` to note that one of the values corresponding to the duplicate indices will be written at that index.
- The compiler looks for function overloads before it looks for full specializations, so I usually write overloads. But in this case, both function overload and full specialization should behave the same, so I am leaving it up to you.
 Jenkins, test this please.
 Can you smash all of these commits into one large commit?
 Thanks, looking through this now.
 Can you do some merging of the registration macros in `scatter_op.cc`?  Certainly add and sub always have the same set of types, and for gpu it looks like everything has the same set of types.  Taking advantage of this would dramatically shorten that section and make it easier to read and modify.  For example, there would be only one `#if GOOGLE_CUDA` line in `scatter_op.cc`.
 I'm likely being a bit dense here, but what file defines the GPU implementations?  I can't find anything in your new single CL.

Once I do find it: we may need to be more specific about what happens for `ScatterUpdate` on larger types.  Currently you guarantee that one of the indices is used (which admittedly is what I said to do), but that's not remotely actually guaranteed for either indices which refer to slices or even to `complex128` on CPU if we were to switch to threads.
 Also, not sure if `complex64` is an issue yet, but once it is we'll want a primitive which is slightly weaker than atomic add / sub, since it's fine to add both components at unrelated points in the sequence.
 @vrv, @martinwicke, @keveman, @girving, the GPU portion of the code LGTM. 

Please feel free to move forward if you think other parts are good as well.
 Jenkins, test this please.
 Jenkins, test this please.
 Jenkins, test this please
 Hold on, this may be a tool failure.
 Jenkins, test this please.
 Jenkins, test this please.
  I'm going to close this in favor of #781
  @tensorflow-jenkins, test this please.
 Yeah it's fine. I'll merge tomorrow
 Merged
  @digitalsword: It sounds like this is a bazel issue rather than a TensorFlow issue if you can't get bazel to compile using the right gcc.  Sorry to redirect, but I don't think we'll be able to fix it on our end. 
  with model_checkpoint_path containing absolute path or path relative
to save_dir.

Fixes #751.
 @tensorflow-jenkins, test this please.
  This assertion comes from Eigen. Assigning Benoit to take a look. It is helpful if Eigen prints out the failed status. 
 Eigen will not print error messages to stderr when it fails to initialize the CUDA runtime (see https://bitbucket.org/eigen/eigen/commits/8bfe1ed866fbd09954d65a644584ce5d1f741aa7 for details). If you pull the latest version of TensorFlow from github and return your model you should get some information to help pinpoint the root cause of the issue.
 I'm going to close this. Please reopen if the problem persists and more diagnostic information is available.
  Thanks for the report, I've pushed a fix.
  This is a question better suited for Stack Overflow, since this isn't a bug / new feature request.
  1) did you run ./configure ?

2) Can you try a newer version of bazel?  I think 0.1.1 is too old now.
 Does somebody want to send us a PR for that?
 Thanks @frankyjuang !  We just merged it -- let us know if the problems are still present by commenting and we'll reopen.
 Bazel tried to run 2to3 if it sees a python file that doesn't have the srcs_version = "PY2AND3" annotation. As it turns out that's the case for functional_ops.py (and a couple of other targets in python/BUILD.

Can you add the annotation to those targets and see if that works for you?
  https://www.tensorflow.org/versions/master/get_started/os_setup.html#optional-install-cuda-gpus-on-linux

Most of the information is there, including ./configure and bazel, if building from sources.

In your case you probably need to pass --spawn_strategy=standalone  to the bazel command line flag -- I guess we should add that to our tutorials but that seems like a temporary bazel bug that I was hoping would get resolved soon...
 @lberki: do you think we should just add --spawn_strategy=standalone to our build instructions? Is that likely to be unnecessary in the future?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Thanks!
  The TensorFlow tutorials cannot replace a deep learning or machine learning textbook or course. We may add pointers to additional resources as those become available.
  Added generate_checkpoint_state_proto() which returns a checkpoint proto with model_checkpoint_path containing absolute path or path relative to save_dir.

Added unit tests.

Fixes #751.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I signed it!
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please.
 LG, can you squash the commits?  Then I'll merge.
  LGTM
 I think it would be good to have a test for this -- right now if we removed it, no tests would fail.
 Would this test fail if the dtype of the cell is float64?

On Mon, Jan 18, 2016 at 8:51 AM, Fabrizio Milo notifications@github.com
wrote:

> @vrv https://github.com/vrv Added the option to the existing test,
> seemed useless to add a complete new test just for that option
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/767#issuecomment-172584885
> .
 Closing due to lack of activity.
 An answer to https://github.com/tensorflow/tensorflow/pull/767#issuecomment-172597373 :)
 This change looks good to me, I think it will work as said. Can we test it and get it in?
 Ok, well then let's address the conflict / squash, then we'll test and merge.
 @rafaljozefowicz 
 > Would there ever be a foreseeable instance where you would not want to have that be the case?

I think 1.0 is the best default value for a variety of tasks. But it's hard to be sure that there is no better starting bias on some particular task. And some people want to faithfully replicate previous work that used bias=0. So it's definitely good to have this option!
 Okay, rebase / address the conflict and we'll test / merge.
 @tensorflow-jenkins: test this please
 @tensorflow-jenkins: test this please
  1) I would not modify the source code like that: try following the instructions here https://www.tensorflow.org/versions/master/get_started/os_setup.html#optional-install-cuda-gpus-on-linux to enable other compute capabilities.

2) The error is:

```
Indices are not valid: not lexicographically sorted or containing repeats.
     [[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT32, valid
```

Make sure to read the documentation of SparseToDense to see what contract your code is violating.

Feel free to comment again if you think there's an actual bug or installation problem and I'll re-open. Otherwise, StackOverflow might be a better place for this type of question.
  Jenkins, test this please.
 I'll merge once you add the comment, and [squash the commits](rebaseandsqua.sh) (or amend the existing one).
 I take that back, your other problem may well be related. 

The #if thing must not be in the header (since we will not know about NUMPY_IMPORT_ARRAY_RETVAL until after numpy/arrayobject.h has been included). Move it to the c file, below the includes, and you should be fine.
 Jenkins, test this please.
 Fortunately, the Jenkins build produced a decent enough error message -- can you see if that's related to your failure or even the same?
 Error is enlightening: 

tensorflow/python/lib/core/py_func.cc:29:31: error: operator '==' has no left operand
 #if NUMPY_IMPORT_ARRAY_RETVAL == NULL

Checking for empty defines is nasty business, look at this: http://stackoverflow.com/questions/3781520/how-to-test-if-preprocessor-symbol-is-defined-but-has-no-value
 Can you fix it in tf_session the same way it's fixed in base.i? I think that's the right thing to do. Might as well make this PR "Fix Python 3 compilation issues" since we're almost there already.
 Jenkins, test this please.
 Just making another log for myself. :)
 The Mac build is broken right now, unrelated to this. :/ 

It's strange because the compiler shouldn't even see the PyString_ version any more. Can you remove the ifdef altogether (and keep only the Py3 branch) and see if that works compiling with Python3? Those two are the only occurrences of this function, so I feel like for some reason the ifdef doesn't work as expected (maybe because it's evaluated by swig, and we need a %{ %} around it).
 I see that this list conspicuously does not include
`PyString_FromStringAndSize`.

Maybe this is a swig problem? What version of swig is this, and does
python3 maybe need a newer version of it?

On Wed, Jan 13, 2016 at 3:08 PM João Felipe Santos notifications@github.com
wrote:

> To add to the weirdness of this issue: something (maybe SWIG?) adds the
> following macros to pywrap_tensorflow.cc:
> 
> #if PY_VERSION_HEX >= 0x03000000
> 
> #define PyClass_Check(obj) PyObject_IsInstance(obj, (PyObject *)&PyType_Type)
> #define PyInt_Check(x) PyLong_Check(x)
> #define PyInt_AsLong(x) PyLong_AsLong(x)
> #define PyInt_FromLong(x) PyLong_FromLong(x)
> #define PyInt_FromSize_t(x) PyLong_FromSize_t(x)
> #define PyString_Check(name) PyBytes_Check(name)
> #define PyString_FromString(x) PyUnicode_FromString(x)
> #define PyString_Format(fmt, args)  PyUnicode_Format(fmt, args)
> #define PyString_AsString(str) PyBytes_AsString(str)
> #define PyString_Size(str) PyBytes_Size(str)
> #define PyString_InternFromString(key) PyUnicode_InternFromString(key)
> #define Py_TPFLAGS_HAVE_CLASS Py_TPFLAGS_BASETYPE
> #define PyString_AS_STRING(x) PyUnicode_AS_STRING(x)
> #define _PyLong_FromSsize_t(x) PyLong_FromSsize_t(x)
> 
> #endif
> 
> These should be defined and avoid the issue without any other workarounds
> but that never happens, which probably means both PY_VERSION_HEX and
> PY_MAJOR_VERSION are not defined correctly.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/764#issuecomment-171466757
> .
 Splendid. Can you squash, please?
 Jenkins, test this please.
 Jenkins, test this please.
 Merged
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 (waiting for the corp CLA to be added before looking at this)
 (Yeah, looking internally it still seems to have not pushed, sorry about the delay).
 Can you check whether it shows up on https://cla.developers.google.com/clas
 ?

On Thu, Jan 14, 2016 at 8:42 AM Jeremy Barnes notifications@github.com
wrote:

> FYI the CLA came back.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/763#issuecomment-171696248
> .
 @martinwicke I checked internally, it has been signed.  Not sure what magic words to trigger to the googlebot.

Let me try a magic incantation:

I signed it!

Datacratic Inc
 CLAs look good, thanks!

<!-- ok -->
 I'll be using reviewable to review this.  Review comments/discussion will be at:

https://reviewable.io/reviews/tensorflow/tensorflow/763
 @jeremybarnes: I'm sorry to hear about the experience -- we were hoping Reviewable would be a useful tool but GitHub's lack of permission control made it impossible for Reviewable to do its job.  We won't be using Reviewable or any other system until GitHub can improve the permission system.

Secondly, your PR went through a few reviews because we thought it was fine (tests were passing, etc).  But we realized this wasn't something to take lightly, and so we had @dvyukov help (FYI he's less "senior" than Jeff and Sanjay :P) -- and he unearthed that the Eigen use of threadpools is deadlock prone given our ThreadPool implementation, which was a much more urgent and important thing to fix.  I was under the impression that this was communicated to you, I'm sorry.  He's actually been working on changing Eigen multi-threading for a few weeks, and will hopefully have something soon.

So the upshot is that your PR was super useful in finding out these issues; we'll credit you for helping us out!
 So the first step is to fix Eigen, as we've agreed.  The second step is to make sure we have benchmarks that allow us to evaluate the merits of any implementation: we don't have those right now.  Only after that we'll be able to compare implementations of ThreadPool.

I (personally) would be happy to see pluggable ThreadPool implementations.  I'm not sure the way we suggested it in this PR is ideal anymore: I am thinking it might be better to have a ThreadPoolFactory and registration mechanism like we have for Devices and Session implementations, so that anyone can dynamically register their ThreadPool implementation, rather than requiring it be checked into our codebase and selected via environment variables.  If you would be interested in helping out on making something like the ThreadPoolFactory, that would be awesome.  Otherwise, we'll try to find time to do this ourselves and then anyone can plug in their implementation if they so desire.  We can then use this to evaluate your ThreadPool and accept it as the default if the benchmarks and metrics show it shines :)
 As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this PR. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.
  I was hoping to get rid of the gemmlowp dependency soon (it doesn't look like we are actually using it), but re2 will likely stick around for a bit.  Not sure what the problem would be.
 Can you try 

```
bazel build -c opt config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/cc:tutorials_example_trainer
```

and if that doesn't work, post the entire logs?
 We'd be happy to help you debug the issue, but we understand if you just want to use the pip package instead.
  For a hacky (unsupported) workaround, you could try the [answer to this StackOverflow question](http://stackoverflow.com/a/33702428/3574081).
 @mrry: Is there a reason to not change _valid_dtypes on our end?
 there's likely a lot more holes in float64 support, I ran into the following ops being float32-only in 10 minutes of porting: image_summary, max_pool and Square
 Yep, changing `_valid_dtypes` on our end means fixing such things.  Are the other reasons?
 Thanks @siddharth-agrawal !  Someone can comment if for some reason #2389 doesn't fix this
  (Feel free to send us a pull request to fix this if you'd like)
 @fivejjs: Would you be up for sending a PR?  Your suggested code is indeed much better.
 this looks like it has been changed already by a different commit
  Looks like @keveman answered this question on StackOverflow, so I'll close this issue.
  I answered a related question on StackOverflow (which could generalize to your setting):

http://stackoverflow.com/questions/34685947/adjust-single-value-within-tensor-tensorflow/34686952#34686952

Can you take a look and see if it helps?
 (Probably a StackOverflow question)
  @tensorflow-jenkins, test this please.
 Yeah, mac builds have been broken for a while -- but otherwise looks good, will merge in a sec.
  Dupe of #751.
 oh, sorry, I didn't look carefully enough.  do you think you know how to fix it?
 I would say that's a proper fix :).  The logic is just incorrect when you pass in an empty list, which seems like a valid thing to pass.  Feel free to send us a PR, or else let us know you don't want to, and we'll fix it.
  (I have a potentially better but bigger change that should help with this, but no guarantee that I'll get it in in the next week).
 @vrv have you implemented your solution to this yet? Would be nice to have some sort of simple command to not call upon the gpu if you already have another session going on. 
 In https://github.com/tensorflow/tensorflow/commit/8421f177ced940374e06a9daf9b4ee638e207f10 I made it so that if you never use the GPU device, you never allocate any memory for it.

Longer term I want to make the memory allocator grow as needed, but my test change showed a 5% reduction in memory capacity due to the additional fragmentation created.

We're working on a bigger change to the memory allocation strategy in general, but hopefully the commit above will help.
 > We're working on a bigger change to the memory allocation strategy in general, but hopefully the commit above will help.

Thanks! Allowing RNN's to take less memory would be such a game changer. I know you have heard that from several individuals already, so don't want to be overbearing. 
 Closing since this is probably addressed for the time being.
  TensorFlow now pulls the eigen code directly from the eigen upstream repository. There is no need to keep a local copy anymore.
  `run` takes as first argument the ops to run. You want to set a feed_dict. 

This is a question better suited for StackOverflow, where you'll get more in depth answers. We try to keep issues for bug reports.
  Could you please try the following in order?
1. Roll tensorflow/python/training/saver.py back to 1c579361cd1e088dd5e05a394b1561a73e3667ba and see if it works?
2. Check the paths in {your-saver-dir}/checkpoint to make sure they are pointing to the right files. Also please attach a copy in this report.
3. Attach your cnn_eval.py.

Thanks,
Sherry
 The problem is caused by the fix for #571.

I will contact MarkDaoust to see if he can provide a fix today. In the meantime, you can do one of the following:
- Rollback to 1c579361cd1e088dd5e05a394b1561a73e3667ba (before the fix for #571)

or
- Add this line to your code that restores from checkpoint:

if ckpt and ckpt.model_checkpoint_path:
  model_checkpoint_path = ckpt.model_checkpoint_path
  if not os.path.isabs(model_checkpoint_path):
    model_checkpoint_path = os.path.relpath(model_checkpoint_path, save_dir)
  # Replace all ckpt.model_checkpoint_path with model_checkpoint_path to make sure
  # we are using the correct path.
  saver.restore(sess, model_checkpoint_path)
  global_step = model_checkpoint_path

Please let me know if it works for you.

Thanks,
Sherry
  @keveman is also working on a better way to add new ops, so stay tuned.
  1) have you verified that this works on python2, or just python3?
2) If it works for both, can you squash the commits?
 Assigning to @danmane for final checking. Thanks!
 @girving: want to take a quick look?  Seems fine to merge but just want your extra eye.
 Looks good, except for a couple of places where `compat.as_bytes` could be used.  I added two comments to that effect.
 Looks good, assuming that's the right way to import `compat`.  @vrv: Did the tests automatically rerun when the PR was changed?
 Nope, you have to invoke it.  Like so:

@tensorflow-jenkins: test this please.
 Merged
  I think I fixed this in 5853ad99dd3f8b443fdb97d83de35fe057db9e7f
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Merged
  Look for something like this
REGISTER_KERNEL_BUILDER(Name("Conv2D")
                            .Device(DEVICE_CPU)
                            .TypeConstraint<float>("T"),
                        Conv2DOp<CPUDevice, float>);

hence, search for "Conv2DOp" to see the implementation
  Can you squash the commits, then I'll merge.
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
  We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 You can just comment on this thread once you've signed (or the people/email addresses authoring the commits have signed), no need to close and submit another one.
  This is probably a question meant for StackOverflow, Github issues are for bugs / feature requests.  You'll get more help from the community there.
  Thank you for the contribution!  What are your sequence length
distributions like and how much speedup to you get from this change?

On Sun, Jan 10, 2016 at 6:18 PM, Martin Wicke notifications@github.com
wrote:

> Assigned #738 https://github.com/tensorflow/tensorflow/pull/738 to
> @ebrevdo https://github.com/ebrevdo.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/738#event-510665510.
 Great to hear!  I was afraid of pushing this exact change because I wasn't
confident the conditional flow control would improve performance
everywhere.  Is this on CPU or GPU?

On Sun, Jan 10, 2016 at 8:30 PM, Kenton Lee notifications@github.com
wrote:

> These are Penn Treebank sentences. With batches of 32 sentences, I'm
> seeing about 30% improvement in throughput during training.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170428206
> .
 LGTM.

On Sun, Jan 10, 2016 at 8:34 PM, Eugene Brevdo ebrevdo@gmail.com wrote:

> Great to hear!  I was afraid of pushing this exact change because I wasn't
> confident the conditional flow control would improve performance
> everywhere.  Is this on CPU or GPU?
> 
> On Sun, Jan 10, 2016 at 8:30 PM, Kenton Lee notifications@github.com
> wrote:
> 
> > These are Penn Treebank sentences. With batches of 32 sentences, I'm
> > seeing about 30% improvement in throughput during training.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170428206
> > .
 @tensorflow-jenkins, test this please.
 Looks like a CPU test failed:

```
INFO: From Testing //tensorflow/python:rnn_test:
==================== Test output for //tensorflow/python:rnn_test:
F................
======================================================================
FAIL: testBidirectionalRNN (__main__.BidirectionalRNNTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py", line 579, in testBidirectionalRNN
    self._testBidirectionalRNN(use_gpu=False)
  File "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py", line 539, in _testBidirectionalRNN
    self.assertEqual(out.get_shape().as_list(), [batch_size, 2 * num_units])
AssertionError: Lists differ: [None, 6] != [2, 6]

First differing element 0:
None
2

- [None, 6]
+ [2, 6]

----------------------------------------------------------------------
Ran 17 tests in 23.687s

FAILED (failures=1)
```

@kentonl: can you take a look and fix?
 Actually, could you also add a version of the bidirectional RNN test when
sequence_length is not passed in (thus all sequences are considered full
length)?

On Sun, Jan 10, 2016 at 9:05 PM, Vijay Vasudevan notifications@github.com
wrote:

> Looks like a CPU test failed:
> 
> INFO: From Testing //tensorflow/python:rnn_test:
> ==================== Test output for //tensorflow/python:rnn_test:
> F................ FAIL: testBidirectionalRNN (_main_.BidirectionalRNNTest)
> 
> Traceback (most recent call last):
> File
> "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py",
> line 579, in testBidirectionalRNN
> self._testBidirectionalRNN(use_gpu=False)
> File
> "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py",
> line 539, in _testBidirectionalRNN
> self.assertEqual(out.get_shape().as_list(), [batch_size, 2 \* num_units])
> AssertionError: Lists differ: [None, 6] != [2, 6]
> 
> First differing element 0:
> None
> 2
> - [None, 6]
> - [2, 6]
> 
> ---
> 
> Ran 17 tests in 23.687s
> 
> FAILED (failures=1)
> 
> @kentonl https://github.com/kentonl: can you take a look and fix?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170433484
> .
 (btw you can infer the batch size by calling inputs.shape, checking if not
None, and getting the first dimension from the input)

On Sun, Jan 10, 2016 at 9:11 PM, Eugene Brevdo ebrevdo@gmail.com wrote:

> Actually, could you also add a version of the bidirectional RNN test when
> sequence_length is not passed in (thus all sequences are considered full
> length)?
> 
> On Sun, Jan 10, 2016 at 9:05 PM, Vijay Vasudevan <notifications@github.com
> 
> > wrote:
> > 
> > Looks like a CPU test failed:
> > 
> > INFO: From Testing //tensorflow/python:rnn_test:
> > ==================== Test output for //tensorflow/python:rnn_test:
> > F................ FAIL: testBidirectionalRNN (_main_
> > .BidirectionalRNNTest)
> > 
> > Traceback (most recent call last):
> > File
> > "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py",
> > line 579, in testBidirectionalRNN
> > self._testBidirectionalRNN(use_gpu=False)
> > File
> > "/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/rnn_test.runfiles/tensorflow/python/kernel_tests/rnn_test.py",
> > line 539, in _testBidirectionalRNN
> > self.assertEqual(out.get_shape().as_list(), [batch_size, 2 \* num_units])
> > AssertionError: Lists differ: [None, 6] != [2, 6]
> > 
> > First differing element 0:
> > None
> > 2
> > - [None, 6]
> > - [2, 6]
> > 
> > ---
> > 
> > Ran 17 tests in 23.687s
> > 
> > FAILED (failures=1)
> > 
> > @kentonl https://github.com/kentonl: can you take a look and fix?
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170433484
> > .
 Thanks, can you squash the commits?
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 These lines can fail because get_shape() will sometimes return None (e.g.
if using a Placeholder with no shape!)

output.set_shape([input_.get_shape()[0], self.output_size])

On Mon, Jan 11, 2016 at 4:48 PM, googlebot notifications@github.com wrote:

> CLAs look good, thanks!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170745134
> .
 Sorry - not none, but a TensorShape with no dimensions.  the **getitem**
will fail.

On Mon, Jan 11, 2016 at 4:50 PM, Eugene Brevdo ebrevdo@gmail.com wrote:

> These lines can fail because get_shape() will sometimes return None (e.g.
> if using a Placeholder with no shape!)
> 
> output.set_shape([input_.get_shape()[0], self.output_size])
> 
> On Mon, Jan 11, 2016 at 4:48 PM, googlebot notifications@github.com
> wrote:
> 
> > CLAs look good, thanks!
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170745134
> > .
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 A cleaner way would be to move the shape check to the top (before anything
happens) and do:

for input in inputs:
  input.set_shape(input.shape.with_rank(2))

But it does indeed seem that this is not necessary.

On Mon, Jan 11, 2016 at 7:01 PM, Kenton Lee notifications@github.com
wrote:

> It looks like TensorShape's **getitem** is ok to call with no dimensions
> (see
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_shape.py#L507).
> I added a test that uses a shapeless placeholder to be sure.
> 
> Note that the extra logic here: kentonl@04036d7
> #diff-9d717423e6d3f4359151c45dfaa554b6R234
> https://github.com/kentonl/tensorflow/commit/04036d7e443da373cc151305ba81ffacafe85d13#diff-9d717423e6d3f4359151c45dfaa554b6R234
> is required for shapeless placeholders to work correctly for bidirectional
> RNNs.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-170768968
> .
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 CLAs look good, thanks!

<!-- ok -->
 I'd be OK with this.

On Tue, Jan 12, 2016 at 10:18 PM, Kenton Lee notifications@github.com
wrote:

> It looks like setting the shape in the RNN is actually not the right fix.
> When the gradients are computed, the shape inference still fails due to the
> conservative logic in the merge op here:
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L1533.
> Would it be acceptable to relax it so that merging tensors with shapes, for
> example, [None, 10] and [None, 10] would produce a [None, 10] tensor?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/738#issuecomment-171181586
> .
 We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.

<!-- need_author_cla -->
 CLAs look good, thanks!

<!-- ok -->
 @tensorflow-jenkins: test this please.
 @tensorflow-jenkins, test this please
 @tensorflow-jenkins: test this please
  @martinwicke: Is this obsolete now that we support 7.5? 
 Yes.

On Mon, Mar 7, 2016 at 5:12 PM Geoffrey Irving notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke: Is this obsolete now that
> we support 7.5?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/737#issuecomment-193540473
> .
  We'll look into modifying the build scripts to that information lands in the logs.
 @caisq can you add some output to the scripts? I think they're all run through configured, so maybe that's the right place to put it.
 Please close the issue. See: https://github.com/tensorflow/tensorflow/pull/979.

Jenkins builds now shows descriptions that contain JSON objects with info about the build, including build container type, command, platform/OS, build tools and source version. For example: 
http://ci.tensorflow.org/job/tensorflow-master/171/
 I have removed the machine configuration from build descriptions. It was making too much noise. The description stays in the output log.

@EricGT what is your use case to ask for this? Note you are able to replicate any of the linux builds trivially. The only thing you need is docker. See [ci_build/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/README.md) These scripts can be used for great dev setup. I will update the documentation how to do it on windows and mac as well.

Also the most of the things you suggest (and we currently print) does not (should not) matter for the build. Swap, cpu, disk space, kernel, java version, and bunch of other things does not matter. On the other hand I'm not sure how long does it take people to figure out they need to use something like "--jobs 1 --local_resources 512,1,1" to build on 4GB ram. We should write a blog about those things.
  LGTM. Thanks @dongjoon-hyun!
 @vrv You can go ahead and merge.
 Merged into master.
  Yeah, for this to work with python 2 and 3, you have to make another macro
NUMPY_IMPORT_ARRAY_RETURN_TYPE, which is set to int or void depending on
the value of NUMPY_IMPORT_ARRAY_RETVAL. I'm wondering whether that exists
already someplace, it seems like an obvious thing to have.

On Sat, Jan 9, 2016 at 3:57 AM Olivier Grisel notifications@github.com
wrote:

> Note: I am using Python 3.5. It might be a problem related to the build
> support of Python 3 as NUMPY_IMPORT_ARRAY_RETVAL is NULL only under
> Python 3.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/733#issuecomment-170229301
> .
 Separate issue I think. If you would submit the import_array fix as a PR
though that would be very much appreciated.

On Wed, Jan 13, 2016 at 7:48 AM João Felipe Santos notifications@github.com
wrote:

> I was having the same trouble on Linux so I applied the fix proposed by
> @martinwicke https://github.com/martinwicke, but the build breaks when
> compiling rule //tensorflow/python:_pywrap_tensorflow.so. Even with
> --verbose_failures it does not show where GCC found an issue:
> 
> ERROR: /tensorflow/tensorflow/python/BUILD:860:1: C++ compilation of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: gcc failed: error executing command
>   (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \
>   exec env - \
>     INTERCEPT_LOCALLY_EXECUTABLE=1 \
>     PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
>   /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_par
>  ty/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__="redacted"' '-D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -fPIC -c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local_linux-py3-opt/bin/tensorf
>  low/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command
>   (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \
>   exec env - \
>     INTERCEPT_LOCALLY_EXECUTABLE=1 \
>     PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
>   /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_par
>  ty/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/gpus/cuda/include -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__="redacted"' '-D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -fPIC -c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local_linux-py3-opt/bin/tensorf
>  low/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> 
> Is this related to the same issue or should I open another issue?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/733#issuecomment-171337706
> .
 I'll close this one then. Make another issue for the other failure.
  @dsmilkov: let me know if this LGTY and I can merge
 LGTM
 Merged
  @dongjoon-hyun, Thank you for your contribution. Unfortunately, TensorFlow doesn't really own stream-executor. And this has to be fixed there first. I will pass on your changes to the stream-executor team to incorporate them upstream. Then TensorFlow will benefit from it automatically. 
 Unfortunately, the stream-executor team is still working on their own open-sourcing. That's why TensorFlow currently carries a copy. When they are ready for their own open-sourcing, we will drop our copy, and refer to the public one. But for now, you are doing the right thing. Although we cannot accept the pull request, we will pass them to the stream-executor to make the change upstream. 
 Closing for now, I assume this is on the roadmap, @zheng-xq and @leary-google ?
  This question is more appropriate to stackoverflow, github issues are for bugs / feature requests.  You'll likely get better community help there!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 I would love to have this in contrib/ can you move the files there? Then we can merge.
 Thanks @ageron! I'm merging this for the greater good, even if it's still broken. 
 Merged. Is the segfault the same problem you had in your first version?  
 I think your version of gcc doesn't know how to parse

"using typename BinaryLinearAlgebraOp<Scalar, SupportsBatchOperationT>::Matrix;" higher in the file -- my suggestion is to either upgrade your gcc and/or maybe downgrade our code to use less sophisticated C++ features :)
 I'm closing this PR (it's merged). Feel free to continue using it as a message board, or move to #1309.
  Yeah, likely a dupe of #713, we can re-open if we find out there's another issue here.
  Dupe of #713
  I believe this was fixed, @colah can you confirm and close?
 @colah: Should this be closed?
 Posting for @colah: Fixed, so closing.
  We want to provide at least apt and homebrew packages. If someone would like to help out, we'd be thrilled. 
 Isn't that only true if we update the binary packages? Which realistically,
we'd only do for releases.

On Thu, Jan 7, 2016 at 4:07 PM Sherif89 notifications@github.com wrote:

> Also, C++ applications'd be able to use the latest version without having
> to recompile.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/720#issuecomment-169845846
> .
 #1309 would solve the bazel issue for Debian.

On Mon, Mar 7, 2016 at 10:00 PM Sherif89 notifications@github.com wrote:

> What's necessary to package Tensorflow?
> Would #1309 https://github.com/tensorflow/tensorflow/pull/1309 make it
> closer?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/720#issuecomment-193620324
> .
  Let's take discussion to that 
  Thanks for the report. The issue is that we took a dependency on paper-tabs but didn't update the root.workspace rule to download paper-tabs (there was a bug in the synchronization script). It's fixed now and should make its way to master shortly.
  @lberki, @davidzchen, @damienmg in case they know the cause of these hangs, or at least are aware of them.
 Closing since this affects an older version of bazel. If it persists, please file a bug with bazel including debug information.
  @danmane: Do you know if this is still an issue? 
 This looks like a duplicate of #1287 , which is fixed as of latest master. @zdx3578 if the fix for #1287 doesn't work for you, feel free to re-open and we'll dig into it a bit more - if you do re-open it, can you please upload an events file for which you aren't able to visualize the graph? That will make it a lot easier to dig into.
  What is the content of your bazel-tensorflow/external/re2/util directory? util.h should be located in there. If it is there, then it seems there must be some sort of path issue.

What version of Bazel are you using?
 Closing due to lack of activity.  Feel free to reopen if the problem persists.
 This seems like the important bit.  It's unlikely to be an issue in tensorflow:

```
src/main/tools/namespace-sandbox.c:558: mount(opt->mount_sources[i], full_sandbox_path, NULL, MS_REC | MS_BIND | MS_RDONLY, NULL): Permission denied
```
 @Kuntal-G To clarify, you received the re2 error prior to trying sudo, and only get the permission denied error when using sudo?
  @fayeshine, we will officially switch to Cudnn R4 when it is officially released. For now, it is a release candidate.
 After commit [22ebf0a](https://github.com/tensorflow/tensorflow/commit/22ebf0a94fd42af2d78b7964e836c92673ddfa31), you can build TensorFlow with Cudnn R3 and R4, after the following changes to your Cudnn library. 

For Cudnn R3:
$ ln -s libcudnn.so.7.0 libcudnn.so.6.5

For Cudnn R4:
$ ln -s libcudnn.so.4.0.4 libcudnn.so.6.5

This is currently still an unofficial feature. TensorFlow will officially switch to R4 soon after the Cudnn R4 final version is released.

The performance with R4 is better than R2. There are still a number of memory and performance improvements we are working on. Please stay tuned for more changes in this area. 

Thanks. 
-XQ
 We are working on upgrading to Cudnn R4, and test it on all platforms. We'll keep this thread posted. 
  I have a suspicion that our mega-commit 1c579361cd1e088dd5e05a394b1561a73e3667ba might have introduced a change that broke this (there are a bunch of changes that could be responsible, including changes to the stream executor and updating to a new version of Eigen, among other things).

It would be great if someone could confirm if it works at 295625035eefc7801cfe831bd1ba851faacb7579 but fails at the commit right after.
 Cool, thanks for verifying.  If you have time:

Revert the WORKSPACE file and eigen.BUILD changes (to use an earlier version of Eigen) and see if the problem goes away.

I'll dig into potential other issues in that commit.
 Ok thanks, that helps.
 @zheng-xq, who independently was trying to debug this.
 Yup, that commit improved speed for some models, @zheng-xq is looking into what broke it.
 I can confirm that with eigen-a0661a2bb165, an Eigen kernel fails to
launch. But with the older eigen-ce5a455b34c0, this problem goes away.

My reproducing command line:

bazel run -c opt --config=cuda
//tensorflow/models/image/alexnet:alexnet_benchmark

Passing to @benoitsteiner

On Thu, Jan 7, 2016 at 11:18 AM, Martin Wicke notifications@github.com
wrote:

> Assigned #713 https://github.com/tensorflow/tensorflow/issues/713 to
> @zheng-xq https://github.com/zheng-xq.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/713#event-508621546.
 @zheng-xq: what eigen kernel failed, so we can figure out which kernel was the problem?
 The kernel that was causing problem is:

EigenMetaKernel_Vectorizable<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float,
1, 1, int>, 16>,
Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float,
Eigen::internal::scalar_product_op<float, float>, true>,
Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, int>, 16> const> const>
const, Eigen::GpuDevice>, int>

It came from cwise_mul:

tensorflow::BinaryOp<Eigen::GpuDevice, tensorflow::functor::mul<float> >

On Thu, Jan 7, 2016 at 6:53 PM, Vijay Vasudevan notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq: what eigen kernel failed, so we
> can figure out which kernel was the problem?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/713#issuecomment-169877615
> .
 When running "bazel-bin/tensorflow/models/image/alexnet/alexnet_benchmark --benchmarks=all" on my machine, I get the following error:
external/re2/re2/regexp.cc:136: Bad reference count 65535
F tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)
 Okay, we rolled back the change to switch to the newer version of Eigen, which apparently had some issues.  https://github.com/tensorflow/tensorflow/commit/22267addd64b00f9457605cdce3d82276a478780

So things should be now be working.

However, this uses an older version of Eigen that has some speed problems.

@benoitsteiner has upstreamed a newer fix to Eigen which should bring back the speed improvements _and_ not crash things, so look for that soon.

Closing this for now, let us know if you still see problems.
  TensorFlow now pulls the eigen code directly from the eigen upstream repository. There is no need to keep a local copy anymore.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Can you squash the two commits?
On Wed, Jan 6, 2016 at 17:54 Benoit Steiner notifications@github.com
wrote:

> TensorFlow now pulls the eigen code directly from the eigen upstream
> 
> ## repository. There is no need to keep a local copy anymore.
> 
> You can view, comment on, or merge this pull request online at:
> 
>   https://github.com/tensorflow/tensorflow/pull/711
> Commit Summary
> - Pruned our local copy of Eigen of code that we don't use in
>   TensorFlow
> - Pruned our local copy of Eigen of more code that TensorFlow doesn't
>   use
> 
> File Changes
> - _D_ third_party/eigen3/Eigen/src/CholmodSupport/CholmodSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-0 (607)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Block.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-1 (126)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Cwise.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-2 (192)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/CwiseOperators.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-3 (298)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/AlignedBox.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-4 (159)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/All.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-5 (115)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/AngleAxis.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-6 (228)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Hyperplane.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-7 (254)
> - _D_
>   third_party/eigen3/Eigen/src/Eigen2Support/Geometry/ParametrizedLine.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-8 (141)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Quaternion.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-9 (495)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Rotation2D.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-10
>   (145)
> - _D_
>   third_party/eigen3/Eigen/src/Eigen2Support/Geometry/RotationBase.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-11
>   (123)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Scaling.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-12
>   (167)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Transform.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-13
>   (786)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Geometry/Translation.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-14
>   (184)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/LU.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-15
>   (120)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Lazy.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-16 (71)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/LeastSquares.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-17
>   (170)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Macros.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-18 (20)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/MathFunctions.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-19 (57)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Memory.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-20 (45)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Meta.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-21 (75)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Minor.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-22
>   (117)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/QR.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-23 (67)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/SVD.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-24
>   (637)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/TriangularSolver.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-25 (42)
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/VectorBlock.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-26 (94)
> - _D_ third_party/eigen3/Eigen/src/Geometry/AlignedBox.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-27
>   (379)
> - _D_ third_party/eigen3/Eigen/src/Geometry/AngleAxis.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-28
>   (233)
> - _D_ third_party/eigen3/Eigen/src/Geometry/EulerAngles.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-29
>   (104)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Homogeneous.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-30
>   (307)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Hyperplane.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-31
>   (270)
> - _D_ third_party/eigen3/Eigen/src/Geometry/OrthoMethods.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-32
>   (221)
> - _D_ third_party/eigen3/Eigen/src/Geometry/ParametrizedLine.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-33
>   (195)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Quaternion.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-34
>   (778)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Rotation2D.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-35
>   (157)
> - _D_ third_party/eigen3/Eigen/src/Geometry/RotationBase.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-36
>   (206)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Scaling.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-37
>   (166)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Transform.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-38 (0)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Translation.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-39 (0)
> - _D_ third_party/eigen3/Eigen/src/Geometry/Umeyama.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-40 (0)
> - _D_ third_party/eigen3/Eigen/src/Geometry/arch/Geometry_SSE.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-41 (0)
> - _D_ third_party/eigen3/Eigen/src/Householder/BlockHouseholder.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-42 (0)
> - _D_ third_party/eigen3/Eigen/src/Householder/Householder.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-43 (0)
> - _D_ third_party/eigen3/Eigen/src/Householder/HouseholderSequence.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-44 (0)
> - _D_
>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/BasicPreconditioners.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-45 (0)
> - _D_ third_party/eigen3/Eigen/src/IterativeLinearSolvers/BiCGSTAB.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-46 (0)
> - _D_
>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/ConjugateGradient.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-47 (0)
> - _D_
>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/IncompleteLUT.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-48 (0)
> - _D_
>   third_party/eigen3/Eigen/src/IterativeLinearSolvers/IterativeSolverBase.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-49 (0)
> - _D_ third_party/eigen3/Eigen/src/Jacobi/Jacobi.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-50 (0)
> - _D_ third_party/eigen3/Eigen/src/MetisSupport/MetisSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-51 (0)
> - _D_ third_party/eigen3/Eigen/src/OrderingMethods/Eigen_Colamd.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-52 (0)
> - _D_ third_party/eigen3/Eigen/src/OrderingMethods/Ordering.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-53 (0)
> - _D_ third_party/eigen3/Eigen/src/PaStiXSupport/PaStiXSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-54 (0)
> - _D_ third_party/eigen3/Eigen/src/PardisoSupport/PardisoSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-55 (0)
> - _D_ third_party/eigen3/Eigen/src/QR/ColPivHouseholderQR.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-56 (0)
> - _D_ third_party/eigen3/Eigen/src/QR/ColPivHouseholderQR_MKL.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-57 (0)
> - _D_ third_party/eigen3/Eigen/src/QR/FullPivHouseholderQR.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-58 (0)
> - _D_ third_party/eigen3/Eigen/src/QR/HouseholderQR.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-59 (0)
> - _D_ third_party/eigen3/Eigen/src/QR/HouseholderQR_MKL.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-60 (0)
> - _D_ third_party/eigen3/Eigen/src/SPQRSupport/SuiteSparseQRSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-61 (0)
> - _D_ third_party/eigen3/Eigen/src/SVD/JacobiSVD.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-62 (0)
> - _D_ third_party/eigen3/Eigen/src/SVD/JacobiSVD_MKL.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-63 (0)
> - _D_ third_party/eigen3/Eigen/src/SVD/UpperBidiagonalization.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-64 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/AmbiVector.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-65 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/CompressedStorage.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-66 (0)
> - _D_
>   third_party/eigen3/Eigen/src/SparseCore/ConservativeSparseSparseProduct.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-67 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/MappedSparseMatrix.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-68 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseBlock.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-69 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseColEtree.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-70 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseCwiseBinaryOp.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-71 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseCwiseUnaryOp.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-72 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseDenseProduct.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-73 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseDiagonalProduct.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-74 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseDot.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-75 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseFuzzy.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-76 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseMatrix.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-77 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseMatrixBase.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-78 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparsePermutation.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-79 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseProduct.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-80 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseRedux.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-81 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseSelfAdjointView.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-82 (0)
> - _D_
>   third_party/eigen3/Eigen/src/SparseCore/SparseSparseProductWithPruning.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-83 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseTranspose.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-84 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseTriangularView.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-85 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseUtil.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-86 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseVector.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-87 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseView.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-88 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseCore/TriangularSolver.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-89 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-90 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLUImpl.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-91 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_Memory.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-92 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_Structs.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-93 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_SupernodalMatrix.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-94 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_Utils.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-95 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_column_bmod.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-96 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_column_dfs.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-97 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_copy_to_ucol.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-98 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_gemm_kernel.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-99 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_heap_relax_snode.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-100 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_kernel_bmod.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-101 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_panel_bmod.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-102 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_panel_dfs.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-103 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_pivotL.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-104 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_pruneL.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-105 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_relax_snode.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-106 (0)
> - _D_ third_party/eigen3/Eigen/src/SparseQR/SparseQR.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-107 (0)
> - _D_ third_party/eigen3/Eigen/src/StlSupport/StdDeque.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-108 (0)
> - _D_ third_party/eigen3/Eigen/src/StlSupport/StdList.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-109 (0)
> - _D_ third_party/eigen3/Eigen/src/StlSupport/StdVector.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-110 (0)
> - _D_ third_party/eigen3/Eigen/src/StlSupport/details.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-111 (0)
> - _D_ third_party/eigen3/Eigen/src/SuperLUSupport/SuperLUSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-112 (0)
> - _D_ third_party/eigen3/Eigen/src/UmfPackSupport/UmfPackSupport.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-113 (0)
> - _D_ third_party/eigen3/unsupported/Eigen/FFT
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-114 (0)
> - _D_ third_party/eigen3/unsupported/Eigen/KroneckerProduct
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-115 (0)
> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/CMakeLists.txt
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-116 (0)
> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/ei_fftw_impl.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-117 (0)
> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/ei_kissfft_impl.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-118 (0)
> - _D_
>   third_party/eigen3/unsupported/Eigen/src/KroneckerProduct/CMakeLists.txt
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-119 (0)
> - _D_
>   third_party/eigen3/unsupported/Eigen/src/KroneckerProduct/KroneckerTensorProduct.h
>   https://github.com/tensorflow/tensorflow/pull/711/files#diff-120 (0)
> 
> Patch Links:
> - https://github.com/tensorflow/tensorflow/pull/711.patch
> - https://github.com/tensorflow/tensorflow/pull/711.diff
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/711.
 CLAs look good, thanks!

<!-- ok -->
 We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.

<!-- need_author_consent -->
 Can you rebase and squash again (see [rebaseandsqua.sh](rebaseandsqua.sh))? The merge made things very ugly.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 Can you rebase to head?
  Thanks!
  We have `tf.floor` and `tf.ceil`, but we don't have round to nearest.  I am conflicted on whether it should be called `rint` or `round`.  It would also be annoying if it depended on the processor flags.
 Sounds reasonable to me.
 I believe this is fixed -- `tf.round` seems to work. It's different from the patch I had prepared, and I think different from @chemelnucfin's PR, but somehow someone wrote it.
  No, it was correct before. [0,0) means excluding 0, [0,0] means including 0, which would also mean that 0 would be valid and the error message would be confusing indeed.
  Do you want to send us a fix to the docs?
 This is an odd one -- we're mimicking python's range behavior, which is hard to properly encode in python because overloading doesn't exist. The docs for tf.range are pretty clear what it does. 

The fact that the signature itself is misleading is unfortunate, but somewhat out of our control (the limit parameter does have a default of None, and start does not have a default value of 0, python doesn't have a way of expressing the signature tf.range(start=0 if limit is None else no default, limit=None) in the signature alone).

I will close this for now. By all means, if you have a better way of expressing the semantics, let us know.
 The signatures in the docs are taken directly from the code by the doc
generator.
On Thu, Jan 7, 2016 at 06:32 Mohammed AlQuraishi notifications@github.com
wrote:

> i see your point. This is pretty confusing though. Are the signatures
> autogenerated? If not one possibility is to write it out to mirror the
> semantics, even though it won't be correct python code. I.e. tf.range(start=0,
> limit, delta=1, name='range'), and then just spell it out in the docs
> that that is the behavior that's effectively achieved, even though it's
> actually implemented differently in python.
> 
> Alternatively maybe say something explicitly in the docs about this
> caveat? My confusion arose from scanning the signature of the function and
> thinking it made no sense. Obviously when I read the text the semantics
> became clearer, but a note explaining the caveat with the signature would
> be helpful.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/707#issuecomment-169679915
> .
  Where is your copy of `swig` installed (i.e. what result do you get when you run `which swig`)?

As a short-term fix, you can modify [`swig.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/swig/swig.sh) to invoke SWIG by its absolute path.
 Just to confirm, if you run `/home/wenhuchen/.local/bin/swig` from the command line, does that work without error?
  Can you squash your commits?
  It looks like you're adding more and more ops to a graph, which is not freed when the session is closed. If that is in fact the case (you can check by making a graph and using it `with graph.as_default()`, and destroying it), then this is intended behavior -- the graph should survive a session.
 Martin's answer looks spot on here. The following code should not leak, and if it does let us know and we'll reopen the issue:

``` python
for i in range(0,10000000):
  t0 = time.clock()

  with tf.Graph().as_default():
    sess = tf.Session()

    a = tf.placeholder(tf.int16, name='a')
    y = tf.identity(a, name='y')

    sess.run(y, feed_dict={a: 3})
    sess.close()

print time.clock() - t0
```
  The name may not be the best one, but we probably want to avoid saying "loss" in the function (since it's use it not necessarily restricted to being used a loss).
 Sounds good to me. @sherrym, I'll assign you for summary judgment. 
 Closing since changing the name is too little benefit for the large amount of required work.
  Since  we're now pulling the tensor code directly from the eigen repository we don't need to keep a local copy anymore.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I signed it

On Tue, Jan 5, 2016 at 12:50 PM, googlebot notifications@github.com wrote:

> Thanks for your pull request. It looks like this may be your first
> contribution to a Google open source project. Before we can look at your
> pull request, you'll need to sign a Contributor License Agreement (CLA).
> 
> [image: :memo:] _Please visit https://cla.developers.google.com/
> https://cla.developers.google.com/ to sign._
> 
> Once you've signed, please reply here (e.g. I signed it!) and we'll
> 
> ## verify. Thanks.
> - If you've already signed a CLA, it's possible we don't have your
>   GitHub username or you're using a different email address. Check your
>   existing CLA data https://cla.developers.google.com/clas and verify
>   that your email is set on your git commits
>   https://help.github.com/articles/setting-your-email-in-git/.
> - If you signed the CLA as a corporation, please let us know the
>   company's name.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/698#issuecomment-169129411
> .
 CLAs look good, thanks!

<!-- ok -->
 Jenkins, test this please
 martin: you need a period after "please" :)

@tensorflow-jenkins: test this please.
 I need to change that regex, please.

On Tue, Jan 5, 2016 at 9:50 PM Vijay Vasudevan notifications@github.com
wrote:

> martin: you need a period after "please" :)
> 
> @tensorflow-jenkins https://github.com/tensorflow-jenkins: test this
> please.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/698#issuecomment-169235658
> .
 Jenkins, retest this please.
 retest this please.
  Is this different than #693 ?  That one seems to handle a few more cases
 No worries, it was just sent a few hours ago :)
  I am working on making save and restore more self-contained. Stay tuned.

Sherry

On Wed, Jan 6, 2016 at 4:09 PM, Jim Fleming notifications@github.com
wrote:

> Ok, so I can partially train from a deserialized graph by manually create
> the missing tf.Variable's and hooking up their internal properties to the
> tensors and operations from the original graph. So maybe the GraphKeys
> don't need to be serialized but I would like to make this process easier.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/696#issuecomment-169504754
> .
 This should have been fixed by 

https://github.com/tensorflow/tensorflow/commit/0269c3690b402c63100869cbb7f8a8ea190e8c0a#diff-6de345a74f7146d4104e2c3c4ad2d89b

Please open a new issue if this doesn't address what you would like to be able to do. Thanks.

Sherry
  Jenkins, test this please.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 (squash the commits once this is done and i'll run a quick test before merging)
 @tensorflow-jenkins, test this please.
 (ugh, give us some time to fix our CI testing, sorry)
 Jenkins, test this please.
 @martinwicke: why is one check stuck?
 We're working on it. Looking good though, I think we can merge (I'll take the beating if I'm wrong).
  This is a better question for stackoverflow.  Github issues are for bugs or feature requests.
  We will release new versions of the wheels when we release a new version of tensorflow. As you note, the code moves quite quickly, and so the wheels seem stale, but the last release is not even a month old. We're still working on our automated testing setup and we'll probably make new binaries shortly after.
  Please ask questions like this on stackoverflow.  Github issues are for bugs and feature requests.  Also, whenever something isn't working, please include the error messages you're seeing.
  I can reproduce this problem on my side. The hang only shows up with
external Eigen library. During the hang, one Eigen kernel was stuck on GPU.
It is unclear whether it is a real hang, or it was just the kernel run
extremely slowly.

It is known that there is a recent slowdown after switching to the external
library. Benoit is currently working on a fix.

On Tue, Jan 5, 2016 at 1:35 AM, Martin Wicke notifications@github.com
wrote:

> Assigned #688 https://github.com/tensorflow/tensorflow/issues/688 to
> @zheng-xq https://github.com/zheng-xq.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/688#event-505699811.
 I believe this was fixed by https://github.com/tensorflow/tensorflow/commit/38f54b55cdc66e12a2f6ce9d6724440e9738ac6f. After the upgrade to the latest version of Eigen, I can now run mnist on GPU in debug mode:

bazel run --config=cuda -c dbg --strip=never //tensorflow/models/image/mnist:convolutional
...
Initialized!
Step 0 (epoch 0.00), 23.4 ms
Minibatch loss: 12.054, learning rate: 0.010000
Minibatch error: 90.6%
Validation error: 84.6%
Step 100 (epoch 0.12), 58.6 ms
Minibatch loss: 3.293, learning rate: 0.010000
Minibatch error: 6.2%
Validation error: 7.1%
Step 200 (epoch 0.23), 57.0 ms
Minibatch loss: 3.525, learning rate: 0.010000
Minibatch error: 14.1%
Validation error: 3.9%
...

I have run 2900 steps with no problem so far, so I'm closing this issue. Feel free to reopen if you still experience this deadlock
  Sorry, I don't know what is going wrong here.  I'll see if I can get someone to look at it.
 (a) the code is broken:  it uses tf.types.float32, which should just be tf.float32.

(b)  The problem is because when you're doing the tf.select(draws, mx_fill, weights), you're potentially sending the gradients to the result of tf.fill().  Fill has no gradient registered.

Fill should have a gradient.  Hang on.
 Fixed.  Should be pushed here within a day.
 (Thanks for catching this!)
 (just to confirm, it got pushed in the above commit, so it's available at HEAD now)
  All of the content on tensorflow.org (without the fancy styling and some math formatting) is also available on github, in the tensorflow/g3doc folder. The HTML on tensorflow.org is generated quite similarly to what github's MarkDown viewer does, so the experience shouldn't be all that different.
  Yes, we are planning to open source it.  No timeline, but there's a tracking bug here: https://github.com/tensorflow/tensorflow/issues/16.
  so ~/homebrew/bin is in your PATH? I think this is a question for the bazel folk. Bazel passes on the PATH environment variable to shell scripts it runs, but I don't know off the top of my head how to set it explicitly.

Closing this here -- feel free to reopen if it turns out we should have written swig.sh differently.
  The docs are not exposed so people don't start relying on this feature (whose interface is very likely to change in the future). It's not part of the public API. I realize the distinction isn't perfect, especially if related things are referenced in the parts of the docs that are referenced from official documentation.

This should be resolved once we publish a more mature version for these ops.
 Can you look if #687 fixes the issue? Clearly, the mismatch between documentation and code is a bug.
  @dsmilkov 
 I believe this is fixed, see: https://www.tensorflow.org/tensorboard/index.html#graphs
  @dsmilkov 
 Hi guys,

This has been fixed, but after the 0.6.0 release. You can either wait for the 0.7.0 which will be released soon, or rebuild tensorboard yourself following the instructions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#tensorboard-development-instructions

Hope this helps!
 @Dringite It seems for you, the whole TensorBoard is not working, not just a broken summary icon. For that, see issue #1076 . There is a fix pushed to the release branch but if you want it sooner, take this commit: https://github.com/tensorflow/tensorflow/commit/68e8b0f1e02f1e0e10f4fdd689ede80f973e2756
 @jbeda To fix the summary-icon, you need to rebuild tensorboard using bazel and run it:

```
bazel build tensorflow/tensorboard
bazel-bin/tensorflow/tensorboard/tensorboard --logdir path/to/log/dir
```

For more information about installing and building TensorFlow and TensorBoard from source, see https://www.tensorflow.org/versions/v0.6.0/get_started/os_setup.html#source
  Dupe: https://github.com/tensorflow/tensorflow/issues/609
  zackchase@, you are right about the current `gradients` function. Currently, you can compute the Jacobian of, say, a vector, by calling `gradients` multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.
 It'd be pretty hard to support gradients of non-scalars with our current setup, since it would require every gradient function to handle extra rank input.  The one possibility I could see would be if we add some sort of map facility to register how to add extra ranks to ops, then compute gradients with respect to extra rank by computing lower rank and calling the registered map transformations.

Someone asked for map a while back, so if anyone wanted to tackle this task that might be the way to go.  Handling it at the gradient function level is probably bad, since it would add required complexity to an existing feature.  Warning: This is a pretty large change, so a good deal of discussion would be in order before starting.
 Tensor rank is very standard terminology: http://mathworld.wolfram.com/TensorRank.html
 Are you saying your network has a bunch of outputs, and then you combine them into a single scalar that you are trying to optimize?  In that case, you should differentiate with respect to that single scalar.
 Ah, sorry for not reading carefully.  You're correct that (as far as I know) there's no easy way to do that in current tensorflow.  According to someone more knowledgeable than I, people generally do such contractive autoencoders by writing out the first derivative manually.  Also, they generally restrict to single layer at a time networks for speed issues, since doing the full Jacobian for a multilayer network is quite expensive.
 Differentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there's more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable

The reason for "one output variable at a time" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover "multiple inputs/multiple outputs" case in the most efficient way could be a lot of extra plumbing.

If you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano's "Rop" operator to differentiable ops and some plumbing to call them instead of existing op "gradient" function (existing gradient function is an equivalent of Lop operation, or "left multiply sensitivity vector by op's jacobian" operation) 
 @yuanbyu: Do you understand this issue with `tf.map_fn`?
 Note for anyone who comes across this thread: `tf.map_fn` is an unrelated thing involving control flow, not something related to mapping over extra rank tensors.
  Thanks for the report - feel free to send us a pull request to fix this!
 I fixed this for the next release!
  @georgedahl: Who's working on partial_run so that I can assign this to them?  Do you know if it's time to close?
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 Cool, squash the commits into one and we'll merge it in.  Thanks!
  Closing due to lack of activity.
  Do you have a link to a model that reproduces this problem?
 If you don't use the meta_graph feature, please add "write_meta_graph=False" option to saver.save().

Sherry
 The error is due to current size limit on protocol buffers in the implementation of message lite. Can you shard the Variable so it's not so large? 

bool MessageLite::SerializePartialToCodedStream(
    io::CodedOutputStream\* output) const {
  const int size = ByteSize();  // Force size to be cached.
  if (size < 0) {
    // Messages >2G cannot be serialized due to overflow computing ByteSize.
    GOOGLE_LOG(ERROR) << "Error computing ByteSize (possible overflow?).";
    return false;
  }
 Closing since this issue seems to have been diagnosed.  Please direct further questions about sharding variables to StackOverflow.
  @liujunyi-sjtu, I cannot reproduce the problem on my side. Could you provide some more information? How did you bulid and run the binary?

If we still cannot reproduce this problem, then you probably have to run cuda-memcheck on your side, and post the relevant results. Thanks. 
 Closing due to lack of information.  I'm happy to reopen if more information arises.
 @johnfrombluff Why do you think it is a related error?  Unrelated errors should be filed as separate issues.
  This is very nice!  Thanks for this contribution -- we'll have @zheng-xq take a look at this soon.
 @leary-google, could you review the stream-execuctor portion of this change? 
 New commits on existing PR seems fine -- we'll just ask to squash the commits prior to validation and merging.
 Because of peculiarities in our internal build process, we won't be able to merge this right now. I'll leave this open since it may be useful for people. When we find someone to resolve the internal problems, we may be able to absorb it at a later time.

I'm sorry about that -- I would love to have this in.
 That, and reuse of shared code elsewhere. The code elsewhere can be updated
(internally) which is why we need someone here to fix that up. The problem
with stream_executor is that it almost has to be treated like generated
code. In sorry this bit you after you've done all this work, I'm still
hoping we can absorb it somehow, and we'll make the restrictions on
stream_executor clearer for the future.
On Fri, Jan 8, 2016 at 07:25 Ville Kallioniemi notifications@github.com
wrote:

> Thanks for the update @martinwicke https://github.com/martinwicke ! Is
> the main issue causing problems with your internal build process the
> automatic generation of the "platform.bzl" file?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-170030821
> .
 I think the biggest problem mentioned in https://github.com/tensorflow/tensorflow/pull/664#issuecomment-170044135 is the stream executor bits -- we don't own that code and it gets upstreamed a lot, so even if we accepted the change, it would likely get overwritten by our next upstreaming :(

We'll try to figure out a better story for stream_executor soon -- the same problem exists for Windows too.
 Check the installation instructions on the website for this one -- it's a
common issue, probably reinstalling protobuf will fix it.

On Thu, Feb 25, 2016 at 1:52 PM Mats Berggrund notifications@github.com
wrote:

> I have also followed the instruction above and all the building activities
> works fine but when I start python and try to import tensorflow I get this
> error:
> 
> MatsMacBookPro2:~ mats$ python
> Python 2.7.10 (default, Oct 23 2015, 18:05:06)
> [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin
> Type "help", "copyright", "credits" or "license" for more information.
> 
> > > > import tensorflow
> > > > Traceback (most recent call last):
> > > >   File "<stdin>", line 1, in <module>
> > > >   File "/Library/Python/2.7/site-packages/tensorflow/**init**.py", line 23, in <module>
> > > >     from tensorflow.python import *
> > > >   File "/Library/Python/2.7/site-packages/tensorflow/python/**init**.py", line 37, in <module>
> > > >     from tensorflow.core.framework.graph_pb2 import *
> > > >   File "/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py", line 10, in <module>
> > > >     from google.protobuf import descriptor_pb2
> > > >   File "/Library/Python/2.7/site-packages/google/protobuf/descriptor_pb2.py", line 1533, in <module>
> > > >     **module** = 'google.protobuf.descriptor_pb2'
> > > >   File "/Library/Python/2.7/site-packages/google/protobuf/reflection.py", line 123, in **new**
> > > >     new_class = superclass.**new**(cls, name, bases, dictionary)
> > > > TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases
> 
> I have a MacBook Pro Retina, 15-inch, Late 2013, GeForce GT 750M, 16GB
> RAM, running OS X 10.11.3
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-189002621
> .
 @matspetter Can you try again after updating protobuf again (to b2)? This may be resolved now.
 For cudnn v5 see #1786.
 @ville-k: any interest in reviving this CL against master?  I think we'd like to merge these changes and then we'll try to keep it working as best we can.
 Doh, sorry for not getting to this -- looks like there are a few more conflicts.  I left some comments but this generally looks great -- nice refactoring!

cc @zheng-xq 
 Okay, this generally looks good!  Let's weed out any test failures: test this please, @tensorflow-jenkins 
 @markb729: right now we don't package one binary that works with both cudnn4 and 5, because the APIs are different.

At some point I think it would be conceivable to implement the stream executor in such a way that different cudnn versions are different implementations of the same interface, and we dispatch exactly once during initialization to the right one.
 @tensorflow-jenkins test this please 
 One more try!   test this please
 Woohoo!!  thank you so much for this contribution.  We'll try our best to keep it working, though without OS X / GPU test machines, we can't promise too much.
 Finally! Thanks for all the hard work! I will get an external GPU so we can test this and make sure it continues to work.
 Yes. We're installing hardware for that over the weekend. A test should
come some time next week.
On Fri, May 13, 2016 at 17:09 Christian Hansen notifications@github.com
wrote:

> Hi guys, I'm new here. I was wondering if there shouldn't be a "MacOS _G_PU
> Tests" in the lists of tests being run at http://ci.tensorflow.org to
> make sure the new CUDA/GPU functionality keeps working?
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-219187149
  I was wondering how can I build the android demo with GPU support. I have a device with CUDA capability (Tegra K1 device). What I did was to pass a `--config=cuda` when building the demo. Is it supposed to be enough to enable gpu support by only passing  `--config=cuda`?

I ./configured the workspace given cuda and cudnn path and also specified proper `compute_capability=3.2`
Now that the app is running on my device, how can I make sure if it has gpu support? I did not see a noticeable speed up by doing this. So, What is the proper way to run and validate it?

P.S.
I am using CUDA 6.0. 
I modified few lines to ignore CUDA 7.5 and use 6.0 instead.
 If you want to make sure you are placing ops on the GPU, you can find a test that can use the GPU (such as the [RoundTest](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops_test.py#L45)), and modify it so the session is created with `force_gpu=True`:

```
with self.test_session(use_gpu=True, force_gpu=True):
```

If the test then fails, that's a strong indicator you're not running a GPU. The RoundTest is a good one because it has only ops that can actually run on a GPU, other tests have a mixture, and `force_gpu` is pretty remorseless.
 Thanks @martinwicke. Actually, the code you mentioned (RoundTest) is written in Python. What I need is a C++ code that I can test it on my android device. So, I need a piece of code that can be compiled with GPU=on and has the capability to be run under my android device. 
I can not test it on my PC, as it does not come with a GPU device. 

Do you know how can I tell bazel to use gpu code rather than scalar version. And does that work for the android demo? I know I can pass --config=cuda for the running `//tensorflow/cc:tutorials_example_trainer`. But it doesn't seem to be enough for the android demo.
 `--config=cuda` will compile for GPU, but that doesn't mean that all ops are necessarily run on GPU. If you have a `SessionOptions` object, you can `set_log_device_placement(true)`, which will log the placement of ops, and you can check that they actually run on GPU.
 Thank you @martinwicke . I will test that option to make sure and will post the result here.
But according to the unchanged performance of the process I guess it is not gpu enabled. 
(I mistakenly close the issue and reopened it)
 I tried to log the used device in native jni part. However, setting `log_device_placement` is not as straight forward as in Python. 
As a desperate try I did the following change into the Android demo:

```
    tensorflow::GraphDef tensorflow_graph;
    LOG(INFO) << "Graph created.";
    graph::SetDefaultDevice("/gpu:0", &tensorflow_graph);
```

Still not sure if it uses CUDA device or not.
 One more update:
`config.device_count_size()` returns 0 at run-time??
 You ran configure with TF_UNOFFICIAL_SETTING, right?

On Thu, Jan 7, 2016 at 10:36 PM Hamid Bazargani notifications@github.com
wrote:

> One more update:
> config.device_count_size() returns 0 at run-time??
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/663#issuecomment-169910128
> .
 That's correct.
 The set_log_device_placement is a c++ function on SessionOptions -- nothing in c++ is as straightforward as in python, but it shouldn't be hard to do if you're running everything from c++ anyway.
 http://cudamusing.blogspot.com/2015/11/building-tensorflow-for-jetson-tk1.html might be a relevant post from shortly after our release -- lots has changed but there might be some helpful hints there.
 @martinwicke I don't find (grep) `set_log_device_placement`. There is no such a method. Can you provide more details.
@vrv I have seen it. I got some of my modifications from that post. However, it is for Jetson device with Linux_for_Tegra that makes the story different.
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/config.proto#L92

Which is a structure within https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session_options.h#L58

which you can pass to the Session() constructor
 Sorry I'm not familiar with Proto message format.
I set the `log_device_placement` to true. And it seems everything is running on cpu0.
At least I know on which device the process is being executed.
Now the main question is still left open. How to make it run on gpu?
 BTW, 
I am able to build other examples like example_trainer with CUDA. 
My problem is the android demo.
 Duh. Sorry, I should have noticed that. It's quite possible that the build rules for android are not properly set up for GPU support at all. Assigning someone with better knowledge about mobile issues.
 The Android TF build targets do not currently support CUDA.

A major reason for this is that Tensorflow currently requires CUDA 7.0 and the NVIDIA Codeworks support for 7.0+ is very limited -- currently only the Shield Android TV is listed as 7.0 capable (see http://docs.nvidia.com/gameworks/index.html#developertools/mobile/codeworks_android/codeworks_android_release_notes.htm).

This could change in the future if support improves for CUDA on Android -- though if you get something working on your own please let us know!
 Thanks @andrewharp. 
I was aware about the point. however, I like to use an unofficial approach. 
I was able to solve all compatibility issues. 
Assuming a CUDA 7.0 device, how we can tell bazel to build the android demo with GPU support?
Passing --config=cuda does not seem to work for the Adnroid demo, but it worked for other native examples on x86.
Some useful effort on bringing TF to Cuda < 7.0 (Jetson TK1) at
http://cudamusing.blogspot.ca/2015/11/building-tensorflow-for-jetson-tk1.html
 @hamidb This is something that possibly may come in the future, but we can't support at this time.

If you wish to proceed on your own the best pointer I can give you is to try the NVIDIA Codeworks combined with a custom Bazel build rule such as [this](https://gist.github.com/andrewharp/e221ca9ae629561442e3) (you would need to modify it obviously), which would let you compile the kernels into libraries that you could then link in as dependencies. I can't speak to any further issues you would encounter, though.
 @bhack We have been working with the Renderscript team (they've recently introduced some intrinsics for key operations we need like GEMMs) but we don't have an estimate for when and how that will be exposed.
 TensorFlow doesn't have Renderscript support in yet, and OpenCL is not in there either.
 I believe the original issue was addressed, closing due to lack of recent activity.
  One issue with this change is that it makes multiple copies of the input data (by default 32), which could have an unfortunate effect on memory usage. Perhaps a cleaner optimization would be to produce intervals (rather than ranges), which could be used to `tf.slice()` the appropriate elements from the input tensor(s), rather than `tf.gather()` them?
 Sorry for the delay in getting back to you! Looking at your PR more closely, it seems like I misunderstood the nature of your change (and the nature of `slice_input_producer()`, which I had mistakenly assumed produces batches rather than single elements...). I'm still a bit unclear about why your change makes things more efficient: as far as I can tell, the change will end up copying more data out of `input_tensor` on each iteration, which is potentially wasteful. Do you have details of a benchmark that shows the improvement?

Looking at the implementation, it also looks like the `array_ops.gather()` is the only thing that your change avoids, but at the expense of copying the input tensor into the queue repeatedly, which could potentially lead to a performance regression. It seems like you could get almost all of the benefit by replacing the `array_ops.gather()` with an `array_ops.slice()`, which in many cases could avoid the copy altogether.

Does that make sense?
 I think that there's a better way to solve your problem:

``` python
def get_output(img):
    img = tf.train.limit_epochs([img], num_epochs=None)  # Produce infinitely many times.
    img = tf.train.batch(img, 128, enqueue_many=True)  # Enqueues a batch at a time.

    img = tf.reshape(img, [-1, 28 * 28 * 3])
    W = tf.get_variable('W', [28 * 28 * 3, 10],
                        initializer=tf.truncated_normal_initializer(stddev=0.1))
    out = tf.matmul(img, W)
    return tf.sigmoid(out)
```

On my workstation, this modified version of your benchmark takes between 0.05 and 0.08 seconds.
 What's the status of this?
 Closing due to inactivity.
  Duplicate of #448
  I'll test internally to make sure this matches exactly the pipeline other
tools have. I was pretty sure that's true, but the trailing '-' scared me a
little. Worst case, we'll have to include only anchors with a trailing '-',
still a big improvement.

On Sun, Jan 3, 2016 at 7:56 PM Vijay Vasudevan notifications@github.com
wrote:

> Assigned #660 https://github.com/tensorflow/tensorflow/pull/660 to
> @martinwicke https://github.com/martinwicke.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/660#event-504391321.
 I looked, there aren't, so don't worry. I'll merge once my tests are done.

On Mon, Jan 4, 2016 at 12:08 PM Sam Abrahams notifications@github.com
wrote:

> From what I've seen, you only end up with a trailing anchor when you do
> something odd like leaving space between the final punctuation words.
> Something like "This header trails off ...". The last
> "create_anchor_from_header" function I posted works for the tests I played
> around with here
> https://github.com/samjabrahams/tensorflow_util/blob/master/md/github_header_test.md
> 
> I'm going to reapply the latest function to double check that there aren't
> any edge cases in the changed files.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/660#issuecomment-168789604
> .
 I'm good to merge this -- but this only does os_setup.md, correct? I'll change the title appropriately, or alternative, you could run this on all the files.
 Great. Thanks a lot for this!
  Closing because yamins81 provided a solution.  Feel free to reopen if this is still a problem!
  This failure occurs before TensorFlow even begins executing. It looks the system you're running on has very little RAM available, and can't handle loading the whole MNIST training set into memory and preprocessing it in python. There might be ways to reduce the total amount of memory used at that stage, but this seems like a particularly pathological situation.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  When writing the seq2seq module, the idea was that the loop_function argument (e.g. of attention_decoder) could be used to provide various forms of decoding. It works for the greedy case, but we have not implemented a beam-search yet. I think it should be possible using loop_function and the top-k op from tensorflow. But we'll see - if it's too hard to do it inside the graph, then we can change the design and go with a python-side decoder. Having a decoder in the graph has advantages though, esp. when building more complex models, so I'd like to try that first. All ideas, comments, remarks and code are welcome of course!
 I wouldn't mind helping out either. I currently use sampling with temperature to generate different outputs given the same input. 
 I like the first code a lot, I think it's advantegous when the beam-search is done in the graph, so we can just feed the graph once and get the whole sequence. I think the only thing missing was pulling out hypotheses, right? But we have the top-k op in TensorFlow, woudn't that suffice?
 > I like the idea of computing top k on each hypothesis, then computing top k on the combination of remaining words.

I like this idea too, but isn't computing top k on the combination of remaining words expensive?
 Just as a comment: there is some experimental support for session.partial_run() in TensorFlow in the 0.7 release. Since partial_run does not deallocate the tensors, it should make step-by-step decoding from seq2seq models much easier. And since we can have a few of them in parallel, it could also greatly simplify beam_search. But it's experimental for now, so beware - I'm just testing it.
  @marcotrombetti, @mrry: Looks like this fell through the cracks.  Is it still an issue?
  This should be a stackoverflow question rather than a github issue.  Github issues are for bugs or feature requests that should be fixed on the tensorflow side.
  Fixed with 1c579361cd1e088dd5e05a394b1561a73e3667ba. The commit is very large since it bundled many changes, but the important change is in minimap.ts. Feel free to reopen if you run into this issue again. You won't see this fix before the next release unless you rebuild TensorBoard yourself following the instructions in  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md
 @frankyjuang The issue you are having might be unrelated to this issue, since you are already using Chrome and not Firefox. Can you share your log directory with us? You can gzip it and ideally attach it here to this issue, or email it to dsmilkov@gmail.com. 
 You should only need to do those installations if you want to rebuild the frontend from source, in which case you would do this in the source (ie git clone) directory. 
For just running TensorBoard as a user, none of those commands are necessary.
  marcotrombetti@, please do send a PR with a fix.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 the internal and OSS versions have diverged for a few days now. Also, they're different, kept in sync with a bunch of script. You can't copy any content from the third_party version into github, it won't work. You'll have to redo the change (it shoudl be possible to apply the patch to the public version you checked out from github, but no guarantees on that either).
 Awesome. I didn't know about the -N uniquification. Do we have a case like
that? I'd like to check to make sure that our website behaves identically.

On Thu, Dec 31, 2015 at 1:55 PM Sam Abrahams notifications@github.com
wrote:

> Finally got my commits squashed properly, snagging all of these
> {#anchors}. See #660 https://github.com/tensorflow/tensorflow/pull/660.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/648#issuecomment-168253516
> .
 This PR is superceded by #660, I'll close it.
  It looks (from your reported output) like this is an Anaconda issue. From what I can tell, the recommended way to install using Anaconda is to do the following:

```
$ pip install virtualenv
$ conda create --name=tensorflow_env python=2.7
$ source activate tensorenv
$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl
```
 Glad to hear it worked! One minor suggestion: sorry for the wrong link, but you should probably install from the following link to get the latest (0.6.0) version of TensorFlow:

https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl 
  Thanks for pointing this out! There's no technical reason why this should be, just an oversight on our part. I'll prepare a patch.
 Should be fixed at HEAD now.
  What version are you using?  I believe @martinwicke fixed this, and it might already be in 0.6.0.
 I'll wait for @martinwicke's reply, but that may mean it's only in git and will be released with the next version.
 Yeah, that change was after 0.6.0. If you update to master (which means
building yourself), you should have universal casts.

On Tue, Dec 29, 2015 at 2:16 PM Geoffrey Irving notifications@github.com
wrote:

> I'll wait for @martinwicke https://github.com/martinwicke's reply, but
> that may mean it's only in git and will be released with the next version.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/645#issuecomment-167889234
> .
 Closing since it's fixed in git.
  @zffchen78: when you get a chance can you take a look at this?
 LGTM
 Hi, mistobaan, please let us know how you plan to proceed on this? I can see the following work items:
1. address the accum issue raised by osdf and wchan. I do not have background to judge whether it's a bug or its your design choice. I can only review the implementation matches the specification, where your specification says accum +=; whether that matches the intended algorithm. It's up to you three to judge;
2. whether to add lr option. It's also up to you and others to discuss and agree on something. 

1 & 2 must be settled before we can add this op since it affects the op's interface / specification and will be hard to fix later.

3) gpu support and test;
4) sparse support and test;

You can get 1&2 done and merged first; and collaborate w/ others to get 3&4 done or you can incrementally get them done later.
 @Mistobaan: any updates?
 Ok, we'll probably soon have GPU testing, right @martinwicke ? :).  Then we can try pulling this in.
 @tensorflow-jenkins: test this please
 tensorflow/core/kernels/training_ops.cc:338:8: error: template-id 'operator()<>' for 'void tensorflow::functor::ApplyAdadelta<Eigen::GpuDevice, double>::operator()(const GPUDevice&, tensorflow::TTypes<double, 1, long int>::Flat, tensorflow::TTypes<double, 1, long int>::Flat, tensorflow::TTypes<double, 1, long int>::Flat, tensorflow::TTypes<double, 1, long int>::ConstScalar, tensorflow::TTypes<double, 1, long int>::ConstScalar, tensorflow::TTypes<double, 1, long int>::ConstScalar, tensorflow::TTypes<double, 1, long int>::ConstFlat)' does not match any template declaration
   void ApplyAdadelta<GPUDevice, T>::operator()(                           \
        ^
tensorflow/core/kernels/training_ops.cc:348:1: note: in expansion of macro 'DECLARE_GPU_SPEC'
 DECLARE_GPU_SPEC(double);
 ^
tensorflow/core/kernels/training_ops.cc:345:41: note: saw 1 'template<>', need 2 for specializing a member function template
       typename TTypes<T>::ConstFlat grad);                                \
                                         ^
tensorflow/core/kernels/training_ops.cc:348:1: note: in expansion of macro 'DECLARE_GPU_SPEC'
 DECLARE_GPU_SPEC(double);
 ^
tensorflow/core/kernels/training_ops.cc:352:17: error: expected constructor, destructor, or type conversion before '(' token
 REGISTER_KERNELS(GPU, float);
                 ^
tensorflow/core/kernels/training_ops.cc:353:17: error: expected constructor, destructor, or type conversion before '(' token
 REGISTER_KERNELS(GPU, double);
 (ping this thread when this is ready -- it looks like you added a commit but don't know if it's ready)
 @tensorflow-jenkins: test this please
 @Mistobaan: I think this looks good pending the GPU tests finishing -- the only other thing I think we need is for you to run

 bazel-bin/tensorflow/core/ops/compat/update_ops tensorflow/core/ops

and add that updated file to your commit for tracking backwards compatibility.
 @tensorflow-jenkins: test this please
 Looks like nothing built -- can you double check and verify that your commit compiles and passes in at least one config?
 @tensorflow-jenkins: test this please
  Duplicate of https://github.com/tensorflow/tensorflow/issues/481
 I think it was a bug somewhere in TF -- if you build from sources you'll get the fix, or in the next release that we cut.
  I just tried this with a fresh virtualenv (on a Mac), and it worked. Which version of Tensorflow is this? This should work if `tf.__version__` is 0.6.0.

I'll close this for now.
  Thanks for spotting the bug - you're right that the code uses the whole multi-layer state to compute the attention query while in the paper it was only the output! It should be corrected, but I'm not sure if your solution will work well if the cell is already wrapped in an OutputProjectionWrapper. I mean - it will work technically, but it will basically first apply the cell to the end (to get a vector of size cell.output_size) and then it'll project it back to just output_size (we should change these names too, I'm sorry for the poor choice of variable names in this function).

I see a few possible solutions. The cleanest might be to forbid the use of attention_decoder without output_projection - in fact most use-cases already specify the output projection separately. But, since this is a breaking change, maybe we should start as you suggest. I'll be happy to review a code if you want to write it, of course!
 This issue probably contributes to the bad translation results people have reported from running the translation demo code (see https://github.com/tensorflow/tensorflow/issues/550). @PrajitR, I understand it may not be straightforward to provide a fix for the general seq2seq case, but what might the fix specifically for the _translate_ demo implementation look like?
 I vote we get rid of OutputProjectionWrapper completely.  This is the kind
of problem that it contributes to.  It's also much slower than doing a
projection on the whole output.

On Thu, Dec 31, 2015 at 3:27 AM, Lukasz Kaiser notifications@github.com
wrote:

> Thanks for spotting the bug - you're right that the code uses the whole
> multi-layer state to compute the attention query while in the paper it was
> only the output! It should be corrected, but I'm not sure if your solution
> will work well if the cell is already wrapped in an
> OutputProjectionWrapper. I mean - it will work technically, but it will
> basically first apply the cell to the end (to get a vector of size
> cell.output_size) and then it'll project it back to just output_size (we
> should change these names too, I'm sorry for the poor choice of variable
> names in this function).
> 
> I see a few possible solutions. The cleanest might be to forbid the use of
> attention_decoder without output_projection - in fact most use-cases
> already specify the output projection separately. But, since this is a
> breaking change, maybe we should start as you suggest. I'll be happy to
> review a code if you want to write it, of course!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/640#issuecomment-168177312
> .
 I'm not so sure about 2 points made above. I agree with the general idea though, but before we accept the statements as facts we need to think about them.

(1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.

(2) I'm not sure using OutputProjectionWrapper is much slower. In fact I ran a few simple tests and it was at most marginally slower or the same. Eugene - do you have numbers for that?

Still - we should think about making the attention and output projection situation cleaner. And I think this will require breaking the current API -- this part was maybe just not designed well enough. Let's think how to make it right.
 > (1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.

I don't have numbers for translation, but I will say that I have used this seq2seq example pretty heavily with the attention on all hidden states from each layer. I would say with 300k steps plus I get pretty acceptable results.

Also, maybe I'm missing something here, but can't we just slice the hidden state before we input it into the attention mechanism? If we are only interested in the hidden state output of the last layer, why don't we just slice that part from the total hidden state?

Just as an update I did:

``` python
cell_output, new_state = cell(x, states[-1])

#I need to do some thinking on begin and size args
input_state_attn = tf.slice(new_state, [0, cell.output_size*(num_dec_layers-1)], [-1, cell.output_size])

attns = attention(input_state_attn)

with vs.variable_scope("AttnOutputProjection"):
  output = rnn_cell.linear([cell_output] + attns, output_size, True)
```

However, when I run it, the model blows up and I get inf perplexity after 200 steps, so clearly I'm doing something wrong. I turned down the learning rate by 10x and it still exhibits the same behavior. 

I looked into the output projection wrapper and it doesn't seem to modify the hidden state argument that is passed into it at all. Therefore, I think you are okay passing a slice of the hidden state into attention, regardless if you're using an output projection or not. 

If anyone can explain the cause of the blowup in this approach, would be very much appreciated.
 @lukaszkaiser @ebrevdo Can one of you modify the title, close, mark as contributions welcome, or similar?  "Attention modifications" isn't very informative as a title.
 I think this has been resolved (in the sense that the current setup works) a while ago, closing.
  @tensorflow-jenkins, test this please.
 This CL passes my local testing. 
 LGTM
  There's a 0.6.0 tag, but you're right, there should also be a branch so
some more parts of github's UI pick it up.
On Mon, Dec 28, 2015 at 07:17 kevin yang notifications@github.com wrote:

> Sometimes the code could not work, if people fetch the master
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/638.
 I made a 0.6.0 branch. Feel free to submit pull requests against it if you want to backport bugfixes.
  This is a subtly different issue: as far as the placer is concerned MaxPool is defined to run on GPU, so it will be placed on a GPU device if available (using hard or soft placement). However, for certain input values (in this case, where the kernel size in the depth dimension is greater than 1) it will raise an error at runtime if the op tries to execute on GPU.

**TL;DR:** For now it's necessary to annotate any depthwise max-pooling ops with a `with tf.device("/cpu:0"):` block.
 Yes, this is more about an unsupported configuration of an op, not soft placement.  Retitling this.
  This kind of question should be asked on stackoverflow, since it is not about a bug or feature request for tensorflow.  However, the answer is yes: the state of optimizers is stored in `tf.Variable` objects just like normal state, and these are saved to checkpoints.
  Looks like the issue is in `TextLineReader`, which only recognizes `'\n'` when splitting a text file into lines. I'll prepare a patch.
 Should be fixed at HEAD now.
  Sorry for the delay, can you update this to handle the conflicts?
 Sorry -- can you merge (again) and make sure to squash the commits into one?
 Closing since it looks like you've given up :(.  Feel free to ping this thread / rebase and we'll re-open and merge.
  The command is fine as-is, it just requires a recent version of adb -- I can verify that it works with ADB 1.0.32 Revision 09a0d98bebce-android (although it did not with another 1.0.32 adb binary). If you don't see "-g" listed as an install option when you type "adb" then you are not using a recent enough version.

This only seems to work here because adb ignores all but the first character after '-'. Additionally, "-g" should not be necessary unless you are using an Android 6.0 or higher device.

If you would like to to modify your PR to clarify the instructions please do so, otherwise I'll update them internally.
  This is a better question for stackoverflow: github is for issues with tensorflow that require code changes to fix.  However, the answer is `tf.Print`, or to use Tensorboard.
  wchan@, I was able to build successfully by following the "Installing from sources" instructions. This seems more like a Bazel issue than a TF issue. Make sure you have Bazel 0.1.1. Perhaps try a `bazel clean` and try building again.
 The latest supported version is 0.1.2, I think.  But yes, maybe file this bug with the bazel team.

(If you want, you could try bisecting the git commits to see where this problem started happening).
 @wchan, did you find a solution to this, did it disappear, or is it still a problem? I've not been able to reproduce.
  Agreed. Out of the box solutions require searchable things to live on
separate pages, which isn't the case for our docs right now. We'll have to
do some restructuring to make this work (well, a badly working search box
is simple, but I'd rather do it right if I'm doing it at all). It's on our
list of todos. Thanks for the report!
On Sat, Dec 26, 2015 at 14:38 Jacob Israel notifications@github.com wrote:

> I think the user experience would be better if we could have a search box
> in tensor API webpage
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/627.
 @martinwicke: Should @xmbrst be assigned for this kind of doc bug?
  Oh no!  Looks like this bug got dropped through the cracks.  Regardless of whether the initial save was valid, the load should not segfault.  Do you still have code that can reproduce this problem?
 Alas!  Sorry we didn't get notice this earlier.  I think we're getting better at not dropping bugs, but there's still a ways to improve.
  Nematode is a common classification for very dark images by the default model, so that part is not surprising.

It definitely shouldn't be crashing your phone, however. What phone/Android version are you using? Could you try repeating this with "adb logcat" running on an attached computer and attach a log, please?

Also, if you modify https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorflowImageListener.java#L41, you can set it to save the bitmaps it's actually sending to the classifier (it overwrites so only the last image is ever saved). It could be helpful to see this image as well to diagnose the problem.
 Just to note, it's actually already set to save preview bitmaps (but probably shouldn't be). This means that currently you don't need to do anything other than "adb pull /sdcard/data/data/saved_images/preview.png" to grab the latest classified preview image.
 I've tried duplicating this on a T-Mobile Samsung Galaxy S6 running Android 5.1.1, but am unable to cause any unexpected behavior. I've also tried clearing the bitmap to be completely black before recognition just to try and rule out a numerical issue in Tensorflow.

If you change the entirety of onImageAvailable() to be:

```
  @Override
  public void onImageAvailable(final ImageReader reader) {
    Image image = null;
    try {
      image = reader.acquireLatestImage();
    } catch (final Exception e) {
      LOGGER.e(e, "Exception!");
    }
    if (image != null) {
      image.close();
    }
  }
```

does the problem still occur? I'm just trying to completely rule out any of the native code (image processing or TF) contained in the app as the cause.

What provider is your phone from?

Also, does the image just look black, or have you verified that the RGB value of every single pixel is actually 0,0,0?
 Sorry for late response, just got back from vacation.

So this problem occurs on a Samsung Galaxy S6 running Android 5.0.2, but not on Samsung Galaxy S6 Edge running 5.1.1, is that correct? Which mobile providers are the respective phones from?

If so, I'm guessing it has more to do with the particular camera drivers on your phone than Tensorflow, especially since you tested with the null code above. 
 Closing due to lack of activity, reopen if still occurring.
  See https://github.com/tensorflow/tensorflow/issues/531 for the first part -- feel free to send us a change to fix this if you'd like.

See https://www.tensorflow.org/versions/master/how_tos/variables/index.html#saving-and-restoring for the second.
 We're waiting for the author to repost to github -- feel free to send a similar change soon if the original author doesn't.
  Hi Jacob,

The 'small trick' refers to the fact that the image tensor returned by `mnist.train.next_batch(100)` has the shape `[100, 784]`. Note that the batch is the innermost dimension. To use it directly, we shape the `W` tensor as `[784, 10]` to use it directly in `tf.matmul(x, W)` to produce `10` activations. So if you transposed the output of `mnist.train.next_batch(100)` to `[784, 100]` and shaped your `W` as `[10, 784]`, then indeed you can call `tf.matmul(W, x)`. Is that what you were trying and were seeing a different accuracy?
 This is a question for stackoverflow, not an issue with TensorFlow.  If the activations have shape `[batch, input]` and the weight matrix has shape `[input, output]`, only `tf.matmul(activations, weights)` makes sense.  The other way around would be a size error.  You're correct that transposing the entire graph should produce something with the same semantics, but note that _every_ part of the graph needs to be transposed, including the input pipeline, loss ops, etc.
  ok, looks good, please squash the commit into one
  We have tried to include examples for complicated functions but more examples certainly wouldn't hurt. We'll gladly accept PRs for example code in function documentation.
 Closing for now since this isn't specific enough as a Github issue.  We're still happy accept PRs improving the documentation, and please file more specific documentation issues as separate bugs.
  Should be fixed at HEAD now.
  Looks like this fell through the cracks.  Anyone know if it's still an issue?
 @sherrym: Can you take a look?
 In general, please make sure that when you use import_graph_def(), you are in a fresh new graph, not the same graph. Something like this:

first_graph = tf.Graph()
with tf.Session(graph=first_graph) as sess:
   # build you graph
   # write your graph

second_graph = tf.Graph()
with tf.Session(graph=second_graph) as sess:
  # import from the written graph
  # run compute, etc

I will take a closer look at each failure case and respond separately.

Sherry
 For Pouya's problem:

When you constructed your input_map, consts() is a map of Tensors (if you print it out, you will see that your consts() are tensors):

<tf.Tensor 'Const:0' shape=() dtype=float32>

Assign Op requires that the first input be a ref type:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/state_ops.cc#L95

To make it work, just remove your input_map.
 @yutachen's problem has the same root cause.

@ingwar, if you are passing in input_map as well, you problem probably has the same root case as well.

Sherry
 If your problem is different from the ones encountered by @po0ya, @yutachen, and @ingwar, and you have followed my instructions, please include your complete program.

Thanks,
Sherry
 Assigned to @mrry since he wrote the instructions. :)
 I think the appropriate fix is to change the tensors that you're remapping in `input_map` from the (names of the) variables themselves to (names of) the _values_ of the variables. I think you can achieve it by replacing the following loop:

``` python
for v in tf.trainable_variables():
    vars[v.name] = sess.run(v)
```

...with this code:

``` python
for v in tf.trainable_variables():
    vars[v.value().name] = sess.run(v)
```
 @pannous Please feel free to open a new issue with a feature request. You might also find the [`freeze_graph.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) script useful for this purpose.
  This is probably a question better suited for stackoverflow. Short version: `write_graph` only writes the graph, not the variable content. You need a `Saver`, see for instance:  http://stackoverflow.com/questions/33689598/how-to-pause-resume-training-in-tensorflow/33690809#33690809
 Take a look at python/tools/freeze_graph.py, which takes the contents of
variables and saves them to the graph as constants. Then when the graph is
loaded it comes with all the data.

On Sun, Feb 14, 2016 at 10:00 AM Hamed notifications@github.com wrote:

> @martinwicke https://github.com/martinwicke I run the loader.cc example
> and it's okay,
> But I have one question which enables me to use c++ with my own model.
> 
> As the tutorial
> https://www.tensorflow.org/versions/v0.6.0/api_docs/cc/index.html I
> wrote the graph and have the file, but in your previous comment you are
> saying that the write_graph only saves the architecture, but not the
> trained model, so:
> 
>    1.
> 
>    If this is the case, so how your Inception model
>    https://www.tensorflow.org/versions/v0.6.0/tutorials/image_recognition/index.html
>    tutorial works?? There you only load the graph to the model + labels and
>    the test image. Where is the saved wights there?
>    2.
> 
>    If not and you should load the model, then how?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/615#issuecomment-183938907
> .
 I think we should take this discussion to stackoverflow. Can you post there and add a link here? We shouldn't abuse the issue tracker for questions. I'll leave this issue open as a feature request for a tutorial on graph import/export.
 I think a new issue would be good, I don't think we have a fix for this yet.

On Fri, Feb 19, 2016 at 7:30 AM Hamed notifications@github.com wrote:

> Yes, of course, here you are @martinwicke https://github.com/martinwicke
> :
> http://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c
> 
> Btw, I want to inform you that the freeze_graph() didn't work because of
> lacking graph_util.convert_variables_to_constants() method in the wheel
> installation by pip for Mac. I managed to run it by cloning and building
> binary. May be I need to open a new issue but I think you will update the
> wheel soon so I didn't pile a new issue on top of them now :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/615#issuecomment-186260605
> .
 👍
On Sat, Feb 20, 2016 at 03:17 Hamed notifications@github.com wrote:

> Which issue you mean?
> As @vrv https://github.com/vrv said in #1199
> https://github.com/tensorflow/tensorflow/issues/1199 he will look for
> the graph_util.convert_variables_to_constants().
> The other problem about the difference I think it's only a matter of
> documentation.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/615#issuecomment-186574996
> .
  Thanks!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
  Looks like this fell through the cracks.  When you say you submitted a contribution, did you make a PR?  There doesn't seem to be any programmatic mention of a contribution here.
 @jskDr: Closing for now due to lack of response, but please open if you still have a contribution! 
  I think we're just missing some declarations of the GPU kernel of Reverse for int32 here:

https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/reverse_op.cc#L148

Whoever fixes this should probably add tests for all supported types here:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/array_ops_test.py#L45
 InvertPermutation does not currently have a GPU kernel at all. We may be
able to cheat our way to one with an appropriate HostMemory declaration, or
we have to properly implement a GPU kernel. Either way, it would be in
transpose_op.cc.
On Sat, Dec 26, 2015 at 17:41 Mohammed AlQuraishi notifications@github.com
wrote:

> Reverse is not the only thing causing the problem though. In fact the
> problem with Reverse could be circumvented with an explicit term. What
> can't be circumvented is the InvertPermutation error.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/610#issuecomment-167374746
> .
 Except for error checking, it's much easier to implement `InvertPermutation` on GPU now that we have code for `tf.scatter` on GPU.  A bit of refactoring would probably let `InvertPermutation` reuse the functors for scatter.
  The [get started page](https://www.tensorflow.org/versions/master/get_started/os_setup.html#source) recommends to run [convolutional.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py) as the first tensorflow neural net model:

```
$ cd tensorflow/models/image/mnist
$ python convolutional.py
```

However, running it brought my machine down. After bringing it up again, the NVIDIA driver didn't work properly anymore (the boot messages showed nvidia-related errors and nvidia-smi became slow). I fixed it by booting into single-user mode and reinstalling the NVIDIA/CUDA drivers.

My desktop has two GPUs of type GeForce GTX 780 Ti with 3GB memory each.

I assume that `convolutional.py` uses too much GPU memory. Maybe a gentler script should be used in the getting-started manual.
 A simple way to shave off memory is to replace the giant testset loaded in a constant with a placeholder that gets fed with a small loop at evaluation time. I didn't do that to keep the code as simple as possible, but since several people have commented on having this issue, it might be worth changing it. I'd like confirmation that this would actually fix the problem however; @markusdr would you be able to hack that together and let me know if that solves your problem?
 Yes, using fewer images for the validation set and the test set solves the issue. I tested with 500 images for each of the two data sets, and it worked well.
 @gotope Here is the diff:

```
--- a/tensorflow/models/image/mnist/convolutional.py
+++ b/tensorflow/models/image/mnist/convolutional.py
@@ -40,7 +40,7 @@ IMAGE_SIZE = 28
 NUM_CHANNELS = 1
 PIXEL_DEPTH = 255
 NUM_LABELS = 10
-VALIDATION_SIZE = 5000  # Size of the validation set.
+VALIDATION_SIZE = 500  # Size of the validation set.
 SEED = 66478  # Set to None for random seed.
 BATCH_SIZE = 64
 NUM_EPOCHS = 10
@@ -126,8 +126,8 @@ def main(argv=None):  # pylint: disable=unused-argument
     # Extract it into numpy arrays.
     train_data = extract_data(train_data_filename, 60000)
     train_labels = extract_labels(train_labels_filename, 60000)
-    test_data = extract_data(test_data_filename, 10000)
-    test_labels = extract_labels(test_labels_filename, 10000)
+    test_data = extract_data(test_data_filename, 500)
+    test_labels = extract_labels(test_labels_filename, 500)

     # Generate a validation set.
     validation_data = train_data[:VALIDATION_SIZE, :, :, :]
```
 Evals should now be batched, pending propagation of the changes.
Feel free to reopen if you see more memory issues.
 I don't think it's been pushed yet -- will leave it open until it is.
 Ok, pushed.
  https://www.tensorflow.org/versions/master/api_docs/python/nn.html#l2_normalize , or an equivalent construct? (You should be able to compute the appropriate norm and scale using reductions / elementwise ops, right?)
 There may be a better way to do it, but if I understand correctly, you may be able to use tf.select to do what you want (may not be the most efficient thing to do memory wise).

Assuming:
w_gradient is the matrix representing the gradients of shape [m, n]
w_gradient_column is whatever reduction function along the column dimension (shape [1, n])
w_tensor is the original weight matrix
w_scaled_tensor is w_tensor with the normalization applied for all columns

First you compute which columns to apply the rescaling to
condition_per_column = tf.greater(w_gradient_column, threshold)

Next you replicate that value along every row to make a boolean tensor of the same shape as w_tensor
condition_tiled = tf.tile(condition_per_column, [m, 1])

Lastly, you create a resulting tensor that applies a mask to select the right value from each tensor.
result = tf.select(condition_tiled, w_scaled_tensor, w_tensor)

Maybe this still isn't what you want, but hopefully you get the point: you can use tf.select and comparison operators to select/mask results conditionally.
 I think a custom op for this is overkill.  Just do `matrix * tf.expand_dims(threshold / tf.maximum(threshold, norms), 1)`.  Hmm.  I guess for norms one has to do `tf.sqrt(tf.reduce_sum(tf.square(matrix), 1)` which isn't pretty, but if any new op is added here it should be an L2 norm op (or possibly some extra arguments the existing `L2Loss` op), not a normalization op.
  @danmane, @dsmilkov: let us know if this is okay to merge
 At present this pull request only changes a compiled file (dist/tf-tensorboard.html) but not the corresponding source file (components/tf-image-dashboard/tf-image-grid.html).

Please change the source as well and then this will be good to merge.
 @danmane: still looks good?  if so i'll merge
 Looks good. Thanks! :)
  You can uncomment Ln270 of batch_matmul.cc to try it out on your machine.
We had mixed experience of the implementation (some reported working great,
some reported it rans out of gpu ram from time to time).

On Fri, Jan 15, 2016 at 3:31 AM ponythewhite notifications@github.com
wrote:

> I would also greatly appreciate this to work on GPUs.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/605#issuecomment-171938542
> .
 This was fixed a while ago.  Woot.
  https://github.com/tensorflow/tensorflow/pull/604
  Looks like this fell through the cracks.  @danmane: Any thoughts? 
 @dsmilkov and @jameswex are point people on graph visualizer issues - I know we've fixed a few issues of this general form, but automatic grouping for repeated nodes isn't perfect yet. Can one of you comment if we should close this issue? Also, would grouping all of these ops under a shared namespace help, or just kick the problem down into a new unmanagably massive namespace?
 I'm going to close this for now since we don't have a repro and we don't know if it is still reproducing. @jstaker7 please re-open if you find a repro.
  You should not need to run modprobe in the container, only on the host.
Once that runs, the /dev/nvidia\* devices are accessible and you can launch
the container.

On Sun, Dec 27, 2015 at 3:52 PM, DC notifications@github.com wrote:

> i was able to build on my local machine for now. going to try again with
> docker later, but i think the issue was my host machine configuration. for
> some reason the docker container is not permitted or cannot access the
> nvidia drivers.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/601#issuecomment-167449139
> .
 @martinwicke, @aselle: Who's the best person for Docker issues?  This issue seems to have fallen through the cracks.   
  Hi Marco. I think what you see is not a bug. For one, 2-layer network of size 256 is simply too small for translation. Second, I don't think you train it long enough. And neural network based methods require more data than hand-programmed systems like Moses. As specified in the tutorial - try running a 3-layer network with size 1024 for ~300-400K steps -- I think you'll see better results.
 Slightly related: How much memory do you guys have on your GPU? I have 3GB on my GPU, and 3 layers with size 1024 runs out of memory.
 Indeed, it runs out of memory on my 970 too. You can try to lower the batch size,  but it might become too slow. We're working on making TensorFlow more memory efficient but it will take a while. It does fit into 12GB, so you can run it on a Titan X / Black or Tesla K40 / K80, but these are quite expensive.
 It might depend on your hyper-parameters. The tutorial model is not tuned and has a lot of data, so it would probably take months to fully converge. But you can tune it to converge faster, and it should get to reasonable perplexity in a few days anyway. Also, real-time is hard to jugde, can differ from machine to machine. What's your step number and perplexity -- that's the question to ask (and some step-number and perplexity ballpark are included in the tutorial).
 Hi! Can you try using this option: --max_train_data_size=500000 ? It should reduce the training data size, and currently the whole data-set is loaded into memory. You can also modify the data-loading function to load it batch-by-batch instead of loading it all into memory at once.
 This is strange: usually it'd start at the latest checkpoint. It's all saved in the --train_dir directory and the "checkpoint" file there specifies what's the latest checkpoint. Could it be that you used /tmp and your checkpoint got deleted, e.g., on restart?
 I think the perplexity needs to go to around 4 for the results to be good. On a single GPU, this can take about a month of training with this simple tutorial script. There is a lot that can be sped up there, and better parameter settings can help, but I hope that gives you a general idea for the expectations.
 It will, but the automatic placement currently might not be optimal. You can place manually with tf.device and that could make it faster (esp. if you place different LSTM layers on separate GPUs, as they can often work in parallel). I think this is a good question, but this bug is not the best place for it, as it's mostly unrelated and will be hard to find. If you want to ask about speeding up training, maybe open a stack overflow thread?
  Like the other one, this needs fixing at the source: https://github.com/tensorflow/tensorflow/blob/091ed8cd4db82212fc6395ce4f1edf6b55ea2181/tensorflow/core/ops/array_ops.cc#L583
  This file was removed in commit f7918e1dcd5b0c1f8114f488fc35a63a81e94535, so it's possible that you have a version conflict. (It sounds like you have an old version of some of the TensorBoard assets.) Can you try upgrading to TensorFlow 0.6.0 and see if this solves the problem?
  What are you referring to? 
 Closing due to lack of activity.  Feel free to reopen if more information is available.
 @shyamalschandra: Martin asked you for specifics and you didn't reply, so I closed it due to lack of activity.  We would be happy to add specific references to papers to specific places in the code, but a blanket "Add more documentation" bug is hard to work with.
  You actually have to update it here: https://github.com/tensorflow/tensorflow/blob/8b5d9ed13f188662dde7cce75362cd2394bde40c/tensorflow/python/ops/image_ops.py#L115

then when we re-run our doc generation script, it will be fixed.
  Hm, looks like we're pretty inconsistent about default args for our optimizers.  In Adam we set default values for all parameters (including learning rate), for others, everything but learning rate has a default, and here everything but learning rate and decay has a default.  

I'd be okay with this, since 0.9 seems to be a pretty common default from what I can tell, but @vincentvanhoucke for validation.

(You should add documentation specifying the default in the docstring, at least).
 Looks reasonable. @vrv what's the protocol for this? Looks like I have the right permissions to merge the pull request, is that something I can just go ahead with and all the merging takes care of itself?
 For simple changes like this, yeah.  But I would recommend that @aymericdamien sqaushes the commits into one first before merging.
  Thank you for reporting this issue.

While is not a documented feature, partly because of issues like this.  We
will work on it but make no promises as to when it'll be ready.

On Wed, Dec 23, 2015 at 4:15 PM, Angel Darquea notifications@github.com
wrote:

> +1
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-167012425
> .
 You should be able to rerun your python test directly from bazel-bin
between changes (the underlying code should be symlinked to the source
code) without rerunning bazel test ....

On Wed, Dec 23, 2015 at 7:35 PM, trubin notifications@github.com wrote:

> Cool. I'm happy to help work on it. Vijay suggested I post the bug in case
> you guys were changing things there, just so you knew I was doing it. I'm
> still having trouble getting my dev environment set up (being able to just
> test the python changes without bazel recompiling all the c++ every time I
> make a small change to the python code), but hopefully I'll get that
> resolved soon.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-167036212
> .
 This should have been fixed at head.
 Yes, we made some major changes to support nested loops.  There is no pending major changes in the pipeline for now.  You are welcome to take a look at this problem.  
 (I do suggest trying to get a PR in in the next couple of weeks, though)
On Mar 21, 2016 4:18 PM, "Yuan Yu" notifications@github.com wrote:

> Yes, we made some major changes to support nested loops. There is no
> pending major changes in the pipeline for now. You are welcome to take a
> look at this problem.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-199531386
 This is a known limitation :(

This will require some revamping of the TensorArray and a bit of additional
work, to make it happen.  No ETA right now.

On Wed, May 18, 2016 at 3:59 PM, Sherjil Ozair notifications@github.com
wrote:

> Tensorflow is still not able to handle backprop over nested scans.
> 
> @yuanbyu https://github.com/yuanbyu @ebrevdo
> https://github.com/ebrevdo, any updates on this?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/593#issuecomment-220182751
 ACT only requires a while_loop inside the rnncell, not a scan, and does not require TensorArray except in the outer dynamic_rnn call. You should be able to implement it without running into this bug.
 Still no eta for fixing nested scan.
 nested functional ops should now work (or at least the change will be pushed to github w/in the next week).  reopen if you're still having a problem after the next push.
  You can pass the indices and values to print separately.
On Dec 22, 2015 9:47 AM, "William Chan" notifications@github.com wrote:

> Is this expected? because an embedding gradient is an IndexedSlices rather
> than a Tensor
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/591.
 Seems like a reasonable feature to have.
 @Mistobaan: Not sure what you mean by a minimal code example.  The desired behavior would be that `tf.Print(input, data)` works even if `data` contains a mix of `IndexedSlices` objects and things that can be converted to tensors, and the desired outcome would be output that looks something like

```
# tf.Print(_, [xs, IndexedSlices(ys, zs)])
... [x...] IndexedSlices([y...], [z...])
```

Since it would be nice if the new functionality allowed `Tensor` and `IndexedSlices` to be mixed in the same call to `Print`, the best way to implement it might be to add an optional list of extra strings to `Print`, perhaps as a "extras: list(string) = []" attr.  The above would then be implemented in Python as something like

```
gen_logging_ops._print(_, [xs, ys, zs], prefixes=["", "IndexedSlices(", ", ", ")"])
```

inside the Python wrapper version in `tf.Print`.
  I believe this has been fixed, closing. Comment if it's still a problem in the recent 0.8 release.
  A test for this might be nice.
 @mrry and @lukaszkaiser might have better answers for you here, though most of us are on vacation for the next few days / weeks.
 How would this be used?  If you already know the whole value including shape, I thought you could just pass the value directly where the initializer would otherwise go.
 I'm planning to work on adding an option to just pass a value instead of an initializer in tf.get_variable. Maybe this initializer will not be needed then. Currently you can use "lamda x: x" as initializer, but we should have a better way soon.
 I've just made a CL that allows to use a Tensor as initializer in get_variable (should appear soon on git). I hope this closes this request, but feel free to re-open if needed.
 @lukaszkaiser: let's only close issues once they are publicly available

(i am pushing soon)
  Jenkins, test this please.
 Assuming the tests pass, LGTM.
 That is, after the line comment.
 Cool, can you squash your commits?  We'll use this change as a way to test our eventual GPU test slave, maybe.
 Thanks! I'll wait to test until our GPU test machine comes online to test this. Should be today, I hope.
 Sorry this is taking so long. Hardware is hard.
 Jenkins, test this please.
 Jenkins, test this please.
 doh, port.h was removed -- @panmari you'll probably have to update the code one more time :(
 On the plus side, we finally have working GPU tests now.

On Tue, Feb 16, 2016 at 1:42 PM Vijay Vasudevan notifications@github.com
wrote:

> doh, port.h was removed -- @panmari https://github.com/panmari you'll
> probably have to update the code one more time :(
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/588#issuecomment-184885352
> .
 What's the failure in particular?
 @panmari: isn't that specifically the kernel you added that is showing different behavior between CPU and GPU?  That probably needs to be fixed, unless I'm mis-reading the error.
 @tensorflow-jenkins: test this please

(i swear, we'll get this in!)
 Merged, woohoo!
  Thanks for the update. Closing as it's not an issue.
  I'd prefer not to have two ways to access (subtly different forms of) the same information. However, I wouldn't be opposed to changing `Tensor.get_shape()` to be a `Tensor.shape` property. Ideally you could use a (fully defined) `TensorShape` anywhere a list of integers is accepted. Would that work for your purposes?
 There are no plans to change the use of `TensorShape`, and we're happy to accept suggestions/PRs that would make `TensorShape` usable wherever a list of integers would be.
 Tensors in TensorFlow can have dynamic shapes. `TensorShape` is used throughout the Python API to represent shapes that might have one or more unknown dimensions, or even an unknown rank; and to combine such shapes easily.

So, yes, I'd be opposed to having two slightly incompatible properties/methods on `Tensor` that convey the same information, but cannot be used the same way. If we add `Tensor.shape`, it should return `TensorShape`, and we should deprecate `Tensor.get_shape()` at the same time.

Which tensor initialization function doesn't support `TensorShape` objects as the `shape` argument? We should fix that.
 @mrry: Should this be contributions welcome, or do we still have plans to do our own refactoring of `.shape` vs. `.get_shape`?
 We can't&mdash;or, at least, _really_ shouldn't&mdash;accept a contribution on this until the internal refactoring is done. However, the internal refactoring is P3, so it might be some time before it rises to the top of the pile.
 Sounds good, let's leave it as is.
  Can you try again with the latest release?

``` bash
$ pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl
```

If that doesn't work, can you also please share information about your Python version?
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#pip-installation-pip_install -- did you use pip3 ?  Full command line and error message would be helpful.
  Thanks, this is nice. Sadly, we won't be able to use the PR, because all the HTML is already auto-generated. I will add something like this to our processing pipeline. I'll probably go for the hover chain icon though. I'll leave this open until I actually do this.
 Eventually I want the whole doc generation pipeline to be open source, so
people can preview their edits properly (and can fix issues in the
processing), but we're not quite there yet.

I'll get to this eventually.
On Fri, Dec 25, 2015 at 13:21 Sam Abrahams notifications@github.com wrote:

> Cool, thanks for getting back on this. I should have realized that the API
> doc was auto-generated, whoops!
> 
> Should be a fairly easy tweak on your end. Let me know if there's anything
> related to this you'd like to delegate.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/pull/584#issuecomment-167262917
> .
  Can you try with a plan that is naively paralelizable to make sure it's not communication bottleneck somewhere?

For instance, rename the attached file into "parallel.py" and then run it as "python parallel.py <numparallelism>". On my macbook pro I get following numbers which shows linear scaling up to 4 cores

python ~/g/src/parallel.py 1
done in 0.99, 10.13 ops/sec

python ~/g/src/parallel.py 2
done in 0.97, 20.58 ops/sec

python ~/g/src/parallel.py 3
done in 1.02, 29.55 ops/sec

python ~/g/src/parallel.py 4
done in 1.04, 38.29 ops/sec

python ~/g/src/parallel.py 5
done in 1.33, 37.45 ops/sec

[parallel.txt](https://github.com/tensorflow/tensorflow/files/70372/parallel.txt)
 This could be related to https://github.com/tensorflow/tensorflow/issues/551: the code ends up spending a lot of time locking and unlocking a mutex in the threadpool code.
 @songgc https://github.com/tensorflow/tensorflow/commit/ab02c5ab2f1f10bc9e51f02f5125abed449cae87 combined with a few other changes to make sure all the TensorFlow operations are multithreaded resulted in a 1.5x to 3x improvement on most benchmarks. Is that what you're seeing?
 As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this bug. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.
  Could you please try writing fileContent to a file to make sure that you have successfully read the file content? Thanks.

Sherry
 I am unable to reproduce the problem.

Sherry

On Tue, Dec 22, 2015 at 6:11 PM, ry notifications@github.com wrote:

> Same code was working before the update. Were you not able to repeat the
> bug with the above?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/582#issuecomment-166784764
> .
 FYI, we recently added custom protobuf pip packages that should overcome the 64MiB limit and also provide fast processing in python: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues
  I have upstreamed a partial fix to Eigen (see https://bitbucket.org/eigen/eigen/commits/f6e3953e6b44a8a0bfde7186cc162a612a904e15 for details).
I'll work on the second half of the problem after the Christmas break.
 This will be fixed with pull request https://github.com/tensorflow/tensorflow/pull/847
 The fix was merged. Closing this issue.
 Reopening since we had to rollback the change.
 I have another set of fixes pending in pull request https://github.com/tensorflow/tensorflow/pull/991
 Pull request https://github.com/tensorflow/tensorflow/pull/991 is now merged in the codebase. I'm closing this issue.
 @benoitsteiner @wicke @vrv 

Update: The build agents have Xeon CPUs, which don't support AVX2. Perhaps that's the cause of the failures below. If this is confirmed, there is no action required on this issue.

Not sure if a new issue should be created since it has been three months since this issue was closed. But today I noticed a large number of test errors while doing 
`bazel test -c opt --copt=-mavx2 //tensorflow/...`
on the master branch, as a part of the effort to add mavx/mavx2 build to Jenkins.

> Executed 414 out of 416 tests: 356 tests pass and 60 fail locally.

The logs from the individual failed test failures are not especially informative. For example: 

> ==================== Test output for //tensorflow/python:matmul_op_test:
> external/bazel_tools/tools/test/test-setup.sh: line 52:  7125 Illegal instruction     (core dumped) "$@"

Testing with --copt=-mavx or no AVX flag didn't see any failures.
 Do Jenkins machines have avx2 support?
 @vrv see updates above. They have Xeon CPUs, which aren't supposed to support either AVX or AVX2?But mavx build passes. 
 Which Xeon?  AVX is pretty old, avx2 is relatively new -- I'd expect avx support on many machines but not avx2.
 Just looked it up: The slaves we have in the particular GCE region have Ivy Bridge Xeon, which supports AVX but not AVX2. So this explains it.
  Simple changes / fixes like that seem fine to me -- I'll route it to the appropriate reviewer.

Note that a lot of the team is out this week and next.
  As @martinwicke mentioned in #558, this change has to be made internally in our website generation code.
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
  Let me try kicking off the test for this PR by saying some magic words
 test this please
 Looks generally fine, assigning to Sherry, the original reviewer.
 @tensorflow-jenkins, test this please.
 (Can you squash the commits if the tests are all passing?)
 @tensorflow-jenkins, test this please.
 Merged (trying a new workflow where I merge manually via rebasing rather than clicking the button for a better history).
  Fixed by:
Change-Id: I47d8536b9b2ed3dcc193d6e6b7f4573a4e22c9b3
 We abandoned the gerrit one -- if https://github.com/tensorflow/tensorflow/pull/573 looks good, we can merge
  Yeah we just merged #568 -- do you want to update change or just send a new PR for the .gitignore?

Yup, we're working on continuous integration tests with python 3 to prevent these regressions.
 (re-open if you'd like)
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 #568 just did this and was a cleaner commit, so we pulled that one instead.  Thanks for the contribution though!
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 CLAs look good, thanks!

<!-- ok -->
 LGTM
  Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 @jendap: done, thanks!
  There's a related issue here: https://github.com/tensorflow/tensorflow/issues/1258
That issue is still open, so I suspects there's no way to change logging settings other than modifying the code and recompiling. For instance, here are instructions on how to change verbose logging level here -- http://stackoverflow.com/a/36505898/419116
 @vrv, @rekhajoshm: What's the status of this after #619? 
  Take a look at this section:

https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-samplers

The sampled softmax loss is here:

https://github.com/tensorflow/tensorflow/blob/20723e2b3d58cc48b2a302f7ea9806c8a75fd18f/tensorflow/python/ops/nn.py#L778

For example used here:

https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/models/rnn/translate/seq2seq_model.py#L98
 @ludimagister, @wchan: From the discussion above it seems like this was a resolved question, especially since #2093 is closed.  Please let me know if we should reopen!
  No. When using ReLUs (or linear) activations, because they are scale-invariant, the batchnorm gamma is redundant with the scale of the weights of the subsequent convolutional or fully-connected layer, so we don't train them.
  I don't think there is a bug. Indeed, softmax_cross_entropy_with_logits does not take weights, and we're working on a loss function that will take integers directly. But in sequence_loss_by_example we do take weights into account later, in the following line:

``` python
  log_perp_list.append(crossent * weights[i])
```

Does this help, does it look ok to you?

Lukasz
 Yes - the backprop starts at the loss function you pass to tf.gradients, not anywhere else, so you can just scale like this. Merry Christmas :).
  Can you try installing the version 0.6.0 package and see if this problem persists?

https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
 Sorry to hear that! If you're used to running on Windows, I'd suggest trying the Docker packages, which should avoid any complications with your Linux configuration. Here are some [instructions](http://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/) that might be helpful.

Since it sounds like the problem is related to your particular machine's configuration, I'm going to close this issue for now, but please feel free to reach out if you have other problems.
  @ebrevdo: I don't think `tf.nn.linear` is supposed to exist, but `models/rnn/linear.py` uses it.  Is this because we're missing a unit test?
 linear is deprecated and shouldn't be used by external users. I'll look
through and see if we use it anywhere but internally in tf.nn.
On Feb 22, 2016 9:54 AM, "Geoffrey Irving" notifications@github.com wrote:

> @ebrevdo https://github.com/ebrevdo: I don't think tf.nn.linear is
> supposed to exist, but models/rnn/linear.py uses it. Is this because
> we're missing a unit test?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/561#issuecomment-187292823
> .
 Closing because folks shouldn't be using linear.  But let me know if this breaks any existing models in python3 and i'll reopen.
  Yeah, we still don't yet have good testing for python 3, sadly.  Want to send us a pull request to fix?
 Thanks!  we merged it.
  When I ran into this, it was because I had another version of numpy installed on my machine in a different location -- I had to uninstall the other (older) version.
  Putting title annotations in the md like that won't work (unfortunately).
We have to fix this here.
On Sun, Dec 20, 2015 at 16:44 Rekha Joshi notifications@github.com wrote:

> #575 https://github.com/tensorflow/tensorflow/pull/575
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/558#issuecomment-166168638
> .
 @martinwicke: It looks like the pages have titles now.  Want to mark as fixed? 
 Fixed.
  Hi @srjoglekar246, this looks pretty cool, and thanks for going through the work to put this together!

One of the ideas we have been working on is a notion of "Functions" in the GraphDef, which would allow for re-usable components.  See [here](https://github.com/tensorflow/tensorflow/blob/d6357a5849db980df51d00d8a9ff874cda2faeb3/tensorflow/core/framework/function.proto#L14) for the proto definition.

It's not ready yet and still needs some work (we may not do this as a proto in the GraphDef) but the underlying idea would be something that would obviate the need for copying elements around with unique names: you could define a subgraph as a function with inputs and outputs, and then re-use them.  So instead of having multiple versions of the graph stamped out at once, each with unique names, you could have one named function that could be called, ported across graphs, etc.

Do you feel that such a feature would accomplish your higher level goals?
 Yeah, there's some more plumbing in the internal execution of graphs that specially handles functions.

For now, do you mind if we keep this pull request open but on the backburner, at least until we figure out whether functions may be an easier to use abstraction?

(If you really want this checked in somewhere and know lots of others are using this, we've been intending to create a 'contrib' directory or repo where these types of utilities / functions could be placed).
 We've considered adding a contrib directory but ownership and bug reports would be hard to manage -- probably needs to be a separate repo.  Adding @martinwicke since I think he's in the process of figuring this out.

There's been more progress over the past few weeks on functions.  I think it's close -- take a look at an example:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function_test.py#L279  and let us know if that would sort of accomplish what you want
 @srjoglekar246, @bhack: we'll try to find something that works for contributions.

As for functions: I'm not entirely sure what the state of it is, but I wanted to solicit early feedback from you since it seems like what you originally were trying to do with this PR.  It's being actively worked on so I'm hoping it will be ready "soon".
 Sorry for the long silence -- if you're still interested, I'd like to merge this into contrib. Can you move the file to tensorflow/contrib/copy_graph/python/util/copy.py?

Also, can you add a license header and the python3 from **future** imports, and can you add a test for this? 
 Just make sure that it tests the functionality you claim your functions provide.
 And can you modify the docstrings to match the tensorflow style guide (look at the "writing documentation" howto)?

Thanks!
 Jenkins, test this please.
 I've had some minor comments about python module things, but otherwise looks good.
 Thanks! 

Jenkins, test this please.
 Thanks!
  Closing this, since it seems related to proxy settings that we can't reproduce. Let us know if you have any more problems.
  The classify_image_graph_def.pb is built explicitly to support a batch size of 1, and cannot support other batch sizes. But nice effort.

Sherry
 In an already built graph, no. Sorry.

On Mon, Jan 18, 2016 at 9:17 AM Дмитрий notifications@github.com wrote:

> Does exist easy way to remove this limitation?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/554#issuecomment-172595552
> .
 That would be a major headache. It's probably possible, but I wouldn't
recommend it. If any of the variables have to change shape for the
different batch size, your checkpoints become useless or you have to
manually add resize or reshape ops.

On Tue, Jan 19, 2016 at 9:33 AM Дмитрий notifications@github.com wrote:

> Those, because graph already built, it is impossible to cut out a part of
> saved graph and connect it with a new in "inference" stage?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/554#issuecomment-172927407
> .
  This looks like a duplicate of issue 548: I'll take a look and confirm whether or not this is the case.
 I believe this is fixed: I am now getting about 600 samples per second with a GTX Titan. Please reopen if this is still a problem for you.
  We are aware of the fact that there are cases where we shard the processing of the Eigen tensors too much, and plan to address this in the future. In the meantime, I'd be very happy to replace the current implementation of the ThreadPool class with a lock free implementation if you can contribute one. It would help speed things up quite a bit.
 As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this bug. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.
  To get reasonable translations you need to train a reasonably large network (say 3 layers with 1K nodes) for at least a few hundred thousand steps. Indeed, the perplexity numbers are just a guide to what's happening, but you can get decodings from the binary as well (or even add your own BLEU score computation using the same function). Did you try to re-run the tutorial setting? That was one decoding I got when training the tutorial before first release:

```
Reading model parameters from /tmp/translate.ckpt-340000
>  Who is the president of the United States?
 Qui est le président des États-Unis 
```

We should be able to get at least something like this, if not, there might be some new bug to correct. I'll start a run to check it, but it usually takes about a week to get to this point, so please, post here if you get some numbers earlier.
 I ran the tutorial model over the weekend + 1 day on a K40. It got to 193K steps (3 layers by 1024) and to reasonable perplexity and samples.

```
global step 193000 learning rate 0.0861 step-time 1.70 perplexity 4.17
  eval: bucket 0 perplexity 5.02
  eval: bucket 1 perplexity 3.27
  eval: bucket 2 perplexity 3.51
  eval: bucket 3 perplexity 4.82

Reading model parameters from /tmp/translate.ckpt-193000
> Who is the president of the United States?
Qui est le président des États-Unis ?
```

So I think it works as expected. Better results can be obtained if you tune the model, hopefully also after we close #640 or with more training time. But what it produces looks reasonable to me, so I'm closing this issue. It would be great if someone could confirm what I see -- please re-open if it does not work for you.
 I think it's not too hard now, just look for perplexity to go down to around 4, maybe 4.2 -- and the lower the better, esp. on more complex sentences. But indeed, it's a bit of a magic number and some examples might be good to have. Code contributions are welcome, just send your CL!
 It would also be helpful to output BLEU scores on the dev data set at each checkpoint.

Someone should train on the same data as this [Moses baseline](http://www.statmt.org/moses/?n=moses.baseline) and compare the final BLEU scores when decoding the test set (23.5 with Moses). That would be a great comparison point and sanity check; it should also train fast as the data is not that large. (Use the same tokenizer as given there instead of the tutorial tokenizer in data_utils.py). 
I've tried it myself, but a model of size 1024 with 3 layers doesn't fit in my 3GB GPU memory, and I haven't figured out yet how to get reasonable performance with a smaller model.
 One thing that needs to be remembered is that neural translation is a young and active area of reseach -- it's not like there are established ways for everything. Let me comment on two points.

(1) Perplexity vs BLEU. In a number of recent papers on neural translation many researchers report that perplexity correlates well with BLEU scores. Some papers even claim that perplexity correlates better with human judgement of translations than BLEU. But these are all just imperfect measures, sometimes imperfect in different ways. Looking at a few self-chosen samples is also an imperfect measure: the ones you consider might just be more outside of the training data (which comes from newspapers if I understand right), but not really harder. So in the end one needs to look at all of them to get a reasonable measurement and think hard what they mean.

(2) OOV words. This is a well-known problem in neural translation models and a there are ongoing research efforts to deal with it. One way that seems to work well is having multiple UNK tokens and doing some replacements, see this paper: http://arxiv.org/abs/1410.8206 . In a basic model like the one in the tutorial the UNK is heavily over-represented in the training data which leads to bad decodings and a bit "cheated" perplexity, as you said. You can try to just decrease UNK probability in the training by using a few different UNKs, or the method from the above paper, or something else entirely.

I'm writing about these points to illustrate one thing: this is an ongoing research effort. We constructed the tutorial to show how to get started with these kinds of models in TensorFlow, but all these research questions remain open. You can now see where the problems are and start working on your own solutions -- that's the intention!
 Just for completeness, I ran the queries you suggested through my model.

```
> What is your name ?
Quelle est votre nom ?
> What is my name ?
Qu ' est-ce que mon nom ?
> How are you doing today ?
Comment _UNK aujourd’hui ?
```

So indeed - there is a strange _UNK there, but it's not bad, right? In fact quite funny in some ways - I think everyday queries are really a bit outside of the training data used there.
 We made our process for accepting contributions as quick and painless as we could, and there are already many developers submitting their CLs. I suggest you try, and let me know personally if you have any trouble -- we want to make it as easy as possible for anyone to contribute!
 @aliabbasjp It looks like your dev data is very different from your training data. Maybe there are many unknown words given the vocabulary size you picked. Try running with dev data that is closer to your training data, e.g., split 10% off your current training data and use it as dev data.  
 I think the comment about the difference between your dev and train data came from your log output like this:

```
global step 374600 learning rate 0.0069 step-time 1.92 perplexity 1.02
  eval: bucket 0 perplexity 137268.32
```

Your training perplexity is 1.02 -- the model is basically perfect on the data it receives for training. But your dev perplexity is enormous, the model does not work at all. How did it look in earlier epochs? I would suspect that there is some mismatch in something. Maybe the tokenization is different for train and dev? Maybe the sizes of the buckets from the original translation model are not appropriate for your use-case? If you share more details, I'll be happy to take a look and try to help (maybe better on the original bug).
 Yes, train more. From the seq2seq tutorial: "one epoch (going through the training data once) takes about 340K steps with batch-size of 64. At this point the model can be used for translating English sentences to French". You're at step 46K, that's not even 15% of the 340K described! (The UNK problem can be better remedied by improving tokenization, but when you train longer it becomes less of a problem in all cases.)
  @nryant, we will look into this problem. But to make sure we are looking at
the same problem, could you provide some more reproducing details? It will
be very helpful if you can point us to a model either comes with TF, or
written by you, that is confirmed to be slower on TF-0.6 and faster on
TF-0.5.

On Fri, Dec 18, 2015 at 8:10 AM, Martin Wicke notifications@github.com
wrote:

> Assigned #548 https://github.com/tensorflow/tensorflow/issues/548 to
> @zheng-xq https://github.com/zheng-xq.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/548#event-495975301.
 That would be very helpful! Thanks.

On Fri, Dec 18, 2015 at 9:46 AM, nryant notifications@github.com wrote:

> Over the weekend I can put up a repo on github containing code to
> reproduce the fully connected model I mentioned. I'll also try downgrading
> to a previous version to see if I can replicate the timings I recorded a
> few weeks ago. Would that be sufficient?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/548#issuecomment-165850002
> .
 We used to compile with our own version of Eigen. We started using the upstream version of Eigen last week, and it turns out that a couple optimizations are missing. I'll fix the upstream version as soon as possible to get back to where we were performance-wise.
 Benoit, is the correct tanh implementation upstream?
On Dec 21, 2015 12:17 PM, "Benoit Steiner" notifications@github.com wrote:

> We used to compile with our own version of Eigen. We started using the
> upstream version of Eigen last week, and it turns out that a couple
> optimizations are missing. I'll fix the upstream version as soon as
> possible to get back to where we were performance-wise.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/548#issuecomment-166438870
> .
 I have cheked in several improvements to the upstream eigen repository such as [this one](https://bitbucket.org/eigen/eigen/commits/a0661a2bb16512dc4359c243db1dbeb9bc32b266). I have also updated the TensorFlow codebase to pull a recent version of Eigen that contains these improvements. This should fix the performance regression issue.
 Thanks for verifying the fixes. I'm closing this issue.
  It looks like the docs are wrong, in this case - the [registration](https://github.com/tensorflow/tensorflow/blob/a0f880c3432462f99c1ba046f0c4193675e7013f/tensorflow/core/ops/nn_ops.cc#L318) for the MaxPool op actually only accepts single-precision floats.
 The current implementation is pretty specialized to `float32`, so it wouldn't be completely trivial to switch it over. However, we'd be glad to take contributions on this if it would be useful.
 Hmm, tried briefly to do this, but hit the lack of `atomicAdd` on GPU for double.
 @siddharth-agrawal: That would be awesome!  You'll likely need atomic add for doubles, but it's available now in `util/cuda_kernel_helper.h` as `CudaAtomicAdd`. 
 @siddharth-agrawal: If the documentation change requires changing the list of dtypes passed to `REGISTER_OP`, you'll break the backwards compatibility tests when doing so (those tests don't notice the lack of kernels).  If the double kernels are coming soon, it may be better to wait.  However, it's also possible (likely?) that I'm misunderstanding the changes, so please tell me if the previous makes no sense. 
 `api_docs/python/nn.md` is autogenerated from code: you can't modify it directly.  Specifically, it's autogenerated from `core/ops/nn_ops.cc`, `python/ops/nn.py`, and `python/ops/nn_ops.py`.
 @siddharth-agrawal: They're autogenerated but also checked in.  That commit also changed the code from which it was autogenerated (scroll down). 
 It's surmountable, but I don't think we can accept pull requests that change the necessary code at the moment due to silly code organizational reasons.  @zheng-xq: I am I right that stream executor changes are difficult to accept from outside?
 Here are the ops that are missing double versions.  I believe the only difficult ones to add are the max pool ops, as described in #547.  Most of them are very neural net specific, which is probably why they haven't been generalized yet: neural net specific applications rarely need double precision.  We'd be happy to accept PRs though!

```
AdjustContrastv2
AllCandidateSampler
AudioSummary
CTCBeamSearchDecoder
CTCGreedyDecoder
CTCLoss
ComputeAccidentalHits
DrawBoundingBoxes
EditDistance
ExtractGlimpse
FixedUnigramCandidateSampler
HSVToRGB
ImageSummary
InTopK
LRN
LRNGrad
LearnedUnigramCandidateSampler
LogUniformCandidateSampler
MaxPool
MaxPoolGrad
MaxPoolGradWithArgmax
MaxPoolWithArgmax
NegTrain
RGBToHSV
SampleDistortedBoundingBox
SparseMatMul
StringToNumber
TensorArrayConcat
TensorArrayGrad
TensorArrayPack
TensorArrayRead
TensorArraySize
TensorArraySplit
TensorArrayUnpack
TensorArrayWrite
ThreadUnsafeUnigramCandidateSampler
UniformCandidateSampler
```
  Thanks @panmari!

There are a few changes that will be necessary to get this to work.

1) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L25  All of the ops that you added are currently only defined for those set of types.  So this file needs to be updated to additionally include the ones you are adding kernel registrations for.  My suggestion would be to change the attribute of T to be "realnumbertype".

2) After doing that, we should also write some tests to make sure these additional kernels are actually invoked properly.  Unfortunately, our current ops/image_ops_test.py does not really test many of the kernels.  Would you be willing to add the tests for these kernels?

The way to do this would be to change https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py#L576 to iterate various numpy dtypes for each of the kernels that we support.  We convert numpy dtypes to the appropriate tensorflow datatype, and this should be sufficient to test each of the kernels.

If this doesn't make sense, or you'd rather us take a look at it, let us know, since this is a change we really should make.
 Is it possible add at least add a test for one of these new types, to make sure it works?  Again, we can take this over if that's too much work for you.
 There's probably some other installation of numpy that is interfering -- try searching for other installations, and if they are not needed, try removing them.  IIRC, this is saying that tensorflow was compiled with 1.10.1 (likely) but the runtime is loading 1.9.  Or maybe change the paths so that 1.10 is preferentially chosen over 1.9?  I'm not an expert here :(
 Hmm, not that I know of.  We want to add 'parameterized test' support to the python unit tests to make this easy.  Alternatively, you could make the base class a test harness that takes in one type, and then instantiate one class for each type being tested, so that the name of the class tells you which type failed.

That being said, it's probably fine for now to leave it as is, since we do this type of test in a bunch of other files.  Thanks for making these changes!

I'll take a closer look tomorrow.
 test this please
 @jendap, tests didn't kick off :(

(Running manually)
 Yes, it was directed at the bot.  We need to change our regex to somehow make that more clear. 
  Currently, TensorFlow does not support Windows (except in a Linux docker container), and this is dependent on our build system (Bazel) adding Windows support. You can track issue #17 to see what progress has been made on this front.
  https://github.com/tensorflow/tensorflow/blob/20723e2b3d58cc48b2a302f7ea9806c8a75fd18f/tensorflow/python/ops/nn.py#L631 

nce_weights / bias are used as input to the nce_loss higher-level function, which also, under the hood, calls embedding_lookup, so the same property is required.

I thought our placement algorithm was supposed to ensure that the variable would not be placed on GPU if all consumers can only be placed on CPU, so hopefully we can dig in later to figure out how to solve this more generally.

We also have a bug internally to add a self-test to word2vec_basic.py so we could catch this earlier.

I'll take a closer look at this on Monday -- thanks for fixing this.
 I'm generally not a fan of soft placement since it masks bugs, but in some cases it is necessary.  Here your annotations can solve the problem so let's go with this. 

Can you squash your commits?  I'll merge soon after that.
 Thanks, we're pretty inconsistent about hanging indents, so it was really about consistency within that file only.
  > > cannot enable peer access from device ordinal 0 to device ordinal 2"

@lglhuada, This is not an error, just log. It just means there is no efficient way to transfer data from gpu:0 to gpu:2. You can exclude either one of them through CUDA_VISIBLE_DEVICES

> > the GPU usage is 95% while the volatile GPU-Util is 0.

This is also expected. TensorFlow always reserves most the GPU memory when it initializes, even before the first GPU kernels are received. So you will see a high memory usage at the beginning. But the fact GPU is not actually utilized means none of the kernels are running on GPU.

Could you try to run the tutorials and see if you can get any GPU utilized? 

> > bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
> > bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu

If you still have problems after that, please provide more information about your machine set up. 
 In this case, it is okay to use bazel to build your project, although it
shouldn't be necessary.

Please make sure you installed the GPU-enabled TensorFlow binaries. If you
built from source, please use:

bazel build -c opt --config=cuda
//tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
pip install /tmp/tensorflow_pkg/tensorflow-0.6.0-cp27-none-linux_x86_64.whl

On Mon, Dec 28, 2015 at 1:21 AM, Guangliang Liu notifications@github.com
wrote:

> @zheng-xq https://github.com/zheng-xq hi, I have checked that I can use
> GPU with bazel-example and the GPU-util is 21%. So now I need bazel to
> build my project, right? thanks
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/543#issuecomment-167519968
> .
  Suggestions:
1. I think your tfrepeat is just
   
   ``` python
   def tfrepeat(a, repeats):
     return tf.tile(a, [1, repeats]))
   ```
2. you can try to add a line in transpose_op.cc Ln186-194 to support bool for transpose. Though, you may not need it anymore if 1 works out for you.
 Yes, transpose should work for any dtype. The preferred solution is to use a suitable macro from register_types.h to make that happen. 
 Looks like `tf.transpose` already works for any type on CPU, but only works for number types on GPU.  I'll fix it to use `TF_CALL_POD_TYPES` instead of `TF_CALL_NUMBER_TYPES`.
 Fix in review.
  Hi Kenton,

Thanks for resubmitting this as a pull request, and for adding the tests. I'd like to push a bit harder against `IndexedSlicesWithoutDenseShapeValue`. I've added some comments to `ops.py` that suggest where the problem with returning `None` could be addressed, and I'd prefer to address those internally. (Ideally all `IndexedSlices` would have dense shapes, but for the meantime we could address this with a helpful error message in the case that the fed value doesn't match the object being fed.)

Derek.
 Not quite: I was suggesting that, in the event `IndexedSlices` doesn't have a `dense_shape`, the fetch function would return a list of two "subfetch" tensors (the indices and values), instead of returning `None` for the `dense_shape`.
 Thanks for indulging me! :)

This LGTM, so I'll go ahead and merge it.
  Thanks for reporting this - there is a bug in the imports for this code. I'm working on a fix, and it should be available soon.
  We now accept pull requests.
 Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit https://cla.developers.google.com/ to sign.**

Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.

---
- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
- If you signed the CLA as a corporation, please let us know the company's name.

<!-- need_sender_cla -->
 I signed!
 CLAs look good, thanks!

<!-- ok -->
 LGTM, Approval
 oops, done.
  Thanks for your report cinjon, I think it's a bug in seq2seq.model_with_buckets.
In particular, I think the problem is in these lines.
      if j > 0:
        vs.get_variable_scope().reuse_variables()
It just set variable sharing for the current scope, which is not the right thing to do: it should create a separate scope (reusing or not, depending on j). For now, this setting leaks outside of this function and compromises your second module.

We'll work on a fix, I'll also test it with a bunch of other models. In the meantime, can you try replacing these two lines in model_with_buckets by something like this:
  with tf.variable_scope("model_with_buckets", reuse=True if j > 0 else None):
 ... and shift the body to be in this scope ...

I think that should help and be better than doing your own variables.

Thanks for catching this problem!
 Hi cinjon. I think I corrected the reuse leaking from model_with_buckets in a recent commit, so now you're hitting a different problem, this one.

> If I remove the reuse=True on that, then the error is again over-sharing, this time because
> modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding already exists.

I think removing reuse=True in your case is the right thing to do (you don't want to reuse across modules, right?). But then - do you want to share the encoder embedding across modules? If you do, then you should create a variable just for the embedding and pass it to EmbeddingWrapper, I think. If you don't, then maybe just put each module in it's own scope (e.g., with variable_scope("module"  + str(num))) -- that will make all variables unique.

Does that help? I'm not fully sure how exactly you want to share variables -- if every module is separate, then I think the best way is to have separate scope for each of them.
 I was offline for a while cinjon, and I'm not sure I understand now what you were trying to accomplish. If you still have this problem, let's take it offline and, if needed, open another issue (just to not prolong this one, it becomes unreadable). Thanks!
  That's a good idea, but it will require some thought because the graph visualizer doesn't current get information about the execution (other than the computed summaries). Assigning to @danmane, who's been tracking feature requests for new versions of TensorBoard.
 Yes, exactly. I will update this issue and close it once the documentation is updated (the commit is under review). The doc talks about this new feature and show users how to use it.

Here is the short doc: To include tensor shapes in the `GraphDef` pass `sess.graph.as_graph_def(add_shapes=True)` to the `SummaryWriter` when
serializing the graph.
 Forgot to mention that the shapes shown in the graph visualizer are the statically inferred shapes from the python client. Most of the shapes are known in practice. However, ops whose output tensor's shape depends on some variable at run-time, will have unknown shape in the graph.
 [Here is](https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html#tensor-shape-information) part of the tutorial explaining how to add tensor shapes to the graph visualizer.
  Typically we use a much larger amount of computation to train a model with TensorFlow (using one or more GPUs if it is a complex model), because the throughput of examples processed per second is what determines how long it takes to train the network.

For inference - depending on the model - it is often possible to scale horizontally by using lower power devices, and scaling out to many independent devices (unless the model is too big to fit on a single device). That's what makes it possible to run inference on a mobile device (like in the [Android tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)), and potentially it could also run inference on a Raspberry Pi.
 This depends a lot on the particular model that you are running. @petewarden would have better statistics than I do, but it's certainly plausible to run the Inception model for image recognition on an Android device.
 To give you a rough idea, a small Inception model like the one used in the Android example can run in under a second on a mid-range phone like a Nexus 5, and will take two or three Joules. 
  Is this with a recent version of the repo?

Prior to commit https://github.com/tensorflow/tensorflow/commit/cd53f3c3302c9312c1840389a9988a879b8b9dd5 this would be possible due to a GC'd Surface resource.
  Tensors on the GPU still store their shapes on the CPU, and `reshape` already works in that case.  The shape will never be stored on the GPU, so I think this should be closed.  Presumably you're seeing an error caused by forcing the shape argument onto the GPU; if so there should possibly be a different bug about placer functionality.
 Wouldn't tf.shape return a tensor that could be shipped onto the GPU? In
that case, all information could be on the GPU, and the actual resizing
code should be able to run on the GPU.

On Fri, Dec 18, 2015 at 4:41 PM Geoffrey Irving notifications@github.com
wrote:

> Tensors on the GPU still store their shapes on the CPU, and reshape
> already works in that case. The shape will never be stored on the GPU, so I
> think this should be closed. Presumably you're seeing an error caused by
> forcing the shape argument onto the GPU; if so there should possibly be a
> different bug about placer functionality.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/533#issuecomment-165927888
> .
 The `Tensor` C++ class unconditionally stores the shape on the CPU regardless of where the data is, and essentially everything assumes this (for error detection, branching, etc.).  I think there's very little chance this situation will change.
 Note that as long as all the shape data is on the CPU, a `reshape` does O(0) GPU work even if the data is on the GPU.
 I think I'm implicitly assuming we're talking about the resize_image ops.
Sorry, that may be misinterpreting panmari's original request.

On Fri, Dec 18, 2015 at 4:54 PM Geoffrey Irving notifications@github.com
wrote:

> Note that as long as all the shape data is on the CPU, a reshape does
> O(0) GPU work even if the data is on the GPU.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/533#issuecomment-165929024
> .
 Ah, all my comments are wrong if this is about image resizing.
 @panmari -- which ops are you thinking about?

On Fri, Dec 18, 2015 at 5:06 PM Geoffrey Irving notifications@github.com
wrote:

> Ah, all my comments are wrong if this is about image resizing.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/533#issuecomment-165929904
> .
 Given @panmari's recent pull request to extend the types supported by resize_image ops, I think it's those.
  I think we disabled this log a while ago.
  kyosha@gmail.com has a change, https://tensorflow-review.googlesource.com/#/c/1252/, which I think will help. You are also welcome to fix the code to fit your need. Cheers,
  I believe missing the TAG is just a warning and shouldn't cause any problems.  If you look at the source code of tensorboard.py, you'll see TAG is just for debugging.

Not rendering any logged events is probably an independent problem -- can you verify / validate / reproduce the problem in a way we can help debug?
 Do you mind filing another issue re: the summary writer not flushing, with a reproduction? Since it is pretty different from the TensorBoard pip installation issue discussed here.
 @jaycode: Can you `ls -lah` your events file and tell me the size? Also, how did you install TensorBoard, and which version of pip are you using?
 @ultrons We actually have an internal debugging tool that is just the graph visualizer without the rest of TensorBoard. It wouldn't be too difficult to export this as a standalone tool. If you want it, can you please file another issue requesting this feature, and we can ask @dsmilkov to take a look.
 @Fhrozen 
You should be able to build both tensorflow and tensorboard using bazel without having the two conflict. I'm not sure what's going wrong in your case - you could try building all your targets (without using bazel run) and then run them by invoking the built target in `bazel-bin`.
If you keep having issues with bazel, please open a separate issue with a clear reproduction of the exact commands you're using and the errors that you're seeing.
  Thanks for reporting, fix is on the way.

On Wed, Dec 16, 2015 at 5:04 PM mac craig notifications@github.com wrote:

> https://www.tensorflow.org/versions/master/resources/index.html
> 
> In the white paper section:
> "...can be found in _out_ white paper"
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/529.
  @martinwicke: Any progress on this?  
 Is this still an issue? If it isn't we'll close it here. There may be a bazel issue which should be tracked elsewhere.
  @noisychannel, could you provide a bit more information about your running
environment. I see that you have two K20m on your machine. Is this a
dedicated machine, or something shared?

On Fri, Dec 18, 2015 at 11:06 AM, Derek Murray notifications@github.com
wrote:

> Assigned #526 https://github.com/tensorflow/tensorflow/issues/526 to
> @zheng-xq https://github.com/zheng-xq.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/526#event-496149991.
 I've started an offline conversation with the stream-executor team, since the error originates from stream-executor. Still wait for their response. 

@leary-google, @eliben, anything from the stream-executor side?
 @zheng-xq: Should we contact the Stream Executor folk offline?  It looks like they might not have Github notifications turned on. 
 Adding @henline, who is the owner of stream-executor. 
  Hey TF, 

This is not a big deal, but you may want to add an identity_like function that theano has here:
http://nullege.com/codes/search/theano.tensor.identity_like

It would be useful for orthogonality with RNN's. 

It would basically be something like:

``` python
def identity_like(input_tensor, scope = None):
  with tf.variable_scope(scope or "identity_like"): #in this linear scope, the library that you're retriving is Linear
    shape_0 = tf.shape(input_tensor)[0]
  return tf.diag(tf.ones(shape_0))
```
 Maybe an op that copies the shape, but lets you pass an initializer (many
of those functions already exist, and more should exist anyway)?

On Thu, Dec 17, 2015 at 9:10 AM Hello1024 notifications@github.com wrote:

> Please consider a different API here.
> 
> I could imagine "ones_like()", "uniform_random_like()", etc. quickly
> causing clutter.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/525#issuecomment-165514703
> .
 Dup of #434.
  I second this as well -- someting that would clearly define gpu compatibility would be great. If the documentation had it, that would be even better. 

On the docs, we could just assume all ops are gpu compatible, but if they are marked as "CPU only", it would help!!!
 The op may be defined on GPU, but just not for those specific set of attributes (e.g., it could have been the case that it was defined for Targmax=DT_INT32).

It would be too cryptic of an error message to enumerate all possible registrations for that Op (which might be tens of kernels) in the error message. 

In addition, it's totally legal for an op to be defined on GPU, but the binary to not include it, so even if we put something in the docs, it wouldn't be right.

I agree that we could do a better job here, but it'll take some care to make sure we don't spam the error message with content that nobody can read.
 Maybe it is enough to change the error by adding the information which
placement was requested:

InvalidArgumentError: No OpKernel was registered to support Op
'MaxPoolWithArgmax' _for device '/gpu:0'_ with these attrs
[[Node: MaxPoolWithArgmax_5 = MaxPoolWithArgmaxTargmax=DT_INT64, ksize=[1,
3, 3, 1], padding="SAME", strides=[1, 2, 2, 1] http://random_normal_4/]]

On Wed, Dec 16, 2015 at 9:15 AM Vijay Vasudevan notifications@github.com
wrote:

> The op may be defined on GPU, but just not for those specific set of
> attributes (e.g., it could have been the case that it was defined for
> Targmax=DT_INT32).
> 
> It would be too cryptic of an error message to enumerate all possible
> registrations for that Op (which might be tens of kernels) in the error
> message.
> 
> In addition, it's totally legal for an op to be defined on GPU, but the
> binary to not include it, so even if we put something in the docs, it
> wouldn't be right.
> 
> I agree that we could do a better job here, but it'll take some care to
> make sure we don't spam the error message with content that nobody can read.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/524#issuecomment-165179772
> .
 @martinwicke, @vrv: Martin's suggested fix seems reasonable.  Should we assign this to someone or give some more details are and mark it contributions welcome? 
 I believe we have fixed the error message to be much more helpful, but someone would have to verify ;)
 It doesn't look like we fixed it: https://github.com/tensorflow/tensorflow/blob/e9953ee70d45bfba8083eaa7ac902714f1a0b0a5/tensorflow/core/common_runtime/simple_placer.cc#L514
 That codepath only gets hit if there are no kernels at all that support that op.  There is no placement specified, and no kernels registered.  I think the situation @sguada has been fixed, you're pointing at a different error path.
 @vrv: Assigning to you since you seem to be the C++ runtime czar.  Feel free to reassign.
 Oops, sorry, missed your comment.  Never mind.
  is your python possibly a different one from that into whose directories tensorflow was installed? Can you do the following and paste the output:

```
cat tools/python_bin_path.sh
python --version
```

(the first one, in the tensorflow base directory)
 Closing due to inactivity.
  Yes, that's a bug.
 Send a PR?

On Wed, Feb 10, 2016 at 11:25 AM Pouya Samangouei notifications@github.com
wrote:

> @eerwitt https://github.com/eerwitt It works flawlessly! thanks.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/521#issuecomment-182539003
> .
 Other than the workaround posted above, no.
  This will (probably) be addressed by the work @keveman is doing to improve registration for user ops, so I'll assign to him for now.
 I added [instructions](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) for building ops and kernels outside of TensorFlow source tree. Please have a look to see if it fits your need.
  You don't need to run configure when you use the provided pip binaries. There are two different wheels that we provide for Linux, one with GPU support, and one without. If you want to use GPU, install this wheel: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
  One possibility is to map `tf.convert_to_tensor` over `grads` so that every element is converted to a (dense) `Tensor`. Does that work for your purposes?
 It would certainly be possible to handle this by handling `IndexedSlices` in a similar manner to `SparseTensor` in `Session.run()`, using the subfeed/subfetch mechanism. We'd be happy to take contributions on this!
  Hi @sdemyanov, thanks for this change.

Unfortunately, we don't yet accept changes via github, we hope to improve this in https://github.com/tensorflow/tensorflow/issues/26

You can take a look at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md to see how to contribute right now.
  thank you!
  Hi @sdemyanov, thanks (again) for this change.  The same comment as the other pull request applies here :(.
  https://github.com/tensorflow/tensorflow/issues/804
  I was thinking about it and there were 2 things that stopped me from working on it. The first and most important one is that I get good results without any special handling for masks, just as it is. Are you sure that a -FLT_MAX really helps, did you try without it? (I think the net just learns to never attend to the padding easily.) The second reason was that one day, when "while" works ok, we might not need to pad at all -- and I did not want to add complexity to the API that is not applicable to that case.

That said, if you find it important and want to do it -- by all means, send a patch :).
 > I get good results without any special handling for masks, just as it is

I too have been using this for about a month, and I also get good results just as it is. I think it learns to never attend to padding as that is a simple if statement. In keras, we also talked about this, and concluded that masking was not necessary because the network learns to ignore padding. 
 Indeed, it's a good argument. Still, as said before, the right solution to be close to the formalism is probably to use "while" (i.e., make the graph dynamically unroll) and then avoid padding altogether. We're working on this and did not want to polute the API with elements that will not be used then (as the padding symbol). But it will take a while before it's ready...
 > The network might be somewhat robust to it as long as the input sequences do not vary too much  

@ogrisel , if you apply enough different types of bucket sizes then you wouldn't really run into this problem. All the sequences would roughly be within 5 or 7 timesteps (assuming you're doing NLP seq2seq). 
 You're right Will - if there is variance in minibatches this might always be a problem. And let's not forget there is another problem with padding - we start the encoder from the first batch element, so the state goes through a bunch of pad symbols before the real sentence starts. Both could be corrected if the padding symbol was passed to the function and it constructed appropriate masks - we should probably do that. Are you guys willing to take a shot at it? (I'll probably not manage to do it before leaving for Christmas.)
 @lukaszkaiser: Can this issue be closed? 
 Yes - let's close this and leave the seq2seq attention model as is (many people are using it successfully). We might implement another attention model with more features at some point, but that's a different issue.
  This is a known issue for large unrolled RNNs in TensorFlow, and we are working on (currently experimental) functionality to express loops directly in the graph, to avoid creating a very verbose representation for those models.

Out of interest, is the step time you report for the first step, or a subsequent step? There is a non-trivial amount of setup work per node in the graph, but this only affects the first step. I would expect it to speed up for subsequent steps.
 Right now, we don't have TensorFlow-specific profiling tools available, but a sampling profiler (such as [`pprof`](http://goog-perftools.sourceforge.net/doc/cpu_profiler.html)) should give enough information to point you in the right direction.
 Can you try to use one of those two (currently not on by default) gradient accumulation methods and see if this improves steptime:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L204
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L231

Also, for experimentation I would recommend to use a small batch_size=1 and make it bigger when everything works to keep memory down.

Overall, we are fully aware that long unrolled RNNs/LSTMs are still problematic in TensorFlow, we are working on making it better.
 Lol @william.  How'd I know you were going to do this.
On Dec 15, 2015 12:24 PM, "William Chan" notifications@github.com wrote:

> @ludimagister https://github.com/ludimagister , not running the test
> because even the fprop alone is insanely slow compared to my framework
> (i.e., I do forward only pass and its still an order of magnitude behind).
> Right now I'm writing a monolithic RNN Op that does the entire RNN sequence
> (w/ dynamic computation but not dynamic memory) in 1 block.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-164912084
> .
 You mark it as a host input when creating your kernel.  See I think my
ReverseSequence kernel.  That has a bunch of tricks.  You can also just
register your kernel as cpu only for now. Everything will stay on cpu.
On Dec 15, 2015 7:54 PM, "William Chan" notifications@github.com wrote:

> @ebrevdo https://github.com/ebrevdo cant be that much harder than
> DistBelief right? Actually, kinda stuck... if my Tensor is in GPU memory,
> how can I get the value in CPU memory? can't find any API function
> available in tensor.h or in Eigen Tensor for that, i.e., for the
> sequence_length to do the dynamic compute.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165005210
> .
 OK FYI though it's not clear that you can work with multiple gpus in one
kernel. I may be wrong.  Derek or Sherry can probably say more.  Depending
how you need to split data you may be limited with the current
infrastructure to tower-like computation.
On Dec 15, 2015 8:30 PM, "William Chan" notifications@github.com wrote:

> thanks for the pointer! no, i need GPU for sure... i dont have google
> infrastructure, definitely need GPUs :p
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165013977
> .
 Bit late to this discussion, but I too am doing seq2seq in tf [here](https://github.com/LeavesBreathe/Project_RNN_Enhancement).

I don't want to beat a dead horse, but I have slow step times, though my compilation isn't too bad. I currently run 2 layers of 1536 units (GRU). However, the biggest hurdle by far is the memory.

When I run these two layers, each of them on a 980 TI, I get 99% usage, and I'm overclocking my GPUs. So I feel that the GPU is being used. However the usage percent is sinusoidal with a average of 50%, but I also had the same experience in Theano.

But because of memory usage, I can only use a batch size of 16 whereas in theano I could use a batch size of 256 easily. And it is the batch size that makes tf very slow for me. 

I know you guys are working hard on improving the memory allocator. I'm very appreciative of it. Just wanted to give my 2 cents. 
 You should be able to use Eigen ops with the gpu device. This seems like a
bug with user op declarations in the build rules.
On Dec 16, 2015 7:05 PM, "William Chan" notifications@github.com wrote:

> ok... I think those are the problem, is there an example I can follow to
> add a custom CwiseBinaryOp for Eigen::Tensor that will work on the GPU, or
> should I wrote my own CUDA kernel? What's the proper way to impl this?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165338454
> .
 That sounds great. Please send to @evrevdo or me and we'll get this in. As
of today (I think) we also accept outside contributions so that's another
option.

On Sat, Dec 19, 2015 at 12:37 AM, William Chan notifications@github.com
wrote:

> FYI, I've got a working implementation for "Gru" and "GruCell" .. GruCell
> is 1 timeslice monolithic op, while "Gru" operates on a sequence (dynamic
> computation). GruCell is roughly 20% faster per step (on TITAN X), and both
> ops have massive edge in the graph creation time (esp if you unroll large
> timesteps). The code is by no means optimized at this point (using 6
> matrices atm, proper impl should concat them and use 3 matrices), and lots
> of code cleanup -- but if anyone needs/wants to try the code, give me a
> ping and I can point you to the right direction.
> 
> I'll work w/ @ebrevdo https://github.com/ebrevdo to get this pushed to
> upstream "soon", but atm, I have other priorities. Also, not in the mood of
> learning how to write tests : p
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165960962
> .
 Assigning to @ebrevdo since he's done some work recently on this.
 Have you tried dynamic_rnn for comparison?
On Feb 17, 2016 7:21 PM, "William Chan" notifications@github.com wrote:

> I wrote a bunch of monolithic ops for the RNNs, is there interest for me to
> push this upstream? I can work w/ Eugene on this.
> 
> ## 
> 
> William Chan
> Carnegie Mellon University
> (650) 450-9455
> williamchan.ca
> 
> On Wed, Feb 17, 2016 at 7:16 PM, Vijay Vasudevan <notifications@github.com
> 
> > wrote:
> > 
> > Assigning to @ebrevdo https://github.com/ebrevdo since he's done some
> > work recently on this.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185524428
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185525264
> .
 Try it. Should solve a lot of memory issues.
On Feb 17, 2016 7:43 PM, "William Chan" notifications@github.com wrote:

> i haven't... but looking at the source, it still won't fix the memory issue
> (i.e., for long sequences > 2000 timesteps)? i.e., the monotholic op is
> much more efficient in the memory, (at least until TF supports optimization
> by merging ops?)
> 
> ## 
> 
> William Chan
> Carnegie Mellon University
> (650) 450-9455
> williamchan.ca
> 
> On Wed, Feb 17, 2016 at 7:37 PM, ebrevdo notifications@github.com wrote:
> 
> > Have you tried dynamic_rnn for comparison?
> > On Feb 17, 2016 7:21 PM, "William Chan" notifications@github.com
> > wrote:
> > 
> > > I wrote a bunch of monolithic ops for the RNNs, is there interest for
> > > me
> > > to
> > > push this upstream? I can work w/ Eugene on this.
> > > 
> > > ## 
> > > 
> > > William Chan
> > > Carnegie Mellon University
> > > (650) 450-9455
> > > williamchan.ca
> > > 
> > > On Wed, Feb 17, 2016 at 7:16 PM, Vijay Vasudevan <
> > > notifications@github.com
> > > 
> > > > wrote:
> > > > 
> > > > Assigning to @ebrevdo https://github.com/ebrevdo since he's done
> > > > some
> > > > work recently on this.
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185524428
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185525264
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185527102
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185529697
> .
 That bug should be fixed at HEAD
On Feb 26, 2016 11:50 AM, "Mohammed AlQuraishi" notifications@github.com
wrote:

> It will be awesome to get this working but I've run into multiple problems
> which I suspect are bugs. I opened a separate issue here #1306
> https://github.com/tensorflow/tensorflow/issues/1306
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-189452127
> .
 @ebrevdo: Is this resolved? 
 Yes, using either functions.Defun or dynamic_rnn.
 Can you post a gist with your model so we can look at it and figure out
what's slowing it down?

On Mon, Jun 13, 2016 at 8:13 PM, lightingghost notifications@github.com
wrote:

> Hi @ebrevdo https://github.com/ebrevdo ,
> 
> I have tried dynamic_rnn. But it is still ultra slow when building the
> graph
> 
> My model is 200~300 steps encoder and 30 steps decoder with attention,
> 
> The mini model of 100 steps encoder and 30 steps decoder takes 400s to
> build.
> 
> I cannot even build the full model on my machine (16 G Memory) due to out
> of memory, while the same model without attention using mxnet takes approx
> 20% of the memory.
> 
> Could you please provide any solution to that?
> 
> Thanks all
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-225768998,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtimwbQ_gGcBuel5rBgZ1l8n9dYL79Fks5qLhx0gaJpZM4G1LC9
> .
 I see.  For now you should write your own encoder using tf.nn.dynamic_rnn.
For the decoder we are working on something equivalent to dynamic_rnn.

On Mon, Jun 13, 2016 at 9:21 PM, lightingghost notifications@github.com
wrote:

> Hi @ebrevdo https://github.com/ebrevdo ,
> 
> Thank you for helping me out. The code was shown in
> 
> https://bitbucket.org/lightingghost/sent2mat
> 
> with the model defined in seq2seq.py
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-225776238,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtim1oVTC2F-LK5AHLVyPAuRMeoqwE2ks5qLiw9gaJpZM4G1LC9
> .
  How are you invoking your script? From the error message, it looks like you are passing `-f` and `--profile-dir` flags, but neither of them is defined in your code.
 Also, @craigcitro, in case he knows of a way to improve our thin flags wrapper in any way.
 In particular, from those flags you're passing, I'm guessing you're trying to do something related to IPython? If so, it's probably possible, but we'd need to know what you were trying to do.

@vrv one option would be to use the [parse_known_args](https://docs.python.org/2/library/argparse.html#partial-parsing) method in `_flags.py`, which would let us parse a set of args but return the remaining ones for the caller to use as they'd like (eg pass them on to ipython). this would be a break from the python-gflags behavior, but possibly useful.
 What was the command-line you used to invoke it? (Can you give us a pointer to where in the tutorial you were?)
 Oh, I see -- are you just following the tutorial from within IPython?
 Well, I think it just means that we didn't anticipate that use case, but should have. :wink: As a short-term workaround, running from the python command-line should get you fixed up.

@vrv it looks like we're going to need something like the fix I was talking about above.
 I think I fixed this in 1c579361cd1e088dd5e05a394b1561a73e3667ba
  Thanks for letting us know about this. Could you tell me whether you're using Python 2 or 3? (Although we're using `from __future__ import division`, this smacks of a Python 3 compatibility issue....)
 This may be a 'feature' -- tensors are intentionally rather strict about
implicit conversions, so I think they don't have **olddiv**. Try casting
the int to a float.

On Mon, Dec 14, 2015 at 1:00 PM Martin Tutek notifications@github.com
wrote:

> I'm using 2.7, however when I commented out the from **future** import
> division the original code worked fine. So it is some "v3" trickery.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/506#issuecomment-164557689
> .
 @mttk: what do you get when you print
`tf.__version__`?
  I'll try to track down this issue. Tomas, would you be able to write a self-contained example that fails with this error (including a definition of the input, any size constants, and the momentum optimizer itself)?
 On further investigation, this looks like a bug in `tf.gather()`'s gradient, when the indices are >1-dimensional. I'm working on a fix, and it should be available shortly.
  Check out examples/image_retraining on master. It's not a tutorial yet, but @petewarden is working on it.
 To be explicit this is an example of retraining the final layer of Inception and leaving the convolutional weights untouched. We see this as being at one end of the spectrum; it's extremely quick to train (e.g. 30 mins on a single CPU), but won't give quite as good results as fine-tuning all the weights (which takes longer).

We plan to support full retraining too, and training from scratch, but wanted to make this example available as soon as it was complete. We haven't updated the website yet (we're still sorting out the large protobuf loading issue that currently requires hacks to the protobuf lib source - #836) but the usage guide is here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/image_retraining/index.md
 You need to pass in the name of the new output layer, '~~final_output~~' (_edit_ - that should be 'final_result'), as the output_layer flag to label_image. I'll update the tutorial documentation to add this info, apologies for the confusion.
 Apologies for my typo above, I've corrected it to read final_result. I will also be adding examples of how to load the models in label_image and classify_image.py.

The port/without port syntax is confusing too, I'll look into why that's different between the demos. In general we use the bare name to reference an operator, and the port version to grab the output of an op.
 The Android code doesn't use GraphDefBuilder, it just loads the file directly into a GraphDef:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/tensorflow_jni.cc#L91
As long as you make sure you've added your model file as an asset, that should work to load it.

There are some additional parameters you'll need to change, like the input image size and the 'mean' to subtract from all the inputs, and you'll need to add a standard deviation parameter too:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/tensorflow_jni.cc#L45

If you look at the default values for those flags in label_image, that should help.

What's the exact error that you're hitting by the way? That may help be more targeted in my ideas.
 I've filed #1253 to cover the work involved in updating the Android demo to accept a retrained model. There are no major code changes required, but it is a bit fiddly to find all the places where references are made to input and output layer names and other model-specific attributes. I think it will be easier to code up the changes than to try to explain them all, but I don't have an ETA for when I'll be able to get to this unfortunately.
 The division sounds like it will give the correct results, but for the wrong reasons. :)

The key difference between the current Android demo code and the label_image example is that they use different versions of the Inception model. The current Android model requires a divisor of 1, and a subtraction of 117. You can see the image mean defined here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorflowImageListener.java#L51
Since the divisor is 1, it's ignored in the current code.

To load the model that's used in label_image and image_retraining, you need to use a subtraction of 128 and a division of 128. You can see those values as mean and std in the command-line arguments here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L246
 Sorry you're hitting problems! Are you loading the new labels file with the --labels command line flag? Here's a full example, from https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html#using-the-retrained-model

```
bazel build tensorflow/examples/label_image:label_image && \
bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \
--output_layer=final_result \
--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg
```
 @dhananjaymehta: I would file a separate bug for that issue. 
 That's a very useful example @eldor4do, thanks! I will update the docs to point to that, if you don't mind?
 @petewarden: This thread seems to have deviated quite a bit from the original request.  Should we mark it resolved and reopen other issues as required? 
 +1

On Mon, Jun 6, 2016 at 11:51 AM Geoffrey Irving notifications@github.com
wrote:

> @petewarden https://github.com/petewarden: This thread seems to have
> deviated quite a bit from the original request. Should we mark it resolved
> and reopen other issues as required?
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/504#issuecomment-224052212,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_SFTinXLttgqlwUgZtN09xXNO6Mgks5qJGw7gaJpZM4G0cHg
> .
 Resolving for now, please open more specific issues if problems persist, or ask on StackOverflow if  you have questions.
  Our policy so far has been only to document the features that we intend to support in TensorFlow long-term. Currently we use [`python-gflags`](https://github.com/gflags/python-gflags) as the implementation for `tf.flags`, but this is subject to change in future. In the meantime, the [documentation for that library](https://github.com/gflags/python-gflags/blob/master/gflags.py) is the best source of usage information.
  It looks like things are failing when Bazel tries to build TensorBoard. Have you had any success in getting another TensorFlow target to build? (e.g. Does `bazel test //tensorflow/python:constant_op_test` work?)
 These tests check for the correct behavior (the proper exceptions) when
trying to create graphs/constants that are too large for protobuf. The
limit is 2G/tensor. On your platform, you run out of memory trying to
allocate/copy that, which is soft of expected given your address space.

I think those two tests should be fixed to catch MemoryError and not count
that as a failure (since if your system cannot allocate 2G + 2G, you
probably won't be creating graphs that big).

On Tue, Dec 15, 2015 at 12:40 AM Víctor Mayoral Vilches <
notifications@github.com> wrote:

> Thanks @mrry https://github.com/mrry for looking into this. Here's
> https://gist.github.com/vmayoral/ec85d8effa13308d5f66 the build output.
> Failed at the end.
> 
> More useful seems to be the log cited at the end
> https://gist.github.com/vmayoral/7665cf9ca9056a15167d. Apparently,
> there's an ERROR: testTooLargeGraph (**main**.ConstantTest).
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/502#issuecomment-164686186
> .
 Looks like this fell through the cracks.  Does anyone know if this is still an issue?
 Closing due to lack of response.
  Pinging @benoitsteiner, since this sounds like it might be an Eigen issue.
 The problem is probably that the scalar_log_op::packetOp method was not marked as EIGEN_DEVICE_FUNC. As a result nvcc doesn't generate the corresponding kernel code in debug mode. In optimized mode, it will inline the code so everything ends up working as expected.

In change 110406666, I updated the TensorFlow build configuration to pull the Eigen code from the upstream repository, where this problem has been fixed, instead of relying on our own local copy of the code. This issue will therefore be fixed in the next TensorFlow release (0.7). 
  We can not replace the `#ifndef` guards with `#pragma once` for various reasons. What would be gained concretely from having both?
 If both are needed in any case, it would mean that the `#ifdef` guards are wrong and bugs will appear if `#pragma once` doesn't work.
  De-duping with #258 -- reopen if this is different.
  I'm not sure what this request was for, but #26 is closed so I assume this one is too.
  @keveman is working on the workflow for user written kernels/ops.
 I added [instructions](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) for building ops and kernels outside of TensorFlow source tree. Please have a look to see if it fits your need.
  You may be looking at the documentation for master or 0.6.0 and running TensorFlow version 0.6.0 or 0.5.0, respectively.  What is `tf.__version__`?

Cc @martinwicke in case there's something we can do to make that happen less often.
 Default view on docs is at head, which is suboptimal. It should be 0.6, but
in this case, that may not be the issue.

On Sat, Dec 12, 2015 at 3:30 PM Geoffrey Irving notifications@github.com
wrote:

> You may be looking at the documentation for master or 0.6.0 and running
> TensorFlow version 0.6.0 or 0.5.0, respectively. What is tf.**version**?
> 
> Cc @martinwicke https://github.com/martinwicke in case there's
> something we can do to make that happen less often.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/495#issuecomment-164201729
> .
 If `tf.__version__` returns an error, your TensorFlow installation is old and didn't have support for elu() yet. Upgrading to 0.6.0 should solve your problem.
  Duplicate of https://github.com/tensorflow/tensorflow/issues/15
  Cc @zheng-xq, but I think we may need more information to help here.
 @zcyang, did you try to use CUDA_VISIBLE_DEVICES to limit the set of GPUs you want to use with TF? 

The current TensorFlow works by going through all the GPUs visible to itself, check which one is compatible and assign a logical index to it. Note that this logical index might be different from your system device index. Then in your graph, you use the TensorFlow logic index to refer to each device. So all the devices will be occupied, regardless which one will be actually used later. 

If you don't want a device to be used by TensorFlow, one solution is to use CUDA_VISIBLE_DEVICES and make it invisible. 
 I believe I  fixed this sometime between 0.6 and 0.7 -- you only allocate memory for a GPU on first real use of it.
  Hey man, I have had and still have the same issue and asked about it here:

https://github.com/tensorflow/tensorflow/issues/352

You can set the `aggregation_method = 2` and that helped me some. But still you're right. Tensorflow sucks up proportionally way too much memory. It has been difficult to deal with. If they could fix this one aspect, it would be a real game changer. 
 Stay tuned, improving memory usage and management is at the top of our list.
 Sounds great to hear that. I love Tensorflow and with improved memory usage it would be the best deep learning platform in my opinion.
 With hundreds of issues still open, this is too general of a request to be useful to keep open -- we're constantly going to be trying to improve performance and memory, of course.
 @vrv can you comment if 0.7.0 has improved memory allocation? I have hesitated to upgrade to 0.7.0 due to reported issues with Saver function. 
  (it is just temporary, we may re-open it soon).
 In #664 @ville-k added support and a few have verified it works -- we'll try our best to keep it running but we don't have a test machine so please continue to contribute to keep it working!   
  Thanks for letting us know. This is now fixed upstream, and should be released soon.
  This error seems to be in the protobuf library (and wasn't something we ran into when testing python 3).  Do you think you could file this bug over at https://github.com/google/protobuf ? I don't think we know enough about protobuf ... :(
  @josh11b: Why isn't this just `input_producer`?  It doesn't seem to be at all string specific.
 @bschreck: `_input_producer` is a fine solution for now, but it isn't part of the public API and will go away once we implement `__all__`.  However, unless there's a reason we should probably remove the underscore and make it part of the public API.
 +1

On Fri, Dec 18, 2015 at 4:35 PM Geoffrey Irving notifications@github.com
wrote:

> @bschreck https://github.com/bschreck: _input_producer is a fine
> solution for now, but it isn't part of the public API and will go away once
> we implement __all__. However, unless there's a reason we should probably
> remove the underscore and make it part of the public API.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/486#issuecomment-165927236
> .
  Thanks, @ebrevdo can fix this.

Btw, that code was never part of the public API :(
 Well, since it wasn't part of the official API, this isn't really a bug. And I believe it still isn't documented, so it's still not "ready".  Closing for now!
  That's a good feature request.  For complex numbers, it should be called `tf.arg` or `tf.argument`.
 If you write the op in all Eigen (which seems entirely achievable), you
should be able to write it once for both CPU and GPU. AdjustContrast is an
example of that, and how the final op is registered.

On Fri, Mar 4, 2016 at 7:48 AM Ryan Young notifications@github.com wrote:

> I'm a part of a group of 4 students from Drexel University who are
> currently working on this. Because we are not very familiar with tensorflow
> we are going to start by implementing tf.argument() directly as a kernel
> without a GPU kernel. After we get that we are going to try to implement
> atan, as implemented by eigen, and then making the tf.argument function a
> composition. Let us know if you have any tips, advice, or feedback.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/483#issuecomment-192329516
> .
 Let's call it `tf.arg` rather than `tf.argument` for symmetry with `tf.abs`.
 Since Eigen already has scalar_arg_op defined, we might as well use that for better performance on both CPU and GPU :)
 Yep, let's wait for the GPU enabled device test.  As for the custom arg code, don't include it unless it is used in the final version.
  If using bazel:

bazel run -c opt //tensorflow/models/rnn/translate:translate -- --data_dir [...]

You need a "--" before "--data_dir"

If running from python, what version of tensorflow do you have installed?  You need 0.6.0 for running translate.py directly to work.
 That's a network error when downloading what looks like dependencies. It's
most likely due to a bad internet connection.

On Sat, Dec 12, 2015 at 12:24 AM vnkmr7620 notifications@github.com wrote:

> thanks. it works, i checked bazel with tensorflow 0.5.0 and python with
> tensorflow 0.6.0
> 
> But anpther problem is in both I am getting the following error after some
> time. please have a look in the attached image
> [image: 13]
> https://cloud.githubusercontent.com/assets/10511526/11760997/b20cd746-a0d7-11e5-960e-58e681af16b4.png
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/482#issuecomment-164125198
> .
 Bazel is already done by the time you get the error. You can try running without bazel altogether (after building):

```
bazel-bin/tensorflow/models/rnn/translate/translate --data_dir /home/cencluster/wmtbazel/
```

But probably, the model is probably too big to run in 2G RAM. 
 Unfortunately, I cannot see the actual error in that screenshot. It's quite likely that bazel wasn't using all that much memory, and that you're still out of memory. You should paste the actual program output here, the screenshot doesn't contain much information.
 De-duping with https://github.com/tensorflow/tensorflow/issues/600, which has more information.
  Apologies: we've noticed this internally and fixing it.
 Fixed today. Will be available in next release.
  I have a fix out for the first issue: feel free to send us a PR for the second one!
 @vrv: If the actual bug is fixed, should we close this?  I don't think it's a TensorFlow bug that `(1)` isn't a `tuple` in Python.
 I think it's fixed.
  Is the version of Python you passed to `./configure` the same as the one you're importing tensorflow from?
 We would ideally provide a better error message in this case, but I'm not sure how to do it.  By the time we'd realize the problem, linking has already taken place.
  What do you get from: `uname -a` ?

Have you tried the same with our 0.6.0 release?
 The Python version is encoded in the wheel filename. You can try to rename
the wheel (cp35), but you may have to install from source.
On Mon, Jan 25, 2016 at 11:57 cnsgcu notifications@github.com wrote:

> Is python 3.5 supported? Having the same problem in python 3.5.1
> 
> $ pip3 install --upgrade
> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl
> tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on
> this platform.
> 
> $ python --version
> Python 3.5.1
> 
> $ uname -a
> ... 3.19.0-43-generic ... x86_64 GNU/Linux
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/477#issuecomment-174640036
> .
 You seem to be using the 0.5.0 wheel, which has no support for Python 3.
Try the 0.6.0 Python 3 version.
On Tue, Feb 2, 2016 at 07:05 Margus Pärt notifications@github.com wrote:

> On ubuntu 14.04 getting the same error.
> 
> /usr/bin/pip3 run on Tue Feb  2 10:02:00 2016
> tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
> Exception information:
> Traceback (most recent call last):
>   File "/usr/lib/python3/dist-packages/pip/basecommand.py", line 122, in main
>     status = self.run(options, args)
>   File "/usr/lib/python3/dist-packages/pip/commands/install.py", line 257, in run
>     InstallRequirement.from_line(name, None))
>   File "/usr/lib/python3/dist-packages/pip/req.py", line 168, in from_line
>     raise UnsupportedWheel("%s is not a supported wheel on this platform." % wheel.filename)
> pip.exceptions.UnsupportedWheel: tensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/477#issuecomment-178621505
> .
 De-duping with #468, which is the more general issue.
  Thanks @ville-k.  Btw, we don't yet accept pull requests from GitHub, only Gerrit.  See https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md for more info

Once you get something working, we'd be happy to accept this improvement!  @zheng-xq for early visibility on this change.
 @elbamos: I think @zheng-xq was probably going to tackle the cudnn versions issue soon.  Not sure how he plans to do it though.
 Ok, we'll close this -- feel free to send us a new PR when it's ready!
  The shape of a tensor is always integers, regardless of the dtype of the tensors.  For example, a 1-D tensor can't have 2.7 elements.
 If you want to see the dtype of labels, do `labels.dtype`.
  @zheng-xq, @noisychannel: Looks like this fell through the cracks.  Is it still an issue?  
 Closing due to lack of response.
  This looks like a numpy version issue. Maybe you have more than one version of numpy installed in various places? If you installed from source using the instructions, you should have built against numpy 1.10 (version a), but maybe your runtime environment is set up using an older version?
 Do the tests pass without your new op?  It seems very weird that the new op would have anything to do with this, but I haven't looked at that code path and could easily be wrong.
 I'm hitting this too now, though I'm not sure if it's the same issue.  If I run `python3` directly, or invoke it indirectly using `bazel run`, `import numpy` pulls in something from `/usr/local/lib/python3.4/dist-packages/numpy`.  This is also the version of numpy picked up if I compile.

However, if I use `bazel test`, it pulls in a version of numpy from `/usr/lib/python3/dist-packages`, which in my case is incompatible.  Presumably bazel is messing with paths in an inconsistent manner, though I haven't investigated fully yet.
 Sounds like it's worth asking the bazel folks about. 
 Bazel folks asked: https://github.com/bazelbuild/bazel/issues/753
 Closing due to inactivity / likely bazel / environment related
  Did you run ./configure  first?
 Hmm, @martinwicke, this might be related to your change earlier today?
 Probably. As a workaround while I look into this, you can edit tensorflow/tools/pip_package/build_pip_package to remove the offending line. Replace 

``` bash
  source tensorflow/tools/python_bin_path.sh
  ${PYTHON_BIN_PATH} setup.py bdist_wheel >/dev/null
```

with

``` bash
  python setup.py bdist_wheel >/dev/null
```

Then rebuild.
 worst-case, we could also just do `${PYTHON_BIN_PATH:-python}`
 Does the change in 7c5c20b2b73c40bb4fd113f26bcda73a5e5bec6d help?
  Thanks -- We'll try to update tensorboard to be python 3 compatible -- will probably be in the next binary release.
 You probably have to replace StringIO with BytesIO somewhere. I've seen
this kind of error before here
http://stackoverflow.com/questions/34416702/tensorboard-graph-visualiyation-error-using-python-3/34459035#34459035.
Is that the same error you're getting?

On Mon, Jan 4, 2016 at 5:57 AM kibtes notifications@github.com wrote:

> To nikitakit,
> 
> i included your fix to the fix suggested by dankolbman but i still have
> the same error.
> 
> Type Error: string argument expected,got 'bytes'
> If any other ideas please help!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/470#issuecomment-168683037
> .
 Thanks for the PR! Since it's merged now, I'm going to close this issue.
  Yeah, I think this is a bazel or tensorflow BUILD issue, not an nvcc one.

Can you try using bazel 0.1.1 or bazel 0.1.2?

@damienmg in case he has some ideas, since I can't reproduce this at HEAD with bazel 0.1.1
 Thanks @damienmg, I'll try to get the fix in today.
 I can't seem to reproduce this, either on my mac or on my linux machine, both pulling tensorflow from HEAD and installing bazel fresh from bazel.io for 0.1.2.

William: by any chance would you have time to figure out what the necessary changes are?  I'd be happy to integrate them.
 Nevermind, it's the GPU only targets that are messed up.  no wonder.
 okay, sort of fixed in https://github.com/tensorflow/tensorflow/commit/a4cefca9f40ae6cfe366b6187d07e5199aa74895

--standalone argument is still required.  Seems like it needs to be addressed by bazel team at https://github.com/bazelbuild/bazel/issues/698
 @damienmg: is this a fix to our BUILD rules?  We might have to update our jenkins gpu pip builder to 0.1.2 I guess.

@jendap: do you know which bazel we're using there?
 Fixed in the above commit, we believe.
 Yep, the build is broken at the moment -- we have a fix we're trying to push through.
  **Note:** If you know lots about python wheels, and can answer some of the questions here, please jump in!

Python wheel files encode metadata about supported architectures into their names, cf [the wheel PEP](https://www.python.org/dev/peps/pep-0427/) and [the suffix PEP](https://www.python.org/dev/peps/pep-0425/).

Currently, when we generate wheel files, we end up with either too-broad names (eg all Mac wheels are `py2-none-any`, which is wrong) or too-specific (some linux wheels are tagged `cp34-cp34m-linux-x86_64`, but we think the abi component should be `none`). This leads to issues like #467.

A part of the issue here is that we're not sure what some of these tags mean -- for instance, the only explanation I've found of the `m` suffix is in [an issue on the wheel bug tracker](https://bitbucket.org/pypa/wheel/issues/61/abi-version-is-not-found) and even that doesn't clear things up.

There are really three parts to fixing this:
- we should know what the "right" filename is for these wheel files.
- we should fix our build process to generate the right output on all architectures.
- we should consider serving our wheel files via a custom PyPA.

More details on the last one: we're making our lives harder by asking people to point to the "right" wheel file. `pip` knows how to take a package name (eg `tensorflow`) and a list of wheel files, and then install the right one. We can and should serve our own PyPA, so that installation instructions on **all** platforms we support would be something like

```
pip install --extra_index_url=https://tensorflow.org/pypa tensorflow
```

/cc @vrv @martinwicke @keveman on the TF side.
 Closing since we're not getting any traction from the community on this.  Sigh.
  Yeah, we're still testing everything out today before announcing that it's ready.

Just curious, what happens if you download the wheel package and rename it tensorflow-0.6.0-cp34-none-linux_x86_64.whl ?
 Yeah, they shouldn't have been pushed to the docs until they were tested fully.  We'll probably just rename the wheel files.
 new wheels with the updated filename are up.

i'll let @martinwicke close this once the website is updated.
 Fixed!
  Using -1 to reject is interesting, but it turns coding errors into silent incorrect behavior so I'm leery of doing it.  For now, you could replace -1s with a positive value represent an "ignored" class and then drop it.

It would be reasonable to add a boolean `drop_negatives` attr to ignore negative values, defaulted to false to preserve the existing error detection behavior.
 Ah, but the fact that it exits without the error message is a bad bug.
 I'll fix the bug part now.
  Agreed, I've wanted this too.  There are some annoying obstacles, since in some places we automatically convert singleton tensors to lists and this feature would be backwards incompatible, but we've had some internal discussion about removing that magical (and fairly unidiomatic) list conversion.  If we did that, we'd be able to add _your_ automatic conversion, which would be great.
 Closing this as a duplicate of #2328.
  Based on reading the code, I think it is expected: there is not yet support for SparseTensors with those three optimizers, since _apply_sparse() function isn't implemented.  Turning this into a feature request.
 Assigning to someone who knows more about this part of the codebase
 I think the issue here is related to a bug in `array_grad._GatherGrad`, and I have a fix in the works. (It seems to be broken when computing gradients for an embedding lookup/gather with a >1-D indices input.)
 Indeed, it will only fix the issues with adagrad and momentum (and other optimizers that support sparse data using embeddings).
 What kind of behavior would you expect from sparse tensors in RMSProp?
There are two options I think:
- Ignore the momentum terms for the embedding rows that are not present in the current batch
- Apply momentum terms to the whole embedding (AFAIR, that's what we do for Adam)

The first might not be correct (though, not sure), the second is very slow. What I typically do now is to split my variables into two groups, and train the dense part with any optimizer I want and the rest with GradientDescent or AdaGrad.
 You're right, the third option you suggested is probably strictly better than the second one. That's not how it's implemented in Adam, though (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py#L138)

For Adagrad, we're only updating non-zero elements and I think this is a common way to do it (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L381)
 Renaming this bug since the AdaGrad and Momentum optimizers should now work.
 I'm going to mark this contributions welcome, since I don't know of anyone working on it.
  Most of the example sources are no longer in the pip package install directory, instead, after installing the pip package, you can clone the git repository to get access to the source (at tag 0.6.0 to be safe), and then you can just do

cd tensorflow/models/image/imagenet
python classify_image.py
  Closing since I think this is covered by #527.  It would be better to fix compilation on Centos rather than ship static versions.
  Yeah, we haven't pushed the new website that has these link fixes yet.  We'll probably do so soon.
 Pushed website, fixed!
  Agreed, this isn't great behavior! It's not that hard to fix so I'll try to get it for you soon :)
 Dan fixed this a while ago.  Thanks Dan!
  We're planning to build closer integration between the graph visualizer and the other event types so that it would be possible to visualize data charts directly on the graph; we haven't started work on it yet, though. 
For now, if you want to inspect values of the tensors you probably want to add a histogram summary, and you can then view the distribution of values from the tensor on the histogram page. (Ideally you would put each run in a separate directory so it's easy to filter between them using the run selector.)
I'll post back with more info when we start building tighter graph vis/event integration. 
Does this answer your question for now?
  Yes, we are about to build the 0.6.0 packages that should work with all the tutorials at HEAD.  Closing because we're building them now...
 Can you download the wheel again? I think this is fixed by now, but I want
to make sure.

On Thu, Dec 10, 2015 at 1:20 AM daikazoku notifications@github.com wrote:

> I'm not sure if I understand. On the website there is a 0.6.0 wheel in the
> instructions, but when I go to run the pretrained inception-v3 model, I
> don't find the imagenet directory:
> 
> ## (d)[nani@nande cat]$ pip show tensorflow
> 
> Metadata-Version: 2.0
> Name: tensorflow
> Version: 0.6.0
> Summary: TensorFlow helps the tensors flow
> Home-page: http://tensorflow.com/
> Author: Google Inc.
> Author-email: opensource@google.com
> License: Apache 2.0
> Location: /home/nani/Desktop/cat/d/lib/python2.7/site-packages
> Requires: six, protobuf, wheel, numpy
> 
> (d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/
> cifar10  **init**.py  **init**.pyc  mnist
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/457#issuecomment-163541754
> .
 When you pip install --upgrade, also use --no-cache-dir

On Sun, Dec 27, 2015 at 12:23 PM, Roman Manukyan notifications@github.com
wrote:

> imagenet model is still missing for me on Mac OS using virtualenv.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/457#issuecomment-167439537
> .
  @kvamaraju, could you sync to the latest TensorFlow and retry? I think this problem should have been fixed by a recent CL. 
  This is a better question for stackoverflow, since it does not seem to be about a bug in tensorflow.  As a first guess, you may need to either wait longer or increase your learning rate, but it is hard to tell from so little information.
 To clarify: it's a good question, but we're trying to keep questions going to stackoverflow since it makes it easier for future people who hit the same problem to find the solution.
  @zfrenchee, can you send this request via gerrit?  See https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md and http://blog.mdda.net/ai/2015/11/10/contributing-to-tensorflow/
 Website is pushed based on the repo, but it's not automatically pushed (yet), so there's some manual delay involved.  We're probably going to push the newest website soon, and if the URL is still wrong, feel free to let us know and we'll fix the website on our end
  @danmane: Do you know what might be going wrong?
 @hamidb: that sounds like a red herring.

Can you try 
bazel clean

followed by

bazel build -j 1 --spawn_strategy=standalone 

(These are the instructions we've been suggested for those having trouble with bazel builds)...
 Sorry, I meant:
$ ~/lab/bazel/bazel-bin/src/bazel build -j 1 --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package

That is indeed weird that it can't open the mocha.git file...
 Assigning to andrew to take a look.
 I am unable to reproduce this issue using a fresh bazel install (from source) and clone of the TF repo. tensorflow/examples/android:tensorflow_demo still builds cleanly for me.

Note that unless you clone the TF repo with --recursive, you will not grab the protocol module into your local tree. Could this be related to your issue?
 To confirm, you are unable to reproduce the issue when building with bazel 0.1.2?

I am unable to follow your reproduction steps for "cd google/protobuf & make clean":

~/tf_git/tensorflow$ cd google/protobuf/ & make clean
[1] 939
make: **\* No rule to make target `clean'.  Stop.

Given that "//third_party:protoc" appears nowhere in the Tensorflow or google/protobuf repos, I'm guessing this sounds like a bad config is being pulled in from somewhere external, likely Bazel.

At its root, when you chase down what tensorflow/core:protos_cc actually is (this nessecitates following several layers of indirection between tensorflow/core/BUILD, tensorflow/tensorflow.bzl, tensorflow/core/platform/default/build_config/BUILD, and tensorflow/core/platform/default/build_config.bzl), you find it's a cc_proto_library from /google/protobuf/protobuf.bzl. 

Are you able to build tensorflow/core:protos_cc independently?
 @ahumesky Does this look like a Bazel issue that was fixed between 0.1.1 and 0.1.2?
 My best guess is that this was fixed by https://github.com/bazelbuild/bazel/commit/c74ee3784d3fd3a722f92c39805e57b4353a4509

When you clean out everything some pregenerated binary must be getting lost. I would suggest using Bazel 0.1.2 or later if that solves your problem. There are no Java protocol buffers used by the demo, so  figuring out a way to somehow force the unecessary //third_party:protoc to exist for < 0.1.2 builds after cleaning would be something of a hack, especially since the root issue has already been fixed.
 @jayendra13 Are you still experiencing your original problem?
 @hamidb 
I'm trying to make sure I completely understand this thread. If I understand correctly, you originally encountered this issue:
"cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'."

What was the solution for you, if you found one, and how was this connected to the protobuf issue you also related here?

@jayendra13 
Which version of Bazel are you using?
 @lberki, @davidzchen in case they've seen this before.

@jayendra13: try searching for "cannot open git-upload-pack"  on the web and see if you can find some resources to help.  At this point it is out of scope of the TensorFlow team.
 @davidzchen: What's the status of this? 
  The file was moved to tensorflow/examples/tutorials/mnist/input-data.py -- I think our website hasn't updated yet.

You might want to consider just git cloning the repository instead of downloading files individually.
  Thanks, that should be fixed.
 This looks like it was fixed
  Nice catch!  Yes, that is a bad bug.
 I have this fixed at the kernel level, so it won't crash anymore.  However, I could also fix it at the shape inference level so it throws a Python exception at graph construction time.  To do this, I'd need to make `tf.Dimension` insist on nonnegative input.

@vrv: Do you think that's wise?  I'd be a bit worried that negative dimensions are used somewhere, but it's currently a bit inconsistent that C++ `TensorShape` requires nonnegative but Python `TensorShape` does not.
 C++ part in review.
 I'm not sure yet if its wise -- the C++ TensorShape class thinks about shapes more concretely: it doesn't support unspecified dimensions / -1 / shape inference, etc.

I think when we push shape inference down into the C++ code, we'll likely have a C++ class that more closely represents what the python TensorShape class does.
 Cool, I'm happy to leave it the way it is now that the C++ is fixed.
  Yeah, @colah needs to update the arrow
 And make sure to make a new image, not overwrite the old one!

On Tue, Dec 8, 2015 at 11:27 AM Vijay Vasudevan notifications@github.com
wrote:

> Yeah, @colah https://github.com/colah needs to update the arrow
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/448#issuecomment-162989238
> .
  The problem is that `get_attr` on a type attr returns the enum value for that type, which happens to be 1 in this case.  It would make vastly more sense if it returned a `DType`.  Until we fix it, you can convert it to a `DType` via `tf.as_dtype(ops[8].get_attr('T'))`
 Retitled
 The T attribute for sub indicates the datatype of the tensors being operated on.  So T:DT_FLOAT just means that the two inputs are floating point tensors.

What value specifically are you trying to get?  The output?  (Do you have a larger piece of code to share so that we have some more context?)
 Use `op.input(n)` for that.
 @girving: do we want to be smart / special about any other types of attributes in get_attr, or is special-casing for types_pb2.DataType enough?
 @vrv: I'm not sure, but the possibilities are shapes, tensors, and lists of types, shapes, and tensors.  I imagine shapes is already fine, and hopefully tensors as well.
 Reopening since we'll use this bug to track cleaning up `get_attr`.
 As for your issues with `op.input`: is that problem that it returns a `Tensor` instead of the actual value?  If so, `tf.tensor_util.ConstantValue` will get you the underlying value (or `None`).  Unfortunately, it's technically not part of the public API, so if you use it your code will break at some point and you'll have to use `tf.constant_value` or whatever we rename it to.
  It is not currently in the works, and it would be a great addition to TensorFlow. Any contribution in this area would be very welcome.
 BTW, scipy lBFGS seems to be slower than Matlab's minFunc
I tried running https://github.com/jatinshah/ufldl_tutorial, which uses scipy's lBFGS, and it felt an order of magnitude slower than original sparse-autoencoder Matlab version from http://deeplearning.stanford.edu/wiki/index.php/Neural_Network_Vectorization
  Looks like this fell through the cracks.  @petewarden: Any thoughts here?
 @martinwicke: What's the state of cmake in contrib?  Would it be practical to get tensorflow compiling _on_ a Pi rather than cross compiling? 
 My plan is to get Blaze running on a Pi if I can. CMake isn't very pleasant to work with for this, in my experience. I hope to get to this asap.
 Thanks for the update! That is helpful to see. I got as far as getting protobuf compiling, but I haven't made it to Bazel or TensorFlow itself yet. I'm hoping to work on that over the next few days.
 Thanks Sam! I managed to get the label_image example compiling and running on my Pi 2, here are my notes:
- I mostly followed the Jetson instructions too.
- There was an error in GPUBFCAllocator that meant I had to comment out a couple of lines.
- I had to link in the rt library to fix a clock_gettime() linking error, by adding "-lrt" to label_image's link_opts.
- The resulting binary ran extremely slowly, taking over a minute to run the Inception network. I believe this is because the compiler is not using NEON by default (since the Pi 1 doesn't have that). I will be retrying with NEON enabled.
 Sorry about the RPi breakage, should be fixed in d2a06c2df73ed09d9eff60efb559fd38bc47e2d3
  Good catch.  We should fix this and add a unit test.
 Fix in review.
  Yep, this is a good feature request.
 Yep.  For future reference: If you include "Fixes #<issue>" in a commit message it'll autoclose.
  Just to make sure: did you run `configure`?
 What version of numpy do you have installed in the python interpreter you
have used with configure?
On Dec 8, 2015 7:26 AM, "Geoffrey Irving" notifications@github.com wrote:

> Just to make sure: did you run configure?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/442#issuecomment-162952660
> .
 Comment if you'd still like help with this and we'll re-open.
  Fixed.  You're correct that the default in Python should be `3` instead of `None` so that it shows up in the docs.  Thanks!
  It's an x86 processor so it should be fine on CPU, and I believe we support the GTX960 (It's a Maxwell GPU with cuda compute capability 5).
 Yeah, it would be supported by TensorFlow, but is definitely on the low-side compared to most compute GPUs.
 @roschler , I would get at least a 980TI, but if I had to do it all over again, I would definitely get the titanx
  Please include the entire error message.
 @vrv: What's our range of supported compilers?  Should gcc 4.7 be fine?
 I'm not sure, 4.8+ is the safe assumption (the earliest we've tried successfully).
 We certainly haven't tested with it, so if it doesn't work, that may be a
sign that it isn't presently supported.

On Tue, Dec 8, 2015 at 9:39 PM Geoffrey Irving notifications@github.com
wrote:

> @vrv https://github.com/vrv: What's our range of supported compilers?
> Should gcc 4.7 be fine?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/439#issuecomment-163116441
> .
 We should give the compiler range in the README.
 Wow, that's fairly unpleasant.  Unfortunate I have no idea what's going wrong, and it seems to be fairly deep inside protobuf.  Would you mind filing a bug over at https://github.com/google/protobuf?  This seems like an issue on their end (apologies for the side stepping).
 (You're definitely helping us to figure out what those dependencies are, so thanks :)
 Incidentally, if you want to get unblocked, you might try clang.  I think we haven't found all these issues yet because most people on OS X compile with clang.  Though I suppose I'm not sure what bazel uses by default.
 https://groups.google.com/forum/#!topic/bazel-discuss/m2y2ZEB7zJE seems like a helpful thread on this topic
  Hm, I can't find basic_operations.py in our repo. I suspect you might have some local changes, possibly intermingled from https://github.com/aymericdamien/TensorFlow-Examples
  Hi Jeff, we sadly don't accept requests through github, only gerrit: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md

If you want, I can integrate these changes myself, or feel free to make the request through gerrit.  Let me know!
 Accepted via gerrit!
  That's a terribly uninformative error message which should be fixed, but until then: can I see the whole `bridge.py` or at least more of the context?
 It looks like you didn't include the code for the line that triggers the error, so I don't know what the problem is.  Wild, unlikely guess: Do you have Python 2 with `unicode_literals` imported from `__future__`?  I'm a bit surprised those variable names are showing up as unicode.
 I think I've made that mistake too, so it would be good to get a more sensible error message.  I'll try to do that, though it not be this year.
  Thanks, that's a good feature request.  Not sure if it'll fly, but in past computational geometry work I also like defining cross product for 2D by 2D to 1D, 1D by 2D to 1D, and 2D by 1D to 1D (treating 1D as vertical).
 I also want an operator that makes a vector v into the matrix V for which
Vw = v x w. Just for completeness, and we'll likely use it quickly if we
ever compute gradients rotations.

On Mon, Dec 7, 2015 at 1:13 PM Geoffrey Irving notifications@github.com
wrote:

> Thanks, that's a good feature request. Not sure if it'll fly, but in past
> computational geometry work I also like defining cross product for 2D by 2D
> to 1D, 1D by 2D to 1D, and 2D by 1D to 1D (treating 1D as vertical).
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/435#issuecomment-162664934
> .
 Yes, and we should have the star operator that converts a vector to that matrix as well, but we should still have the normal cross product operator for efficiency.
 I don't think anyone is working on it internally, but patches are welcome if you want to give it a try!  If anyone does so, it's fine to ignore my 2-D desires in the first pass; they have somewhat ambiguous shape semantics anyways.
 I'd expect well over 3x faster if native, but that's > 3x for just this op, not necessarily for your whole graph.
 Thanks!
  Hey TF,

In your seq2seq library you define your weights W, by setting your initializer to none:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L675

I'm trying to build Identity RNN's and Unitary RNN's in tensorflow. For identity rnn's they need to be initialized by a identity matrix. [Numpy easily makes this](http://docs.scipy.org/doc/numpy/reference/generated/numpy.identity.html)

How can this be done using `get_variable`? Is there way you can custom build your own matrix and then use `get_variable` and its initializer?

Also, when you set `initializer = None` in `get_variable` what is created? A matrix full of zeros? Thanks alot!  
 This is a better question for stackoverflow, but the answer is easy, so: `get_variable` takes an `initializer` argument which you can set to `constant_initializer` with any numpy array.  You can also easily build your own initializers if you want to build the initializer with tensorflow ops; look at the source for `constant_initializer` to see how.
 Thanks girving, I apologize for asking this in the wrong place. I will look at the source code for constant_initializer. If I get it working, I'll post here with the solution for identity matrix. 
 Actually, reopening and saying enhancement, since this is clearly something common enough to want in the code.
 @alexatknit: That looks pretty good, though before we check it in we'll probably want to split it into two functions: one op the does the work and one `identity_initializer` that uses it.
 @alexatknit thanks for the help! Works great. I'll be working on an intializer for complex numbers for the unitary RNN's . Again I appreciate both of your help. 
 drive-by: probably would be easy to do using np.diag()  (or tf.diag(), its equivalent).
 I think get_variable can now take a tf.Tensor as an initializer, which is probably sufficient if that tensor is a tf.constant set by a numpy array.  Closing due to inactivity, though we could re-open if this was insufficient.
  @colah: Do you have any idea what might be going on?
 Actually: can you show exactly when you did those print statements?  Did you import `mnist_softmax` or modify it in place?
 In fact, the contents of `W` are **not** all zeroes, but the first three and last three rows always are. (Presumably, this is because the top left and bottom right corners of the MNIST data are always zero, and `W` is initialized to zero, so those units are never activated.)

Printing individual rows shows that the first 12 units are set to zero, but subsequent rows have values:

```
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ -7.28181760e-07  -3.82109283e-06  -6.77165575e-04  -1.74198867e-05
  -1.33392937e-03  -3.06003931e-05   2.73057609e-03  -3.93098890e-06
  -9.00173545e-05  -5.72962977e-04]
[ -2.09516406e-06  -8.40815756e-06  -1.45535904e-03  -3.82293329e-05
  -2.93175876e-03  -6.70131849e-05   5.96566778e-03  -9.00283248e-06
  -1.97683170e-04  -1.25611678e-03]
[ -5.57180044e-07  -4.59386023e-08   3.04910136e-05  -9.54624539e-08
  -1.21446392e-05  -9.88030724e-09  -1.48593099e-05  -4.39917812e-07
  -6.41222869e-07  -1.69754321e-06]
...
```
  For now, we accept changes only via Gerrit, see: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#contributing-code
  We're fixing this in the next release so you can just type 'python ptb_word_lm.py', but thanks for the report.
 The pip install for 0.6.0 should have fixed this
 Updating the website now.

On Wed, Feb 10, 2016 at 12:18 PM Amr Abed notifications@github.com wrote:

> You still need to update the website tutorials to match this fix:
> https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#download-and-setup
> (0.5.0 -> 0.6.0)
> and
> 
> https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html#run-the-code
> (remove hint about bazel build)
> I didn't realize I was using version 0.5.0 until I came across this issue
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/431#issuecomment-182563896
> .
  Thanks, this is a good feature request.
 In review.
 I fixed this ages ago, but for some reason the commit message does show up in the history.
  @craigcitro: Do you know what's happening me?  My knowledge of pip is scant.
 I'd guess that the download failed for some reason, and we're just getting a bad error from it.

@severun  -- Can you run again with `bazel build -s` to see the full command-line and output?
 @severun well, it confirms that the failing step is indeed something about copying the file out of the downloaded six archive.

i'll admit that i'm shooting in the dark a bit here; two things to try:
- could you try running again with `-j 1` and see if that fixes things? (it'll be _much_ slower, since it forces bazel to do things serially, but we've seen lots of nondeterminism issues.)
- similarly, can you run `blaze build -s @six_archive//:copy_six` to confirm it works by itself?
 So this one is also an error just trying to copy files; two questions:
- any chance you're doing this on a partition that's nearly full?
- when you cloned the github repo, did you do `--recursive` (which also clones protobuf, the one git submodule we have)?
 `--recursive` and `--recurse-submodules` are the same, so no worries there.

It looks like bazel is using `TMPDIR=/home/alexander/development/alexander/.tmp`; can you do `df -h /home/alexander/development/alexander/.tmp` and see if it's short on space?
 OK, so all my easy hypotheses are no good.

Summoning @lberki, knower of all things bazel -- Lukas, how should we diagnose what's happening here? `bazel build` is failing at a copy step, even with `-j 1`.
 @lberki for the docker containers, we use 0.1.1. @severun -- what version of bazel are you using locally?
 (and install instructions suggest 0.1.1.)
  This is a perfectly reasonable feature request. I've change the title to be more general.

For uncompressed images in particular the workaround is rather easy: If you can load your image into a [width, height, channels] shaped numpy array, you can pass that to any TensorFlow function. That doesn't solve the reading from file question for ppm/bmp files or the like, but there are solutions for that in python, see for instance the pillow module: https://python-pillow.github.io/
 You can also do some small amount of decoding using [decode_raw](https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#decode_raw) and [string_to_number](https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#string_to_number).

It would be easy to add python wrappers around these for specific formats, assuming the image format encodes exactly as a raw byte grid.
 TensorFlow uses Eigen Tensor::Map to map memory internally.  Tensors are always assumed dense, row-major, and memory-aligned; for example, [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_types.h#L80) is the definition of a Matrix.  Working with external memory directly may be possible, but would require a lot of additional consideration; copying is really much easier.  And anyway for most image stuff you'd want to work on the GPU anyway, so you'll want to copy. Anyway, @keveman or @vrv could say more about working with existing memory directly.  You'd have to create an op that's either similar to a [VariableOp](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/variable_ops.cc) or that subclasses / is similar to [InitializableLookupTable](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/initializable_lookup_table.h#L25) and returns the frames wrapped as Tensors.  Again, there'll be some memory management considerations.
  Did you run ./configure from the base of the source tree?

On Mon, Dec 7, 2015 at 5:34 AM, Víctor Mayoral Vilches <
notifications@github.com> wrote:

> root@debian-arm:~/tensorflow# ../bazel-0.1.1/output/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --local_resources 2048,.5,1.0
> INFO: Waiting for response from Bazel server (pid 3825)...
> INFO: Found 1 target...
> INFO: From Executing genrule //util/python:python_check:
> 
> ERROR: Cannot find 'util/python/python_include'.  Did you run configure?
> 
> ERROR: /root/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command /bin/bash -c 'source tools/genrule/genrule-setup.sh; OUTPUTDIR="bazel-out/local_linux-opt/genfiles/util/python/"; ./util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked': com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 46.917s, Critical Path: 1.53s
> 
> Checked around and i don't seem to be missing any fundamental package. Is
> this a configuration issue and if so, how can I bypass it?
> 
> Thanks,
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/427.
  This is a better question for stackoverflow, since if we answer it there it'll be easy for future users to find the answer via search.  Can you repost and add the question link here?
  https://www.tensorflow.org/install/install_linux#common_installation_problems ?
  Hmm, this one is confusing, especially since process-tools.c is a part of bazel, not tensorflow.  I'm not sure why bazel would be doing anything of significance after the job is launched.  @vrv?
 @davidzchen, if he knows more
 Others have had the same error after they were out of memory. Could this be the problem? Is this still a problem in the latest release?
 This error generally seems to be a memory issue. There are a few other
people that have seen this and it was always OOM.

On Tue, Dec 15, 2015 at 11:59 AM roostapour notifications@github.com
wrote:

> Thanks everyone. I tried the latest release with @philwo
> https://github.com/philwo's steps and here are the last lines of the
> log:
> 
>   reading data line 16600000
>   reading data line 16700000
>   reading data line 16800000
> Killed
> 
> I think it is a memory limitation problem. I set up the TensorFlow on
> another VM machine with a bit more RAM and ran the translate example again.
> The error happened after several thousands more lines were processed.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/422#issuecomment-164877742
> .
 Closing due to inactivity.  Probably can better be addressed by running the example via binary install than through bazel.
  Hi Hamid,

The libpthread.so target is just a kludge to get around the fact that the
protobuf library attempts to link it in, despite this not being necessary
(or possible) on Android.

A cleaner workaround for you is to edit google/protobuf/BUILD and change
the line:

LINK_OPTS = ["-lpthread"]
to:
LINK_OPTS = [""]

Ideally the protobuf linkopts would be dependent on build target, or the
dummy libpthread.so file could be generated at compile-time so that it
would match the target architecture (will look into it).

On Sat, Dec 5, 2015 at 5:19 PM, Hamid Bazargani notifications@github.com
wrote:

> I tried to build the demo application for Android for KitKat device (API
> 20). I was able to modify the app not to use new hardware.camera2 and it
> worked on Android Lollipops successfully. However there is an issue
> targeting older Android device.
> It turned out that the app crashes right after trying to load the native
> shared library, tensorflow_demo in my case.
> And that's due to dependency of libpthread.so to libmediandk.so that is
> not supported for API older than 21. See the following logcat message:
> 
> Error: ..dlopen failed: could not load library "libmediandk.so" needed by
> "libtensorflow_demo.so"; caused by library "libmediandk.so" not found
> 
> I also desperately tried to replace the cloned libpthread.so with the
> same file from my /usr/arm-linux-gnueabi/lib/ directory with no success.
> I am using:
> 1- android-ndk-r10c
> 2- api_level=20
> 3- build_tools_version = "20.0.0"
> 
> Did I missed anything in that process?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/419.
 The libpthread.so in the TF repo is not actually a copy of libpthead.so
(it's actually arbitrarily cloned from libmediandk.so, which could explain
your earlier link errors), so it's not possible that it could be providing
anything. Those functions are found in other libraries on Android.
-DHAVE_PTHREAD still applies, it just doesn't need to actually link in
pthread.

If you:
1. remove the "-lpthread" from google/protobuf/BUILD,
2. delete the ":dummy_pthread" dep in tensorflow/examples/android/BUILD
3. build with:
   bazel build tensorflow/examples/android:tensorflow_demo
   --config=android_arm

with no other changes do you still get the same errors? Please attach a log
if so.

On Sat, Dec 5, 2015 at 8:13 PM, Hamid Bazargani notifications@github.com
wrote:

> Hi Andrew,
> I think pthread also resolves some POSIX symbols like getPageSize, strtof,
> ... that are used elsewhere.
> By removing -lpthread, I still get unresolved reference errors.
> How should I resolve them by removing pthread ?
> Thanks,
> Hamid
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/419#issuecomment-162261581
> .
 Thanks Hamid. I'm guessing this might be a discrepancy between what the
compiler expects of API level 21 and what the device provides at level 20.

I've just tried setting my WORKSPACE ndk api_level to 20 and found that the
Android demo still builds. Could you try the same and see if that corrects
the issue?

On Sat, Dec 5, 2015 at 9:33 PM, Hamid Bazargani notifications@github.com
wrote:

> I followed your instruction. Still no success.
> Following is my steps:
> 1- changed google/protobuf/BUILD
> 
> // [""] returns bazel error
> LINK_OPTS = []
> 
> 2- changed tensorflow/examples/android/BUILD
> 
> cc_library(
>     name = "tensorflow_native_libs",
>     srcs = glob(["jni/**/*.cc"]),
>     hdrs = glob(["jni/**/*.h"]),
>     copts = [
>         "-std=c++11",
>         "-mfpu=neon",
>         "-O2",
>     ],
>     linkopts = ["-llog -landroid -lm -ljnigraphics"],
>     tags = [
>         "manual",
>         "notap",
>     ],
>     deps = [
>     #    ":dummy_pthread", // commented
>         "//tensorflow/core:android_tensorflow_lib",
>     ],
> )
> 
> 3- My workspace looks:
> 
> android_sdk_repository(
>     name = "androidsdk",
>     api_level = 20,
>     build_tools_version = "20.0.0",
>     # Replace with path to Android SDK on your system
>     path = "/hamidb/software/TADP/android-sdk-linux",
> )
> android_ndk_repository(
>     name="androidndk",
>     path="/hamidb/software/TADP/android-ndk-r10c",
>     api_level=21)
> 
> 4- I issued (I also tried without android_arm)
> 
> bazel build --verbose_failures
> //tensorflow/examples/android:tensorflow_demo --config=android_arm
> After running the apk file my logcat logs:
> 
> --------- beginning of /dev/log/main
> D/Raydium_ts_main( 242): ## Report rate in 10 seconds =1213
> I/art ( 1219): GcCauseBackground partial concurrent mark sweep GC freed
> 6011(335KB) AllocSpace objects, 27(1351KB) LOS objects, 12% free, 6MB/7MB,
> paused 5.823ms total 48.435ms
> --------- beginning of /dev/log/system
> I/ActivityManager( 904): START u0 {act=android.intent.action.MAIN
> cat=[android.intent.category.LAUNCHER] flg=0x10200000
> cmp=org.tensorflow.demo/.CameraActivity bnds=[984,1381][1176,1573]} from
> pid 1219
> W/art ( 166): Could not get current activity
> I/ActivityManager( 904): Start proc org.tensorflow.demo for activity
> org.tensorflow.demo/.CameraActivity: pid=11918 uid=10073 gids={50073, 1028,
> 1015}
> W/NvAppProfileService( 904): App Profiles: Enabled
> I/PowerServiceCient( 904): Successfully bound to service
> _E/art (11918):
> dlopen("/data/app-lib/org.tensorflow.demo-1/libtensorflow_demo.so",
> RTLD_LAZY) failed: dlopen failed: cannot locate symbol "getpagesize"
> referenced by "libtensorflow_demo.so"..._
> D/AndroidRuntime(11918): Shutting down VM
> E/AndroidRuntime(11918): FATAL EXCEPTION: main
> E/AndroidRuntime(11918): Process: org.tensorflow.demo, PID: 11918
> _E/AndroidRuntime(11918): java.lang.UnsatisfiedLinkError: dlopen failed:
> cannot locate symbol "getpagesize" referenced by "libtensorflow_demo.so"..._
> E/AndroidRuntime(11918): at java.lang.Runtime.loadLibrary(Runtime.java:364)
> E/AndroidRuntime(11918): at java.lang.System.loadLibrary(System.java:526)
> E/AndroidRuntime(11918): at
> org.tensorflow.demo.TensorflowClassifier.(TensorflowClassifier.java:46)
> E/AndroidRuntime(11918): at
> org.tensorflow.demo.TensorflowImageListener.(TensorflowImageListener.java:51)
> E/AndroidRuntime(11918): at
> org.tensorflow.demo.CameraConnectionFragment.(CameraConnectionFragment.java:212)
> E/AndroidRuntime(11918): at
> org.tensorflow.demo.CameraConnectionFragment.newInstance(CameraConnectionFragment.java:91)
> E/AndroidRuntime(11918): at
> org.tensorflow.demo.CameraActivity.onCreate(CameraActivity.java:30)
> E/AndroidRuntime(11918): at
> android.app.Activity.performCreate(Activity.java:5231)
> E/AndroidRuntime(11918): at
> android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1087)
> E/AndroidRuntime(11918): at
> android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2160)
> E/AndroidRuntime(11918): at
> android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2246)
> E/AndroidRuntime(11918): at
> android.app.ActivityThread.access$800(ActivityThread.java:136)
> E/AndroidRuntime(11918): at
> android.app.ActivityThread$H.handleMessage(ActivityThread.java:1197)
> E/AndroidRuntime(11918): at
> android.os.Handler.dispatchMessage(Handler.java:102)
> E/AndroidRuntime(11918): at android.os.Looper.loop(Looper.java:136)
> E/AndroidRuntime(11918): at
> android.app.ActivityThread.main(ActivityThread.java:5030)
> E/AndroidRuntime(11918): at java.lang.reflect.Method.invoke(Native Method)
> E/AndroidRuntime(11918): at
> com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:793)
> E/AndroidRuntime(11918): at
> com.android.internal.os.ZygoteInit.main(ZygoteInit.java:609)
> W/ActivityManager( 904): Force finishing activity
> org.tensorflow.demo/.CameraActivity
> W/ActivityManager( 904): Activity pause timeout for
> ActivityRecord{65595fd8 u0 org.tensorflow.demo/.CameraActivity t73 f}
> I/Process (11918): Sending signal. PID: 11918 SIG: 9
> W/InputMethodManagerService( 904): Window already focused, ignoring focus
> gain of: com.android.internal.view.IInputMethodClient$Stub$Proxy@65367aa0
> attribute=null, token = android.os.BinderProxy@651cfa9
> https://github.com/android.os.BinderProxy/tensorflow/commit/651cfa98
> I/ActivityManager( 904): Process org.tensorflow.demo (pid 11918) has died.
> D/Raydium_ts_main( 242): ## Report rate in 10 seconds =1038
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/419#issuecomment-162264595
> .
 Great, glad that worked!

I have a patch for the original libpthread.so issue coming, so that it will
generate the dummy lib at compile-time and make things more portable.

On Sun, Dec 6, 2015 at 10:03 AM, Vincent Vanhoucke <notifications@github.com

> wrote:
> 
> Closed #419 https://github.com/tensorflow/tensorflow/issues/419.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/419#event-483728500.
 It should, if you follow these steps:
1. patch CameraConnectionFragment.java to remove the use of the camera2 API and use android.hardware.Camera instead
2. change the API level defined by AndroidManifest.xml
3. set the API level of your NDK appropriately in WORKSPACE

Note that "--config=android_arm" is not necessary or beneficial, and was only included by mistake in earlier comments.
 @CalmWaves 
You can checkout my fork from [here](https://github.com/hamidb/tensorflow/tree/api20) enabling API<20. 
  We're tracking our current lack of indexing fanciness here: #206.

However, I'm less clear on the second part.  What errors are you seeing?  `tf.gather` can't yet take multiple index arguments (that's also part of #206); is that what you mean?
 Method 1 won't work until we fix #206.  I don't know why method 2 doesn't work, since you didn't include the error message.
 Thanks!  The `cpu` case is a bug in the gradient of `tf.gather`, which wasn't correctly updated when I made `gather` handle arbitrary rank indices (including scalars).  I'll fix that.

I'm not sure about the `gpu` case, but it's possible that we don't yet allow scalars in some places on the GPU for Eigen-related reasons.  @benoitsteiner: Is that possible?
 Upon further investigation, fixing (2) in an efficient way seems tricky.  The problem is that an optimizer step for two nested calls to `tf.gather` can't be efficient unless `tf.scatter` handles multiple index arguments.  Specifically, we'd need to make `IndexedSlices` accept multiple index arguments so that the gradient of `tf.gather` could build one `IndexedSlices` and then build a deeper one for the next `tf.gather` call.

I'm going to leave this open as a bug and reference it from #206.  Apologies that I won't have a fix for you soon.
 Here is a small failing test case for when someone gets to this.

```
def testNestedGather(self):
  """Catch https://github.com/tensorflow/tensorflow/issues/418."""
  with self.test_session() as sess:
    init = np.arange(6).reshape(2, 3).astype(np.float32)
    var = tf.Variable(init)
    i = tf.constant(1)
    j = tf.constant(2)
    loss = tf.gather(tf.gather(var, i), j)
    sess.run(tf.initialize_all_variables())
    self.assertEqual(loss.eval(), init[1, 2])
    # Run one step of optimization
    optimizer = tf.train.GradientDescentOptimizer(0.01)
    train = optimizer.minimize(loss)
    sess.run(train)
```
 Unfortunately no.  As a workaround until it gets fixed, does this work for you?

```
tf.gather(tf.reshape(matrix, [-1]), i * tf.shape(matrix)[1] + j)
```
 Will try to work on an efficient multi-index gather this week.  No promises on timelines though.
 Please see the new op gather_nd in array ops at HEAD/ in the nightly build.
 Closing as a duplicate of part of #206.
  I think the udacity course materials are the right place to put these types of docs, since they're more geared towards teaching the theory behind the practice.
  Thanks for the report, fixed now.
  I recently fixed `tf.range` to accept between 1 and 3 arguments (like Python's `range`).  This change will be part of the upcoming 0.6.0 release, but you're using the 0.5.0 release, and a version of cifar10 from git.  Quick fix: use `tf.range(0, FLAGS.batch_size)` instead of `tf.range(FLAGS.batch_size)`.
  If building from HEAD, you should be able to use ./configure to specify the path of the python you want to use -- let us know if that works.
 where is your libpython2.7.so.1.0?  what directory is it in?

have you tried using a virtualenv with that python, and pointing configure at that virtualenv?
 Can you do something for me?  Load up the version of python you set up with config, and run:

from distutils import sysconfig
print sysconfig.get_config_vars('LIBPL')

it should return a path.  can you reply with that path?  also, for the path, type: ls -l THAT_PATH  (and paste the results here as well)
 @girving  we may need to point the pythonX_path to a script (copied from a template via e.g. sed) which sets the LD_LIBRARY_PATH correctly based on e.g. LIBPL output, and calls the python interpreter.
 Interesting.  Can you run the python-config associated with that python
bin?  i imagine it's something like:

/share/software/python/2.7.10/Python-2.7.10/release/python-config

can you run it separately with each of: --ldflags, --cflags, --includes,
--libs, --prefix, --exec-prefix?

thanks!

On Sat, Dec 5, 2015 at 9:40 PM, Luzy notifications@github.com wrote:

> Hi ebrevdo,
> libpython2.7.so.1.0 is under
> /share/software/python/2.7.10/Python-2.7.10/release/lib
> 
> Here is the output.
> print sysconfig.get_config_vars('LIBPL')
> 
> ['/share/software/python/2.7.10/Python-2.7.10/release/lib/python2.7/config']
> 
> ls -l
> /share/software/python/2.7.10/Python-2.7.10/release/lib/python2.7/config
> total 9768
> -rw-rw-r--. 1 xxxx 2200 Oct 2 15:44 config.c
> -rw-rw-r--. 1 xxxx 1507 Oct 2 15:44 config.c.in
> -rwxrwxr-x. 1 xxxx 7122 Oct 2 15:44 install-sh
> -rw-rw-r--. 1 xxxx 9883714 Oct 2 15:44 libpython2.7.a
> -rw-rw-r--. 1 xxxx 49734 Oct 2 15:44 Makefile
> -rwxrwxr-x. 1 xxxx 7431 Oct 2 15:44 makesetup
> -rw-rw-r--. 1 xxxx 6256 Oct 2 15:44 python.o
> -rw-rw-r--. 1 xxxx 18479 Oct 2 15:44 Setup
> -rw-rw-r--. 1 xxxx 368 Oct 2 15:44 Setup.config
> -rw-rw-r--. 1 xxxx 41 Oct 2 15:44 Setup.local
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/414#issuecomment-162272677
> .
 On my machine, libpython...so is in
  /usr/lib/python2.7/config-x86_64-linux-gnu
and my python-config --ldflags adds this line:
  -L/usr/lib/python2.7/config-x86_64-linux-gnu -L/usr/lib

in contrast, on your system we can't use python-config to get the location of the so file.

In addition, while on my system I can find a copy of the .so file in my LIBPL directory, on your system it isn't there.  It's very strange.

You seem to have a fairly nonstandard installation of python 2.7.  Are you an administrator of this machine?  Perhaps you could reinstall python?  Or install a version of python in your home directory, where you have write permissions and can install a properly built version?
 @vrv: Reassigning since Eugene is out.
 The warnings are fine. The underlying Eigen library doesn't always mark
functions as OK to run on a gpu, even if they are. That results in many of
the warnings.
On Dec 8, 2015 7:25 PM, "Luzy" notifications@github.com wrote:

> Hi,
> Here is an update. Unfortunately I don't have admin access to the machine.
> I installed another python with config
> ./configure --prefix=$PWD/release/ --enable-shared
> LDFLAGS="-L$PWD/release/lib/python2.7/config
> -Wl,--rpath=$PWD/release/lib/python2.7/config -L$PWD/release/lib/
> -Wl,--rpath=$PWD/release/lib/" CPPFLAGS="-I$PWD/release/include"
> (I found these from some stackoverflow answers, though I don't fully
> understand how it works yet)
> 
> though libpython...so is still not under lib/python2.7/config/, nor does
> python-config --ldflags
> -lpython2.7 -lpthread -ldl -lutil -lm -Xlinker -export-dynamic return
> -L....
> 
> I was able to get pass that libpython2.7.so.1.0 not found error building
> tensorflow.
> 
> But I encountered error
> /usr/bin/env: python2.7: No such file or directory
> ERROR: /share/software/tensorflow/google/protobuf/BUILD:270:1: Linking of
> rule '//google/protobuf:protoc' failed: crosstool_wrapper_driver_is_not_gcc
> failed: ....
> 
> because the command generated by bazel seems not to set the PATH variable
> (I do not understand why)
> (cd
> /home/zhiyunlu/.cache/bazel/_bazel_zhiyunlu/d568262eb4464bf011ab3d998aff21ac/tensorflow
> && \
> exec env - \
> 
> third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc....)
> 
> So I fixed the problem by changing the first line of
> crosstool_wrapper_driver_is_not_gcc to be
> #!/share/software/python/2.7.10/Python-2.7.10-tf/release/bin/python2
> to run with specific python I installed.
> 
> With some other modifications to use local gcc installation version (
> issue649 https://github.com/bazelbuild/bazel/issues/649), I am able to
> build tensorflow successfully.
> 
> I saw many warnings, for example
> warning: calling a _host_ function from a _host_ _device_ function is not
> allowed.
> But I assume it is okay...?
> 
> So I think the libpython...so problem is fixed now. Thanks a lot for
> helping this out!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/414#issuecomment-163113984
> .
  I'm not entirely sure, this might be a better question for the bazel team at https://github.com/bazelbuild/bazel.  Hopefully the use of 'bazel' isn't viral.
 Can you set up your binary similar to any of the checked-in model? 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/alexnet/BUILD

You can run either as: 
bazel run -c opt --config=cuda third_party/tensorflow/models/image/alexnet:alexnet_benchmark

Or 
bazel build -c opt --config=cuda third_party/tensorflow/models/image/alexnet:alexnet_benchmark
bazel-bin/third_party/tensorflow/models/image/alexnet/alexnet_benchmark
 This is great! I'll make this part of os_setup.md if you don't mind (or you can)?
  How are you using `train_op` downstream?  What you're doing seems like it should work as you expect from what you've written, so I think we need to see the rest of the context.
 Closing due to lack of activity / follow up.  Feel free to comment and we'll re-open.
 Are you running the `train_op`?
  I was able to reproduce the error. Working on a fix. 
  I'm not sure exactly how you are trying to read the two files in parallel.  Doing that correctly can be a bit awkward, especially if you want to use multiple threads.  What reader(s) are you using?  Can you show the code you are using for your input pipeline?  The most robust approach would be a custom reader.
 Looks like this fell through the cracks.  @panmari: Is this still an issue, and do you have code if so?  For better or worse, tensorflow exposes a lot of the complexity of asynchronous programming to users, so it may be that this is intended behavior.
  What version of bazel do you happen to be running?

@davidzchen, in case he knows :)
 This does seem like a bazel bug, btw -- this doesn't even hit the tensorflow side of things before it fails.  Can you report a bug there and then come back to us if it turns out that it's a problem on our end?
 Closing for now until there's more evidence there's something the TF team can help with.
  Matrix-vector can be done via a reshape of the vector, though #216 is the feature request to make a 'numpy.dot' equivalent that automatically broadcasts.

I'm not an expert, but if bilinear tensor product is roughly equivalent to tensor contraction, then it should be somewhat straightforward to add tensor contraction as an operation: our CPU matmul is actually using eigen's tensor contraction underneath.
  Do you want to follow our [contributing](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) guidelines and send us a request via Gerrit, or would you rather us make the change for you?
 I'll apply it now.  Thanks!
 @azzolini: To satisfy my curiosity, what was the error message?  It's useful to include the full error message for this kind of issue.
 In review.
  Do you want to follow our [contributing](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) guidelines and send us a request via Gerrit, or would you rather us make the change for you?
 What error message do you get?  I'd like to understand why auto is problematic.
 @azzolini: we now accept PRs, so if you want to make the fix and also let us know what the error you were getting was, please do!
 Closing due to lack of activity.  Please reopen if this is still an issue!  If so, please include the error messages that result.
  Good request! Bidirectional RNN code is checked in internally, will go out some time soon with the next release (within a few weeks).
 Is the code within github now? or is it still to come as you said in the next release?
 Our policy is that until it is documented, assume it is private and not ready to be used.

@ludimagister: let's reopen this until it's actually done and public
 Okay great! That's a pretty smart policy. Thanks @vrv 
 Yes, for the backward RNN it will internally reverse the input sequences up to a specified length (per sequence for the given batch) and reverse then this output to produce the backward output which is then concatenated with the forward output, so the padding will be correct. The unit test should make this clear once the code is pushed.
 No, it doesn't support padding at the front, it only supports to pass in the lengths of each individual sequence in the batch, and will do the correct forward/backward pass on them, up to the specified length. Since every sequence is independent so I am not sure why it would make sense to support aligning them on the right (we picked to align them on the left).
 I plan to add support for this but it won't happen until at least January.
On Dec 9, 2015 5:37 PM, "Raingo" notifications@github.com wrote:

> @ludimagister https://github.com/ludimagister Padding on the front
> (aligning on the right) is usually used for the inputs of the sequence to
> sequence model.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/403#issuecomment-163477316
> .
 Yes we should remove this.  +ludigmaster@
On Dec 16, 2015 5:17 PM, "Stephen Merity" notifications@github.com wrote:

Even though this is not yet public / documented I have a small note.
Sequence length should not be required for bidirectional_rnn as _reverse_seq
simply reverses the input if lengths is not given. This trivially covers
cases where the input is already padded or where the input is a static
number of time steps.

   if not isinstance(inputs, list):
     raise TypeError("inputs must be a list")
-  if not sequence_length:
-    raise ValueError("sequence_length has to be defined")
  if not inputs:
   raise ValueError("inputs must not be empty")

—
Reply to this email directly or view it on GitHub
https://github.com/tensorflow/tensorflow/issues/403#issuecomment-165325888
.
 @Smerity Yes, very good point, will fix tomorrow.
 On Tue, Dec 29, 2015 at 10:37 AM, Fabrizio Milo notifications@github.com
wrote:

> Any good paper that you would recommend explaining bi-directional RNN ?
> 
> http://lmgtfy.com/?q=bidirectional+recurrent+neural+networks
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/403#issuecomment-167849917
> .
 @ludimagister, @ebrevdo: What's the status of bidirectional RNNs?  
  I believe that the currently released model corresponds to: http://arxiv.org/abs/1409.4842
@Yangqing would know exactly.
We don't have more details to release about that particular model beyond what's published.
A more recent version with lots of details about the training process is described here: http://arxiv.org/abs/1512.00567
  I think https://github.com/tensorflow/tensorflow/pull/337 might have fixed this -- this will be in the next release but you can probably build from sources to get the fix now.
 @NumesSanguis As well as relative paths being fixed, I believe it now prints the path right before the line `Starting TensorBoard on port 6006` (although I'm not sure when that change was made).
<img width="730" alt="screen shot 2015-12-23 at 2 11 09 pm" src="https://cloud.githubusercontent.com/assets/511499/11983846/bca15cf8-a984-11e5-946d-5291ca41021d.PNG">
  The tutorial has been released.
https://www.tensorflow.org/versions/master/tutorials/image_recognition/index.html

Below is a cut-and-paste from a recent announcement sent to discuss@tensorflow.org.

# 

Dear TensorFlow community,

Today we are releasing our best image classifier trained on ImageNet data. As described in our recent Arxiv preprint at http://arxiv.org/abs/1512.00567, an ensemble of four of these models achieves 3.46% top-5 error on the validation set of the ImageNet whole image ILSVRC2012 classification task (compared with our ensemble from last year that won the 2014 ImageNet classification challenge with a 6.66% top-5 error rate).

In this release, we are supplying code and data files containing the trained model parameters for running the image classifier on:
Both desktop and mobile environments
Employing either a C++ or Python API.

In addition, we are providing a tutorial that describes how to use the image recognition system for a variety of use-cases.
    http://www.tensorflow.org/tutorials/image_recognition/index.html

This release allows one to compute higher-level visual features and/or perform image recognition on the ImageNet 1000 object label set*. We are actively working on refactoring our code so that we can open-source a complete training system for this model so that others can train on their own data or fine-tune it for other purposes (Some custom training operations for this model rely on code that is shared between our non-open-sourced DistBelief system and TensorFlow and we are actively working on disentangling this).

We wish to especially acknowledge Christian Szegedy, Sergey Ioffe and Vincent Vanhoucke for developing this network and helping make this happen.

Happy Holidays-
Sherry, Pete, Chris, Jon with contributions from many team members
- The 1000 object labels are listed at:
  http://image-net.org/challenges/LSVRC/2014/browse-synsets
  Probably a dup of https://github.com/tensorflow/tensorflow/issues/209 -- Yann's website is probably getting taken down by all of these downloads occasionally.  We haven't put up a mirror since I don't think we (yet) have the rights to - de-duping with 209 for now.
 It's possible - it's probably worth you instrumenting https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py#L59 to figure out what you need to get access.
  If you include the full logs of the output when the GPU runs out of memory, we might be able to help.

We are also working on user tools to diagnose memory problems, so look out for that in the future
 Going to de-dupe with the more general 'memory issue' here: https://github.com/tensorflow/tensorflow/issues/492 since I think there's a lot of stuff we need to improve.
  Fixed in next push.
  If you build from source, you need to use bazel to build TensorBoard, e.g. `bazel build //tensorflow/tensorboard:tensorboard` and then `./bazel-bin/tensorflow/tensorboard --logdir=foo`. Or, for convenience, `bazel run //tensorflow/tensorboard:tensorboard -- --logdir=foo`. It's bazel that takes responsibility for finding TensorBoard's external dependencies and setting them up properly.

If you want to be able to run TensorBoard just using the python command, and you also want to build from source, you can build the pip package (tensorflow/tools/pip_package:build_pip_package) and then if you install from that pip package you'll have a totally self-contained TensorBoard.

Going to leave this open, please close it if this fixes the issue for you (also, maybe I should update the README with this info...)
  Updated subject to reflect the environment you're trying to run in.  Hopefully someone in the community who knows more about bumblebee/optimus laptops might be able to help!
 @vrv: Assigning you since it doesn't let me assign zheng-xq.  Do you know why?
 @girving: as discussed offline, fixing that.

Yeah, CUDA_ERROR_UNKNOWN is not very helpful.  hopefully @zheng-xq might know more what's going on here
 @jpmerc, could you run your command line through sudo, similar to you C++ examples? I wonder whether it is the root access that is making the difference. The initialization logic should be the same between C++ and Python clients. 
 Interesting. Could you add the path to your Cuda 7.0 runtime to LD_LIBRARY_PATH? 
 Could you run `nvidia-debugdump -l` or `nvidia-smi` and paste the output? I had a similar problem and in the end it was a lack of power for the graphics card. 
 Well, it was worth a shot.

On Tue, Dec 15, 2015 at 3:28 PM PeterBeukelman notifications@github.com
wrote:

> Found 1 NVIDIA devices
> Device ID: 0
> Device name: GeForce GTX TITAN X (*PrimaryCard)
> GPU internal ID: 0420115018258
> 
> Tue Dec 15 23:56:17 2015
> 
> +------------------------------------------------------+
> 
> | NVIDIA-SMI 352.63 Driver Version: 352.63 |
> 
> |-------------------------------+----------------------+----------------------+
> | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
> | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
> 
> |===============================+======================+======================|
> | 0 GeForce GTX TIT... Off | 0000:04:00.0 On | N/A |
> | 22% 33C P8 17W / 250W | 441MiB / 12287MiB | 1% Default |
> 
> +-------------------------------+----------------------+----------------------+
> 
> +-----------------------------------------------------------------------------+
> | Processes: GPU Memory |
> | GPU PID Type Process name Usage |
> 
> |=============================================================================|
> | 0 1270 G /usr/bin/X 174MiB |
> | 0 2183 G compiz 112MiB |
> | 0 2575 G ...ves-passed-by-fd --v8-snapshot-passed-by- 127MiB |
> 
> +-----------------------------------------------------------------------------+
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/394#issuecomment-164926222
> .
 @jpmerc, could you try to set LD_LIBRARY_PATH inside your sudo? That should make sudo preserve the environment variables. 

sudo LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.0/lib64 optirun python convolutional.py 

@PeterBeukelman, to make sure you have the same problem, could you run the C++ tutorial? 

bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
 @jpmerc, could you confirm that you run TF_UNOFFICIAL_SETTING=1 ./configure before starting the build?

@PeterBeukelman, I think they are most likely separate issues. Note that Bazel is best supported on Ubuntu 14.04 at the moment. The default gcc with that is 4.8. 
 @martinwicke, @zheng-xq: Is this obsolete now that we support 7.5?
 It should be fixed. I'll close this for now -- we can reopen if it's still a problem.
  Have you tried following this tutorial? http://www.tensorflow.org/how_tos/summaries_and_tensorboard/index.html

Note you need to create `scalar_summary` ops and run them, and write them to the SummaryWriter - just creating a SummaryWriter on its own is not enough.

In the invocation you showed there should be a graph definition written - just making sure, have you clicked the "Graph" tab in the top right corner? 
 Which browser are you on? I think the graph visualizer is broken on FF at the moment. Does everything work if you're on chrome?
 @esraahassan can you explain what you mean by "please solve this problem in video"?
  This should be fixed in our 0.6.0 release.
  https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/tile_ops.cc#L383

Looks like it :(

@girving: is there a fix on the way or is this something more fundamental?
 Tile is a nop for scalars, so we just need to copy the input to the output.  Eigen can stay broken. :)
 Fix in review.
 Fixed.  @bndnn: Thank you for reporting!
 We push to github/googlesource at the same time -- I'll try to upstream our commits in a few hours.
  Want to send us a PR to fix?
  Is this error produced reliably if you run cifar10_eval.py? If so, would you describe exactly how you produced this error as I am not seeing this error when I run the script? Thanks!
 I presume this error is not longer an issue so I will be marking this issue as closed but please feel free to reopen if this error persists.
 I don't see this in behavior in the latest version (head)
 @ebrevdo: Do you have any thoughts here?  Reassigning to you since Sherry is out. 
 @yuanpengX @atamahjoubfar is this still an issue?  Is it just the verbose logging that's troubling you?  when you reach the end of the input file, the queue dies and the training is restarted from the beginning.  that may explain all the warnings.
 @atamahjoubfar Thanks!  Closing for now; please comment or file another if it comes up again.
  Moving email discussion here:

Rust bindings would be cool!  I don't think there's anyone working on rust bindings so far, though it might be worth an email to discuss@tensorflow.org to check.  Agreed that a separate project is probably good to start out.

To start the conversation, there are a couple different levels of bindings:
1. Bindings that do not know about specific ops, but can create graphs "manually" or load them from GraphDefs, evaluate graphs (so that tensorflow models can be run inside rust servers), etc.  There's hopefully nothing blocking that.
2. Autogenerated bindings for each C++ op.  We do this for Python, and similar things should work for Rust (ideally with a lot more type safety).  I have limited intuition for rust's handling of ad-hoc polymorphism, so I can't judge how easy it'll be to fit tensorflow's notions of polymorphism into rust's.  Again, there's hopefully nothing blocking this.
3. Idiomatic bindings.  We currently have a lot of logic in pure Python, including both per-op sugar and key features such as automatic differentiation.  We will likely eventually move this to C++, but until then it will be difficult to capture this functionality in rust bindings without duplicating a ton of effort.  Unfortunately, automatic differentiation in particular is (mostly) necessary if you want to train models.

It's probably best to shoot for (1) to start and only move to (2) once it's clear how much of (3) will be a blocker for the desired applications.
 Nice!  I'm happy to help with any issues you run into.
 Cool!
  A lot of the functionality of tensorflow is written in python, and will
migrate only slowly down to C++.

The GraphDef proto should only change slowly (and with the best guarantees
for compatibility), so writing protos directly will require the least
velocity of change to keep things working. If your main goal is to run
existing graphs from R (that's unlikely to be the case, but that's the
situation for node.js or Java where the main focus is on running
pre-trained models), then this would be sufficient.

On the other hand, the API will be significantly less powerful if you cut
away the python layer entirely. Mainly this is because the gradients and
shape inference layers are written in python, but also some ops are python
functions that combine kernels into more complex ops (image_ops is a good
example). This makes #3 look much better than it otherwise would, even
though the API is changing faster than the GraphDef is.

On Sun, Dec 13, 2015 at 4:27 AM JJ Allaire notifications@github.com wrote:

> @girving https://github.com/girving Interested in your guidance on the
> various ways to pursue R bindings. Read your comment here providing the lay
> of the land for Rust (#388 (comment)
> https://github.com/tensorflow/tensorflow/issues/388#issuecomment-161019498).
> We obviously can pursue SWIG bindings to the existing C++ classes and then
> pickup additional functionality from the C++ layer once more of the
> features from the python layer are moved there.
> 
> However, we're most interested in creating idiomatic bindings for R and to
> get this exactly right are in no way deterred by the fact that it may take
> a lot of effort (much of which is duplicated). I can think of a few ways to
> approach creating these bindings:
> 
>    1.
> 
>    Create a wrapper for sessions / invocation using the C API then create
>    the graph definition bindings by going directly to proto. This possibility
>    seems to be anticipated/encouraged here:
>    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h#L55-L59
>    2.
> 
>    Call the C++ layer via SWIG bindings.
>    3.
> 
>    Call python directly from R using e.g. boost::python or pybind11.
>    Assuming that NumPy arrays can be marshaled to from R arrays with no
>    copying I'm assuming this would have no performance problems since the
>    python code is just defining a graph.
> 
> To me the first option has the greatest appeal because there would be no
> impedance problems associated with translating idiomatic R into graphs,
> we'd just figure out the right syntax and express it as a graph definition
> rather than contorting it through the C++ layer. The comment above seems to
> imply that this would be an effective path, but I haven't spent enough time
> looking to know whether this would be a monumental amount of work that
> would be constantly challenged to keep up or just something to grind
> through once (not deterred at all by having to spend on the order of
> hundreds of hours in the initial effort). I also like the idea of using the
> C API because those entry points could be loaded dynamically, meaning that
> the R bindings could be built separately and made available on CRAN and
> could work with any installed version of TensorFlow.
> 
> We could also combine the creation of proto-based idiomatic R bindings
> with another binding that parrots the python API. This would allow R users
> to easily translate and use python or C++ API based examples but at the
> same time have a first class R binding that makes maximum sense to R users.
> 
> Your thoughts and feedback very much appreciated!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/387#issuecomment-164253513
> .
 We have the op registry, but it only contains the C++ api, as well as the functions for gradients and shape inference. The closest we have for the python layer is the API docs, which is generated by scraping the source code, and therefore should always be up to date. This quickly turns icky, but you could actually get a reasonable snapshot by going through the python doc .md files and collecting headlines.
  Hey TF,

Trying to integrate the new Unitary RNN's into tensorflow. Unfortunately, they require Fast Fourier Transform and Inverse Fast Fourier Transforms. 

For efficiency this needs to be done on the GPU. On the author's code (theano), they do this by making a Theano Op, and inserting scikit's Fast Fourier and Inverse Fast Fourier Here:

https://github.com/amarshah/complex_RNN/blob/master/fftconv.py

How can this be done in TF? Upon completing the Unitary RNN, I plan to share it to everyone! I asked on the google groups, but didn't get a reply. 
 We'll need to add a few FFT ops to get this done -- probably using cufft or fbfft for GPU.  Assigning to @zffchen78 since he has experience in ffts and adding ops :)
 Thank you @vrv and @zffchen78 . Basically, these new types of RNN's are very promising and I hope to integrate them into TensorFlow over the next week if possible. I'm starting to work on it here:
https://github.com/LeavesBreathe/Seq2Seq_Upgrade_TensorFlow

However, it will not be possible to do without FFT and IFFT support so I really appreciate your help!
 Well the problem is that the actual unitary RNN uses FFT and IFFT within its cell. Ideally you want the cell's computation done in parallel on the GPU. If its on the CPU, its going to take forever.

Here's the paper: Look at their main equation http://arxiv.org/pdf/1511.06464v1.pdf (its number 6)
 We'll try to provide an implementation for both.
 Perfect! Thanks! @adder, I primarily use GPUs and if you're going to use unitary RNN's for some serious language modeling, you gotta use GPU's (10x faster training time). 

I know they may converge more quickly (one of the benefits), but keep in mind that an epoch usually takes at least 24 hrs with normal LSTM's and GRU's for big datasets. So we simply can't afford to put them on CPUs. 
 @LeavesBreathe  , can you elaborate a bit more on your requirements about FFT/IFFT?
Specifically,
1) do you need r2c (real-to-complex), c2r(complex-to-real), or c2c(complex-complex) fft/ifft support?
2) do you need 1D, 2D, or 3D fft/ifft? or all of them?
3) do you need support for batching?
4) In your original email, you pointed to theano's code fftconv.py, which uses fft/ifft to implement conv2d operation. Do you need conv2d in the end? If that's all you want, tf has conv2d operation already and they run on gpu using cudnn. As I was told, newer version of cudnn uses fft under the hood already.
 @LeavesBreathe, is this the paper you are trying to reproduce? http://arxiv.org/pdf/1511.06464v1.pdf And formula (6) is the core computation you need to do. AFAIK, you need 2D, non-batched, complex-to-complex fft/ifft. Is that correct?
 Thanks @adder! @zffchen78, thanks for your help in this matter. I will do my best to answer your questions. The ultimate goal is to replicate this paper: http://arxiv.org/pdf/1511.06464v1.pdf 

To do so, I was planning on making a new class in RNN_cell.py. Therefore, I believe with need complex to complex along with 2d support. 

The reason why I wanted a pythonic tensorflow op is so that we can assign multiple gpu's to multiple unitary RNN's when the whole integration is complete. So one uRNN would be on gpu 1, and another uRNN would be on gpu 2. 

I don't know how hard it is for you to implement these fft's and ifft's but I think 3d support would be nice for future users who may try unitary conv nets. I certainly don't want to ask too much of you though!

If this is of any help to you, the goal is to replicate "Complex_RNN" here: https://github.com/amarshah/complex_RNN/blob/master/models.py#L532
 Hey @zffchen78 , I just wanted to follow up and ask if this was implemented yet? I can't make any progress on the unitary RNN's without it. Don't mean to bring on any pressure! Just wanted to ask. 
 I implemented 2D fft in TF code base already. Hopefully, it'll be copied in OSS soon.
 Okay thanks for the headsup. Looking forward to it!
 @LeavesBreathe, we pushed changes today which enables fft2d and ifft2d on gpu. You can take a look of python/kernel_tests/fft_ops_test.py to see if it works for you. We are still figuring out license issues w/ fftw where cpu support for fft2d/ifft2d needs. Hopefully, gpu supports are sufficient for you now. Let me know if you see any problems. 
 Awesome, gpu support is really all i needed so let me test it out and get back to you. I am going to be pretty busy over the holidays but come January 4 I should be back to testing this!
 @LeavesBreathe, I took a brief of the paper you plan to replicate in TF. I suspect you'll encounter some nuances. E.g., some operations may not have support for complex64 as time being. They are minor problems because they can be easily patched by updating a few .cc file to expand the types they support. 
 OKay this is very good to know @zffchen78. I will be sure to look into these formats. 
 We welcome the community to contribute the code.

If one wants to make the fft op supports more complete in tensorflow, feel
free to send us changes. It shouldn't be too hard to copy the fft2d impl,
and add fft1d, fft3d, and batched_fft1d, batched_fft2d, batched_3d, etc. as
needs arises, plus testing, etc. These ops' impl will pretty much look like
  stream->CreatePlan(...);
  stream->DoFft(...);

On Fri, Jan 15, 2016 at 9:10 AM Raingo notifications@github.com wrote:

> I think @LeavesBreathe https://github.com/LeavesBreathe answer to
> @zffchen78 https://github.com/zffchen78 question was wrong.
> 
> What uRNN needs is batched 1d fft. Both the paper and their theano
> implementation indicate this.
> 
> Paper wise, the Equation (6) is certainly multiplied with a complex vector
> for each data instance.
> 
> Implementation wise, check the following call stack,
> 1. https://github.com/amarshah/complex_RNN/blob/master/models.py#L14
> 2. https://github.com/amarshah/complex_RNN/blob/master/fftconv.py#L147
> 
> To TF core developer: why not simply implement an interface like
> scikits.cuda:
> https://github.com/lebedov/scikit-cuda/blob/master/skcuda/fft.py
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/386#issuecomment-172018139
> .
 We can't pull in FFTW because of its license implies complicated legal issues.
 If your usage is not performance critical yet (if you do, probably you better get a gpu), you can always fall back to numpy for prototyping your model. E.g., here is how you can use numpy fft modules to in tf:

```
# a bit excotic function (rfft)
with self.test_session():
  x = tf.constant([1., 2., 3., 4.], tf.float32)
  def rfft(x):
    return np.fft.rfft(x).astype(np.complex64)
  y, = tf.py_func(rfft, [x], [tf.complex64])
  self.assertAllClose(y.eval(), np.fft.rfft([1., 2., 3., 4.]))
```
 Ryan's right. Though a tiny correction (might not matter due to auto placement),  the code misplaced the with tf.device('/gpu:0') I think. It should be 

```
  # Construct a computation graph and place a constant e and fft(e) on /gpu:0
with g.as_default(), tf.device('/gpu:0'):
  # Here we start to add nodes into 'g'
  e = tf.constant(x, tf.complex64)
  fft = tf.batch_fft2d(e)
  # Here we finish adding nodes into 'g'. 
  # Typically, a tf proram is easier to read and is more
  # robust to keep the graph g intact after this point.

  with tf.Session() as sess:
    for _ in range(100):
      sess.run(barrier)

```

tf.device('/gpu:0') has no bearing on how sess.run exectuting stuff. It affects how the computation graph is placed.
  @maxcuda: thanks for doing that work!

Some of those changes (the ifdef ones) might be worth integrating into TensorFlow -- feel free to send us a change via Gerrit.  Not sure what to do about the nvcc failures though -- hopefully a newer version of nvcc might help.
  You haven't defined the graph yet: "It requires a session in which the graph was launched."

If you create your graph with all of the variables / ops that you want to restore, and then create the Saver, it should work.
 (Feel free to reopen if creating/launching the graph doesn't solve the problem for you).
  Try building with --verbose_failures so we can more easily help
 Sorry for dropping this -- I can't really see why gcc is failing to execute the command.

@davidzchen, any ideas what else we should ask our users to pass to bazel to help debug these kind of failures?
 Closing out stale-bug queue -- post more information if you're still having problems and we'll re-open
  Try cd to the root tensorflow directory and then do `bazel build tensorflow/examples/label_image/...` ?
 We need to see your exact command lines and the directory you're trying to run from, etc. 
 try `bazel build tensorflow/examples/label_image/...`.  you have an extra 'tensorflow' in your version.
 It looks like your BUILD file is corrupted -- those characters are not in the BUILD file that I can see.

try `cat tensorflow/examples/label_image/BUILD` to make sure it matches https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/BUILD
 You need to use git to clone the project, and then the files will be available as expected.  The following link might be helpful: https://www.udacity.com/course/how-to-use-git-and-github--ud775 -- Let us know if we can be of any further help.
  We're aware of the issues surrounding having multiple build systems. We'd definitely prefer if bazel would just work for all platforms (in particular, #16, #17, #110), and we're pretty confident it will work for most people in the near future.

However, it's clear that for some platforms (especially embedded), that's a while off, and cmake seems like a good solution, especially if we can find a (semi-)automatic solution to generate the cmake. It would also solve the immediate packaging issues mentioned in #720.
  Looks like we're getting hit by: http://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse

It's doing bool("False") --> True.

I'll try to get it to support:

--tea  --> tea is True
--tea=True -> tea is True
--tea=False -> tea is False
--notea --> tea is False
  Thanks for the fix!  We'll try to get that reviewed and integrated soon.
 @vrv: Are you on this, or should I grab it?
 Sorry, was supposed to assign this to @zffchen78 
 ZF: can you review https://tensorflow-review.googlesource.com/#/c/1154/ ?
 @vrv, @zffchen78: What happened to this? 
 It was submitted to gerrit, but it broke the build so we had to roll it back.  So I guess this is back to 'contributions welcome' :(
 For whoever looks at this next: I fixed `tf.test.compute_gradient_error` a while ago to handle `complex64` input, so it should be easy to test this change.
  The current Windows solution is to run TensorFlow in a Docker container. This StackOverflow thread has discussion of how to do this: http://stackoverflow.com/questions/33616094/tensorflow-is-it-or-will-it-sometime-soon-be-compatible-with-a-windows-work . If you have questions about how to get this running, S

Otherwise, adding more Windows support is an open issue (#17), so I'll close this for now.
  See also this issue, incl. fix: https://github.com/tensorflow/tensorflow/issues/121
 Let us know if @markusdr's fix doesn't work -- otherwise feel free to reopen #121 
  Thanks for the report -- we're trying to figure out a better solution to custom ops than the user_ops directory, and in doing so should hopefully address these cuda header issues.
 I added [instructions](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) for building ops and kernels outside of TensorFlow source tree. Please have a look to see if it fits your need.
 (Reopen if this new feature isn't working as expected)
  Have you installed tensorflow?  Does `import tensorflow` work in a Python shell?
 Closing due to lack of activity.  Please reopen if you're still having issues.
  It's intentional.  The C++ SparseTensor code has a [IndicesValid](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/util/sparse/sparse_tensor.h#L68) method for checking validity of a SparseTensor; and this check would fail on repeated indices.  However most of the ops that use STs don't run it for efficiency.

Please note our current contributions flow is not through GitHub; see [contributing](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md).
 So @ebrevdo, the answer is to document this difference in behavior?
 @andyljones: we now accept PRs, so if you want to fix the docs, please send it!
  Thanks, we'll fix the typo.

The format of tfrecords is not a human-readable string: most likely the depth value is encoded as a field in a _binary_ serialized protocol buffer.

How are you trying to consume the output of tfrecords?  
 If you don't provide us enough information to help you (or why you are disappointed) we can't really help you.  Please feel free to re-open the bug when you have more information that we can use to help you with.
  You're right, we should have an op like choice. Currently this is not easy to do, even though a similar functionality exists in candidate sampling ops (http://www.tensorflow.org/api_docs/python/nn.html#candidate-samplers). We are working on re-implementing candidate sampling in a cleaner way, and we'll take a look at adding a choice-like op when doing this.
 @lukaszkaiser, @gouwsmeister: Stephan said he doesn't have active plans to work on `tf.choice`, so we're happy to accept PRs if anyone else wants to add it.  
 @lukaszkaiser Does `tf.multinomial` obviate `tf.choice`?
 I think it does, thanks! With the example above, I think we can close this issue. Great thanks nikitakit!
  control_flow_ops.While is not part of the public API (hence there is no documentation about it) because it is still not ready to use externally yet.
 @yaroslavvb: Were you pondering linear algebra gradients?
 Just curious, what is the use-case for differentiable Cholesky?
 The best way would be to make a pull request. That's the best way for us to see and comment on the changes. It's ok to mark the PR as [WIP]. (see examples on our PR page).
 Hi Alex,

Sorry for the delayed response. I'd be happy to review your code and help
getting it integrated into the TensorFlow core.

Rasmus

On Wed, Mar 2, 2016 at 5:45 AM, Alexander G. de G. Matthews <
notifications@github.com> wrote:

> @rmlarsen https://github.com/rmlarsen Would you like to help?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/367#issuecomment-191243120
> .
 FYI: Alex's excellent code in PR 1465 was merged today, so you should be able to use cholesky with gradients now. We still need to add support for batch_cholesky, so I'm leaving the bug open for now.
  Fix should be up shortly!
 ... and we're fixed.
  - @craigcitro 
 @martinwicke probably knows more about the details here ...
 We disabled the more involved navigation features on mobile to expedite moving to the new website.
 You are seeing the new website, which we launched without the hamburger menu in order to launch it quicker. :) It'll come back, eventually.
 @martinwicke: Do we have hamburgers again?
 No. All we have is this issue and @danmane.
On Tue, Mar 8, 2016 at 13:41 Geoffrey Irving notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke: Do we have hamburgers
> again?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/365#issuecomment-193979968
> .
 Will likely happen once we move doc systems again.
  Yep, we want this internally too.
 This is done, and should be out in git soon.  `tf.random_uniform` now works for `int32` and `int64`.  Like the floating point case, it takes a half open range as `[minval, maxval)`.  In the integer case, `maxval` must be specified (it does not default to 1, since random zeros are not particularly useful).
 @michaelguia: That is, I don't know a good default range for the integer case (at least on the top), so I made it required.
  Checking for integrity of all the files would be tedious. We could provide an option to always re-download, which might make DOSing the source web site a little too easy if people get used to enabling it.
The simple 'rm -rf /tmp/cifar10_data' is a natural way to deal with this, so I'd be inclined not to do anything here. 
 I'm not sure exactly what the problem is:

https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/models/image/cifar10/cifar10.py#L484 

Can you instrument that code and let us know if something is not behaving as expected?
  Thanks for pointing this out! We have a fix in the pipeline, and it should appear on the website soon.
 Thanks for the prompt reply!
 Again I found very minor wrong number in the tutorial: http://www.tensorflow.org/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners
It mentions the shape of training images are [60000, 784], but actually [55000, 784]; also for the labels: [55000, 10] instead of [60000, 10].
  Sadly, nothing we can do.  https://code.google.com/p/gitiles/issues/detail?id=7 seems relevant.
  Sure, we'll take that into consideration.

Most computational operations, when expressed in our python API, return a tf.Tensor object, and passing a Tensor object to session.run does indeed fetch the result back -- you can just run the operation by passing set3_op.op instead.
 I don't think it's practical to have a warning like this, unfortunately.  Calling `session.run()` on large tensors seems like pretty standard behavior for machine learning.
 Fair enough, I was a bit misled by the title change.  Reopening.
 Incidentally, it would be better to fix the performance issue entirely.  There are a couple possible ways to do that, though I'm not sure if any are practical:
1. Automatically replace `scatter` with `scatter.op` in `session.run`.
2. Same as (1), but on the C++ side somehow.
3. Return views into the tensor rather than the whole thing.

@michaelisard: Do any of these seem sane / practical? 
  Sorry, we'll look at it soon -- we are still working on our tooling to accept external contributions, so give us a little bit more time.
 First successfully merged contribution I believe. Congrats! :)
  `StringPiece` is faster than `string` in some cases, so we don't want to change the signature in that way.  Can you add the full error message?  Also, what happens if you change `::tensorflow::StringPiece` to just `tensorflow::StringPiece` (i.e., remove the `::`)?
 Thanks, I'll fix this in the code.
  @danmane, @martinwicke: I assume this is just a css style change?
 Not quite but almost. Dan is on it.
On Mon, Nov 30, 2015 at 22:13 Vijay Vasudevan notifications@github.com
wrote:

> @danmane https://github.com/danmane, @martinwicke
> https://github.com/martinwicke: I assume this is just a css style
> change?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/354#issuecomment-160866764
> .
 Done.
  I bet these numbers (both ours and yours) are probably out dated -- re-open if there's still a problem with newer releases.
  Hey TF,

I have been using your translate model from your seq2seq tutorial and everything seems to work great. 

However, I have encountered a substantial problem. On my 980 TI with 6gb of memory:
- 5 GRU layers, 512 cells, batch size 32 -- _works_
- 1 GRU layer, 1024 cells, batch size 2 -- _barely works_ (tried batch size of 2, 4, 8, 16)

In Keras, I could run 2 GRU layers each with 2048 cells on my the 6gb memory. So the question I have is: how is this possible? What is taking up so much memory when you increase the cell size? 

I have a second 980 TI as well. I was really hoping I could put one layer 2048 cells on each card. Thanks again! 
 When it runs out of memory, it should print information about the memory usage  -- can you include that?
 Sure! Here is the pool allocation when it _does_ work -- GRU with 1024 cells at batch size 2:

```
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 7923 get requests, put_count=3499 evicted_count=1000 eviction_rate=0.285796 and unsatisfied allocation rate=0.697211
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4623 get requests, put_count=2605 evicted_count=1000 eviction_rate=0.383877 and unsatisfied allocation rate=0.656284
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 176 to 193
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6 get requests, put_count=2029 evicted_count=2000 eviction_rate=0.985707 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1034 evicted_count=1000 eviction_rate=0.967118 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1044 evicted_count=1000 eviction_rate=0.957854 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=1054 evicted_count=1000 eviction_rate=0.948767 and unsatisfied allocation rate=-nan
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=1087 evicted_count=1000 eviction_rate=0.919963 and unsatisfied allocation rate=-nan
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 7337 get requests, put_count=6254 evicted_count=2000 eviction_rate=0.319795 and unsatisfied allocation rate=0.437509
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 1400 to 1540
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 12607 get requests, put_count=10862 evicted_count=1000 eviction_rate=0.0920641 and unsatisfied allocation rate=0.235583
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 2478 to 2725
```

Here is 2 GRU layers of 1024 batch size of 2. When it runs out of memory, the error message keeps repeating over and over again. I've included the print out right as it is about to compute. You can see that it starts to allocate and pool data but then it simply runs out:

```
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 256 (256B) Pool: chunks: 2048 free: 247 cumulative malloc: 10736 cumulative freed: 8935
Number of chunks: 2048, in_use chunks: 1801
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 512 (512B) Pool: chunks: 64 free: 64 cumulative malloc: 360 cumulative freed: 360
Number of chunks: 64, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1024 (1.0KiB) Pool: chunks: 64 free: 62 cumulative malloc: 352 cumulative freed: 350
Number of chunks: 64, in_use chunks: 2
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2048 (2.0KiB) Pool: chunks: 128 free: 128 cumulative malloc: 414 cumulative freed: 414
Number of chunks: 128, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4096 (4.0KiB) Pool: chunks: 256 free: 2 cumulative malloc: 963 cumulative freed: 709
Number of chunks: 256, in_use chunks: 254
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8192 (8.0KiB) Pool: chunks: 128 free: 63 cumulative malloc: 1052 cumulative freed: 987
Number of chunks: 128, in_use chunks: 65
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 16384 (16.0KiB) Pool: chunks: 1344 free: 709 cumulative malloc: 14046 cumulative freed: 13411
Number of chunks: 1344, in_use chunks: 635
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 32768 (32.0KiB) Pool: chunks: 384 free: 247 cumulative malloc: 2187 cumulative freed: 2050
Number of chunks: 384, in_use chunks: 137
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 65536 (64.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 9 cumulative freed: 9
Number of chunks: 2, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 131072 (128.0KiB) Pool: chunks: 2 free: 1 cumulative malloc: 2 cumulative freed: 1
Number of chunks: 2, in_use chunks: 1
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 229376 (224.0KiB) Pool: chunks: 44 free: 44 cumulative malloc: 242 cumulative freed: 242
Number of chunks: 44, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 327680 (320.0KiB) Pool: chunks: 4 free: 3 cumulative malloc: 4 cumulative freed: 3
Number of chunks: 4, in_use chunks: 1
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 425984 (416.0KiB) Pool: chunks: 68 free: 68 cumulative malloc: 796 cumulative freed: 796
Number of chunks: 68, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 524288 (512.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 6 cumulative freed: 6
Number of chunks: 2, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1048576 (1.00MiB) Pool: chunks: 190 free: 69 cumulative malloc: 768 cumulative freed: 647
Number of chunks: 190, in_use chunks: 121
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2228224 (2.12MiB) Pool: chunks: 119 free: 119 cumulative malloc: 276 cumulative freed: 276
Number of chunks: 119, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4456448 (4.25MiB) Pool: chunks: 62 free: 0 cumulative malloc: 164 cumulative freed: 102
Number of chunks: 62, in_use chunks: 61
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8912896 (8.50MiB) Pool: chunks: 123 free: 0 cumulative malloc: 529 cumulative freed: 406
Number of chunks: 123, in_use chunks: 122
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 17825792 (17.00MiB) Pool: chunks: 61 free: 0 cumulative malloc: 226 cumulative freed: 165
Number of chunks: 61, in_use chunks: 60
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 41943040 (40.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1
Number of chunks: 1, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 67108864 (64.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 2 cumulative freed: 2
Number of chunks: 1, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 134217728 (128.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1
Number of chunks: 1, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 335544320 (320.00MiB) Pool: chunks: 7 free: 3 cumulative malloc: 22 cumulative freed: 18
Number of chunks: 7, in_use chunks: 4
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:345] Aggregate Region Memory: 5596086272 (5.21GiB)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:347] Aggregate Chunk Memory: 5595824128 (5.21GiB)
W tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:89] Out of GPU memory, see memory state dump above
W tensorflow/core/kernels/matmul_op.cc:143] Resource exhausted: OOM when allocating tensor with shapedim { size: 2048 } dim { size: 1024 }
W tensorflow/core/common_runtime/executor.cc:1027] 0x58dae900 Compute status: Resource exhausted: OOM when allocating tensor with shapedim { size: 2048 } dim { size: 1024 }
     [[Node: gradients_3/model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device="/job:localhost/replica:0/task:0/gpu:0"](model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/Linear/concat, gradients_3/model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/add_grad/Reshape)]]
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 256 (256B) Pool: chunks: 2048 free: 247 cumulative malloc: 10736 cumulative freed: 8935
Number of chunks: 2048, in_use chunks: 1801
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 512 (512B) Pool: chunks: 64 free: 64 cumulative malloc: 360 cumulative freed: 360
Number of chunks: 64, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1024 (1.0KiB) Pool: chunks: 64 free: 62 cumulative malloc: 352 cumulative freed: 350
Number of chunks: 64, in_use chunks: 2
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2048 (2.0KiB) Pool: chunks: 128 free: 128 cumulative malloc: 414 cumulative freed: 414
Number of chunks: 128, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4096 (4.0KiB) Pool: chunks: 256 free: 2 cumulative malloc: 963 cumulative freed: 709
Number of chunks: 256, in_use chunks: 254
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8192 (8.0KiB) Pool: chunks: 128 free: 63 cumulative malloc: 1052 cumulative freed: 987
Number of chunks: 128, in_use chunks: 65
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 16384 (16.0KiB) Pool: chunks: 1344 free: 710 cumulative malloc: 14046 cumulative freed: 13412
Number of chunks: 1344, in_use chunks: 634
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 32768 (32.0KiB) Pool: chunks: 384 free: 248 cumulative malloc: 2187 cumulative freed: 2051
Number of chunks: 384, in_use chunks: 136
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 65536 (64.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 9 cumulative freed: 9
Number of chunks: 2, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 131072 (128.0KiB) Pool: chunks: 2 free: 1 cumulative malloc: 2 cumulative freed: 1
Number of chunks: 2, in_use chunks: 1
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 229376 (224.0KiB) Pool: chunks: 44 free: 44 cumulative malloc: 242 cumulative freed: 242
Number of chunks: 44, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 327680 (320.0KiB) Pool: chunks: 4 free: 3 cumulative malloc: 4 cumulative freed: 3
Number of chunks: 4, in_use chunks: 1
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 425984 (416.0KiB) Pool: chunks: 68 free: 68 cumulative malloc: 796 cumulative freed: 796
Number of chunks: 68, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 524288 (512.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 6 cumulative freed: 6
Number of chunks: 2, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1048576 (1.00MiB) Pool: chunks: 190 free: 69 cumulative malloc: 768 cumulative freed: 647
Number of chunks: 190, in_use chunks: 121
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2228224 (2.12MiB) Pool: chunks: 119 free: 119 cumulative malloc: 276 cumulative freed: 276
Number of chunks: 119, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4456448 (4.25MiB) Pool: chunks: 62 free: 0 cumulative malloc: 164 cumulative freed: 102
Number of chunks: 62, in_use chunks: 61
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8912896 (8.50MiB) Pool: chunks: 123 free: 0 cumulative malloc: 529 cumulative freed: 406
Number of chunks: 123, in_use chunks: 122
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 17825792 (17.00MiB) Pool: chunks: 61 free: 0 cumulative malloc: 226 cumulative freed: 165
Number of chunks: 61, in_use chunks: 60
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 41943040 (40.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1
Number of chunks: 1, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 67108864 (64.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 2 cumulative freed: 2
Number of chunks: 1, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 134217728 (128.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1
Number of chunks: 1, in_use chunks: 0
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 335544320 (320.00MiB) Pool: chunks: 7 free: 3 cumulative malloc: 22 cumulative freed: 18
Number of chunks: 7, in_use chunks: 4
```

It will keep repeating this over and over again.

Has anyone had similar problems? It just doesn't seem right that a 1024 GRU should barely fit on 5gb of memory. I am purely doing text to text (50 words input and 50 words output), so the input data shouldn't be that heavy. Any help would be **immensely** appreciated! 
 Ah, this is likely a result of fragmentation caused by our initial / older memory allocator.

If you build from source (at HEAD), you will by default use a better memory allocator that is less likely to run into these problems.  It will be in our next binary release, if you'd rather wait until then.
 Hey @vrv thanks for your help. I just built it from source and it did help some. Now, I can run a 1536 GRU layer at a batch size of 8 -- which is definitely an improvement! However, I can't run the desired 2048 unless I decrease the batch size to 4. 

I'll close this issue as things are certainly better. Hopefully, the gpu allocator can be further improved upon? It seems that multiple layers is great (I can use 6 GRU layers with 512 cells/layer with a batch size of 16). It just seems that these large layer sizes really sucks up memory compared to Theano. 
 Cool, glad it helped at least a little.

@rafaljozefowicz may have some other ideas too.   Memory improvements will certainly come as the project matures :)
 @rafaljozefowicz I apologize for the late response. Thank you for letting me know about this! 

Unfortunately, I'm away from my computer because of the holidays, but on Sunday/Monday I will try this, and post back here with results. Again, I really do appreciate the help.

If it slows it down, that's okay. The main thing I'm hoping for is a GRU of 2048 cells with a batch size of hopefully 16. 
 @rafaljozefowicz thank you for the help. Using `aggregation_method = 2` got me to a gru of 1536 with a batch size of 16 (which is double the size from what I had previously!). So this is a definite upgrade. 

I'm starting wonder if the softmax is sucking up alot of gpu memory. @vrv is there a way to see how much op is taking GPU memory? This would allow us to diagnose where GPU memory shortages are stemming from. 
 @LeavesBreathe: not yet, but we're working on providing instrumentation so that it can be made available to end-users for debugging.
 That would be such a great upgrade. I'll close this thread, but I tested similar architecture in Theano

Theano Max with 12gb: two GRU's with 2048 cells with a batch size of 256

TensorFlow Max with 12gb: Two GRU's with 1536 cells with a batch size of 16
  Have you seen the updated instructions from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md, and if so, do they still not help?
 Can you run it in gdb and paste a backtrace?
On Nov 25, 2015 4:25 PM, "Sean" notifications@github.com wrote:

> yeah, actually I just cloned it today.
> the only step different is wget, I downloaded it from browser and move it
> to the data folder, then I ran the unzip command below successfully.
> $ unzip tensorflow/examples/label_image/data/inception5h.zip -d
> tensorflow/examples/label_image/data/
> 
> and two bazel command were ran, but still go wrong.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/351#issuecomment-159764884
> .
 After going through the new download process, it looks like the name of the file has changed from the expected one. I'll update the command-line flag default to point to the correct path here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L66
When I run the following command passing the correct path with --graph:
`bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb`
I see the same SEGV fault, so it looks like there's something different between the Android graph file which we're now using here, and what the code here is expecting. I'll keep digging.
 It turns out there were two problems. I needed to update the default flag to point to the new labels file location, and I hadn't added error handling for the case when the labels file wasn't found, so it just crashed mysteriously when it tried to print the labels. We'll get that updated in our next push, but for now the workaround is to specify the correct labels file location on the command line. Here's one that I've tested:
`bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb --labels=tensorflow/examples/label_image/data/imagenet_comp_graph_label_strings.txt`

Does that help?
 The latest version of the imagenet graphdef changed the names of the input and output nodes.

1) Fetch the latest source from git and it should work: we updated the source to use the new names.

2) Add the following flags to your current build.

 --input_layer=Mul
--output_layer=softmax
  What do you get from the output of `uname -a` ?

(We only support 64-bit platforms at the moment).
 Do you know what version of glibc you have installed on your machine?  (I think `ldd --version` might tell you).

I think we require something later than 2.14 or 2.17...
 Hmm :(.  All the info online suggests some kind of glibc version skew.  You might want to try the instructions to install from source in the meantime.
 Is this still an issue with the new release?
 Closing due to likely GLIBC issues.  
  Correct. Thanks!
  I'm getting a compile error when trying to compile the `example_trainer` using gcc 4.8.1:

```
$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer  --verbose_failures 
____From Compiling tensorflow/core/kernels/conv_grad_ops.cc:
tensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]
::__lambda3':
tensorflow/core/kernels/conv_grad_ops.cc:621:22:   required from 'struct tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; 
T = float]::__lambda3'
tensorflow/core/kernels/conv_grad_ops.cc:632:7:   required from 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T =
 float]'
tensorflow/core/kernels/conv_grad_ops.cc:1265:1:   required from here
tensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__col_buffer_data' before deduction of 'auto'
                     &size_A](int64 start, int64 limit) {
                                                        ^
tensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__col_buffer_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__input_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__input_data' before deduction of 'auto'
tensorflow/core/kernels/conv_grad_ops.cc:623:46: error: use of 'input_data' before deduction of 'auto'
           auto input_data_shard = input_data + shard_id * input_offset;
                                              ^
tensorflow/core/kernels/conv_grad_ops.cc:623:46: error: invalid use of 'auto'
```
 It seems the compile error was introduced recently. It goes away when I downgrade to an older version; this version was still OK:

```
commit 011e9baccd343eb943d25014c4e8aec53eac396b
Author: Vijay Vasudevan <vrv@google.com>
Date:   Thu Nov 12 18:34:45 2015 -0800
```
 Thanks, we're looking at it now...
 This was fixed some time last week.
  Did you use the binary install?  I think we fixed a bug in dropout shape inference, so it is fixed in the source. 

I'll de-dupe with https://github.com/tensorflow/tensorflow/issues/184, since it is likely related.
  http://www.tensorflow.org/get_started/os_setup.html#pip-installation-issues ?  Let us know if those instructions don't help
  We have SparseTensor and embedding lookup ops that with with SparseTensor.
Can you clarify your exact use case?
On Nov 25, 2015 5:15 PM, "chenghuige" notifications@github.com wrote:

> It seems tesnsorflow right now does not support general sparse matrix yet.
> Will probably have to convert each mini-batch data into dense matrix but
> that will make matrix vector operation much slower..
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/342#issuecomment-159772556
> .
 For this type of calculation, we have [embedding_lookup](http://www.tensorflow.org/api_docs/python/nn.html#embedding_lookup).  See the example in [word2vec_basic](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py#L145).  Your "X" would be the output of embedding_lookup.
 In fact, for X*w, see [embedding_lookup_sparse](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L108) which does much of the hard work for you.
 @chenghuige Since you have 400k features, in practice you must somewhere store a 400k x feature_depth matrix, no?  This is what you pass to embedding_lookup_sparse and it performs the sparse multiplication for you.
 There are a few things in that example that I can't see, for example sparse2dense is not defined.

Also, this is a good question for stackoverflow; github issues is more focused towards reporting bugs.
 Suppose you have a minibatch of 2 entries.  The first entry has sparse ids [53, 87, 101], values [0.1, 0.2, 0.3] and the second has sparse ids [34, 98], weights [-1.0, 3.5].  Suppose your total vocab size is 500.  Suppose also that the hidden layer has depth 25 (25 units).

then:

``` python
X = tf.Variable(tf.truncated_normal([500, 25], stddev=1/500.0))
sp_indices = tf.placeholder(tf.int64)
sp_shape = tf.placeholder(tf.int64)
sp_ids_val = tf.placeholder(tf.int64)
sp_weights_val = tf.placeholder(tf.float32)
sp_ids = tf.SparseTensor(sp_indices, sp_ids_val, sp_shape)
sp_weights = tf.SparseTensor(sp_indices, sp_weights_val, sp_shape)
y = tf.nn.embedding_lookup_sparse(X, sp_ids, sp_weights, "sum")
tf.initialize_all_variables().run()  # initialize values in X

y_values = tf.run(y, feed_dict={
  sp_indices: [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # 3 entries in minibatch entry 0, 2 entries in entry 1.
  sp_shape: [2, 3],  # batch size: 2, max index: 2 (so index count == 3)
  sp_ids_val: [53, 87, 101, 34, 98],
  sp_weights_val: [0.1, 0.2, 0.3, -1.0, 3.5]})
```

y_values should be the output of X*[w1, w2], where w1 and w2 are the two minibatch entries.
  Hey TF, love your code.

I recently forked and cloned TF because I wanted to add some new features to it. 

However, previously I pip installed it, so my question is:

How do I change it so that when I call in `import TensorFlow`, it comes from my specified directory (cloned dir) and not the one in `dist-packages`? 

Do I need to re-install TensorFlow? Or can I just simply use the scripts found in my TensorFlow clone?

Thanks!
 I would probably pip uninstall tensorflow's binary package if you're building from source, just to avoid any problems.  the binary install is for those who just want to use the library as is without modification.
 You can use this pip install option if you want to modify the python code
live. Any changes to the c++ build or dependencies requires a recompile.
On Nov 24, 2015 5:33 PM, "Mark Daoust" notifications@github.com wrote:

> How do I change it so that when I call import TensorFlow, it comes from my
> specified directory (cloned dir) and not the one in dist-packages?
> 
> I think that's the more important part of the question.
> 
> Often you can just pip install -e ... but I don't see any way to install
> tensorflow in an "editable" mode so you can easily try out changes, without
> rebuilding/reinstalling the wheel.
> 
> Am I missing something? Is there a simple way?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/341#issuecomment-159458024
> .
 Thank you for suggesting to pip uninstall it. I can do that, but I thought it wouldn't be favorable to do so. I'll give a try and see if it works! Thanks!
  Sorry about that -- we already have a change internally that addresses this though.  Will be pushed out soon.
  Closing due to lack of activity.  Can you reopen if it doesn't seem to be a memory issue?
  None yet. gzipping will unlikely result in significant compression due to the high entropy of the weights, but we're very interested in enabling compressed model representations at rest in general, especially for models that have to fit on mobile devices. I'll leave this bug open to track the feature request.
 I completely misunderstood your point, for some reason I assumed you meant checkpoints. FR stands, I agree that it might make a lot of sense to support something like that.
  @vrv 
 Tested that and it didn't work for me on OS X, possibly because Tensorboard changes the cwd?
 Hm, interesting.  Could replace os.getcwd() with os.path.realpath('.').  (The main point was that passing that field through all of the functions isn't really necessary, since it's not going to be configured by external callers.
 Yup definitely, when I originally made this PR it was necessary because the current path was [changed in the launch function](https://github.com/tensorflow/tensorflow/blob/f41959ccb2d9d4c722fe8fc3351401d53bcf4900/tensorflow/tensorboard/tensorboard.py#L103).  Looks like we can remove that extra complexity and do what you suggest now that that bit was removed.  I'll commit it in a bit.
 Committed your suggestion and tested, looks good! Tested with both absolute and relative.

<img width="730" alt="screen shot 2015-12-23 at 2 11 09 pm" src="https://cloud.githubusercontent.com/assets/511499/11983134/1e2a9292-a97f-11e5-8d02-4c77b55d9688.PNG">

<img width="1752" alt="screen shot 2015-12-23 at 2 11 33 pm" src="https://cloud.githubusercontent.com/assets/511499/11983135/1e2b1b4a-a97f-11e5-9e7a-02c6ce3c0ea5.PNG">
 Thanks!  If you can squash the commits, I'll merge right after.
  This seems more like a bazel-related question -- can please you file an issue at their github repo?
 This question is probably better asked over at bazel, they're more likely
to be able to help with this.

On Wed, Feb 10, 2016 at 1:13 AM Peter Braden notifications@github.com
wrote:

> Is there any way to do this without editing build files?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/336#issuecomment-182267676
> .
  @danmane: Thoughts?
 From an API perspective, how do you imagine this working? The ImageSummary op would take in an image input and also (optionally?) a text label input? I've also heard requests for bounding boxes on the images. (What happens if you want to make multiple predictions on a single image?)

The image viewer is definitely one of the least-developed parts of TensorBoard at the moment. I want to improve it in the future, but it's not very high on my queue at the moment as I'm prioritizing features that are domain independent and thus useful to every user of TensorBoard (general usability, hyperparameter search, etc). If you want this to get fixed sooner, the best way is to come up with a plan for what the API should look like, get buy-in, and then submit a pull request - I'd be happy to help coordinate the process :)
  De-duping with https://github.com/tensorflow/tensorflow/issues/4 -- also we have this as a common problem listed on our install website: http://www.tensorflow.org/get_started/os_setup.html#common_install_problems
  The error indicates that the optimizer could not find any gradients for the variables to optimize.

This can be caused by several problems:
 1- The optimizer could not find any variable to optimize.
 2- The loss does not depend on any variable.

For 1-: Optimizers look  for trainable variables in the default graph. They use `tf.trainable_variables()` as the list of variables to optimize.  So try calling that function and print its output before calling `.minimize()`.  If you get an empty list then the optimizer won't find variables either.

Why can't it find variables?
-  The variables are looked for in the _default graph,_ which is a thread-local  variable.  You mention "outside the main thread" which hints that your program is using multiple python threads.  If this is the case you need to pass the  default graph from the main thread to the other thread and install it as the default graph.  See `tf.get_default_graph()` and `Graph.as_default()` in http://tensorflow.org/api_docs/python/framework.md#Graph .
-  Maybe you have not created any variables?  Or you only create variables with  `trainable=False`?

For 2-: The optimizer computes gradients of the loss with respect to variables.   If there are  no variables there are no gradients and nothing to optimize.  For example, if you pass a constant to `minimize()` nothing can be done.  Now, the fact that you say it works when called from the same file seems to rule out that possibility, but check anyway.

Finally: The error message clearly needs to be improved.
 Closing due to staleness, I think we committed a change a few weeks ago that fixed the error message to be better.  Let us know if that's not true!
  It sounds like you're using an older version of glibc than we support (2.2.5, rather than 2.17). You could try adding `"-lm"` to the returned value from `tf_copts()` in `tensorflow.bzl` (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L40)), but there might be other problems that arise afterwards.

I'd recommend trying to update the version of libc on your machine, or using a Docker container to experiment with a different operating system in a known-good configuration.
 @mrry: Should we close as won't fix due to the version issue?
 I think so. It sounds like a Bazel issue, not picking up the appropriate libraries from a non-standard location.
  Thanks -- we're working on this internally and we're pretty close to something that works for us. De-duping with https://github.com/tensorflow/tensorflow/issues/26
 FYI, we're now writing individual commits for each of our internal changes -- it's working reasonably well so far.
  Not any more.
  Cc @danmane.
 Yeah, we definitely want to make comparing different charts easier. No timeline for when we'll have this, though.
 As a temporary workaround, you can make the plot in matplotlib, and then inject it into a TensorFlow image_summary so that it will display an updated copy in TensorBoard.
  Closing.  For future reference, this kind of question is better on stackoverfow.
  See #187.  I think for now you can modify this line:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L702

to point to your python headers folder.  Please confirm this works for you.  We're working on a fix.
 @chenghuige we're working on providing something like tensorflow.version or tensorflow.**version**.

Looks like your other problem has been resolved, so I'm closing this issue.
  If you continue having trouble with sudo, consider using a virtualenv following instructions in the howto [here](http://tensorflow.org/get_started/os_setup.md#virtualenv-based_installation).
 @ad26kt: if you do, let us know.
  Fixed, the current version of the doc doesn't have these typos and has a full, runnable code example.
(And the code example is even tested.)
  Consider increasing the epsilon value of the optimizer.
 One thing you can do is run with a tiny learning rate, or even zero learning rate. If you still have divergence then, you have a bug in your setup. If not, increase your rate slowly and see if there is a regime in which things train without diverging. It's completely possible to have weights that are in a good range, but activations or gradients going to infinity because of the shape of the loss, or too high a learning rate. It's obviously always a possibility that there is a bug in the optimizers, but in my experience, every single instance of this kind of problem could be traced back to a weirdly wired model, learning rate issues, bad randomization of the input examples, or - in the case of Adam or RMSProp - issues with the epsilon value.
 @jpiabrantes One thing that stands out here is that you use a square root in your loss. That's known to be very unstable numerically for very small values. Can you try 1) not taking the square root or 2) adding a small constant, e.g.: tf.sqrt(1e-4 + ...).
If that's indeed the problem, then one possible approach we can provide tooling around is to optionally cap the gradient of the sqrt function, because it's a common use case. Another possibility is to use gradient clipping in general (see: clip_by_norm(), which we should provide better examples for).
Let me know what happens.

The other avenue is to tune your learning rate decay. Gradients tend to paradoxically grow in magnitude as training progresses, and sometimes that introduces numerical issues late in training. It is still possible that we have issues with numerical precision somewhere, but my hunch is that these errors are legitimate divergence issues (for some twisted definition of 'legitimate').
 @navraj28 diverging when you use SGD can happen for many reasons:
- your learning rate is too high. Note that the right learning rate can be data dependent.
- your input data is not well randomized, or highly redundant, which could happen if you're just feeding the same data with few distortions.
  This bug is about the AdamOptimizer. I am going to close it. Feel free to reopen if you have new evidence of a bug.
  I'll leave it open for tracking purposes, but if someone wants to tackle this it's probably better to start off as a separate repo.  I'm not sure it makes sense to integrate it into TensorBoard at first, but @danmane may have further thoughts.

If someone does give this a try, please comment here!
 Yeah, we've talked about this internally - it would be really cool to have a GUI for modifying the graph, but it seems like a lot of work and I suspect it would still not be powerful enough for most use cases. For it to be really useful it would need to be embedded in a broader gui-based execution system for TensorFlow, which also sounds very challenging to get right.

An easier path to fast-feedback graph creation would be to embed the graph visualizer into iPython notebook - you'd still be modifying the graph with code, but with a tighter feedback cycle.

Of course, if someone wants to try building the GUI graph builder, they're welcome to :) and I'd love to see what comes of it.
  https://github.com/tensorflow/tensorflow/commit/854f49bd43588c062b046384f239f64a3d819702 Fixed this a while ago, I believe.
  (let us know if those aren't sufficient)
  You can try the Ubuntu 14.04 deb, or use the tgz installer here:

https://developer.nvidia.com/cuda-toolkit-70

under the "Linux x86" tab.
 De-duping with https://github.com/tensorflow/tensorflow/issues/20
  Thanks for the feature request!  We're also experimenting with support for defining "functions" in the graph that would basically allow re-usable components to achieve the same effect.
 Looks like this fell through the cracks.  PRs have been enabled for a while.  @martinwicke: Does this seem appropriate for `contrib`? 
 @vrv, what's the status of graph functions? We probably should standardize to one way of doing things, although even in the presence of functions functionality like this seems valuable to extract things from an existing graph.
 @zffchen78 
 I think we accepted this to contrib, closing.
  You can use different optimizers for any subset of variables in your graph to get per-layer learning rates.

The 'minimize' function of an optimizer http://www.tensorflow.org/api_docs/python/train.html#optimizers allows you to specify which variables should be updated.  Let us know if this isn't what you need ...
 This should really be posted on StackOverflow -- you'll get more community help there :)
  The tooling has improved in 0.8: you can call `tf.all_variables()` to get a list of all of the variables in your model, and use the [`inspect_checkpoint.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py) tool to get the list of variables in a checkpoint file, so it should be easier to build the list of variables to restore.

I don't think we'd want to add a less-strict version of `restore()` that ignores missing values, although we'd consider contributions of a tool that integrates the `inspect_checkpoint.py` and `tf.all_variables()` code to make doing this easier.
 @futurely, how do you want to work around this restriction? Do you want to restore all you have, and initialize the rest to what's in the checkpoint? If that's the case, what you can do is this:
1. Initialize all the variables.
2. Get all the variables in the checkpoint by using get_variable_to_shape_map() and restore those only.

See https://github.com/tensorflow/tensorflow/blob/8df836f2611c533c54d2bdaaf107c67afac8d6a5/tensorflow/python/training/saver_test.py#L1214

for usage example.

@koonyook, how do you want to use a new variable with previously saved variables?
  This may be caused by incompatible versions of protobuf installed by python.  Are you using the python protobuf package version 3.0.0a3?
 This is the same version you've got installed in your python interpreter?
Are you using blaze commands to run the scripts, or are you running python
yourself?

On Fri, Nov 20, 2015 at 9:57 AM, Steven Kearnes notifications@github.com
wrote:

> @ebrevdo https://github.com/ebrevdo I'm using the version that comes
> with git clone --recurse submodules:
> 
> $ git submodule status
>  55ad57a235c009d0414aed1781072adda0c89137 google/protobuf (v3.0.0-alpha-4-179-g55ad57a)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/310#issuecomment-158474448
> .
 This looks like a bug either on the protobuf side or somewhere in the tensorflow/protobuf build interaction.  I was able to replicate; we're working on a solution.
 Just to confirm; are you running bazel test?  (looks like it)

It may be that the wrong version of protobuf is being used.  Is this in linux?

Try _also_ installing the protobuf package via pip and making sure you're using the right version in your system python.  Does that help the problem?
 Sounds like this may be a bug in the version of protobuf we check out. Any
chance you could file a bug there and cross link it here? You can also try
using the master HEAD version for our submodule and see the problem has
been fixed.
On Nov 30, 2015 3:45 PM, "Steven Kearnes" notifications@github.com wrote:

> I'm running bazel build, as described in OP. This is on Ubuntu 14.04. It
> seems that //google/protobuf/python/google/protobuf/pyext/... is not being
> built at all. Nothing in the BUILD references any of those files. I've been
> trying to hack the build rules, but that is quickly becoming a rabbit hole.
> 
> Following your suggestion, I have been able to install protobuf from the
> branch that comes with tensorflow into my python, but I'm still getting
> errors like ImportError: cannot import name _message when I try to use it
> with PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp.
> 
> I'll note that python setup.py test --cpp_implementation (from the
> protobuf python install) also gives an error: AttributeError: 'module'
> object has no attribute 'python_message'.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/310#issuecomment-160800476
> .
 Sent PR https://github.com/google/protobuf/pull/1029 to google/protobuf.
  @mbaddar1: Very cool!

@martinwicke: Do you think this should go in core or the upcoming model zoo?
 Everything that doesn't have a full tutorial associated with it should go
into the zoo (which we will announce shortly). If you want to write a
tutorial we'd be happy to merge it into core.
On Mon, Jan 25, 2016 at 11:17 Geoffrey Irving notifications@github.com
wrote:

> @mbaddar1 https://github.com/mbaddar1: Very cool!
> 
> @martinwicke https://github.com/martinwicke: Do you think this should
> go in core or the upcoming model zoo?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/309#issuecomment-174626520
> .
 Zoo: shortly. Nobody is working on a time series example as far as I know.
On Mon, Jan 25, 2016 at 17:22 andrewcz notifications@github.com wrote:

> and is there a link to zoo?
> Cheers,
> Andrew
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/309#issuecomment-174757369
> .
  Most likely a transient issue, related to https://github.com/tensorflow/tensorflow/issues/209
  @mxrguspxrt: so we can understand what specific improvements would help, what is your current understanding of that error message? 
  Hi,
This behavior should be fixed in with commit https://github.com/tensorflow/tensorflow/commit/854f49bd43588c062b046384f239f64a3d819702

An upcoming change will push all of the colorspace conversion into native code as well.
 color space conversion change in bf6b536bde7d8060c489b51fedb58968b8cbfd7c
 I see, thanks for the update. Does this occur at all preview resolutions?

On Fri, Dec 4, 2015 at 2:13 AM, barami98 notifications@github.com wrote:

> I found that Y channel is OK but U, V channels are all zero because of low
> android version.
> It was fixed in Android 5.1.1
> 
> https://code.google.com/p/android/issues/detail?id=81984
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/306#issuecomment-161895881
> .
  Agreed, scatter on GPUs would be useful.  Also gather.
 Progress here: https://github.com/tensorflow/tensorflow/pull/782
 There are unit tests that are failing on the GPU configuration due to this issue: 
//tensorflow/models/embedding:word2vec_test
//tensorflow/models/embedding:word2vec_optimized_test

When is the ScatterSub support on GPU coming? As fpmchu pointed out, there are a few other ops such as "NegTrain", that are not supported on GPU either yet. Is there also plan to get the GPU support for those ops covered? Thanks.
 I believe ScatterSub on GPU now is implemented thanks to @fpmchu 
  Should be fixed now, thanks!
  This is out of scope for the main tensorflow project.  Projects that generate TF graphs from other intermediate representations are certainly possible, but they would belong in separate repos.
  Thanks, we've noticed this and it should be in the next git sync. 
  Thanks @panmari!  Yes, this is a better question for stackoverflow. 
  Thanks for reporting - we had a brief outage, but the site's back up now!
  We have fixed the bug @cesarsalgado found, it should appear soon. 
 Should be fixed now.
 @cesarsalgado's issue is fixed. The accordion is still not done.
 @martinwicke, @danmane: Is this still an issue? 
 @cesarsalgado What browser are  you using and on what OS? This works for me.
 We're working on backend changes, this will eventually be resolved by that work. Hang tight.
  This is somewhat covered by the queue functionality.
  Hi there,

Are you referring to the API docs for Operation and Tensor? These are provided here:
- http://tensorflow.org/api_docs/python/framework.html#Operation
- http://tensorflow.org/api_docs/python/framework.html#Tensor

If there's anything that's not clear in here, or that you think would be worth adding, please let us know!
  This was done, sort of.  We removed the large model files and some animated gifs, then rewrote the history.  Unfortunately, despite some checks on our side, the animated gifs made their way back in a more recent commit. :(

The repo is a lot smaller without the models in the history, and we don't want to rewrite the history again.  We'll live with our 20-30MiB mistake.
  Do you have `from __future__ import division` at the top, and are you running against tensorflow installed via pip?  As part of #1 we added `__truediv__` support to tensorflow but made the mistake of using it in the tutorials.  It should work if you remove that import, and I'll fix it so that the HEAD tutorials are compatible with the 0.5.0 release.
 Oh, actually that isn't a tutorial.  I think we won't try to keep stuff in `models` at HEAD backwards compatible to older versions of tensorflow.  You can fix it locally by either removing that  import line or using the version of `ptb_word_lm.py` from the `0.5.0` tag.  In general, running part of tensorflow at `HEAD` against an older release is not guaranteed to work.
 Reopening until the diagnosis is confirmed.
 Closing for now, since I believe this is fixed.
 @darolt: This shouldn't be an issue now at 0.6.0 is out.  In general, there's no guarantee that git versions of tensorflow models will work against old releases.
  Yes, that tensor has rank 2, since it's a 2-dimensional array (list of lists).
  Thanks, special functions are well worth adding, and we'd welcome contributions in this direction.
 @ebrevdo is already half-way to implementing this, I think (I think he added it to Eigen for both CPU and GPU? not sure if it's been upstreamed yet).
 Yeah; this has been implemented and should make it into the next upstream release.  I've added lgamma to Eigen (both ours & upstream) and added an lgamma unary op.

Digamma should also make its way in, in the next 2 weeks.
 It'll get pushed to the public repo next week.
On Jan 3, 2016 8:49 AM, "David Moore" notifications@github.com wrote:

> @akuz https://github.com/akuz I've been using a hack where you just
> define a series expansion within the TF graph. It's not pretty and I don't
> guarantee accuracy, but if you really can't wait for proper support it
> might be worth a try:
> 
> def gammaln(x):
>     # fast approximate gammaln from paul mineiro
>     # http://www.machinedlearnings.com/2011/06/faster-lda.html
>     logterm = tf.log (x \* (1.0 + x) \* (2.0 + x))
>     xp3 = 3.0 + x
>     return -2.081061466 - x + 0.0833333 / xp3 - logterm + (2.5 + x) \* tf.log (xp3)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/291#issuecomment-168517728
> .
 CPU and GPU versions of lgamma, digamma, erf, and erfc are in, igamma/igammac/betainc are coming soon.  Marking as done.
 As far as I can tell, the digamma function is only used for entropy and
other log-expectation calculations.  The rest use gamma or log-gamma.

On Tue, Mar 15, 2016 at 4:48 AM, dush64 notifications@github.com wrote:

> Sorry to dig this up again, but are there any plans to incorporate the
> polygamma function (1) in the near future? Anything using a Dirichlet /
> Beta prior distribution will utilise the digamma function, for which the
> gradient is the polygamma.
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly or view it on GitHub:
> https://github.com/tensorflow/tensorflow/issues/291#issuecomment-196783111
 it is worthwhile to add trigamma or polygamma from cephes.  you will have to add it to Eigen first, and verify CUDA support works.  see bitbucket.org/eigen/eigen/ and specifically my PRs adding lgamma/digamma/igamma/igammac/erf/errfc to that repository.  happy to review your code there.
 However, let's split any new special functions off into a new issue.
 Agreed; this issue is closed.  First push changes to Eigen and then open an issue to bring tri/polygamma into TensorFlow once that's done.
  You're right, they should be 1D batch-size tensors of integers. Correcting, great thanks for bringing this up!
 If there are comments to be fixed, please send PRs for them -- I think we correct some of these already.
  Have you followed all of the steps in the tutorial in order? Near the beginning it has instructions to [start a `tf.InteractiveSession`](http://tensorflow.org/tutorials/mnist/pros/index.html#start-tensorflow-interactivesession). If you do so, that error should never be raised. 
  We don't have `sort` and `argsort` ops at the moment, but would welcome contributions.
 The only reason is limited resources. :)  They should clearly exist, so we'd love contributions in case we don't get to it in the near term.
  Are you using the 0.6.0 package? This should be fixed in HEAD on github.
On Jan 31, 2016 6:28 PM, "Hujie Wang" notifications@github.com wrote:

> bug confirmed
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/287#issuecomment-177717230
> .
 Should be fixed in 0.7+
  This is out of scope for tensorboard, which is a visualization platform rather than a graph generation platform.  A separate project building on tensorflow might be interesting, though!
  This looks like an issue for skflow, which is a separate project from tensorflow.
 Unfortunately it's still a separate project, so we can't track its issues here.
  Thanks! We'll update the docs accordingly.
  Most likely the runtime is selecting too old a version of protobuf -- we require protobuf 3.0.0.a3 or higher.

I'd see if any other installed version of protobuf is somehow interfering.
 Related to not having the right version of protobuf installed, closing due to inactivity.
  I.e., -1 is not just for flattening, which is what the current docs say.
 In review, should show up in git fairly soon.
  Unfortunately due to the large filesize the model file assets are no longer included in the git repo, but must be downloaded and installed separately.

Additional steps required (also in readme):
$ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip -O tensorflow/examples/android/assets/inception5h.zip
$ unzip tensorflow/examples/android/assets/inception5h.zip -d tensorflow/examples/android/assets/

I'm working on a fix to have this done automatically, but it will require an update to Bazel so that new_http_archive works properly with android_binary assets.
 To improve on the instructions above, you should delete the zip after extracting (or download it somewhere else in the first place) so that the resulting APK does not also bundle it.
  Likely a dupe of: https://github.com/tensorflow/tensorflow/issues/136

We've fixed this at HEAD for reasonable GPU memory sizes, it will be in our next binary release.  If you feel comfortable building from source, you can make progress :)
  https://www.tensorflow.org/versions/master/how_tos/reading_data/index.html#csv-files
is what we have now.  Is there a standard data set that comes in CSV format?
  This is a good suggestion -- (we named the existing op _with_logits because we anticipated this request, not because we like long function names ;).  I don't have a roadmap for exactly when this will be done though, but thanks for filing the feature request!
 @chuanwen: I'm not sure what exactly you mean, is that some combination of tf.where and tf.gather?
 `sparse_softmax_cross_entropy_with_logits()` is now available
  Hi @aliabbasjp: This kind of question is better suited to the StackOverflow forums or the discuss@tensorflow.org mailing list -- can you please repost there?  Thanks!
 Currently in the translate model, they take the argmax of these logits for the next word. The words aggregated together give the predicted sentence. 
  (Stack overflow is indeed the right venue for this type of question, given the information so far)
  De-duping this with: https://github.com/tensorflow/tensorflow/issues/164 (try not to consolidate discussion about a feature request or topic if possible -- otherwise it makes it hard for us to keep track of all these issues :).  We'll split them into new issues if needed though).
  Did you do as suggested and apt-get install python-dev ?
  Links are working now I think.
  "ImportError: No module named core.framework.graph_pb2" happens typically when you're trying to run in the same directory as the root of the source tree.  If you change into a different directory, does the problem go away?
 That link is showing old contents: we've updated our git repo with the new instructions (as you note, the instructions on the website doesn't work).

Can you get things to work if you use virtualenv and use the binary pip install, by any chance?
 Sorry, I didn't get a chance to look at this myself -- do you mind explaining what you did so others who run into the same problem might know the answer?
  Hi, what is the content of your google/protobuf directory? If empty, try git cloning the repo with the --recursive flag to grab all of the Tensorflow submodules.
  https://github.com/tensorflow/tensorflow/issues/1, when resolved, will add support for python 3 -- at that point hopefully this issue will go away :)
  https://github.com/tensorflow/tensorflow/issues/136 - deduping
  Sounds like an interesting idea - feel free to create a repo that does this and update this issue if you do!
 I would probably ask on the discuss mailing list to get more community input :).
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community
  There's now a toggle all runs button in the runs pane (see: [example](https://www.tensorflow.org/tensorboard/cifar.html#events))
  Yes, this has been fixed already in our repo.  Thanks for reporting!
  It's here:
https://github.com/tensorflow/tensorflow/blob/1d76583411038767f673a0c96174c80eaf9ff42f/tensorflow/python/ops/nn_ops.py#L24
Not sure why it's not exposed in the documentation yet, possibly because the API isn't considered stable.
 https://github.com/tensorflow/tensorflow/blob/1d76583411038767f673a0c96174c80eaf9ff42f/tensorflow/python/ops/nn_ops.py#L29 suggests it isn't agreed that the op is doing deconvolution.  Hence why it wasn't made public yet.
 @vincentvanhoucke: I'm happy to expose it in the public API as long as we're fine with the misleading name.
 @girving I remember being very, very confused when the term 'deconvolution' started popping up in the literature for this operator. I blame Matt (Zeiler) ;-) I had to go back to his papers to convince myself of what they were doing and that it had nothing to do with actually deconvolving the input.
Matt Zeiler calls the entire stack Deconvolution Networks:
http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf
but only ever refers to this operator as a 'projection operator' as far as I can tell.
Caffe calls it Deconvolution (@Yangqing) :
http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1DeconvolutionLayer.html

Since 'deconv' is apparently here to stay, should we consider making the name a bit more explicit? On the table from my POV:
1- transpose_deconv2d()
2- project_deconv2d()
3- deconv2d()

Note that I actually don't know what an alternative 'deconv2d' might look like. The process of deconvolving an input is ill-posed and there are many ways to go about it, so it's possible that option 3 is just fine as long as there is an abundance of documentation. @shlens and @josh11b might have opinions on the matter. 
 I think `transpose_conv2d` or `conv2d_transpose` are the cleanest names.  `transpose_deconv2d` implies it is the inverse transpose of `conv2d`.  `convt` is clever but very easy to mistake for `conv`.
 conv2d_transpose SGTM, it would put it jut after conv2d in alphabetical ordering. We should emphasize in the doc that it's what's often referred as 'deconv' or 'deconvolution', so that it shows up if anyone searches the docs for those terms.
 @jeffschecter: What kind of error do you get with the kernel crash?  This may be the same thing as #449, in which case it's fixed in 5de908567355337fdebd997fb5c60993cbe9ba2e and will be part of 0.6.0 soon.  That is, it should still generate exceptions, but will no longer crash the process.
 @jeffschecter: Oops, no, it's an independent bug.  I'll fix that now.  Thanks for the catch.
 @futurely: I'm not sure what you mean by those references, since they are to actual deconvolution ops.  There are many ways to perform deconvolution, but the transpose of convolution is not one of them. 
 @futurely: Feature requests about deconvolution should go in a separate issue.  This thread is about the transpose of convolution, which is unrelated.
 The transpose-convolution operator already exists in TF, I think it is one of the conv_2d_backprop_*() functions.  If we were to give it another name as part of exposing it in the api, I'd prefer conv_2d_transpose or some such and having documentation that some sources mistakenly refer to that op as deconvolution.  I think we should not contribute to the misuse of the deconvolution term -- it leads to things like futurely@'s confusion.
 Yep, Vincent and I agreed on `conv2d_transpose` earlier in the thread.
 @ry: Questions like that are better suited to stackoverflow.  This thread should stay focused on the  missing `conv2d_transpose` op.  (However: it isn't related to upsampling, you may be looking for `tf.resize_bilinear` and friends).
  Do you have a GPU in your machine that you want to use?  

If not:
1) consider using the CPU binary, not the GPU binary
2) you should be able to ignore these warnings.  they are helpful for debugging for those who do want to use GPUs.
 Then that means that our library is unable to detect your GPU.

1) What GPU do you have?
2) What cuda library version do you have installed?
 Just FYI, Quadro 600 might be a little bit old for running TensorFlow... We officially support cuda compute capability 3.5 (and 3.0 seems to be working well too). Quadro 600 is at 2.1, so there might be a few bumps.

If you have cuda installed, running the device query binary from nvidia's samples would usually help checking if the GPU is properly detected.
 https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 if you're comfortable building from source -- may allow you to run on your K600 GPU, though it may not be that much faster than CPU.

Closing this for now -- feel free to re-open if there's anything else you need!
  We have not tried -- if you (or someone else) gives it a shot, let us know!
  I think tf.slice(...) might be what you want for getting 1 column.  Let us know if that doesn't work!

You might also want to follow https://github.com/tensorflow/tensorflow/issues/206 

As for text classification: I'm sure that will be added over time (either by us or externally).
  As a team we don't use PyPy day-to-day, but we would welcome contributions if it's easy to do this without breaking CPython compatibility.

My guess is that the two stumbling blocks would be TensorFlow's reliance on NumPy in the Python front-end, and SWIG for interfacing with the C++ backend. Are you aware of any other issues that one would face?
 I'm not sure what the state of PyPy binding layers is these days, but there's a good chance this would require rewriting the whole swig interface.
  Can you paste your env vars here?
  We could add a .gitignore for "bazel-*" in the short term -- would that be palatable?
 De-duping with https://github.com/tensorflow/tensorflow/issues/199
  Don't worry, we've actually fixed it already in the repo.  We're trying to update the site with it soon.

https://github.com/tensorflow/tensorflow/issues/78 -- deduping
  This is fixed in git, pending a push to the website.  Thanks for the report!
  The translate modules are indeed currently not part of the install (they're currently more of a reference).

We're thinking about how to repackage some of the files in our codebase (e.g., tutorials, docs, example models, etc), so we'll have a better answer for you soon.  Thanks!
 The translate modules (as well as the other model resources) are for the most part just an example that can be moved / copied.  Some of them  had a few out-of-file helper modules that were not part of the pip package that would break during execution of say, translate.py, but we have added them to the module for our next release, to get things working as written.

In general, the pip package will probably just be for installing the core tensorflow python library, and any helper libraries to get our tutorials to work.

Users should still git clone the source repo to get access to examples, tutorials, etc, and should write their code assuming they are using the public API, where the public TensorFlow API is defined as what is documented at http://www.tensorflow.org/versions/master/api_docs/index.html
 Fixed in 0.6.0
  De-duping with https://github.com/tensorflow/tensorflow/issues/208
  @gouwsmeister has a fix out and we'll update the repo soon.  Thanks!
 Should be fixed at HEAD
  This was fixed a while ago, should be fine in 0.7 and up.
  We're just about there now with jenkins, thankfully.
 Please close this issue. Solution provided by: https://github.com/tensorflow/tensorflow/pull/1255
  As noted in our contributing.md, currently our changes have to go through gerrit (which I see you've done, and we'll try to integrate soon).  Closing for now, thanks!
  As noted in our contributing.md, currently our changes have to go through gerrit (which I see you've done, and we'll try to integrate soon).  Closing for now, thanks!
  Duplicate of https://github.com/tensorflow/tensorflow/issues/185.  Already fixed in the source but not yet pushed to the website.
  Duplicate of https://github.com/tensorflow/tensorflow/issues/236, which is itself a duplicate of an earlier bug.
  Can you build from source with the flag "-c dbg" and provide the crash log?
 I think it's just that the kernel doesn't handle the empty input (e.g., input.NumElements() == 0).

I guess we could check that input or filter are empty, and if so, to produce an empty tensor of the correct shape.
 We added a simple check here: https://github.com/tensorflow/tensorflow/commit/04f1932f053dd7865b191719b33860270461943a#diff-f9ae051d576c85eab133e77af84c1937R170

Let us know if that doesn't solve the problem (at least for this op -- I'm sure there are others that will pop up).
  My best guess: did you train with --size=256 ?  If so, you need to decode with the same set of command line flags as the one you trained with.

The error is basically saying that there is a size mismatch: the variable being assigned to is of size [256] but it expected [1024].  I don't see any sizes of '256' in the translate.py example, so that's why I suspect it was overridden via flags.
  It's updated in our repo, but not on our website yet.  We're going to update the website to reflect the current state of the repo soon.

(We get this question a lot, has it been hard to search for in our existing issues list?)
  bazel run [options] [target] -- [--commandlineflags]
 We've already fixed it in our internal repo :)  We need to push out the change early this week to both git and the website -- thanks for the feedback!
  That looks like a typo on your end (I can't find that snippet of code anywhere) -- please re-open with a way for us to reproduce the problem if there's a bug in our tutorial.  Thanks!
  Your GPUs are a little too old to be used in our 0.5.0 release.  See https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 for how to build from source if you want to try to get cuda 2.0/3.0 cards working.

From there, you can also use the environment variable [CUDA_VISIBLE_DEVICES](http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/) to control which GPUs are exposed to the process.
  Thanks for the report -- this has been updated in our repository but not yet pushed to the website.  Will probably be updated this week!
  Are you running a version of the tutorials from HEAD against tensorflow installed via pip or similar?  We just added `__truediv__` support as part of #1. 
 Yep, you're running 2.7, but how did you install tensorflow and where did you get the tutorial?  I think you might be running a newer version of the tutorial than tensorflow itself. 
 @arieltci: For now, deleting that import is the right thing for you to do.  @vrv: Not sure if we should close or remove that division from the tutorial so that it's more backwards compatible. 
 I'm going to fix the tutorial in git to be compatible back to 0.5.0.
 Fixed, will be part of the next git commit.  Thanks for reporting!
  If you're comfortable building from source, https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 should have an answer for you, though we haven't tested it on anything below 3.0.

Will de-dupe this bug with that one, thanks!
  Should be fixed in 0.6.0
  Hey @FabHan, can you post the entire code example? Will be easier for me to help debug. You could use a https://gist.github.com/ 
 I'm having trouble reproducing your error, but here's a working example in which the entire mnist_softmax tutorial has been modified into interact well with TensorBoard. (I'll merge this code example into the TensorBoard tutorial to keep people from running into your issue in the future.)

``` python
"""A very simple MNIST classifer, modified to display data in TensorBoard

See extensive documentation for the original model at
http://tensorflow.org/tutorials/mnist/beginners/index.md

See documentaion on the TensorBoard specific pieces at
http://tensorflow.org/how_tos/summaries_and_tensorboard/index.md

"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

# Import data
import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)

import tensorflow as tf
sess = tf.InteractiveSession()

# Create the model
x = tf.placeholder("float", [None, 784], name="x-input")
W = tf.Variable(tf.zeros([784,10]), name="weights")
w_hist = tf.histogram_summary("weights", W)
b = tf.Variable(tf.zeros([10], name="bias"))
b_hist = tf.histogram_summary("biases", b)
with tf.name_scope("Wx_b") as scope:
  y = tf.nn.softmax(tf.matmul(x,W) + b)
y_hist = tf.histogram_summary("y", y)

# Define loss and optimizer
y_ = tf.placeholder("float", [None,10], name="y-input")
with tf.name_scope("xent") as scope:
  cross_entropy = -tf.reduce_sum(y_*tf.log(y))
  ce_summ = tf.scalar_summary("cross entropy", cross_entropy)
with tf.name_scope("train") as scope:
  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)

with tf.name_scope("test") as scope:
  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
  accuracy_summary = tf.scalar_summary("accuracy", accuracy)

merged = tf.merge_all_summaries()
writer = tf.train.SummaryWriter("/tmp/mnist_logs", sess.graph_def)
tf.initialize_all_variables().run()

# Test trained model

for i in range(1000):
  if i % 10 == 0:  # Record summary data, and the accuracy
    feed = {x: mnist.test.images, y_: mnist.test.labels}
    result = sess.run([merged, accuracy], feed_dict=feed)
    summary_str = result[0]
    acc = result[1]
    writer.add_summary(summary_str, i)
    print("Accuracy at step %s: %s" % (i, acc))
  else:
    batch_xs, batch_ys = mnist.train.next_batch(100)
    feed = {x: batch_xs, y_: batch_ys}
    sess.run(train_step, feed_dict=feed)

print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))
```

If you post your full code so I can run it and repro your error, I'll try to help figure out why TensorFlow was throwing that error :)
  Thank you for your contribution!  In general we encourage users with nonstandard setup to build their own pip packages from source.  And we are specifically encouraging the CUDA 7 runtime with CUDNN 6.5 because it's one less variable to consider when debugging (and this is the version we use and test with).  Since it doesn't seem like there's any issue, I'm closing / archiving this for now.
  We should probably clear this up somewhere, but at the moment there is only one 'CPU device' per process.  So cpu:0 has access to all 8 of your cores via threading.

(We thought it would be useful to have the ability to specify multiple CPUs in the future if it turns out that NUMA-aware assignment was helpful)
 @vrv -- presumably the fix here is to just document this?
 https://www.tensorflow.org/versions/r0.7/how_tos/using_gpu/index.html description of devices is reasonable but could be better.  Happy to accept a PR to make this even more clear.  Closing to reduce the size of our enormous open issues list.
 @zkl99999 this sounds like a connectivity problem; are you still seeing this?
  Thanks for pointing it out. We will fix it at the source.
 @colah, can you take a look at this?
 was fixed ages ago
  matrix_inverse unfortunately is not yet implemented on GPU, but some of the other operations are, so I suspect there's some amount of memory copying.  MatrixInverse on CPU is just calling Eigen::FullPivLU: https://github.com/tensorflow/tensorflow/blob/d6357a5849db980df51d00d8a9ff874cda2faeb3/tensorflow/core/kernels/matrix_inverse_op.cc#L50, so the optimizations probably have to be made at that level.

(We also don't distribute our binary package using avx/avx2 optimizations so they can run on more platforms, so it's possible things might run faster when running from source and building with more optimizations for your platform).
 We're still going through all of these issues and prioritizing, so we don't have a roadmap ready yet.

Once we are more able to accept external contributions, improvements like these from the community are welcome!  A quick search suggests that cublas has a matrix inversion function that might be appropriate.  Eager contributors should discuss with us here or on the discussion mailing list to figure out the best way to integrate such a change.

Renaming the title of this issue accordingly!
 I'm not sure the CPU vs. GPU aspect is the only issue, since I'd be surprised if scipy used the GPU for matrix inverse.
 Indeed, even on CPU it's slow.  Should this bug just be the catch-all for now?
 Hmm, one of the measurements says tensorflow is twice as fast, the other says it is three times as slow.  @delip: Is it still consistently 3x slower for you?
 Thanks for confirming @delip!  We should leave the bug open until the gap is better understood (and ideally closed).   Could I have your script to reproduce?
 @girving: are we still looking at this? worth keeping open?
 I'll check this once I get tensorflow building on my Mac.
 @rmlarsen: Vijay and I are getting around 40% slower than scipy with this benchmark.  Turning off the symmetry checking made no difference.  We're dramatically closer to good than we were at the beginning of this bug, but I'll leave it up to you whether we should close it or do further investigation. 
 Closing this since there's been no activity on this for a while -- 40% slower on OS X for matrix_inverse isn't great, but suffices for now.  Looks like we're faster on other platforms so, there's always that.
  Currently, the android demo is built with Bazel - one way to go is to put your android app under bazel, and follow a similar build rule as highlighted by the Android example.

If that is not possible, you can manually copy the header files and the pre-built .so file to your Android Studio project. The libtensorflow_demo.so can be found in the bazel cache. If you use Linux, it is usually located at ~/.cache/bazel. Build the target first using

``` shell
bazel build //tensorflow/examples/android:tensorflow_demo -c opt --copt=-mfpu=neon
```

, and find the file with:

``` shell
find ~/.cache/bazel -name "libtensorflow_demo.so"
```

(Another way is to rename the *.apk to *.apk.zip and unzip it. The resulting .so file is the same as the one found above.)
 Assuming the above addresses your problem, I will close this issue - feel free to reopen if you encounter problems further.
  Are you able to install the binary wheel via the getting started guide,
without going through the pip build process?
On Nov 14, 2015 6:53 AM, "Yi Leung" notifications@github.com wrote:

> I setup tensorflow according to
> http://tensorflow.org/tutorials/seq2seq/index.md. When I do the step as
> below:
> bazel-bin/tensorflow/tools/pip_package/build_pip_package
> /tmp/tensorflow_pkg,but the below error was throw:
> Sat XXXXXXXXXXXXX CST 2015: ===Building wheel
> Traceback(most recent call last):
> File “setup.py”,line 3,in
> from setuptools import find_packages,setup,Extension
> ImportError:No module named setuptools
> BTW, I had no do the step for (Optional) Enable GPU Support
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/219.
 You need [python setuptools](https://pypi.python.org/pypi/setuptools) installed. Either `sudo apt-get install python-setuptools` or `sudo pip install setuptools` should install that on a Ubuntu machine.
 Let us know if that isn't sufficient.  Thanks!
  `bazel run -c opt tensorflow/models/rnn/translate:translate`

in bazel you have to build / run the target as defined in the BUILD file, rather than the source file.
 Good call -- sent out the fix internally and we'll push the change out soon.  Thanks!
  Thanks for the report -- we'll look into this at some point!  Hopefully our code is at least internally consistent :)
 @martinwicke: Do we have a plan in terms of linting and/or pep8 compliance?  Should this be closed?
 Plans, yes, time to do it, no. I would like to make much more of the pylint
we're already running, and ideally add cpplint as well.

On Mon, Jun 6, 2016 at 10:30 AM Geoffrey Irving notifications@github.com
wrote:

> @martinwicke https://github.com/martinwicke: Do we have a plan in terms
> of linting and/or pep8 compliance? Should this be closed?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-224028856,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_VoOya6ZrBTvi7Gs1zoghoDgvTGvks5qJFkvgaJpZM4GiS_S
> .
 @martinwicke: I'll leave it assigned to you for now, and you can reassign to other testing folk as appropriate. 
 @staranjeet: I think we may need to figure out a policy of just how pep8 compliant we want to be first.  For example, I'd hate to give up our 2 space indent paradise.
 Definitely not while we still maintain a 80-character hard limit.

On Mon, Jun 6, 2016 at 10:13 PM Geoffrey Irving notifications@github.com
wrote:

> @staranjeet https://github.com/staranjeet: I think we may need to
> figure out a policy of just how pep8 compliant we want to be first. For
> example, I'd hate to give up our 2 space indent paradise.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-224179082,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_TqzEtKeX6LZw0zbNhLTi5PsFRKdks5qJP4OgaJpZM4GiS_S
> .
 @staranjeet We'd like to ignore some thing (in particular, the 4 space indent), but otherwise, proper linting would be great. We already run pylint as part of the build, but we disabled all but the most terrible errors (syntax errors, unknown names) in order to make it blocking without damaging productivity.

It would be nice to have a stricter pylintrc that we can use for informational purposes. Especially if we can generate statistics on how many lint issues were added (and make that a blocking error). 

I haven't used flake8 miself, if it supports both python 2 and python 3 and is sufficiently flexible we could use that as well.
 Closing this -- if there are specific violations, or a nicer linter for our tests, I'd love more concrete issues or PRs.
 We currently use pylint: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh

We can talk about changing options to make it more powerful -- it will need some experimentation to get the right balance between false positives (which we have to whitelist) and enforcing the rules.
 E402 has valid uses (conditional imports to avoid breaking on missing soft
dependencies) and we should probably ignore those if they are used for such
a purpose.

E129 is a bit iffy, i have to check how these look.

I'd like to find a way to enforce these going forward (=add a stricter
linter test), but we have to make sure that we don't conflict with our
internal tools. Otherwise I agree, these should be fixed.

On Sun, Jun 12, 2016 at 01:27 Taranjeet Singh notifications@github.com
wrote:

> The errors which we are planning to ignore
> Error Error meaning
> E501 line too long
> E111 indentation is not a multiple of four
> E114 indentation is not a multiple of four(comment)
> E701 multiple statements on one line (colon)
> 
> Now the stats while running pep8(ignoring the above mentioned erros) tests
> are as follows:
> Error Error meaning Count
> E101 indentation contains mixed spaces and tabs 2
> E115 expected an indented block (comment) 4
> E116 unexpected indentation (comment) 1
> E121 continuation line under-indented for hanging indent 27
> E123 closing bracket does not match indentation of opening bracket's line
> 18
> E124 closing bracket does not match visual indentation 15
> E125 continuation line with same indent as next logical line 195
> E126 continuation line over-indented for hanging indent 22
> E127 continuation line over-indented for visual indent 102
> E128 continuation line under-indented for visual indent 145
> E129 visually indented line with same indent as next logical line 106
> E131 continuation line unaligned for hanging indent 7
> E201 whitespace after '[' 14
> E202 whitespace before ']' 1
> E203 whitespace before ':' 3
> E222 multiple spaces after operator 1
> E225 missing whitespace around operator 21
> E226 missing whitespace around arithmetic operator 300
> E227 missing whitespace around bitwise or shift operator 2
> E231 missing whitespace after ',' 58
> E241 multiple spaces after ',' 1
> E251 unexpected spaces around keyword / parameter equals 15
> E261 at least two spaces before inline comment 6
> E262 inline comment should start with '# ' 1
> E265 block comment should start with '# ' 57
> E266 too many leading '#' for block comment 78
> E271 multiple spaces after keyword 5
> E272 multiple spaces before keyword 2
> E301 expected 1 blank line, found 0 98
> E302 expected 2 blank lines, found 1 55
> E303 too many blank lines (2) 7
> E304 blank lines found after function decorator 1
> E402 module level import not at top of file 43
> E502 the backslash is redundant between brackets 3
> E704 multiple statements on one line (def) 1
> E711 comparison to None should be 'if cond is None:' 4
> E731 do not assign a lambda expression, use a def 155
> W191 indentation contains tabs 2
> W291 trailing whitespace 43
> W293 blank line contains whitespace 15
> W391 blank line at end of file 15
> W503 line break before binary operator 58
> 
> I think that these should be removed. Let me know your opinion
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-225416784,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_ZJoZzSpTLcHP507AoFlMlkzJCFBks5qK8L1gaJpZM4GiS_S
> .
 Actually, we want to enforce line-too-long, but with some exceptions
(import statements for instance can make unavoidably long lines)
On Sun, Jun 12, 2016 at 10:00 Martin Wicke wicke@google.com wrote:

> E402 has valid uses (conditional imports to avoid breaking on missing soft
> dependencies) and we should probably ignore those if they are used for such
> a purpose.
> 
> E129 is a bit iffy, i have to check how these look.
> 
> I'd like to find a way to enforce these going forward (=add a stricter
> linter test), but we have to make sure that we don't conflict with our
> internal tools. Otherwise I agree, these should be fixed.
> 
> On Sun, Jun 12, 2016 at 01:27 Taranjeet Singh notifications@github.com
> wrote:
> 
> > The errors which we are planning to ignore
> > Error Error meaning
> > E501 line too long
> > E111 indentation is not a multiple of four
> > E114 indentation is not a multiple of four(comment)
> > E701 multiple statements on one line (colon)
> > 
> > Now the stats while running pep8(ignoring the above mentioned erros)
> > tests are as follows:
> > Error Error meaning Count
> > E101 indentation contains mixed spaces and tabs 2
> > E115 expected an indented block (comment) 4
> > E116 unexpected indentation (comment) 1
> > E121 continuation line under-indented for hanging indent 27
> > E123 closing bracket does not match indentation of opening bracket's line
> > 18
> > E124 closing bracket does not match visual indentation 15
> > E125 continuation line with same indent as next logical line 195
> > E126 continuation line over-indented for hanging indent 22
> > E127 continuation line over-indented for visual indent 102
> > E128 continuation line under-indented for visual indent 145
> > E129 visually indented line with same indent as next logical line 106
> > E131 continuation line unaligned for hanging indent 7
> > E201 whitespace after '[' 14
> > E202 whitespace before ']' 1
> > E203 whitespace before ':' 3
> > E222 multiple spaces after operator 1
> > E225 missing whitespace around operator 21
> > E226 missing whitespace around arithmetic operator 300
> > E227 missing whitespace around bitwise or shift operator 2
> > E231 missing whitespace after ',' 58
> > E241 multiple spaces after ',' 1
> > E251 unexpected spaces around keyword / parameter equals 15
> > E261 at least two spaces before inline comment 6
> > E262 inline comment should start with '# ' 1
> > E265 block comment should start with '# ' 57
> > E266 too many leading '#' for block comment 78
> > E271 multiple spaces after keyword 5
> > E272 multiple spaces before keyword 2
> > E301 expected 1 blank line, found 0 98
> > E302 expected 2 blank lines, found 1 55
> > E303 too many blank lines (2) 7
> > E304 blank lines found after function decorator 1
> > E402 module level import not at top of file 43
> > E502 the backslash is redundant between brackets 3
> > E704 multiple statements on one line (def) 1
> > E711 comparison to None should be 'if cond is None:' 4
> > E731 do not assign a lambda expression, use a def 155
> > W191 indentation contains tabs 2
> > W291 trailing whitespace 43
> > W293 blank line contains whitespace 15
> > W391 blank line at end of file 15
> > W503 line break before binary operator 58
> > 
> > I think that these should be removed. Let me know your opinion
> > 
> > —
> > You are receiving this because you modified the open/close state.
> > Reply to this email directly, view it on GitHub
> > https://github.com/tensorflow/tensorflow/issues/217#issuecomment-225416784,
> > or mute the thread
> > https://github.com/notifications/unsubscribe/AAjO_ZJoZzSpTLcHP507AoFlMlkzJCFBks5qK8L1gaJpZM4GiS_S
> > .
 We can't really add the clauses until the code is clean, as currently set
up, pylint errors are a test failure blocking a commit.
On Fri, Jun 17, 2016 at 00:32 Taranjeet Singh notifications@github.com
wrote:

> So I should start fixing pep8 warning or add clauses first in existing
> pylintrc?
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-226700696,
> or mute the thread
> https://github.com/notifications/unsubscribe/AAjO_RQmNv_b1B85iA5QO86TrPj3kw9iks5qMk2igaJpZM4GiS_S
> .
  element-wise mul should do broadcasting already, but feature requested noted for matmul.  Thanks!
 I was going to open my own issue on this, but it seems that this one is being used for everything related to batch matrix-vector ops, `numpy.dot`, etc. For those looking for a workaround, I have two implementations of batch vector x matrix multiplication:

``` python
def batch_vm(v, m):
  shape = tf.shape(v)
  rank = shape.get_shape()[0].value
  v = tf.expand_dims(v, rank)

  vm = tf.mul(v, m)

  return tf.reduce_sum(vm, rank-1)

def batch_vm2(x, m):
  [input_size, output_size] = m.get_shape().as_list()

  input_shape = tf.shape(x)
  batch_rank = input_shape.get_shape()[0].value - 1
  batch_shape = input_shape[:batch_rank]
  output_shape = tf.concat(0, [batch_shape, [output_size]])

  x = tf.reshape(x, [-1, input_size])
  y = tf.matmul(x, m)

  y = tf.reshape(y, output_shape)

  return y
```

The first is based on the broadcasting behavior of `tf.mul`, while the second relies on reshaping all but the last dimension of the input to reduce the operation down to matrix-matrix multiplication. I suspect the second method is more efficient, but it might rely on the tensors being in the right memory order (although I think tf, unlike numpy, only uses C order).
 This issue appears to be covered by the more specific #1062.  @vladfi1: Does #1062 do what you want, or does `tf.batch_matmul` work now?  We can reopen if not, but we should try to make the issue more specific if so. 
  Hi, this is probably a better question for either the discussion mailing list or StackOverflow -- can you please repost at one of those [venues](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community-)?
 (Discuss mailing list is probably the best for this)
  I'd probably encourage you to write a C++-based beam search instead of trying to do it with graph ops.  That said, we will likely provide some sort of specialized self-contained beam search for CTC with the CTC loss.  Additionally there's a PriorityQueue in the works which, when completed, will provide the ability to sort groups of Tensors.  Not sure if this would help you.  Another good project to look at for implementing a beam search is [OpenFST](http://www.openfst.org/), which was coincidentally partly developed within Google Research :)
 Marking this as a duplicate of #654  cause it seems like that's what you want to do.
 Do you mean CTC beam search specifically?
  Thanks, we'll take a look to see if we can reduce the confusion!
 @vrv: Did this ever happen?
 I doubt it -- assigning to Lukasz
 Improved the tutorial to clarify which sets of variables are meant in a recent CL, is now live on master.
  If you create both of your graphs within the same session, and as long as they are distinct subgraphs within the graph, you should be able to run both models within the same session.

pseudo-example:

```
with tf.Session() as sess:
  # Build graph 1
  model1_output_node = build_model_1()
  model2_output_node = build_model_2()

  model1_output = sess.run(model1_output, feed_dict={...})
  model2_output = sess.run(model2_output, feed_dict={...})

  .. compare the result ...
```

Let me know if that doesn't work for some reason.
 @lukaszkaiser: is this a matter of wrapping the model creation inside a name or var scope?
 I'm not 100% sure, but I think that the problem might be that the Seq2SeqModel class has some non-sharable things, like global_step and a separate saver. So if you just make 2 of them, you'll have 2 global_steps, 2 savers, and so on. I think you need to move the common things out of the class. And yes - best wrap the 2 models you're creating in different tf.variable_scope to not share parameters (or the same variable_scope with reuse=True to share them).

Let us know if that helps or if there are more problems!
 Hi cinjon,

Sorry for the delay. I was thinking a bit about what you're doing and it took me a while to wrap my mind about it, but I think I can see why the first model loads ok but the second does not.

Here is how I see it. Both model1 and model2 have a saver, right? Created like this:

``` python
  self.saver = tf.train.Saver(tf.all_variables())
```

This is ok for the first model, as it saves only the variables belonging to it. But -- because of the use of tf.all_variables() -- the second model with save both itself and model1. Does that make sense?

I see 2 ways to correct that. For one, you could filter the variables saved in model2 to start with the prefix you're giving it. Another, and I think better way, is to remove the saver from the 2 models entirely, and have it only once in your main loop - after you've created both models. Then you'll have only a single checkpoint file and you can still use tf.all_variables() without the risk of forgetting anything, and it should all work.

Hope that helps!

Lukasz
  Thanks for this feature request! In the meantime, if you want to test your model without explicitly specifying the device for each op, you can do:

``` python
sess = tf.Session(tf.ConfigProto(allow_soft_placement=True))
```

...when constructing your session. This allows you to request a GPU device for any of your ops, and it will fall back to running on a CPU if there is no GPU kernel available.
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dnn.h#L822 does exist, if someone wants to plumb that call through, I think we'd have LRN support for GPU :)
 Right, cudnn has an LRN implementation, so the point is to have it use that one.
 Thanks, removing the contributions welcome tag, since this is being worked on.
 @vrv: Does the person working on it have a Github username?
 I don't know, @rryan do you know?
 Once Stream Executor support is finished I was planning to work on it -- so you can assign me if you'd like :).
 @rryan: Assigned, thanks!
  Thanks for the request!  For the purposes of prototyping, you could probably implement this using more primitive operations (e.g., using tf.exp, tf.reduce_sum , etc) -- let us know if there's something missing from the set of primitive operations.
 I think this is best implemented with `reshape` and `transpose` followed by the existing functionality.
  Page looks to be back up, so I'm closing this for now.
  Our white paper mentions a number of control flow operations that we've experimented with -- I think once we're happy  with its API and confident in its implementation we will try to make it available through the public API -- we're just not quite there yet.  It's still early days for us :)
 See the highly alpha and unsupported control_flow_ops.While, the
TensorArray python class, and nn.dynamic_rnn.  All at HEAD.
On Feb 16, 2016 7:48 AM, "Dmitrij Koniajev" notifications@github.com
wrote:

> +1
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-184739842
> .
 You can now use control_flow_ops.{map, foldl, foldr}  with forward and backprop, and you can call these functions from inside each others' lambdas.  For RNN, you can use dynamic_rnn which does the same.  If you have more complex eneds you can comment them here or use control_flow_ops.While and TensorArray.  I'm marking this as fixed.
 @ebrevdo: are those functions:

1) public
2) documented?
 Indeed not, but I believe {map_fn, foldl, foldr} are ready to be added by
referencing them in the header.  We'll get a CL out soon.

On Wed, Mar 9, 2016 at 9:21 AM, Vijay Vasudevan notifications@github.com
wrote:

> @ebrevdo https://github.com/ebrevdo: are those functions:
> 
> 1) public
> 2) documented?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-194410139
> .
 See the implementation of dynamic_rnn for a comprehensive example.
On Mar 9, 2016 6:59 PM, "rakeshvar" notifications@github.com wrote:

> A toy RNN example with these functions would go a long way; especially
> since they do not directly correspond to the theano.scan functionality.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-194637554
> .
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py#L256 !
 Scan uses the same approach as dynamic_rnn.  The latter was made first.
Whereas in theano, scan was the fundamental loop method, in tensorflow that
behavior is subsumed my while loop and tensorarray.
On Apr 7, 2016 2:45 PM, "Robert DiPietro" notifications@github.com wrote:

@rakeshvar https://github.com/rakeshvar here is a vanilla-RNN example:
https://nbviewer.jupyter.org/github/rdipietro/tensorflow-notebooks/blob/master/tensorflow_scan_examples/tensorflow_scan_examples.ipynb

(dynamic_rnn currently uses While and TensorArray, not scan.)

—
You are receiving this because you were mentioned.
Reply to this email directly or view it on GitHub
https://github.com/tensorflow/tensorflow/issues/208#issuecomment-207100390
 How to use map and scan together??????? Anyone have any idea?

x=tf.constant(
[[[1,2,3],[10,20,30]],
[[1,2,3],[10,20,30]],
[[1,2,3],[10,20,30]], 
[[1,2,3],[10,20,30]]])

def sum(x): return  tf.scan(lambda y, z : tf.add(y,z), x)
tf.map_fn(sum ,a )

Then errors pop up!!!! Is there any way using these together?
@rdipietro I have been through your example notebook but how to use it for batch????
 The implementation of both scan and map_fn uses TensorArray. Nesting scan inside map_fn is essentially nested while loops. Unfortunately, there is a known bug when TensorArray and nested while loops are used together.  We have been working on a fix. 
 Ok thanks.
 I am using scan to deal with different sequence length. But having problem to deal with batch! @rajarsheem can you write your solution for my above  problem??? If I use for loop then its same like online learning! No need to use batch then! So no need to use GPU then!

Please Google solve this nested while loops. ...
 @rdipietro Thanks a lot. I will go through the your example!
 Finally I have figured out how to use scan with batch. So I have written some simple code for dynamic  [RNN](http://knhuq.github.io/RNN.html) and [GRU](http://knhuq.github.io/GRU.html). Have conducted a little hack as Tensorflow was raising low level error.  The github repo is [here](https://github.com/KnHuq/Dynamic_RNN_Tensorflow)
  In your code, it doesn't look like you wrapped the declaration of 'c' within the 'with tf.device()' block, so it doesn't have a device assigned to it.  Because we have an implementation of matmul on GPU, we automatically place the op on GPU if the op isn't hardcoded to a device.

I bet if you moved  c = tf.matmul(a, b) into the `with` block,it would probably do wht you expect.

As for the benchmark: GPUs are generally good for large matrix multiplies, so it's not a bad benchmark, but it depends on what you intend to use TensorFlow for.  The best benchmark is a real application :)
 Yeah, matrix inverse has not yet been implemented for GPU, so when you say 'run this on GPU', we can't find an implementation for it and we return that error.
 Those are the list of the files, but some of those files have multiple op implementations in them.

I would love to be able to automatically generate the list of implemented ops for each platform so they don't go out of date.  Maybe one day soon I'll get to doing this :)
 (should have a separate feature request for this)
  We should make our slicing and assignment ops more general to capture more of the functionality of numpy slicing, and add `__getitem__` sugar for all of it.  Specifically,
1. We should have a 2.5 dimensional set of ops, with dimensions (1) get vs. set, (2) slice type, and for the assignment ops (3) the update op.  Currently we have `slice`, `assign_update`, `assign_add`, `assign_sub`, `gather`, `scatter_update`, `scatter_add`, `scatter_sub`.  We should also have `assign_slice_update`, `assign_slice_add`, `assign_slice_sub`.
2. Both slicing and slice assignment should support strides, with no performance cost if strides aren't used.
3. Ideally, the slice ops should support negative indexing a la Python.  Since the slice parameters are already CPU, this is implementable with near zero cost.   The unfortunate bit is that since we picked the wrong format for specifying ranges (start + length instead of start : end), negative indexing might be awkward.  Thus, it might be best left to a separate bug.
4. Support numpy-like boolean indexing.
5. Generalize `gather` and `scatter_*` to take an array of input index tensors, efficiently broadcast them, and do multidimensional indexing similar to numpy.
6. Make `__getitem__` provide sugar for all of the above.  Ideally we'd have something idiomatically similar at least to `__setitem__`, but this is problematic since the returned assignment op is important to have, `__setitem__` does not return a value, and the nice range sugar is available only inside indexing / assignment calls.

@ebrevdo: I'm assigning this to you for now since you might get to it first, but feel free to grab only the piece of it that you need for now.
 Lasse requests the equivalent of numpy mixed indexing:

```
x[:, tensor]
```

which combines slicing with indexing-by-tensor.
 Numpy also has newaxis and "Ellipsis" objects. IE, to prepend an axis using numpy notation
a[np.newaxis,...]

http://docs.scipy.org/doc/numpy-1.10.0/reference/arrays.indexing.html
 Yes, newaxis is essential. 
 As part of this, we should make https://github.com/tensorflow/tensorflow/issues/418 work.
 @olange-google: I think we're unlikely to implement an axis parameter since it's beyond numpy features, but what you want is covered by the _combination_ of slice indexing and advanced indexing. 
 Are you sure?  If in numpy I use:

array[:, :, [1, 2, 3], :]

then that's equivalent to gather([1, 2, 3], axis=2)

is it not?

On Wed, Feb 17, 2016 at 11:06 AM, Geoffrey Irving notifications@github.com
wrote:

> @olange-google https://github.com/olange-google: I think we're unlikely
> to implement an axis parameter since it's beyond numpy features, but what
> you want is covered by the _combination_ of slice indexing and advanced
> indexing.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-185353303
> .
 We're saying the same thing.  I'm not objecting to that functionality, just to exposing it via that sort of axis parameter rather than as a special case of the combination of slice indexing and advanced indexing.
 I plan to work on this but not until after next week. If you want to work
in this, I suggest supporting GPU and CPU in your kernels. If you can't
implement that, you may want to wait for us to implement it.
On Feb 23, 2016 11:45 AM, "Mohsen Hejrati" notifications@github.com wrote:

> @ebrevdo https://github.com/ebrevdo What is the status on this issue?
> I am interested to implement this if no one is working on it.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-187869273
> .
 OK recently added the gather_nd op, which performs a special subset of the required functionality: given a tensor of indices, gather the requested values.

Advanced slicing is on the radar.
 Yeah - we haven't written the gradient implementation for gather_nd yet.
It's essentially a reshape followed by a call to sparse_to_dense; but
sparse_to_dense doesn't have a GPU implementation (on my TODO) so I'm not
using it yet.

On Fri, Apr 8, 2016 at 7:24 PM, Waleed notifications@github.com wrote:

> @ebrevdo https://github.com/ebrevdo I tried using the gather_nd op to
> get the last relevant output from a variable length LSTM network. I'm
> passing sequence_length to the RNN, which means that the last few outputs
> of most examples are zeros, so I'm trying to read the last non-zero output.
> I'm getting this error, though, in the training phase:
> 
> _NotImplementedError: Gradient for gather_nd is not implemented._
> 
>   outputs, state = rnn.rnn(multi_rnn_cell, inputs, dtype=tf.float32, sequence_length=lengths_ph)
> 
>   indicies = tf.concat(1, [
>       tf.expand_dims(lengths_ph - 1, 1),
>       tf.expand_dims(tf.range(tf.shape(vectors_ph)[0]), 1),
>       tf.expand_dims(tf.zeros_like(lengths_ph), 1),
>       ])
>   output_tensor = tf.pack(outputs)
>   relevant_output = tf.gather_nd(output_tensor, indicies)
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-207684602
 I will implement a gradient in the next week.  Keep in mind that it will be
CPU-only for now.

On Wed, Apr 20, 2016 at 2:27 AM, Norman Casagrande <notifications@github.com

> wrote:
> 
> While we wait for gather_nd for supporting gradients, this is a temporary
> solution:
> 
> x = tf.constant([[1, 2, 3],
>                  [4, 5, 6],
>                  [7, 8, 9]])
> idx = tf.constant([1, 0, 2])
> idx_flattened = tf.range(0, x.shape[1]) \* x.shape[0] + idx
> y = tf.gather(tf.reshape(x, [-1]),  # flatten input
>               idx_flattened)  # use flattened indices
> with tf.Session(''):
>   print y.eval()  # [2 4 9]
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-212347297
 @keithshep PRs are most welcome.  I would start with adding negative indexing to the current getitem/setitem (the one that currently works on the outermost dimension).  This will require changes to both the kernel and the python wrappers, and updates to the shape inference code.
 Cc @aselle.
 @keithshep: Do you mind if we give this to @aselle as a starter project?  He just joined Brain. :)
 For `__setitem__`: we can't use it directly because it doesn't return a value, and usually you need the return value of assignments (to make sure they execute).  However, we could make `x[i].assign(y)` work with a bit of class trickery.
 @keithshep: Unfortunately I don't know what you mean.  We can't manipulate a computation graph until we have one to manipulate, and `__setitem__` discards the output of the assign.  Normally that return value needs to be used later on, typically as part of a training op which is then run.
 @keithshep: That approach would be reasonable, but unfortunately I think it's too big a jump from what we have currently.
 @shoyer: I don't think you need to elaborate.  Outer indexing makes sense, but it isn't what numpy does.  Numpy is also more flexible than outer indexing.  Finally, I like numpy indexing. :)  Thus, unless I see a bunch of support from other people for deviating from numpy, we'll stick with numpy.
 @shoyer What do the odds look like for that PR being accepted on the Numpy side?
 @shoyer: Please get it merged on the numpy side and then file a separate issue.  I predict a 0% chance that numpy will remove the existing indexing support or make it not the default: that would break millions of lines of code and alienate huge numbers of numpy users.  If they add it as an optional feature, TensorFlow could easily do the same thing with the same syntax.
 @danijar, @shampool: Yes on both counts.  
 @danijar: Let's keep this thread focused on the new feature.  Please ask side questions on StackOverflow. 
 I'm working on a gather_nd extension to support inner slicing.. Note that
gather_nd still has no gradient.
On Jun 8, 2016 4:51 PM, "Ben Usman" notifications@github.com wrote:

> hi, I attempted to write a temporary workaround (that someone might
> possibly find useful)
> https://gist.github.com/MInner/8b0c0a0e528303b132bf02e277199996
> 
> it's more or less clear (at least in simplest case) how to do
> A[:, b, :] # => (10, 50, 30) - transpose, gather, transpose back
> or
> A[a, b, c] # => (50, ) - pack, gather_nd
> , but it's not clear to me how could one do A[a, b, :] # => (50, 30)
> without creating super-large index matrix for gather_nd, though it's
> clear how to do (50, 50, 30)
> 
> A is float [10, 20, 30] in (0..1);
> a, b, c are ints [50,] in [0..10]
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-224765129,
> or mute the thread
> https://github.com/notifications/unsubscribe/ABtim2OtDT1jm1ulE1a0NYpSTv0AzN95ks5qJ1VlgaJpZM4GiDCf
> .
  Thank you for the suggestion.  In general, docker is the best approach for building a hermetic installation of TensorFlow and I encourage most heavy TensorFlow users to follow this approach.  In general, however, pip installs are easier for users who just want to play with TF.  And while I would encourage most such users to install in virtualenv, in practice it's always less friction to just gamble that there won't be any conflicts with system installed packages in a regular pip install.

All that to say is, [there's more than one way to skin a cat](http://www.phrases.org.uk/meanings/there-is-more-than-one-way-to-skin-a-cat.html).  Closing this for now since #149 is tracking building proper Docker images.

@timshephard and @cauerego PTAL at [this script](https://github.com/tensorflow/tensorflow/blob/41930f0b81b52a34fb56d921c9bad65c36168323/tensorflow/tools/docker/docker_run_gpu.sh) for details on how to forward GPU support into the docker container.  This has worked for us.
  http://tensorflow.org/get_started/basic_usage.md#variables  -- you might find that useful.

You can only assign to a tf.Variable -- that is the only mutable state you can use the assign ops with.

We'll see if we can come up with a better error message in the meantime.  Thanks for the report!
 @michaelisard: Assigning to you since presumably this will change as part of upcoming work. 
 Switching to @prb12: Should we mark this as contributions welcome or is something more elaborate than an error message change warranted?
  Thanks for the note!  We're working on improving our suite of indexing / assignment ops, and we'll definitely want a good tutorial once they're in place.  There's a tracking bug for the necessary functionality here: https://github.com/tensorflow/tensorflow/issues/206
 Closing as duplicate of #206.
  Thanks for the report! Indeed, it should point to http://tensorflow.org/tutorials/mnist/pros/index.md

We'll check in that fix, and it will appear on the website when it's next updated.
  What version of bazel are you using?  Is it the one suggested in the getting started guide?  Do you have gcc/g++ installed on your machine?  If none of these help, try running the build with --verbose-failures and report back.
 We're working on a way to automatically infer the location of the numpy headers.  The current locations are [here](https://github.com/tensorflow/tensorflow/blob/72a5a60dd4664a7caa4611344364ac7851464a60/tensorflow/python/BUILD#L698).
 I'm closing this for now unless you have any other issues we can help with.  I know the title is about having a Makefile, but the question seemed to be more related to bazel issues.  I don't think we have Makefile/CMakeFile creation on the horizon but feel free to reopen a new issue limiting the description to just that fact and we'll keep it in mind in future planning.
  I am not able to reproduce this unfortunately:

```
Epoch: 0001 cost= 29.860476766
time this epoch= 6.674506
Epoch: 0002 cost= 22.030540740
time this epoch= 3.98675
Epoch: 0003 cost= 21.065916606
...

time this epoch= 6.122648
Epoch: 0023 cost= 18.377196394
time this epoch= 6.448846
Epoch: 0024 cost= 18.326277539
time this epoch= 6.469679
Epoch: 0025 cost= 18.222360275
time this epoch= 6.436373

```

It's a bit noisy, but it doesn't look like it's consistently increasing.
 (Clearing out our open issues list: feel free to reopen if you still think there's a problem)
  Both of these are fixed in git, pending a push to the website, hopefully soon!
  Thanks - the padding documentation definitely needs modification. The padding scheme is not as simple as you mentioned though - it follows some legacy reason which involved some non-trivial padding computations. I'll update the documentation to add the details.

For the time being, if you are interested in the the exact math, it is described here:

https://github.com/Yangqing/caffe2/blob/master/caffe2/proto/caffe2_legacy.proto#L8
 Thanks again for reporting this. We have fixed this in our internal codebase and will be pushing it soon. Closing this for now.
 Hi,

Beginner questions are best suited for Stack Overflow, not random semi-related bugs. Please post your question there instead.
  I'm going to close this as a duplicate of #73, we can discuss translating to other language there.
  Apologies for the inconsistency.  We fixed the code to allow one-argument range, but the release hasn't changed and that's probably what you're using.  Good to know about the synchronization issue; we'll keep it in mind in future. 
 I'm going to fix this in git to be compatible back to 0.5.0.
 Fixed, will be part of the next git commit.
  Building from source is on our [installation page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md), but we haven't tested it for BSD so it may not work out of the box.
 Can you give the build a try on BSD, and if it fails, add the flag --verbose_errors and paste them?  Would love to see what issues you have.  Please also provide us with details about your setup (BSD type and version, architecture, etc.)
 What version of jdk do you have installed?  How did you install bazel?  Can you run the build with the flag --verbose_failures?
 @auroua looks like you're missing the python-dev package, and your question is unrelated to the one above.  if you continue to have problems please open a new issue instead of commenting in this one.

@yurivict sadly we don't have any FreeBSD boxes to play with so you're our guinea pig.  the problem stems from the "jdk" definition [here](https://github.com/bazelbuild/bazel/blob/687550660db90b007741e1f67a8092b08945f8e3/tools/jdk/BUILD) in the bazel code.  Do you have java/javac in your path?  If you do, can you try building bazel 0.1.0 on your machine and see if that solves the problem?
 We were using a version newer than the latest tagged version, which was extremely unlikely to be installed anywhere. To keep the installation simple, we packaged protobuf as a submodule. Assuming we find no more issues in protobuf that we have to fix at head, we can move to making protobuf a library dependency once all our fixes are in a reasonably common released version.
 Closing due to inactivity, related items are #1069, and possibly items that are bazel related.
 #2571 details how to compile TensorFlow on FreeBSD. Posting because this thread is near the top in search results.
  Solutions described in our [install instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md) at the bottom.  Let us know if that doesn't help!
  Yeah, bazel run syntax requires `--` to separate. 
 Drop the .py from the end of the bazel run command.
On Nov 13, 2015 11:49 PM, "mikeconnors909" notifications@github.com wrote:

> When I try to run this command:
> 
> bazel run -c opt tensorflow/models/rnn/translate/translate.py -- --
> data_dir ../data/translate/
> I get the following error:
> 
> ...................
> ERROR: Cannot run target //tensorflow/models/rnn/translate:translate.py: Not   executable.
> INFO: Elapsed time: 1.537s
> ERROR: Build failed. Not running target.
> 
> Any ideas how to resolve?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/191#issuecomment-156664411
> .
  Can you run the build with the flag --verbose_failures?  Perhaps this will give more info.
 Can you say what version of bazel you are using?  And try @SWu's advice and see if it solves the problem?
 Are you on a 32 bit system?
On Nov 13, 2015 5:40 PM, "xuezhisd" notifications@github.com wrote:

> I used bazel-master.
> 
> When I try to compile and install bazel 0.1.0, I also encounter some
> errors.
> I excute follow command,
> 
> [root@/home/share/TensorFlow/bazel-0.1.0]#./compile.sh build --verbose_failures
> 
> and I got follow error information.
> 
> INFO: You can skip this first step by providing a path to the bazel binary as second argument:
> INFO:    ./compile.sh build /path/to/bazel
> 🍃  Building Bazel from scratch............
> 🍃  Building Bazel with Bazel.
> .Extracting Bazel installation...
> Sending SIGTERM to previous Bazel server (pid=42474)... done.
> ........
> INFO: Found 1 target...
> INFO: From Linking src/main/native/libunix.so:
> /usr/bin/ld: bazel-out/local_linux-
> fastbuild/bin/src/main/native/_objs/libunix.so/src/main/native/unix_jni.pic.o: relocation
> R_X86_64_PC32 against symbol `_Z13PostExceptionP7JNIEnv_iRKSs' can not be used
>  when making a shared object; recompile with -fPIC
> /usr/bin/ld: final link failed: Bad value
> collect2: error: ld returned 1 exit status
> ERROR: /home/share/TensorFlow/bazel-0.1.0/src/main/native/BUILD:28:1: Linking of rule
> '//src/main/native:libunix.so' failed: gcc failed: error executing command /usr/bin/gcc
> -shared -o bazel-out/local_linux-fastbuild/bin/src/main/native/libunix.so -Wl,-whole-archive ...
> (remaining 16 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException:
> Process exited with status 1.
> ERROR: Sandboxed execution failed: //src/java_tools/buildjar:skylark-deps..
> ERROR: Sandboxed execution failed: //src/main/protobuf:proto_crosstool_config_srcjar..
> ERROR: Sandboxed execution failed: //src:android_tools_repository_zip..
> ERROR: Sandboxed execution failed: //src/main/java:shell-skylark..
> Target //src:bazel failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 6.436s, Critical Path: 2.87s
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/190#issuecomment-156605327
> .
 This issue is tracked by Bazel [here](https://github.com/bazelbuild/bazel/issues/359) where they offer two possible solutions.
  Let us know if you have any other issues.
  Yes, thanks for reporting this. I sent out a fix; it should be updated pretty soon.
 Fixed and pushed in https://github.com/tensorflow/tensorflow/commit/a9ca5173b2252b0de5dd754147b275e85298e522 
  https://github.com/tensorflow/tensorflow/commit/bf6b536bde7d8060c489b51fedb58968b8cbfd7c added the ability to configure the python paths -- as a consequence, we require users to now run ./configure before invoking bazel so they can specify the paths.

Let us know if this works for you!
  Hm, our doc links are sort of broken on the tensorflow.org website.  We'll fix it, but in the meantime: 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/state_ops.md#Saver
 new website update fixes this: http://www.tensorflow.org/api_docs/python/state_ops.html#Saver
  (This has been fixed already in our repo, pending a push to the website).  Thanks for reporting!
  Thanks for pointing this out! The root cause is related to the p_keep placeholder not having any shape information. By digging into this, we found that passing an unshaped tensor for the optional noise_shape argument also loses shape information. We're preparing a fix.
 This should be fixed in the latest commit https://github.com/tensorflow/tensorflow/commit/6b12d081d54b89869e26d9d99828f13de381761e
  Nope - at the moment TensorBoard only allows you to visualize and inspect existing models that were created with the Python API. Enabling the TensorBoard graph inspector to modify models would be extremely cool, and we've discussed the idea a bit, but it would also be extremely complicated to get it right, so we're not likely to prioritize it any time soon.
  Exercise for the reader: finding a configuration of light sources that actually produces that shadow :)
TL;DR: we know, and you can't unsee it once you notice. I myself like to think that we're disciples of the [Wrong Theory](http://www.wired.com/2014/09/wrong-theory/) of design, but the reality is that it is this way because the asymmetric shadow or the symmetric T just don't look as good.
  Our website has some incorrect code at the moment that we've already fixed in the repo -- if you can look at the corresponding docs in the git repository, it should be fixed.
  Yup, you are right -- should be sess.graph_def.  We'll try to fix this soon, thanks for the report!
 Fixed some time ago.
  Possibly dumb question: is there a need to sudo bazel build?  what if you remove the sudo ?
 Any chance this is related to https://github.com/tensorflow/tensorflow/issues/187 ? (are you using anaconda python?
 Hm, I'm not entirely sure what the answer is here.  https://github.com/BVLC/caffe/issues/1284 seems to suggest a similar problem, but the solution to 'reinstall numpy' doesn't really apply here.  Hopefully someone from the community can chime in and help.
 We look for the numpy headers here:

/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include

which is different from your setup.  Try changing [this line](https://github.com/tensorflow/tensorflow/blob/72a5a60dd4664a7caa4611344364ac7851464a60/tensorflow/python/BUILD#L23) accordingly.
  De-duping with 53
  Thanks, that's a documentation bug.  I'll fix it now.  As you point out, the number has to be known at graph construction time since it determines the topology.
 Fixed, will be pushed out in the next repo sync.  Thanks again for the catch!
  We are not working on it right now -- would be happy to have contributions though!
  I think some of those models aren't in our pip package yet -- building from source works for those.  We'll try to get around to fixing this soon, but for now building from source should work.
 The reason this won't using just the binary installed version of pip is because it relies on some additional includes not in the pip package (like reader.py in the same directory).  We didn't include the tutorials in the pip package because the package is mainly meant for modeling code.

As a hack, you could try setting your PYTHONPATH to the tensorflow/python/ subdirectory but that will probably break other things.

You can also move the python files in the ptb/ directory somewhere else, keep only the 'import tensorflow as tf' import and change the import of reader.py to 'import reader'.  That'll get you part of the way there.
 Yes, rnn and rnn_cell _are_ included in the pip package.

To build tensorflow from source, use:
  bazel build -c opt
without the --config=cuda flag.

You can use this to build and run the build_pip_package target.  The build_pip_package will then create a binary wheel without the cuda.

This is in the tutorials.
 Follow the instructions [here](http://www.tensorflow.org/get_started/os_setup.md) under the section "Create the pip package and install".  However, also pass in the flag "--config=cuda" as well as the current "-c opt" in order to build with CUDA.  Remember to run the configure script before running the bazel build.
 Ah sorry - you were not interested in having a cuda build.  Ignore my suggestion of adding "--config=cuda" and just follow the directions as they are.
  Can you upload the checkpoint file for inspection?
 It is looking for:

embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix

but your checkpoint contains:

embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix
and
embedding_attention_seq2seq/RNN/MultiRNNCell/Cell1/GRUCell/Gates/Linear/Matrix

basically the graph you loaded from has 3 stacked RNNs (looking for Cell2) but the checkpoint file was created with a graph trained with only 2 stacked RNNs (it has Cell0 and Cell1).
 Try padding the flag "--num_layers=2" to the second call.  My guess is the default value is 3.
 When you run decoding, you basically have to have the same parameters as when you trained.  For example, you forgot to add --size=64 to the second command, and the number 64 is one of the incompatible dimensions.  I bet if you look at the code, you'll see that the default value for size is 1024.
 A program _could_ try to pick up the parameters from the serialized model -- but for the tutorial we'll keep it as is for now.  Please re-open if there are any other issues here!
  Just to clarify what we do here:

Sigmoid input is x, output is y, target is z:

y = 1 / (1 + exp(-x))
1 - y = exp(-x) / (1 + exp(-x))

Logistic loss 
= -[z \* log(y) + (1-z) \* log(1-y)]
= z \* log(1 + exp(-x)) - (1-z) \* [-x - log(1 + exp(-x))]
= z \* log(1 + exp(-x)) + x + log(1 + exp(-x)) - z_x - z \* log(1 + exp(-x))
= x - z_x + log(1 + exp(-x))

To avoid overflow and branching in TensorFlow this is calculated as:

LogisticLoss = max(x, 0) - z*x + log(1 + exp(-abs(x)))
 It would be useful. Here's a  no-frills CL which adds a numeric op (squared difference of two tensors) - https://github.com/tensorflow/tensorflow/commit/bf536bcc888768b586440579b3fdeecf80a48acf It looks complicated because it works for multiple backends (CPU and GPU) and multiple front-ends (Python/Go/etc)
  Thanks for pointing this out. The looping was designed for LSTMs, where we may have >10 instances of the same section and displaying all of them is messy. In this case though, it's just confusing! I think we'll both disable looping for small numbers of repetitions, and also work on making the looped layout a bit easier to understand in general.
 This should be closed now. Won't link the commit since it was an early commit that bundled many commits together. Let us know if you run into this issue again. Thanks!
  The code attempts to download the data files from the MNIST web site, and assumes it's properly downloaded if the file is present locally on your system. You might have a corrupted file, in which case deleting it and retrying might help. Otherwise, try to get the data via your browser directly from:
http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
  Pushed to git in https://github.com/tensorflow/tensorflow/commit/6b12d081d54b89869e26d9d99828f13de381761e  -- thanks!
  Fixed by 1d76583411038767f673a0c96174c80eaf9ff42f.  Also see #1 .
  How important is it to be installed for you? That documentation is more for our websites / github.  We could include it if it was important...
 In general we avoid installing g3doc as part of the package because it is not part of the core functionality.  Similarly with the tutorials; they're kept out so that people don't have to dig around in dist-packages & site-packages for the files.  They are instead made available in the git repo.
 Sounds good, closing for now then.
  Only strides / padding are constants, input and filter can be dynamic sizes: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/nn.md#conv2d
  I added this some time last week to the source -- let me know if this still doesn't work at HEAD.  (will be in our next binary release)
  This is something we are actively working on, although it is in the early stages.  It is discussed briefly in the second-to-last paragraph of the Future Work section of the white paper at http://tensorflow.org/whitepaper2015.pdf:

"We also have a number of concrete directions to improve the performance of TensorFlow. One such direction is our initial work on a just-in-time compiler that can take a subgraph of a TensorFlow execution, perhaps with some runtime profiling information about the typical sizes and shapes of tensors, and can generate an optimized routine for this subgraph. This compiler will understand the semantics of perform a number of optimizations such as loop fusion, blocking and tiling for locality, specialization for particular shapes and sizes, etc."
  These are all things we are actively working on. We don't yet have a more aspirational longer term timeline yet, when we do, we will publish that as well. That may include things we know we want but are not yet working on. 

Where there are things that are useful for the community to take on, we have marked them with the [contributions welcome](https://github.com/tensorflow/tensorflow/labels/contributions%20welcome) tag. For instance, many issues relating to performance improvements are marked this way.
 In case of #22, We are coordinating with the people on that thread (and
others). It hasn't gotten beyond initial planning yet. If you are
considering contributing for anything, do comment on the associated issue
to find out the current state of affairs and to avoid duplicating effort.
The issues will be updated with major developments, but usually the actual
development happens outside the issue thread.

On Thu, Jan 14, 2016 at 9:27 AM bhack notifications@github.com wrote:

> @martinwicke https://github.com/martinwicke Ok thank you for the
> clarification. But e.g. #22
> https://github.com/tensorflow/tensorflow/issues/22 is tagged
> contributions welcome but it is also in the Roadmap. Is it tagged
> contributions welcome because is "long term" or do you have already started
> to design/code on it? I'm asking you this because the mean of my previous
> comment was to clarify, for a contributor, what make sense to start in his
> own fork to not conflict further with an emerging internal code developed
> on same target.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/162#issuecomment-171714012
> .
  Thanks for the help @yyttr3!
  --use_gpu seems unnecessary
--config=cuda is necessary: we've updated our git docs, our website hasn't been updated.  should be updated tomorrow though!
  Not at the moment -- you might try building from source for arm.  We don't really have a test infrastructure yet to make sure we keep it working, which is why we don't build the binary package for it.
 @petewarden: What's the status of this now? 
 I'm hoping to work on this over the next couple of weeks.
  As noted in our contributing.md, currently our changes have to go through gerrit (which I see you've done, and we'll try to integrate soon).  Closing for now, thanks!
  With the latest BFC allocator I think you mentioned this is no longer necessary.  We want to keep convolutional.py pretty simple -- if this is required for even smaller GPUs, we might want to just mention that in the file and point to an external example.  Thanks for looking into this and helping to verify the fix!
  As noted in our contributing.md, currently our changes have to go through gerrit.  Closing for now, thanks!
  Can you try to build the pip package from source following the instructions on tensorflow.org, install that, and report back?
 Closing due to inactivity -- reopen if still interested and the problem hasn't been fixed in the past few months.
  Re-open if CUDA_VISIBLE_DEVICES isn't good enough
 @zheng-xq: is it possible to query the status of the exclusive mode bit?
 @noisychannel, even if we can query which GPU is available, and by the time we actually try to assign the GPU, it could be taken by other processes. This is not very different from having a script query GPU status through nvidia-smi, and set the GPU id accordingly. 

The only way I can think of that makes your case easier is actually to create the context, and if it fails, ignore that failure and move on to the next. However, this is undesirable for many more common cases, where such context creation errors are signs of bigger problems. 
 This would work fine if the framework only wants to use a single GPU. TensorFlow is designed to use multiple GPU seamlessly at the same time. So at the initialization stage, it grabs all the visible devices that are compatible, since the client program might use all of them. The actual device used is only known at the graph construction/execution time, which is at a much later stage.

For most of our existing users, the GPUs that are available to a particular job is known when it starts. So it is okay for TensorFlow to take all of them. A few options to think about: 
1. Is it possible for the job scheduler to reserve the GPUs in your clusters? That would work best with TensorFlow. 
2. If you only know the list of candidate GPUs, and the number of GPUs to use, we can add a special mode where we try to create context in the candidate list, and stops if desired number of GPUs are taken. This requires more plumbing to both TensorFlow and the underlying Stream-Executor that actually manages GPUs. We would much prefer #1 if that is possible for you. 
 @noisychannel, please see if you can work around this issue at the moment. Meanwhile, I will investigate if we can officially support the soft-try-and-device-limit approach in TensorFlow. 
 @zheng-xq: Is this still an issue?
  We don't have a conv3d op yet, but there will eventually be!   We'll use this issue to track this feature request.
 https://github.com/tensorflow/tensorflow/commit/6a187ccddaebb741ea77fc3201c6e36625f0aadb added 3d convolution support, including the pooling ops.
  Hi @ruffsl -- just to ask, by "official" do you mean having an org on Docker hub?

If so, it's definitely something we want to do; in fact, I registered the Tensorflow org on docker hub -- I just didn't know how to experiment with moving over pre-release without accidentally making the org public. :wink: 

If you've done this before, I'd love help/pointers.
 - instructions for building the docker containers are in [the README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md).
- we can move the dockerfiles into their own repo if/when it's a problem -- right now, we're still making sure we've got syncing and external contribution flows in order, so no need to add extra complexity yet.
- sadly, the CUDA image still doesn't have all the libraries we need (cudnn isn't there).
 @ruffsl whoa, that cudnn issue being resolved is fantastic! thanks for doing the legwork there.

currently, i was thinking we'd do three collections of containers:

| Name | From | Contents |
| --- | --- | --- |
| tensorflow | ubuntu | base TF install, no deps (bazel, java, etc) |
| tensorflow-full | tensorflow | full source install, with all deps |
| tensorflow-full-gpu | tensorflow-full | same as tensorflow-full, but built with GPU support |

it's actually possible that we'd tweak the from lines, depending on how we build the containers (eg script vs. `Dockerfile`), but the net effect would be the same.

that said, I'm now curious: is multiple collections of tags the more common approach in the docker world?  in particular, would we also maintain `latest`, `latest-full`, and `latest-gpu`? (i feel like `latest` gets special treatment from docker, but `latest-gpu` and `latest-full` wouldn't, right?)
 Sorry for the lull here -- was waiting for the NVidia images to get pushed publicly, and now they're live! Adding in @ruffsl @ebrevdo @jendap for discussion as well.

I now see what you mean about multiple tags in the repo, which I think is what we want to go with. For better or worse, I think we want to go with four tags:

| Tag | From | Contents |
| --- | --- | --- |
| `latest`/`<release>` | `ubuntu:14.04` | minimal container with TF, no GPU support |
| `latest-gpu`/`<release>-gpu` | `cuda:cudnn-runtime` | minimal container with TF and GPU support |
| `devel`/`<release>-devel` | `ubuntu:14.04` | full image with all TF deps + source code, no GPU |
| `devel-gpu`/`<release>-devel-gpu` | `cuda:cudnn-devel` | full image with all TF deps + source + GPU support |
- We'll update these as new versions of TF get released, so that over time we'll end up with a number of older tags.
- My main motivation for the two sets of images is just size: I think the `latest` image can be slimmed down a bit, so it'll end up ~1/2 the size of the image with GPU support. For the "I just want to kick the tires" use case, this seems valuable.
- The naming scheme is also similar to the one for the CUDA images, except that we elide the `runtime` suffix.

Thoughts?
 OK -- I was waiting for the new release, which is imminent, so new images are pushed.

We now have `b.gcr.io/tensorflow/tensorflow:<tag>`, for `tag` all four variations mentioned above, as `latest` and `0.6.0` versions. I'm also busy uploading new containers to dockerhub.

I think we can close this for now, with a caveat that we need to come back in ~2 months and figure out if there are enough downloads that we want to submit a PR to become an official repo?
 @ruffsl yep, that's the one. 

so closing for now, but we can either reopen or create a new issue later.
  Thanks for the report -- our website generation is undergoing fixes / changes this week -- it is also quite behind our github documentation.  We'll close this bug once we've fixed these issues, thanks!
  The current release doesn't support multiple machines, but we're working on it. Please see #23 for more details. 

Closing as duplicate of #23.
  This is fixed at HEAD, hasn't yet been pushed to the website.  Thanks for the report!

De-duping with https://github.com/tensorflow/tensorflow/issues/78
  The string "<$url_to_binary.whl>" should be replaced with the correct URL; please see elsewhere in that document to see which version of the whl file is right for you.

For the second error, it looks like you already have virtualenv set up.
 That page lists it under OS X section:
https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl
  Cool, will update our docs today if I can, thanks!
 Fixed in a recent check-in, website will be updated soon.
  No, you are correct. Truncated BP works well for when whatever you are optimizing happens (roughly) in the currently unrolled window. With partial unrolling (compared to complete unrolling) you can do more updates over time, which usually improves convergence speed. Obviously you don't want to unroll for too few steps, as that will increase the error during back-propagation too much. For your problem I would recommend to fully unroll (just increase the num_steps to your maximum sequence length).  
  This is correct.  We currently do not support AWS GPU instances because of their 3.0 cuda compute capability.  You can try disabling that requirement in the source code and seeing if it works for you.
 zheng-xq is working on adding configurable support to it.

For every compute capability you add, the compile time and binary size increases significantly, so we're trying to find a solution.

In any case, thanks for the report -- de-duping with https://github.com/tensorflow/tensorflow/issues/25.
  Thanks, fixed this in our internal repo -- will push out the fix some time this week.
  This is a good question.  Our current suggestion is to pad your inputs to a fixed length, then slice them into reasonable frame count chunks (e.g. 50); and use truncated BPTT.  The PTB tutorial has an example of truncated BPTT, but not of padding.  However this is easy enough to do in python for now.

We are considering other ways to work with dynamic length sequences, but nothing is in the release as of now.  But even with purely dynamic RNNs you'll still probably have to pad if you want to minibatch.

A way of dealing with masking is to add a num_steps length list of [batch_size] weight vectors.  Then perform a distributed multiply this with the loss output, aggregate and make that the new loss.  Set weights past your sequence length for each minibatch entry to zero; these particular gradients won't be backpropagated.
 Thanks for the question!  These types of questions should be posted to the groups list (github issues are for bugs / installation issues).  https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss
  It looks like the problem arises because (i) `tf.nn.moments` relies on knowing the fully-defined shape for its argument, and (ii) the `x` argument that you passed to `tf.nn.moments` has one or more undefined dimensions.

This can arise when shape inference doesn't have enough information to infer all of the dimensions in your tensor. In this case, the best thing to do is the following:

``` python
x = ...
axes = [...]
x.set_shape([size_in_dim_0, ...])  # Each `size_in_dim_i` must be an integer.
x_moments = tf.nn.moments(x, axes)
```

If the size in any of those dimensions is dynamic, the current implementation of `tf.nn.moments()` will not work, but it is possible to add a custom version that does the trick:

``` python
def moments(x, axes, name=None):
  """Version of tf.nn.moments that supports variable-sized tensors.

  N.B. The rank must be known statically for this version to work.
  """
  with tf.op_scope([x], name, "moments"):
    x = tf.convert_to_tensor(x, name="x")
    divisor = tf.constant(1.0, dtype=x.dtype)
    x_dynamic_shape = tf.shape(x)
    for d in xrange(x.get_shape().ndims):
      if d in axes:
        divisor = divisor * x_dynamic_shape[d]
    divisor = tf.inv(divisor)
    mean = math_ops.mul(math_ops.reduce_sum(x, axes), divisor, name="mean")
    var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x - mean), axes),
                       divisor, name="variance")
    return mean, var
```

A version that works with dynamic rank tensors would also be possible, but it would be quite a bit more complicated.

It would also be possible to replace the `divisor` computation with a call to `tf.mean()`, but as the comment in [the implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L539) mentions, there are sum performance issues when using `tf.mean()` on GPU at present.
 Thanks for sharing your workaround! We're working on a fix that should address this in the core library.
 Should be fixed by @mrry in 1d76583411038767f673a0c96174c80eaf9ff42f, which I just pushed to HEAD.  Will be in the next binary release.
  Duplicate of https://github.com/tensorflow/tensorflow/issues/136
  If you don't have enough memory on your GPU to fit the whole test data, you could feed it in small batches to the eval graph using feed_dict like the example does with the training data.
 @mondatu: just to be clear, you're saying that just using BFC alone is enough to solve the mnist tutorial out-of-memory?  No need to do the batching in mnist?

(The default allocator and the BFC allocator do dynamic memory allocation but the default one is more prone to unrecoverable fragmentation -- BFC behaves better for more models).
 Cool, glad to know it worked!  We made the BFC allocator the default yesterday, so if you sync to head you probably don't need your additional modifications.  This will make it into the next binary release.  Thanks again!
 Sounds like this has been resolved.  Let us know if this issue comes back.
  I've also encountered the missing setup.py error with an older version of pip. After upgrading pip it went through fine.
 Great!  We added these instructions to our installation docs.  Hopefully it addresses most people's issues.
  Did this issue get resolved? A lot of things have changed with 0.6.0.
  No plans to use these by the Google team at this point. We'll definitely evaluate how our current communication channels are working over time and consider alternatives.
  :+1: we use `0.1.1` in the Docker builds, so I can confirm TF will build with it.
  David: should we wait for 0.1.1 before updating the docs, or just do this now?
 Okay, updated in https://github.com/tensorflow/tensorflow/commit/72a5a60dd4664a7caa4611344364ac7851464a60  -- thanks!
  There are unfortunately a lot more when compiling with --config=cuda -- we'll use this bug to track eliminating most of these warnings.
 We've eliminated most of these warnings.  signed/unsigned comparisons are always annoying and will likely creep up, but we'll address them individually as needed.
  Likely the culprit -- reopen if this was not the case!
  Tensorflow pip packages were compiled on Ubuntu machines. Can you try building the pip package from source and installing it?
  Yup, we're working on it.
  What version of bazel are you using?
 Try using 0.1.1 -- our bazel install instructions have been updated here: http://www.tensorflow.org/get_started/os_setup.html#installation-for-linux

(re-open if this is not sufficient -- this is most likely a bazel issue from what I understand)
  Closing this as a duplicate of #17 - please feel free to continue the discussion there.
  sess.graph is a python tf.Graph object, whereas SummaryWriter takes a tf.GraphDef (proto).  I think you can get further if you use sess.graph_def rather than sess.graph.  Let us know if that helps!
 Also, consider using StackOverflow for these kinds of help questions -- you'll probably get better community support there.

(See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#help--support--how-do-i- )
  Thanks for reporting this, I'm looking into it.
 Thanks @vrv for reviewing, we should be able to close this out now.
 Indeed, should be fixed by https://github.com/tensorflow/tensorflow/pull/337

Thanks for the fix!
  De-duping with https://github.com/tensorflow/tensorflow/issues/17
  D'oh. I think I was half asleep when I made that. Here's a fixed image:

![softmax-regression-scalarequation](https://cloud.githubusercontent.com/assets/61658/11082384/5f4fb1ca-87d9-11e5-93d6-4130585466b8.png)

The repo should be fixed shortly.
 colah just fixed this in f6d5a2caedd3982a1e6dcaa99bfafe0539527144.  Will take some time to push to the website though. Thanks for the report!
  Thanks for the report!  We have a pending fix this, will be updated soon.  De-duping with https://github.com/tensorflow/tensorflow/issues/78
  Hey Jeff - your implementation looks all right. I'll take a look at that tomorrow when I am in office. From the face of it, seems like the automatic device placement put some of the backward ops on the CPU for some reason.
 Hi Jeff - it turns out that there is an implicit memcpy going on when the session is run. Basically, when session.run(target) is called, we also fetch all the targets into numpy arrays (which is a feature that is not very well documented...). Since the gradients are on GPU and the numpy arrays are going to be on CPU, a memcpy is triggered that causes a nontrivial amount time.

The reason we started to see this performance hit after adding FC layers is - as one may expect - because FC layers. I've added a proposed change to the code for experimentation on your side.

I'll let the guys know and add these notes to the documentation. Thanks for digging into this!
 Yeah, I was surprised too, although with a follow-up discussion it seems that the control flow was needed there as well. Sent a fix to Soumith for that earlier today.

Thanks Jeff for initiating the investigation! I really appreciate it.
  Hey @delip, we currently don't have support for GPUs on Mac OS X.

Out of curiosity, what GPU are you trying to get working with TensorFlow?
 @delip: so do you also have AMD GPUs?
 Gotcha, so that would require OpenCL support, which we're tracking here: https://github.com/tensorflow/tensorflow/issues/22.  I'll de-dupe with that feature request.
  Since we depend on bazel, this sounds like a bazel issue.

Feel free to re-open if bazel ends up supporting 2.12 or lower, and we can see what we can do.
  I am trying to compile build_pip_package on Ubuntu 12.04 with gcc 4.8, but it gives an error during compilation:

```
$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures

tensorflow/python/client/tf_session_helper.cc:418:41: error: 'PyArray_SHAPE' was not declared in this scope
   dims.push_back(PyArray_SHAPE(array)[i]);

ERROR: /home/local/ANT/x/tensorflow/tensorflow/python/BUILD:666:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' failed: gcc failed: error executing command 
  (cd /home/local/ANT/x/.cache/bazel/_bazel_x/29843df2d2b24eaae7acd7ee881b7bec/tensorflow && \
  exec env - \
    INTERCEPT_LOCALLY_EXECUTABLE=1 \
    PATH=/home/local/ANT/x/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/bin/X11:/usr/games:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -I/usr/include/python2.7 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__="redacted"' '-D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.d -fPIC -c tensorflow/python/client/tf_session_helper.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command 
  (cd /home/local/ANT/x/.cache/bazel/_bazel_x/29843df2d2b24eaae7acd7ee881b7bec/tensorflow && \
  exec env - \
    INTERCEPT_LOCALLY_EXECUTABLE=1 \
    PATH=/home/local/ANT/x/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/bin/X11:/usr/games:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin:/home/local/ANT/x/bin \
  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -I/usr/include/python2.7 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__="redacted"' '-D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.d -fPIC -c tensorflow/python/client/tf_session_helper.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.223s, Critical Path: 2.46s
```
 I can compile that particular source file after adding this to the gcc command:

```
-I/usr/local/lib/python2.7/dist-packages/numpy/core/include/
```

But when I rerun the bazel command it tries to compile it again without the needed include path and fails.
 @markusdr: what version of numpy do you have installed, by any chance?  I have a suspicion that it's probably earlier than numpy 1.8.2 :)
 I believe it's numpy 1.9.2.
 Gotcha -- that version does have PyArray_SHAPE.  Do you have any other numpy directories installed on your machine that might be interfering somehow?

If not, can you try changing tf_session_helper.cc to switch from PyArray_SHAPE to PyArray_DIMS ? (That one is available in earlier versions of numpy and apparently PyArray_SHAPE is an alias)
 Right, the installation is using /usr/include/python2.7/, which doesn't have PyArray_SHAPE. 

But PyArray_SHAPE is defined in /usr/**local**/lib/python2.7/dist-packages/numpy/core/include.

How can I tell bazel to use the latter python installation path?
 When I add the correct numpy include path to the tensorflow/python/BUILD file it seems to compile, but at the end it complains:

```
$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package 
tensorflow/python/BUILD:666:1: in cc_library rule //tensorflow/python:tf_session_helper: 
The include path '/usr/local/lib/python2.7/dist-packages/numpy/core/include' references 
a path outside of the execution root..  
```
 OK, this workaround gets it to compile:

```
cd third_party
ln -s /usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy .
```

Then add `-Ithird_party` to the includes in `tensorflow/python/BUILD` and `tensorflow/tensorflow.bzl` 
 There may be a way to infer the numpy header  location at compile time. I'll take a look.
 @auroua Here is the diff:

```
diff --git a/tensorflow/python/BUILD b/tensorflow/python/BUILD
-    copts = numpy_macosx_include_dir + ["-I/usr/include/python2.7"],
+    copts = numpy_macosx_include_dir + ["-Ithird_party", "-I/usr/include/python2.7"],

diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl
-                    ] + ["-I/usr/include/python2.7"],
+                    ] + ["-Ithird_party", "-I/usr/include/python2.7"],
```

Alternatively, I could have symlinked `/usr/local/lib/python2.7/dist-packages/numpy/core/include/numpy` into `/usr/include/python2.7`, but I didn't have root access.
 If you sync to HEAD you should be able to use ./configure to select which python to use.
 I believe so:  ./configure will call python_config.sh (https://github.com/tensorflow/tensorflow/blob/master/configure#L25) which gets the numpy headers from https://github.com/tensorflow/tensorflow/blob/master/util/python/python_config.sh#L64
  Nice work @jimfleming!

"This comment in the TensorFlow source makes it sound like it’s optional but it’s not:" -- want to send us a change to fix it, or suggest an improvement?  Should we switch 'can' to 'must' ?
 If someone sends us a PR, I'd be very happy to merge it. 
 the shared library was added a while ago, yay!
  Likely a dupe of https://github.com/tensorflow/tensorflow/issues/56: let us know if it isn't (answer is to upgrade pip).  Re-open if that's not the case!
  From the stack trace, it seems likely that this is either:

1) A bazel problem
2) A connection problem

Can you validate that just plain 'bazel' is working properly?  If not, that's probably something to ask the bazel team.
 (Please re-open if you find this is TensorFlow related)
  From quick searches on that error message, it looks like this is Alpine specific: alpine seems to use a different c library than glibc, which we probably don't support. :(
  Thanks for the PR!  I think this was independently fixed in a commit earlier this week -- we appreciate the effort.
  Yes, sorry, this will soon be fixed.
 Fixed in a recent commit, thanks!
  Hi @Botev, it appears as if this is a bazel compile issue, I would recommend following up with at the bazel issues page, since they are better suited to help you :)

(Feel free to re-open if you think this is TensorFlow specific)
 Great, please let us know!  We can update our documentation if this ends up being a common problem.  Thanks!
  You can also install in a virtualenv; six will then be installed locally.
 Thank you for this suggestion.  We are not currently accepting pull requests via github.  We will update the [common problems](http://tensorflow.org/get_started/os_setup.md#common_install_problems) section of our Get Started guide appropriately.
  Hi there, we've seen quite a few configurations where it's been difficult to upgrade the `six` library shipped with OS X to the appropriate version. The most successful approach so far has been to use [HomeBrew](http://brew.sh/), and `brew install python` to get a clean Python setup.

Alternatively, some people have had luck using `sudo easy_install -U six` to upgrade `six` before installing TensorFlow.

Can you try one of these approaches and let us know if it works? Thanks!
 (Please re-open if that does not solve the problem, or if our documentation here does not help: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#common_install_problems)
  Closing -- looks like these issues are resolved.  Please re-open if there's something else to do here.
  Can you provide more information about your machine, version of python, etc?
 The output of the following would be helpful:

``` bash
uname -a
```

Our built pip packages are only for 64-bit platforms: I suspect you're either on a 32-bit platform or you're trying to install the linux package on a non-linux machine.
 As far as I can tell, we do not support 32-bit machines at all.
 I updated the docs to mention Linux 64-bit -- updated in git repo and website should be updated today.
  I believe the problem is a bit more complicated once you introduce memory constraints and scheduling :).  Regardless, this type of question is probably better suited to the [tensorflow discussion mailing list](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss) rather than a github issue.  

Thanks for your interest!
  We hear you all -- the current workflow is not ideal and is a pain for us too.  There are several reasons why we can't just switch solely to github today, but since it _is_ a pain for us, we're motivated to try to make things better :).

We'll keep this bug open to track the request.
 De-duping with https://github.com/tensorflow/tensorflow/issues/26
  Please follow:
https://github.com/tensorflow/tensorflow/issues/5
https://github.com/tensorflow/tensorflow/issues/3
  Our current suggestion for dealing with incompatible versions of six is to either use a brew install of python, or install the tensorflow package inside a virtualenv.  The virtualenv will guarantee that you install and see the correct version of six.  See [common problems](http://tensorflow.org/get_started/os_setup.md#common_install_problems)
  Yeah, sorry about that.  We're working on the better contribution flow.  On the bright side, we already had a few bug reports about this and we've already fixed it -- we'll be pushing out the fix later today.
  We're working on making more kernels runnable on GPUs. In the code, you can tell which kernels support GPU by going to the core/kernels directory to see which ops have .cu.cc cuda files. If you want to help out, pick a CPU only kernel and write a GPU capable version either by using Eigen (for example, see the argmax or the adjust_contrast kernels), or write a plain cuda kernel (for example, see the check_numerics_op).

In order to avoid duplicating work, I would create an issue called "XX_op does not have a GPU kernel", and then leave a comment that you're starting work on this. Then we won't do that internally without checking in with you first.

Finally, when you have something written, you have to submit a patch through tensorflow.googlesource.com (instructions are [here](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md)). 
 This is too broad of a request, finer grained issues for individual ops are probably better.
 Please file finer grained issues. It's clear that we need more GPU ops, but we'd love to understand which ones people are actually running into.
  We have no plans to break up tensorflow, or separate out components for independent distribution. 
  Closing as duplicate
  This error often happens if you open a python shell in the base bazel directory.  The directory "tensorflow" in there confuses python.  Try running python from another directory; let us know if `import tensorflow` does not work then.
  For a temporary fix, remove the "lib/analytics.js" line from the top of app/index.html and `gulp vulcanize` will work fine. That said, I haven't actually tested the open-source build of the TensorBoard frontend so you might run into other problems. I'll test it out and comment back once I have a first class solution on the way.
 Note by the way, you don't need to use gulp at all, unless you are recompiling the TensorBoard frontend from source.
 glad to hear it :)
 We've fixed this :)
  Can you do the following:

Build //tensorflow/models/mnist:convolutional via:

bazel build -c dbg //tensorflow/models/mnist:convolutional

And run with gdb:

(assuming your python is in /usr/bin/python2.7)
gdb --args /usr/bin/python2.7 bazel-bin/tensorflow/models/mnist/convolutional

(press "r" to run it)

wait until it crashes; and type "bt" for backtrace

paste the resulting output here.
 I believe this is fixed as of 9c3043ff3bf31a6a81810b4ce9e87ef936f1f529, will be in our next binary release
  Thanks for the report -- we'll update the image soon and let you know when this is fixed!
 Fixed in f6d5a2caedd3982a1e6dcaa99bfafe0539527144 by colah.  Thanks!
 (will take some time to propagate to the website though)
  De-duping with https://github.com/tensorflow/tensorflow/issues/17 to keep the discussion in one place :)
  Please see if our detailed installation instructions page helps:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#common_install_problems

On Mon, Nov 9, 2015 at 9:09 PM, Uday Singh notifications@github.com wrote:

> Currently this is my output after installing Tensorflow. I thought it
> installed six as well, but was wondering why copyreg doesn't exist in six.
> Anyone know what to do?
> 
> tensorflow % python
> 
> Python 2.7.6 (default, Sep 9 2014, 15:04:36)
> [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
> Type "help", "copyright", "credits" or "license" for more information.
> 
> import tensorflow as tf
> Traceback (most recent call last):
> File "", line 1, in
> File "/Library/Python/2.7/site-packages/tensorflow/_init_.py", line 4, in
> from tensorflow.python import *
> File "/Library/Python/2.7/site-packages/tensorflow/python/_init_.py",
> line 13, in
> from tensorflow.core.framework.graph_pb2 import *
> File
> "/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py",
> line 8, in
> from google.protobuf import reflection as _reflection
> File "/Library/Python/2.7/site-packages/google/protobuf/reflection.py",
> line 58, in
> from google.protobuf.internal import python_message as message_impl
> File
> "/Library/Python/2.7/site-packages/google/protobuf/internal/python_message.py",
> line 59, in
> import six.moves.copyreg as copyreg
> ImportError: No module named copyreg
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/75.
  Yes, the intention was to pick a size larger, but not too much larger (for performance reasons), than the 224x224 input of the Inception model bundled with the demo. There's a fix incoming that will pick a size dynamically.
  We may internationalize part of the website, but almost certainly not the API docs. The reason is simply that we wouldn't be able to keep the translated documentation up to date with the code, so it would be of very limited use.
  Can you try again with `--verbose_failures` and paste in any more information that displays about the failure? Thanks!
 De-duping with https://github.com/tensorflow/tensorflow/issues/187
  I suspect the GPU is a red herring. Without more info, the fact that /bin/bash can't seem to be able to execute standard tools like 'touch', 'tail' or 'sed' suggests something is very weird about your system.
 After reinstalling your OS, are you able to run the commands `/usr/bin/touch`, `/usr/bin/tail` and `/usr/bin/sed` from the Terminal? If so, this step in the installation should succeed.

To clarify: **it is possible** to build TensorFlow from source without a GPU.
   Apparently this is working as intended - in general models should be sourced from the repo, not the pip package.
 So are our instructions incorrect somewhere?  We should update them if that's the case.
 People get confused because build_pip_package imports models/rnn/{rnn,rnn_cell,seq2seq}.  If/when we move these into python.ops.nn  (e.g. something like nn.rnn, nn.rnn_cell, nn.seq2seq); that should fix some of the confusion.
 Agreed.  Until we are ready to support those interfaces indefinitely, is there something we should do to avoid that confusion?  Should we remove models/rnn from the pip package?
 No, they are the proper rnn/lstm implementation and should be made
available with the rest of the ops.
On Nov 10, 2015 1:08 PM, "Vijay Vasudevan" notifications@github.com wrote:

> Agreed. Until we are ready to support those interfaces indefinitely, is
> there something we should do to avoid that confusion? Should we remove
> models/rnn from the pip package?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/69#issuecomment-155566092
> .
  Sure, typo fix suggestions welcome!  We'll get in a fix to this documentation soon.  Thanks for the report!
 Fixed in b4629c0e9f2b2bfab50ed7a705763421950234d4, Thanks!
  I'm getting the following error when trying to build from source:

```
$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
ERROR: /home/name/tmp/tensorflow/tensorflow/core/platform/default/build_config/BUILD:36:1: no such package '@jpeg_archive//': Error downloading from http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/name/.cache/bazel/_bazel_mddreyer/44409223966f9cb8e480014c7f376bcc/external/jpeg_archive: Error downloading http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/name/.cache/bazel/_bazel_mddreyer/44409223966f9cb8e480014c7f376bcc/external/jpeg_archive/jpegsrc.v9a.tar.gz: Server returned HTTP response code: 403 for URL: http://www.ijg.org/files/jpegsrc.v9a.tar.gz and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.
ERROR: Loading failed; build aborted.
```
 Is it possible that this was a transient issue when fetching http://www.ijg.org/files/jpegsrc.v9a.tar.gz? I was able to fetch it using wget on my laptop.

If you fetch that URL using wget, do you also get a 403 error?
 I tried it multiple times but it didn't work. My coworker had the same problem. But downloading jpegsrc.v9a.tar.gz using wget and copying it to /home/name/.cache/bazel/_bazel_... was OK as a workaround.
 This seems more like a bazel issue -- closing for now.  Please re-open if you think we need to fix this differently in our BUILD files!
  There isn't actually an error, that's just an ugly warning in Eigen :).  It looks like it was succesfully built!

``` bash

Target //tensorflow/cc:tutorials_example_trainer up-to-date:
bazel-bin/tensorflow/cc/tutorials_example_trainer
```
 (Feel free to re-open if you disagree)
   Uninstalling six-1.4.1:
Exception:
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/basecommand.py", line 211, in main
    status = self.run(options, args)
  File "/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/commands/install.py", line 311, in run
    root=options.root_path,
  File "/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_set.py", line 640, in install
    requirement.uninstall(auto_confirm=True)
  File "/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_install.py", line 716, in uninstall
    paths_to_remove.remove(auto_confirm)
  File "/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_uninstall.py", line 125, in remove
    renames(path, new_path)
  File "/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/utils/**init**.py", line 315, in renames
    shutil.move(old, new)
  File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py", line 302, in move
    copy2(src, real_dst)
  File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py", line 131, in copy2
    copystat(src, dst)
  File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py", line 103, in copystat
    os.chflags(dst, st.st_flags)
OSError: [Errno 1] Operation not permitted: '/tmp/pip-XcfgD6-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'
 I'm not sure exactly what command you're running, but you'll probably need administrator privileges to uninstall from that directory. Try running the same command again prefixed by `sudo`.
 Also: see https://github.com/tensorflow/tensorflow/issues/11 for common OS X installation issues.

Edit: sorry, I meant https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#on-macosx-
 sudo -H pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl
 it relates to uninstalling six 1.4.1
 I install python via brew and now it works.

So we can say that it does not work with the default python on MacOS 10.11.1 (15B42)
 Closing for now as there seems to be some resolution here (though using an esoteric solution).
  I think TensorFlow should probably handle this underneath rather than giving control over to the user (which would permit user-caused bugs).  We probably don't do the most optimal things today but over time we hope to evolve to automatically do this.
  For this to work, you'll have to install the PIP package. You can either download the pre-built package, or build it from source by following the instructions here: http://tensorflow.org/get_started/os_setup.md#create-pip

If you've already installed the package, there might be some PYTHONPATH confusion due to your current working directory containing a directory called "tensorflow" without all of the generated protobuf files. One solution would be to `cd` into `tensorflow/models/image/mnist` and run `python ./convolutional,py` from there.
 Yeah, this might be yet another instance of https://github.com/tensorflow/tensorflow/issues/51

Given how often this is happening, I wonder if there's a better solution ...
 @alexryan: It looks like the TensorFlow Python code is being loaded from the directory `/Users/alexryan/projects/machineLearning/tensorFlow/clone/tensorflow/`, which is an unusual path for the PIP installer to use. Did you also clone the git repository and add that directory to your PATH or PYTHONPATH environment variables? If so, try removing it - the problem arises because it tries to load from the unbuilt project source, which doesn't include the generated code for the protocol buffers (e.g. graph_pb2.py). If you have also installed it using PIP, it should be in your path already.
  Hi @ptarjan, as mentioned in our [Contribution doc](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md), we don't accept pull requests through github yet, and our contribution flow through googlesource.com isn't that easy yet -- for a change like this, would you mind if we fix internally and push the change out for you?
 Nice :).  Yeah, we'll look into automatically converting the pull requests, and in the meantime we'll address this internally first.  Thanks for the feedback!
 Thanks @mrdrozdov.  Closing for now -- feel free to send through gerrit in the mean-time.
  hi @jonathanmalkin: if you fill in the path to the .whl file for the platform you are building, it should work.

e.g., if you were installing for OS X, you should do:

``` bash
pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl
```
  Likely a dupe of https://github.com/tensorflow/tensorflow/issues/11 -- see the solutions near the very end to see if they help.  Thanks!
 It's probably something to do with the path -- can you verify that there are no other protobuf libraries on your system that might be interfering?  This problem only occurs when it's using the proto2 version of the python libraries: our python generated files use python3.

To help debug, maybe also try to print the version of the protobuf library when in python?

``` python
import google.protobuf
print google.protobuf.__version__
```

or something like that?
  I'm getting the following error when trying to install on a GPU desktop using pip as described in the [tensorflow setup page](http://tensorflow.org/get_started/os_setup.md):

```
$ sudo pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
  Downloading tensorflow-0.5.0-cp27-none-linux_x86_64.whl (50.5Mb): 50.5Mb downloaded
  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
    Traceback (most recent call last):
      File "<string>", line 14, in <module>
    IOError: [Errno 2] No such file or directory: '/tmp/pip-kYFqUa-build/setup.py'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
  File "<string>", line 14, in <module>
IOError: [Errno 2] No such file or directory: '/tmp/pip-kYFqUa-build/setup.py'
```

See [this Stackoverflow page](http://stackoverflow.com/questions/2204811/pip-install-a-python-package-without-a-setup-py-file) that discusses the problem and indicates that a `setup.py` file is missing in the package.
 Upgrading pip to the latest version has solved this issue.
 @markusdr: do you happen to know which version you had and which version you upgraded to?  We'd love to update our docs to indicate a minimum pip version.
 It was pip 1.0 on Ubuntu 12.04. I upgraded to 7.1.2 and it worked.
  Up to you -- we've fixed it internally and will push it out to the git repo soon :).  Thanks for the typo fix!
  De-duping with https://github.com/tensorflow/tensorflow/issues/20
  I think we do require glibc >2.17 at the moment. Closing for now but if someone has a solution for earlier versions, would be great to have it here.  Thanks!
  Fixed in the source in https://github.com/tensorflow/tensorflow/commit/41930f0b81b52a34fb56d921c9bad65c36168323.  Will be also pushed out in the next binary release.
  Are you running python inside the tensorflow source directory?  If the directory 'tensorflow' is in your pwd, it may break imports.  Try running python outside the source tree.

If that's not it, please give some info about your system, version of python, etc.
  As you have rightfully guessed, the core TensorFlow team doesn't have plans for Ruby on the horizon, so if you want to contribute it, please go ahead! Submitting drafts for frontends such as this on the discuss mailing list early on is probably a good idea.
  I think so - tensorflow requires protobuf 3. Issue #11 may be helpful.
 Closing as duplicate of 11 to keep discussion to one issue.  Thanks!
  We currently don't have any plans to provide a 'canned' ImageNet object detector as part of the TensorFlow distribution, although I hope that researchers will soon build their own object detection systems on top of it. Such model is unlikely to become part of the core codebase however, so I'll close this issue.
  Oops, sorry.
  Yes -- thanks @mrob27 for figuring out the right fix; we've updated the docs and these instructions will be in the next push.

(Closing, reopen if needed.)
  Hey ne0shell: we require a newer version of six than the one default installed on Macs.  Take a look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#on-macosx- for some info on how you might get around this issue and please let me know if your problems are addressed :)
 Or do `sudo easy_install -U six` to upgrade the system-wide version of six.
 Actually, if you compare the tracebacks, this is progress: six isn't an issue, and now you're hitting issues trying to uninstall numpy.

If you want to do a global install, you'll probably need to do a `sudo pip install ...`.
 Yeah, I think the `sudo` bit was the key here -- I suspect just adding `sudo` in the initial install would have worked.

In any event, happy it's working -- closing for now, reopen if you hit trouble.
 Sorry, I was looking at too many issues at once. 

That said, it looks like you're up and running?
  Hi there,

Can you check whether the `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION` environment variable is set in your shell? It looks like it might be set to `"cpp"`, which is causing an issue with the version of protobuf that ships with the binary distribution.

The following should work:

``` bash
export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
python
>>> import tensorflow as tf
>>>
```
  Merging this discussion with https://github.com/tensorflow/tensorflow/issues/17 to keep all the good info in one place.
  Hi jimfleming, please see: https://github.com/tensorflow/tensorflow/issues/6 for tracking this issue.  Thanks!
  Hi Cristian, thanks for the report!  We fixed this in https://github.com/tensorflow/tensorflow/commit/db0b5da485e1d1f23003ee08ed2e191451ee0319, hasn't yet been pushed to our website, which should happen soon :).  Please re-open if this hasn't been fixed on the site in a day or two.  Thanks!
  Yup! De-duping with https://github.com/tensorflow/tensorflow/issues/1.  Thanks!
 Yup, de-duping with https://github.com/tensorflow/tensorflow/issues/1.  Thanks!
  Hey webmaven: as mentioned in our [Contribution doc](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md), we don't accept pull requests through github yet.  However, if you don't mind, I will make these edits internally and update the repository on our next upstream push with credit given to you.
  This is something the core TensorFlow team is unlikely to tackle in the near future, so if you want to contribute it, please go ahead! I would recommend circulating a proposed implementation on the discuss mailing list early on, so that a consensus about where such API might live (in repo / off repo / in 'contrib' directory) can be reached ahead of time.
 @peterbraden sorry for the prolonged silence. We are building out the C++ API, and it will grow over time. I expect the most useful bits to be the parts that are needed to run an existing graph. The C++ graph building API is being redone right now, so it's not particularly useful to spend much time on it.

I'd love it if you prepared a PR to put these bindings into contrib/nodejs.
  For this to work, you'll have to install the PIP package. You can either download the pre-built package, or build from source by following the instructions here: http://tensorflow.org/get_started/os_setup.md#create-pip
 Can you try the following?

``` bash
export LC_ALL=en_us.UTF-8
export LANG=en_us.UTF-8
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```
  A few questions:
- How did you install TensorFlow?
- What directory are you running that command in?
- What happens if you open a python interpreter and type `import tensorflow as tf`?
 Closing this for now -- looks like the issue is resolved
  hey all -- it looks like we picked up a permissions problem. looking now.
 Should be fixed now.
 I'm closing, but feel free to ping/re-open if you spot this again.
 @mmolaro Actually, GCR is built to support public images (like this one), as well as serve more core infrastructure needs ("I want to serve images to other parts of my cluster"). But I'm looking into getting us on dockerhub, too.
 @maadcodr I can't seem to repro this -- let's move to a new issue, and include some details about your setup (at least OS, docker version)?
  Hi there! We've only released binaries for (and tested TensorFlow with) 64-bit platforms so far. You might have more luck building from sources: http://tensorflow.org/get_started/os_setup.md#installing_from_sources
  Thank you for inquiring.  We are working on an implementation.  It will be released if/when we are happy with its performance, API, and documentation.
 Just an update on timelines: we're getting closer but it won't happen before January sometime at the earliest.
 We're closer. Still cleaning up code for release.
On Jan 15, 2016 6:41 AM, "qingzew" notifications@github.com wrote:

> what's the timeline now @ebrevdo https://github.com/ebrevdo
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/32#issuecomment-171977442
> .
 Coming very soon (I hope).
On Feb 22, 2016 5:34 AM, "bmilde" notifications@github.com wrote:

> Also highly interested in this and I hope that you can release your
> implementation soon! @ebrevdo https://github.com/ebrevdo
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/32#issuecomment-187175302
> .
 The CTC loss and two decoders (greedy & beam search for CTC) should be in the next push.
 (it'll be accessible via tf.contrib.ctc.ctc_loss etc.)
 https://github.com/tensorflow/tensorflow/commit/8509f88ade3fc65f4cc530780d06100f0b1ea108

ctc_loss and friends is now part of contrib directory -- once it matures and we fix any issues, we'll add it to the core.
  Is there any Slack channel connected to Tensorflow that you could access to get help?
 We have no plans to maintain an official one so far.
 Since it's not open for the public, I think it should either be an official engagement from the maintainers. Or another "official" slack-channel for unofficials (aspiring TensorFlowers)

but i will try to connect their! thanks @sdgandhi.
 Wow! Thanks @sdgandhi . Il close this for now @vincentvanhoucke 
 @sdgandhi I opened it!
 I think if it's unofficial, it doesn't make sense to add it to our official documentation.
  Hey, thanks! De-duping https://github.com/tensorflow/tensorflow/issues/10 for concentrating discussion about the Go API, so we can coordinate effort on it if needed.
  Thanks!  De-duping with https://github.com/tensorflow/tensorflow/issues/25
  Thanks for the feedback -- this is tracked in https://github.com/tensorflow/tensorflow/issues/22
  Hi delip, we fixed this in https://github.com/tensorflow/tensorflow/commit/430a054d6134f00e5188906bc4080fb7c5035ad5 -- when we mint our next version of the pip binaries, this should be fixed.   Thanks for your report!
  Thanks!  We'll use this bug to track the improvements to the contribution process, which we are working on improving.
 You're correct. We're working on the process. It'll get better.

On Sat, Dec 12, 2015 at 1:57 AM bhack notifications@github.com wrote:

> @vrv https://github.com/vrv github history/log is full of "TensorFlow:
> Merging changes from internal". Not very readable.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/26#issuecomment-164135615
> .
 @bhack: fixed our scripts to have more informative / separate commits: https://github.com/tensorflow/tensorflow/commits/master

Hope that helps a bit.
 @bhack: other projects probably have a more natural relation with github because they only have one source of truth.  We have git, and we have our internal repository that we also contribute to actively, so the merge commit is unavoidable when trying to merge two different lineages.

That being said, now that we've moved to individual commits, the merge commit shouldn't be that bad.  And it's basically the same as the merge commit from a pull request on github.
 We are looking at that plugin as one potential option.

The last commit will always be a 'merge commit' message.  https://github.com/BVLC/caffe/commit/79539180edc73d3716149fb28e07c67a45896be1 is the same, except we don't have a github pull request to reference, since the source isn't from github.  Is there a better 'merge commit' message you would like?
 @vrv: Should we just include the two commits hashes that were merged in the title? 
 The merge commit is merging a series of commits from our internal repository.  https://github.com/tensorflow/tensorflow/commit/8b5d9ed13f188662dde7cce75362cd2394bde40c is just the merge of the four previous commits, which each individually have the descriptions and change history you want.

For now, I'd suggest just skipping the "Merge commits from internal" commits when looking through the history.  Maybe soon we can just have the merge commit message identify which commit hashes it merged so it's more easily linked.
 There is the equivalent of potentially many unrelated PRs in any such
merge, that's why it's hard to find a decent description for the sum of
them.
On Sat, Dec 19, 2015 at 10:42 bhack notifications@github.com wrote:

> Yes what I mean is if that upstream "gerrit commits" merges are
> assimilable to a PR as conceptual description. If so probably you could use
> this concept in the merge description. I.e. many project ask to squash
> commits on PRs before mergining. But if that are sparse uncorrelated commit
> I think not description make sense.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/26#issuecomment-166013282
> .
 We've switched to github and there are ongoing improvements to automated testing, etc.  But the basics have been improved, so closing this catch-all bug.
  @infojunkie I applied your fix, but I got lots of nan's in the computation output:

```
$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
000006/000003 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000004/000003 lambda = 2.000027 x = [79795.101562 -39896.468750] y = [159592.375000 -79795.101562]
000005/000006 lambda = 2.000054 x = [39896.468750 -19947.152344] y = [79795.101562 -39896.468750]
000001/000007 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000002/000003 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000009/000008 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000004/000004 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000001/000005 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000006/000007 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000003/000006 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
000006/000006 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]
```
 Yes, I was using Cuda 7.0 and Cudnn 6.5 on an EC2 g2.2xlarge instance with this AIM:
cuda_7 - ami-12fd8178
ubuntu 14.04, gcc 4.8, cuda 7.0, atlas, and opencv.
To build, I followed the instructions on tensorflow.org.
 The convolutional.py example ran out of GPU memory for me too, on a GeForce GTX 780 Ti.
 @infojunkie: yes, this isn't ideal (@zheng-xq and I discussed this a bit during the review!).

We'll try to think of a better way to handle this, though we would like to keep the ability for the runtime device filtering to be in sync with the way the binary was built (hence needing to edit the source code for both compile and runtime).  Otherwise users get hard-to-debug errors.

We'll continue to work on making this easier, but hopefully this allows some forward progress for you.
 the __ldg primitive only exists for 3.5+ I think.  We have an internal fix to support both that we'll try to push out soon.

See https://github.com/tensorflow/tensorflow/issues/320 for more details
 I'll close this bug. Please open a new one with a more specific title if some issues in here remain unresolved.
 I'm not sure whether we should call TF_UNOFFICIAL_\* "not a hack", but yes, it _should_ work. If it doesn't, it's likely unrelated to Cuda 3.0 per se, and we should have a more specific bug.
 I don't know. But if that's still an issue with 0.6.0, it should be a bug, just a more specific one about multiple GPUs.
 @Dringite, you can enable Cuda 3.0 using the following: 

TF_UNOFFICIAL_SETTING=1 ./configure

It should be functional. And if it doesn't, feel free to file another issue to track it. 
  For now, you can build your own pip package via instructions [here](http://tensorflow.org/get_started/os_setup.md); see "Create the pip package and install".  Let us know if that doesn't work.
 Please re-open if there's something left to diagnose here!
  Thanks for the question!  To reiterate what I said [here](https://github.com/tensorflow/tensorflow/issues/12#issuecomment-155150681), we are working on making a distributed implementation available, it's currently not in the initial release.  Please stay tuned, and take a look at the cifar multi-gpu tutorial for a flavor of how we handle multiple 'devices': http://tensorflow.org/tutorials/deep_cnn/index.md
 Our current internal distributed extensions are somewhat entangled with Google internal infrastructure, which is why we released the single-machine version first. The code is not yet in GitHub, because it has dependencies on other parts of the Google code base at the moment, most of which have been trimmed, but there are some remaining ones.

We realize that distributed support is really important, and it's one of the top features we're prioritizing at the moment.
 We've started work on this using gRPC. We hopefully have something to show soon.
 I just pushed an initial version of the distributed runtime, based on gRPC. Currently, `tensorflow/core/distributed_runtime` contains most of the C++ implementation needed to get started, and we're still working on the higher-level libraries that will use multiple processes. However, you can get started today by taking a look at the [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/README.md).
 Heh, looks like merging another code base can inadvertently close issues :). Reopening this, because there's still work to do.
 E.g. in this paper arxiv.org/abs/1602.02410 we trained LSTMs that have 200M+ parameters and 3B+ total parameters in some cases.
 @kanwar2preet: Can you open another issue with details of (i) the TensorFlow program that you ran, and (ii) the configuration you used for the servers? Thanks!
 @jramapuram: Thanks for pointing this out. We have a fix to the docs coming later today. (Also, these interfaces are subject to a little bit of churn before they freeze in the next release, so please bear with us!)

The segfault is rather embarrassing, so we should fix that. Can you please create an issue with the exact code that reproduces it?

Regarding Ansible, I don't have any experience with that platform, but we would be glad to accept contributions... feel free to open another issue to suggest that. (https://github.com/tensorflow/tensorflow/issues/1686 suggests adding support for Slurm, for example.)
 I just wanted to draw everyone's attention to https://github.com/tensorflow/tensorflow/commit/6d838744c43ccaaa821bfcdfb8e2b1d39fa21f45, which modifies the interface to some of the distributed runtime methods. In particular, `tf.GrpcServer` becomes `tf.train.Server`, and its constructor is more ergonomic, so you no longer need to construct a (now-renamed-to) `tf.train.ServerDef` proto to instantiate a server. The [docs in the repository](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md) are now updated, but haven't yet made it onto the website.

Let me know if there are any questions!
 Since 0.8 is now released, I think it's time to close this issue. Please create new issues for anything that arises with the distributed version, and thanks for all of your input!
  At the very least, the [Eigen](http://eigen.tuxfamily.org) library would have to support OpenCL.
 @benoitsteiner knows more about interested parties that may not have shown
up in this thread (or this issue). I'd wait for him to coordinate to make
sure we avoid duplicating work.

On Tue, Jan 19, 2016 at 11:42 AM Dan McLaughlin notifications@github.com
wrote:

> I just assumed Benoit because he self assigned the feature, but I think
> you've got it Junli! Maybe start with an email or forum thread of
> interested parties?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/22#issuecomment-172963537
> .
 My apologies for not contributing more to this discussion recently, my plate has been more than full these past 2 weeks.

I'll be coordinating the OpenCL effort on the TensorFlow side. Our current thinking is:
- TensorFlow relies on c++11 and has taken a "single source" approach, so SYCL seems like a great fit.
- We don't have a lot of OpenCL experience in house, so we're collaborating closely with Codeplay to bridge this gap. In particular, Codeplay is currently leading the effort to add support for SYCL to the Eigen tensor library.
- TensorFlow relies on the cuDNN library to compute convolutions on NVidia GPUs. If somebody is interested in contributing an OpenCL equivalent, we'd be happy to help.

In order to help structure the effort, I created a mailing list: tensorflow-opencl@googlegroups.com.
 @karlrupp, @bhack The tensorflow approach is to rely on a hardware abstraction (the tensor module) for the majority of the operations needed in by a typical neural network, while relying on specialized libraries (such as cudnn) for the few operations that are really critical performance wise. The hardware abstraction enables us to implement most TensorFlow operations once and have them run on an accelerator with more than good enough performance.
 Oh GitHub
 I'm just going to blame GitHub for everything, including lack of OpenCL support. ;)

@benoitsteiner might be able to comment more though.  I don't really know what you mean by 'stream executor' strategy.  We currently use a version of stream executor and CuDNN and Eigen and they all play well together, so I'm not sure how any plans have changed for the OpenCL side of things
 @bhack StreamExecutor provides functionality equivalent to that of the CUDA runtime and some of the CUDA libraries (such as cublas or cudnn). However you still need to write your GPU kernels, which is what we use Eigen for.
 @bhack  Eigen enables you to write an expression that describes the computation you want to perform once, and automatically generate a kernel (which we call the evaluator) to evaluate that expression on CPU and another kernel to evaluate the expression on a CUDA device. Once we have support for OpenCL in Eigen (we're getting close), it will be possible to also generate the OpenCL kernel automatically.
For a few TensorFlow operations that are performance critical (such as convolution), we use hand optimized kernels and/or third party libraries. In these cases, we'll need a good OpenCL implementation of these operations.
 @bhack: Codeplay has made a lot of progress on the opencl front. Stay tuned for a big push to https://bitbucket.org/benoitsteiner/eigen-opencl in the next few weeks.
 @naibaf7: A fast implementation of the convolution operation would be extremely helpful in TensorFlow. Looking forward to it. 
 @NEELMCW Codeplay has just released partial support for OpenCL to Eigen Tensors. The code is available in this [bitbucket repository](https://bitbucket.org/benoitsteiner/opencl). For the most part, TensorFlow depends on Eigen tensors. There are additional dependencies on Eigen for the linear algebra operations, but we don't have to provide an OpenCL compatible implementation of these operations (at least not initially). Therefore we're in a very good position to start supporting OpenCL in TensorFlow.

If you are interested in contributing, I have started to track what needs to be done in [this spreadsheet](https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit?usp=sharing)
  hey @sibleyd -- yes, that Dockerfile was still undergoing a little churn. An updated version will come along shortly, and I'll update this issue as soon as it's ready.

The image is already up: b.gcr.io/tensorflow/tensorflow-full-gpu ... you'll need to follow [these instructions](http://tensorflow.org/get_started/os_setup.md#install_cuda) to pick up the CUDA libraries. The Dockerfile will assume those files are available locally (which is the missing `cuda/` path you spotted).
 hey @sibleyd -- this should be cleaned up now:
- Back to a [single Dockerfile](../blob/master/tensorflow/tools/docker/Dockerfile.gpu_base)
- Complete instructions (well, with pointers) in [the README](../blob/master/tensorflow/tools/docker/README.md)
 Closing for now, but reopen if you hit issues.
  Out of curiosity, have you set your LD_LIBRARY_PATH to your cuda installation's lib64 directory?
 Long story short: tensorflow currently requires cuda 7.0.  If you install version 7.0 in a separate directory from 7.5, and point tensorflow at it via the configure script (or LD_LIBRARY_PATH), it will work.  Leaving this open to track future upgrades to the 7.5 SDK.
 @zheng-xq recently added a feature so that:

```
TF_UNOFFICIAL_SETTING=1 ./configure
```

allows you to configure which version of cuda/cudnn/etc to use without requiring manual symlinks. 
 The pip packages for 0.7.0 should be unversioned -- they should look for libcudart.so, do you have logs to suggest this isn't happening?
 Weird, we were supposed to have built the wheels unversioned.  https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=libcudart shows no missing hardcoding of 7.0 anywhere...

@martinwicke, @jendap, any ideas?
 Release 0.7.1 wheels now are built assuming cuda-7.5 and cudnn v4.  If you want earlier versions supported, you can build from sources and set the appropriate library paths / versions during ./configure.
 If you build from the latest TF, it would ask you for Cuda SDK and Cudnn versions. You can point it to a different version. 
  A Swift frontend, especially for simply running graphs for inference on mobile would be great. I don't know of current plans to provide one, so feel free to dive in. Since our exact plans on accepting external contributions are still in flux, it would be a good idea to check in with the discuss mailing list with a draft of the code ahead of time to figure out where it should live exactly.
  This is something the core TensorFlow team is unlikely to tackle in the near future, so if you want to contribute it, please go ahead! I would recommend circulating a proposed implementation on the discuss mailing list early on, so that a consensus about where such API might live (in repo / off repo / in 'contrib' directory) can be reached ahead of time.
 As I think we have mentioned elsewhere, there is a lot of code in python that we do hope to move into the core C implementation (shape inference, gradients, etc), so that the language bindings only have to be thin, idiomatic wrappers around the core functionality.  But as you know, it is quite a bit of work to do this, and we already have some 100+ open issues to prioritize against too, so please be patient with us.
 Great to hear that you are willing to help!  We're not at a state where the work can easily be parallelized across multiple developers, but once we are, we'll definitely try to conscript the community to help.
  One interesting piece of news: @shanselman wrote a [blog post](http://www.hanselman.com/blog/PlayingWithTensorFlowOnWindows.aspx) about running TensorFlow on  Bash for Windows.

We still intend to provide first-class Windows support, but adventurous users might find this a good way to get started in the mean time.
  Sorry about that Alex, we'll update the docs. We are actively working on iOS support, though I can't give a timeline I'm afraid.
 Yes, we do plan on making it available as open source, and we'll welcome contributions!
 Reopening as a tracking bug.
 Just a comment on the progress. We were blocked on a build issue for quite a while, but with the latest release of Bazel (0.2.2) we have linkopts which we hope should let us get https://github.com/tensorflow/tensorflow/pull/1631 integrated soon.
 I'm working with the protobuf team to get https://github.com/google/protobuf/pull/1500 in, so we can update protobuf in TensorFlow and get iOS support checked in.
 The documentation is still in progress, and we're still testing it, but you can build an iOS version of the library by using the new makefile system:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile

After that's built, the tensorflow/contrib/ios_examples folder has a couple of sample applications.
  We are working on support for operators and kernels that take quantized (fixed-point) data -- we're still working on the right APIs to expose them, and with that we'll have more documentation on why and how they are used.  Stay tuned!
 Closing for now. When the quantized ops come online, they'll come with documentation.
  The syntax is, in fact, doxygen: part of our docs generation pipeline calls doxygen. We are working on making more of the doc generator publicly available, but it requires a decent amount of untangling.
  This release does not support distributed computation.
 Hi kmatzen: As mentioned in our FAQ, this release does not have an RPC and distributed implementation available yet: http://tensorflow.org/resources/faq.md#running_a_tensorflow_computation, but we are working on it and hope to make it available when it's ready.

As a short-term proxy, you can see the Cifar tutorial for an example of how training and distributing among multiple GPUs works -- http://tensorflow.org/tutorials/deep_cnn/index.md
 (De-duping with https://github.com/tensorflow/tensorflow/issues/23)
  Okay, I was able to reproduce this in virtualenv by installing protobuf==2.6.1

The short answer is that we depend on protobuf 3.0.0, and having the protobuf pip library installed seems to interfere with ours.

Two solutions:

1) pip install protobuf=3.0.0a1 or higher, then pip install tensorflow
2) pip uninstall protobuf first, and then install tensorflow again -- it should bring in the dependency

I was able to do both in virtualenv and they worked -- let me know if either suffices for you.  We might add protobuf >= 3 to our whl dependencies if so.
 flyingmutant: to appease my curiosity, can you let us know what you get when you do the following in python:

``` python
import google.protobuf
>>> print google.protobuf.__version__
3.0.0a4
```

(it's possible google.protobuf.__version__ doesn't exist either in one of the environments I've tested).

Basically the symptom of the problem is that python is finding the older version of the protobuf library that doesn't support the generated python proto3 we require.
 Thanks for the help debugging everyone!  We'll try to update our 'common problems' section soon to include these suggestions.
 By the way, if you would like to use homebrew, you can install protobuf 3 via:

brew install --devel protobuf

For more details please refer to https://github.com/grpc/grpc-common/issues/162
 Woops, sorry liuyipei@!  Will re-open the original one.  It's the same issue, just on a different platform.
 We've updated our docs to what we believe can help this -- closing this bug for now but we can re-open if there are some other related issues unaddressed
  Hi!,
I'm really interested on contribute in my spare time on this project, it  would be great for me port the Python libraries to Go, or help with this task, the problem is that after read the "Contributing guidelines" I still don't know how to do it.
I tried to sing the Individual CLA, but I get an error that says: "You must be an owner of the contributors group in order to submit this CLA.".

Btw, are you working, or do you plan to implement soon the Go libs?

Some related projects that I implemented in Go: https://github.com/alonsovidales/pit - This is a distributed recommender system based in the "Adaptive Bootstrapping of Recommender Systems Using Decision Trees" paper by Yahoo. The site of the project: http://pitia.info/
And the Go ML: Some machine learning algorithms in Go: https://github.com/alonsovidales/go_ml I also implemented Neural Networks with CUDA: https://github.com/alonsovidales/go_ml/tree/cuda_implementation

Thanks,
 I still have no answer, but we could start working on the libs, and if Google wants, they can use the code, if not we can release the libs as a separate project :) I think that this shouldn't be a problem since the project is licensed using an Apache 2.0 license.
My main concern is that we could be doing the same that they are doing the same in parallel, so, by the moment we can keep reading and trying to understand how the currently available libs works, and if this weekend we don't have any news, we can prepare, plan and distribute the different tasks to do the libraries.

What do you think?
I'm going to send an e-mail to the golang-nuts mail list.
 The current state of Go is that we have a few people who've expressed interest internally in contributing language bindings, but nothing concrete at the moment. I would suggest that if you got started on this, which would be awesome, you could whip up a proposal (preferably in the form of prototype code) and run the design by the discuss mailing list.
 Thanks @vincentvanhoucke , then, let's try to prepare a prototype and let's see how it works :) I had been digging a bit in the Python libs, and it shouldn't be too difficult to prepare the Go libraries.

Btw, which is the discuss mailing list?, I can't find it.

I love the project and it would be awesome for me be able to contribute.
 Found the discuss mailing list, sorry :)
 Sure, better keep this issue open so other people can find it easily.

I think that it would be better to move the conversation about the prototype, etc to another place, we can:
- Use the official "discuss mailing list"
- Create a new mailing list just for this
- Use the issue tracker of a GitHub repo, I created this repo just to start playing around: https://github.com/alonsovidales/tensorflow

Regarding to the prototype, my proposal is to use one of the examples:
http://tensorflow.org/tutorials/mnist/beginners/index.md
And studying how the Python libs works, port just all the involved code to execute the example, since an example like the one for beginners covers most of the functionality, and can give us a great overview of how to port all the libraries.
To reproduce the behaviour of the Python libs, the best option would be to use TDD, we can go method by method and based on the input/output of the python code prepare the test on Go.

We can distribute the tasks to prepare the prototype or work in parallel and put all our ideas together at the end. Since for me the goal here is try to get a general idea about how to implement the final version, tasks that we have to do, etc, I think that work in parallel can be a good approach.

What do you think?
 Just a little update of what I have been researching this weekend. I couldn't find too much time for this, but I made some small progresses.

I have been researching how the Python libs works internally, and basically, the logic to generate the graph and plan how to execute it, etc is managed from Python that sends all the graph to be executed to the C++ libraries, this libs take the graph and executes all the nodes of the graph. They are working on a pure C++ version of this logic, but it is still not ready.
In order to execute the C++ code from python they use SWIG: www.swig.org and to build the system Bazel: http://bazel.io/ .
The good news is that SWIG also supports Go ( http://www.swig.org/Doc2.0/Go.html ) and Bazel introduces support for Go ( http://bazel.io/docs/be/go.html )  after the 0.1.1 version.
The bad news are that I couldn't build the system using Bazel 0.1.1 by many different problems with the linker, etc that I'm trying to resolve. And the other problem is that we have to create the .i files for SWIG in order to prepare the build for Go, I almost have this part done, but I'm also having a lot of problems.
The main issue is that I have no experience with Bazel and SWIG, so I'm spending most of the time reading documentation and trying to fix stupid problems caused because of I misunderstand something from the docu.

In case of I finally can't prepare a wrapper using SWIG, I think that I'm going to try to prepare the prototype using cgo directly, I already have a small proof of concept that generates the session in Go, but it would be much better if the libs works with SWIG and Bazel as the Python libs does.

I'm not having too much spare time lately, but it is being super interesting for me to research how all this system is done, the code is pretty well documented, and easy to understand.
 I think that it would be possible to do a small workaround using a sh_binary to compile run swig like they are doing with the Python libs:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L21
But by the moment for the PoC I'm using an ugly MakeFile:
https://github.com/alonsovidales/tensorflow/blob/master/tensorflow/go/makefile
And setting the flags for the linker here:
https://github.com/alonsovidales/tensorflow/blob/master/tensorflow/go/cgoflags.go
And also a wrapper to access the C++ libraries from C:
https://github.com/alonsovidales/tensorflow/tree/master/tensorflow/go/wrapper
But this is just for the prototype, I plan to move all to Bazel + SWIG :)
 Well, my idea is to build the graph in the Go libs as it is being builded by the Python libs. They are working on the C++ code, but by the moment it is a WIP and it is, by the moment, unstable.

I'm not sure about their future plans, but from my point of view have this process in Python is not a bad approach since the tensors takes a lot of advantage of the dynamic typing and operators overloading that Python offers, this makes the code more simple to write and to understand, this is being one of the biggest problem I'm having porting the code to Go. I also think that keep the graph building process in Python is a good idea since this process requires almost no time compared with the operations that are implemented in C++, so scarify simplicity in favour of performance don't makes too much sense. I think that they make the good choice here.

I'm planning to port the process to Go by two reasons:
- You could create a pure Go application to train your model, that could be interesting for some situations.
- I'm working on just a PoC, and I prefer to explore all the code in order to get a global idea of how all the different pieces works together.

And well, port the process is going to be funny for me :)
 No :'( I had a personal problem and I couldn't spend a single min on personal projects during last month, I'll try to resume it on next week.
 tmc sent a pull request - https://github.com/tensorflow/tensorflow/pull/1237 - which we should be able to get in with a few iterations.
 @dave-andersen What's the status of this? 
 Closed by #1771.
 Realistically, TBD.  I've maybe found someone @ google who might be interested in taking ownership of the bigger picture of TF + go.  I think we're of a mind that the current state isn't what we need yet -- it's not integrated with the normal tests, nor is it go-buildable in the normal way.  Those are the major blockers to getting it into the main branch, but I think there's a further concern that because of the amount of magic that still happens only in Python with shapes & gradients, we're not at a point where we can have the Go bindings we'd like as a first-class supported language.  There's some ongoing work looking at improving our ability to support more languages, and I think some of the momentum on Go is stalled waiting on that.
 Let's leave this bug open then, to go along with the bunch of other open language bugs that are much less far along.
  I just pushed the fix in https://github.com/tensorflow/tensorflow/commit/db0b5da485e1d1f23003ee08ed2e191451ee0319

Thanks!
  Hi Avanti -- internally we've been working on iterating the API for RNNs, and we were happy enough with the current API to use it in the tutorial, but we're making sure it's solid before promoting it to the public API, since we'd then have to support it indefinitely.  (Anything not in the public API is a work-in-progress :)

We'll keep this bug open in the meantime, and for now you can look at the source code documentation if you're interested in playing around: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L9
 On your first question, see https://github.com/tensorflow/tensorflow/issues/208.

On your second question: the core TF engine currently only sees the GraphDef produced by python, so the RNN example is an unrolled one today.

I'm not super familiar with that RNN example -- @lukaszkaiser or @ludimagister might know better.
 I zer0n,

the current RNN is statically unrolled, there is no (not yet) dynamic unrolling based on the length of the sequence. Th dynamic calculation means the graph is unrolled up to the max_sequence_length, but if a sequence_length is provided the calculations on the unrolled graph are cut short once the sequence_length is reached, using a conditional op. Depending on the application this may result in shorter processing time.
 Yes, to add to what @ludimagister says: the conditional op will plug in zeros to the output & state past max(sequence_length), thus reducing the total amount of computation (if not memory).

I may actually modify this so that instead of plugging in zeros to the state, it just copies the state.  This way the final state will represent the "final" state at max(sequence_length).  However, I'm undecided on this.  If you want the final state at time sequence_length, you can concat the state vectors and use transpose() followed by gather() with sequence_length in order to pull out the states you care about.  That's probably what you would want to do, in fact, because if you have batch_size = 2 and sequence_length = [1, 2], then for the first minibatch entry, the state at max(sequence_length) will not equal the state at sequence_length[0].

An alternative solution is to right-align your inputs so that they always "end" on the final time step.  This breaks down the dynamic calculation performed when you pass sequence_length (because it assumes left-aligned inputs).  I may extend this by adding a bool flag like "right_aligned" to the rnn call, which assumes that calculation starts at len(inputs) - max(sequence_length), and copies the initial state through appropriately.  But that doesn't exist now.
 @zer0n It depends on your task.

Returning zeros is fine if you only care about outputs (i.e., you're not hooking up to a decoder); and your loss function knows to ignore outputs past the sequence_length.

Returning the state from the end of the last time step might also be considered "wrong", but will generally always happen if you have inputs of different lengths (and aren't performing dynamic computation).  This is a typical approach to performing RNN with minibatches.  For this reason when performing encoding/decoding, people usually right-align with left-side padding instead, so the last input of any example always corresponds to the very last state.  This seems like the cleanest solution for now.

Anyway, this part of the API may change; not sure yet the best approach.
 (also, specifically returning the state at sequence_length for every entry is taxing both in terms of computation and in terms of memory, both in short supply with RNNs )
 (This discussion is probably better off had on the discussion mailing list, rather than this bug about documentation)
 @webmaven: We are going to be using semver, and we will publish exactly what we mean by that too. We'll try reasonably hard to maintain API stability even before 1.0.0, especially in the parts of the API that are official. For now, we have decided that something becomes "official" when it shows up on the API docs. You'll notice that many other functions are documented but not included in the docs, that means their interface is still in flux.

I'm closing this bug for now.
 Yeah, we're definitely considering adding something like this since it helps to get community feedback on some experimental APIs before they are fully ready.
 We are reconsidering the api, deciding if we should make the current
rnncell implementations stateful, or whether to store some of the non
variable state in graph collections.  After that decision is made, we will
likely add documentation.
On Dec 20, 2015 6:45 AM, "Michael R. Bernstein" notifications@github.com
wrote:

> @vrv https://github.com/vrv and @martinwicke
> https://github.com/martinwicke, so getting back to the original topic,
> is the RNN API currently a good candidate for such 'draft' treatment, and
> does that mean this issue on documentation should be re-opened?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-166134490
> .
 The main consideration is in how calculations common across time are
cached.  Simpler caching is traded off against having RNNCell be a pure
logic object (non-stateful).

For example, if you access two Variables at each time step and then concat
them, using the result in your **call** calculation, then this is something
that should be cached beforehand because it creates redundant computation.
The two approaches to caching are:
1. RNNCell is stateful: create and cache this Tensor inside the RNNCell
   object
2. RNNCell is non-stateful: **call** looks for the cached Tensor inside a
   graph collection; if it doesn't exist, it creates it (similar to using
   get_variable).

With a stateful RNNCell, Variables are created when the RNNCell is created;
and so that variable scope is used.  With a non-stateful RNNCell, Variables
are created / accessed during **call** and the variable scope used is
whatever it was when you ran rnn() or bidirectional_rnn() or whatever.

Because of this, moving from non-stateful RNNCell to a stateful one (and
modifying the associated implementations of LSTM, GRU, etc cells) would be
a breaking change.

I personally prefer stateful objects, because it's easier to understand and
debug them.  But there are arguments in both directions that have to be
considered.

On Sat, Dec 26, 2015 at 2:50 PM, Michael R. Bernstein <
notifications@github.com> wrote:

> @ebrevdo https://github.com/ebrevdo what are the considerations
> surrounding this decision between stateful rnncell implementations vs.
> storing the state in the graph collections?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167369971
> .
 For example:

Moving to stateful objects now would break a bunch of external dependencies.  Of course, this is an undocumented API and therefore folks should expect it to break in their projects.  However, I'm afraid of breaking external projects in subtle ways that don't emit errors.  This indeed may happen with this change.  Especially for those who depth-stack RNNs on top of each other using the same instance.

In addition, there are those who argue that the RNNCell should continue to be a purely logical object with no state, so you can reuse the same instance of RNNCell across multiple RNNs without fear of reusing the same variable in multiple places (though get_variable's checks for over-sharing may ameliorate this somewhat).

EDIT: scratch that last sentence.  the get_variable would then be called only once in the RNNCell's initialization, and all those get_variable protections would go out the door :(.
 Currently you can.  You can also use it with a shared name scope to tie the
parameters across multiple rnns.
On Dec 29, 2015 3:31 PM, "Michael R. Bernstein" notifications@github.com
wrote:

> So, you _can't_ actually reuse the same RNNCell across multiple RNNs in
> either case?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167899548
> .
 Right. It gains you the ability to tie parameters not only within one lstm,
but also across multiple lstms.
On Jan 5, 2016 3:52 PM, "Michael R. Bernstein" notifications@github.com
wrote:

> OK, so you _can_ currently reuse an instance of RNNCell across multiple
> RNNs without worrying about reusing the same variable in multiple places.
> 
> Does using a shared name scope with a single RNNCell instance across
> multiple RNNs gain you anything? Or is that really for reusing parameters
> across multiple RNNCell instances in multiple RNNs?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-169172049
> .
  You can find one pre-trained network as part of the Android example, at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/assets/tensorflow_inception_graph.pb
This implements a version of the Inception architecture for Imagenet classification.
 Reopening to de-dupe future requests for this
 This is working as expected. We've moved the model out of the source repo, but it's available for separate download. See the android example README for the latest instructions:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md
Currently the link is https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip but the README will hold the most up-to-date location going forward.
 The inception model checkpoint is downloadable (and we have examples using it for classification and deepdream), and more models are being added to https://github.com/tensorflow/models. Closing this.
 @shlens, can you comment on this?
 Yes, Inception is known to be far smaller in terms of parameter count then VGG. The first version of Inception contained roughly 5M model parameters; VGG was >75M model parameters (I don't know the exact the number for VGG). As @vodp mentions, the primary reason for this difference in parameter count is due to the fact that VGG uses several large fully connected layers at the top where as Inception minimizes to just 1 layer for classification.
  Nice!
 Moving this comment here from https://github.com/tensorflow/tensorflow/issues/3:

---

There's a testsuite with fairly good converge, but currently it's mostly Python with a few C++ tests. There's also a lot of functionality for building graphs that's currently Python only, in particular the automatic differentiation functionality, though that doesn't matter for evaluation of graphs in Java. There are plans to move this functionality into the underlying C++ in future, at which point Java SWIG bindings would be more useful for creating graphs.

If someone takes up the Java SWIG challenge, we'd be happy to accept it upstream pending review, etc., at which point it would be part of our continuous testing. The details of accepting contributions is in flux at the moment, but that will stabilize.
  We definitely depend on protobuf 3.0 (it's what we use as a git submodule). 

To help us debug, can you please try following the instructions to install within a virtualenv here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#virtualenv-based-installation- ?
  Although we may not pursue this ourselves, the internal formats used for communication (`GraphDef`, etc.) are all platform independent protobuf.  Thus, it is possible to implement all or part of the tensorflow API in Java while preserving communication compatibility with the existing code.  Since a Java version would likely be slower, one useful bit would be a pure inference layer that evaluates graphs but isn't necessarily able to build them; this would allow graphs built in Python and trained in Python / C++ on GPUs to be run from Java servers.
 There's a testsuite with fairly good converge, but currently it's mostly Python with a few C++ tests.  There's also a lot of functionality for building graphs that's currently Python only, in particular the automatic differentiation functionality, though that doesn't matter for evaluation of graphs in Java.  There are plans to move this functionality into the underlying C++ in future, at which point Java SWIG bindings would be more useful for creating graphs.

If someone takes up the Java SWIG challenge, we'd be happy to accept it upstream pending review, etc., at which point it would be part of our continuous testing.  The details of accepting contributions is in flux at the moment, but that will stabilize.
 If you search for `RegisterGradient` in the python directory, you'll see all the places we define the gradients of various ops.  The gradients are defined by Python code, so they aren't available from pure C++ as yet (we hope to change this at some point).
 Both of those might work, but I don't know how much of a savings they are compared to doing it the right way and porting the code to C++.
  This should be fixed by https://github.com/tensorflow/tensorflow/commit/430a054d6134f00e5188906bc4080fb7c5035ad5 .
 We haven't updated the binary package until our next binary release -- it is fixed at HEAD if you build from source though.
  Currently we only support Python 2.7, but we should support Python 3.
 Main things this involves: `print -> print()`, handle `__floordiv__` / `__truediv__` / `__div__` correctly.
 We're working on it.
 I'm running futurize on the code at the moment; once that's done it'll be easier to parallelize the remaining work.  So far `futurize --stage1` is checked in and I'm working through `futurize --stage2` now (checking each of our divisions very carefully :)).

Unfortunately our contribution process needs a bit of improvement (we're working on streamlining it), but I'll see if there are natural chunks of work to break off and ask for help in fixing once the initial futurize push is done.  Before that it's tricky to parallelize.

The only non-obvious questions (so far) are what to do with unicode and where.  Cc'ing @mrry since he was taking a look at that.  Most of the "names" for things, such as names for ops and names for tensors in a graph, are already restricted to be roughly alphanumeric, so hopefully we should be able to leave them as C++ `string` while still accepting unicode input from Python.
 Also, question for people that have done such conversions before / recently: is `six` still the recommended way to make code transparently support both?  In particular, we need stuff like `xrange` and `iteritems` as symbols that exist in both 2.7 and 3.x.
 @madisonmay: Thanks, should have asked you earlier.  Modernize probably would have been a better choice, since after reading that page I am essentially taking the output of futurize and then rewriting it into what modernize might have already spit out.  Mostly done with that phase, though, so I'll probably just finish it up using futurize.

For the record: `futurize --stage` was mostly `print_function` and a few other safe trivialities.  I'll write a detailed comment on what `futurize --stage2` involved for us once I'm done with it (still in the middle of reading diffs).
 Thank you @Peque!  It would be great if further comments were limited to technical discussions about Python 3 support.
 Sorry, I meant Python 3, not 3.0 -- was in a rush to fix :P.  I'll fix this today
 @goodmami: Yes, we will likely only support 3.3+, possibly 3.4+ (I'm not sure what the differences are, but will look that up).  I also changed the issue title.
 @jli05: https://docs.python.org/3/whatsnew/3.4.html makes it look like supporting 3.3 won't be any harder than 3.4, so we should be good to go.  It is possible our internal tests will be run only in 3.4 (and 2.7), but if anything ever breaks we'll be happy to accept patches.
 We don't support 3.x yet, but we're getting closer.  As of 1d765834110, all our Python files import `absolute_import`, `division`, and `print_function` from `__future__`, and `Tensor` objects have the correct suite of division operators for compatibility with 2.7 and 3.x.  This involved running `futurize` and them inspecting every call to division to see whether it should be `/` or `//`.  I've also scrubbed all of our uses of `xrange` / `range` / `zip` / `map` / similar.

Next steps: set up testing with 3.x, deal with unicode correctly, fix imports for libraries that have moved around, etc.
 We're most of the way there: all the hard bits are hopefully done and only build issues and occasional incompatibilities remain.  I don't have a timeline though.
 @benhamner: As of today, we're extremely close!  All of the tests pass, but there are a few build cleanups before it'll be usable outside of my hacked up git repo.  Unfortunately it proved impractical to divide the work up, since it was largely a mechanical process of going through and sticking b's in front of strings and other test-driven development.
 @stonebig: We're very close!  I was waiting on some build cleanups, but those are checked in now and I'm going to try them out tomorrow.  When I left off, all of tensorflow's source files were Python 3 compatible; only build and configure stuff remained. 
 @Netizen29: We should have a better answer soon about tutorials, models, etc. in pip (see #247).

Status update: Build fixes for tensorflow proper are in, so we just need to upstream a small BUILD change to protobuf.
 As of cdf0dbff784c55b2e599e956148b5c48828435e7, tensorflow should be Python 3 compatible.  Anyone want to try it out and report problems?  I'll leave this bug open for a day or two for that purpose.

Note: To try it out, grab an up-to-date git tree, run `configure`, and enter `/usr/bin/python3` or the equivalent when it asks for the path to Python.

Assuming testing is positive, we should have a 0.6.0 release incorporating Python 3 support and various other features soon.
 Thanks @xysmas!  We'll hopefully have a 0.6.0 release out with this and other stuff fairly soon. 
 @ixjlyons: That's good to know; cc'ing @ebrevdo and @vrv in case there is something we can do about that when we push out 0.6.0.
 We must change [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L31) to 'wheel >= 0.26', i believe.
 @joshburkart: Unfortunately not just yet, though they are planned.  That is: all of the unit tests do work with Python 3 at the moment, but we are not yet running them automatically.  This will hopefully change soon.
 Closing since it seems to work.  Python 3 support will ship with version 0.6.0, which is coming very soon.  Future Python 3 bugs should be filed as separate issues!
 @thelostscientist: Do you mean when?  This will be part of 0.6.0. 
