  Thanks for the submission.  We're testing this along with upgrading to 0.12.3 right now and will keep you posted when we commit.
  We shipped 6.8.1 recently, but I don't see any tags/releases?

https://github.com/twitter/finagle/releases
  There's some trickiness with the backport of http headers from netty 4 that collides with our headers map in finagle.  If you want to take a look at it, we'd appreciate it.
  It should be pretty simple to adopt to the new APIs. 
 (This is really cool btw.)
 I think we just want this to be part of the inet! resolver.
  More generally, we hope to remove the thrift dependency altogether.
  It's about time Finagle get a legit logo.

We'll poll the community for ideas before we kick off a 99designs contest and vote amongst the submissions. Here is what we have so far for ideas:
- Peregrine falcon as the fastest bird by flight speed? And it starts with F
- Something to do with filters
- Something to do with plumbing
- allusion to a steam governor

We would like to hear from the community what they would like to see in a logo or if they have any suggestions. 
 Someone mentioned "coffee filters" as an idea.
 Perhaps something involving a bird in an engine room?
 How about some sort of fast bird (or a more cuddly animal), perhaps mechanized, with a big helmet on? We want to emphasize: fast, safe.
 Thanks everyone for your input, I will start the contest this week and we can go over submissions while we narrow things down.
 The contest has started:
http://99designs.com/logo-design/contests/logo-finagle-254061/welcome

There will be a few days of submissions as we get to whittle them down based on our likes.

If you have anymore ideas on what the Finagle logo should look like, let me know.
 You can take a peak at the current set of designs:
http://99designs.com/logo-design/contests/logo-finagle-254061

Anything stand out that folks like?
 Been lost the last thirty minutes looking at steampunk birds, this one is amazing:
http://cdn.designcontest.com/images/twitter-all-stars/steampunk_512x512.png
 Quick poll here for people who want to help narrow down potential designs:
http://99designs.com/logo-design/vote-oxdzrx
 Another vote to finalize the designers for the next round of iteration:
http://99designs.com/logo-design/vote-js23nd
 After voting, these two designs/designers stuck out:
http://99designs.com/logo-design/contests/logo-finagle-254061/entries/61
http://99designs.com/logo-design/contests/logo-finagle-254061/entries/64

Is everyone comfortable moving forward with those two designers to have them battle it out for the final design?
 We finalized our designers and are now experimenting with some more designs:
http://99designs.com/logo-design/vote-rvz15k

Please comment on each design, it really helps the designers. We will keep iterating and in another poll or two, call for the final vote. Thanks for everyones help!
 Ok, here's the final poll to select the logo and designer to work with:
http://99designs.com/logo-design/vote-m6nvce

We will have an opportunity to slightly tweak the final logo selected so keep that in mind when you're writing the reviews for the logo in the poll.
 The voting is done and we went with this one:
https://twitter.com/finagle/status/394137210489823232

Thank you to everyone who participated in the voting process.
  Reported by @rstrickland, with the following stack trace:

```
2013-08-19 07:55:14,025 ERROR c.w.d.d.FourInfoActor [New I/O  worker #22] <* DC [Failed] : java.lang.Exception: write while request pending (Chan=sms, Prod=severe, Addresses=List(6786707884))
com.twitter.finagle.WriteException$$anon$1: java.lang.Exception: write while request pending
    at com.twitter.finagle.transport.ClientChannelTransport$$anonfun$2.apply(ChannelTransport.scala:99) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.transport.ClientChannelTransport$$anonfun$2.apply(ChannelTransport.scala:96) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.util.Proc$class.$bang(Chan.scala:34) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.util.Proc$$anon$1.$bang(Chan.scala:47) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.transport.ClientChannelTransport.write(ChannelTransport.scala:146) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.dispatch.SerialClientDispatcher.dispatch(ClientDispatcher.scala:37) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.dispatch.SerialClientDispatcher$$anonfun$apply$2.apply(ClientDispatcher.scala:50) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.dispatch.SerialClientDispatcher$$anonfun$apply$2.apply(ClientDispatcher.scala:49) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Future$$anonfun$onSuccess$1.apply(Future.scala:645) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Future$$anonfun$onSuccess$1.apply(Future.scala:644) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Promise$Monitored.apply(Promise.scala:40) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Promise$Monitored.apply(Promise.scala:31) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Promise$$anon$2.run(Promise.scala:519) ~[Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.concurrent.Scheduler$LocalScheduler.run(Scheduler.scala:60) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.concurrent.Scheduler$LocalScheduler.submit(Scheduler.scala:40) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:26) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Promise.runq(Promise.scala:248) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:492) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Promise.update(Promise.scala:475) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.channel.ConnectionLifecycleHandler$$anonfun$com$twitter$finagle$channel$ConnectionLifecycleHandler$$channelDidConnect$1.apply$mcV$sp(ConnectionLifecycleHandler.scala:19) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.netty3.RichChannelFuture$$anonfun$onSuccessOrFailure$1.apply(ChannelFuture.scala:126) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.netty3.RichChannelFuture$$anonfun$onSuccessOrFailure$1.apply(ChannelFuture.scala:124) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.netty3.RichChannelFuture$$anon$1.operationComplete(ChannelFuture.scala:28) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:428) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:414) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelFuture.setSuccess(DefaultChannelFuture.java:363) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.AbstractChannel$ChannelCloseFuture.setClosed(AbstractChannel.java:355) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.AbstractChannel.setClosed(AbstractChannel.java:185) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.AbstractNioChannel.setClosed(AbstractNioChannel.java:204) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.NioSocketChannel.setClosed(NioSocketChannel.java:85) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:717) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:99) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:774) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:338) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:585) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:338) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.channel.ChannelStatsHandler.closeRequested(ChannelStatsHandler.scala:83) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:585) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.http.HttpClientCodec.handleDownstream(HttpClientCodec.java:97) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:585) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelDownstreamHandler.closeRequested(SimpleChannelDownstreamHandler.java:154) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelDownstreamHandler.handleDownstream(SimpleChannelDownstreamHandler.java:76) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:585) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelDownstreamHandler.closeRequested(SimpleChannelDownstreamHandler.java:154) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelDownstreamHandler.handleDownstream(SimpleChannelDownstreamHandler.java:76) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:585) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:338) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:585) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:576) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.Channels.close(Channels.java:820) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.transport.ClientChannelTransport.close(ChannelTransport.scala:156) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.util.Closable$class.close(Closable.scala:12) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.transport.ClientChannelTransport.close(ChannelTransport.scala:88) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.transport.ClientChannelTransport.fail(ChannelTransport.scala:115) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.transport.ClientChannelTransport.handleUpstream(ChannelTransport.scala:135) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:378) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.channel.ChannelStatsHandler.exceptionCaught(ChannelStatsHandler.scala:94) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:553) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:533) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:652) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.http.codec.ResponseDecoder.messageReceived(ResponseDecoder.scala:20) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:103) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:148) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:455) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:538) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:437) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.channel.ChannelStatsHandler.messageReceived(ChannelStatsHandler.scala:74) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at com.twitter.finagle.channel.ChannelRequestStatsHandler.messageReceived(ChannelRequestStatsHandler.scala:35) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:553) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:84) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:471) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:332) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [Distribution-Server-1.1.4-RELEASE-jar-with-dependencies.jar:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_17]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_17]
    at java.lang.Thread.run(Thread.java:722) [?:1.7.0_17]
Caused by: java.lang.Exception: write while request pending
    ... 139 more
```
 @rstrickland: Would you mind clarifying some things?

The exception is presumably seen at the “top-level” service? I.e. in

```
val f = service(request)
```

`f` is what exhibits the exception.

In the code you’ve shared with me, you configure retries — presumably retries are active at the time you are experiencing this error as well?
 Okay thanks. That gives me enough to try to simulate the error.
 @rstrickland Do you always use the service (from `ClientBuilder.build`) directly? Or do you also use it via `ServiceFactory`?
 @rstrickland Do you know if these clients are using TLS or not?
 @rstrickland Can you do me a favor? Would you be able to try finagle 6.5.2? (Latest release).

Second, do you know if there are any entries like the following: “Exception propagated to the root monitor!” in your logs?

The only way I can make sense of your stack trace is that an exception was thrown in a `respond` block which propagates. Now, finagle monitors all such exceptions and logs the above message, but a somewhat recent change means that such exceptions aren’t propagated anymore (they never should’ve been).

If this is what’s happening, then you’ll get a proper error (the exception thrown in `respond`) and the connection state won’t be faulty (subsequent requests should continue to work).
 I think they are binary compatible, but no promises. If that doesn’t work out I can nudge the Cassie maintainers to provide an updated artifact :-)
 Great, thanks! Let me know.
 Re: reproducing the edge case -- to trigger the issue requires a specific ordering of events which is probably difficult to do synthetically.

Re: 6.5.0 yes that's new enough. 
 The problem is that it depends on the connection behavior of the server as well. For example, how a server chooses to close an HTTP connection once the result is served. It also depends on which thread ends up executing the `respond` block which throws an exception (for example if it has been chained with other actions).

The totality of the situation would be very difficult to reproduce in a closed setting.
 Did you have a chance to try this out?
 Sounds good.
  I might be misunderstanding, but wouldn't a Filter fit your needs?  Something like [LoggingFilter](https://github.com/twitter/finagle/blob/master/finagle-core/src/main/scala/com/twitter/finagle/filter/LoggingFilter.scala) seems like it solves this specific problem.
 Take a look at Http [LoggingFilter](https://github.com/twitter/finagle/blob/master/finagle-http/src/main/scala/com/twitter/finagle/http/filter/LoggingFilter.scala), confusingly also named LoggingFilter (for historical reasons) :)

Filters are designed to be composed in the way that you're describing, `filter andThen service`.  Here are [some](https://github.com/twitter/finagle#Building%20a%20Robust%20Server) [docs](http://twitter.github.io/finagle/guide/ServicesAndFilters.html) that might be useful.
  ## motivation

I want to be able to be able to set session variables at connection time.
## implementation

I talked to a Database guy at Tumblr, and he said that the way that he had seen it implemented before was usually that when you established the connection, you had the option to pass in arbitrary code that would be executed (ie a block in ruby).  This permits a similar syntax, with the added restriction that the session variable must be set before the Service may begin to serve requests (via the map on `hook(service)`).  

I chose the type of hooks to be Service[Request, Result] => Future[Unit] so that users don't get confused about how the return value is used.  Because the user must discard any computation they do in the callback, it is transparent that the computation is ignored.
## feedback I'd like

I don't really like the name `hooks`.  Does anyone have a better idea for what it should be?
## testing

I tried it out by changing a session variable, pretty sure it worked.
 I don't know.  I'll talk to someone more knowledgeable on my team.
 I talked to some team members, and they point out that is a pretty useful point to be able to inject arbitrary code.  It's not an especially concerning vector for attack, but it's fairly useful to be able to do things like collect data about connection churn, etc.  From the point of view that it could be used for plenty of things other than just session variables, I think it makes sense to keep it a function.
   thanks!
  ## motivation

Some people have mysql databases with different character encodings.  finagle-mysql doesn't support a lot right now, but it should in the future.
## implementation

This is a more complicated problem than you might think at first, because different columns can theoretically be different charsets within a single database.  
## todo
### prepared statements

I only added support for Latin-1 in this PR, because it lets me punt on encoding prepared statements.  For prepared statements, we don't need to declare anything about the charset to mysql, but we do need to encode them.  Because latin-1 is compatible with utf-8, this just assumes every string in a prepared statement will use utf-8.  More work will need to be done to guarantee that every string in a prepared statement is encoded properly.
### other charsets

The other part of this that isn't complete is that the conversion from mysql collation number to a java.nio.charset is completely punted on, and it always returns utf-8.  In the future, there should be a map from collations to charsets.
## thanks

Thanks @roanta for bearing with me as I muddled through the finagle-mysql code, and providing guidance.
 @roanta tells me that this has been merged internally.  I think that only up until twitter/finagle@a51ec07 was merged internally.  I pushed to this branch, forgetting that it hadn't been pushed back into the open source repo.

I'm not sure what to do in this situation.  Should I revert twitter/finagle@a2df331 on this branch?  I have a new branch with a new pull request in #154.
  Fixes #143
## motivation

Described in #143
## implementation

Described in #143
## changes in api

The http logging filter's type has changed slightly, so that .log takes a REQUEST, where REQUEST <: Request.  Internally, this is never breaking, because we can prove that we always send REQUEST to log.  However, someone could theoretically be abusing this log, although it seems unlikely.  They would have to have subclassed the http LoggingFilter, and be making calls to it from inside of that subclass, passing it a Request, of a different type from the REQUEST that they parameterized LoggingFilter with.
## second thoughts

I added the exception logging because the http LoggingFilter logs exceptions, but I'm beginning to think it might be scope creep, since it's easy to just add a MonitorFilter which logs exceptions.  On the other hand, if you hate it so much, you can override LoggingFilter's logException method to be a noop.  I think this is the right way to do it, because it gives users more power, which seems like the scala way.
 No problem, happy to contribute.
  ## motivation

The code for [LoggingFilter](https://github.com/twitter/finagle/blob/master/finagle-http/src/main/scala/com/twitter/finagle/http/filter/LoggingFilter.scala) is already extremely generic, but it only works for Http, basically because it was written for finagle-http.  My proposal is to finagle it to work for any protocol.

I was implementing a generic LoggingFilter, and realized that it was basically a copy of the http version, which seems dumb.  I think it's a pretty useful thing, and I'd like to be able to use the raw finagle version.
## proposed implementation

Move the LoggingFilter and LogFormatter code over to finagle-core making them generic, and leave a LoggingFilter and LogFormatter in finagle-http, but marked as deprecated.  Bump the minor version.  In the next major version, strip the http versions.

We should also add a logError method to the LoggingFilter class, so that we can log exceptions differently from completions.

The proposed method signature for logError would be 

``` scala
def logError(responseTime: Duration, request: Req, error: Throwable)
```
## who

I'd be happy to do it, and will assign it to myself after we've reached a consensus on what we want to do.
## misc

I'm interested in hearing suggestions, or reasons why we don't want to do this.
 Alright, I made a pull request, the api changed slightly, I described the change in my PR.  Changed to logException.
  ## motivation

These classes are useful for anyone who wants to write an integration test, but right now, in order to use them, you have to have a test where your package is com.twitter... which is a pain.
## implementation

No longer marked private[twitter].
  ## motivation

This is a super tiny change, but I think it makes sense for two reasons.  First, DRY.  Second, for users who don't have a lot of experience with partial functions, mixing them together might not be intuitively obvious to them.  When they come here to look at how RetryFilters work, and look at TimeoutAndWriteExceptionsOnly as an example, it will encourage the mixing together way of using partial functions.
## change in behavior

no change in behavior
  In the redis protocol, there are four commands which take a WITHSCORES argument.  In the finagle-redis protocol, in the SortedSets file, every command which takes a WITHSCORES argument has an option in the argument for the constructor that lets you specify it.

However, in the finagle-redis client methods, only the methods zRangeByScore and zRevRangeByScore require that you pass in a withScores boolean, whereas zRange and zRevRange do not let you pass in a boolean.  This means that you can only get scores from zRangeByScore-style calls, although you might want to get them from zRange-style calls, which is suboptimal.  Also, zRangeByScore-style calls return ZRangeResults, whereas zRange-style calls return ChannelBuffers, which actually means that each call can only return one configuration of withScores: if you try to call zRangeByScore with false as the argument to zRangeByScore, it fails.  Similarly, zRange does not support withScores, and so only passes by ChannelBuffers.

I think that the correct way to handle this is to have both styles return an Either[ZRangeResults, Seq[ChannelBuffer]], and to add a withScores argument to zRange.
 Great, thanks for moving quickly on this.
  Thanks! Pulled locally, should show up here whenever hte code review goes through.
  We should just adjust the options in sbt. There's no reason this can't run on a 32-bit JVM.
 @caniszczyk yes. First, we're not actually bundling sbt; the included `sbt` script is a bootstrapping script. So, in order to build these projects, all that is required is a JRE install. Pretty nice.

But more importantly: sbt versioning is really confusing. You might have the simultaneous need of sbts 0.7.x, 0.11.x, 0.12.x. And not only that, sbt will output only cryptic errors if you use one sbt version when you should have used another.

Simply being able to build finagle with

``` sh
$ ./sbt compile
```

is a wonderfully simple thing, in my mind.
  Not an issue.
  I've pulled this internally, with one small change: use a second constructor instead of a default argument, since this doesn't break the Java API.
 (It should appear here shortly)
  ## motivation

immutable.Set is the default Set, so if it's described as collection.Set, you can't just say the return value of something that returns a client.sMembers is a Future[Set[ChannelBuffer]], it's a Future[scala.collection.Set[ChannelBuffer]].  There is also no reason to create ambiguity as to whether it's immutable or not, because it makes much more sense for the api to return an immutable version.
## implementation

changed all references of collection.Set to collection.immutable.Set.  Also changed the imported name from CollectionSet to ImmutableSet.  I cannot use the native "Set" name for the class because it is obscured by the Set Command class in finagle-redis.
  @asrinivas or @xiangxin72 — can either of you take a look?
  There’s also: [JavaLoggerStatsReceiver](https://github.com/twitter/finagle/blob/master/finagle-core/src/main/scala/com/twitter/finagle/stats/JavaLoggerStatsReceiver.scala) and [SummarizingStatsReceiver](https://github.com/twitter/finagle/blob/master/finagle-core/src/main/scala/com/twitter/finagle/stats/SummarizingStatsReceiver.scala)
  Reopening. Missed that this was in the `gh-pages` branch.
  I’ve just pushed the fix for this.
 517f308a5efd
  Thanks! pulled internally, should show up here within a day.
  This is all Ruben’s code -- I’m submitting it here so we have a forum to do a code review!
 Some general comments from my review:
- try to use `ChannelBuffer`s throughout as they make it easier to optimize away allocations &c.; especially for down the road
- make sure you test the frame decoder for fragmented packets
- it would be great to have package docs that give a brief overview of the protocol, the structure of the code, and links to protocol documentation
- apply whitespace consistently throughout: `if (`, `// a comment`

Not for now, but possibly a simplification in the future: it would also be possible to use a `Dispatcher` to do packet defragmentation. In some sense, it's a more natural abstraction: the pipeline codec produces and consumes Packets, and is stateless. The dispatcher consumes and produces requests/responses, and is stateful. Besides a cleaner separation of responsibilities, you also have the full power of composable futures at hand, which makes it possible to write imperative-looking code. You could do stuff like:

``` scala

readPacket() flatMap { packet =>
  if (fragmented(packet))
    defrag(packet)
  else
    decode(packet)
}
```

etc.
 I’ve pulled this internally; should show up here soon (probably Monday). Not yet published.
  I assume what you mean is: what happens when the client sends an invalid message? That's really protocol specific. Generically we cannot do anything other than close the connection, but some protocols may have support for returning errors in such scenarios.
  This was disabled temporarily because of a scala 2.8 dependency diamond issue. It's re-enabled internally, and works well. I'm going to push this to GitHub momentarily.
  hey George—I'm currently on vacation and will have a look when I get back (in ~1 week)
 I think I was referring to the POM :-)

My chief objection to making finagle-protobuf truly first class at this point is that I'd really like to separate the encoding from the protocol. There are some upcoming changes in `finagle-thrift` which will make it a very appealing (and generic) RPC transport, upon which we can use protocol buffers, thrift, whatever.

I'll probably factor that out into something called "finagle-rpc"
 @george-vacariuc how do you feel about closing this, and porting just the serialization code to use Mux instead? I promise, it is much better.
  I think this accomplishes it without checking versions:

```
cut -f2 -d'=' | awk '{print $1}'
```
 I've pulled this internally, should make it out here within the day.
  I'm going to pull this into the internal repo–should appear here soon.
 We've merged it; just haven't pushed yet. I've been on vacation so pushes haven't happened in a little while. I'm going to get someone to do it.

```
* commit ec764afdc105d2f785570191acb6f12e87edc6a4
| Author: Chris Birchall <>
| Date:   Thu Jun 14 14:51:13 2012 -0700
| 
|     finagle-redis: switch from Int to Long for representing Redis integers
|     Redis supports 64 bit signed integers (as stated in the docs, e.g. for the INCRBY command), and so Redis integers should be represented by Long rather than Int.
|     
|     Here is an example of a Redis exchange that is currently not possible using the finagle-redis client:
|     
|     redis 127.0.0.1:6379> SET foo 1
|     OK
|     redis 127.0.0.1:6379> INCRBY foo 1000000000000000000
|     (integer) 1000000000000000001
|     In order to support this I've switched from Int to Long in the following places:
|     
|     INCRBY, DECRBY command arguments
|     Integer replies
|     Unfortunately this switch from Int to Long means a breaking change in the API, but it should be very easy for users to fix their client code.
|     
|     Signed-off-by: Anirudh Srinivas <anirudhs@twitter.com>
|     
|     RB_ID=70910 (https://reviewboard.twitter.biz/r/70910)
| 
```
 32a1aa8e1456782817f498f3cf5236d6db223764
  You need to use the 2.9.1 version of finagle. In sbt, you do this with "crosspaths", so your dependency should look like this:

```
libraryDependencies += "com.twitter" %% "finagle" % "3.0.0"
```

Note the double `%%`
 This is due to using 2.8 artifacts with scala 2.9
  I think doing something like this,

``` scala
diff --git a/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/HttpDechunker.scala b/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/HttpDechunker.scala
index 52faaa6..fb7c7e9 100644
--- a/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/HttpDechunker.scala
+++ b/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/HttpDechunker.scala
@@ -40,11 +40,13 @@ class HttpDechunker extends BrokerChannelHandler {

           if (chunk.isLast) {
             ch.close()
-            sendOf andThen error(err, EOF)
+            sendOf.sync() ensure error(err, EOF)
           } else {
             ch.setReadable(false)
-            sendOf andThen ch.setReadable(true)
-            read(ch, out, err, close)
+            sendOf.sync() ensure {
+              ch.setReadable(true)
+              read(ch, out, err, close)
+            }
           }
```

is a bit simpler, doesn't involve the use of a `Future`, and makes the sequencing more explicit.
 Pretty much any Spec, eg.: https://github.com/twitter/finagle/blob/master/finagle-native/src/test/scala/com/twitter/finagle/ssl/SslSpec.scala

Note that it is a `class` (not an `object`) and mixes in `SpecificationWithJUnit` not `Specification`
 And yes, please remove the additional case in EndToEnd. Thanks!
 Looks great! I'm going to cherry pick in the other commit.
 I had to add the following build dep. Did you forget to add it?

``` scala

diff --git a/project/Build.scala b/project/Build.scala
index d7923d4..752f011 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -206,7 +206,7 @@ object Finagle extends Build {
       sharedSettings
   ).settings(
     name := "finagle-stream"
-  ).dependsOn(finagleCore, finagleKestrel)
+  ).dependsOn(finagleCore, finagleKestrel, finagleTest % "test")

   lazy val finagleThrift = Project(
     id = "finagle-thrift",
```
 Pushed: 919fd54392b035389937e7a8e67d9565b6f381d6
  This is somewhat interesting—I'm a bit split about what to do exactly. You can always get the behavior you want by sequencing syncs; but it seems like serialization of messages is desirable generally. I think, however, that the correct solution is to sequence these explicitly: don't wait for the next upstream until the previous syncd.
 Fixed with 919fd54392b035389937e7a8e67d9565b6f381d6. Thanks!
  Hey, sorry about this! I'm working on pushing out a permanent fix for these build divergences that happen from time to time.
 Should be Monday a.m.—I have the build working internally, but need to polish it a little before pushing it out.
 (Btw: this is already done and pushed to [util](https://github.com/twitter/util/))
 This is now pushed to finagle; it builds well now, with sbt11.
  I've merged this manually in c39bb8c0
  I'm not sure it should. Connection management should be disassociated from request management.
 The problem is that services are request oriented, but service factories are connection oriented—mixing the two is going to cause resource exhaustion like this.

There is a solution to your problem, however, and that's to modify the codec to ensure that proper connection management is applied: a connection is given back to the pool only after it has been released. (We're currently enforcing correctness by ensuring clients are used just once, which is a kludge—finagle-stream was originally designed for "infinite" streams).
 Something like this would probably do:

``` scala

diff --git a/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/Stream.scala b/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/Stream.scala
index 9341534..f720aed 100644
--- a/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/Stream.scala
+++ b/finagle/finagle-stream/src/main/scala/com/twitter/finagle/stream/Stream.scala
@@ -1,17 +1,46 @@
 package com.twitter.finagle.stream

+import com.twitter.concurrent.Channel
+import com.twitter.finagle.ServiceNotAvailableException
+import com.twitter.finagle.{
+  Codec, CodecFactory, Service, ServiceFactory, ServiceProxy, TooManyConcurrentRequestsException}
+import com.twitter.util.{Future, Promise}
 import java.util.concurrent.atomic.AtomicBoolean
-
 import org.jboss.netty.channel.{ChannelPipelineFactory, Channels}
 import org.jboss.netty.handler.codec.http.{
-  HttpServerCodec, HttpClientCodec, HttpRequest, HttpResponse}
+  HttpClientCodec, HttpRequest, HttpResponse, HttpServerCodec}

-import com.twitter.concurrent.Channel
-import com.twitter.util.Future
+/**
+ */
+private[stream] class DelayedReleaseService(self: Service[HttpRequest, StreamResponse])
+  extends ServiceProxy[HttpRequest, StreamResponse](self)
+{
+  @volatile var done: Future[Unit] = Future.Done

-import com.twitter.finagle.{
-  Codec, CodecFactory, Service, ServiceProxy, ServiceFactory}
-import com.twitter.finagle.ServiceNotAvailableException
+  override def apply(req: HttpRequest) = {
+    if (!done.isDefined)
+      Future.exception(new TooManyConcurrentRequestsException)
+    else {
+      val p = new Promise[Unit]
+      done = p
+      self(req) map { res =>
+        new StreamResponse {
+          def httpResponse = res.httpResponse
+          def messages = res.messages
+          def error = res.error
+          def release() {
+            p.setValue(())
+            res.release()
+          }
+        }
+      } onFailure { _ => p.setValue(()) }
+    }
+  }
+
+  override def release() {
+    done ensure self.release()
+  }
+}

 object Stream {
   def apply(): Stream = new Stream()
@@ -42,24 +71,11 @@ class Stream extends CodecFactory[HttpRequest, StreamResponse] {
           pipeline
         }
       }
-      override def prepareConnFactory(
-        underlying: ServiceFactory[HttpRequest, StreamResponse]
-      ): ServiceFactory[HttpRequest, StreamResponse] =
-        underlying map { service => new UseOnceService(service) }
-    }
-  }
-
-  private class UseOnceService(underlying: Service[HttpRequest, StreamResponse])
-    extends ServiceProxy[HttpRequest, StreamResponse](underlying)
-  {
-    private[this] val used = new AtomicBoolean(false)

-    override def apply(request: HttpRequest) = {
-      if (used.compareAndSet(false, true)) underlying(request) else {
-        Future.exception(new ServiceNotAvailableException)
-      }
+      override def prepareServiceFactory(
+        underlying: ServiceFactory[HttpRequest, StreamResponse]
+      ): ServiceFactory[HttpRequest, StreamResponse] = 
+        underlying map(new DelayedReleaseService(_))
     }
-
-    override def isAvailable = !used.get && underlying.isAvailable
   }
 }
diff --git a/finagle/finagle-stream/src/test/scala/com/twitter/finagle/stream/EndToEndSpec.scala b/finagle/finagle-stream/src/test/scala/com/twitter/finagle/stream/EndToEndSpec.scala
index 8fe948c..0dc074c 100644
--- a/finagle/finagle-stream/src/test/scala/com/twitter/finagle/stream/EndToEndSpec.scala
+++ b/finagle/finagle-stream/src/test/scala/com/twitter/finagle/stream/EndToEndSpec.scala
@@ -4,7 +4,7 @@ import com.twitter.concurrent._
 import com.twitter.conversions.time._
 import com.twitter.finagle.builder.{ClientBuilder, ServerBuilder}
 import com.twitter.finagle.{
-  ServiceNotAvailableException, ClientCodecConfig, SimpleFilter, Service}
+  TooManyConcurrentRequestsException, ClientCodecConfig, SimpleFilter, Service}
 import com.twitter.util._
 import java.net.InetSocketAddress
 import java.nio.charset.Charset
@@ -108,7 +108,7 @@ class EndToEndSpec extends SpecificationWithJUnit {
       "the client does not admit concurrent requests" in {
         val clientRes = client(httpRequest)(1.second)
         client(httpRequest).poll must beLike {
-          case Some(Throw(_: ServiceNotAvailableException)) => true
+          case Some(Throw(_: TooManyConcurrentRequestsException)) => true
         }
       }
```
 I'm pushing something similar to this change internally, so it should appear on GitHub in a few days.
 Done in 686dae852a8b5
  Thanks!

Yikes, this is an ugly bit of Synchronized\* in Scala-- might consider a more explicit approach, too.
  Thanks!
  oh. that's .. unfortunate.

i'll fix this; thanks!
 This is fixed.
  That's right. These probably refer to the [version of `within` that takes an implicit](https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Future.scala#L441)
  I tried to reproduce it; I could get it hang a couple of times. I can't get it to hang any longer. When it happened it was hanging in connection establishment. It seems that the api.authorize.net has very high variability in resolution-- so that might be an issue. Anyway, I'm going to keep trying (I now have added debug code to give me a better idea of what's going on by setting a low tcp connect timeout and seeing what's reported).
 Is this still an issue?
  Thanks!
  Thanks!
  Thanks!
  Finagle now builds fine with sbt11, and on scala 2.9.x
  This should be fixed.
  Yep, I was confused.
 Thanks!
  Is this particular protocol `(code32, length32, message)` used by somebody else, or is it your own? If not, it  would be nice to build more room for extensibility (eg. to ship trace ids) from the start. In thrift we do an upgrade dance, but that's only to maintain backwards compatibility.

Also, it would be nice to build a protobuf compiler plugin to generate `Future`-full bindings as well.
 I want to merge this, but we need to put some more work in. My biggest concern so far is the actual protocol: does it comply to some other widely-used one, or is it a custom one of yours? If it's a custom one, I think we should figure out a way to make it more extensible for features like tracing.

I'm willing to merge this in so that we can work on it, but without publishing it yet. How does that sound?
 (Another thing that would be nice to support is multiplexing.
 I'm going to merge this, but not publish it yet-- this way we can work on it to get it up to shape!
 It's merged! I'll write my thoughts about a protocol in an email. Are you subscribed to the finaglers group?
  Please use sbt11 to build finagle open source.
  This works.
  This should all be resolved. Finagle should build with

```
   ./sbt test
```
  Thanks!
  This question was answered on the mailing list:

> You're encountering a problem due to the asymmetry of the String codec — which is really just a toy codec used as a minimal example for codec construction: it expects requests to be terminated with a newline, but the server codec strips the delimiter. So if you modify your proxy service a little, adding a newline to the request:
> 
> ```
> val proxyService = new Service[String, String] {
>   def apply(request: String) = {
>     println("I'm router, I received request message:" + request)
>     client(request+"\n").onSuccess {
>       result => println("Router received result asynchronously: "
>     ...
> ```
> 
> everything should work.
> 
> Note that a well-designed codec will almost always have the property that a proxy in finagle can simply be implemented by doing:
> 
>   val client = ClientBuilder().codec(Blah())....build()
>   val server = ServerBuilder().codec(Blah())....build(client)
> 
> Another note on your code: you shouldn't release the client in the server, lest you wish to server only one request.
  It sounds like perhaps you're using the 2.8.1 artifacts? 2.9.1 are published with the "_2.9.1" suffix, thus you need:

```
"com.twitter" % "finagle-core_2.9.1" % ...
```

etc.
 @danielschonfeld just did. sorry about the delay!
  Finagle only builds with sbt 0.7.x. Note that this doesn't mean you have to use it in _your_ projects -- it's only required to compile finagle itself.

Furthermore we publish binary artifacts so you needn't compile it yourself if you don't want.

I'll add some shell scripts to the finagle source (as well as util) to make it self-bootstrapping so that you don't need to install sbt yourself.
 Sure-- there's no real reason scala bootstrapper can't use sbt 0.11-- standard project and friends are ported.
 @jponge they're on our own maven repo: http://maven.twttr.com/. I just realized that this is in fact not documented. I'm going to bring this up to date tomorrow. I'm also working on getting us published to sonatype.
 Finagle builds by default with sbt11 now.  `./sbt test`
  thanks!
  This should be resolved now.
  this has been fixed internally, and i'll make a code push today together with the release.
  thanks!
  thanks!
  it's comforting that our implementation matched verbatim!
  thanks!
  thanks!
  looks good, thanks for the changees!
  This should be fixed.
  Thanks!
  hello— you must use our fork of the thrift compiler. it's at:

  https://github.com/mariusaeriksen/thrift-0.5.0-finagle
  yep.  the sbt plugin simply invokes the finagle thrift compiler [here](https://github.com/mariusaeriksen/thrift-0.5.0-finagle).

this is a modified apache thrift compiler with the finagle bindings. we're hoping to make this a lot easier in the near future.
  thanks!
  yeah, we're not quite ready for 2.9 yet. for pure Scala, it's pretty simple. however, there have been some significant changes in the way certain types are represented, which is causing some issues for Java compatibility. these changes were introduced in 2.8.2 as well, so even to go that far, we'll have to tackle them.

i have an internal branch that does compile (util, finagle -- we build all of these as part of one source tree, so changes like this are easier to make), but i can't promise anything before a few weeks hence.
 We have been publishing 2.9.1 releases for a while now. The maven artifacts are suffixed with "_2.9.1". E.g.

  "com.twitter" % "finagle-core_2.9.1" % "1.9.12"
 I'm working on getting those branches to sync with github, too. Watch this space in a few days.
 we're publishing 2.9.1  artifacts now (and have been for a while).
  great, thanks!
  great, thanks!
  thanks!
  yikes! i guess i never tested the server side of this :-/
 just fix the tiny style nit, then i'll merge.
  thanks!
  Thanks!
  please make the change in the internal (birdcage) repo.
  fixed: thanks!
  host validation was added in ebe2fcfa548c4ccb145c8d5b3257a071e513b50d
  this is just a case of finagle being too verbose -- i'm landing a fix for this today or tomorrow
  fixed in 1.7.5
  It should not be public.
  can you include:
1. the builder code for the client
2. how your invoke it?

these exceptions should never happen if you just use .build().
 the outlined usage seems correct to me.

which version of finagle are you using?

also: are you keeping stats (through .reportTo)? the counters/gauges/metrics would be handy.

also: when these errors occur, are there any other concurrent exceptions?

p.s. there should be no difference between using .build() and calling .service() on your factory -- except that you only get retries with `.build` and not `.buildFactory`.
 another thing that would help me with trying to reproduce this is: approximate failure rate of requests, latency of requests and request concurrency.
 re: the WriteExceptions-- these are annoying but harmless. a change is being introduced very soon that quenches these.

it's interesting that you're canceling the Futures.

proper cancellation support in finagle was introduced in 1.7.3. i can also think of at least one condition under which 1.6.x would fail on cancellation (i also rewrote Future cancellation completely in the meantime, to avoid exactly these kinds of bugs).

could you upgrade to 1.7.3 and see if the problem persists?
 great. the WriteExceptions you are seeing are likely valid, too -- i'm adding some code shortly that will quench a class of exceptions that get logged, but shouldn't be.
 should be this week
  pulled, thanks!
  this is now fixed.
  i think this is no longer the case. i was able to build the current master just fine outside of Twitter's environment.
  the README has since been rewritten
  This is done.
  LGTM!
  LGTM!
  there's a race between `maybeLifeTimeExpire` and the completion of the underlying request: `maybeLifetimeExpire` can fire, and between `maybeExpire()` and `expired = true`, the request can complete, leaving `didExpire()` uncalled.
 i've added you to the twitter team.  you should be able to push now
  LGTM
  LGTM
