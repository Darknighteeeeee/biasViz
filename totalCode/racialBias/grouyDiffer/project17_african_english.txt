CLABot, you crazy.
> https://github.com/notifications/unsubscribe/AAjO_YYU_CGN5lm_h7_uxFpUoVuzo80Dks5qNp5QgaJpZM4I5QyO
@ilblackdragon  could you please take a look at this?
@RafaelCosman what is the version of TF are you running?
@ilblackdragon There were some PRs regarding resnet.py. Seems like unnecessary indentation was introduced from some previous internal merges.
Below is the code.
#batchX is a time-major 3D tensor
@jlowin Thank you!! This would have never ocurred to me. I tried `emb.set_shape([maxLength, batchSize, embDimSize])` but I got an error saying I cannot use 'Tensor' in the `set_shape` argument. Then I gave up hope. Thank you very much again!!
@bhack, in case of LSTM, the biggest concern is lack of customization. In general, having access to the source code gives us better ability us to tweak our code to better serve TensorFlow users.
That said, we won't hesitate to pick any good functionalities from any library including Cudnn, if there is evidence showing a sizeable benefit.
@nanddalal can you sign the CLA?
key: "squeeze_dims"
File "/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
Added @yuanbyu just in case.
Thanks, @jinfengfeng .
@jyshee go for it if you need it.
cc @3XX0 and @rdadolf
Adding the stream-executor owner, @henline here.
@kirilg, @nfiedel is there still something amiss with the session_bundle?
@dansbecker can you tell us which bazel version you're using?
@ebrevdo is there a conversion howto? That would be helpful.
On Thu, Jun 16, 2016 at 20:25 ebrevdo notifications@github.com wrote:
Which version of bazel are you using?
--input_node_names=Mul \
CLAbot can be slow with company CLAs, don't worry about it.
@gideonite @ilai-deutel I pretty much disabled restore code, because it's not really working well with current way of handling the graph. This example should remove usage of restore and just point into the same directory. If you want to change it or I can create a PR with your commits and the change in saving/restoring.
There are a number of operators (especially those working on integer types) which are not registered for GPU, and for some fairly complex reasons this is not likely to change in the near future.
@yuanbyu  might have more to say on this issue?
preference.
On Fri, Jun 17, 2016 at 11:56 Robin-des-Bois notifications@github.com
The segfault is suspicious. Well behaving TensorFlow or stream-executor code should check for error conditions and quit with a clear error message.
Really adding Jason. +@henline
Is this request specifically for truncated BPTT?  or something more general?
``` python
Could you please attach the log?
Can you document what you're trying to do as well as the symptoms that you experience ?
ema = tf.train.ExponentialMovingAverage(0.9)
print 'avg', sess.run(ema.average(x))
avg 3.1
> https://github.com/notifications/unsubscribe/ABtimzwFpoEiihHDl58C8jZYsD0u64Kbks5qLMxhgaJpZM4Iz71c
> Codename: xenial
> uname -a
> 'third_party'. This will be an error in the future.
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6Tensor16CopyFromInternalERKS0_RKNS_11TensorShapeE+0x109)[0x7fae8eebecf9]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x1cdb96e)[0x7fae8ee2796e]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x1cd0cf5)[0x7fae8ee1ccf5]
/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPool4Impl10WorkerLoopEv+0x17b)[0x7fae8ef4639b]
/gpfs/home/chaowei/software/gcc-6.1.0/lib64/libstdc++.so.6(+0xbe232)[0x7faebd7b4232]
00400000-007f8000 r-xp 00000000 00:15 3515267                            /gpfs/home/chaowei/development/python/python3_tf_cpu_vir/bin/python3.4
3ba8403000-3ba8404000 rw-p 00003000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0
3bab610000-3bab80f000 ---p 00010000 08:05 18612329                       /lib64/libbz2.so.1.0.4
3bace00000-3bacf48000 r-xp 00000000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6
3bacf48000-3bad147000 ---p 00148000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6
3bad147000-3bad151000 rw-p 00147000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6
@danmane  This link still appears broken...
@ebrevdo  I think you modified parts of map_fn recently - do you have any idea what might be happening here?
@caisq Can you comment?
(Reassigning to Benoit as my best guess is that it's an Eigen threadpool issue.)
Just for clarification, when you say "virtual cores" do you mean hyperthreads, or are you running in a virtualized environment (and if so, which one)?
``` c++
Most of the CPU to GPU and GPU to CPU copies come from the fact that the exponential moving average variables are all placed on CPU, which causes a lot of small data transfers. Can you try to move it to GPU and see if that helps reduce the likelihood of the GPU hanging ? It will also help the model train faster.
@yuanbyu we may have to either plumb infer_shape=True through scan/map_fn/etc or just set infer_shape=False everywhere.  what do you think is best?
@petewarden
Closing since this isn't a specific bug, and RaspberryPi is not an offically supported platform (afaik).
> https://github.com/notifications/unsubscribe/AABaHCP2OjbIFIlkinllWA4jlh27W5Chks5qKeQvgaJpZM4Iy5CE
Good catch @kingtaurus. ~99% accuracy confirmed with TF 0.9.
@ebrevdo Can you comment?  Is `output_size` a new or old thing?
> cuda 7.5
> Pei
> https://github.com/notifications/unsubscribe/AABaHFnqyuOysiI0m0iMzMSSPxPQQA_xks5qKS0WgaJpZM4IyxXa
> https://github.com/notifications/unsubscribe/AABaHBpeOFIkhXeYs36Yu3qEOR72TlS5ks5qKaxOgaJpZM4IyxXa
@jiapei100 The source version will say 0.8 until we release 0.9; currently we're at 0.9.0rc0 < 0.9.
slightly more complicated for Github to grasp ("fixes github issue
> https://github.com/notifications/unsubscribe/AAkLHpDHlnUsuzAfX927wAtVFFh1Ht2Lks5qMW5hgaJpZM4IyloD
@amineHorseman Could you give run this through gdb and give us a stack trace?  It's not enough information to debug as is.
If you would like to discuss this limitation further, please feel free to ask on StackOverflow and tag it with the `tensorflow` tag.
``` python
``` python
PS. This is probably more appropriate as a Stack Overflow question, rather than a GitHub issue.
This does seem to be very specialized code that belongs in its own repository.
@ebrevdo Do you think this is a reasonable approach?  I don't know enough about the API to answer.
@ilblackdragon @martinwicke  are there really no tests for this? :)
@ilblackdragon the code is wrong and there is no test for it, so we need a test to validate this is workign as intended before accepting.  Can you suggest a good place for a test?
@petrjanda you can implement test that uses custom Estimator with class weight - that should hit this code.
@martinwicke This seems like a Bazel issue.  Have you seen it before?
also pinging @damienmg and @kchodorow  (did something change in bazel 0.2.3?)
> bit weird, we generally use $PYTHON_RUNFILES if it exisrts or $0.runfiles
> build step for bazel?
@damienmg do we effectively require bazel 0.3 at this point?
with special casing ops that don't work well with chaining (first arg not
>    inspired by PrettyTensor (initially) but with some changes, the original
@yangmch @Anjum48 tf.learn and tensorflow in general are better used with named arguments.
@keveman @yuanbyu
File "/tmp/imm_venv/local/lib/python2.7/site-packages/tensorflow/contrib/immediate/python/immediate/itensor.py", line 103, in __add__
It's an alternative implementation.
any errors when you do "bazel build"?
> https://github.com/notifications/unsubscribe/AABaHFxHqyPAzkuS9Horc_GFHVLAOAWyks5qJ2P2gaJpZM4Ixi2T
cudnn version: 5
where cudnn is installed:
> https://github.com/notifications/unsubscribe/AABaHJgPOU2DB4KOP942x1v4yN2foV6Zks5qKE_OgaJpZM4Ixi2T
@ispirmustafa: Is `regressor.fit` supposed to take a steps argument?
unfortunately there are some conflicts :(
It looks like Eigen doesn't exist.  Might that be the problem?
different version of eigen will work at all. In fact it's quite likely it
> https://github.com/notifications/unsubscribe/AAjO_bBDHVepl5QBpiSamg8hinQtjQnuks5qJ1HxgaJpZM4IxdOc
@hsaputra I'm sorry for not looking at this sooner. My bad!
@gaohuazuo you'll have to ping us when this is ready for review
@ilblackdragon this will likely break some tests. At the very least this change would need some changes to the examples? Maybe it's better to adjust the mnist tutorial text to change the reference to the input.
@danmane Care to comment?  Should we mark this contributions welcome?
Also, shouldn't there be a test of this behavior?
On Sun, Jun 12, 2016 at 20:12 ebrevdo notifications@github.com wrote:
@ebrevdo: Do summaries work inside control flow?
I don't know what pyinstaller is, and consequently I'm certain that we don't support it.
@ibab Alas, re the eigen change.  Is there something I should do to help that process along?  If not, can you send me a mention here when we're ready to go?
@RafaelCosman: I think the conclusion is that we don't want to support the independence you want to rely on, but we also don't want to guarantee that the exception will be thrown.  Thus, what you want to do might happen to start working at some point in the future.
@jendap @caisq: Does either of you know why the "Linux CPU Tests CMAKE" test might be failing with the following error?
(syntax may be screwy), and add that as a data dependency to the python_check target?
@ilblackdragon @martinwicke need moar test coverage
Unfortunately, TensorFlow is not yet supported on Windows.
Unfortunately I still don't understand your issue, but I suspect it is closer to a StackOverflow question than a problem with TensorFlow.  You could try posting your code changes, or you could try posting them on StackOverflow with the `tensorflow` tag.
@SunAriesCN: Questions about how to write new code on top of TensorFlow belong better on StackOverflow.
@samfux84: Did you point `./configure` to the right place?  Having `Python` in `$PATH` may be problematic for bazel, but I believe this should be solved by passing the absolute path to `configure`.
> Please see below my bazel.rc:
Try `tf.nn.rnn_cell` and similar.
PRs welcome!
@ajaybhat Search for classes which inherit from `Optimizer`.  However, note that for Hessian free methods the standard split into `compute_gradients` and `apply_gradients` won't work, since you need to compute partial second order gradients.  There are a few different ways one could handle that; the simplest would be to compute gradients as normal during `compute_gradients` and do the higher order stuff in `apply_gradients`.
@petewarden Do you know how the cc code here could be formulated to satisfy the Switch op? Alternatively, is this a situation in which [strip_unused.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py) could be used to simply remove it?
``` python
import imp
1. Rename `/home/prayalankar/imp.py` to something else, so that it doesn't clash with the name of a built-in Python module, or
2. Change your PYTHONPATH so that it no longer includes `/home/prayalankar`.
@ilblackdragon looks like comments were addressed?
@mouradmourafiq let's either use `Estimator` then or change `TensorFlowEstimator.fit` to take `batch_size` for consistency.
@ilblackdragon Ping. Could you review this and the other PR @mouradmourafiq created? I am not too sure what the future refactoring plan is. @mouradmourafiq Thanks for your patience.
@jendap: Assigning you since you commented; feel free to change.
@mrry
@yaroslavvb: Are bugs in shape functions the only issues here?
This seems like a bazel issue, unfortunately.  @damienmg: Do you have any ideas?
``` python
![worker_hosts = FLAGS.worker_hosts(",")](http://i.imgur.com/7eH14HE.png)
My major concern here is bringing in the Android support libraries as a dependency, since that complicates the build. I can test the previous PR again and see if that builds internally now, if this works for you.
@domluna: Assuming the meta file is a MetaGraphDef, can you check whether it's the `GraphDef` part that's growing?  One possibility is that you are adding new ops to the graph each step, causing the graph to grow linearly.
@domluna: Try printing out the node names in the graph each step.  If you see it growing over time, it probably means you're adding new nodes to the graph over time.
This is unexpected, and definitely a bug. When you run it in the single-machine mode, are you using the `tf.train.Supervisor`?
That's strange, because I think `grpc::DeserializeProto()` (`#1` on the stack trace) no longer exists in the 0.14 release of gRPC, which we're now using.
@aselle: To pull an offline comment into the thread, is there a way we could detect the size mismatch (possibly at runtime) and bail with an explanatory error?
@ibab, @benoitsteiner: The easiest way to fix this would be to do two scan products to get the sequence of partial products from both directions, then multiply them together to get all products with one element removed.  Unfortunately, we don't yet have scan.
``` python
``` python
``` python
@zjhzxhz: Can you check if doing `import numpy` before `import tensorflow` fixes the problem, as per #2034.  Alternatively, you could try upgrading to 0.9, which shouldn't have this problem.
I think this might be related to the problems affecting "setting up for development" #2497 i.e. in
Specifically, The instructions have to be different on what you symlink into the python build directory dependent on the version of bazel (which ones to do when are outlined in #2497).  Are you able to get those instructions to work and sucessfully import tensorflow from python?
Looping in @ibab. Did you expect your CL would cover this case as well?
Yes, the warning messages are a separate (known) issue which we are currently working on.  (also reported in [this StackOverflow post](http://stackoverflow.com/questions/37849009/warnings-when-executing-tensorflow-sample-code-mnist-with-summaries-py))
They are most likely due to differeces between versions of libcupti, and should be harmless (other than the undesirable log spam)
(static const bool kLittleEndian = true; )
@girving - the issue is that we don't use that flag in the PNG decoder, we use the IS_LITTLE_ENDIAN flag, which is only defined by some transitively included google-internal header.
@beopst - huge thanks for providing an easy way to reproduce with your bug report!
Cc @zheng-xq in case he knows better
However, this is quite annoying so I will make an exception and fix it for Safari :)
Hi haraldschilly,
Adding @yuanbyu@, @ebrevdo.
We don't support taking gradients of nonscalars, so I wouldn't expect this to work.  However, this particular error message is pretty confusing.  @yuanbyu: Is there a way we could improve this error message if someone tries for a Hessian in the naive way and control flow is involved?
I've asked question here with all the commands I've used to get into this weird state:
@girving can you assign to appropriate folks?
Seems reasonable to add another alias so that dialated_conv2d == atrous_conv2d.  You can add the alias into nn_ops.py like we do for other kinds of renames here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L1212
@gpapan what do you think?
@ry I prefer atrous_conv2d.
Indeed - your dynamic bidirectional rnn PR is the one i'm talking about ;)
ping for @ebrevdo
> https://github.com/notifications/unsubscribe/ABtimwGtHxN1ISsuG_2iujnK03rBfYErks5qJhMGgaJpZM4Ip5gv
ping for @ebrevdo
@Shaikjalal Are you sure it's getting stripped out? You should be passing bypassing the DecodeJpeg node completely on Android.
@kuza55 Agreed that the code seems to imply we support jpeg on Android, which is inconsistent with the fact that we don't. I'll take a look at providing a better error message in the invalid configurations.
@kuza55 jpeg.h and png.h have been updated to explicitly exclude mobile builds so that it's clear we don't support them there.
@Shaikjalal What's the command line you're giving to strip_unused? It sounds like it's not reading from a valid text-format protobuf file.
@maelp Sorry, I'm not sure what you mean. AFAICT libjpeg and libpng are not included in any mobile build. The makefile-based iOS build uses https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/gen_file_lists.sh to create the source file list, and this specifically excludes png and jpeg files.
@sun9700 Thanks for reporting. However, we should fix it in data_feeder instead. Some recent changes broke the example and [this test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/tests/test_data_feeder.py#L137) is not checked on Jenkins since we don't have it installed there. @ilblackdragon @martinwicke Moving forward, are we planning to test those on Jenkins? What's the status on data_feeder refactoring?
@Nessphoro that answer is incorrect.  RNNs are a stable API.
/usr/local/cuda/lib/libcudart.7.5.dylib
dyld: warning, LC_RPATH third_party/gpus/cuda/lib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
dyld: warning, LC_RPATH third_party/gpus/cuda/extras/CUPTI/lib in /Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path
File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/__init__.py", line 23, in <module>
File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/__init__.py", line 48, in <module>
File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/Users/yaroslavvb/tfimmediate_fresh.gpu/tensorflow/_python_build/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
> > https://github.com/notifications/unsubscribe/AABaHCqorwoJnQtIockTMRBWMr32Nqsfks5qHb3cgaJpZM4IpDEQ
@kashif 4 days old is pretty recent, seems likely problem is somewhere else
We've had problems with environment variables in bazel test before. :/
@kashif, thanks for getting to this quickly.
Thank you @kashif!
@Mistobaan is there something in particular you would think should be triggered automatically?
Closing as infeasible as @jendap discussed.  @Mistobaan: Please let us know if there are particular things we should fire off automatically.
uh, good question.  @ebrevdo
On Sat, May 28, 2016 at 12:39 AM, baoblackcoal notifications@github.com
> baoblackcoal@hotmail.com
> CC: baoblackcoalmailto:baoblackcoal@hotmail.com; Author<mailto:
> https://github.com/notifications/unsubscribe/AAKtfjEjaK7YySE2gCxVLAQtokghiAWcks5qF_EmgaJpZM4Io_G2
Illia Polosukhin
@ilblackdragon @terrytangyuan do skflow examples not generally come with tests?
@vrv Ping. @ilblackdragon We'll probably need to update the examples if we are deprecating things soon.
-rw-r--r-- 1 root root   1649726 May 24 18:12 /usr/local/cuda/lib64/libculibos.a
-rwxr-xr-x 1 root root  36816424 May 24 18:12 /usr/local/cuda/lib64/libcusparse.so.7.5.18
yaroslavvb@lenin:~/tfimmediate_src.gpu/tensorflow$ bazel test -c opt --config=cuda  --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/python:batch_matrix_band_part_op_test --test_output=streamed
yaroslavvb@lenin:~/tfimmediate_src.gpu/tensorflow$ bazel-bin/tensorflow/python/batch_matrix_band_part_op_test
@caisq, @gunan: Do you know what's going on with the ci.tensorflow.org failure?
@ibab: Looks like you'll have to drop the 1 argument test since numpy doesn't support it for the version Jenkins uses.  Ours should, though.
Ready to merge @girving  ?
@vrv: Yes!  Thank you for yet another contribution @ibab!
Somewhat related, Select gradient is also failing the test when there's GPU
I should have a PR for the cuda autoconf later this week.
There are basically two images:
Right, the .so file is the native library that contains the actual Tensorflow code. Eclipse can't do all the building by itself (unless you configure extra build steps to handle the native libs).
LGTM, thanks!
@3XX0 what is the nvidia oppinion? We should finally fix this. It is getting back too often.
- @petewarden
cc @ebrevdo @mrry @josh11b
@danmane: Unless you're planning to work on this soon, let's mark as contributions welcome.
There's no Java API for Tensorflow yet, so this is not possible to do directly.
BTW, when tracking capabilities through code, one thing to keep in mind is
cc @kchodorow
@kchodorow btw, perhaps related, at HEAD I'm seeing following ominous-looking warnings
BTW, current instructions for latest bazel are a slightly broken:
> BTW, current instructions for latest bazel are a slightly broken:
It looks like your embeddings have NaNs, most likely caused by your model training not being stable, which I suspect is not a TensorFlow bug.  You might get more help on StackOverflow for non-bugs like these (unless you can demonstrate to us that there's a bug here).
@ebrevdo should still look at this though
``` python
> merged with 1429abd
@zhongzyd is a prior committer and has signed the CLA, so this is fine.
Here is my code:
It's a bug indeed.
@KyungJunAn Could you please clarify how it doesn't work?
Cc @mjanusz
@ebrevdo, @yuanbyu  to let me know if this is a feasible feature request
@girving is this just a doc bug?
> yesterday, my codes might be awkward!
@ebrevdo: Is this fixed?
``` python
cc @mrry in case I'm misunderstanding something.  You probably want to reset the cluster and/or scope the variable names if you intend to reuse variable names across multiple runs.
@danmane: Can you comment on this?  It seems like a reasonable thing to mark "contributions welcome", but we should get your input first on how best to express it if so.
@jendap: Want to comment?
Switching to @caisq.
@benoitsteiner @rmlarsen: Benoit, Rasmus, are there other configuration options for Eigen that could help improve the performance here? Would any of the post-0.8 performance improvements be likely to help here?
The threading issue is suspicious.  @rmlarsen, @benoitsteiner: Any thoughts as to why turning off threading would make Eigen 10x faster?
@cgel: Apologies for the brief hallucination: what I said above is of course wrong.
The longer version is that most components in TF are registered using C++ macros that look like REGISTER_KERNEL("SomeOp", SomeKernelClass).
> https://github.com/notifications/unsubscribe/AAjO_UFDiO5H3CEANTW5aR_H8luBttf_ks5qH8FPgaJpZM4IjXMh
I thought it is there for ios. More pain to maintain macs. Ok, I'll leave macs to caisq or gunan ;-)
If you grep third_party/gpus/cuda/include/cudnn.h, do you see the same thing? You could also try to set LD_LIBRARY_PATH to point to the right library.
@hunkim: The commit author is currently "Your name" (you can see this if you do `git log`).  The CLA check won't pass until you fix the author to your email address.
@hunkim: Can you squash?
@hunkim: Thank for the fix!
As to why TensorFlow doesn't do this automatically, there are clearly [lots of different ways to do batching](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline), and TensorFlow can't reliably infer the user's intent. Therefore, we provide higher level libraries to allow users to build the appropriate input pipelines.
@orome: Unfortunately I think the feature request to have TensorFlow automatically shard ops is too broad to leave as a Github issue.
Thanks @alrojo for the upcoming fix!
@caisq: In that case I'll close this request, because it seems spurious.
Can you add a `complex128` version of the the binary op version of `testComplex64Basic` in `cwise_ops_test.py`?
Looks like `complex128` `tf.conj` needs a test too (copy the `complex64` one).
Unfortunately, TensorFlow doesn't overload the `==` and `!=` operators: they are the same as `is` for `Tensor` objects in Python.  However, it's worth refactoring that routine so that we do test `equal` and `not_equal` for the two complex types.
Putting `complex64` and `complex128` next to each other is good as he suggests.
@ibab: What's the error message with `__ldg` on `complex128`?  We do want to preserve `__ldg` semantics here since it can improve performance, so we probably want to write a little wrapper around __ldb in `cuda_kernel_helper.h` that calls `__ldg` twice for `complex128`.
@ibab: If you're unable to run GPU tests, it's going to be hard to iterate, so it may be worth splitting the changes out of this PR.  I could take over that bit of it if you want.
@ibab: Cool, `float4` sounds perfect.  I'll take a look.
@ibab: Please push your rebased and squashed version (reduced to one commit).
``` python
A few experiments to try:
atrous_conv(input, rate=rate, stride=stride)
atrous_conv(subsample(input, stride), rate=k, stride=1)
To sync @nsthorat
@vinhqdang you'll just save your map somewhere
I think the most realistic code for generating TFRecord files of Example protos is here: https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py
LGTM, thanks!
Assigning to @danmane in case there are any other tricks you could use.
On Wed, May 18, 2016 at 9:59 PM, ZhuFengdaaa notifications@github.com
name: GRID K520
Is this a misoperation?
I am pretty sure this is misoperation. Closing the PR.
> So is there something wrong in my multi-machine code?
replica_id = FLAGS.task_index, total_num_replicas = num_workers)
``` python
@mrry: How does one set a variable without creating a `tf.constant`?
@girving: You can initialize it from a `tf.placeholder()` and feed a value when you run the variable's initializer op. There's no sugar for it, but something like the following should work:
``` python
@wb14123: A Python loop doesn't support automatic gradient computation, which is the typical reason to use the fancier control flow ops.  I'm going to close this for now, since questions about how to use TensorFlow are better suited to StackOverflow (use the `tensorflow` tag).
ping for @ilblackdragon  (i hope you are getting these mentions)
Come to think of it, I don't know how one would do that in TensorFlow. @girving do you know who wrote the tutorial and whether they had a specific trick in mind for locally connected, non-convolutional layers?
[chaowei@node07 tensorflow]$ export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'
[chaowei@node07 tensorflow]$
`[chaowei@mgt ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 6.5 (Santiago)`
[chaowei@node07 gcc-6.1.0]$ gcc -v
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/gpfs/home/chaowei/software/gcc-6.1.0/libexec/gcc/x86_64-pc-linux-gnu/6.1.0/lto-wrapper
Configured with：./configure --prefix=/gpfs/home/chaowei/software/gcc-6.1.0
Thread model：posix
@333caowei: Where is your `zlib.h`?
@ilblackdragon: Is this a duplicate of #2167?  If so please close as such.
This kind of question is better suited to StackOverflow or an email to discuss@tensorflow.org.  It is not an issue with the TensorFlow code.
Actually, by "this operation" do you mean the whole thing, or do you want to allocate a `Z` and do a bunch of separate scatters into it before deallocating it?
@altaetran: We could certainly make a deallocation op, but I don't think we have one at the moment?  However, using it would be somewhat awkward.
@yuanbyu: Do you have an ideas?
Judging by #2367, it appears that @altaetran also requires gradients for this operation. It sounds to me like a functional op would be preferable for this purpose.
@ilblackdragon @martinwicke This may have conflicts with my last PR with baseEstimator
Change looks good to me, thanks!
@ebrevdo: Is there a way to get both bits of functionality in the same routine currently?  If not, what would be the best way to support it?
@kangxin That's strange. Based on the output from --inspect it looks like TensorBoard is detecting the scalar and histogram data.
Hi @kangxin,
It looks suspiciously similar to #2607, where someone reported that the demo instance at https://www.tensorflow.org/tensorboard/index.html#events is not showing them events or histograms, so maybe it is a problem affecting a particular browser/OS.
ping for @girving and @yaroslavvb
@girving Correct, it's a useless queue. With my "convert_to_tensor" suggestion it will fail with "tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: min_after_dequeue 4 must be < capacity 4" which is more informative than original division by zero in Summary construction. Throwing explicit ArgumentError sounds good to me too
I think that I have fixed the problem in Eigen with these 2 commits: https://bitbucket.org/eigen/eigen/commits/1ba80c847ac312333e13f2274c6e69d4654966d9 and https://bitbucket.org/eigen/eigen/commits/0e3f9602b2aa787d851c6aa85930fa1dcac7f693
@yuanbyu @keveman
@adarob using `tf` or some other name is up to the user, the framework itself doesn't change anything in the original `tf` or `tensorflow` namespace. You could use `tfi` instead of `tf` for instance. I've been using `tf` in order to reuse existing tests, but run them in immediate mode, but when you want to mix things, I would use `tfi`. Also, Python's symbol resolution happens during module loading time, so when SyntaxNet is loaded it remembers what `tf` was during that time, so if you redefine it later, it won't be affected.
Cool, thanks!  This used to not work because we compiled an Eigen version of contraction which didn't support double, but cublas supports it, so we're all good.
``` python
LGTM, thanks!
``` python
@ajaech is absolutely right. This is intended behavior: `tf.random_uniform()` and the other random ops produce new random numbers each time they execute, and each call to `{y,z}.eval()` will execute the random op another time.
... lots of complicated graph construction code ...
``` python
``` python
OK, it looks like `ipdb` does something strange to `sys.modules`:
``` python
I don't quite understand your `GruCell` example. Could you provide a concrete example?
Is it a python compile error or a TensorFlow graph build error?
I don't think you are using too many files, however using large amounts of data makes it more likely to expose underlying bugs. The error indicates that "bottleneck_string" contains some non-numeric characters. Not familiar with that part of code, maybe @petewarden  has an idea how non-digits could've snuck into `bottleneck_string`?
But for 600k you probably will need several passes to get convergence (a
@sesse: I'm happy to review if you want to make the change, but won't be able to work on this soon.
+ebrevdo@. One way to do it is to extend TensorArray a bit so we could convert between a (python) list of tensors and TensorArray.  It would also be nice to change session slightly so we could return a list of tensors for a single output.
@jihunchoi I don't know that anyone is working on it currently.
> https://github.com/notifications/unsubscribe/AAjO_baHZU43Sz4HYMkJsqTYQ2Bq-X_5ks5qJhH9gaJpZM4IaJBS
@edi-bice: Let us know if you want to send us a PR with `-lm`!
``` python
@petewarden @vrv Could any of you run the Jenkins again? I just found a huge portion of repeated code - maybe from conflicts not being resolved correctly during internal branch merge. Also cc @ilblackdragon
@ilblackdragon Good to know. I'll take a look at graph_actions later when I get a chance. I've reverted the changes. Let's just keep this PR to be removing those repeated lines (tests passed since the repeated code are inside a function block). Could you take a look and merge this first or people will have some issues?
cc: @ilblackdragon
@ilblackdragon Not so much special handling is needed. I have added an example and a test case. I think this is ready to merge.
Could you pass the absolute path to your python, such as?
name: Quadro M2000M
Aborted
Assigning to @poxvoculi
The code base is fast changing. The only way to ensure it works for everyone is to maintain a high standard of the code quality. One important aspect is to make sure testing is done well. In your case, can you add proper test cases covering the operation this PR touches? It should not be too complicated. Most relevant tests should be in python/kernel_tests/cwise_ops_test.py. You can grep test cases for complex64, most of which is just matter of extending the type list w/ the new type.
@colah did you make this one?
Which versions of bazel are you using?
Adding @mrry to point out if there are better ways.
@girving Will this be a backward-compatibilty issue?
Adding @girving to give guidance on how to deal with the backward compatibility test.
In general, it is a good idea to run the TensorFlow tests locally.
Adding @a-dai to comment.
However, since this is about tensorflow/models, not tensorflow/tensorflow, it should be refiled at the right repo.
Hm, could it be numpy display issue? What if you do
`[[-inf -inf -inf -inf -inf]]`
![bad-sun](https://cloud.githubusercontent.com/assets/23068/15034991/3019a0e0-1231-11e6-9697-7f26bfd28253.jpg)
> complex64, int64"
difference could be quite large.
> library libcublas.so locally
> library libcudnn.so locally
> library libcurand.so locally
@alexlee-gk, the problem could be attributed to small differences introduced in the non-deterministic behavior on GPU. Or some numerical instability due to the precision difference between CPU and GPU.
@alexlee-gk, thanks for bringing this issue to our attention.
@mrry, any idea here?
This is a pip problem.  Running
@yuanbyu: Should I mark this contributions welcome?  It seems like it would be nice if `tf.add_check_numerics_ops` just did the right thing in this case.
Adding @zffchen78 to comment whether it is okay to int64. It is possible that you need to change several places to make this work.
> @danmane https://github.com/danmane yea sure. I'll check it out tomorrow
@danmane how different is this from google style? I'm happy to use a standard if there is one.
breaks.
Most likely we don't work properly for cudnn6.5, since it's now two generations old.
Cc @jendap @caisq
Yeah it's currently not possible. I'll submit a fix soon. cc: @ilblackdragon
@mirikle It requires different mechanism to make it work. I'll try fix it later when I get a chance.
@ilblackdragon Could you re-open this and assign to me? I'll try work on this soon.
Adding @martinwicke.
@keveman, do you know what could be going on?
@fcole90 Do you mind trying this wheel package for protobuf?
1. Use Tensor form and avoid the loop as much as possible. Give each node more things to compute tend to improve performance.
In Cudnn documentation about this particular error:
CUDNN_STATUS_MAPPING_ERROR
@agupta83: What platform are you on?  I'd like to know if it's Ubuntu version specific, since @waTeim says it goes away at a later version.
@caisq: Can you take a look as someone more familiar with pip-land?
@ebrevdo next time squash before merging
``` python
@mrry: `tf.reverse` is a better way to reverse a Tensor that extracting each component and manually reassembling them in the opposite order.
@maniteja123 please feel free to take it.  Besides all of the above suggestions, I'd imagine adding an additional bool flag (`reorder`?) to the Python function can be useful, with default to True.
On Wednesday, June 15, 2016, Maniteja Nandana notifications@github.com
> Also the implementation AFAICU would be something like
@taylorhughmorgan, could you try the latest TensorFlow again, and see if you still have this problem? Thanks.
Nice catch. Very obscure, much wow.
Illia Polosukhin
@ilblackdragon: If this is fixed at master, can you close it?
@girving someone just needs to retrain the model. @shlens FYI
Ironically, I did the inverse to make classification work in the deepdream
@altaetran: It's conceptually straightforward: one has to go through and sanitize all the occurrences of `//` in `math_grad.py`.  Unfortunately there are a bunch of them for a variety of different ops, and the fixes look different in the different cases.
@altaetran: To make my initial comment make sense: we want to produce nice error messages even for malformed ops, so we want to fix the underlying divisions by zero regardless of whether we fix the gradient code.
@rbharath: Ah, yes: the fix I just made is Python only.  However, we don't have any setup for using different Python alongside C++, and in any case you'd have to carefully cherry-pick just my change on top of 0.8.0 to have a hope of succeeding.
cc @Mistobaan
Wahh @caisq what happened
Oh @poxvoculi introduced a problem during the merge conflict, nevermind.
No good way really, besides running and seeing if you run out of memory.
@ilblackdragon Should transform or fit_transform be preferred here?
@poxvoculi transform should be used usually - as in serving you don't actually know what would be mean / std.dev of the distribution.
Closing this for now; as it sounds straightforward for you to create this custom LSTMCell in your own repository.
The results from `simpleP2P` seem like the problem is a CUDA configuration issue and not a TensorFlow-specific issue. I'm closing this for now.
@Froskekongen The problem is that we're not sure if that cxx_flag will break other people.  Do you have a better sense for what it does, and whether other breakage is likely?
Hi harpibot, any update on this? Let me know if you still see the problem!
I think this is a known flaky test.
Yes, it would be very useful to have better tools to analyze and predict TensorFlow program costs, in terms of time, memory and other resource needs.
TensorFlow programs can be analyzed both statically and dynamically.  Your questions seem to me to suggest that maybe the properties in which you're interested can be discovered through static analysis.
@ziky90: Does @callicles's fix solve your issue?
@ppwwyyxx, @liuyipei, @yosinski, in order to debug this problem, it is very useful to list all threads information, even better with call stacks, at least for the unique ones. Also please run the same experiment with environmental variable "CUDA_LAUNCH_BLOCKING=1" set.
Ah, also the shape is ambiguous in cases with `stride > 1`.
there are conflicts :(
@liumilan: It does sound like you'll need to install from source.  I'm going to close this bug for now: if you still have issues please reopen a separate bug about the problems installing from source.
@mikowals can you please resolve conflicts?
@mikowals There's still conflict
epsilon=1e-5,
epsilon=epsilon,
scope='learn_bn')
Sorry @mikowals, this PR dropped off the attention list.  @ilblackdragon what do you think about his suggestion?
@girving, a PR is welcome.
Also has different random seeking functionality and is widely used in scientific community.
Any thoughts? Should I do that in a following new PR? @ilblackdragon
@ilblackdragon Conflicts fixed, example added, delaying refactoring after internal changes.
@tensorflow-jenkins please test this!
@yuanbyu Could that be related to the recent gradients code in while/loop?
On Apr 27, 2016 11:32 AM, "ofirnachum" notifications@github.com wrote:
> Is it actually reading an unwritten slot? At least in the forward stage,
On Apr 28, 2016 7:29 PM, "ofirnachum" notifications@github.com wrote:
On Apr 28, 2016 7:49 PM, "ofirnachum" notifications@github.com wrote:
@ebrevdo, @yuanbyu: What's the status of this?
Cc @aselle since `unravel_index` sounds vaguely index related, but I'll mark this contributions welcome for now.
@zheng-xq @fayeshine any updates?
queues.  That said, this approach has a certain appeal to me.
@caisq  -- seems like more platform python issues (can't import default/ dir).  I assume that's fixed internally and just needs to be pushed?
@yuanbyu - any ideas?
``` python
Although this behavior is consistent with the dataflow model of TensorFlow,
It seems to have been a deliberate stylistic choice not to define **repr** within tensorflow.  Not being a python enthusiast myself, I can't comment on the virtues of that decision.  If you feel differently, perhaps the tensorflow discussion group would be a good place to raise the issue.  [https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/discuss]
@leary-google any ideas why you did it the other way?
Thanks - this does look suspicious and I see the same behavior on my machine.
@danmane: Any chance this is obviated by improvements since then?
I am not famiilar with Caffe but:
linkopts = [
"-landroid",
"-ljnigraphics",
protobuf (3.0.0b2)
@martinwicke: Is there any way we can shield people against these torments, or at least generate nicer errors?
@aselle is there a way to find a mismatch at runtime? Maybe by comparing the size of an unordered_map returned from a function in each version of the library? If we can identify the problem we could at least die with informative last words.
@lichan, do you think stricter symbol visibility solves this problem as well (similar to #2646)?
ping @ilblackdragon @terrytangyuan
@jendap: Do you know if this is still an issue?
@weiliu620, @ethereon   Are you also running a multi-gpu config?
@@ -93,6 +93,7 @@ string DebugString(const Tensor& x, const Tensor& y) {
It looks like it's got enhanced with some logic which I don't fully understand. @girving -- do you see any scenarios where "import_array()" isn't going to run?
@chengdianxuezi, @mouendless, @fxia22: What versions of numpy do you have?
@caisq for pip build info
It does look like an old numpy is creeping in from somewhere.
https://github.com/tensorflow/tensorflow/pull/2114 fixes the `py_func` weirdness, but I still don't think it's related to the original issue.
@kanwar2preet: Is it possible to get a stack trace from that crash by running Python inside gdb?  Also, unfortunately I didn't quite follow the set of commands that work and do not work from the above description.  Can you say which commands you mean again?
That does seem like a pretty terrible bug in scipy though?
Probably because that updated scipy or numpy as a side effect.
> https://github.com/notifications/unsubscribe/AAjO_Y81C43-2fWqec_YERAKYwrI2jPvks5qGQ8agaJpZM4ILdja
@amineHorseman If importing numpy doesn't fix it, please file a different issue.  It is probably unrelated, unless you have reason to believe otherwise.
Well "personally" is a little harsh -- nobody has yelled at me on the street about this yet. But it is a support burden we'd rather not go back to. I'm sad about it too.
@gilberthendry
@jcyk, @arlejeun: Does updating to a more recent version fix this for you?
exception.
Shape and type are different things.
@caisq  feel free to remove self; just wanted you on the participants list so you see relevant updates here.
contains enough complexity in one fused op that it's already a stretch to
On Fri, May 6, 2016 at 2:47 PM, Eugene Brevdo ebrevdo@gmail.com wrote:
@Mistobaan i don't think so - @zheng-xq for context.
What's the status of this?  What is blocking this  cc @wchan @ebrevdo @caisq
> https://github.com/notifications/unsubscribe/ABtimzX8zGVJPDNsPQZIkqXBN6njngvjks5qJhPggaJpZM4IJMkM
@concretevitamin: Are you working on a out-of-place version of scatter?  I'd prefer that confusion matrix was implemented as a Python wrapper on top of something more fundamental like that.
@lucasmoura: Your original interpretation was correct: it's better to remove the C++ implementation and just have the few lines of Python.
@lucasmoura could you paste the errors somewhere?  `tf.sparse_add()` should support both S+S and S+D.
> Ug, build timed out. @martinwicke https://github.com/martinwicke:
On Wed, May 11, 2016 at 20:39 stationedabroad notifications@github.com
> Hi - does anyone know what the path is to download using anaconda for a
On Wed, May 11, 2016 at 10:53 PM stationedabroad notifications@github.com
> no way to install tensorflow on windows through anaconda? the message i get
I will. But let us first see if it clears up the question(s) by lahwran@.
Although, if what you want right now is to visualize your pbtxts without needing to turn on TensorBoard, you can already do that: navigate to [https://www.tensorflow.org/tensorboard/index.html#graphs](https://www.tensorflow.org/tensorboard/index.html#graphs) and click on `Upload: Choose File` in the left sidebar.
@ultrons, There's a way to show graph visualization in IPython notebook,  see the function "show_graph" in Alexander's deep dream [notebook](http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl
LGTM
Jenkins, this this PLEASE!
Man, sometimes he's really stuck up.
surprising choice.
@rmlarsen might be good persons to look at SparseTensor-related changes!)
> surprising choice.
> > https://github.com/notifications/unsubscribe/AAkLHld0QxdDB5qSAFGmU2MU_9MeyezPks5qImfqgaJpZM4IIer2
> https://github.com/notifications/unsubscribe/ABtim3ryad0V6lkPzSXxdLwEdKN_K_ecks5qImm3gaJpZM4IIer2
@jmugan, I agree with you that the model is probably too big for your system.
@pkmital, could you please file a bug if you believe the memory leak is in TensorFlow, and not in the way your model is constructed?
``` python
somehow we killed jenkins, going to wait for it to restart to try again.
Can you clarify on what "dimension variable inputs" means?
LGTM. All PR tests passed.
I don't know what spyder is, and the fact that it runs ok in plain python makes me think this is not actually our error.
(Low level code is here:
Rasmus
Sweet, tests pass.  @ibab: Shall I go ahead and merge?
I wonder if this is related to ptx-sass compilation somewhere.
@beniz, thanks for offering to help. What is the command line you are using for compiling your opencv_tensor binary? I doubt that the problem involving `dlopen` and the one you are facing are the same, but you never know.
-ltensorflow \
It's a very complex lighting setup, that's for sure. I think it can only be explained by participating media.
It seems to me you must have an older TensorFlow somewhere on the system, could you check that it's clean?
This is a pip problem.  Check out #2212
``` python
@mrry: what is the exact problem?
``` python
Rasmus
Hi @oweingrod,
I'm not using Cuda myself, so I'm not sure of the details here, but wanted to get it logged so somebody more knowledgeable can take a look.
@kmuriki What location did you enter for python while running `./configure`?
@kmuriki Gentle ping. Please let me know if this is still an issue.
alright then!  LGTM.  @tensorflow-jenkins: test this please
(difference seems to be gpu-working vs. gpu-slave)
@ilblackdragon, Saver.save() takes a save_path as an argument. You just need to change TensorFlowEstimator's save() in estimators/base.py to pass in save_path.
In the latter case, it's possible that 30% of machine B is being consumed by proxying RPCs between the client and machine A. You might be seeing latency because the "queue runners" that prefetch input data run sequentially and now have two network round trips per record, and you're now "I/O bound" (not actually bound by the disk, but by the steps that have to run to populate the input queues). You could try adding prefetching threads to the input pipeline to see if that improves things. (I'm assuming you're using a version of [`cifar10_multi_gpu_train.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py), so no data is being fed or fetched in each training step.)
That's surprising. Are you fetching anything in the call to `sess.run()` that you're measuring?
epsilon is a good solution.
representation will be modified.
> in SpecialFunctions.h in eigen use equality tests. For example, igamma
> implementation of the zeta function. Is there something special about the
I was looking at the eigen implementation and saw that there may be a missing registration for the CUDA (float4 or double2) packet ops.  Can you revisit and make sure you added all the correct items?
I merged #1829 and #1832, but conflicts.
@ebrevdo: Should we mark this contributions welcome, or are you working on it?
Rasmus
@geoffreyi pbar.
It would also be necessary to pass in a worker-specific device prefix to the Collect() method of the GPUTracer for use [here](https://github.com/tensorflow/tensorflow/blob/545df470168f52a369b5f1510f26ad001f48c650/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L550).
> https://github.com/Yunlong-He: Do you know if it's still an issue?
> https://github.com/notifications/unsubscribe/AAjO_cp12zLYLXhVar3BSorIP-5Pobupks5qJLmMgaJpZM4ICoAr
@gpapan I assigned you based on your email to @yaroslavvb. Thanks!
You are very welcome @fyu! Hope that people find this feature useful.
@dominichamon, @ebrevdo: What was the result of the offline discussion?
I think it's a bug in your code rather than in TensorFlow. Your confidences converge to 0's and 1's, then you take a log of 0 which gives -Inf, and multiply it by 0, which gives NaN
Could this be related to #2646 ?
@mrry, @zheng-xq: This whole "hand off" thing doesn't seem to work very well. :)
@anuragkr90: Is your issue different from @avati's?
@anuragkr90: A floating point exception typically means integer division by zero.  Could you recompile the code in debug (pass `-c dbg` to Bazel) and rerun gdb to see where it's dividing by zero?  Otherwise we'll probably need a minimized test case for reproduction purposes.
@anuragkr90, I am not able to reproduce this problem locally.
File "/usr/local/google/home/andrewharp/.cache/bazel/_bazel_andrewharp/7d6b411acf30faa7da8a8cd3f21cf053/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/user_ops/ackermann_test.runfiles/tensorflow/core/framework/graph_pb2.py", line 6, in <module>
> Also- with this changes, should I make tweaks to the OneHot definition in
@samjabrahams I've been using `one_hot` for a few weeks without problems. I recently reinstalled the master branch of tensorflow with `sudo pip3 install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl` but there doesn't seem to be any update to the signature of `one_hot` - it still takes only int64 tensors and required manual on_value and off_value.
Ok, I installed the nightly build, which provides default on_value and off_value. However `one_hot` still demands `int64` for the input. If I pass in `int32` I get an error:
``` Python
@meereeum Any updates?
@meereeum: Closing for now since it seems like it might have been caused by a malformed use of `import_meta_graph`.  I'm happy to reopen if it's a persistent issue.
@ebrevdo: @meereeum's example with `y.name == 'foo_1/Variable:0'` looks like a bug.  Could you take a look?
Yes, we don't release anaconda wheels (yet) :(.  You might want to try building from sources.
Thanks @benbarsdell: want to send a PR to fix either one?  Also when I wrote that up, it wasn't clear to me how a 'filter' could have a format of NCHW, which is why I used the data layout.  Filters don't talk about batch, just input and output channel / height / width, so I wasn't really sure what to do there.
Hi @eriophora,
Thanks for the catch and the fix @fayeshine!
It is possible that TensorFlow is choosing a bad placement of operators across your two GPUs, and slowing things down by copying data between the GPUs. Would you try restricting to a single GPU:
I think we should pull this in as soon as these comments are done and we know there's a way to build & run the tests.  The one thing I'd ask is that you put a comment in the example imagenet program that says this isn't really the right way to do it - pending a way to modify the graphdef, it's much more efficient to directly couple the input and recognition path so that the image doesn't get copied back to python/CPU between the steps. :)
Curious to see what this does to Jenkins - will it break the build or be happy?
I know very little about go, so that sounds fine, but a go expert should probably chime in here.  In some future world we'll hopefully have shared library distributions of the C++ core that doesn't require building during go get, and then this can probably be cleaned up.
I concur - go gettable is critical, and making someone manually install tensorflow in a special way first is probably a reasonable cost.  LMK once that's there and I'll test out the workflow also.
Alonso, thanks for your patience with this!
--cpu=armeabi-v7a \
--cpu=armeabi-v7a \
jhspaybar, I believe that you should fix this on the python side of things.
@lukemetz, thank you for the contribution!
``` python
``` python
> Hi ebrevdo ,
> Ah, now i get it, you want to use a SparseTensor since the target
@ebrevdo @raindeer What's the status of this issue?
Great!  I need a couple of days to import your changes on our end.  In the meantime look at my recent push of igamma/igammac as a reference for wrapping zeta/polygamma.
Is your username popo or poporo? I notice it changes in the two printouts you've provided.
This is a bug.
@jendap, @caisq  to help validate, if possible
Can you run `uname -a` in your terminal and report what gets printed?
@Dominator008, thanks for your comments. We did start out with maintaining both gerrit and github and accepting changes via both systems. However, the maintenance burden became too much, and with the limited human resources we had (have), we decided to focus on making the  github process as robust as possible. Gerrit indeed has superior code review interface, but unfortunately, we are unable to dedicate resources to maintain multiple sources of truth for TensorFlow code.
name = "androidsdk",
api_level = 23,
api_level=21)
@elanmart, feel free to reopen if you still see the problem with the latest TensorFlow, Cuda SDK and Cudnn library.
@ebrevdo: is it worth adding C++ tests that would catch this memory leak?  (It's interesting that the python one doesn't expose it).
@MisayaZ, it is true that the filter size [5, 5, 3, 64] entails five times more computation than [1, 5, 3, 64]. However, the actual run times depend heavily on the implementation, memory access to computation ratio etc. So there is no way to guarantee any correlation between actual computation and run times. Also, we make use of external implementation such as cuDNN for the kernels. Hopefully those libraries will keep getting better. I am closing this issue as we don't plan to do anything special in TensorFlow for this.
@fayeshine: can you try accessing https://storage.googleapis.com/download.tensorflow.org/deps/gmock-1.7.0.zip or is that also blocked?
From a quick look at the Slurm docs, it looks like it could well support running TensorFlow, since (i) there's [decent support for running Python jobs](https://support.nesi.org.nz/hc/en-gb/articles/207782537-Python). I don't have access to a Slurm cluster, so I'm going to tag this "Contributions welcome", but I'd be happy to work with you on this Issue to get your cluster running on Slurm.
has been compressed with SNAPPY:
@ebrevdo: why does he have to rebase to HEAD?  Do you think there will be a conflict in doing so?
@jerabaul29, generalizing convolutions to higher dimensions would be a great addition to TensorFlow, especially the GPU implementation. It will be great if someone in the community can take this up.
Thanks for the contribution!  IANAL, but I don't think this is needed -- you have to maintain at least the copyright year of the first distribution (2015).  Some resources online say it is okay to update on major updates / releases though, but I'd rather just keep the original year unless lawyercats suggest otherwise.   git history is a more accurate source of truth than what we put in this file anyway ;)
Hm, that is indeed weird.  Not sure why CUDA_VISIBLE_DEVICES isn't helping you.
@fchollet, can you please take this?
@darrengarvey Hi, are you building tensorflow/core:tensorflow_android_lib directly (as opposed to //tensorflow/examples/android:tensorflow_demo)? If so, you need to tell Bazel to actually build for Android, e.g. with the flag --fat_apk_cpu=armeabi-v7a
``` python
``` python
`grace_hopper.jpg  imagenet_comp_graph_label_strings.txt tensorflow_inception_graph.pb`
@martinwicke you have checked only mac0-slave, right?
@caisq can you upgrade bazel on mac1-slave, please?
@petewarden is this PR still active or can it be closed?
@mrry is probably the right person for grpc questions.
@cuiguoxin, you might have misunderstood the code. The code is initializing the backprop state starting from y(s). So it needs to look at y's inputs. If you think it's wrong,  do you mind constructing a test case to show it?
@fayeshine: Since we have a confusion matrix op as of #1999, is this obsolete?
@fayeshine: Note that `confusion_matrix` was implemented on top of existing TensorFlow ops.  Does that mechanism suffices for your use cases?  If not, can you propose a specific semantics of what you want?
@ilblackdragon: I can merge if you'd like (no testing needed it seems)
@andrewharp: is this a bug in bazel or a bug in TensorFlow?  If the former, have we filed a bug with them?
@Muaazbinsaeed
Yes, this sounds like it requires custom kernels to have reasonable performance, though I don't see the word "pointer" in the paper so I can't confirm.
@Kuntal-G Which version of Bazel are you using?
Also, other users have had problems with a trailing slash on paths, so I'd try removing the one on your ndk path and see what happens.
@hassanabidpk What version of Bazel are you using?
`$ bazel version
@hassanabidpk have you tried @kmader's solution?
https://github.com/WojciechMula/sse-popcount
SparseTensor SparseTensor::Slice(
const SparseTensor& input_tensor,
mdan is implementing SparseTensor slice this week.  I think there's some
On Monday, May 16, 2016, ebrevdo notifications@github.com wrote:
Try passing "https://github.com/tensorflow/tensorflow/blob/30b52579f6d66071ac7cdc7179e2c4aae3c9cb88/tensorflow/core/protobuf/config.proto#L35" set to true as an argument to your Session's config arguments, it might help a little, though it won't release memory, it just allows growth at the cost of some memory efficiency.
Not sure, you could try invoking python's garbage collector?
@danmane: Do you have enough information to look at this?
@Mistobaan -- I see other people requesting it, this would be a useful addition
It's a strange failure, let's retry the test.
the branch currently has conflicts, and yes, removing complex64 support for now is fine -- just leave a comment saying something was broken when you tried it, so others can pick up from where you left off.
@Mistobaan: ...apparently not. @caisq, @ebrevdo, @martinwicke: can one of you please kick off the tests?
Still seeing errors for complex64, and also for float16.
The one reason I can think of why the undefined symbol error could be happening for you is, pywrap_tensorflow.so is not dlopened with `RTLD_GLOBAL` for some reason. Look at `tensorflow/python/__init__.py` for these lines :
The performance is completely dependent on the implementation of your custom op.
thanks @ecobost for the correct info :)
@brantbzhang, could you try the latest TensorFlow, which enables auto-tune on conv and see if that is faster?
@lberki, do you have any ideas? It seems similar to https://github.com/bazelbuild/bazel/issues/698, but this is happening with Bazel 0.2.0
I believe that test should not be run on GPUs.  @zffchen78.
1) Are you building with -c opt too?
2) Pointers to logs would be helpful.
url = 'http://commondatastorage.googleapis.com/books1000/'
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
Hi @Mistobaan, unfortunately there are conflicts that need to be resolved.
@josh11b: In a perfect world the op spec would say you can't take the real part of a `complex128` and get a `float32`.  Reasonable to just ban that by not registering the kernels?
@ibab: If so, your solution is fine.  I'd change it to `Tout=in.dtype.real_dtype`, but otherwise good.  There will be no backwards compatibility issues since both defaults are the old values.
@ebrevdo can you please resolve the conflicts? Ideally rebase to new master?
On balance, those two changes gave us a lot more flexibility:
``` python
@rajarsheem, you'll have to post the errors here. In general, TF is tested on compute_35. While compute_20 may work for now, which I am not sure, there is no guarantee that it will keep working in the long term.
> ERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:212:1:
> /home/rajarshee/.cache/bazel/_bazel_rajarshee/0d043bf46cad9f31127eb8d06453610d/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/gather_op_gpu/tensorflow/core/kernels/gather_op_gpu.cu.d
have a few options:
- Modify
Unfortunately, Eigen starts to pick up warp shuffle capabilities recently,
@rajarsheem in this thread wants to support compute_20 device. However, since Eigen starts to use warp shuffling instructions, which is only available for compute_30 devices and later, it won't work.
@rajarsheem @marcdumon: What are your use cases for GPUs that only support compute capabilities 2.1 ?
@caisq: This fails only GPU_PIP, because tf.test is not available there, is that expected? It's possible we simply have to blacklist this test, but I may be missing something.
Ping @caisq
ping @caisq @martinwicke
:+1: for a different host
`url = 'http://commondatastorage.googleapis.com/books1000/'`
@rdipietro: Sorry this fell through the cracks.  I agree that `params` is an odd name, but it doesn't really indicate any special purpose: it's hard to think of a less specific name for a parameter than "params".
The Linux CPU test has failed //tensorflow/tools/docs:gen_docs_test. @iblis17 can you fix it please?
LGTM, thank you @iblis17
I don't understand, http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz is there, where does "403 Forbidden" error come from?
What is the error?
> `url = 'http://commondatastorage.googleapis.com/books1000/'
> 11 raise Exception(
Try updating your version of TensorFlow: 0.5 was the original release and there have been a ton of bug fixes and improvements since then.
``` python
Rasmus
Rasmus
at http://rebaseandsqua.sh/
Rasmus
(alternatively, we could have squashed/merged ourselves).
Also, the mac test failed with:
self.runFiniteDifferences(shapes)
Rasmus
@danmane: Want to comment?  I think we'd be happy to accept patches for this.
@mrry: Was there some plan to change `get_shape()` to be less verbose as well?
Thanks for this change @shekkizh! We appreciate the fix.
``` python
``` python
...however, either of these examples seems to work in NumPy (albeit with potentially wacky results, since NumPy converts `False` to `0.0` and `True` to `1.0`), so I'm not quite sure of the types.
``` python
``` python
@hrajanie Which version of Bazel are you running? The `imports` attribute was added in 0.2.0.
Yikes.  Good to know @aymericdamien, thanks!
The permission denied bit might mean that the permissions for that file are weird.
Hi @terrytangyuan @makseq @dansbecker @elqursh @dgboy2000 @cbonnett @ivallesp @frol @liyongsea,
Your code snippet is incomplete, since you haven't shown us the definition of `local3_value` and `centroids`.  There are also some strange bits, such as allocating `image_result_table` but never using it.
@girving: the problem here is that the program is constantly adding more nodes to the graph because it is mixing graph construction with graph execution.
Ah, right, forgot about our quadratic time behavior.  @MisayaZ: Move the `mat_result` and `reduce_sum` definitions outside the loops and you should be fine.
"accidental" may be a better word than "negligence". :)
Next question: what exception are you seeing?  It may just be my eyes glazing over, but it looks like the initial comment has a traceback but not the actual exception.
@amirkhango Is this still an issue with 0.7.1? Or are you deliberately trying to install 0.5.0?
This is caused by changes in the behavior of zeros_like (See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L641).
@ibab: Taking a look now.  As for `TF_COMPLEX`, let's have both `TF_COMPLEX` and `TF_COMPLEX64` with the same enum value (https://stackoverflow.com/questions/11412516/enum-why-can-two-different-enumeration-constants-have-the-same-integer-value).
@ibab: Actually, let's hold the kernel updates to a separate PR.  Adding just the dtype and associated framework support is a nice standalone step.
@ibab: Unfortunately the backwards compatibility test files have been changed since your update, so the test fails again for me when I try to merge.  Can you merge, regenerate those files, and then squash into one commit?  There's a bit of an unfortunate race condition here.
Incidentally, in a future commit we should add a note to `RELEASE.md` saying TensorFlow has complex64 support now. :)
@ibab: Do you mind if I pull this in and commit it myself (crediting you explicitly of course)?  Looks like our Jenkins setup is experiencing some issues, and I'd like to get this in before the compatibility test bit rots again.
@kashif: Actually the backwards compatibility test is failing, which does require an update on your end.  Please do this:
then they stop descending into the tree. You can blame me, because I added
Yes, `complex128` would be good to have, but I don't think anyone's working on it at the moment.
@ibab: Yep, it'd be `DT_COMPLEX128 =  18` and `DT_COMPLEX128_REF = 118`.
@sbyma Can you please verify and close this issue?
@Fhrozen did @sbyma's solution fix your problem?
Looks good once the C++ changes are removed!
pip uninstall protobuf
pip uninstall protobuf
@shengjt Thank you very much! Problem solved!
@shengjt Yes, I have modified the `tensorflow/workspace.bzl` as follows:
name = "grpc",
name = "grpc",
Hi, @fayeshine @myme5261314 @shengjt , I cloned the latest master branch [b1aeb4495bd10b3f1f0d4aed64880f8896fe990b] and modified the `tensorflow/workspace.bzl` with #1626, along with the following lines:
url = "http://denemo.org/~jjbenham/gub/downloads/libpng/libpng-1.2.53.tar.gz",
It's possible that TensorFlow may be better at this kind of flexibility at some point, so I'll leave this bug open.
Computing log Z can be rewritten in terms of regular matrix multiplies, but Viterbi is an interesting example. You need efficient way to  compute matrix vector min-plus product r=Av as  r(i) = min_j A(i,j) + v(j)
@danmane: Do you know what the issue is here?
Closing since the discussion indicates it may not be a TensorFlow specific issue.  Please comment if more information appears or file a separate issue.
This is probably more a question for the bazel team, but bazel clean is supposed to clean up all state to try to provide hermeticity in builds.
Unless you think this is a bug in TensorFlow, this question is better suited to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow)
@kalleknast That does seem like it could be a similar problem, but please file a separate issue anyway since we're trying to keep the issue tracker organized and this one has already been fixed.  Apologies for the bureaucracy, but keeping issues focused helps us not let fall through the cracks (and not get fixed).
@ebrevdo Is this still an issue?
This doesn't look like an issue with TensorFlow -- please comment if you disagree.
Just pushed b3dfff2a23b435f14cca7377f8e0e5ad7583b45e which added reduce_mean support for GPU, so that will no longer be the reason.  @vincentvanhoucke does this look good?
@colah: friendly ping?
Thank you for the submission @basveeling.
Flags is just a thin wrapped around argparse, in what way would it have to
Maybe worth trying cuda tools but:
That's really strange -- can you try installing in a virtualenv from scratch, to isolate the installation?
@keskival Not sure if this is the issue, but the 970 has a strange architecture with 3.5GB of fast memory and 0.5GB of slow memory - from what I've read it isn't recommended to go over the 3.5GB when running cuda programs. This might interact poorly with tensorflow's default allocation of all available GPU memory, although I don't see why you would get NaNs.
LG, can you rebase to master?
pip uninstall protobuf
Flaky test failed.  Merging.
@ebrevdo: I think configure is the way to go.
@ilblackdragon: Reassigning to you; feel free to adjust if that's wrong.
@danmane can you take a look?
On Feb 26, 2016 4:40 PM, "nryant" notifications@github.com wrote:
@ebrevdo: Assigning to you if something is in the works.
alquraishi: can you provide a list of things still broken at head of master branch?
``` python
merged
``` python
``` python
So this require the entire world to upgrade to bazel 0.2, right?
Some target in protobuf is missing a `srcs_version = "PY2AND3"` annotation. See
> ❯❯❯ bazel test //...
Can you update the commit hash for protobuf?
> https://github.com/notifications/unsubscribe/AAjO_VGti6nugRU9vmAFc_GOhwHn_B-1ks5qFh3NgaJpZM4HjOh8
We are sorry for the inconvenience @StephenOman!
head. Silly.
Hi @dgolden1,
That's a strange setup.  If you're going to build from source, can you grab tensorflow via git normally rather than checking pip into git?
@jackpaparian, you are right, you should be able to build a new op with Tensorflow binary pip installed.  Can you try installing the pip package from [here](http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) and then try compiling your user op? The pip package for r0.7 had some issues related to user ops. See #1569 for some related discussion.
@keveman, @josh11b
bazel-bin/tensorflow/python/tools/strip_unused --input_graph=inception.pb --output_graph=/tmp/stripped_inception.pb --input_node_names="Mul" --output_node_names="final_result" --input_binary=true
We'll need more information to help: what is the error you're getting "from MatMul"?
@Palang2014, a typical set up is to have all the variables on CPU, and all the computation on GPU. If you have multiple GPUs, it is a good idea to merge the gradients on them before sending the delta back to CPU.
@Mistobaan: I don't much like it either, but I haven't thought of something good yet.  Do you have suggestions?
Ah, got it!  `diag_part`.
pip uninstall protobuf
How do you build auc.so?
Feel free to open a new issue asking specifically for (SparseTensor, SparseTensor) matmul.
Agreed.  I think a final question for this - perhaps one that's deferrable, particularly if we put it in contrib - is:  What to do with Tensors once they're in Go?  A quick check doesn't suggest that there are immediately obvious libraries to use beyond 2D matrices, but I might be missing something.
@danmane This looks like a tensorboard dependency, can you verify?
Yes, that is a TensorBoard dependency. It looks like something went wrong in the connection to GitHub while Bazel was downloading plottable (unexpected end of file from server).
Can you try re-building and see if it works? If it still doesn't work try `bazel clean` and try again?
Thanks for this contribution @rekhajoshm, but we don't want to introduce command line flags to our library like this -- most options should be passed via ConfigProto or another proto.  We could maybe add a LogOptions structure to hold options related to logging.
+++ b/protobuf.bzl
+  if "visibility" in kargs:
@flow-ryan: Thank you for the instructions, but unfortunately this sounds like a bug below the level of TensorFlow, either in Linux or the GPU driver.  Since it's already fixed in more recent Ubuntu, I'm going to close.  We'll happily accept a PR if you find a reasonable workaround for the problem, though.
Mysterious. I don't know what cv2 does, but since you are building it with Cuda, it could be preallocating GPU memory (similar to what tensorflow does). The symptom is decidedly odd though. @zheng-xq, do you have an idea?
From your description, it is very hard to tell what, if anything, is wrong with TensorFlow, so this is probably a better question for the mailing list, or, if you have a more specific question about using multiple devices, for stackoverflow.
``` python
(This is a weird edge case @willnorris)
@danmane: what do you think?  Originally they were given names (which led to unused_var lint errors).   Do you still think it's worth capturing the return values?
Duplicate?
LGTM
@jendap's comment addressed. Squashed.
@danmane: is this correct?
This is the mirror of #1154
@Mistobaan can you also squash the commits please?
Martin's unassignment of @jendap was an accident.
@vrv Is this still an issue?
@tobegit3hub yes, that does not work. Python 3.5 will be part of net release. In the meanwhile you can try nightly build - http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/
@smcantab: Many people have requested better double support and it's critical for a lot of numerical applications, so we should do it even it increases binary size.  complex128 is rarer but the same principle applies.
AdjustContrastv2
ExtractGlimpse
InTopK
LRN
LRNGrad
LearnedUnigramCandidateSampler
MaxPoolWithArgmax
NegTrain
RGBToHSV
SampleDistortedBoundingBox
SparseMatMul
TensorArraySplit
ThreadUnsafeUnigramCandidateSampler
@wlsc I'm curious how did you notice this bug? Did you see something visually different in the graph, or just by looking at code?
@jramapuram Fractional striding is hard to implement efficiently using anything like direct convolution; normally I'd just write it using FFTs (which already works).
@Mistobaan I think we are missing the GPU files like https://github.com/tensorflow/tensorflow/blob/f2bd0fc399606d14b55f3f7d732d013f32b33dd5/tensorflow/core/kernels/cwise_op_gpu_cos.cu.cc for these new ops.
Also @shlens in case he knows.
@sguada FYI
pip uninstall protobuf
@shaileshahuja @Smerity I can run the code fine at the head of the TensorFlow tree. Are you using a specific version of TensorFlow? The Docker container?
Odd problem with urllib.
@jendap, what was the problem there?
LGTM
And, of course, tanh. :)
Also all the hyperbolic trig functions.
> Exception information:
Since you're using anaconda, you should be able to install without sudo,
https://groups.google.com/forum/#!topic/theano-users/-ZT-QObECXQ may be relevant -- probably has something to do with the cuda driver, not TensorFlow specifically.  If you find that TensorFlow is doing something specifically to trigger this problem, please feel free to leave a comment and we'll reopen.
http://www.ijg.org/files/jpegsrc.v9b.tar.gz
This is exciting and if it all works I would love that change.
Is this still an issue?
I'm worried that either or both of `_FORCE_INLINE` and `_MWAITXINTRIN_H_INCLUDED` could cause strange regressions on other platforms.  What do they do?
> cally
@vrv, @zffchen78: Is this still an issue?
@ibab I believe all of those semantics are backwards compatible with what we currently have, but @rmlarsen can correct me if I'm wrong.
I suspect it's historical -- in the beginning few ops had working float64
@zheng-xq: Is https://github.com/tensorflow/tensorflow/blob/8048088bfb82846078f8023bc6199e6424926624/tensorflow/core/common_runtime/gpu/gpu_device.cc#L683 properly capturing this?  We probably want to just iterate over all compute capabilities and check for equality, unless I'm misunderstanding how compute capabilities work :)
- What version of bazel are you running
Rebased and squashed
And cherry-picked into r0.7
@mebersole Can you clarify "built from TOT"? Did you built it using `bazel` or using `gulp`? The right way is to run:
If you open the console while your browser is pointed to 0.0.0.0:6006 and type Polymer.version do you get 1.1.5 or 1.2.4 or something else?
What version of protobuf do you have? Did you accidentally move the
On Wed, Feb 10, 2016 at 11:56 PM mondatu notifications@github.com wrote:
@ziiw: master should work for you now, let me know if not.
@ebrevdo @ludimagister too
@ebrevdo: Do we have dynamic RNNs yet?
Marking "works as intended" for now, since the dynamic birnn should solve your problem.
@fayeshine: Thank you!  Let me know if you have questions.
@fayeshine: So close!  It looks like `resize_images` goes out of its way to break only if the _rank_ of the input tensor is unknown.  Try using `shape=None` in your placeholder instead of `shape=[None, None, None, None]` and you should see a problem.  However, I'm not sure if this is the same problem that  @mackcmillion was experiencing.
@fayeshine: Yep, the cleanest way is probably to make all the C++ resize ops accept any number of dimensions >= 3 and treat the initial dims-3 dimensions as batch.  There are a lot of other ops that behave this way: mixing together the last few dimensions and broadcasting over the first few.
LGTM. I was about to make the same change to fix a bunch of test failures under Python3.
On Tue, Feb 9, 2016 at 23:26 Panmari notifications@github.com wrote:
@ebrevdo: Do you have any ideas here?
followed by serial (--jobs=1) bazel test.
merged
@caisq, this looks like a test failure (this is an error you can apparently fix by reinstalling protobuf, according to #487). Is there an old protobuf version in the container?
additional command flags for Docker and the build command.
I like this. @jendap?
I addressed comments from @jendap. In addition, I realized that there was an issue with BUILD_TAG values from matrix builds. Those tags have special characters such as "," and "=", which make the Docker image names illegal. I modified ci_build.sh a little to solve that problem.
over at bazel, they're more likely to know the internals.
@kibtes Did you try "sudo ./deviceQuery"?
@kibtes  As the error message says "-> CUDA driver version is insufficient for CUDA runtime version", you need to check the version of your NVIDIA driver.
CLA didn't make it through somehow?
LGTM, please rebase :+1:
http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/29/
This has an interesting behavior where all the computation is enqueued on the GPU device, but if you don't ask to copy any results back from the GPU the call to run() can return immediately.
BTW: I believe docker and cuda together will get better. But it is not going to be a great experience any time soon if ever.
@cancan101 How does it work with the symlink? nvidia-docker is working for some people but it require to create that symlink? Inside of the container? If so we can just add it to the container tomorrow :-)
That's not official container...
@cesarsalgado: You can pad with a constant other than zero by subtracting a constant from the input, padding with zero, and compensating in the output.
suggests that you didn't point to the right cudnn path.
Most likely the flags for you are passing for STRICT_ANSI etc aren't making it to nvcc.  You might need to edit the crosstool file in third_party/gpus/ to pass the flags there.
@bhack under which conditions do you 'lose' the pickle? Is it because you're using an ephemeral docker container?
@rthouvenin your approach is a net improvement over the status quo.
> Patch Links:
LGTM
@thagikura can you squash your commits?
Recently, for deep RNN's, sequence wise batch normalization has proven to be very helpful. [Deep Speech 2](http://arxiv.org/pdf/1512.02595v1.pdf) in section 3.2 explains this in more detail. Sequence-wise batch normalization is described in section 4.1 in [Batch Normalized RNNs](http://arxiv.org/pdf/1510.01378v1.pdf).
@kbrems, with your use case, it is not recommended to use TensorFlow operators in the same thread as other Cuda runtime calls. TensorFlow uses stream-executor a faster Cuda runtime alternative. If standard Cuda runtime APIs is called on the same context, it confuses stream-executor with what context is bound to what thread, and many of its internal data structure.
If you have to make Cuda calls, either:
1. Call the stream-executor alternative.
ah, this one is interesting: the problem here is that while `docker run` happily allocates more memory to the container, the problem is that the underlying **vm** (owned by `docker-machine`) doesn't have any more memory to give.
ping for @keveman
LGTM @vrv
LGTM @vrv
LGTM @vrv
Reverted the change that updated Eigen.  Let us know if this was not enough and we'll investigate further.
ping for @ebrevdo
Lukasz knows this code better.
origin/r0.7
@craigcitro any gotcha going down that route?
LGTM @vrv
Arpan, there should be no need for anyone to build a container. Was the doc updated?
-    "    import pickle\n",
@shreyasva thanks! I would slightly prefer no line wrapping:
@shreyasva hold tight a little longer: I've just committed a change that upgrades all the notebooks to v4 to make such diffs easier down the road. Sorry about that :/
LGTM @vrv
This looks more like a StackOverflow question rather than a bug report.  You might try using a control dependency to make sure your assigns are happening after the subtractions.
LGTM @vrv
@robagar It all depends on how large your network is and whether you intend to train the model on TK1 or just run inference. Two GB of memory is plenty to run inference on almost any model.
I still haven't been able to install bazel. That said, the assertion you're facing seems to be triggered by the variadic template at line 195 of ./tensorflow/core/lib/strings/strcat.h. I would just comment this code and see how it goes.
LGTM, thanks.
@shendiaomo - sorry, still not on my near or medium term agenda :)
Hi, aadih,
You've commented some things out in that cell that shouldn't be.
ping for @dsmilkov or @danmane
I'm not sure about RNN usage, sorry. (Maybe @martinwicke can advise or point in the right direction.)
Yeah, I think this is some sort of horrible swig issue, so I'll need a way to reproduce.  Can you make a minimal test case that causes the exception?
np.diag can
return diag
In my defense, that's four different things copying Matlab's arguably sketchy choice, but I see your point.  I'm okay if we bow to peer pressure.
That could very well be a bug.  Sherry, could you look at this?
> We're using a command line, not the green button, and it inconsistently
> So I suspected the race condition.
It seems the best solution would be for github to have special magic like "Closes #PR" but for "Merges #PR".
LGTM
@lberki @davidxchen @damienmg
I think @martinwicke was playing around with jenkins, giving it another shot:
Also, I'm not sure if it should be inclusive or exclusive.
@ibab @girving Eigen currently doesn't provide the functionality needed to efficiently implement an  accumulate op. You're more than welcome to contribute the code to Eigen, that would be a very useful extension.
@ibab The TensorReduction.h code is very optimized and therefore pretty complex. A simpler operation that you could use as a template is TensorReverse.h (which reverse the order of the coefficients along one or more dimensions) or TensorShuffling.h (which reorders the dimension - this is a transposition in the 2d case)
I'm not sure whether it is supposed to be public or not.
inlined in cifar.
`nvidia-smi`
/usr/local/cuda/include/cudnn.h
also pinging @ebrevdo who may have hit this more recently.
LGTM
Sherry, this looks related to 110808120 or 110781462.
Options
Actually, there's a wrinkle: `None` is used to indicate a variety of different things:
@mrry: What do the think about the `Zeros` solution?
Martin
> fpmchu@121784b
@fpmchu, I apologize, I should have checked Thrust's documentation before sending you on a wild goose chase. But you are right, since we have committed to this in the documentation, it'll be nice to support repeated indices. My belief is, the performance may not be terrible if you turned the `+=` to `atomicAdd`. Do you mind giving it a shot and running the benchmarks?
Also, not sure if `complex64` is an issue yet, but once it is we'll want a primitive which is slightly weaker than atomic add / sub, since it's fine to add both components at unrelated points in the sequence.
@digitalsword: It sounds like this is a bazel issue rather than a TensorFlow issue if you can't get bazel to compile using the right gcc.  Sorry to redirect, but I don't think we'll be able to fix it on our end.
2) Can you try a newer version of bazel?  I think 0.1.1 is too old now.
Thanks @frankyjuang !  We just merged it -- let us know if the problems are still present by commenting and we'll reopen.
The TensorFlow tutorials cannot replace a deep learning or machine learning textbook or course. We may add pointers to additional resources as those become available.
LG, can you squash the commits?  Then I'll merge.
LGTM
Would this test fail if the dtype of the cell is float64?
@rafaljozefowicz
2) The error is:
Error is enlightening:
It's strange because the compiler shouldn't even see the PyString_ version any more. Can you remove the ifdef altogether (and keep only the Py3 branch) and see if that works compiling with Python3? Those two are the only occurrences of this function, so I feel like for some reason the ifdef doesn't work as expected (maybe because it's evaluated by swig, and we need a %{ %} around it).
Maybe this is a swig problem? What version of swig is this, and does
> To add to the weirdness of this issue: something (maybe SWIG?) adds the
> but that never happens, which probably means both PY_VERSION_HEX and
> PY_MAJOR_VERSION are not defined correctly.
Let me try a magic incantation:
For a hacky (unsupported) workaround, you could try the [answer to this StackOverflow question](http://stackoverflow.com/a/33702428/3574081).
(Probably a StackOverflow question)
Dupe of #751.
Assigning to @danmane for final checking. Thanks!
@girving: want to take a quick look?  Seems fine to merge but just want your extra eye.
LGTM.
On Sun, Jan 10, 2016 at 8:34 PM, Eugene Brevdo ebrevdo@gmail.com wrote:
On Sun, Jan 10, 2016 at 9:11 PM, Eugene Brevdo ebrevdo@gmail.com wrote:
if using a Placeholder with no shape!)
Sorry - not none, but a TensorShape with no dimensions.  the **getitem**
On Mon, Jan 11, 2016 at 4:50 PM, Eugene Brevdo ebrevdo@gmail.com wrote:
> Note that the extra logic here: kentonl@04036d7
LGTM. Thanks @dongjoon-hyun!
though that would be very much appreciated.
@dsmilkov: let me know if this LGTY and I can merge
LGTM
Thanks @ageron! I'm merging this for the greater good, even if it's still broken.
Dupe of #713
@colah: Should this be closed?
Posting for @colah: Fixed, so closing.
@lberki, @davidzchen, @damienmg in case they know the cause of these hangs, or at least are aware of them.
@danmane: Do you know if this is still an issue?
What version of Bazel are you using?
@Kuntal-G To clarify, you received the re2 error prior to trying sudo, and only get the permission denied error when using sudo?
@fayeshine, we will officially switch to Cudnn R4 when it is officially released. For now, it is a release candidate.
For Cudnn R3:
For Cudnn R4:
It came from cwise_mul:
However, this uses an older version of Eigen that has some speed problems.
> - _D_ third_party/eigen3/Eigen/src/CholmodSupport/CholmodSupport.h
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/LU.h
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/Lazy.h
> - _D_ third_party/eigen3/Eigen/src/Eigen2Support/LeastSquares.h
> - _D_ third_party/eigen3/Eigen/src/Geometry/Umeyama.h
> - _D_ third_party/eigen3/Eigen/src/Jacobi/Jacobi.h
> - _D_ third_party/eigen3/Eigen/src/MetisSupport/MetisSupport.h
> - _D_ third_party/eigen3/Eigen/src/OrderingMethods/Eigen_Colamd.h
> - _D_ third_party/eigen3/Eigen/src/PaStiXSupport/PaStiXSupport.h
> - _D_ third_party/eigen3/Eigen/src/PardisoSupport/PardisoSupport.h
> - _D_ third_party/eigen3/Eigen/src/QR/ColPivHouseholderQR.h
> - _D_ third_party/eigen3/Eigen/src/QR/ColPivHouseholderQR_MKL.h
> - _D_ third_party/eigen3/Eigen/src/QR/FullPivHouseholderQR.h
> - _D_ third_party/eigen3/Eigen/src/QR/HouseholderQR_MKL.h
> - _D_ third_party/eigen3/Eigen/src/SVD/JacobiSVD.h
> - _D_ third_party/eigen3/Eigen/src/SVD/JacobiSVD_MKL.h
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseColEtree.h
> - _D_ third_party/eigen3/Eigen/src/SparseCore/SparseFuzzy.h
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU.h
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_copy_to_ucol.h
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_heap_relax_snode.h
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_pivotL.h
> - _D_ third_party/eigen3/Eigen/src/SparseLU/SparseLU_relax_snode.h
> - _D_ third_party/eigen3/Eigen/src/SuperLUSupport/SuperLUSupport.h
> - _D_ third_party/eigen3/unsupported/Eigen/FFT
> - _D_ third_party/eigen3/unsupported/Eigen/KroneckerProduct
> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/ei_fftw_impl.h
> - _D_ third_party/eigen3/unsupported/Eigen/src/FFT/ei_kissfft_impl.h
>   third_party/eigen3/unsupported/Eigen/src/KroneckerProduct/CMakeLists.txt
>   third_party/eigen3/unsupported/Eigen/src/KroneckerProduct/KroneckerTensorProduct.h
> Patch Links:
No, it was correct before. [0,0) means excluding 0, [0,0] means including 0, which would also mean that 0 would be valid and the error message would be confusing indeed.
This is an odd one -- we're mimicking python's range behavior, which is hard to properly encode in python because overloading doesn't exist. The docs for tf.range are pretty clear what it does.
The fact that the signature itself is misleading is unfortunate, but somewhat out of our control (the limit parameter does have a default of None, and start does not have a default value of 0, python doesn't have a way of expressing the signature tf.range(start=0 if limit is None else no default, limit=None) in the signature alone).
``` python
I need to change that regex, please.
It is unclear whether it is a real hang, or it was just the kernel run
Initialized!
@jbeda To fix the summary-icon, you need to rebuild tensorboard using bazel and run it:
@liujunyi-sjtu, I cannot reproduce the problem on my side. Could you provide some more information? How did you bulid and run the binary?
code. In sorry this bit you after you've done all this work, I'm still
On Fri, Jan 8, 2016 at 07:25 Ville Kallioniemi notifications@github.com
For cudnn v5 see #1786.
Thanks @martinwicke. Actually, the code you mentioned (RoundTest) is written in Python. What I need is a C++ code that I can test it on my android device. So, I need a piece of code that can be compiled with GPU=on and has the capability to be run under my android device.
@vrv I have seen it. I got some of my modifications from that post. However, it is for Jetson device with Linux_for_Tegra that makes the story different.
A major reason for this is that Tensorflow currently requires CUDA 7.0 and the NVIDIA Codeworks support for 7.0+ is very limited -- currently only the Shield Android TV is listed as 7.0 capable (see http://docs.nvidia.com/gameworks/index.html#developertools/mobile/codeworks_android/codeworks_android_release_notes.htm).
I was aware about the point. however, I like to use an unofficial approach.
http://cudamusing.blogspot.ca/2015/11/building-tensorflow-for-jetson-tk1.html
@hamidb This is something that possibly may come in the future, but we can't support at this time.
``` python
> something odd like leaving space between the final punctuation words.
Closing because yamins81 provided a solution.  Feel free to reopen if this is still a problem!
This failure occurs before TensorFlow even begins executing. It looks the system you're running on has very little RAM available, and can't handle loading the whole MNIST training set into memory and preprocessing it in python. There might be ways to reduce the total amount of memory used at that stage, but this seems like a particularly pathological situation.
@frankyjuang The issue you are having might be unrelated to this issue, since you are already using Chrome and not Firefox. Can you share your log directory with us? You can gzip it and ideally attach it here to this issue, or email it to dsmilkov@gmail.com.
marcotrombetti@, please do send a PR with a fix.
It looks (from your reported output) like this is an Anaconda issue. From what I can tell, the recommended way to install using Anaconda is to do the following:
$ source activate tensorenv
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
LGTM
Hi, mistobaan, please let us know how you plan to proceed on this? I can see the following work items:
@Mistobaan: any updates?
@Mistobaan: I think this looks good pending the GPU tests finishing -- the only other thing I think we need is for you to run
``` python
If anyone can explain the cause of the blowup in this approach, would be very much appreciated.
LGTM
This is a subtly different issue: as far as the placer is concerned MaxPool is defined to run on GPU, so it will be placed on a GPU device if available (using hard or soft placement). However, for certain input values (in this case, where the kernel size in the depth dimension is greater than 1) it will raise an error at runtime if the op tries to execute on GPU.
@wchan, did you find a solution to this, did it disappear, or is it still a problem? I've not been able to reproduce.
Nematode is a common classification for very dark images by the default model, so that part is not surprising.
@Override
If so, I'm guessing it has more to do with the particular camera drivers on your phone than Tensorflow, especially since you tested with the null code above.
For Pouya's problem:
@yutachen's problem has the same root cause.
If your problem is different from the ones encountered by @po0ya, @yutachen, and @ingwar, and you have followed my instructions, please include your complete program.
``` python
...with this code:
``` python
transpose_op.cc.
> Reverse is not the only thing causing the problem though. In fact the
> can't be circumvented is the InvertPermutation error.
Except for error checking, it's much easier to implement `InvertPermutation` on GPU now that we have code for `tf.scatter` on GPU.  A bit of refactoring would probably let `InvertPermutation` reuse the functors for scatter.
@gotope Here is the diff:
@danmane, @dsmilkov: let us know if this is okay to merge
@danmane: still looks good?  if so i'll merge
On Fri, Jan 15, 2016 at 3:31 AM ponythewhite notifications@github.com
Looks like this fell through the cracks.  @danmane: Any thoughts?
@martinwicke, @aselle: Who's the best person for Docker issues?  This issue seems to have fallen through the cracks.
Hm, looks like we're pretty inconsistent about default args for our optimizers.  In Adam we set default values for all parameters (including learning rate), for others, everything but learning rate has a default, and here everything but learning rate and decay has a default.
While is not a documented feature, partly because of issues like this.  We
code) without rerunning bazel test ....
On Wed, Dec 23, 2015 at 7:35 PM, trubin notifications@github.com wrote:
This is a known limitation :(
On Wed, May 18, 2016 at 3:59 PM, Sherjil Ozair notifications@github.com
> than a Tensor
@Mistobaan: Not sure what you mean by a minimal code example.  The desired behavior would be that `tf.Print(input, data)` works even if `data` contains a mix of `IndexedSlices` objects and things that can be converted to tensors, and the desired outcome would be output that looks something like
Assuming the tests pass, LGTM.
@panmari: isn't that specifically the kernel you added that is showing different behavior between CPU and GPU?  That probably needs to be fixed, unless I'm mis-reading the error.
This could be related to https://github.com/tensorflow/tensorflow/issues/551: the code ends up spending a lot of time locking and unlocking a mutex in the threadpool code.
@songgc https://github.com/tensorflow/tensorflow/commit/ab02c5ab2f1f10bc9e51f02f5125abed449cae87 combined with a few other changes to make sure all the TensorFlow operations are multithreaded resulted in a 1.5x to 3x improvement on most benchmarks. Is that what you're seeing?
Just looked it up: The slaves we have in the particular GCE region have Ivy Bridge Xeon, which supports AVX but not AVX2. So this explains it.
Let me try kicking off the test for this PR by saying some magic words
LGTM
@jendap: done, thanks!
``` python
@ebrevdo: I don't think `tf.nn.linear` is supposed to exist, but `models/rnn/linear.py` uses it.  Is this because we're missing a unit test?
Yeah, there's some more plumbing in the internal execution of graphs that specially handles functions.
(2) OOV words. This is a well-known problem in neural translation models and a there are ongoing research efforts to deal with it. One way that seems to work well is having multiple UNK tokens and doing some replacements, see this paper: http://arxiv.org/abs/1410.8206 . In a basic model like the one in the tutorial the UNK is heavily over-represented in the training data which leads to bad decodings and a bit "cheated" perplexity, as you said. You can try to just decrease UNK probability in the training by using a few different UNKs, or the method from the above paper, or something else entirely.
I'm writing about these points to illustrate one thing: this is an ongoing research effort. We constructed the tutorial to show how to get started with these kinds of models in TensorFlow, but all these research questions remain open. You can now see where the problems are and start working on your own solutions -- that's the intention!
So indeed - there is a strange _UNK there, but it's not bad, right? In fact quite funny in some ways - I think everyday queries are really a bit outside of the training data used there.
@aliabbasjp It looks like your dev data is very different from your training data. Maybe there are many unknown words given the vocabulary size you picked. Try running with dev data that is closer to your training data, e.g., split 10% off your current training data and use it as dev data.
Benoit, is the correct tanh implementation upstream?
The current implementation is pretty specialized to `float32`, so it wouldn't be completely trivial to switch it over. However, we'd be glad to take contributions on this if it would be useful.
AdjustContrastv2
ExtractGlimpse
InTopK
LRN
LRNGrad
LearnedUnigramCandidateSampler
MaxPoolWithArgmax
NegTrain
RGBToHSV
SampleDistortedBoundingBox
SparseMatMul
TensorArraySplit
ThreadUnsafeUnigramCandidateSampler
Thanks @panmari!
@jendap, tests didn't kick off :(
@lglhuada, This is not an error, just log. It just means there is no efficient way to transfer data from gpu:0 to gpu:2. You can exclude either one of them through CUDA_VISIBLE_DEVICES
Please make sure you installed the GPU-enabled TensorFlow binaries. If you
pip install /tmp/tensorflow_pkg/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
``` python
Thanks for your report cinjon, I think it's a bug in seq2seq.model_with_buckets.
I was offline for a while cinjon, and I'm not sure I understand now what you were trying to accomplish. If you still have this problem, let's take it offline and, if needed, open another issue (just to not prolong this one, it becomes unreadable). Thanks!
Typically we use a much larger amount of computation to train a model with TensorFlow (using one or more GPUs if it is a complex model), because the throughput of examples processed per second is what determines how long it takes to train the network.
For inference - depending on the model - it is often possible to scale horizontally by using lower power devices, and scaling out to many independent devices (unless the model is too big to fit on a single device). That's what makes it possible to run inference on a mobile device (like in the [Android tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)), and potentially it could also run inference on a Raspberry Pi.
Sorry, that may be misinterpreting panmari's original request.
@panmari -- which ops are you thinking about?
Given @panmari's recent pull request to extend the types supported by resize_image ops, I think it's those.
kyosha@gmail.com has a change, https://tensorflow-review.googlesource.com/#/c/1252/, which I think will help. You are also welcome to fix the code to fit your need. Cheers,
@Fhrozen
@noisychannel, could you provide a bit more information about your running
@leary-google, @eliben, anything from the stream-executor side?
``` python
I agree that we could do a better job here, but it'll take some care to make sure we don't spam the error message with content that nobody can read.
> registrations for that Op (which might be tens of kernels) in the error
That codepath only gets hit if there are no kernels at all that support that op.  There is no placement specified, and no kernels registered.  I think the situation @sguada has been fixed, you're pointing at a different error path.
Yes, that's a bug.
On Wed, Feb 10, 2016 at 11:25 AM Pouya Samangouei notifications@github.com
@ogrisel , if you apply enough different types of bucket sizes then you wouldn't really run into this problem. All the sequences would roughly be within 5 or 7 timesteps (assuming you're doing NLP seq2seq).
Right now, we don't have TensorFlow-specific profiling tools available, but a sampling profiler (such as [`pprof`](http://goog-perftools.sourceforge.net/doc/cpu_profiler.html)) should give enough information to point you in the right direction.
Also, for experimentation I would recommend to use a small batch_size=1 and make it bigger when everything works to keep memory down.
But because of memory usage, I can only use a batch size of 16 whereas in theano I could use a batch size of 256 easily. And it is the batch size that makes tf very slow for me.
> FYI, I've got a working implementation for "Gru" and "GruCell" .. GruCell
> On Wed, Feb 17, 2016 at 7:37 PM, ebrevdo notifications@github.com wrote:
@ebrevdo: Is this resolved?
> https://github.com/notifications/unsubscribe/ABtimwbQ_gGcBuel5rBgZ1l8n9dYL79Fks5qLhx0gaJpZM4G1LC9
> https://github.com/notifications/unsubscribe/ABtim1oVTC2F-LK5AHLVyPAuRMeoqwE2ks5qLiw9gaJpZM4G1LC9
This may be a 'feature' -- tensors are intentionally rather strict about
> division the original code worked fine. So it is some "v3" trickery.
Pinging @benoitsteiner, since this sounds like it might be an Eigen issue.
Cc @zheng-xq, but I think we may need more information to help here.
Sounds great to hear that. I love Tensorflow and with improved memory usage it would be the best deep learning platform in my opinion.
Thanks, @ebrevdo can fix this.
That's a good feature request.  For complex numbers, it should be called `tf.arg` or `tf.argument`.
If using bazel:
most likely due to a bad internet connection.
@vrv: If the actual bug is fixed, should we close this?  I don't think it's a TensorFlow bug that `(1)` isn't a `tuple` in Python.
What do you get from: `uname -a` ?
> Exception information:
@elbamos: I think @zheng-xq was probably going to tackle the cudnn versions issue soon.  Not sure how he plans to do it though.
Sounds like it's worth asking the bazel folks about.
this kind of error before here
> To nikitakit,
Yeah, I think this is a bazel or tensorflow BUILD issue, not an nvcc one.
@jendap: do you know which bazel we're using there?
**Note:** If you know lots about python wheels, and can answer some of the questions here, please jump in!
Currently, when we generate wheel files, we end up with either too-broad names (eg all Mac wheels are `py2-none-any`, which is wrong) or too-specific (some linux wheels are tagged `cp34-cp34m-linux-x86_64`, but we think the abi component should be `none`). This leads to issues like #467.
A part of the issue here is that we're not sure what some of these tags mean -- for instance, the only explanation I've found of the `m` suffix is in [an issue on the wheel bug tracker](https://bitbucket.org/pypa/wheel/issues/61/abi-version-is-not-found) and even that doesn't clear things up.
There are two options I think:
> ## (d)[nani@nande cat]$ pip show tensorflow
> Location: /home/nani/Desktop/cat/d/lib/python2.7/site-packages
> (d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/
@kvamaraju, could you sync to the latest TensorFlow and retry? I think this problem should have been fixed by a recent CL.
@zfrenchee, can you send this request via gerrit?  See https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md and http://blog.mdda.net/ai/2015/11/10/contributing-to-tensorflow/
@danmane: Do you know what might be going wrong?
@hamidb: that sounds like a red herring.
bazel clean
That is indeed weird that it can't open the mocha.git file...
Given that "//third_party:protoc" appears nowhere in the Tensorflow or google/protobuf repos, I'm guessing this sounds like a bad config is being pulled in from somewhere external, likely Bazel.
@hamidb
Which version of Bazel are you using?
Nice catch!  Yes, that is a bad bug.
@vrv: Do you think that's wise?  I'd be a bit worried that negative dimensions are used somewhere, but it's currently a bit inconsistent that C++ `TensorShape` requires nonnegative but Python `TensorShape` does not.
Yeah, @colah needs to update the arrow
Retitled
@girving: do we want to be smart / special about any other types of attributes in get_attr, or is special-casing for types_pb2.DataType enough?
Accepted via gerrit!
It looks like you didn't include the code for the line that triggers the error, so I don't know what the problem is.  Wild, unlikely guess: Do you have Python 2 with `unicode_literals` imported from `__future__`?  I'm a bit surprised those variable names are showing up as unicode.
@colah: Do you have any idea what might be going on?
On Wed, Feb 10, 2016 at 12:18 PM Amr Abed notifications@github.com wrote:
@craigcitro: Do you know what's happening me?  My knowledge of pip is scant.
@severun  -- Can you run again with `bazel build -s` to see the full command-line and output?
@severun well, it confirms that the failing step is indeed something about copying the file out of the downloaded six archive.
i'll admit that i'm shooting in the dark a bit here; two things to try:
people that have seen this and it was always OOM.
Hi Hamid,
(it's actually arbitrarily cloned from libmediandk.so, which could explain
> Hamid
Thanks Hamid. I'm guessing this might be a discrepancy between what the
> 1- changed google/protobuf/BUILD
> // [""] returns bazel error
>         "-mfpu=neon",
>     linkopts = ["-llog -landroid -lm -ljnigraphics"],
>     name = "androidsdk",
> pid 1219
> RTLD_LAZY) failed: dlopen failed: cannot locate symbol "getpagesize"
Thanks!  The `cpu` case is a bug in the gradient of `tf.gather`, which wasn't correctly updated when I made `gather` handle arbitrary rank indices (including scalars).  I'll fix that.
@girving  we may need to point the pythonX_path to a script (copied from a template via e.g. sed) which sets the LD_LIBRARY_PATH correctly based on e.g. LIBPL output, and calls the python interpreter.
> Hi ebrevdo,
You seem to have a fairly nonstandard installation of python 2.7.  Are you an administrator of this machine?  Perhaps you could reinstall python?  Or install a version of python in your home directory, where you have write permissions and can install a properly built version?
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/alexnet/BUILD
bazel-bin/third_party/tensorflow/models/image/alexnet/alexnet_benchmark
Looks like this fell through the cracks.  @panmari: Is this still an issue, and do you have code if so?  For better or worse, tensorflow exposes a lot of the complexity of asynchronous programming to users, so it may be that this is intended behavior.
What version of bazel do you happen to be running?
@azzolini: we now accept PRs, so if you want to make the fix and also let us know what the error you were getting was, please do!
Yes we should remove this.  +ludigmaster@
@ludimagister, @ebrevdo: What's the status of bidirectional RNNs?
@NumesSanguis As well as relative paths being fixed, I believe it now prints the path right before the line `Starting TensorBoard on port 6006` (although I'm not sure when that change was made).
Employing either a C++ or Python API.
Updated subject to reflect the environment you're trying to run in.  Hopefully someone in the community who knows more about bumblebee/optimus laptops might be able to help!
@jpmerc, could you run your command line through sudo, similar to you C++ examples? I wonder whether it is the root access that is making the difference. The initialization logic should be the same between C++ and Python clients.
@jpmerc, could you try to set LD_LIBRARY_PATH inside your sudo? That should make sudo preserve the environment variables.
@esraahassan can you explain what you mean by "please solve this problem in video"?
@girving: is there a fix on the way or is this something more fundamental?
@ebrevdo: Do you have any thoughts here?  Reassigning to you since Sherry is out.
@yuanpengX @atamahjoubfar is this still an issue?  Is it just the verbose logging that's troubling you?  when you reach the end of the input file, the queue dies and the training is restarted from the beginning.  that may explain all the warnings.
@atamahjoubfar Thanks!  Closing for now; please comment or file another if it comes up again.
Rust bindings would be cool!  I don't think there's anyone working on rust bindings so far, though it might be worth an email to discuss@tensorflow.org to check.  Agreed that a separate project is probably good to start out.
To start the conversation, there are a couple different levels of bindings:
away the python layer entirely. Mainly this is because the gradients and
> of the land for Rust (#388 (comment)
For efficiency this needs to be done on the GPU. On the author's code (theano), they do this by making a Theano Op, and inserting scikit's Fast Fourier and Inverse Fast Fourier Here:
https://github.com/amarshah/complex_RNN/blob/master/fftconv.py
https://github.com/LeavesBreathe/Seq2Seq_Upgrade_TensorFlow
However, it will not be possible to do without FFT and IFFT support so I really appreciate your help!
@LeavesBreathe  , can you elaborate a bit more on your requirements about FFT/IFFT?
2) do you need 1D, 2D, or 3D fft/ifft? or all of them?
I don't know how hard it is for you to implement these fft's and ifft's but I think 3d support would be nice for future users who may try unitary conv nets. I certainly don't want to ask too much of you though!
If this is of any help to you, the goal is to replicate "Complex_RNN" here: https://github.com/amarshah/complex_RNN/blob/master/models.py#L532
> scikits.cuda:
# Typically, a tf proram is easier to read and is more
--notea --> tea is False
For whoever looks at this next: I fixed `tf.test.compute_gradient_error` a while ago to handle `complex64` input, so it should be easy to test this change.
So @ebrevdo, the answer is to document this difference in behavior?
The format of tfrecords is not a human-readable string: most likely the depth value is encoded as a field in a _binary_ serialized protocol buffer.
Just curious, what is the use-case for differentiable Cholesky?
Rasmus
- @craigcitro
No. All we have is this issue and @danmane.
I'm not sure exactly what the problem is:
https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/models/image/cifar10/cifar10.py#L484
@danmane, @martinwicke: I assume this is just a css style change?
> @danmane https://github.com/danmane, @martinwicke
[[Node: gradients_3/model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device="/job:localhost/replica:0/task:0/gpu:0"](model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/Linear/concat, gradients_3/model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/add_grad/Reshape)]]
--input_layer=Mul
What do you get from the output of `uname -a` ?
Closing due to likely GLIBC issues.
On Nov 25, 2015 5:15 PM, "chenghuige" notifications@github.com wrote:
In fact, for X*w, see [embedding_lookup_sparse](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L108) which does much of the hard work for you.
@chenghuige Since you have 400k features, in practice you must somewhere store a 400k x feature_depth matrix, no?  This is what you pass to embedding_lookup_sparse and it performs the sparse multiplication for you.
There are a few things in that example that I can't see, for example sparse2dense is not defined.
``` python
I completely misunderstood your point, for some reason I assumed you meant checkpoints. FR stands, I agree that it might make a lot of sense to support something like that.
This question is probably better asked over at bazel, they're more likely
@danmane: Thoughts?
The image viewer is definitely one of the least-developed parts of TensorBoard at the moment. I want to improve it in the future, but it's not very high on my queue at the moment as I'm prioritizing features that are domain independent and thus useful to every user of TensorBoard (general usability, hyperparameter search, etc). If you want this to get fixed sooner, the best way is to come up with a plan for what the API should look like, get buy-in, and then submit a pull request - I'd be happy to help coordinate the process :)
This can be caused by several problems:
Why can't it find variables?
It sounds like you're using an older version of glibc than we support (2.2.5, rather than 2.17). You could try adding `"-lm"` to the returned value from `tf_copts()` in `tensorflow.bzl` (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L40)), but there might be other problems that arise afterwards.
Cc @danmane.
@chenghuige we're working on providing something like tensorflow.version or tensorflow.**version**.
One thing you can do is run with a tiny learning rate, or even zero learning rate. If you still have divergence then, you have a bug in your setup. If not, increase your rate slowly and see if there is a regime in which things train without diverging. It's completely possible to have weights that are in a good range, but activations or gradients going to infinity because of the shape of the loss, or too high a learning rate. It's obviously always a possibility that there is a bug in the optimizers, but in my experience, every single instance of this kind of problem could be traced back to a weirdly wired model, learning rate issues, bad randomization of the input examples, or - in the case of Adam or RMSProp - issues with the epsilon value.
@jpiabrantes One thing that stands out here is that you use a square root in your loss. That's known to be very unstable numerically for very small values. Can you try 1) not taking the square root or 2) adding a small constant, e.g.: tf.sqrt(1e-4 + ...).
@navraj28 diverging when you use SGD can happen for many reasons:
@zffchen78
@futurely, how do you want to work around this restriction? Do you want to restore all you have, and initialize the rest to what's in the checkpoint? If that's the case, what you can do is this:
@koonyook, how do you want to use a new variable with previously saved variables?
Just to confirm; are you running bazel test?  (looks like it)
> trying to hack the build rules, but that is quickly becoming a rabbit hole.
@mbaddar1: Very cool!
into the zoo (which we will announce shortly). If you want to write a
> and is there a link to zoo?
@mxrguspxrt: so we can understand what specific improvements would help, what is your current understanding of that error message?
color space conversion change in bf6b536bde7d8060c489b51fedb58968b8cbfd7c
On Fri, Dec 4, 2015 at 2:13 AM, barami98 notifications@github.com wrote:
Agreed, scatter on GPUs would be useful.  Also gather.
@cesarsalgado's issue is fixed. The accordion is still not done.
@martinwicke, @danmane: Is this still an issue?
If there's anything that's not clear in here, or that you think would be worth adding, please let us know!
Thanks, special functions are well worth adding, and we'd welcome contributions in this direction.
Digamma should also make its way in, in the next 2 weeks.
>     # fast approximate gammaln from paul mineiro
As far as I can tell, the digamma function is only used for entropy and
> gradient is the polygamma.
$ unzip tensorflow/examples/android/assets/inception5h.zip -d tensorflow/examples/android/assets/
@chuanwen: I'm not sure what exactly you mean, is that some combination of tf.where and tf.gather?
Hi @aliabbasjp: This kind of question is better suited to the StackOverflow forums or the discuss@tensorflow.org mailing list -- can you please repost there?  Thanks!
Matt Zeiler calls the entire stack Deconvolution Networks:
Caffe calls it Deconvolution (@Yangqing) :
Can you paste your env vars here?
``` python
See documentaion on the TensorBoard specific pieces at
Thank you for your contribution!  In general we encourage users with nonstandard setup to build their own pip packages from source.  And we are specifically encouraging the CUDA 7 runtime with CUDNN 6.5 because it's one less variable to consider when debugging (and this is the version we use and test with).  Since it doesn't seem like there's any issue, I'm closing / archiving this for now.
@zkl99999 this sounds like a connectivity problem; are you still seeing this?
@colah, can you take a look at this?
We're still going through all of these issues and prioritizing, so we don't have a roadmap ready yet.
Closing this since there's been no activity on this for a while -- 40% slower on OS X for matrix_inverse isn't great, but suffices for now.  Looks like we're faster on other platforms so, there's always that.
> https://github.com/notifications/unsubscribe/AAjO_VoOya6ZrBTvi7Gs1zoghoDgvTGvks5qJFkvgaJpZM4GiS_S
@staranjeet: I think we may need to figure out a policy of just how pep8 compliant we want to be first.  For example, I'd hate to give up our 2 space indent paradise.
E129 is a bit iffy, i have to check how these look.
> Error Error meaning
> E116 unexpected indentation (comment) 1
> E251 unexpected spaces around keyword / parameter equals 15
> E129 is a bit iffy, i have to check how these look.
> > Error Error meaning
> > E116 unexpected indentation (comment) 1
> > E251 unexpected spaces around keyword / parameter equals 15
``` python
pseudo-example:
Hi cinjon,
``` python
``` python
@ebrevdo: are those functions:
Yes, newaxis is essential.
@olange-google: I think we're unlikely to implement an axis parameter since it's beyond numpy features, but what you want is covered by the _combination_ of slice indexing and advanced indexing.
On Feb 23, 2016 11:45 AM, "Mohsen Hejrati" notifications@github.com wrote:
Advanced slicing is on the radar.
Cc @aselle.
@danijar, @shampool: Yes on both counts.
> https://github.com/notifications/unsubscribe/ABtim2OtDT1jm1ulE1a0NYpSTv0AzN95ks5qJ1VlgaJpZM4GiDCf
@timshephard and @cauerego PTAL at [this script](https://github.com/tensorflow/tensorflow/blob/41930f0b81b52a34fb56d921c9bad65c36168323/tensorflow/tools/docker/docker_run_gpu.sh) for details on how to forward GPU support into the docker container.  This has worked for us.
http://tensorflow.org/get_started/basic_usage.md#variables  -- you might find that useful.
Switching to @prb12: Should we mark this as contributions welcome or is something more elaborate than an error message change warranted?
@auroua looks like you're missing the python-dev package, and your question is unrelated to the one above.  if you continue to have problems please open a new issue instead of commenting in this one.
@yurivict sadly we don't have any FreeBSD boxes to play with so you're our guinea pig.  the problem stems from the "jdk" definition [here](https://github.com/bazelbuild/bazel/blob/687550660db90b007741e1f67a8092b08945f8e3/tools/jdk/BUILD) in the bazel code.  Do you have java/javac in your path?  If you do, can you try building bazel 0.1.0 on your machine and see if that solves the problem?
Can you say what version of bazel you are using?  And try @SWu's advice and see if it solves the problem?
On Nov 13, 2015 5:40 PM, "xuezhisd" notifications@github.com wrote:
> I used bazel-master.
> 🍃  Building Bazel from scratch............
> 🍃  Building Bazel with Bazel.
> ERROR: /home/share/TensorFlow/bazel-0.1.0/src/main/native/BUILD:28:1: Linking of rule
Any chance this is related to https://github.com/tensorflow/tensorflow/issues/187 ? (are you using anaconda python?
As a hack, you could try setting your PYTHONPATH to the tensorflow/python/ subdirectory but that will probably break other things.
embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix
embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix
embedding_attention_seq2seq/RNN/MultiRNNCell/Cell1/GRUCell/Gates/Linear/Matrix
http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
@ruffsl whoa, that cudnn issue being resolved is fantastic! thanks for doing the legwork there.
It looks like the problem arises because (i) `tf.nn.moments` relies on knowing the fully-defined shape for its argument, and (ii) the `x` argument that you passed to `tf.nn.moments` has one or more undefined dimensions.
``` python
``` python
divisor, name="variance")
@mondatu: just to be clear, you're saying that just using BFC alone is enough to solve the mnist tutorial out-of-memory?  No need to do the batching in mnist?
Did this issue get resolved? A lot of things have changed with 0.6.0.
What version of bazel are you using?
(re-open if this is not sufficient -- this is most likely a bazel issue from what I understand)
Gotcha, so that would require OpenCL support, which we're tracking here: https://github.com/tensorflow/tensorflow/issues/22.  I'll de-dupe with that feature request.
ERROR: /home/local/ANT/x/tensorflow/tensorflow/python/BUILD:666:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' failed: gcc failed: error executing command
(cd /home/local/ANT/x/.cache/bazel/_bazel_x/29843df2d2b24eaae7acd7ee881b7bec/tensorflow && \
(cd /home/local/ANT/x/.cache/bazel/_bazel_x/29843df2d2b24eaae7acd7ee881b7bec/tensorflow && \
a path outside of the execution root..
Then add `-Ithird_party` to the includes in `tensorflow/python/BUILD` and `tensorflow/tensorflow.bzl`
@auroua Here is the diff:
From the stack trace, it seems likely that this is either:
1) A bazel problem
2) A connection problem
(Please re-open if you find this is TensorFlow related)
From quick searches on that error message, it looks like this is Alpine specific: alpine seems to use a different c library than glibc, which we probably don't support. :(
(Feel free to re-open if you think this is TensorFlow specific)
uname -a
Exception:
shutil.move(old, new)
Closing for now as there seems to be some resolution here (though using an esoteric solution).
@alexryan: It looks like the TensorFlow Python code is being loaded from the directory `/Users/alexryan/projects/machineLearning/tensorFlow/clone/tensorflow/`, which is an unusual path for the PIP installer to use. Did you also clone the git repository and add that directory to your PATH or PYTHONPATH environment variables? If so, try removing it - the problem arises because it tries to load from the unbuilt project source, which doesn't include the generated code for the protocol buffers (e.g. graph_pb2.py). If you have also installed it using PIP, it should be in your path already.
Likely a dupe of https://github.com/tensorflow/tensorflow/issues/11 -- see the solutions near the very end to see if they help.  Thanks!
``` python
import google.protobuf
As you have rightfully guessed, the core TensorFlow team doesn't have plans for Ruby on the horizon, so if you want to contribute it, please go ahead! Submitting drafts for frontends such as this on the discuss mailing list early on is probably a good idea.
python
@mmolaro Actually, GCR is built to support public images (like this one), as well as serve more core infrastructure needs ("I want to serve images to other parts of my cluster"). But I'm looking into getting us on dockerhub, too.
On Jan 15, 2016 6:41 AM, "qingzew" notifications@github.com wrote:
On Feb 22, 2016 5:34 AM, "bmilde" notifications@github.com wrote:
but i will try to connect their! thanks @sdgandhi.
@sdgandhi I opened it!
There is the equivalent of potentially many unrelated PRs in any such
merge, that's why it's hard to find a decent description for the sum of
them.
@infojunkie I applied your fix, but I got lots of nan's in the computation output:
cuda_7 - ami-12fd8178
I'm not sure whether we should call TF_UNOFFICIAL_\* "not a hack", but yes, it _should_ work. If it doesn't, it's likely unrelated to Cuda 3.0 per se, and we should have a more specific bug.
Thanks for the question!  To reiterate what I said [here](https://github.com/tensorflow/tensorflow/issues/12#issuecomment-155150681), we are working on making a distributed implementation available, it's currently not in the initial release.  Please stay tuned, and take a look at the cifar multi-gpu tutorial for a flavor of how we handle multiple 'devices': http://tensorflow.org/tutorials/deep_cnn/index.md
@kanwar2preet: Can you open another issue with details of (i) the TensorFlow program that you ran, and (ii) the configuration you used for the servers? Thanks!
Regarding Ansible, I don't have any experience with that platform, but we would be glad to accept contributions... feel free to open another issue to suggest that. (https://github.com/tensorflow/tensorflow/issues/1686 suggests adding support for Slurm, for example.)
At the very least, the [Eigen](http://eigen.tuxfamily.org) library would have to support OpenCL.
- TensorFlow relies on c++11 and has taken a "single source" approach, so SYCL seems like a great fit.
- We don't have a lot of OpenCL experience in house, so we're collaborating closely with Codeplay to bridge this gap. In particular, Codeplay is currently leading the effort to add support for SYCL to the Eigen tensor library.
@naibaf7: A fast implementation of the convolution operation would be extremely helpful in TensorFlow. Looking forward to it.
hey @sibleyd -- yes, that Dockerfile was still undergoing a little churn. An updated version will come along shortly, and I'll update this issue as soon as it's ready.
If you build from the latest TF, it would ask you for Cuda SDK and Cudnn versions. You can point it to a different version.
A Swift frontend, especially for simply running graphs for inference on mobile would be great. I don't know of current plans to provide one, so feel free to dive in. Since our exact plans on accepting external contributions are still in flux, it would be a good idea to check in with the discuss mailing list with a draft of the code ahead of time to figure out where it should live exactly.
We still intend to provide first-class Windows support, but adventurous users might find this a good way to get started in the mean time.
Reopening as a tracking bug.
Hi kmatzen: As mentioned in our FAQ, this release does not have an RPC and distributed implementation available yet: http://tensorflow.org/resources/faq.md#running_a_tensorflow_computation, but we are working on it and hope to make it available when it's ready.
``` python
import google.protobuf
Woops, sorry liuyipei@!  Will re-open the original one.  It's the same issue, just on a different platform.
I'm really interested on contribute in my spare time on this project, it  would be great for me port the Python libraries to Go, or help with this task, the problem is that after read the "Contributing guidelines" I still don't know how to do it.
I tried to sing the Individual CLA, but I get an error that says: "You must be an owner of the contributors group in order to submit this CLA.".
And the Go ML: Some machine learning algorithms in Go: https://github.com/alonsovidales/go_ml I also implemented Neural Networks with CUDA: https://github.com/alonsovidales/go_ml/tree/cuda_implementation
I'm going to send an e-mail to the golang-nuts mail list.
Just a little update of what I have been researching this weekend. I couldn't find too much time for this, but I made some small progresses.
The bad news are that I couldn't build the system using Bazel 0.1.1 by many different problems with the linker, etc that I'm trying to resolve. And the other problem is that we have to create the .i files for SWIG in order to prepare the build for Go, I almost have this part done, but I'm also having a lot of problems.
The main issue is that I have no experience with Bazel and SWIG, so I'm spending most of the time reading documentation and trying to fix stupid problems caused because of I misunderstand something from the docu.
But by the moment for the PoC I'm using an ugly MakeFile:
I'm planning to port the process to Go by two reasons:
- You could create a pure Go application to train your model, that could be interesting for some situations.
Hi Avanti -- internally we've been working on iterating the API for RNNs, and we were happy enough with the current API to use it in the tutorial, but we're making sure it's solid before promoting it to the public API, since we'd then have to support it indefinitely.  (Anything not in the public API is a work-in-progress :)
@zer0n It depends on your task.
object
a breaking change.
parameters across multiple rnns.
@shlens, can you comment on this?
pi@raspberrypi:~ $
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
(In AWS terms, g2.2xlarge to g2.8xlarge).
``` python
import os
- [ ] cudnnOpTensor
- [ ] cudnnDestroySpatialTransformerDescriptor
- [ ] cudnnSpatialTfSamplerForward
- [ ] cudnnSpatialTfSamplerBackward
### Other Questions
I hope this helps in organizing the work around the cudnn and inspire the community to contribute. I will try to keep this issue up to date.
btw, I am new to tensorflow.
``` python
Hi Petewarden,
``` python
# PRelu
root@100cd4fb5bca:/notebooks# python -c "import tensorflow; print(tensorflow.__version__)"
Explicitly specify a cuda device in you environment variable:
**6. libcuda.so.1 not found**
**7. ELFCLASS32 error**
code:
I do not know why it happened, please help me ,thank you!
# iris_custom_decay_dnn.py
``` python
iris.target,
random_state=42)
File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 620, in convert_to_tensor
im_o+=padres
I have been experimenting with different way of reading data in tensorflow, namely:
EDIT: seems also tf.linspace and tf.range
Please specify the Cudnn version you want to use. [Leave empty to use system default]:
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
Setting up CUPTI include
(py35)Dans-iMac:tensorflow dan$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
My apologies.  As @martinwicke figured, I was using an old version of bazel (0.1.4).
still happens with a fresh cloned branch i.e.
NotFoundError: Tensor name "global_step_7" not found in checkpoint files ./ckpt_dir/model.ckpt-0
super(ZMQIOLoop, self).start()
File "/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py", line 515, in build
This is my code:
import os
os.makedirs(checkpoint_dir)
Undefined symbols for architecture armv7:
@albhaf That looks like a different problem.
time-reversed sequence
However, pay attention to a part of `def bidirectional_rnn` :
-rw-r--r-- 1 root root   560184 juin  15 16:06 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 juin  15 16:06 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
-rwxr-xr-x 1 root root   394472 juin  15 16:06 /usr/local/cuda/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 juin  15 16:06 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 78065952 juin  15 16:10 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 78065952 juin  15 16:10 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 78065952 juin  15 16:10 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 68709594 juin  15 16:10 /usr/local/cuda/lib64/libcudnn_static.a
## Problem
### Related to
@petewarden
I have no idea why these issues occur.
(Also, the email address in the commit was wrong, causing additional problems for CLAbot, which, BTW, did go through! eventually).
``` python
it is wrong
while the libcuda soname major version number should be "1").
Two issues:
Ubuntu 15.10
So it looks like the GPU libraries are failing on the first two docker images. What's interesting to note here is also that all three python statements still execute without error. I.E., running `print tensorflow.__version__` completes (and prints the correct version) and python exits with return code 0, even though the GPU failed to load properly. This is not a bug, since the fallback behavior of running on the CPU should still work. However, it also seems this is causing problems for the CI tests, since it isn't detecting a failure:
I'm still learning Docker, so it's entirely possible that I've made some blunder here, but it does seem like there's something going on here between the docker images.
- There was a bug which confused residual blocks with residual groups,
- Changed some of the variable names to clarify the distinction between
@ilblackdragon Cool. Just pushed a small change. Does this address your comments?
- adium-theme-ubuntu (0.3.4)
- apt-xapian-index (0.45)
- argparse (1.2.1)
- auxlib (0.0.39)
- colorama (0.2.5)
- conda (4.0.8)
- debtagshw (0.1)
- dirspec (13.10)
- duplicity (0.6.23)
- gyp (0.1)
- Jinja2 (2.7.2)
- Mako (0.9.1)
- mercurial (2.8.2)
- oauthlib (0.6.1)
- oneconf (0.3.7.14.04.1)
- PAM (0.4.2)
- pomodoro-indicator (0.1.0)
- protobuf (3.0.0b2)
- psutil (3.2.2)
- pycosat (0.6.1)
- pycrypto (2.6.1)
- pycups (1.9.66)
- pycurl (7.19.3)
- pygame (1.9.1release)
- pygobject (3.12.0)
- pyne (0.5.0-rc1)
- pyparsing (2.0.1)
- pyrtlsdr (0.2.0)
- pyserial (2.6)
- PyTAPS (1.4)
- pytz (2012c)
- PyYAML (3.11)
- scipy (0.13.3)
- sympy (0.7.4.1)
- Twisted-Core (13.2.0)
- Twisted-Web (13.2.0)
- urllib3 (1.7.1)
- vboxapi (1.0)
- wsgiref (0.1.2)
- zope.interface (4.0.5)
 ~  python
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
``` python
epoches = 100
Hmm, this might be an indication that what I'm doing is unreasonable :-) But my understanding is that "more data is better", and that 12 hour training runs are not super uncommon in the world of DNNs.  (My inexperience may show through here!)  I had actually (obviously, incorrectly) assumed that the type of that Variable would have been some kind of floating point datatype, given that it wasn't specified in the documentation.
I faced some problems when training model with word-embedding. For simplify the case, I make a helloworld example in this file:
The following are problems & questions:
2. op SumCoherencies with inputs: gterm, ant2, model_vis, flag, ant1, ant2
@prb12 Thanks for the detailed feedback, its very helpful for understanding Tensorflow's transfer handling.
from /data2/users/usman/anaconda2/envs/tfpy3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
@machomachopadre no, I am still confounded!!! However I have been able to run the Dockerfile and build it using the docker service.
@machomachopadre syntaxtnet doesn't work with bazel 0.2.3 check the installation instructions over [here](https://github.com/tensorflow/models/tree/master/syntaxnet)
Even if you disable GPU support when configuring `TensorFlow`, you will still need CUDA to build it. Indeed, `Eigen::half` (half floats) should only be used when running on a GPU, it's not CPU-compatible.
It looks like the issue is with the Neon GCC compiler instruction. I was able to build TensorFlow 0.9rc0 today by simply taking out `--copt="-mfpu=neon"`. I'm not an expert in the GCC instructions for ARM, but I'll continue to look into options for various optimizations that may be possible without messing up the compiler.
@MrSlayer02 linked [his error log](http://pastebin.com/zUfP4xsP) in the parallel issue thread on the TF on RPi repo- you can see it's complaining about converting to a Neon type.
Related to #2733.
followings are my code
524     unique_fetches = processed_fetches[0]
492                           'must be a string or Tensor. (%s)'
@sdemyanov is probably using a Tensor in the moving average rather than a Variable, since the documentation for `tf.train.ExponentialMovingAverage.apply` specifies that "For Tensor objects, the shadow variables are initialized to 0."
Installing TensorFlow from source fails on my system. There seems to be some Python-related error when building 'half_plus_two'.
> Codename:   xenial
> uname -a
michael@the-beast:~/devel/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(
_ZN10tensorflow6Tensor16CopyFromInternalERKS0_RKNS_11TensorShapeE+0xe6)[0x7fa8356c17d6]
/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(
/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(
/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(
/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(
00400000-00401000 r-xp 00000000 00:32 518803                             /home/bnaul/miniconda3/envs/
00601000-00602000 rw-p 00001000 00:32 518803                             /home/bnaul/miniconda3/envs/
7fa80e98a000-7fa80e98e000 r-xp 00000000 00:32 93983454                   /home/bnaul/miniconda3/envs/
7fa80e98e000-7fa80eb8d000 ---p 00004000 00:32 93983454                   /home/bnaul/miniconda3/envs/
7fa80eb8d000-7fa80eb8e000 rw-p 00003000 00:32 93983454                   /home/bnaul/miniconda3/envs/
7fa80ec8f000-7fa80ec99000 r-xp 00000000 00:32 93983517                   /home/bnaul/miniconda3/envs/
@caisq I'm running `tensorflow==0.9.0rc0` under Python 3.5, Linux+GPU (installed from the wheel).
cuda-7.5
bazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:2512:22: warning: unused variable 'swig_empty_runtime_method_table' [-Wunused-variable]
bazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:5392:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
Hi @caisq
> pip install --upgrade $TF_BINARY_URL
3. Scratch head
And thank you for your attempt to help me!
name: Quadro K2200
Aborted (core dumped)`
- bolo tie (940): 0.0145023
- bolo tie (940): 0.0171362
--input_layer="Mul:0" \
- pool_1 between conv_4 and mixed
Is there any reason? I am quite new to tensor flow so there could be something wrong on my side.
- bolo tie (940): 0.0200896
bolo tie (940): 0.0171362
in Eigen/Core and a couple of other places (Tensor)
@ebrevdo Please look at my comment [here](https://github.com/tensorflow/tensorflow/issues/1828#issuecomment-225338777).
``` python
Sooo much mutexes, my eyes are bleeding.
Is it a traceback during the "hang"?
Looks like the cuda driver is misbehaving.
Is it possible that the gpu has a defect?
> Does it use contraction with some unusual rhs/lhs operands?
$ uname -a
``` python
To see the following error:
It seems that Lite support has been added to the master branch of protobuf however: https://github.com/google/protobuf/issues/1443
@juansalas hard to say actually since I was compiling a bunch of things, but I think doing a full protobuf reinstall from the master branch should fix it.
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp35-cp35m-linux_x86_64.whl
Its not the same model:
Hi ebrevdo,
cuda 7.5
cudnn 5.0.5
Pei
Weird that I just git clone tensorflow, and have it built with bazel 3.0.0, only tensorflow-0.8.0 has been built out....
jiapei@jiapei-GT72-6QE:~/Downloads/machinelearning/deeplearning/tensorflow$ cat .git/HEAD
jiapei@jiapei-GT72-6QE:~/Downloads/machinelearning/deeplearning/tensorflow$ cat .git/refs/heads/master
![cuda](https://cloud.githubusercontent.com/assets/5551707/15954993/e761dda4-2efa-11e6-8d70-a36a8ad02239.png)
How can I run Machine Translation example on a smaller dataset.
Cuda 7.5
Cudnn 5
1. Create a conda environment
Segmentation fault: 11
File "/Users/amine/anaconda/envs/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/__init__.py", line 23, in <module>
File "/Users/amine/anaconda/envs/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 48, in <module>
File "/Users/amine/anaconda/envs/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/Users/amine/anaconda/envs/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
Referenced from: /Users/amine/anaconda/envs/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
cuda 7.5 and cudnn v4.0
1. start samba in ubuntu
4. run mninst
When loading from a checkpoint there is an error:
Raises:
if not bias:
return res + bias_term`
``` python
Bazel (0.2.1 / 0.2.3)
I strictly followed https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installation-for-linux, but run into the same error
> rsync: change_dir "/home/jiapei/Downloads/machinelearning/deeplearning/tensorflow//bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external" failed: No such file or directory (2)
> dagre                          iron_dropdown               iron_resizable_behavior    paper_header_panel   paper_tabs
> iron_a11y_announcer            iron_iconset_svg            org_tensorflow             paper_menu_button    protobuf
Pei
jiapei@jiapei-GT72-6QE:~$ bazel version
What if the current bazel-git? Will that work as well???
jiapei@jiapei-GT72-6QE:/usr/local/lib/bazel/bin$ ls
[sfux@develop01 ~]$
So I made it work but it's hacky:
[sfux@e3001 tensorflow]$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
[sfux@e3001 tensorflow]$
3.) cd $TMPDIR
Sam
Warning: ignoring _JAVA_OPTIONS in environment.
[sfux@euler04 ~]$
Can you upgrade to bazel 0.3.0?
.tanh_layer(10)
``` python
root
root
.tanh_layer(9)
root2
root2
@girving The proposal is to eventually move the module `tensorbuilder.patches.tensorflow` out of TensorBuilder to a repo managed by the TensorFlow team, that way TensorBuilder can concentrate in being a wrapper that gives nice syntax to Tensor-based libraries.
> TensorBuilder takes inspiration from [prettytensor](https://github.com/google/prettytensor) but its internals are simpler, its API is smaller but equally powerfull, its branching mechanism is more expresive and doesn't break the fluent API, and its immutable nature helps avoid a lot of conceptual complexity.
In the last release I removed the core functionality for creating layers and delegated that to libraries through patches. Now TensorBuilder's focus is to enable Tensor-based libraries with nice syntax, branching capabilities, and the DSL.
>   File "/home/yangmch/Documents/FB word2vec/RNN/FB RNN.py", line 38, in <module>
Operating System:MAC OSX El Captain
TypeError: fit() got an unexpected keyword argument 'steps'
@petewarden Done;
uname -a -m
Codename:   xenial
1. Built a new version of `protoc`:
cd protobuf-3.0.0-beta-3
Priority: extra
EIGEN_HASH=d02e6a705c30
tar xzf /tmp/eigen-${EIGEN_HASH}.tar.gz -C ${DOWNLOADS_DIR}
EIGEN_HASH := d02e6a705c30
EIGEN_HASH=d02e6a705c30
tar xzf /tmp/eigen-${EIGEN_HASH}.tar.gz -C ${DOWNLOADS_DIR}
But:
No worries. Unfortunately I am out in the woods with limited access to Internet, so I will resolve the conflict and rebase by late Monday
Test cases
uname -a
CUDA not installed
### What do I mean by inconsistent?
- CuDNN 4.0
@jiaboli007 I have seen that it is possible to have parallel computations and yet have deterministic behaviour (e.g.: Matlab simulink can optimize models for parallel computations yet assuring deterministic behaviour).
@girving Thank you for re-opening the issue but I believe tensorflow could do better than just documenting this, what seems to be an unintended non-deterministic behaviour. Also, I would have loved to work on the PR but will be too much digression from my work at the moment.
I have been trying to retrain the inception v3 network and TensorBoard has been the only real way to introspect the network and it's implementation so far.
Ubuntu 15.10
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl
1. Working on MWE...
Edit: I should mention that I'm training on a GPU (Titan X).
Please, find the main training loop below. The complete code is here: [https://github.com/abezuglov/ANN/tree/master/code], where ilt_one_layer.py, ilt_two_layers.py, etc. are the models; ilt_default_feed.py -- training on a single device, ilt_multi_gpu_feed.py -- training on multiple devices. Let me know if I can be of further help.
import gzip
import os
from six.moves import urllib
SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
job_name=FLAGS.job_name,
task_index=FLAGS.task_index)
train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')
test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')
test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')
# Bias and rectified linear non-linearity.
pool,
pciBusID 0000:09:00.0
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
e.code)
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Yes, you need cudNN v4. v5 is not supported yet afaik. See:
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Tensorflow version is 0.8.0.
Do you have any plan to release the RNN related API documents? I still cannot find them in TensorFlow R0.9 API doc: https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html.
On Fri, Jun 17, 2016 at 11:28 AM ebrevdo notifications@github.com wrote:
``` python
``` python
``` python
biasBinarized=False
new_c = tanh(c * sig_activation(f + self._forget_bias) + sig_activation(i) * (j))
``` python
state_is_tuple=False, activation=tanh, add_summary=True):
for var in inputs:
@vrv @mrry @girving
- Renamed the ops to `cumsum` and `cumprod`
Or not. Go for it @ibab
``` python
# Word2vec
return  mem_state_current, hops
``` python
# Word2vec
return  mem_state_current, hops
**Hence, the error might appear after several trails**
``` python
e.code)
Caused by op u'gradients/AddN', defined at:
File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 676, in _AggregatedGrads
``` python
e.code)
``` python
``` python
- boringssl
- farmhash
- highwayhash
As corp game-changing labs
@girving FYI.
Installation method anaconda.
[sfux@e2190 tensorflow]$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package
Warning: ignoring _JAVA_OPTIONS in environment.
[sfux@e2190 tensorflow]$
Python 3.3 is installed in a non-standard location (/cluster/apps/python/3.3.3/x86_64) and it was installed from source. The bin directory of the Python installation is in $PATH, the directory, where libpython3.3m.so.1.0 is located is in $LD_LIBRARY_PATH and the library has the correct name:
[sfux@e2190 tensorflow]$
[sfux@e1001 tensorflow]$
Sam
Please see below my bazel.rc:
[sfux@e1001 tensorflow]$
[sfux@e3001 tensorflow]$
[sfux@e2050 tensorflow]$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
[sfux@e2050 tensorflow]$
[sfux@e2050 tensorflow]$ ls /scratch/20942509.tmpdir/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/
dagre                          iron_form_element_behavior  org_tensorflow         paper_radio_group
eigen_archive                  iron_icon                   paper_behaviors        paper_ripple
iron_a11y_announcer            iron_menu_behavior          paper_dropdown_menu    paper_toolbar
iron_a11y_keys_behavior        iron_meta                   paper_header_panel     plottable
[sfux@e2050 tensorflow]$ ls /scratch/20942509.tmpdir/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/
[sfux@e2050 tensorflow]$
Sam
> https://github.com/notifications/unsubscribe/ADb6MQaGRiOtSaxBIEBNpI5e0qM4mBoZks5qMU-OgaJpZM4Iu8Yg
Added an override for `tf.size()`, that takes care of `SparseTensor` objects as well. Added tests and verified locally. This partially addresses #1968.
@ebrevdo Is there something I need to do here? I mean apart from the rebase.
@ebrevdo Could you comment on @concretevitamin's comment on dtype?
affn1 = _affine(resh1, 896, 128)
However, I meet some other errors:
Is there anything improper here?
I kind of don't understand why it works in python, but not in c++?
@petewarden
> > > /home/prayalankar/imp.py
@ilblackdragon I reviewed your comments, let me know if I misunderstood something during the reviewing.
@ilblackdragon I opened a new PR #2817  to update the `TensorFlowEstimator.fit` function.
-rw-r--r-- 1 root root  28585480 Aug 15  2015 libcublas_device.a
-rwxr-xr-x 1 root root 111231960 Aug 15  2015 libcufft.so.7.5.18
-rw-r--r-- 1 root root   1649726 Aug 15  2015 libculibos.a
-rwxr-xr-x 1 root root  36816424 Aug 15  2015 libcusparse.so.7.5.18
FLAGS.random_crop = 5
FLAGS.random_scale = 5
FLAGS.random_brightness = 5
Error: unexpected EOF from Bazel server.
job_name=FLAGS.job_name,
task_index=FLAGS.task_index)
Error with:
The Distributed TensorFlow documentation said to open an issue to request support for a specific cluster manager, support for YARN would be beneficial.
The code is for deep q-learning and for reference it's [here](https://github.com/domluna/deep-rl-gym-tutorials/tree/master/q_learning) and the keras model is [here](https://github.com/domluna/deep-rl-gym-tutorials/blob/master/q_learning/models.py)
``` python
``` python
(gdb) print buf
It looks like nonsharded savers save to the chief worker (if called from that process) and the sharded saver saves to the parameter server, weird
(gdb) print buf
(gdb) print buf
$1 = 0x26de4b0 "Unknown error 58"
I'm not super familar with the details of dlopen and symbol resolution, but the `RTLD_GLOBAL` when it loads `_pywrap_tensorflow.so` might be causing it to shadow the symbols in `libprotobuf.so` (for the cpp-enabled protobuf python package).  The C++11 vs non-C++11 libraries use different headers for `std::unordered_map` (in bits vs tr1), and things blow up when they start using the wrong library's version of the code.  (Correct me if my understanding of RTLD_GLOBAL is wrong and that's irrelevant).
@aselle Yeah, youre right, it sounds like it'd be a pain to check c++11 vs c++0x for every dependency, and it's unclear if theres enough information as-is to determine that.  I think the changes mentioned above that restrict global symbols should address my concerns though.
Is this a typo, or a reference to a now unavailable resource?
Thanks @aselle.
Have tried Bazel releases 0.2.3, 0.2.2, 0.2.1 and 0.1.4
``` python
[[Node: Variable/read = Identity[T=DT_FLOAT, _class=["loc:@Variable"], _device="/job:localhost/replica:0/task:0/cpu:0"](Variable)]]
2. Import Tensorflow `import tensorflow`
3. Got error `Segment Fault`
Here's the result of `uname -a`:
cffi (0.8.6)
chardet (2.3.0)
protobuf (3.0.0b2)
pycparser (2.10)
pycups (1.9.63)
Pygments (2.0.1)
pysmbc (1.0.15.3)
roman (2.0.0)
scipy (0.17.0)
SOAPpy (0.12.22)
Theano (0.8.2)
Raises:
The gradient of the complex op didn't take into account that it can broadcast its parameters.
``` python
W = tf.complex(W, 1.0)
`TypeError: DataType complex64 for attr 'T' not in list of allowed values: float32, float64, int32, int64, uint8, int16, int8, uint16`
``` python
I was working based off of the error message in #2255 and hoped that the gradient code would automatically work with complex numbers.
``` python
ValueError: Shapes (2,) and () are not compatible
``` python
W_ = tf.complex(W, W)
``` python
@ops.RegisterGradient("Complex")
def _ComplexGrad(_, grad):
``` python
@ops.RegisterGradient("Complex")
def _ComplexGrad(op, grad):
Thanks @ibab! That did the trick.
-rwx---r-x 1 root root 311596  2월 24 18:12 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18
-rw----r-- 1 root root 558020  2월 24 18:12 /usr/local/cuda-7.5/lib/libcudart_static.a
-rwxr-xr-x 1 root root 61453024  3월  3 03:30 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7
Any TF code that invokes loading of `libcupti.so` will face the same error, but for convenience I will share a code that can run standalone: [19-mnist-profiling.py](https://gist.github.com/wookayin/06631c68bb48fc1d0a4eee77cbbba5f1)
- After this commit, it seems that path to `libcupti.so` goes wrong. (but why?)
A strange thing to me is that tensorflow already has [a unit test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline_test.py) for CUPTI and GPU tracing functionalities, so CI must have run this test as well. This bug might be happening in some environments only (like nightly build I installed via `pip`), or it can be a a bazel-related problem (when generating packages).
I think this might be another issue.
Correct, the file [adadelta_test.py](https://github.com/tensorflow/tensorflow/blob/16d395e5dea687ab3aece0a462e631de25c8d77d/tensorflow/python/training/adadelta_test.py) passes before and after the fix. Maybe it is a tolerance issue? Or the values used in the test are too small for the sqrt to have an effect?
Sample data [5237, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] [b'anarchism', b'originated', b'as', b'a', b'term', b'of', b'abuse', b'first', b'used', b'against']
3084 b'originated' -> 12 b'as'
3084 b'originated' -> 5237 b'anarchism'
12 b'as' -> 3084 b'originated'
12 b'as' -> 6 b'a'
6 b'a' -> 12 b'as'
6 b'a' -> 195 b'term'
195 b'term' -> 2 b'of'
195 b'term' -> 6 b'a'
313         `Tensor` that doesn't exist.
super(ZMQIOLoop, self).start()
self.target_voca)
pciBusID 0000:08:00.0
pciBusID 0000:09:00.0
``` python
print sk_im
> $ lscpu
> Byte Order:            Little Endian
> BogoMIPS:              4797.20
# Swap Bytes
3. get bazel from GitHub
____Loading package: @protobuf//
____Loading package: @re2//
'/home/sio/usr/local/include/c++/4.9.3/bits/stl_algobase.h'
'/home/sio/usr/local/include/c++/4.9.3/clocale'
'/home/sio/usr/local/include/c++/4.9.3/bits/cxxabi_forced.h'
Thanks, aslle! I'll try to build older version while waiting the response. I will report its result again. :-)
What version of bazel are you using?
Can you try downgrade bazel?
- bazel : 0.2.3
- CUDA : 7.5.18
- cuDNN : 5
ERROR: /home1/chkim/.cache/bazel/_bazel_chkim/20a7d6c35f46e64c010eb39bcbc0e998/external/highwayhash/BUILD:17:1: undeclared inclusion(s) in rule '@highwayhash//:sip_hash':
``` python
import cifar10
import os
height = FLAGS.resized_image_size
width = FLAGS.resized_image_size
Added an override for `tf.rank()`, that takes care of `SparseTensor` objects as well. Added tests and verified locally. This partially addresses #1968.
$ tensorboard --port=7878 --logdir=.
Error code: 404
``` python
``` python
`docker run -p 8888:8888 -it --rm b.gcr.io/tensorflow-udacity/assignments:0.5.0`
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 664, in convert_n_to_tensor
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 620, in convert_to_tensor
Yes, it should be a typo.
I think it is trace of the days where concatenated states are used, and it would be nice to return forward and backward outputs separately.
@ebrevdo Modified! It now returns `(outputs, states)`, where `outputs` is a tuple of the forward and the backward outputs, and `states` is a tuple of the forward and the backward final state.
@ebrevdo Now it uses keyword arguments, and I refined some of the docstring.
Caused by op u'atrous_conv2d_7/SpaceToBatch', defined at:
def body(hops):
return hops
def condition(hops):
@aselle I managed to resolve my issue with a workaround, but the jpeg.h file I linked to still references header files not in the repo. It's not enabled on Android by default, but still seems broken...
@kuza55 kuza55,
Google account: sunyu.buaa@gmail.com
GitHub account: sun9700@foxmail.com
@terrytangyuan
reraise(*excinfo)
sel[arg]
raise TypeError("PointSelection **getitem** only works with bool arrays")
I'm probably not the first to stumble into this problem, but I feel like I just wasted hours of my life on a trivially avoidable problem and I hope no-one else has to waste time on that little bit of protobuf trivia.
It would allows Multi-dimensionnal input such as :
RNNs are being reworked,
With that, you should be able to operate with multidimensional input/output/states.
-it b.gcr.io/tensorflow/tensorflow
@arita297 I believe docker will only get you at most the 0.8 release. To get the bleeding-edge master branch you should download the sources and compile.
CUDAMemory(a), CUBLAS_DATA_HALF, lda,
I needed to upgrade bazel as well, but that's just an FYI.
@peyush - not sure why it's failing for you.
@Mistobaan yes there is a newer cuda toolkit that fixes the xcode issue. I am just about to make a cask pull-request to update cuda, after which there will be no need to downgrade xcode etc.
@Mistobaan ok i spoke too soon, seems like some kind soul has already done that... so no need to downgrade xcode 🎉
(py35)Dans-iMac:tensorflow dan$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
``` c++
#define TF_CUDA_DATA_HALF CUBLAS_DATA_HALF
thank you! @jendap
In edit_distance_op.cc, there are several code pieces like:
baoblackcoal@hotmail.com
CC: baoblackcoalmailto:baoblackcoal@hotmail.com; Authormailto:author@noreply.github.com
@ilblackdragon I have said "I signed it!", but @googlebot ignored me. should i pull another request ?
The wording in the docstring for nn.embedding_lookup is confusing and refers to subsets as partitions (https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#embedding_lookup)
Here is an example of incorrect usage:
And this is just really confusing:
This PR adds `complex128` to various places where it was still missing (mostly tests and docstrings).
Would `Assert` ops be the right tool for that?
I've enabled `complex64` and `complex128` on the CPU for `tf.tile` and added them to the tests.
checking for pow... no
checking for pow in -lm... yes
``` python
``` python
"pushd $$workdir",
"popd",
"popd",
genrule(
"pushd $$workdir",
"popd",
"popd",
zlib.h is in the CPATH location:
zconf.h  zlib.h
Ubuntu 14
File "/home/ubuntu/yangqichuan/my_tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 30, in run
File "/home/ubuntu/yangqichuan/my_tensorflow/my_seq2seq/my_seq2seq_model.py", line 152, in **init**
File "/home/ubuntu/yangqichuan/my_tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py", line 961, in model_with_buckets
File "/home/ubuntu/yangqichuan/my_tensorflow/my_seq2seq/my_seq2seq_model.py", line 151, in <lambda>
File "/home/ubuntu/yangqichuan/my_tensorflow/my_seq2seq/my_seq2seq_model.py", line 115, in seq2seq_f
``` py
`````` py
After searching in StackOverflow and we found `b.gcr.io/tensorflow/tensorflow-full` works.
affn1 = _affine(resh1, 896, 128)
using tensorflow::Flag;
// string image = "tensorflow/examples/label_image/data/grace_hopper.jpg";
Flag("graph", &graph),                //
@petewarden
So for example, we might see cuDNN 4.5 with the same soname (`libcudnn.so.4`).
|    0      2833    C   python                                        3801MiB |
|    1      2833    C   python                                          37MiB |
|    2      2833    C   python                                          37MiB |
|    3      2833    C   python                                          37MiB |
|    0     14226    C   python                                        3841MiB |
|    1     14226    C   python                                        3841MiB |
|    2     14226    C   python                                        3841MiB |
|    3     14226    C   python                                        3841MiB |
lrwxrwxrwx 1 sal sal       17 Mar  1 18:27 /home/sal/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7
(tensorflow)sarroff@eltopo:~$ ls -l /usr/local/cuda/lib64/libcud*
(tensorflow)sarroff@eltopo:~$ python -c "import tensorflow; print(tensorflow.__version__)"
``` python
>>> # Expected derivative
According to the [TensorFlow API documentation](https://www.tensorflow.org/versions/r0.8/api_docs/python/test.html#compute_gradient), `compute_gradient` returns the analytical and numerical Jacobians
Both seem a little hacky.
Multi-epoch use of queues might be simplified by adding one of the following:
This code is an instance in http://download.tensorflow.org/paper/whitepaper2015.pdf
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/platform/app.py", line 30, in run
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 333, in run
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 573, in _run
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 648, in _do_run
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 668, in _do_call
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/platform/app.py", line 30, in run
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py", line 193, in minimize
*args, **kwargs)
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py", line 250, in compute_gradients
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py", line 481, in gradients
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py", line 41, in _SumGrad
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py", line 33, in _safe_shape_div
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py", line 1173, in maximum
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py", line 455, in apply_op
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 620, in convert_to_tensor
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/constant_op.py", line 179, in _constant_tensor_conversion_function
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/constant_op.py", line 166, in constant
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 2240, in create_op
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 1224, in __init__
depth=FLAGS.num_features)
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/nn.py", line 712, in moments
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/nn.py", line 649, in sufficient_statistics
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py", line 909, in reduce_sum
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py", line 2087, in _sum
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py", line 704, in apply_op
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 2240, in create_op
File "/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 1224, in __init__
Ugh, this is inconvenient:
with tf.Session(graph=vae.graph) as sess:
tf.train.SummaryWriter("/tmp/vae_logs", sess.graph)
Ubuntu 15.10
-rwxr-xr-x 1 root root 61453024 apr  3 10:09 /usr/local/cuda-7.5/lib64/libcudnn.so
-rwxr-xr-x 1 root root 61453024 apr  3 10:09 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7
epsilon = 1e-4
Is this a known bug?
File "train_mnist_vae.py", line 374, in <module>
e.code)
File "train_mnist_vae.py", line 374, in <module>
However, when trying to test the seq2seq model in TensorFlow 0.7.1 right after training (in a different session, but within the same python script), an error appears.
I didn't know that this is internally fixed, so I will left only PTB examples.
This is the error:
What is the problem here?
DISTRIB_CODENAME=rafaela
NAME="Ubuntu"
VERSION="14.04.4 LTS, Trusty Tahr"
ID=ubuntu
2. Tensorflow version: 0.8.0
In python.
That results in errors related to using undeclared identifiers:
@martinwicke it looks like this got merged with 1429abd -- did that lose commit history? I was looking for how much of this still was made up of my initial contribution but it looks like that history may have been lost.
2. Correct the link to "Laplacian pyramid".
I get this error when creating the session in the Deep Dream notebook example without any modifications. Does anybody know what does exactly mean?
Manu
I tried both Distributed [tensorflow](https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html) and [minist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt). They both hang at the prepare_or_wait_for_session() function with non-chief worker tasks.
I can not find out the reasion why it behaves like this.
File "/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/variables.py", line 209, in __init__
@s0okiym @mrry  Is there any news on how to fix this? I am experiencing the exact same problem.
|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |
``` python
`tensordot` can contract multiple indices at the same time which is different with what mentioned in #216
If not, is there a workaround to input multdimensionnal time series ?
It means this is not possible.
Input Xi i=1..N samples
There is the initial transformation:
2) The input can be transformed into sub-space through W.....
ht:  (m2,  d2)     hidden state
Operating System: mac
``` python
``` python
``` python
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py", line 123, in rnn
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py", line 109, in zero_state
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py", line 109, in <genexpr>
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py", line 249, in pack
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1126, in _pack
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 704, in apply_op
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2242, in create_op
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1696, in set_shapes_for_outputs
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py", line 439, in _PackShape
File "/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 554, in merge_with
ValueError: Shapes () and (2,) are not compatible
2. With `MultiRNNCell`
I made a PR modifying this issue. Since I started studying TensorFlow yesterday, my codes might be awkward!
The kubernetes status:
``` python
/usr/local/cuda-7.5/
cudnn-7.5-linux-x64-v5.0-ga-tgz
libcudnn.so.5
2. On 18, May , I Get tensor flow via git clone --recurse-submodules https://github.com/tensorflow/tensorflow
2. Then I added one extra line in png_archive/BUILD:38 by copts = ["-Iexternal/zlib/usr/include"], and I get the error  "/home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/external/png_archive/BUILD:33:1: undeclared inclusion(s) in rule '@png_archive//:png':
/tmp/tmp.l1NdEzzkxa /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow/external/png_archive/libpng-1.2.53 /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow
checking for ar... ar
checking for dlfcn.h... yes
checking malloc.h presence... yes
checking for malloc.h... yes
checking for pow... no
checking for pow in -lm... yes
e.code)
It seems to be a general problem cross multiple runs:
e.code)
However, another problem occurs when I run `my_mudule.my_ops` in `with tf.Sessin():`.
So far, I I have to say that TensorBoard is a really great tool to analyze the training procedure.
Currently, the only way to "document" a Run is the name of the log directory - so I find myself encoding all kinds of cryptic acronyms into the directory name.
@danmane Thanks, that's great to hear.
RUN git ???
``` python
import os
I noticed that a TensorFlow version of the algorithm was running about 10-20x slower than a theano version of the same algorithm.
I guess my issue/question is: Is there any ongoing work to improve performance for Reinforcement Learning scenarios like these, especially on CPUs?
Thanks for the suggestion, unfortunately this particular mac pro doesn't seem to have fma:
[1]    13850 illegal hardware instruction
## Problem
Tensorflow Version: 0.7.1
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 4.0.7
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
Setting up CUPTI include
python tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py
File "tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py", line 236, in <module>
File "tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py", line 232, in main
File "tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py", line 221, in run_benchmark
File "tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py", line 177, in time_tensorflow_run
No one has ever met??
#if !defined(CUDNN_H_)
Just a typo fix.
``` diff
``` python
[[Node: pack_7 = Pack[N=4, T=DT_COMPLEX64, _device="/device:GPU:0"](Complex_4, Complex_5, Complex_6, Complex_7)]]
for complex64 and
``` python
[[Node: pack_7 = Pack[N=4, T=DT_COMPLEX128, _device="/device:GPU:0"](Complex_4, Complex_5, Complex_6, Complex_7)]]
for complex128.
@girving Cool!
@girving cool!
Is there any configuration file which can be used to configure the default behavior such as which gpu device to use and so on? Just like the .theanorc file in package Theano? If there isn't, I hope some one can add this feature in the future code.
than `boundaries`.)
and occasional crashes for certain specific operations, like
Because of this I (seem to) need to resort to approaches like
which clearly doesn't scale.
Is there a better way? Specifically:
or, alternatively
So I guess as long as I'm "doing it right", I'm not worried; and the question boils down to that: Is it idiomatic to be manually breaking up and reassembling data fed to TF operations as needed when they result in calculations that are too big for the hardware?
@orome I've written a tool called [hypercube](https://github.com/ska-sa/hypercube) to reason about problem sizes and memory requirements that you may find useful.
Anaconda uses Python 3.5 as its version of Python 3.
presumably because the wheel is made for Python 3.4 instead of 3.5
Twitter: @fabmilo
## Github: http://github.com/Mistobaan/
Welch)
Perfection must be reached by degrees; she requires the slow hand of time
The best way to predict the future is to invent it (Alan Kay)
Creating bottleneck at /tmp/bottleneck/roses/8949720453_66e8304c30.jpg.txt
``` python
``` python
``` python
Operating System:  Mac OsX (El Capitain)
App will crash intermittently somewhere completely unrelated in code (usually native).
I know this is user error, but it would have been really helpful if an exception had been raised.  I wasted a lot of time trying to hunt down a mystery error when it turned out it was the saver all along.  This can be confusing to beginners such as myself.
java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_9-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_8-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_7-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_6-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_5-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_4-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_3-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_2-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_1-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_0-classes.dex"],nativeLibraryDirectories=[/vendor/lib, /system/lib, /vendor/lib, /system/lib]]] couldn't find "libtensorflow_demo.so"
It'd be great to be able to sample from gamma distributions in TF. That would make it possible to sample from many other popular distributions, such as:
- Inverse-Gamma (just the inverse of a gamma r.v.)
- Gumbel (location-scale transformation of a log-transformed inverse-gamma)
`testOverloadComparisons` doesn't test for `complex64` because we can't define `greater/less` ops for it.
The second part of `testOverloadComparisons` only seems to operate on booleans.
(And, strangely, tests `tf.equal` and `tf.not_equal` instead of the overloaded operators).
Then it would make sense to add `complex64` and `complex128` to the test.
I've tried to always append `complex128` at the end, as that's what has seemingly been done for previous types.
Calling `__ldg` on a `complex128*` doesn't compile for me, which means that I can't call
Installed version 0.8.0 with anaconda.
File "/hltsrv0/rocha/rte-lstm/src/rte_lstm.py", line 189, in __init__
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 241, in compute_gradients
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 485, in gradients
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1784, in tuple
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1664, in with_dependencies
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 655, in apply_op
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2154, in create_op
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1165, in __init__
File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1318, in _recompute_node_def
MemoryError
pciBusID 0000:09:00.0
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
Aborted (core dumped)
@gpapan
GCC 4.8.4
Cuda 7.5
Cudnn 4.0.7
Bazel 0.2.2b
ERROR: /home/peiguo/.cache/bazel/_bazel_peiguo/67521296d96959142b2d5d303d9c774c/external/re2/BUILD:9:1: undeclared inclusion(s) in rule '@re2//:re2':
If on amazon use this ami number ami-a19b67c1
Can you try updating Bazel to 0.2.3?
ERROR: /scratch/peiguo/tensor-flow/tensorflow/tensorflow/core/kernels/BUILD:1267:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:batchtospace_op_gpu':
cpu: "armeabi-v7a"
toolchain_identifier: "stub_armeabi-v7a"
# Android tooling requires a default toolchain for the armeabi-v7a cpu.
abi_version: "armeabi-v7a"
abi_libc_version: "armeabi-v7a"
host_system_name: "armeabi-v7a"
supports_normalizing_ar: false
target_libc: "armeabi-v7a"
target_cpu: "armeabi-v7a"
target_system_name: "armeabi-v7a"
toolchain_identifier: "stub_armeabi-v7a"
linking_mode_flags { mode: DYNAMIC }
linker_flag: "-Wl,--hash-style=gnu"
linker_flag: "-Wl,-rpath,/home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/local_config_cc"
linker_flag: "-L/home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/local_config_cc"
supports_normalizing_ar: false
mode: DBG
linking_mode_flags { mode: DYNAMIC }
no without --config=cuda
@DKP-90 No I am not on Amazon.
But one more question, is there any solution for building with cuda support?
ERROR: /home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/re2/BUILD:9:1: undeclared inclusion(s) in rule '@re2//:re2':
COLLECT_GCC=gcc
Thread model: posix
Anyhow, the fix for cuda should be to add the following line:
@Override
buildTypes {
java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_9-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_8-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_7-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_6-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_5-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_4-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_3-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_2-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_1-classes.dex", dex file "/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_0-classes.dex"],nativeLibraryDirectories=[/vendor/lib, /system/lib, /vendor/lib, /system/lib]]] couldn't find "libtensorflow_demo.so"
Lifu Huang
Then code:
``` python
import pandas as pd
File "/Users/qdang/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/trainer.py", line 49, in train
``` python
@vinhqdang `pd.factorize` return a tuple; the encoded values and the index/labels.
``` python
3. building with bazel fails
'/include/c++/4.8.5/bits/stl_algobase.h'
'/include/c++/4.8.5/clocale'
'/include/c++/4.8.5/bits/cxxabi_forced.h'
'/include/c++/4.8.5/cerrno'
Cuda 7.0
cuDNN 4.0.7
rdipiet2@jhu.edu@login-node03 tensorflow $ bazel-bin/tensorflow/tools/pip_package/build_pip_package testonly
Tue May 17 11:09:33 EDT 2016 : === Using tmpdir: /tmp/tmp.akIEI1WBAX
This PR updates documentation of installation instructions for conda users.
here is my code-
import random
lorange= 1
hirange= 10
amplitude= random.uniform(-10,10)
random.seed()
tau=random.uniform(lorange,hirange)
bias_2)
`nan` can appear in many situations like, for example, division by zero.
name = "grpc",
A tangle of problems.....
@mrry    yea, it works, thank you !
Exception:
shutil.move(old, new)
--data_dir=/data1/imagenet1k \
Caused by op u'FFT', defined at:
Thanks, it was the big `#ifdef GOOGLE_CUDA` was the problem. It looks like there is already ab implementation, at least the convolution utilizes FFT (https://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/third_party/eigen3/unsupported/Eigen/CXX11/src/NeuralNetworks/TensorConvolutionByFFT.h#L234). Maybe I'm misunderstanding this line (the `.template` is a little bit confusing).
File "/home/padraig/.local/lib/python3.4/site-packages/tensorflow/**init**.py", line 23, in <module>
File "/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/**init**.py", line 20, in <module>
File "/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/**init**.py", line 22, in <module>
File "/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/io/**init**.py", line 20, in <module>
File "/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py", line 23, in <module>
Sulaiman
replica_id = FLAGS.task_index,
e.code)
e.code)
So is there something wrong in my multi-machine code?
I can not find out the reasion why it behaves like this.
x is a list of list of tensors, and im passing in one list at a time to the rnn but I get this hard to understand error message
I have started learning tensorflow recently. I am trying to input my custom python code as training data. I have generated random exponential signals and want the network to learn from that. This is the code I am using for generating signal-
import random
lorange= 1
hirange= 10
amplitude= random.uniform(-10,10)
random.seed()
tau=random.uniform(lorange,hirange)
It doesn't make sense. Tensorflow can't solve the problem when the graph is very big and deep? There must be a solution. @mrry
``` python
activation=[tf.nn.tanh, tf.nn.relu6]
verbose=1
``` python
Edit: Actually meant Resnet-164, not Resnet-152
I tried to learn Tensorflow and ran the .py file I created by copying the example code on the Tensorflow website. it runs well initially and printing it's training epochs. But finally, it gave some error showing as below:
(tensorflow)xu@xu-ThinkCentre-M72e:~ $ python BuildConvNet.py
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 502, in eval
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 340, in run
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 564, in _run
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 637, in _do_run
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 659, in _do_call
e.code)
[[Node: Mean_3/_1035 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_911_Mean_3", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 295, in conv2d
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 655, in apply_op
File "/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1154, in __init__
``` python
Isn't this means a batch of 50 images? Sorry, I'm a beginner.
But maybe that is not the best thing to do. So, any suggestions?.
Tensorflow version: 0.8.0
Vanishing-Gradient problem is occurred at conv2.
Hi @girving,
I think this may be a typo, because `1e-4` is same value with conv1 layer.
neuron=relu
I think this may be a typo, because `1e-4` is same value with conv1 layer.
layer.
Logits.
Alessandro
I guess I met the same problem, @aleSuglia .
So I guess that's a bug? Thanks much!
Ziyu
Exactly @LittleYUYU!
[[1,2,3,PAD,PAD]
[6,7,8,9,PAD]]
then the mask can be
2.VirutalEnv install
@mrry Thanks and big facepalm for me :).
Han
I'm using Tensorflow [last succeed build #85](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/lastSuccessfulBuild/)
lambda: (ema_mean, ema_var))`
Knowing that software history is indispensable for developers (e.g., developers need to refer to history several times a day), I would like to ask **TensorFlow** developers the following four brief questions:
3. Do the _newcomers_ faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems?
`ind = [0, 1, 1, 0, 0]`
Twitter @rocksetta
@ebrevdo: I've had a look at this and realized that the output shape of `PaddingFIFOQueue` is still dynamic where the input shape was dynamic. This makes sense, as we can only find out what the maximum size per dimension is when we execute `dequeue_many` at runtime.
Anaconda Python 2.7
./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/mnist_logs/
`DEBUG:tensorflow:Opening a record reader pointing at /tmp/mnist_logs/test/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`
`DEBUG:tensorflow:No more events in /tmp/mnist_logs/test/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`
`INFO:tensorflow:No path found after /tmp/mnist_logs/test/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`
`audio -`
`histograms`
`layer1/biases`
`layer2/biases`
`images`
`scalars`
`mean/layer2/biases`
`min/layer1/biases`
`min/layer2/biases`
`audio -`
`histograms`
`outoforder_steps     []`
`images`
`outoforder_steps     []`
`scalars`
`outoforder_steps     []`
`audio -`
`histograms`
`layer1/biases`
`layer2/biases`
`images`
`scalars`
`mean/layer2/biases`
`min/layer1/biases`
`min/layer2/biases`
`audio -`
`graph`
`outoforder_steps     []`
`histograms`
`outoforder_steps     []`
`images`
`outoforder_steps     []`
`scalars`
`outoforder_steps     []`
@danmane Thank you for quick response. I have attached the event files in
Hi @danmane,
cudnn version 5
``` python
initializer=PRETRAINED_EMBD)
Side note: when training with multiple cards (independent runs), the frequent data transfer between gpu and cpu is particularly harmful in my case.
@vrv Thanks for your attention. Updated. Do you know why the test failed on the matmul?
OS:  redhat 6.7
building with cuda
export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'
I am not entirely sure whether this is a TensorFlow issue (the error seems to indicate that we need to add a `ld` flag) or a Bazel issue in the gcc wrapper.
+cc @lberki @damienmg
weights: Tensor, [batch_size, feature_size], linear transformation
matrix.
Just pulling in the conversation from #2318, since it's really more appropriate here.
When training longer, the loss will eventually become `Nan`. The error does not occure, when using the following `Loss` Code:
ze = z.eval()
print('random value y:')
random value y:
[False False]]
CUDA version: 7.5
cuNN: 4.0.7
ERROR: /home/julialintern/tensorflow/tensorflow/core/BUILD:756:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command
TMPDIR=/tmp/user/1001 \
Is it other issue that i haven't noticed?
@beopst
@mrry
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
--data_dir=/data1/imagenet1k \
File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
When reading the threading_and_queues how_to and the doc of QueueRunner the phrase "repeatedly run an enqueue op" confused me.
If you are not accustomed to the queue class it is not obvious that repeatedly runing an enqueue op will block.
Should I raise a separate issue for non-scalars then? The behavior is surprising and in my opinion quite likely to result in bugs. It's true that it's documented, though.
So this caught my eye because of the pandas tag, but I suppose if I am to be consistent, it should apply to #2328 as well.
My feeling is that this kind of everything-should-map-to-everything coercion is harmful to a programming environment, mostly because it undermines the type system. Type errors turn into logical errors, which are far harder to debug. This is the reason I avoid pandas (sorry, @shoyer!), and I'd rather it not be in TensorFlow either.
> This is the reason I avoid pandas (sorry, @shoyer!), and I'd rather it not be in TensorFlow either.
I probably should have directed my emoji-flavored vitriol elsewhere. Or perhaps, you know, actually taken the time to write a coherent response.
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
lateset one
When using tensorflow with `tf.app.run()` one cannot `import ipdb`. I have tested several Tensorflow Versions and did run Tensorflow in several environments, all of them seems to be affected by this Bug.
File "/home/marvin/.virtualenvs/tensorvision/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 29, in run
The suggested usage could be (pseudocode):
``` python
The first argument could be the last output (state), then all `non_sequences` could be passed and, finally, the current input `el`. The pseudocode above would produce the following operations:
Unfortunatelly I haven't found any hack that would achieve the same behavior (especially when `non_sequences` contain objects other than tensors, such as `GruCells`). Maybe there is some, in that case I would really appreciate some hint :)
However, imagine that `c1` is `GruCell`. It is forbidden to access such computational nodes that are not passed directly by scan from the inner function (afaik).
initializer=init_state_bh)`
/usr/local/cuda
/usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/include
/usr/local/cuda/include/cudnn.h
when asked for `libcudart.so`?
https://github.com/hholst80/tensorflow is a snapshot of the customizations I made to build TensorFlow v0.8.0 on Ubuntu 16.04 LTS.
ERROR: /home/alberto/Temporary/tensorflow/tensorflow/tools/pip_package/BUILD:8:1: error loading package 'tensorflow/core': Extension file not found. Unable to load package for '//google/protobuf:protobuf.bzl': BUILD file not found on package path and referenced by '//tensorflow/tools/pip_package:other_headers_gather'.
> Caused by op u'random_crop/Assert'
The error seems to be `_parse_bazel_version` not expecting a non decimal character.
Have you tried bazel 0.2.2?
Yes. Bazel 0.2.2 worked perfectly.
Tensorflow version: 0.7.1
Oren
EDIT: After leaving the crashed and out of memory Tensorflow process running, I saw this new error:
FLAGS.hidden1,
FLAGS.hidden2)
epel-release-6-8.noarch
1. bazel clean
Edited google/protobuf/BUILD
changing
Is there some plan to implement some more complex `crop_tensor` function that would work with dynamic tensors? And that would possibly be able to handle https://github.com/tensorflow/tensorflow/issues/2284
``` python
pciBusID 0000:09:00.0
I also noticed that the documentation for [Using GPUs](https://www.tensorflow.org/versions/r0.8/how_tos/using_gpu/index.html) doesn't mentioned about tf.Variable, it only involves the tf.constant and tf.matmul.
I want ask that since tf.Variables is pinned to CPU by tensorflow, could we fix this error? Do we need to looking very carefully to exclude the tf.Variable declaration outside the `with tf.device('/gpu:xx')` scope, or use netsted `with tf.device(None)` to handle it?
``` python
File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 77, in _IndexedSlicesToTensor
Is it a bug?
Hi, girving:
The problematic IndexedSlices comes from "tf.gradients(cost, tvars)". Because if I remove the wrapper "tf.clip_by_global_norm" and leave "grads, _ = tf.gradients(cost, tvars)" alone, the same error shows again.
``` python
``` python
Seems tensorflow has optimized when the previous `a` is a placeholder. Here's another version of toy snippet.
3) Install bazel 0.2.1 via brew.
[1]    78583 segmentation fault  python
@qbx2 Hm, I'm not sure what you mean? `_pywrap_tensorflow.so` exists in that location, and if you're referring to `@rpath/libcudart.7.5.dylib`, the file is in /Developer/NVIDIA/CUDA-7.5/lib, which is where my CUDA was installed (default location from NVIDIA installer). That's why I set DYLD_LIBRARY_PATH.
I think the original dtruss output may have been from a shell containing a tensorflow directory, which would have caused the output in the first Gist. Sorry for the confusion.
``` python
super(ZMQIOLoop, self).start()
Is there anything I have to do ? I can see the exception comes from `.eval()` function. I guess @vincentvanhoucke might be right person to ping.
@ebrevdo Any update on this?
@ebrevdo Ping
Reason for commit: For a student who has been going through the udacity examples, this particular use of the minimize function is a bit confusing as its difference between the previous examples is subtle. To the student it appears that only the softmax weights are being modified.
Does anyone have an idea what could cause these TLB shootdowns or what can I do to find out more about what is going on?
![Image](http://i.imgur.com/RaivaXq.png)
Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
Cuda 7.0
cuDNN 6.5
export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'
# ll libcud*
1. stackoverflow ;-)
name: Quadro M2000M
name: Quadro M2000M
Added the complex128 support, based on the guidance from @girving and others in PR #2244. All tests (in CPU) including backwards_compatibility_test are passed! I couldn't test the GPU version.
@zffchen78, I don't think we have tests for complex128 for now, but we will have some very soon.
epel-release-6-8.noarch
cd bazel/
Looks like the bazel is too new.
bazelbuild/bazel@857cda2
Its very nice that you support so many complex number calculations like `tf.complex_abs` and `fft`. I am trying replicate this [Associative LSTM paper ](http://arxiv.org/abs/1602.03032)where complex numbers are needed.
However, when I try to calculate the gradient using `tf.gradient`, I get the traceback below. Is it not possible to calculate the gradient if complex numbers are used with type `tf.complex64`? If not, this would be an incredibly useful feature as there are several new RNN papers that require complex numbers to be used.
``` python
if complex_weights:
`````` python
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 94, in _IndexedSlicesToTensor
TypeError: DataType complex64 for attr 'T' not in list of allowed values: float32, float64, int32, int64, uint8, int16, int8, uint16```
We also need to operate in the complex domain. Now you can simulate imaginary numbers with float32 numbers, but still -- we can't use FFT's which is a critical piece.
There are many other networks that use complex numbers like Associative LSTMs which could receive significant speed boosts if complex64 could be backproped. Thanks!
Maybe it's a specific op that hasn't been extended to `complex64` yet that's causing the problem?
``` python
z = tf.complex(x, x)
#above line works if changed to out_=diag1 * out_
File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py", line 620, in convert_to_tensor
File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py", line 94, in _IndexedSlicesToTensor
TypeError: DataType complex64 for attr 'T' not in list of allowed values: float32, float64, int32, int64, uint8, int16, int8, uint16, float16
I've submitted a PR that enables complex numbers for the segment reduction ops.
@ibab thanks for the update. I was using the transpose op as well, so this is probably what was causing the issue. I'm in the middle of testing other architectures right now, but when I get a chance, i will test the Unitary RNN  and Associative LSTM and post back. I will build from the current master.
@ibab thanks for your pull as it did resolve the `tf.transpose` issue that was occuring. However, a new issue has arisen. When I `optimizer.apply_gradients()`, I get the following error message:
gradient code:
``` python
if grad is not None and grad.dtype.name != 'complex64':
``` python
Basically, there are both complex64 and float32 trainable variables that I have in the `seq2seq.py` which is based upon tensorflow's `seq2seq.py`. These trainable variables enter the params argument in `apply_gradients`.
Is there a problem when you use trainable variables of both `float32` and `complex64` types? I really appreciate your help. I'm running rc version 0.9.
~~That's strange, as MatMul has support for `complex64`, although only on the CPU.
Currently complex support is a bit poor for GPU ops, as discussed in this issue: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matmul_op.cc#L219~~
The `MatMul` is actually the input and it's the optimization framework that's complaining about its dtype.
I'm not sure whether it makes sense to run an optimization with complex parameters or a complex loss function.
``` python
z = tf.complex(x, y)
Thanks again @ibab -- can't believe I didn't think of that. I'll try that out and get back to you with results.
Reporting back to @ibab  -- I tried `tf.complex(var,var)` and unfortunately it produced the same error as above. I looked at your crossed out message and I am running this on a titan x -- so this could be the problem. Here's my weight matrix code:
``` python
if complex_weights:
@ibab, the loss function is the seq2seq sequence loss function from tensorflow ops:
``` python
Raises:
Raises:
``` python
z1 = tf.complex(0., x)
``` python
def complex_mul_real( c, r ):
def complex_abs_sq(z):
return complex_mul_real( in_, scale )
return tf.complex(re,im, name=name)
with vs.variable_scope(scope or "ULinear"):
return complex_mul_real(in_c, ( nn_ops.relu(n+bias)*scale ))
with vs.variable_scope(scope or 'ULinear'):
return complex_mul_real(in_c,scale)
@khaotik thanks for sharing your unitary code. I have a similar script for unitary. I think as the implementation stands right now, the gradient you're processing from your Unitary Cell is not correct as @ibab illustrated with his newly raised issue. Hopefully this will be resolved in a later build.
@ibab maybe I'm misunderstanding something here. In both the Associative LSTM and Unitary, there are intermediate complex calculations that are called for. How could all gradients be real if there are complex calculations occurring?
Perhaps I'm missing something here, but it seems clear that complex gradients need to be established before either of these two types of RNN's can work.
Maybe some of the variables that you differentiate by are still complex?
@LeavesBreathe
``` python
They are diagonal.
> Maybe some of the variables that you differentiate by are still complex?
@ibab  You could be right. I will carefully analyze all of my `trainable variables` and ensure that they are all real.
@khaotik
- Added Out Bias
``` python
def complex_mul_real( c, r ):
def complex_abs_sq(z):
return complex_mul_real( in_, scale )
return tf.complex(re,im, name=name)
with vs.variable_scope(scope or "ULinear"):
return complex_mul_real(in_c, ( nn_ops.relu(n+bias)*scale ))
with vs.variable_scope(scope or 'ULinear'):
return complex_mul_real(in_c,scale)
@LeavesBreathe Thanks for improving my code. I just put the new code in a new [repo](https://github.com/khaotik/char-rnn-tensorflow), which is a fork of [char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow). Feel free doing experiments or contributing.
Nltk version 3.2.1
Segmentation fault
When importing nltk and then tensorfklow:
gcc: error: language cuda not recognized`
-x language
" "] + cuda_copts
but the -x cuda syntax appears a bit weird!! what am i missing  here??
____[1 / 481] Linking external/re2/libre2.pic.a
> A Tensor. Must be one of the following types: float32, float64, int32, complex64, int64
cudnn-6.5-linux-x64-v2.tgz of
Loaded plugins: fastestmirror, langpacks
- epel: mirrors.neusoft.edu.cn
My Os is cenos7.1.
Why do I have this error?
This is an Docker-Issue.
7.5 is CUDA
Code is also in this stack Overflow:
Instinctively, I have the impression that this would need a new C++ op that generalizes `While`.
``` python
# true if I am a leaf (could have probably derived this from if I have
if node.isLeaf:
Now I am running into weirdness that I cannot explain...
@mrry
$ which jupyter
This is very strange. I will investigate further.
``` python
So I follow the code of "Inception":
BTW, is there any RNN sample code that could run on multi-cards(multi-tower style) or distributed multi-machines? I tried to port multi-tower from CIFAR-10 example code to my RNN code, after 1 week's work I failed(I'll report the bugs in another issue). Then I gave up and turn to distributed style. The "Inception" example code needs BIG data set ILSVRC2012 and it's time consuming to download the data(especially from China...). Since RNN is especially usefull in many application domains, would you mind show me the multi-cards or multi-machines existing code if you know?
@mrry
verbose=True)
e.code)
I found I have created a big SparseTensor
CUDA Version : 7.5
Tensorflow version: 0.8.0
plt.title("BILINEAR")
plt.imshow(img_bic)
plt.title("BICUBIC")
plt.title('AREA')
![bicubic](https://cloud.githubusercontent.com/assets/650407/15034081/88c2a41c-12a4-11e6-9681-f008ec36ff0f.png)
-          complex64);
+         int64, complex64);
BTW, this fix was inspired by other ops like pow:
int64, complex64);
@yaroslavvb I see the point now. Thanks for the explanation. @zheng-xq suggested API doc update. I guess it would be a simple solution and let users promote their int values to float.
Caused by op u'Conv2D_19', defined at:
"A Tensor. Must be one of the following types: float32, float64, ~~int32~~, complex64, ~~int64~~"
That being said, I just want to point out that this behavior is not limited to GPU builds. There were sets of hyperparameters in which I would get high accuracy in Mac OSX (CPU) and Ubuntu 14.04 (CPU), but a lower accuracy in Ubuntu 14.04 running on AWS (CPU).
https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#log says "x: A Tensor. Must be one of the following types: float32, float64, int32, complex64, int64", but int32 and in64 throws InvalidArgumentError:
e.code)
Caused by op u'Log', defined at:
-rw-r--r--   1 root root    383336 Eyl 19  2015 libcudart.so.7.5.18
=> libre2.a and so on
protobuf re2 \
protobuf_lite \
import cifar10
tf.app.flags.DEFINE_string('checkpoint_dir', 'cifar10_train/',
But it occurs the bug:
File "/home/swoda/tensorflow/tensorflow/models/image/cifar10/cifar10.py", line 192, in inference
### Todo
Exception:
shutil.move(old, new)
raise Error, errors
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
Given the code
``` python
``` python
e.code)
I'm running Tensorflow installed from pip, version 0.8.0. As far as I can tell, this is caused by the while_loop creating a new control flow context, but the check numerics op living outside that context.
``` python
The problem is that in tensorflow 0.7, I get the following error when the session is closing (exiting python):
Aborted (core dumped)
The error is being thrown here:
@zheng-xq I don't understand these things that well, why would you prefer signed to unsigned?
Like for example in Torch:
@danmane yea sure. I'll check it out tomorrow
1. Install anaconda
3. Follow tensorflow install guide for Anaconda
Part of the problem is the Conda installation instructions are incorrect. It says to create a new virtual environment with Python 3.5 but the wheel is for 3.4.
@akors
cudnnTensorFormat_t format;
PATH=/home/ubuntu/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
PATH=/home/ubuntu/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
For Cuda:
sudo apt-get install -y cuda
sudo cp cuda/include/cudnn.h /usr/local/cuda-7.5/include/
@cristianmarti Thanks, what version of Bazel are you using?
Bazel 0.1.4
with vs.variable_scope(scope or type(self).__name__):  # "GRUCell"
File "/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py", line 247, in <lambda>
File "/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 554, in merge_with
ValueError: Shapes (32, 33) and (32, 132) are not compatible
@terrytangyuan Thank you so much!
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl
with vs.variable_scope(scope or type(self).**name**):  # "GRUCell"
I invoke the GRU cell in the following manner
I am wondering if Tensorflow has some op for fast calculation of distances. For example like Matlab's [pdist](http://nl.mathworks.com/help/stats/pdist.html) or [pdist2](http://nl.mathworks.com/help/stats/pdist2.html)
``` python
I've already cut-pasted the cudnn libs (above) here are the others
ls -l /usr/local/cuda/lib64/libcublas*
-rwxr-xr-x 1 root root 23938736 Aug 15  2015 /usr/local/cuda/lib64/libcublas.so.7.5.18
-rwxr-xr-x 1 root root 111231960 Aug 15  2015 /usr/local/cuda/lib64/libcufft.so.7.5.18
ls -l /usr/local/cuda/lib64/libcurand*
pciBusID 0000:09:00.0
Aborted (core dumped)
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl
1. Here is the code I was using: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5%20-%20User%20Interface/graph_visualization.ipynb
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl
``` python
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 565, in convert_to_tensor
I understand the primary use case for `one_hot` will be when reading in data from a file, so having to convert TensorFlow objects isn't crucial, but the thought was to support as many data types as possible.
- Am I going overboard with the tests? I modified them to iterate over NumPy dtypes and TensorFlow dtypes, as we weren't testing `tf.constant` objects before ([which gave us surprise errors](https://github.com/tensorflow/tensorflow/issues/1799#issuecomment-215913080)). However, this jumps the test time from ~2.5 seconds to ~5 seconds for CPU-only, and from ~4.5 seconds to ~9 seconds with GPU.
@ebrevdo Thoughts, when you get the chance?
In the tutorial 'Convolutional Neural Networks', tensorflow just computed the accuracy, but I wanted to compute the confusion matrix. I have trained the model, saved the variable and don't want to do it again. I am able to load the variables. Then three approaches crossed my mind: give new input to cifar10.inference; replace with new operations; directly output the prediction result. But I failed all of them, because: the inference does not take placeholder, no such operation could be implemented; multi-threaded code stopped me.
The current description, "also be used with higher dimensional shapes" is not very clear.
Is  there a way to transpose a SparseTensor without converting it to dense?
Also the implementation AFAICU would be something like
-rw-r--r--   1 root root    383336 Eyl 19  2015 libcudart.so.7.5.18
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
2. I started a tensorflow environment: "source ~/tensorflow/bin/activate"
Yeah so this is just like some horrible edge case and part of the reason for the UNIQUE_SYMBOL mess in NumPy I guess.
ubunutu 14.04 LTS
@abronte I tried so many things but I couldn't find any solution. Sorry. This is very critical and fundamental problem. Without fixing the graph collection errors, this can't be solved clearly.
@abronte No I didn't try yet. I will test the master branch few days later.
<unknown>
<unknown>
<unknown>
<unknown>
When I'm running the Inception examples from the Image Recognition tutorial (https://tensorflow.org/tutorials/image_recognition/), I'm getting a lot of warnings in the following form:
I recently upgraded Tensorflow to version 0.8.0, and I'm working on Mac OS X 10.10.2. While this isn't harming the performance, I'm wondering if there's a fix or an update that I'm missing.
I obtain a floating point exception when I run the following code
I suspect that the output of redsum = [], which makes l2 disconnected from A_ph for the given ind. Fed forward to computing l2, I get the correct value of 0.0 (since there is nothing to be summed). However, for backpropagation of gradients, I think the output of redsum=[] chokes the algorithm, and causes it to evaluate some illegal operation using an empty tensor, instead of just backpropagating 0.0.
``` python
Anyway, the warning seems to indicate that the O2 flag is missing somewhere.
2) Today I managed to build TF for the first time, so I can't tell, but I
``` python
A modified GRU cell to handle weigths (took only minimal modification to **init** and **call**)
``` python
"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""
with vs.variable_scope(scope or type(self).__name__):  # "GRUCell"
``` python
biases = []
biases.append(b)
## Questions
@ebrevdo I agree with the gates, also they would only work for basic GRU/LSTM.
``` python
"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""
candidate_activation = tanh):
with vs.variable_scope(scope or type(self).__name__):  # "GRUCell"
biases = []
Any news @ebrevdo?
When I visualize it on tensorboard, thickness of edge(tensor) looks weird.
@MikulasZelinka I only conduct the second step u mentioned after altering gcc-5.3 to gcc-4.9, then it works. I'm on ubuntu 16.0.4 with cuda 7.5 cudnn v5.
`cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"` in tensorflow/third_party/gpus/crosstool/CROSSTOOL
``` python
subdir
Current difficulties:
[New Thread 0x7ffed0abc700 (LWP 46109)]
pciBusID 0000:42:00.0
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
at ../Objects/abstract.c:2529
(gdb)
did you look into this @lukaszkaiser @poxvoculi
I will try and write one big script after my finals to see if I can reproduce this. Tensor flow is the first thing I import in all my classes. I guess I can try importing Tensorflow at the end and see if it helps.
There are two ExponentialMovingAverage in cifar10_multi_gpu_train.py.
``` python
P.S. The document of ExponentialMovingAverage (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L172-L177) says
> `````` python
``` python
If I put sess.close() before coord.join() the threads are killed but there is still a weird warning which should not exist because I called queue.close() before.
-rw-r--r--. 1 root root   1649726 Aug 16  2015 libculibos.a
-rwxr-xr-x. 1 root root  36816424 Aug 16  2015 libcusparse.so.7.5.18
3, try cudnn V4, V5
(gdb) bt
I seem to be having this problem too, or maybe the one in #2034. Tensorflow runs just fine, as long as numpy has already been imported.
(gdb) r
Detaching after fork from child process 10971.
@poxvoculi , the program has only one line "import tensorflow"
Yes! Incredibly import numpy before tensorflow fixes this issue. What's weird is this issue started happening today, and I've run this same code 50+ times before this started happening.
21   syntax='proto3',
Should try to get the version of protobuf in the path with:
`protoc -v`
@girving
I've switched to anaconda and I get back to this issue right now.
(gdb) bt
@bhack this is the best I can do for now, pls feel free to correct me and provide some more thoughts. thx.
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 306, in _SwitchRefOrTensor
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 565, in convert_to_tensor
Cuda: 7.5.18
Cudnn: 4.0.7
|    0      3504    C   python                                       11379MiB |
P2/P3 - Balanced 3D performance-power
@girving I am still not sure why shape is ambiguous with `stride > 1`? Is this statement only valid for `padding=VALID` and `stride>1`?
MacPorts
With bazel 0.1.1:
With Bazel 0.2.1:
'/home/david/.local/include/c++/4.9.3/bits/stl_algobase.h'
'/home/david/.local/include/c++/4.9.3/bits/cxxabi_forced.h'
'/home/david/.local/include/c++/4.9.3/bits/stl_algo.h'
'/home/david/.local/include/c++/4.9.3/bits/stl_algobase.h'
'/home/david/.local/include/c++/4.9.3/bits/cxxabi_forced.h'
'/home/david/.local/include/c++/4.9.3/bits/stl_algo.h'
With Bazel 0.1.5:
'/home/david/.local/include/c++/4.9.3/bits/stl_algobase.h'
'/home/david/.local/include/c++/4.9.3/bits/cxxabi_forced.h'
'/home/david/.local/include/c++/4.9.3/bits/stl_algobase.h'
'/home/david/.local/include/c++/4.9.3/bits/cxxabi_forced.h'
'/home/david/.local/include/c++/4.9.3/bits/stl_algobase.h'
'/home/david/.local/include/c++/4.9.3/bits/cxxabi_forced.h'
`ERROR: /home/yadid/tensorflow/tensorflow/WORKSPACE:21:1: Traceback (most recent call last):
invalid literal for int(): "2b".`
I am guessing some inconsistency between the latest bazel and latest tensorflow?
@Dapid: I think you are experiencing some misalignment between bazel and tensorflow on top of this bug.
not sure what is going on.
Now you have to configure the GCC build. For details, check out the [GCC configuration page](https://gcc.gnu.org/install/configure.html). I suggest installing into a custom prefix, such as `/opt/gcc-4.9`. I suggest enabling only C and C++ and skipping the rest of the GCC languages to save time and disk space. The options --with-as, --with-ld and --with-nm are required, because otherwise TensorFlow build will fail, complaining that those binaries cannot be found.
## Compiling bazel
To compile bazel, you need to specify
- -c opt: Don't know, says so in the docu
@akors: I see this issue happening not on fedora but ubuntu as well. Just a plain development setup from sources cloning master bazel and tensorflow
Explanations extended:
build: what bazel should do
@akors I followed your instructions, but unfortunately I keep getting the same failures. No idea what the difference between your system and mine may be.
@Dapid did you use Bazel HEAD and Tensorflow HEAD? And you are still getting "missing dependency declaration" errors?
@Dapid weird. If you have modified your third_party/gpus/crosstool/CROSSTOOL file to point to your private install, then I am out of ideas. What happens if you add another line
@akors well spotted.
Custom GCC, Bazel 0.2.3 (Release), TensorFlow head (b3621c95160a916d4d255f9f44318b9d465701af) with CROSSTOOL `cxx_builtin_include_directory` modified to the custom GCC include paths: compilation succeeds.
Bitmap bitmap = BitmapFactory.decodeFile("/storage/8A38-1900/1/1.jpg");
The code for begginers tutorial however works well in both the gpu and cpu.
- CUDA: 7.5
- cuDNN: 5
- Anaconda:  4.0.0 (64-bit)
- Tensorflow: 0.8.0 (running on conda environment)
1. I tried the MNIST tutorial for begginer and it works well on both cpu and gpu
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Aborted (core dumped)
-rwxr-xr-x 1 root root 5396088 Apr 22 15:05 libcuinj32.so.7.5.18
Am I misunderstanding something here? I fail to see floats.
ubuntu@ip-172-31-32-186:~$ python -c "import tensorflow; print(tensorflow.__version__)"
**After uninstall TensorFlow0.8 and install TensorFlow0.7.1, the problem disappeared!**
>>> pandas.__version__
>>> scipy.__version__
2. What might be the reason of this problem since I am not familiar with PIL or TensorFlow code ?
or just the apache part?
William Chan
William Chan
@ilblackdragon The skflow is not suitable for out-of-core hdf5 data, some hdf5 or matlabv7.3 data are the same size as ImageNet. Maybe a reader randomly acesss hdf5 data and feed the model with reasonbale memory is still necessary.
cc: @ilblackdragon
When I downgrade TensorFlow to 0.7.1 everything runs fine, with both, Matplotlib 1.5.1 and 1.4.3.
Operating System: Manjaro Linux
`uname --all`
Linux 4.4.6-1-MANJARO #1 SMP PREEMPT Sat Mar 19 06:00:37 CET 2016 x86_64 GNU/Linux
funcsigs==1.0.1
louis==2.6.5
pickleshare==0.7.2
protobuf==3.0.0b2
pyparsing==2.1.1
pytz==2016.4
Surprisingly:
What kind of dark magic is this?^^
Mysterious...
fi
You may also want to remove license and pypi badges.
for example in speech recognition: Bahdanau/Chorowski et. al.,
William Chan
Well, the python side does not know what mask to generate (i.e., it doesnt
William Chan
> But why would it need a special op to make the mask a function? If it's a
> not a mask?
William Chan
> > not a mask?
See http://stackoverflow.com/questions/36807107/strange-error-when-taking-gradient-of-tensorarray
(the TensorArray is written-to to yield a new TensorArray which is passed to the next iteration - the difference in my code is that the array is also read from).
@eriophora you can use a placeholder, no need to add code
coor_shape = np.shape(unravel_pool1_coordinates)
we will get a bug like
insert `cxx_flag: "-D_FORCE_INLINES"` into `tensorflow/third_party/gpus/crosstool/CROSSTOOL`
Johnny
Thanks @poxvoculi for the response.
``` python
$CUDA_HOME/lib/libcudadevrt.a
$CUDA_HOME/lib/libcudart.so
$CUDA_HOME/lib64/libcudnn.so
$CUDA_HOME/lib64/libcudnn.so.4
$CUDA_HOME/lib64/libcudnn.so.4.0.7
$CUDA_HOME/lib64/libcudnn_static.a
ema = tf.train.ExponentialMovingAverage(decay=0.9)
Looks like something is wrong in backward pass call to cudnn.
Where is wrong? It seemed that the cuda and cudnn went wrong, but how to fix it?
Please help me!
@poxvoculi
On Apr 29, 2016 1:31 AM, "wangbm" notifications@github.com wrote:
this is my code
Ps: I'm running on GPU machine using the last build [tensorflow](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/lastBuild/)
@laouer or @ilblackdragon Could one of you please tell me how I get at that best step value in python, the one that's printed to the console after stopping.
scope = "myscope"
I am a little bit curious on that decision, and therefore opened a thread in the discussion group: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!topic/discuss/u8eIulwX_OY
2. Make the GPU version do a different operation - I added 2 instead of 1 - so you can verify which version is running
AssertionError
We first noticed this on a much more complicated custom operator. This is a regression from the tensorflow version 0.7.1, which worked for us. The steps above are the result of several days spent trying to reproduce the problem with a minimal operator. The critical things seem to be using floats instead of ints and using the standard session instead of the tf test one.
2. Applied the following patch to TensorFlow: https://gist.github.com/akors/5db13e874c144b3b111f0e0326d7b771#file-tensorflow-custom-gcc-patch
ERROR: /home/alexander/.local/src/tensorflow/tensorflow/cc/BUILD:28:1: Executing genrule //tensorflow/cc:random_ops_genrule failed: namespace-sandbox failed: error executing command
ps.: Out of curiosity, what are you TensorFlow devs using for a development machine? Has any of you actually tried using a modern Linux distribution that comes with GCC newer than 4.9 for compilation? You really should. It's quite the experience.
@black-puppydog Thanks for your information.
Now you have to configure the GCC build. For details, check out the [GCC configuration page](https://gcc.gnu.org/install/configure.html). I suggest installing into a custom prefix, such as `/opt/gcc-4.9`. I suggest enabling only C and C++ and skipping the rest of the GCC languages to save time and disk space. The options --with-as, --with-ld and --with-nm are required, because otherwise TensorFlow build will fail, complaining that those binaries cannot be found.
## Compiling bazel
To compile bazel, you need to specify
- -c opt: Don't know, says so in the docu
@aehlig
INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
'build' options: --crosstool_top=//third_party/gpus/crosstool --define=using_cuda=true
INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
'build' options: --crosstool_top=//third_party/gpus/crosstool --define=using_cuda=true
> INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
> INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
>   'build' options: --crosstool_top=//third_party/gpus/crosstool --define=using_cuda=true
> INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
> INFO: Reading options for 'build' from /home/alexander/.local/src/tensorflow/tools/bazel.rc:
>   'build' options: --crosstool_top=//third_party/gpus/crosstool --define=using_cuda=true
> libcudart.so.
Hi, @itsmeolivia , is there any particular reason why you closed this issue?
@akors Thanks a lot for pointing this out - it took me quite some time until I found this thread...
> libprotobuf.a
> libprotobuf_lite.a
> libre2.a
name = "androidsdk",
api_level = 23,
It really occur.  very very strange.
protobuf re2 \
protobuf_lite \
2. Upgrade the protobuf
`(gdb) bt
@amrmousa I tried to reinstall protobuf 3.0.0b2 with modified _kDefaultTotalBytesLimit_ in `coded_stream.h` manually, while it does not have segment fault, but it is still has 64MB limit error.
@doubler I am interested to know if you have this problem while installing tensorflow from source or just installing it using pip only? In my installation, I can see that protobuf 3.0.0b2 is installed but I did not test this 64MB limit issue.
@amrmousa I solve this problem by installing tensorflow from source. And it does not have the 64MB limit. While I don't know whether it is the fast protobuf version?
So, I understand that the fast c++ one is the only one that does not have the 64MB limit.
@amrmousa I think the document say "Note that the binary pip package" the pip package means the `protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl` not the tensorflow package.
-uninstall protobuf
-go to `tensorflow/google/protobuf`
@VDalibard Seems still not work:-(
pip install --no-cache-dir --upgrade /export/liujia/tensorflow_pkg/tensorflow-0.8.0-py2-none-any.whl
Processing /export/liujia/tensorflow_pkg/tensorflow-0.8.0-py2-none-any.whl
pip uninstall protobuf
cd google/protobuf
cd python
$ pip list | grep protobuf
protobuf (3.0.0b2)
$ pip list | grep protobuf
protobuf (3.0.0b2.post2)
@cegeme you can install the fast protobuf manually.
And you can refer to @VDalibard said.
@VDalibard your symptoms sound similar to mine:
- I have installed tensorflow by pip and anaconda and also installed the caffe.
I got a problem by anaconda so I reinstalled the tensorflow 8.0 by pip
What are the relevant versions of Pandas on the failing machines?
Dask requirements list `pandas >= 0.16.0`.
@meanmee @liuyifly06 @byungjo @ruilog @jessebett
I have neither dask nor pandas installed on my ubuntu machine. I still see this error for tensorflow 0.8 as soon as I import tensorflow. Although tensorflow 0.7.1 works fine for me.
@terrytangyuan how are you directing people to install dask?  If you are using a requirements.txt file then the correct entry is probably `dask[dataframe]` rather than just dask.
Ah, are they `pip installing dask` on their own then?  (I'm getting these error reports myself now)
Had a related (I presume) issue with current tensorflow@d42facc3 and bazel@c728a631, just adding this here for reference:
I strictly followed https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installation-for-linux, but run into the same error
> rsync: change_dir "/home/jiapei/Downloads/machinelearning/deeplearning/tensorflow//bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external" failed: No such file or directory (2)
> dagre                          iron_dropdown               iron_resizable_behavior    paper_header_panel   paper_tabs
> iron_a11y_announcer            iron_iconset_svg            org_tensorflow             paper_menu_button    protobuf
Pei
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 4
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
Setting up CUPTI include
I work on ubuntu 14.04
File "/home/matthieu/external/tensorflow/_python_build/tensorflow/python/__init__.py", line 59, in <module>
NDK 10re
UBUNTU 14.04
> kaushalya@kaushalya-HP-350-G1:~/TensorFlowAndroidDemo-master/jni-build$ make
> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= tensorflow_jni.cc
> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= imageutils_jni.cc
> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= jni_utils.cc
> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= rgb2yuv.cc
> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= yuv2rgb.cc
> [armeabi-v7a] SharedLibrary  : libtensorflow_demo.so
(I've also upgraded to TensorFlow r0.8)
> Path: /home/kaushalya/.android/avd/Nexus_5X_API_23.avd
> Skin: nexus_5x
> SD Card: /home/kaushalya/.android/avd/Nexus_5X_API_23.avd/sdcard.img
> tag.id: google_apis
> avd.ini.encoding: UTF-8
CUDNN: cudnn-7.0-linux-x64-v4.0-prod.tgz
(gdb)
Oh wait.. the problem appears if we put TensorFlow _before_ numpy... maybe not it
It seems that in my case it wasn't just numpy...
import scipy
@mouendless, @chengdianxuezi  and @zhang8473 all seem to have a problem with just the single `import tensorflow` include. I apparently only have the issue when scipy.ndimage is imported after tensorflow...
![screen shot 2016-04-28 at 2 51 29 pm](https://cloud.githubusercontent.com/assets/12982311/14902573/baaba1ee-0d50-11e6-874e-efdbdc42f569.png)
Hey, progress! I downgraded to NumPy 1.8.2 and the issue is no longer there.
But at the same time, try as I can, I can't figure out where the problem is in TensorFlow.
``` python
from scipy.special import *
>>> from scipy.special import *
I'm tempted to hack in a PY_ARRAY_UNIQUE_SYMBOL thingy into f2py and see what happens.
https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#train-your-first-tensorflow-neural-net-model
- Cuda 7.5, CuDNN 4
$ uname -a
Requires: numpy, six, protobuf, wheel
Classifiers:
Intended Audience :: Developers
Intended Audience :: Science/Research
from /home/moose/.local/lib/python2.7/site-packages/numpy/core/multiarray.so
## However
>>> import keras
I reinstalled `scipy` and `numpy` by:
@girving This issue is in r0.8.
@zszhong  Hey, thanks for your comment! But are you sure about your readline version? I just found readline 2.4.2 as the oldest version. I tried by reinstalling the newest version but the error is still there, maybe in my case it doesn't work with readline...
@zszhong Okay I see..I will try later, I was just wondering how you realized that you should reinstall readline? For me it seems a very random discovery...
@amineHorseman Are you sure you have the latest version (`print(tensorflow.__version__)`)?
GCC version:4.8.5
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Floating point exception
# My custom model.
... custom model definition here ...
I'll dig into it.  But in the meantime, this is fine to merge ([signcla/kashif](http://signcla/kashif))
Also, I see that you have the CLA check required, so you can't just merge past this (probably not a good idea until we have better tools to manually override the CLA check).  In the meantime, @kashif would you mind leaving a comment here?  Doesn't matter what it is, but a comment from you will force the CLA check to run again.
Wrong email address vis-a-vis CLA is fixed.
CUDA 7.5
cuDNN 5.0.4
I run some black-box test (as i don't know a better way to debug it), and found that it raises error when it try to compute gradients with the statement
sys.path.append('/home/jiahua/tensorflow')
Starting TensorBoard  on port 6006
`(tensorflow)[ arnaudlejeune /tmp/mnist_logs ] find /tmp/mnist_logs/ | grep events | xargs ls -lh
cc: @ilblackdragon
LSTMCellBlock, LSTMCellBlockGrad, LSTMBlock, LSTMBlockGrad
Epoch: 13 Train Perplexity: 35.804
Acknowledgements: lots of discussions w/ ebrevdo!
@vrv , @martinwicke warning: i only looked at cuDNN once so i could be wrong, but iirc, cuDNN requires a workspace scratch for the fprop and bprop, meaning u need to allocate memory in the fprop to pass back thru in the backprop:
I agree cuDNN is important, but that I think it should be independent?
William Chan
> tensorflow/contrib/rnn/ ?
> See some of the other examples of custom C++ ops in contrib/ for how to
I also have a feeling that this LSTM block is very specialized to be part of tensorflow's API.
@keveman , FYI, had to add
Is Eigen Contraction supposed to support doubles?
fyi, i found a workaround.
https://github.com/iassael/torch-bnlstm
ImportError: cannot import name rnn_test
William Chan
On Wed, May 11, 2016 at 10:27 PM, ebrevdo notifications@github.com wrote:
@Mistobaan, the prev code is here:
@wchan
but its complaining the import is not allowed:
ImportError: cannot import name rnn_test
Error etc.
William Chan
@ebrevdo Can you say when this performance improvement will be merged into master?
``` python
``` python
@lucasmoura you also have to run:
@girving Wait, I think I misunderstood what I should do. I originally thought that I should reimplement the confusion matrix using only a python wrapper for SparseTensorDenseAdd, but should I left the c++ implementation and add this new python code as well ?
@girving I have tested the code locally and there is a problem in the fact that both sparse_add parameters are not SparseTensors.
``` python
/home/lucas/Envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.pyc in sparse_add(a, b, thresh)
--> 195     raise TypeError("At least one input should be SparseTensor; do you mean to"
Weird, I don't see any relation of both build failures with the code I am adding. I will look into the BUILD file I have modified to run the confusion matrix tests to see if it is the cause of the trouble.
/cc @mesosphere
@mckelvin I'm interested in the implementation.  Is it possible to share with the rest us that the possibilities opened by `tfmesos`?  For example, how does it simplify the process of running a distributed tensorflow?
``` python
from tfmesos import cluster
mesos_master = sys.argv[1]
mckelvin@mesos1 ~ $ cat ./run.sh
-e MESOS_MASTER=mesos1 \
-e DOCKER_IMAGE=tfmesos/tfmesos:latest \
-v /home/mckelvin/demo.py:/tmp/demo.py \
mckelvin@mesos1 ~ $ ./run.sh
/cc @mtamburrano
experimental work is here:
https://github.com/douban/tfmesos/pull/3
K8s is in house but I don't that @mrry is against Mesos contribution.
@mrry Yes probably in tensorflow.contrib to have a little bit more exposition to the project.
cuda-7.5
cuDNN 5
2. import scipy.misc
It seems cudnn doesn't like `ksize<stride` when `ksize>1`, thus the commented out test.
I think the way Theano does it seems more centered.
discarding /home/dlm/anaconda3/bin from PATH
(tensorflow)dlm@pc-aero-01:~$ python -V
(tensorflow)dlm@pc-aero-01:~$ pip -V
(tensorflow)dlm@pc-aero-01:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl
(tensorflow)dlm@pc-aero-01:~$ pip3 -V
(tensorflow)dlm@pc-aero-01:~$ sudo -H pip3 install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl
(tensorflow)dlm@pc-aero-01:~$ python
(tensorflow)dlm@pc-aero-01:~$ python -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))'
(tensorflow)dlm@pc-aero-01:~$ python3 -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))'
(tensorflow)dlm@pc-aero-01:~$ python -m tensorflow.models.image.mnist.convolutional
(tensorflow)dlm@pc-aero-01:~$ python -c "import tensorflow; print(tensorflow.__version__)"
discarding /home/dlm/anaconda3/bin from PATH
(tensorflowX)dlm@pc-aero-01:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl
(tensorflowX)dlm@pc-aero-01:~$ python -m tensorflow.models.image.mnist.convolutional
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
dlm@pc-aero-01:~$ wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl
discarding /home/dlm/anaconda3/bin from PATH
(tensorflow3)dlm@pc-aero-01:~$ pip install --ignore-installed --upgrade tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl
(tensorflow3)dlm@pc-aero-01:~$ python -V
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Please help me, i don't like to use python 2.7.
``` python
- Run steps in parallel but provide nonsense values for graph dependencies on the last step
- Do magic beyond my ken to skip ahead in serial computations? if so, can I tell it to only run the last one, thereby creating a halting oracle?
Operating System: mac os 10.11
alembic (0.8.5)
Babel (2.2.0)
cffi (1.5.2)
colorama (0.3.7)
cryptography (1.3.1)
Flask-Cache (0.13.1)
Flask-Migrate (1.8.0)
Flask-Script (2.0.5)
Flask-WTF (0.12)
gnureadline (6.3.3)
gunicorn (19.4.5)
idna (2.1)
itsdangerous (0.24)
Jinja2 (2.8)
Mako (1.0.4)
pickleshare (0.7.2)
pyasn1 (0.1.9)
pycparser (2.14)
vboxapi (1.0)
@dsmilkov , I learnt from @danmane , that such a feature already exists for debug purposes in tensor flow, not yet available for users. What do think about possibility of making it accessible to users.
This works for mac:
> falls are absolutely gorgeous this time of year.
@ilblackdragon Don't waste your time, I already fixed most of it yesterday. Just wanted to make sure that it should be markdown. Check pull request
Aborted (core dumped)
Should I be using an older version of Cuda and/or Cudnn? Thank you
`pip show numpy six protobuf`
From: ebrevdo
-rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7
-rw-rw-r-- 1 ubuntu ubuntu 590993388 May 11 07:51 gan.ckpt-1400
1. Use 'conda env' instead of virtualenv
File "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
python cifar10_train.py
> > Downloading cifar-10-binary.tar.gz 100.0%
tar -xf cudnn-7.0-linux-x64-v4.0-prod.tar
fortran \
bumpy \
scipy \
gensim \
sacred \
pymongo \
cd bazel
I also compiled the cuda samples and they all run without a problem
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
name: GRID K520
Initialized!
![Imgur](http://i.imgur.com/JLnLCB6.png)
Starting TensorBoard  on port 6006
Operating System: Linux Mint 17 Qiana
``` python
fetches = {
``` python
``` python
the fetches dict.
so in vision this would correspond to input images of varying size within a batch? If so I would be muchmuch interested in that, too, but suspect it's not going to happen any time soon :P
@black-puppydog yes it corresponds to input images of varying size in computer vision
I am doing the project using Tensorflow. I set up it on Clound9 since It run on Ubuntu. I chose python 2.7. However, It kept showing an error: enter image description here . Does anyone have any idea why it keep happening? By the way, I clone tensorflow github in my repository. Thanks
(tensorenv)ngoduyvu:~/workspace (master) $ pip install --upgrade https://storage.googleapis
.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl
However, it gave me the error:
Exception:
*_kwargs
raise ArgumentError(action, message % conflict_string)
raise ArgumentError(action, message % conflict_string)
@keveman  I use Kubuntu 15.10
``` python
``` python
``` python
_temp2 = tf.reshape(temp1, [5000, 5000, 25]) - A # OOM error.
Or, is my code wrong?
But, I'm a little confused why the following code still gives OOM error.
``` python
``` python
Help!!!!!
@hunkim's overlapping PR is a bit strange.
@ibab Thanks for pointing it out. I was not aware of this PR. I think we should go for your PR first.
In order to get the tests to pass, I've converted a few more of the ops to `complex128`, but I tried to keep it minimal in order to not conflict with @hunkim's PR too much.
@ibab if you want, feel free to grab my changes into your PR.
> complex128, but I tried to keep it minimal in order to not conflict with
@hunkim:
(cuda-gdb) bt
from /home/ryan/src/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
from /home/ryan/src/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
from /home/ryan/src/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
(cuda-gdb) info threads
(cuda-gdb) thread 3
(cuda-gdb) bt
(cuda-gdb)
/home/bnaul/miniconda3/envs/plt_tf/lib/python3.5/site-packages/IPython/core/formatters.py in __call__(self, obj)
340             # Finally look for special method names
-> 2180                     **kwargs)
/home/bnaul/miniconda3/envs/plt_tf/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, *args, **kwargs)
It is much more than just `jpeg`. I've been hit by this problem for weeks now, and investigating a bit further, I have a snippet of C++ code below that helps reproducing the weird behavior:
./opencv_tensor grace_hopper.jpg
Ubuntu14.04
4.2.5-1-ARCH
@yaroslavvb I didn't think about it.
Or maybe the T is not a shadow at all but just a painting on the backplane ;)
Tensorflow version: 0.6.0
Exception:
shutil.move(old, new)
Alex
CUDA: 7.5
Is this very slow speed intended?
Oren
Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']
3084 originated -> 5239 anarchism
Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']
originated -> anarchism
originated -> as
I actually had a look at doing it myself but I was getting some strange numerical behaviour from the finite differences test. Possibly just something I am doing wrong. If we can work out what is going on with the test I would be happy to help.
I then played with adding the MatrixTriangularSolve op here:
CUDA - 7.5.18
CuDNN - 4.0.7
ERROR: /global/home/users/kmuriki/.cache/bazel/_bazel_kmuriki/7a46079e1611cbcdacd2bbe4113de14c/external/re2/BUILD:9:1: C++ compilation of rule '@re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 36 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
Manjunath,
PYTHONPATH.
these values weren't of the exact right type- it now casts the values to
Pinging @ebrevdo for review.
throws a type error because it's expecting float values for on_value and off_value.
@ebrevdo See the latest changes- is this closer to what you're looking for?
The other issue is that, unfortunately, string values aren't working properly in `one_hot` at this time. Try running this command instead and see if it works:
I signed the cla.
@ilblackdragon just tried it. Still seeing absolute paths in there - we had `./my_model` before and I tried `my_model` and `my_model/`, both of which still created a checkpoint file with absolute paths.
Still some old version of documents
``` python
I am guessing that the email used for commit was lukaszbk@users.noreply.github.com, since I created the commit in the web UI I had "keep my email private" setting checked. I think I'll close this pull request and try another one with a different email.
I think it is scipy or numpy's issue.
under anaconda python 3.5
scipy (0.17.0)
File "/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/sklearn/manifold/t_sne.py", line 832, in _tsne
File "/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/sklearn/manifold/t_sne.py", line 387, in _gradient_descent
File "/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/scipy/linalg/misc.py", line 129, in norm
Under anaconda.
2.$ brew install bazel swig
Starting TensorBoard  on port 6006
Or What is the way to lanuch doc
I'm having second thoughts about the suggested implementation. I'm sure it works fine but I'm a bit worried about consistency: a number of functions in `SpecialFunctions.h` in eigen use equality tests. For example, `igamma` tests for equality with zero and I also test for equality with one in the implementation of the zeta function. Is there something special about the representation of one and zero in memory that we don't have to worry about serialisation issues? Would it be worthwhile adding a helper function `integer_equal_impl` for all such comparisons?
@ilblackdragon README seems to be broken in `.md` format.
@maniteja123 I would suggest looking at how to implement `tf.pow()` as it is a binary operator. I have already figured out how the unitary operator code and tests need to look like, so I will be putting them out one by one.
@ebrevdo The implementations were based on @concretevitamin's comment [here](https://github.com/tensorflow/tensorflow/issues/1828#issuecomment-225325574), which was a reply to @mrry's comment just before that (which raised the same issues you raised). I think an option should be added later for the operators to make the result tensor dense, if and when applicable.
I also receive the following error:
[[Node: Skipgram = Skipgram[batch_size=16, filename="wbu.txt", min_count=5, subsample=0.001, window_size=5, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
e.code)
[[Node: Skipgram = Skipgram[batch_size=16, filename="wbu.txt", min_count=5, subsample=0.001, window_size=5, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
Caused by op u'Skipgram', defined at:
The issue seems to be that `corpus_size_` is an int rather than long and so overflows...
/usr/local/cuda-7.5:
tag   "b.gcr.io/tensorflow/tensorflow:latest-devel-gpu"
yunlong@dl-y9:~/github/tensorflow$ ls -l /usr/local/cuda/lib/libcud*
yunlong@dl-y9:~/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
yunlong@dl-y9:~/tmp$ git clone https://github.com/google/boringssl.git
Cloning into 'boringssl'...
Your tutorial on deep dream was great !!
Laurent
So it seems that I cannot simple replace a TensorFlow operation with a Python function. Is there a way to implement a Siamese network using built-in operations in TensorFlow?
i'm happy to help with the integration if you are.
3) Optimizer: AdamOptimizer.
I'm not hugely experienced with either neural networks or Tensor Flow, but it sounds like it could be the "vanishing gradients" problem. When you have a very deep network (and30 layers is _very_ deep), the gradients in the loss function become very small, and I have seen elsewhere that this can cause NaN values. It sounds like in the single step you mention, the gradients reduce and cross a precision threshold, and therefore become undefined.
Jiabo
However when I call image_identity->DebugString(), it succeeds. I see in the source code that DebugString() calls name(). But in the return string there are messy code,the character '{' all become messy code while others are correct.
@eldor4do
I already checked [1534](https://github.com/tensorflow/tensorflow/issues/1534) but in my case the compute mode for GPU is set to default. So I don't think that is the problem.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Floating point exception (core dumped)
Type "apropos word" to search for commands related to "word"...
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
(gdb)
(gdb) bt
from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
😬
When I remove the random_crop the error doesn't occur.
``` python
I did some looking into this, and it looks like the error is due to using `tf.convert_to_tensor` as the way to handle different-typed tensors on [these lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1860-L1861). This works when passing in Numpy arrays, but not with TensorFlow constants.
~~@ebrevdo @girving, would it be possible to import `tf.cast` in `tensorflow/python/ops/array_ops.py`? I think using that instead of `convert_to_tensor` would fix this.~~
Current hack-y workaround is to manually specify variable name as `"{}/name".format(previous_scope_name)`
``` python
@gaohuazuo hmmm that assert statement returns True (as long as you are consistent about `vscope` vs `vs`); however...
>>> u'foo/Variable:0'
Oren
`Extracting data/train-images-idx3-ubyte.gz`
`Extracting data/train-labels-idx1-ubyte.gz`
`Extracting data/t10k-images-idx3-ubyte.gz`
`Extracting data/t10k-labels-idx1-ubyte.gz`
this is a bit of a hack, but copying the gradient for maxpool and registering it for max_pool_with_argmax works. The function signature has an additional arg (presumably this has to do with the argmax). Also the op doesn't have a data_format, so that is left default (NHWC).
``` python
data_format='NHWC')
Warning: ignoring _JAVA_OPTIONS in environment.
Recompiling Bazel from source should be the correct solution.
Probably this `.whl` installation is not meant for Anaconda.
Add a wiki reference for ReLu in tutorial TF mechanics 101
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
Aborted
Aborted
cc: @ilblackdragon
This issue is a feature request to add support to tensorflow to implement networks with stochastic depth (http://arxiv.org/abs/1603.09382).
Jiajie
name = "androidsdk",
api_level = 21,
api_level=21)
Error Code:
Ubuntu 15.10
cuda v 7.5
jdk8
980Ti
MNIST example and CIFAR10 example run very slow on GPU, much slower than posted benchmarks for my specs (2 titan X).  GPU utilization very low when running these models.  Loading 20k images for CIFAR10 takes more than 10 mins
Downloading cifar-10-binary.tar.gz 100.0%
pciBusID 0000:09:00.0
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
pciBusID 0000:09:00.0
Initialized!
ok, let me update to cudnn r4
@yazabaza : it seems cuDNN 2.0 solved the problem for both @kmitchner and me. I am running CUDA 7.5 and he is running CUDA 7.0. Let us know what solves the problem for you.
- TF_FLOAT
- TF_BOOL
- Modified AsBool, AsFloat32, … by Bool, Float32, …
I think that could be a good idea if we send an e-mail to some groups like golang-nuts@googlegroups.com or discuss@tensorflow.org to try to get more feedback, what do you think?
The last modifications applied are:
- Modified 'd by 'i' on GetVal
Fixed the problems with the GetVal and also the typos :)
Saving to: ‘tensorflow/examples/label_image_go/data/inception_dec_2015.zip’
➜ ~/go-src/src/github.com/tensorflow/tensorflow (go_bindings_tensors) unzip tensorflow/examples/label_image_go/data/inception_dec_2015.zip -d tensorflow/examples/label_image_go/data/
bolo tie : 0.01450234
➜ ~/kaggle/state_farm sudo find / | grep tensorflow | grep -v avidales | grep "\.so"
Cons:
$ ln -s $GOPATH/src/github.com/alonsovidales $GOPATH/src/github.com/tensorflow
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
ERROR: /home/ling/Code/tensorflow/tools/defaults/BUILD:11:1: no such package 'third_party/java/android/android_ndk_linux/toolchains': BUILD file not found on package path and referenced by '//tools/defaults:crosstool'.
- A more obscure but related issue: https://github.com/tensorflow/tensorflow/issues/1511
on each slice WxH slice A.
b'originated' -> b'anarchism'
b'originated' -> b'as'
b'as' -> b'a'
b'as' -> b'originated'
b'a' -> b'as'
b'a' -> b'term'
b'term' -> b'of'
b'term' -> b'a'
Several problems with the code:
Aborted (core dumped)
Results in this error
ppanda = np.zeros(predictions.shape,np.float32)
However, when implemented in C++, these 2 tensor input caused me a lot of troubles!
e.code)
I'm a little confused where the tensor with `shape[10000,23000]` comes from... The weights in the first layer should be [724,23000], and in the second layer they should be [23000,10].
1. I use anaconda environment
``` python
# synthetic data
[[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=["loc:@Variable_1"]](Variable_1)]]
# pragma omp parallel for simd
[sudo] password for sandy:
IOError: [Errno 2] No such file or directory: '/tmp/pip-8L3Eho-build/setup.py'
IOError: [Errno 2] No such file or directory: '/tmp/pip-8L3Eho-build/setup.py'
python retrain.py --image_dir ~/flower
Requires: numpy, protobuf, wheel, six
``` python
inputs: 3-D `float` `Tensor` sized
Raises:
``` python
@ebrevdo
Hi ebrevdo ,
Im probably misunderstanding what `labels` represents.  In the docs it says:
@akashgit, the eigen code in TensorFlow supports the functions but they won't be available from python until the PR above is merged. You can have a look there if you need them urgently.
log_device_placement=FLAGS.log_device_placement))
I wonder if someone also tried to do some double gpu training using adam.
Changes:
I'll need to grab a AWS AMI to setup Jenkins with GPU support to test it. How about separate gpu-specifics to another PR?
`poporo@poporo-All-Series://tensorflow/tensorflow/examples/android$ bazel build //tensorflow/examples/android:tensorflow_demo
cannot create symbolic link bazel-out -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out:  /tensorflow/bazel-out (Permission denied)
cannot create symbolic link bazel-tensorflow -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow:  /tensorflow/bazel-tensorflow (Permission denied)
cannot create symbolic link bazel-bin -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-opt/bin:  /tensorflow/bazel-bin (Permission denied)
cannot create symbolic link bazel-testlogs -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-opt/testlogs:  /tensorflow/bazel-testlogs (Permission denied)
cannot create symbolic link bazel-genfiles -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-opt/genfiles:  /tensorflow/bazel-genfiles (Permission denied).
Error: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
SEVERE: Error during merging resources
Error Code:
bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
Error Code:
bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory
name = "androidsdk",
api_level = 23,
path = "/home/poporo/android-sdk-linux",
path="/home/poporo/android-ndk-r10e",
api_level=21)
zel-genfiles (Permission denied).
This is a different issue which should not happens in tensorflow. Did you run ./configure?
developersriharsha@instancenew:~/tensorflow/tensorflow$
-rw-r--r--  1 root root 11318 Apr  1 07:29 bower.BUILD
version of Bazel :bazel-0.1.4
SEVERE: Error during merging resources
Error Code:
Error Code:
FYI there's a torch implementation here: https://github.com/iassael/torch-bnlstm
File "/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py", line 30, in run
File "/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 477, in gradients
File "/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_grad.py", line 137, in _TensorArrayPackGrad
File "/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_grad.py", line 62, in _GetGradSource
Uncaught exception. Entering post mortem debugging
(Zaremba, et. al.) Recurrent Neural Network Regularization
The exact results may vary depending on the random initialization.
# The alternative version of the code below is:
if verbose and step % (epoch_size // 10) == 10:
iters * m.batch_size / (time.time() - start_time)))
if not FLAGS.data_path:
verbose=True)
On Thu, Mar 31, 2016 at 7:22 PM, ebrevdo notifications@github.com wrote:
cc: @ilblackdragon
``` python
``` python
pciBusID 0000:82:00.0
This is really anoying, because I have to kill the python kernel to free the resources!
/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _extend_graph(self)
ema = tf.train.ExponentialMovingAverage(.9)
return ema.average(inpt)
Curiously, if I remove the `tf.identity` above in the definition of `update`, neither of them performs an update after starting a new session.
Even more confusing to me:
ema = tf.train.ExponentialMovingAverage(.9)
avg_without_update = ema.average(x)
@mikowals is correct, tf.cond() seems to be executing both fn1 and fn2 regardless of the condition. I have figured out a work around without using tf.train.ExponentialMovingAverage
prev_ema = tf.Variable(np.array([0.0]))
ema = tf.train.ExponentialMovingAverage(.9)
return ema.average(inpt)
> /Users/clsung/git/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: fatal error:
>       'eigen-eigen-3d9f227afae2/unsupported/Eigen/CXX11/Tensor' file not found
This seems very non-standard to have such a small dev set that changes from checkpoint to checkpoint.
LGTM.
So I don't know what the appropriate solution here is, but the problem seems to be the result of a convergence of semi-hacky solutions, any one of which alone might not be an issue, but work together to cause problems:
- The symlink approach is kind of coarse and tricky to script around.
@rdadolf I'm also using a shared system. But for python, I installed an anaconda just for me. So there won't be any problem to influence others' environment.
The changes to protobuf have not solved these issues, they've just broken them in different ways.
name = "androidsdk",
api_level = 23,
path = "/Users/mchong5/Library/Android/sdk",
path="/Users/mchong5/android-ndk-r10e/",
api_level=21)
Well, it was that trailing slash!
cc: @ilblackdragon
name = "androidsdk",
api_level = 23,
api_level=21)
- updating bazel from 0.2.0 to master
- including `-fPIC` flag in `gpus/crosstools/CROSSTOOL`
- deleting `-fPIE` flag from the same file
the error is:
(cd /home/marcin/.cache/bazel/_bazel_marcin/259f71b95269c287fa42ef117628b45f/tensorflow && \
Mac os 10.10.5
$ sudo python cifar10_train.py
``` python
``` python
``` python
images = np.load("iamges_value.npy")
the code
Current Bazel version is {}, e...))
bogon:~ liu$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl
bogon:~ liu$
- Created a virtual env via pyvenv (not virtualenv, but I think this is irrelevant)
@danmane Aha! This is where the issue was:
envy@ub1404:~/os_pri/github/tensorflow$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
ERROR: /home/envy/os_pri/github/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:207:1: no such package '@grpc//': Tag mismatch! and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_server_lib'.
envy@ub1404:~/os_pri/github/tensorflow$
ops = ta_mem_leak()
@ebrevdo Sounds good, I submitted the PR #1691. Please review.
``` python
import os
For the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.
Ideas?
I would love to gather more debug information. Right now, all I can see is that a `std::system_error` is thrown somewhere, although it looks like it is thrown somewhere in the CUDA code, not in Tensorflow (I couldn't find any matching line in Tensorflow's codebase). Do you have any advice on how I could get a more meaningful stack trace?
``` python
self.b = K.zeros((self.nb_filter,))
Not sure it's related, but we had a similar difference in performance between standalone benchmark binary and test app on the Nexus 5X for our neural networks, and we tracked it down to the compiler inlining some functions in the binary and not in the .so library. We fixed it by adding linker-time optimization (LD_FLAGS += -flto -O3), that fixed the issue and also reduced the library size.
Yes,I see your logic now! But in this python code string the name of variable is `x_with_bias` which means "x with something else", or "not only x." And, according to that string, x does not contains a 1 entry, it's `x_with_bias`. It's all just a matter of taste of course and not that important :)
Suggest to rename because cell uses var which is not defined in it, but
code) because this code cell is supposed to be like previous but with
when I use rnn,it's ok!
aws
KeyboardInterrupt
^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C
this makes it appear weirdly inside tensorboard.
AK
imports...
os.makedirs(folder)
boringssl in grpc was previously held in googlesouce which block people under the wall,
upgrade to this commit version will change its source to https://github.com/google/boringssl.git
Excuse me if something is wrong, have little to no experience in python
and tensorflow
To throw in my two cents: I think that, while training materials shouldn't be overly tedious in terms of time-to-completion, in this case it's more important to demonstrate the usage of an important API component. Since dropout is such a common and important tool, showcasing how to use it properly with TensorFlow in the MNIST tutorial sets users up for more success (and less frustration) in the long term.
@samjabrahams
@fayeshine - If this was the "MNIST for Beginners" tutorial, I'd agree with you, but seeing as this is tutorial is "for experts", we should assume that users will have some experience building neural networks in other frameworks or languages, and will likely have used dropout before. Dropout is such common step to take in these sorts of models that taking it out of what is supposed to be a "moderately sophisticated" model would be conspicuous. This would lead to users to go looking for dropout in the API and figure out how to implement it anyway.
@samjabrahams
@fayeshine Absolutely- I think a separate tutorial with a targeted example showcasing when and how dropout is useful would be a great resource for those learning about neural network models. I think our disagreement about this particular section stems from our thoughts on what the primary purpose of this tutorial should be. Correct me if I'm wrong, but from what I gather your opinion is that the most important aspect of this tutorial is to teach users how to design and setup a model that A, works and B. is as efficient as possible. I completely respect this, and it is an important skill for anyone who's learning machine learning to get a grasp of. My opinion, in contrast, is that this tutorial's primary goal should be to teach the fundamental concepts and frequently-used components of the API that one would expect to see in a convolutional neural network. In this way, it can serve as an alternative way of learning the API- showing users how different components are used in practice instead of only having the plain documentation.
@cuiguoxin Thank you for your comments. I think that some of this confusion is due to this API being in an intermediate state (it currently just supports functions and not general graphs). As commented in the header file, this API is under construction, and will be changing frequently. Currently, the gradients passed in 'y_grad_node_outputs' are passed through to the input of the 'y_nodes', but these nodes are handled in a specialized way in the current implementation (i.e. function arg nodes). In a future version, the 'y_grad_node_outputs' will contain the gradient node outputs corresponding to the 'y_node_outputs', and will be propagated into the 'y_nodes' during backprop init.
@andydavis1 . Thanks. Does it mean that if I construct a graph and I want to auto differentiate the graph from the y to x, I still can't use the api in gradient.cc? Can you show me how to do it in c++ api?
@girving thanks for adding confusion matrix, however we may still need groupByKey or reduceByKey for data preprocessing
Signed the CLA.
Oh, thank you @ilblackdragon and @vrv .
Is there any operation which generates Confusion Matrix in Tensorflow?
import pandas as pd
for p in ans.T:
confusion[p[0],p[1]]+=1
print(pd.DataFrame(confusion))
@fayeshine thanks
He has graciously open-sourced his neural network library but I'd like to utilize these same ideas in my models, which are implemented in Tensorflow.
name = "androidsdk",
api_level = 23,
path = "/home/kuntal/Android/Sdk"
path = "/home/kuntal/knowledge/IDE/android/android-ndk-r10e/",
api_level = 21
*_ERROR: *_/home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 13 argument(s) skipped).
`ERROR: /Users/hassanabid/Documents/hassan/open_source/tensorflow/tensorflow_source/tensorflow/examples/android/BUILD:43:1: Linking of rule '//tensorflow/examples/android:libpthread.so' failed: arm-linux-androideabi-gcc failed: error executing command
(cd /private/var/tmp/_bazel_hassanabid/96ab15ac510a01a39eca351cb09446d8/tensorflow_source && \
(cd /private/var/tmp/_bazel_hassanabid/96ab15ac510a01a39eca351cb09446d8/tensorflow_source && \
2. `-rwxr-xr-x@ 1 hassanabid  staff  747340 Mar 15 13:18 toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-gcc`
/cc @mrastegari if interested
``` c++
typename MatMulTypes<T>::in_type array,
MaskMatrix &out)
typename MatMulTypes<T>::in_type array,
MaskMatrix &out)
typename MatMulTypes<T>::in_type a,
typename MatMulTypes<T>::in_type b,
typename MatMulTypes<T>::out_type out)
MaskMatrix a_;
MaskMatrix b_;
unsigned int value =popcnt(a_(ar, c) ^ b_(br,c));
I'm getting the exact same issue as @ricochet2200  with the shape of `Tensor u'pool_3/_reshape:0'` except I'm running on Ubuntu 14.04 and commit `46bfa0feef`
This is a stretch, but possibly related to #1277 where batch norm updates are being run but there is no batch norm in the graph.
``` python
if BREAK_ME:
File "/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py", line 332, in run
File "/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py", line 599, in _do_run
e.code)
File "/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/gen_data_flow_ops.py", line 359, in _queue_dequeue_many
File "/home/indico/Apps/tensorflow/_python_build/tensorflow/python/framework/ops.py", line 1129, in __init__
``` python
``` python
del sess
``` python
del sess
Note that this time I used a tensorflow compiled from source, since the 0.7.1 release does not have `allow_growth` option yet.
What about the first problem? Normally deleting an object in python does not guarantee releasing memories, also the case here relates to GPU. It is up to tensorflow to decide what to do. Is that right? Or am I missing something?
@vrv @ebrevdo
I spent quite a bit of time but I couldn't figure out why the complex64 atan function was failing. As you see from the output  seems that the problem is a sign in the second element.
To move this patch forward and prevent from falling behind master I removed the complex64 support from atan for now. What do you think ?
@vrv rebased
`undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev`
Anyways, I tested on a Ubuntu 14.04 machine with Python 2.7.6 and gcc 4.8.4, where the custom op indeed loads just fine. It could be an issue with newer Python/gcc?
g++-4.8 (GCC) 4.8.5
e.code)
File "/home/yy/virtualenv/env1/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py", line 951, in matmul
...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/attention_decoder/MultiRNNCell_3/Cell0/BasicLSTMCell/Linear/MatMul', defined at:
but it have two experiment result.
@sherrym  the implementation of my custom op is exactly same as the matmul op, however the performance is different.
bazel version
2.  tried HEAD on protobuf
- //google/protobuf:protoc
//google/protobuf:protobuf
- //google/protobuf:protoc.
git submodule foreach git clean -dfx
I install the SSL certification but still got this error
**Exception:**
sudo pip install --upgrade $TF_BINARY_URL
the ATLAS environment variable.
the BLAS environment variable.
Blas (http://www.netlib.org/blas/) sources not found.
the BLAS_SRC environment variable.
the ATLAS environment variable.
the LAPACK environment variable.
the LAPACK_SRC environment variable.
Exception:
Gave it a try, however, computation of the theoretical jacobian seems to fail in the test.
Merge: b9e9c9a bc8d5fe
ERROR: /home/saurabh/Downloads/tensorflow/tensorflow/examples/android/BUILD:43:1: Linking of rule '//tensorflow/examples/android:libpthread.so' failed: arm-linux-androideabi-gcc failed: error executing command
(cd /home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/tensorflow && \
(cd /home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/tensorflow && \
bazel version
/home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/external/re2/BUILD:9:1:
/home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/tensorflow
PATH=/home/saurabh/anaconda2/bin:/home/saurabh/bin:/usr/lib/jvm/java-1.7.0-openjdk-amd64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
-no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a'
-MD -MF
arm-linux-androideabi-gcc failed: error executing command
/home/saurabh/.cache/bazel/_bazel_saurabh/b5e3c50abcfa34dcecd4eb2855a378fe/tensorflow
PATH=/home/saurabh/anaconda2/bin:/home/saurabh/bin:/usr/lib/jvm/java-1.7.0-openjdk-amd64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
-no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a'
-MD -MF
Saurabh Gupta
'androidndk' is 'r9b'.
'androidndk' is 'r9b'.
'androidndk' is 'r9b'.
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:616:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
'//tensorflow/core/kernels:cwise_op_neg.cc' directly. You should either
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
'//tensorflow/core/kernels:cwise_op_tanh.cc' directly. You should either
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
'//tensorflow/core/kernels:xent_op.h' directly. You should either move the
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
'//tensorflow/core/kernels:resize_nearest_neighbor_op.cc' directly. You
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
WARNING: /home/saurabh/Downloads/tensorflow/tensorflow/core/BUILD:649:12:
> with Bazel 0.2.0
Saurabh Gupta
import scipy.misc
import scipy.misc
This is also an issue with opencv.
[[Node: n4 = ZerosLike[T=DT_BOOL](n1)]]
[[Node: n4 = ZerosLike[T=DT_BOOL](n1)]]
[[Node: dx = SymbolicGradient[Tin=[DT_FLOAT, DT_BOOL, DT_FLOAT], Tout=[DT_FLOAT, DT_BOOL], f=Reverse[T=DT_FLOAT], _device="/job:localhost/replica:0/task:0/gpu:0"](_recv_x_0/_2, _recv_dims_0/_4, _recv_dy_0/_6)]]
Looking through the code, it seems that there's no zeroing kernel for bools, so I'm not sure how this is supposed to work?  The obvious change to add a kernel, jlebar/tensorflow@286c1647cca9ebcdbce4497995c794d4b0c55633, doesn't work -- we seem to invoke the new kernel, but the whole program just silently dies.
I'm pretty confused by what's going on here, what with the To32Bit functor being applied to an array of doubles (?) and so on.  Any pointers would be very much appreciated.
A hackish workaround is too edit tools/test/test-setup.sh to set LD_LIBRARY_PATH correctly in the Bazel source.
Sorry, that may have been user error.
``` python
``` python
Operating System: Current Goobuntu
google/protobuf@fb714b3606bd663b823f6960a73d052f97283b74
\+ @tra
Aborted (core dumped)
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
I removed the mnist data files present in the data folder and then again executed the python command  : python convolution.py , Now surprisingly it is giving a bit different error as :
``` python
Out of memory: Kill process 26313 (cc1plus) score 213 or sacrifice child
Vijay, **-j 1** definitely slowed it down but it's still crunching through the compile.  I'll let it run overnight and check up on it in the morning.  Thanks for the tips and I'll keep you posted.
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
bazel-out/local_linux-opt/bin/tensorflow/python/pywrap_tensorflow.cc:3828:111: warning: 'arg2' may be used uninitialized in this function [-Wmaybe-uninitialized]
Yin
cc: @ilblackdragon When will these tests be running? Looks like it's stuck last night.
``` python
``` python
``` python
``` python
File "/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py", line 332, in run
File "/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/client/session.py", line 536, in _run
e.code)
[[Node: add = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](Variable/read, Variable_1/read)]]
File "/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/variables.py", line 544, in <lambda>
File "/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/variables.py", line 559, in _RunOp
File "/home/sperkins/work/ska/code/tensorflow/_python_build/tensorflow/python/ops/gen_math_ops.py", line 44, in add
Now that `complex128` is supported in tensorflow, we should extend existing ops like `complex`, `real`, `imag`, … to support `complex128` as well.
``` c++
.Input("in: complex64")
``` c++
.Output("out: Tout")
(It also turns `Tout` into an optional argument).
I'd like to express "If `T==complex128`, then `Tout==float64`" somehow.
``` python
if in.dtype == tf.complex64:
Tout = tf.float32
Tout = tf.float64
return generated_real(in, Tout=Tout)
@ibab @girving I believe [numexpr](https://github.com/pydata/numexpr) has to consider similar issues when dealing with conversion between between float32, float64, complex64 and complex128. A contributor suggested the use of a casting table, https://github.com/pydata/numexpr/pull/181 may provide more insight.
code:
``` python
bi = rnn.bidirectional_rnn(cell_fw,cell_bw,inputs,cell_fw.zero_state(3, tf.float32),cell_bw.zero_state(3, tf.float32),sequence_length=5)
error:
@ebrevdo, You are right. Problem has been solved。
is this wrong ?
@mrry, I find this because I am running an AlexNet, and after a few iterations, i get Floating Point Exception, I guess this maybe the number is too large, can you give some advice to fix this?
``` python
``` python
@mrry
ERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:517:1: output 'tensorflow/core/kernels/_objs/adjust_contrast_op_gpu/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.o' was not created.
I looked over the tests, it is a bit strange that the CPU tests pass, but not the GPU tests.  Perhaps you might know the reason for that and/or can help me fix it and test it quickly?  I feel bad holding you guys up on this too.
It seems the Apache server at http://yaroslavvb.com is configured to
What are you trying to achieve?
Sorry in advance for being a bit pedantic... I bring this up because of lack of clarity when it comes to `gather`.
- `to_float` and others use `x` instead of `tensor`
- `shape`, `size`, `rank`, and others use `input` instead of `tensor`
- `reshape` and others use `tensor`
- `gather` uses `params` instead of `tensor`
`params` suggests that `gather` is special purpose, perhaps for efficiency or some other reason. Is it? If so, should we update the documentation? If not, can we change the name of `params`?
In addition, I think `value`(s) and `input` should be changed: a `Tensor` isn't a value and `input` is a Python built-in.
Hi @jendap
1. run python  cifar10_train.py
Aborted (core dumped)
|    0     27004    C   python                                        1670MiB |
<h1>Forbidden</h1>
`url = 'http://commondatastorage.googleapis.com/books1000/'
raise Exception(
Exception                                 Traceback (most recent call last)
11     raise Exception(
`docker run --net host -p 8888:8888 -it --rm b.gcr.io/tensorflow-udacity/assignments:0.5.0`
envy@ub1404:~/os_pri/github/tensorflow$ python3
envy@ub1404:~/os_pri/github/tensorflow$ git log
Merge: aea1bf5 993c77d
envy@ub1404:~/os_pri/github/tensorflow-deepq$ PYTHONPATH=~/os_pri/github/tensorflow-deepq:$PYTHONPATH python3
> > >   File "/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
SEVERE: Error during merging resources
Error Code:
Error Code:
I think it is dependencies problem
SEVERE: Error during merging resources
Error Code:
Error Code:
ERROR: /home/islamoc/tensorflow/tensorflow/core/BUILD:647:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib' failed: arm-linux-androideabi-gcc failed: error executing command
So I made a code like
This will change a lot of functionality.
The implementation is a blocked cholesky backpropagation as advocated by Dr. Iain Murray in this recent paper http://arxiv.org/abs/1602.07527
Alex
Alex
It is a public holiday here in the UK tomorrow (Friday) and Monday but I will be back at my desk on Tuesday.
Alex
Alex
Alex
import os
`gpu` is a string. '0' for GPU 0, '0,2' for GPUs 0 and 2, etc.
**len** seems to cause problems though, will look into it.
Is the `template flat` a typo, or some feature of C++ that I'm not familiar with?
(ran into bazel issues)
In the following piece of code:
>        [False, False,  True],
>        [False, False,  True],
ERROR: /home/igorkorsunov/tensorflow/tensorflow/core/BUILD:98:1: //tensorflow/core:worker_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.
ERROR: /home/igorkorsunov/tensorflow/tensorflow/core/BUILD:121:1: //tensorflow/core:master_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.
ERROR: /home/igorkorsunov/tensorflow/tensorflow/python/BUILD:154:1: Target '//tensorflow/core:protos_all_py' contains an error and its package is in error and referenced by '//tensorflow/python:framework_for_generated_wrappers'.
Bazel version is 0.2.0
`ERROR: /home/hieunguyen/tensorflow/tensorflow/core/BUILD:98:1: //tensorflow/core:worker_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.
ERROR: /home/hieunguyen/tensorflow/tensorflow/core/BUILD:121:1: //tensorflow/core:master_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.
ERROR: /home/hieunguyen/tensorflow/tensorflow/cc/BUILD:61:1: Target '//tensorflow/core:tensorflow' contains an error and its package is in error and referenced by '//tensorflow/cc:tutorials_example_trainer'.
The specific error was:
We have the same error like @hrajanie and @corcra in AWS EC2.
``` python
hb = tf.Variable(tf.zeros([100]), name="hbias")
Since there's no way to cache the intermediate result, you can only get one `Tensor` result for one `computation`. There's no way to avoid the duplication computation and `X` memory migration cost.
``` python
@girving I have no idea the C++ code implementation for `paritial_run`, but there seems kind of memory mallloc and release problem.
``` python
it' make no sense.....
sudo cp include/cudnn.h /usr/local/cuda/include/
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 7.0
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
name = "grpc",
path = google/protobuf
[submodule "third_party/boringssl"]
path = third_party/boringssl
url = https://github.com/doubler/boringssl.git
pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-linux_x86_64.whl
ok, can you define weird?
Thanks for pointing out girving.
File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
Am I doing something wrong? Can I use cuda 6.5 or 7.0?
Why cuda version is hard-coded?
On Wed, Mar 9, 2016 at 4:43 PM, elqursh notifications@github.com wrote:
> > APIs
@ilblackdragon How about the `"cla: no"` thing? Can we still merge it which actually makes sense since they signed `cla` from skflow already?
import os
logdir = "/tmp/tflogs"
os.makedirs(logdir)
@vgatto  I'd also like to point out that this makes it impossible to visualize the graph in tensorboard, since it just hangs parsing the constants in the graph def. Can you explain why and how can I display the graph?
//third_party/gpus/crosstool/crosstool_wrapper_is_not_gcc on line 47 to 50:
Using the recent TensorFlow pip install 0.7.0 I have encountered some frustrating behavior. It might be something I'm doing wrong on my end, but I've ruled out everything I can.
http://lifescience.opensource.epam.com/indigo/
Unfortunately on both machines Indigo package not installed... and model loads on both host without errors.
@girving: some minutes ago I achieved the same result as in GPU on CPU host - no difference. But now I retraining my model on GPU host with last code modifications. I think this problem may be caused by pyc (python file) files...
[sudo] password for martin:
Exception:
@alex-pardo did you `./configure` with `TF_UNOFFICIAL_SETTINGS=1` and specified `3.0` at the cuda compute capability step ?
Sorry for my bad english :D
GCC 4.9.3
Building with Cuda and cudnn
--- a/third_party/gpus/crosstool/CROSSTOOL
+++ b/third_party/gpus/crosstool/CROSSTOOL
Modified https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_gfile.py#L63 to `self._fp = open(name, mode, encoding="latin-1")`
Extracting tar file ./training-giga-fren.tar
@amirkhango have you solved this problem?
@amirkhango
I'm running through the [Mandlebrot Set tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mandelbrot/index.html) and hit a snag.
Caused by op u'zeros_like/Cast', defined at:
However, the error below is still exists.
@thinxer Thanks for you reply.
I also try to build from source, but it failed again, it seems that it could be related to brazel,
could you please elaborate the steps to build tensor flow including the version of brazel.
@girving Below is the complete information
I've started implementing support for `complex128` in order to fix #1420.
Okay, I've added the style changes.
I've fixed the problem with `scomplex` and `dcomplex` earlier today in https://github.com/tensorflow/tensorflow/commit/03422935b98145f5b1060198e1f7274b665ddd87
It seems that @yaroslavvb is not a `tensorflow member` and Jenkins won't listen to him :disappointed:
@girving how do you run tests when you develop locally? Do you simply do:
@girving squashed
@girving no worries! squashed
from there
Starting TensorBoard  on port 6006
127.0.0.1 - - [07/Mar/2016 11:45:07] "GET /external/dagre/dist/dagre.core.min.js HTTP/1.1
Starting TensorBoard  on port 6006
Starting TensorBoard  on port 6006
@danmane
DEBUG:tensorflow:No more events in /tmp/mnist_logs/events.out.tfevents.1460670145.Moko
INFO:tensorflow:No path found after /tmp/mnist_logs/events.out.tfevents.1460670145.Moko
DEBUG:tensorflow:No more events in /tmp/mnist_logs/events.out.tfevents.1460670145.Moko
INFO:tensorflow:No path found after /tmp/mnist_logs/events.out.tfevents.1460670145.Moko
siakhnin@mac-korbel21:~/tools/tensorflow$ ls -lah /tmp/mnist_logs/train/
drwxr-xr-x  3 siakhnin  wheel   102B 15 Apr 11:59 ./
drwxr-xr-x  4 siakhnin  wheel   136B 15 Apr 11:59 ../
-rw-r--r--  1 siakhnin  wheel    19M 15 Apr 12:00 events.out.tfevents.1460714373.mac-korbel21.wlan.embl.de
siakhnin@mac-korbel21:~/tools/tensorflow$ ls -lah /tmp/mnist_logs/test
drwxr-xr-x  3 siakhnin  wheel   102B 15 Apr 11:59 ./
drwxr-xr-x  4 siakhnin  wheel   136B 15 Apr 11:59 ../
-rw-r--r--  1 siakhnin  wheel   2.6M 15 Apr 12:00 events.out.tfevents.1460714374.mac-korbel21.wlan.embl.de
@danmane - All the tabs are empty. Looking at the page with Dev Tools in Chrome it looks like the CSS stylesheet is missing from the deployment directory for some reason. And indeed the GET call for it results in a 404
Any plans for a fix? I'd love to be able to use tensorboard.
edit: Omg getting teary eyed.. Tensorboard is awesome
But TensorFlow currently only supports `complex64`.
Are there any plans for supporting `complex128` as well in the future?
and the other
@sbyma thanks for the help. Yes, it fix my problem. Just wondering two things:
@girving Wait, isn't the c++ code necessary?
@girving removed.
Bazel: 0.2.0
Cuda: 7.5
CudNN: 4
pip uninstall protobuf
Now about @RasmooL's answer, again, still the same error:
And in my PYTHONPATH I have:
pip uninstall protobuf
from there
Mani
Mani
|-- protobuf
name='protobuf',
Cloning into 'grpc'...
Cloning into 'third_party/boringssl'...
Submodule path 'third_party/boringssl': checked out '9f897b25800d2f54f5c442ef01a60721aeca6d87'
Cloning 'boringssl'...
* Establish HTTP proxy tunnel to boringssl.googlesource.com:443
> CONNECT boringssl.googlesource.com:443 HTTP/1.1
Host: boringssl.googlesource.com:443
Proxy-Connection: Keep-Alive
Pragma: no-cache
< Pragma: no-cache
ERROR: /home/wenjian/.cache/bazel/_bazel_wenjian/d25f05b3e081482e9605a49c384a6c89/external/grpc/BUILD:852:1: Label '@grpc//:src/cpp/common/core_codegen.h' is duplicated in the 'srcs' attribute of rule 'grpc++'.
@fayeshine have you successfully completed the build for distributed tensorflow? I hit different compile errors for each build after fix the boringssl issue.
@zszhong have you passed the rest build process?
@zszhong it seems we build different packages, I am trying to run: bazel build -c opt --verbose_failures //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server, the distributed tensorflow, but failed.
Use bazel compiled from HEAD!
@fayeshine Yes, you are right, just tell somebody who has proxy but failed to compile tensorflow.
``` python
print("Train", aa)
print("Test", aa)
print("Train", aa)
print("Test", aa)
Using a different semiring practically means redefining the `plus` and `times` operations.
The [max-plus semiring](https://en.wikipedia.org/wiki/Max-plus_algebra) (aka Viterbi or tropical semiring) redefines `plus` as `max` and `times` as `plus`. This is useful for finding the best path (assuming all matrix entries are log numbers).
cuda too.
@panmari, I don't think so, but I noticed the mac test failing.
**Questions**
https://www.udacity.com/course/viewer#!/c-ud730 is a really cool course in the context of Machine Learning and TensorFlow. After completing Coursera Andrew Ng course, they give valuable reasoning why and how something should be done in TensorFlow.
I am encountering a strange Multi-GPU performance issue. To validate I've run the following script [TensorFlow Examples: Multi-GPU basics](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py).
`uname -or`:
> backports-abc==0.4
> Mako==1.0.1
> mercurial==3.6.2
> protobuf==3.0.0b2
> pyparsing==2.1.0
> pytz==2015.7
1. different scripts and setups
I was not trying the CIFAR10 example, but could do this later. The author of this very simple example: [TensorFlow Examples: Multi-GPU basics](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py) claims a speedup with Multi-GPU setup while on my system it actually took longer compared with single GPU calculation.
Your results are strange. I cannot explain it.
I have enjoyed using tensorflow much. Thanks for the commitment to make this happen.
After writing many code based on it, I found it is very inconvenient that when the C++ code meets an error, it just breaks instead of passing the error to the python front end, so it is impossible to deal with the error gracefully.
First I have to confess I did not notice the OpError class before. Now I could
The error with more context is:
but it would much more convenient when a NaN number is taken as an exception,
fix #1392 tutorial typo
Exception:
File "/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py", line 315, in run
File "/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py", line 564, in _do_run
File "/home/shiyemin/code/tensorflow/_python_build/tensorflow/python/client/session.py", line 586, in _do_call
e.code)
The problematic portion of the code is:
name = "grpc",
[submodule "third_party/boringssl"]
path = third_party/boringssl
url = https://github.com/doubler/boringssl.git
@yangyanli  not easy to be a programmer...
@   melody-rain I also meet the problem. I don't see git_repository name = "grpc" in my workspace. What I see is new_git_repository.
I use https://github.com/ethereon/caffe-tensorflow to convert caffe model to tensorflow and the GoogLeNet is selected to construct our network.
elif lstm_type == 'GRU':
When i add this LSTM layer to GoogLeNet, "Failed precondition: Attempting to use uninitialized value  RNN/GRUCell/Gates/Linear/Bias" occurs. But when i using the code from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py, everything works well.
Here's the exception stack:
batch_size=FLAGS.batch_size)
File "/home/sarah/Documents/SVHN/cifar10_input.py", line 161, in distorted_inputs
- This is necessary; otherwise `gcc` can't find `as`.
third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-017cff30cf74 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-017cff30cf74 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__="redacted"' '-D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.d -fPIC -c tensorflow/core/kernels/matrix_solve_ls_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/matrix_solve_ls_op/tensorflow/core/kernels/matrix_solve_ls_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl
1. import gi
2. import gi
CUDA complier version:
``` python
``` python
@girving
(gdb)
(gdb) bt
nore_unsupported_sandboxing.
OSx
2.ipython nootebook
File "/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py", line 74, in catch_config_error
File "/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py", line 74, in catch_config_error
File "/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py", line 74, in catch_config_error
File "/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py", line 74, in catch_config_error
File "/Users/maheshwarligade/tensorflow/lib/python3.5/site-packages/traitlets-4.1.0-py3.5.egg/traitlets/config/application.py", line 416, in initialize_subcommand
uname -or
It seems "__shfl_down" was added in Kepler (compute capability == 3.x )
and as my cards are pre-Kepler, this seems to be the culprit. Can anybody confirm this ? or might there be a solution ?
Have you found the solution to undefined __shfl_down?
I am trying to install cunn for lua and I have the same issue:
/tmp/luarocks_cunn-scm-1-7288/cunn/lib/THCUNN/SpatialClassNLLCriterion.cu(16): error: identifier "__shfl_down" is undefined
`export LD_LIBRARY_PATH="/path_to_your_cudnn/cuda/lib64":$LD_LIBRARY_PATH`
CLA has been signed for kmader
e.code)
[[Node: mul_4305 = Mul[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](Variable_1954/read, Variable_1956/read)]]
Caused by op u'mul_4305', defined at:
e.code)
[[Node: mul_4305 = Mul[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](Variable_1954/read, Variable_1956/read)]]
Caused by op u'mul_4305', defined at:
Not sure what is going on.
@vrv and @mrry
linkopts = [
"-Wl,-Bsymbolic",
"-Wl,-Bsymbolic",
I read [here](http://stackoverflow.com/questions/34514324/error-using-tensorflow-with-gpu) that Tensorflow always reserves all but 200MB of the GPU's memory, so the memory usage is always going to be very high. I've drastically reduced my model down in size and am still getting the nan outputs (as well as other nonsensical values such as negative norms), so I'm not sure if memory use is the culprit.
``` python
I'm getting the same problem with the newest Tensorflow (0.8.0). For a bigger batch size I get NaN:s even when limiting gradients (could not reproduce with batch size of 1). Following the min and max of all learned variables does not show any explosions. Running the same program on CPU (CUDA_VISIBLE_DEVICES="") does not give the NaNs.
@girving just trying to get it in front of the only people who will have the answer.
``` python
File "/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 638, in _AggregatedGrads
File "/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 94, in assert_is_compatible_with
``` python
## Specific information:
- CuDNN 4, CUDA 7.5
- Bazel 0.2.0
This results in the following error:
Specifically, I've inserted `cxx_flag: "-D_FORCE_INLINES"` into `tensorflow/third_party/gpus/crosstool/CROSSTOOL`:48.
if complicated_logic_true:
2. python cifar10_eval.py
cifar10_eval.py
total_sample_count = num_iter * FLAGS.batch_size
Error i am getting :
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
Storing debug log for failure in /home/hunter/.pip/pip.log
SNIMissingWarning
InsecurePlatformWarning
ld: unknown option: -Bsymbolic
I have the same issue as michaelnhw has.
I have not interrupted bazel.
-CUDA_VERSION = ""
+CUDA_VERSION = ".7.5"
then bazel clean --expunge
checking for mawk... mawk
checking for ar... ar
checking for mt... mt
checking for dlfcn.h... yes
checking malloc.h presence... yes
checking for malloc.h... yes
checking for pow... no
checking for pow in -lm... yes
Loaded plugins: fastestmirror, langpacks
- epel: mirrors.neusoft.edu.cn
Loaded plugins: fastestmirror, langpacks
- epel: mirrors.neusoft.edu.cn
In https://www.tensorflow.org/versions/r0.7/how_tos/tool_developers/index.html, some of the links that referred to this repository contain "%0A" in random places which make them invalid.
``` python
Tensorflow version: 0.7.1, installed with pip. Operating system: Ubuntu 14.04
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
``` python
Operating System: **Debian**
Attempting with: GRUCell
``` python
``` python
`>> python tensorflow/tensorflow/tensorboard/tensorboard.py --logdir=bla_logs`
Redhat 6.7
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
File "/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/layers/python/framework/tensor_util.py", line 21, in <module>
File "/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 25, in <module>
File "/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 21, in swig_import_helper
I can confirm that this works with the GPU version. Thanks very much. Including `EXTRA_BAZEL_ARGS` and the build command below for completeness. Some flags were included simply because they were suggested in other threads to circumvent other issues, but notice `--conlyopt="-std=c99"`. It wouldn't compile for me without this.
export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 4'
pciBusID 0000:08:00.0
cuda 7.0
cudnn 6.5
@rdipietro Sorry. By "can't run" I mean I saw out of memory errors from tensorflow.
On Mar 29, 2016 7:39 PM, "digitalsword" notifications@github.com wrote:
> @rdipietro https://github.com/rdipietro Sorry. By "can't run" I mean I
A slightly fancier version of the above that encloses it within scope `'my_scope'` using `tf.variable_scope` gives the following error:
@alquraishi I'd like to ask a question if I may. In your example, the **batch size** is 4(static), and **num_stepss** also is a static vector with length 4. But if I have another batch with different **num_stepss** [14, 30, 24, 17] (Which I mean every batch has different num_stepss...), how could I use the dynamic_rnn model?
@ebrevdo sorry I didn't see that this was still open. I'm using dynamic_rnn with no problems. I will close.
The problem seems to be that the first letter of each file name is in lowercase, so the links look like this:
This is a tracking bug for adding support for the half type (aka float16, or fp16) in TensorFlow. Half computation is supported by GPUs only, although newer Intel CPUs (Haswell and newer) have support for converting back and forth between fp16 and fp32 in hardware (F16C). CUDA has some support for half since 7.5, although it's a bit cumbersome (it's not a first-class type, but relies on macros containing asm statements; effectively intrinsics).
I'm currently working on this. There's a fair amount of work to do—the CUDA “half” type is just a struct of uint16_t, which doesn't really match what Eigen expects. In particular, Eigen expects all such arithmetic types to have operator overloads, converts to and from other arithmetic types (including even bool), etc. The actual arithmetic intrinsics require CUDA compute 5.3 (which right now means essentially Tegra X1 only), so mostly, we'll have to rely on casting on both sides and then doing the actual calculations in float.
@sesse , that's fast. Great work!
Also some interesting "VBR" approach is emerging http://arxiv.org/abs/1511.06393
@sesse great job with adding all these operations! Any estimate on how long it might take to support convolutions?
Also looking forward to seeing elu added.
cuda version: 7.0
cudnn version: 6.5
1. Download the following numpy array in .npy format: https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0
``` python
``` python
One more thing, I also tested in b2bac69ed2 that is from 11 days ago and the same problem happened there too.
@cesarsalgado did you resolve your issue with cudnn r4 ?
1. I've been working through the Udacity DeepLearning course. After encountering this error, I killed the jupyter kernel for 5_word2vec, and re-run my previously-saved notebook for the fourth exercise, 4_convolutions.ipynb, and it executes without error on the gpu. Which I think must confirm that TensorFlow is installed OK.
``` python
; NodeDef: Variable/Adagrad/_84 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_550_Variable/Adagrad", tensor_type=DT_FLOAT_REF, _device="/job:localhost/replica:0/task:0/cpu:0"](^gradients/concat_6/_86, ^gradients/concat_7/_88); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
Is it difficult to add support for that?
### Logs
repository for including protobuf.
Opened google/protobuf#1402.
❯❯❯ bazel test //...
Odd. Looking
``` python
import os
os.makedirs(INCEPTION_LOG_DIR)
Maybe related to #716
@dgolden1 and @ffmpbgrnn, did you rebuild the Tensorboard frontend and backend explicitly? Perhaps rebuilding from the TF root doesn't rebuild the Tensorboard components?
bazel clean
./bazel-bin/tensorflow/tensorboard/tensorboard --logdir /tmp/inception_v3_log/
@danmane, can you confirm if there are additional steps beyond gulp vulcanize and bazel build? Thanks.
Could this be a Python 2 vs. 3 issue?
That is a separate python 3 tensorflow (but not tensorboard) issue that I believe is a known issue
File "/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py", line 329
envy@ub1404:~/os_pri/github/tensorflow$ git status
envy@ub1404:~/os_pri/github$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
## envy@ub1404:~/os_pri/github$ pip show protobuf
Name: protobuf
## envy@ub1404:~/os_pri/github$ pip show tensorflow
envy@ub1404:~/os_pri/github$ python tensorflow/tensorflow/models/image/mnist/convolutional.py
File "/home/envy/os_pri/github/tensorflow/tensorflow/**init**.py", line 23, in <module>
File "/home/envy/os_pri/github/tensorflow/tensorflow/python/**init**.py", line 41, in <module>
File "/home/envy/os_pri/github/tensorflow/tensorflow/python/**init**.py", line 35, in <module>
envy@ub1404:~/os_pri/github$
/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:22:35: fatal error: google/protobuf/arena.h: No such file or directory
In file included from /home/jpaparia/anaconda2/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:22:0,
/home/jpaparia/anaconda2/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/
Any idea what might cause this error? Thanks!
@keveman, thanks for your response. I installed TensorFlow from the pip package you pointed me to. I'm now getting a new error when I compile `ZeroOut`:
/home/jpaparia/anaconda2/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:26:62: fatal error: tensorflow/core/framework/selective_registration.h:
Ahmeds-MacBook-Pro:tensorflow ahmedabobakr$ ./bazel-bin/tensorflow/tensorboard/tensorboard --help
File "/Users/ahmedabobakr/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/tensorflow/tensorboard/backend/tensorboard.py", line 36, in <module>
@aidangomez I would like to help you out! :)
For Python3, there is only a pre-built for Python3.4, while arch is with Python3.5. You can either compile tensorflow yourself, or download the binary for 3.4 and rename "cp34" to "cp35" to make it work.
There is no binary for 3.5, so this is just a hack that may not work.
Alternatively, install Python3.4 (e.g. from https://aur4.archlinux.org/packages/python34) then create the virtualenv for that version:
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py", line 30, in run
File "lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py", line 150, in train
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10.py", line 119, in distorted_inputs
batch_size=FLAGS.batch_size)
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py", line 153, in distorted_inputs
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py", line 104, in _generate_image_and_label_batch
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 496, in shuffle_batch
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 287, in dequeue_many
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 319, in _queue_dequeue_many
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 664, in apply_op
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1834, in create_op
File "/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1043, in __init__
The only difference of my implementation according to me (I would like to be mistaken) with the paper is the use of `tf.reduce_mean` instead of `tf.reduce_sum` (which led me to exploding ReLU grad).
trying the zero_out op, there are a couple of places where the doc is not clear and also after hacking my way around it the op failed to work.
1- Where should the bazel build rule be placed?
#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
@islamoc Hi, I did the following (you'll find some steps you already did, so ignore them :) )
{{"Mul", input_tensor}});
affn1 = _affine(resh1, 896, 128)
06-06 12:41:59.923 24534 24552 E native  :   [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]
- i changed the things "@syed-ahmed commented on 21 Mar" mentioned,
I have the same problem as @Shaikjalal
``` python
``` python
if n.type == "MatMul":
<some code>
Hamid
@zer0n
@xiaodonghe
Caused by op u'model/embedding_lookup'
Hamid
Thank you girving and zheng-xq for explanations.
> diagonal part of A"). It's unfortunately that numpy is different, but
> get_diag is just as bad there.
Ubuntu 14
dima@dima-TP-T410s:~/data/repos$ python3 -c "import tensorflow; print(tensorflow.__version__)"
envy@ub1404:~/os_pri/github$
## envy@ub1404:~/os_pri/github$ pip show protobuf
Name: protobuf
Location: /home/envy/.local/lib/python2.7/site-packages
## envy@ub1404:~/os_pri/github$ pip show tensorflow
Location: /home/envy/.local/lib/python2.7/site-packages
envy@ub1404:~/os_pri/github$
1. I tried using the command      **tensorboard --logdir=/home/tattoo/Tabor_Stuff**
I'm new to ubuntu.
Weird, I tried it again and I have it launch now. Thank you.
pip uninstall protobuf
pip uninstall protobuf
yosemite os x
Issue is not closed I am getting below exception when I use cp35.
**Exception**:
shutil.move(old, new)
os.unlink(src)
When attempting to install in a conda env. The python 2 version works, but not python 3. It seems to want to install globally instead of in the env.
envy@ub1404:~/os_pri/github/tensorflow$ sudo rm -rf ~/.cache/bazel   # just try at second time
envy@ub1404:~/os_pri/github/tensorflow$ sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow/cc:tutorials_example_trainer
(cd /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/tensorflow && \
(cd /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/tensorflow && \
envy@ub1404:~/os_pri/github/tensorflow$
----> 1 tf.load_op_library('./auc.so')
``` python
I would like to take this up @cesarsalgado
@vrv I tried doing this, I feel that I'm still not ready for writing an Op (I'm not familiar with Eigen functionalities, and don't understand what is efficient v/s what is not). I will look at some C++ issues before coming back to this. Are there some issues that will help me with this, or do you recommend another way?
Caused by op u'geomnet/save/Assign_6', defined at:
Is this expected behavior?
**gcc**
and **bazel**
$ bazel version
I just cloned the TensorFlow repository (commit: 225de5e336d4d241ce02652eb961cdb6e0eec847) and I run most of the TensorFlow compilation. But at a certain point I get this error:
root@soc:~/tensorflow$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package
./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:67: fatal error: eigen-eigen-c5e90d9e764e/unsupported/Eigen/CXX11/Tensor: No such file or directory
It includes most of the modifications commented here:
- Modified the "Image Recognition" tutorial to include a section similar to the C++ API section but using the Go API, also the code was ported to Go
while running fully_connected_feed.py i had this error. I was trying the TensorFlow Mechanics 101 Tutorial
I attach the whole execution of the py file.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
os.makedirs(path, mode)
Silvio
python cifar10_train.py
@ebrevdo No, I installed TensorFlow via the following command
`softmax_cross_entropy_with_logits` and `sparse_softmax_cross_entropy_with_logits` both have useful  behavior that conflicts with current documentation.
In the seq2seq models, paddings are applied to make all sequences in a bucket have the same lengths. And apart from this, it looks like no special handling is applied to the paddings:
I realize `dynamic_rnn` isn't official yet, but I'm hoping to see whether I'm using it correctly / whether there's a bug.
Whereas in the `rnn.dynamic_rnn` case, we have
Do I have to manipulate these in some way before applying them?
num_steps is a value (an integer), not a list, so it was a typo to take `len(num_steps)`.
@martinwicke . I changed that, too. :)
I agree with @keveman, it feels weird to inject the error message and the limit. I would add it as a special check in a try catch on the same operation ...
HI, @Mistobaan and @keveman .
Thank you, @vrv , @keveman , @josh11b , and @Mistobaan .
Is this a known issue? Please help!
cuda : 7.5.18
#install cuda
tar -xf cudnn-7.5-linux-x64-v5.0-rc.tar
fortran \
byobu \
bumpy \
scipy \
gensim \
sacred \
pymongo \
pandas \
bs4
cd bazel
@Dringite I didn't do data augmentation, and the data preparing time is not counted in the total training time
1. The functions of interest contain many dynamic_partitions
2) Activate venv
---> 58     magic = _read32(bytestream)
59     if magic != 2051:
49 def _read32(bytestream):
364             while size > self.extrasize:
366                     if size > self.extrasize:
367                         size = self.extrasize
--> 292         magic = self.fileobj.read(2)
293         if magic == b'':
See http://izeye.blogspot.com/2016/03/error-expected-body-of-lambda-expression.html
ERROR: /home/panmari/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow/tensorboard:tensorboard' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.
### Logs
`Extracting MNIST_data/train-labels-idx1-ubyte.gz`
`Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.`
`Extracting MNIST_data/train-images-idx3-ubyte.gz`
`magic = _read32(bytestream)`
Name: pip
File "/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py", line 23, in <module>
File "/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 41, in <module>
File "/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 35, in <module>
File "/home/noname/tensorflow/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py", line 6, in <module>
File "/home/noname/tensorflow/lib/python2.7/site-packages/google/protobuf/descriptor.py", line 46, in <module>
9. Finally install tensorflow with GPU support enabled: `pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.0-py2-none-linux_x86_64.whl`
(tensorflow) noname@noname-desktop:~/Downloads/software$ which python
/home/noname/tensorflow/bin/python
(tensorflow) noname@noname-desktop:~/Downloads/software$ which python-config
/home/noname/tensorflow/bin/python-config
(tensorflow) noname@noname-desktop:~/Downloads/software$ python --version
fix issue #1169
this one is for r0.7 branch.
fix issue #1169
I'm working on compiling Tensorflow from source, using non-standard GCC/etc. installations.  Environment info: RHEL 6.7, GCC 5.2.1, Bazel 0.1.5.  I'm installing Tensorflow from HEAD (commit f82ad36).  I'm using a non-CUDA configuration.  I've followed the steps @sethbruder suggests in his comment on [bazel#649](https://github.com/bazelbuild/bazel/issues/649), including copying the contents of `tools` from bazel into `tensorflow/tools/` and into `tensorflow/google/protobuf/tools/`.  This is possibly related to #332, as I'm getting the same error, but at build time as opposed to API usage.
I've tried to set up the relevant paths for my non-standard system resource install using the following settings before invoking bazel:
export BAZEL_ARGS="--verbose_failures"
export EXTRA_BAZEL_ARGS="${EXTRA_BAZEL_ARGS} --linkopt=-Wl,-rpath,/u/drspeech/opt/jdks/jdk1.8.0_25/lib"
export EXTRA_BAZEL_ARGS="${EXTRA_BAZEL_ARGS} --linkopt=-lz"
#export EXTRA_BAZEL_ARGS="${BAZEL_ARGS} --linkopt=-Wl,-rpath,/usr/local/cuda-7.0/lib64"
export MYBAZEL=/u/drspeech/opt/bazel-0.1.5/0.1.5/bazel-0.1.5/output/bazel
When I invoke bazel with
${MYBAZEL} build -c opt //tensorflow/tools/pip_package:build_pip_package
https://github.com/tensorflow/tensorflow/blob/master/tensorfl%0Aow/python/tools/graph_metrics.py
3. I installed Scipy Pack comprising numpy, and the rest.
Exception information:
I agree that usually ops won't be an issue for save and restore. But what happen to me is that I'm using ExponentialMovingAverage on `c`, then there will be a variable that inherit the inconsistent name from `c`, and then cause problems for save/restore.
After successful bazel build, running into error when executing example with gpu. Please suggest on how to resolve. The GPU has been used with MathConvNet (another deep learning package) without this error.
Aborted (core dumped)`
CUDA 7.5
'/usr/local/cuda/include/curand.h'
'/usr/local/cuda/include/curand_discrete.h'
'/usr/local/cuda/include/curand_mrg32k3a.h'
'/usr/local/cuda/include/curand_mtgp32.h'
'/usr/local/cuda/include/curand_philox4x32_x.h'
'/usr/local/cuda/include/curand_globals.h'
'/usr/local/cuda/include/curand_uniform.h'
'/usr/local/cuda/include/curand_lognormal.h'
'/usr/local/cuda/include/curand_poisson.h'
OS:
mgix@alphanor:~/tensorflow$ lsb_release -a
Codename:   wily
mgix@alphanor:~/tensorflow$ bazel version
Build label: head (@12ecc0b)
mgix@alphanor:~/tensorflow$ gcc -v
Thread model: posix
mgix@alphanor:~/tensorflow$ pwd; git log | head -1
mgix@alphanor:~/tensorflow$ bazel build --spawn_strategy=standalone  --verbose_failures -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
'/usr/local/cuda/targets/x86_64-linux/include/curand_globals.h'
'/usr/local/cuda/targets/x86_64-linux/include/curand_lognormal.h'
'/usr/local/cuda/targets/x86_64-linux/include/curand_poisson.h'
to file `tensorflow/third_party/gpus/crosstool/CROSSTOOL`
seem to be a workaround.
Internal reference: b/29006900
@jendap updated and squashed.
If this is an ABI issue, then the following should help:
It looks like the segmentation fault is being caused when protobuf is imported, specifically the following:
``` Python
``` python
/home/bmcfee/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py in <module>()
/home/bmcfee/miniconda/envs/py35/lib/python3.5/site-packages/google/protobuf/descriptor_pb2.py in <module>()
You can also build Python 3.5 from source but arguably getting it from anaconda is probably faster.
@jjhelmus I installed from your anaconda channel on py3.5 linux-64 and got the following error:
@yoavram What Linux are you running running this on?  Unfortunately due to the method in which tensorflow is built (Bazel) it will likely only run on Linux systems with a version of glibc equal too or newer than the one present on Ubunutu 14.04 (glibc 2.19).
GLIBC_2.14
CXXABI_1.3.5
GCC_3.0
GLIBCXX_3.4.19
My os is ubuntu 14.04
I had the same issues while trying to install tensor flow 0.70 GPU for python3 (pip3). For whatever reason, it was just the FILENAME which was wrong. It has to be in a special format to be legal. You can check yourself by following the instructions [here](http://stackoverflow.com/questions/28107123/cannot-install-numpy-from-wheel-format) which tupels of tags are legal for your machine. In my case the command
Exception:
Storing debug log for failure in /home/artem/.pip/pip.log_
Same issue on Ubuntu 14.04 x64 Python 2.7 and using GPU, downloading locally and renaming .whl file doesn't help, Tensorflow.6.0 worked fine. I did upgrade to Cuda7.5 and cudnn4  first.
Password:
Exception information:
Would be greatly appreciated if you could do this, I need to test a project which uses TensorFlow on Travis CI and don't want to have to build TensorFlow for each Python version every time I test.
@Jx-b you are not alone. Using OS X 10.11.5 and having the same issue.
I am raising this issue following [this stackoverflow discussion](https://stackoverflow.com/questions/35443080/tensorflow-critical-graph-operations-assigned-to-cpu-rather-than-gpu?lq=1). I found that there is not that much support for float64 ops, that is needed for some numerical applications. A number of other issues  #1061 #761 #547 and PR #1089 have been raised around this.
I want to start contributing to Tensorflow, so thought this issue would be a good start. I started with MatMul from among the ops mentioned by @smcantab. In the op implementation I found [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matmul_op.cc#L210), float64 seems to be purposely commented out. Is there a reason for this?
as @dave-andersen pointed out [here](https://stackoverflow.com/questions/35443080/tensorflow-critical-graph-operations-assigned-to-cpu-rather-than-gpu) some operations have not been registered for double simply to reduce the bloat in the binaries, given that, as far as I understand, it was not clear whether there'd be much interest in supporting double precision. Obviously there's been quite some interest so we should go ahead. For some operations it should be as simple as registering the kernel for double, and checking that the tests are there. Some other operations might require tailored optimization. I have been busy with some other stuff, so I have not been able to work on a PR yet, but @siddharth-agrawal if you start one I'll follow it and contribute as soon as possible.
Fractional striding on 1d convolutions would be helpful to downsample 1d signals. Currently there is no way to do this with 2d convolutions as mentioned by @alphaf52 .
File "/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 970, in save
File "/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 990, in export_meta_graph
File "/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1315, in export_meta_graph
File "/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/training_util.py", line 70, in write_graph
File "/Users/aymeric/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/default/_gfile.py", line 295, in MakeDirs
os.makedirs(path, mode)
File "/Users/aymeric/anaconda2/lib/python2.7/os.py", line 157, in makedirs
``` zsh
Hi, the proposed upgrade for Polymer worked just fine for me. I am using anaconda
//tensorflow/user_ops:ackermann_test: Test execution time (0.0s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout="short" or size="small".
I compiled Tensorflow from source. Running cifar10_train.py outputs the following warning message:
Many non-experts are using the following code http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top.
@pawni You have to use a Python boolean for `is_training`. It cannot be a `tf.cond`.
Here's the exception stack:
@akhld @Kecksdose
I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:
CODE (with error):
# Variables.
/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.pyc in _apply_sparse(self, grad, var)
Also I think that this should fail gracefully with a message saying that "XXX is not supported with AdagradOptimizer" instead of a cryptic message that makes the user believe that it's his fault.
@shaileshahuja - whilst not equivalent, a temporary helper would be to use `AdamOptimizer` as it will give you many of the adaptive learning rate advantages of `AdagradOptimizer`.
@Smerity Thank you!
The code works with Adam and SGD, but not RMSProp.
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 68, in testEmpty
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 31, in _testBitcast
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 46, in testLarger
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 31, in _testBitcast
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 51, in testSameDtype
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 31, in _testBitcast
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 56, in testSameSize
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 31, in _testBitcast
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 40, in testSmaller
File "/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py", line 31, in _testBitcast
(extra **num_decoder_symbols** parameter) which caused
raise ArgumentError(action, message % conflict_string)
python cifar10_train.py
bazel test ...
with the pastebin showing the full error
Exception information:
conda install -c https://conda.anaconda.org/jjhelmus tensorflow
x = tf.Variable(3.) # this is an custom op
Is this safe way to apply custom gradients only with python?
Would be nice if I could define custom gradients for my Python ops.
``` python
But having to generate a unique name is quite ugly.
When I leaned TensorFlow, I felt a big barrier between the beginner's mnist tutorial and the expert's one.
pciBusID 0000:08:00.0
@vrv In my case, I didn't see another process setting the cuda context.
Hope someone can edit that, because it's a begginer tutorial, and can be very frustrating to fail running the very first piece of code of the first and most basic tutorial.
# when training this is the Error
./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/cifar10_train
130.253.128.188 - - [12/Feb/2016 11:50:11] "GET /external/dagre/dist/dagre.core.min.js HTTP/1.1" 200 -
130.253.128.188 - - [12/Feb/2016 11:50:15] "GET /external/dagre/dist/dagre.core.min.js HTTP/1.1" 200 -
These seems to be some issue with path like
Pooran
external/jpeg_archive/jpeg-9a/jcmaster.c:726:28: error: cannot convert 'bool' to 'boolean' in assignment
@sesse Thanks for the information and things are now working as expected.
We need bazel 0.2 to have this work.
@kcgthb - I don't think the protobuf Git submodule has been removed yet. Does running `git submodule update --init` work for you?
(cd /home/nmrp3/.cache/bazel/_bazel_nmrp3/af5448b32eaac46105d430d1e2c5b789/tensorflow && \
PATH=/home/nmrp3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
(cd /home/nmrp3/.cache/bazel/_bazel_nmrp3/af5448b32eaac46105d430d1e2c5b789/tensorflow && \
PATH=/home/nmrp3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
My system is unbuntu
$ uname -a
Linux taiwaif 4.2.0-27-generic #32-Ubuntu SMP Fri Jan 22 04:49:08 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
/opt/cuda
/opt/cuda
- bazel 0.1.5-1
- cuda 7.5
``` diff
--- a/third_party/gpus/crosstool/CROSSTOOL
+++ b/third_party/gpus/crosstool/CROSSTOOL
+  cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"
+  cxx_flag: "-D__STRICT_ANSI__"
cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED" but not strict ANSI
``` diff
--- a/third_party/gpus/crosstool/CROSSTOOL
+++ b/third_party/gpus/crosstool/CROSSTOOL
+  cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"
``` diff
--- a/third_party/gpus/crosstool/CROSSTOOL
+++ b/third_party/gpus/crosstool/CROSSTOOL
+  cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"
+  cxx_flag: "-D__STRICT_ANSI__"
-  cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"
Just a related question --- If test would not run under GPU (CUDA) configuration, how can we convince ourselves that CUDA kernels for several TF operations do work correctly?
Have tried Bazel 1.0.0, 1.0.4, 1.0.5
Whats wrong?
The command '/bin/sh -c bazel build -c opt --config=cuda //tensorflow/cc:tutoria
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/A_qsCKiEKGE
after step 1800 I'm getting the following errors:
Aborted (core dumped)`
I built a pip package with  ̀-c opt` in one machine and use the same package on another machine. They don't have different version of core i7 processor. With the second machine, Tensorflow runs 10x slower than with the first. Could it be a compilation optimization problem ?
envy@ub1404envy:~/os_prj/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
envy@ub1404envy:~/os_prj/github/tensorflow$
envy@ub1404envy:~/os_prj/github/tensorflow$ git status
try both bazel 1.4 and 1.5
envy@ub1404envy:~/os_prj/github/tensorflow$ ./configure
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
envy@ub1404envy:~/os_prj/github/tensorflow$
Build a bleeding edge version of tensorflow.
it should be some PATH issue
Please specify the Cudnn version you want to use. [Leave empty to use system default]:
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
ERROR: /tensorflow/WORKSPACE:77:1: new_git_repository rule //external:iron-dropd
ERROR: /tensorflow/WORKSPACE:84:1: new_git_repository rule //external:accessibil
ERROR: /tensorflow/WORKSPACE:98:1: new_git_repository rule //external:iron-icons
ERROR: /tensorflow/WORKSPACE:126:1: new_git_repository rule //external:iron-flex
ERROR: /tensorflow/WORKSPACE:133:1: new_git_repository rule //external:iron-auto
ERROR: /tensorflow/WORKSPACE:147:1: new_git_repository rule //external:iron-comp
ERROR: /tensorflow/WORKSPACE:182:1: new_git_repository rule //external:marked-el
ERROR: /tensorflow/WORKSPACE:203:1: new_git_repository rule //external:iron-chec
ERROR: /tensorflow/WORKSPACE:224:1: new_git_repository rule //external:es6-promi
ERROR: /tensorflow/WORKSPACE:231:1: new_git_repository rule //external:promise-p
ERROR: /tensorflow/WORKSPACE:245:1: new_git_repository rule //external:paper-men
ERROR: /tensorflow/WORKSPACE:252:1: new_git_repository rule //external:iron-icon
ERROR: /tensorflow/WORKSPACE:259:1: new_git_repository rule //external:iron-meta
ERROR: /tensorflow/WORKSPACE:273:1: new_git_repository rule //external:iron-resi
ERROR: /tensorflow/WORKSPACE:280:1: new_git_repository rule //external:iron-fit-
ERROR: /tensorflow/WORKSPACE:287:1: new_git_repository rule //external:iron-over
ERROR: /tensorflow/WORKSPACE:301:1: new_git_repository rule //external:iron-a11y
ERROR: /tensorflow/WORKSPACE:322:1: new_git_repository rule //external:iron-vali
ERROR: /tensorflow/WORKSPACE:329:1: new_git_repository rule //external:sinon-cha
ERROR: /tensorflow/WORKSPACE:343:1: new_git_repository rule //external:iron-inpu
ERROR: /tensorflow/WORKSPACE:350:1: new_git_repository rule //external:iron-menu
ERROR: /tensorflow/WORKSPACE:364:1: new_git_repository rule //external:iron-list
ERROR: /tensorflow/WORKSPACE:385:1: new_git_repository rule //external:iron-rang
ERROR: /tensorflow/WORKSPACE:399:1: new_git_repository rule //external:web-anima
ERROR: /tensorflow/WORKSPACE:434:1: new_git_repository rule //external:paper-rad
ERROR: /tensorflow/WORKSPACE:441:1: new_git_repository rule //external:iron-sele
ERROR: /tensorflow/WORKSPACE:469:1: new_git_repository rule //external:iron-beha
viors's name field must be a legal workspace name.
ERROR: /tensorflow/WORKSPACE:483:1: new_git_repository rule //external:iron-coll
ERROR: /tensorflow/WORKSPACE:497:1: new_git_repository rule //external:paper-rad
ERROR: /tensorflow/WORKSPACE:504:1: new_git_repository rule //external:paper-hea
ERROR: /tensorflow/WORKSPACE:511:1: new_git_repository rule //external:prism-ele
ERROR: /tensorflow/WORKSPACE:525:1: new_git_repository rule //external:paper-men
ERROR: /tensorflow/WORKSPACE:546:1: new_git_repository rule //external:iron-icon
I'm a TensorFlow rookie, so good chance I did something wrong.
@mikehaley did you build with GPU support `-c opt --config=cuda`?  Maybe that will help narrow down the issue.
File "/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/queue_runner.py", line 112, in _run
File "/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 368, in run
File "/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 444, in _do_run
e.code)
File "/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py", line 414, in count_up_to
File "/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py", line 110, in count_up_to
File "/Users/blakec/virtual_envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1043, in __init__
``` python
Bazel version 0.1.4 and 0.1.5
What Bazel version should we use? The site says 0.1.1
Does anyone have this problem and manage to fix it? I used both bazel-0.1.5-installer-linux-x86_64.sh and bazel-0.1.4-installer-linux-x86_64.sh with no luck. The version of Tensorflow I tried is c4207090d68151fb967672a90ea6d574e62cfba0 (the latest master version at the moment)
This is my protobuf version:
Nice thx @nysalad :)
@lukaszkaiser Any word on this?
throws the following exception:
I also made up a workaround for this:
@girving I tried to use tf.placeholder with no shape, however I still cannot reproduce this bug.
PATH=/usr/local/cuda/bin:/opt/ros/fuerte/bin:/opt/lfd/scripts:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/opt/gurobi650/linux64/bin:/home/#####/bin \
PATH=/usr/local/cuda/bin:/opt/ros/fuerte/bin:/opt/lfd/scripts:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/opt/gurobi650/linux64/bin:/home/#####/bin \
except Exception, e:  # pylint: disable=broad-except
@danmane @martinwicke as of the current master, tensorboard installed through pip does not work since one of the files necessary (tensorboard_server.py) is missing. Also the issue mentioned in #1000 is solved in this PR.
@caisq cool! Would make life easier :D
I am trying to build the Tensorflow Android Camera Demo for x86 (or x86_64, or mips) architecture but bazel seems to be ignoring the --android_cpu flag and always builds for armeabi-v7a.
For mips we cannot still compile it. The problem seems to be different there, after removing the flag:
undefined reference to `__atomic_fetch_add_8'
(cd /home/enrico/.cache/bazel/_bazel_enrico/a722061e4e14ee10d0cf53e5e96fcc56/tensorflow && \
(cd /home/enrico/.cache/bazel/_bazel_enrico/a722061e4e14ee10d0cf53e5e96fcc56/tensorflow && \
Hi renats,
would be nice to have an official clarification though
![big center icon version](http://coderbox.in/github/img/TensorFlowBC.png)
544                        divisor, name="variance")
/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)
/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
``` python
``` python
``` python
divisor, name="variance")
Oren
Then I run `nvidia-docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow-devel-gpu` it downloads everything and runs the docker container. Next I run ipython and try to import tensorflow but I get the following errors:
**I think I just have a lack in understanding about how I should run the TensorFlow container, or maybe I have to build the container using nvidia-docker.
e.code)
@caisq  yes this is the result of it
Sure, I just do not get what do you mean by the third bullet point "add a: valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) where the other calls are"
Sure, @danmane . After doing that, please just close this one. :-)
'data': tf.VarLenFeature(tf.float32),
When raise Exception at line 327, 'pickle_file' is not predefined and raise error.
When raise Exception at line 327, 'pickle_file' at line 328 is not predefined and raise error.
@jendap:
private:
Yes, FIFO is the issue.
(lots of flags here)
I'm not sure whether it was caused by tensorflow or my configuration, but I did find some suspicious code in `tensorflow/python/client/tf_session.i`, that closely resembles the problematic code:
LEon
run on virtualbo
I will ask on Stack Overflow
@sungjinhwang What about pad with constant value other than zero? This would be useful, because I want to set the constant value to the average values of the input. It would also be useful to choose a constant value for each channel.
@girving That is clever =) Thanks!
[[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message="ReluGrad input is not finite.", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/add)]]
[[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message="ReluGrad input is not finite.", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/add)]]
e.code)
[[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message="ReluGrad input is not finite.", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/add)]]
File "/Users/yoav/projects/music_rnn/model.py", line 61, in __init__
File "/Users/yoav/projects/music_rnn/model.py", line 58, in __init__
perform multilabel classification where a picture can contain both an elephant
and a dog at the same time.
$python cifar10_train.py
Aborted (core dumped)
An error would be nice, it could save other beginners a headache.
Cuda: 7.5
Cudnn: 6.5
Bazel: 0.1.4
Now the error is about **sparse_xent_op.h**.
_unsupported_sandboxing.
PATH=/home/master/04/weitang114//bin/:/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/local/cuda-7.5/bin:/home/master/04/weitang114//bin \
iles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-b45554449873 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-b45554449873 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt
./tensorflow/core/kernels/sparse_xent_op.h(58): error: explicit type is missing ("int" assumed)
./tensorflow/core/kernels/sparse_xent_op.h(73): error: explicit type is missing ("int" assumed)
./tensorflow/core/kernels/sparse_xent_op.h(90): error: explicit type is missing ("int" assumed)
./tensorflow/core/kernels/sparse_xent_op.h(104): error: explicit type is missing ("int" assumed)
./tensorflow/core/kernels/sparse_xent_op.h(122): error: identifier "int64" is undefined
./tensorflow/core/kernels/sparse_xent_op.h(133): error: identifier "int64" is undefined
./tensorflow/core/kernels/sparse_xent_op.h(91): error: identifier "labels" is undefined
/opt/cuda
/opt/cuda
--define=D_MWAITXINTRIN_H_INCLUDED=1 \
- bazel 0.1.5-1
- cuda 7.5
``` diff
--- a/third_party/gpus/crosstool/CROSSTOOL
+++ b/third_party/gpus/crosstool/CROSSTOOL
+  cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"
+  cxx_flag: "-D__STRICT_ANSI__"
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
Looking around, I see eigen3 under third_party/eigen3
Eigen:
Cholesky  Dense  Geometry     Jacobi       LU            QR               SVD
unsupported:
Eigen
@zheng-xq @girving
+  rand_ = getpid();
+  unsigned rand_;
(gdb) info threads
(gdb) bt
error as follows:
ERROR: /Users/delip/Softwares/tensorflow/WORKSPACE:73:1: iron-ajax is not a legal workspace name.
ERROR: /Users/delip/Softwares/tensorflow/WORKSPACE:80:1: iron-dropdown is not a legal workspace name.
ERROR: /Users/delip/Softwares/tensorflow/WORKSPACE:87:1: accessibility-developer-tools is not a legal workspace name.
(I tested by changing my local bazel setting)
Aborted (core dumped)`
/home/b3432/ShareCache/houzhi/data
train-images-idx3-ubyte.gz
/home/b3432/ShareCache/houzhi/data
train-labels-idx1-ubyte.gz
/home/b3432/ShareCache/houzhi/data
t10k-images-idx3-ubyte.gz
/home/b3432/ShareCache/houzhi/data
t10k-labels-idx1-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/train-images-idx3-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/train-labels-idx1-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/t10k-images-idx3-ubyte.gz
Extracting /home/b3432/ShareCache/houzhi/data/t10k-labels-idx1-ubyte.gz
Aborted (core dumped)
My issue is probably script specific. I ran the demo mnist model and it was running fine.
I'm getting this error. Are there any workarounds? @xlangacadia, what do you mean by "disabled GPU for Theano"? In .theanorc? I restarted the computer (Theano was running previuosly), so that should have taken care of getting Theano off the GPU, I would think. Restarting did not help.
@jmugan Make sure you don't have a `import theano.tensor as t` line lying around in your code -- this will cause this issue to happen.
This is the first time taking a course on udacity. With atmost interest opened up the first assignment and ended up in Exception:
Though, as the screenshot shows, I have allocated 5GB of memory to the Docker container running [the course's first Jupyter notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb), I can't seem to get past this cell because of Python's `cannot allocate memory` error. Sometimes it happens with `notMNIST_large/B` and sometimes with `notMNIST_large/C`. I don't see why this error is thrown when the container reaches 1.4GB, since it should be able to go up to 5GB...
I doubt this is a problem with TensorFlow but, since it's part of the course, I figured this issue might be helpful to other students as well.
Meanwhile the usage stats of the container (`docker stats udacity_deep_learning`) show that it's nowhere near the limit (the most it went up to before crashing was about 1.5GB)
hope this helps  @mamigot
The 2nd line of syntax is wrong:
Hi jeandut
Hi martin-gorner thanks for making this thread live on !!! By reading your code hastily I am kind of curious about what you are doing but will try it tomorrow for sure. However what intrigues me the most is the last line, where does Wtag come from ? Is it a way to make it evolve through time ? (like any othe summary except images that seem to be stuck)  Because in mark daoust's trick you already had a kind of reshaping into a grid of weights. Could you explain the advantages of your method ?
@jeandut
Jeandut, I can understand your answer is not satisfactorily answered. I join you on the quest for nice tutorials.
darwin,
``` c++
``` c++
``` c++
if flops is not None:
Flops should be ~ 7200
``` python
import theano
import theano.tensor as T
Right now I could use a double casting as a work around, but why is it different?
``` python
Host: El Capitan 10.11.3
File "/Users/xxx/tensorflow/lib/python2.7/site-packages/tensorflow/python/**init**.py", line 13, in <module>
protobuf==3.0.0a3
Pierre
pip uninstall protobuf
tried Grzego's approach. Got a new error though...
pip uninstall protobuf
pip uninstall protobuf
Thanks to everyone who developed TensorFlow, it's a fascinating tool.
typedef typename RhsMapper::Scalar RhsScalar;
same error as rizzomichaelg.
It's not working on mac...
If the gradients don't propagate down to the embeddings, the input of the decoder RNN will be the random _initial values_ throughout the training. This might be weird, for example, when the embeddings for all words have the same initial values (word 'ils' corresponds to `[1, 0, 0, 0, 0]`, word 'personnes' corresponds to `[1, 0, 0, 0, 0]` too, ...).
mcarbonell@asterix:~/tensorflow$ git branch -a -v
I did the three differents installations
train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
@waoudi I got my
Hopefully http://yann.lecun.com/ will come back online soon.
http://yann.lecun.com/exdb/mnist/ it is back again
Simple typo in the description.
I am a fresher in tensorflow. May I ask you one more question?
Vocab size:  71290  + UNK
I'm trying to install TensorFlow with GPU support for a CUDA Capability 3.0 device. I have followed the instructions found here: https://gist.github.com/Mistobaan/dd32287eeb6859c6668d, and everything compiles without error, but TensorFlow doesn't recognize my GPU device. The steps that I'm taking are below, as is the error output.
``` python
``` python
Second, `char2id` needs to be changed this way:
``` python
print('Unexpected character:', char)
Finally, there are places where the division has to be made to `\\` because in python3, the division is floating point by default. For example, in `6_lstm.ipynb`,
``` python
``` python
On Wed, Jan 27, 2016 at 10:00 PM, ebrevdo notifications@github.com wrote:
with bazel 0.1.4
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__="redacted"' '-D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command
leads to the exception being thrown.
PTAL at https://github.com/tensorflow/tensorflow/pull/948.
The problem I am facing is, when I do steps 1 and 2 as given above _outside_ TensorFlow, the model works well. However, when I try to do the computation of derivatives inside a TensorFlow Graph, there seems to be some noise seeping into the values of the variables. Heres the code:
What is the reason for this? Is this somehow because of the Optimizer (I can't figure out why that would be)? Or am I doing something wrong?
here is stackowerflow question
Patrick
Patrick
Aborted
// Support 5 or more arguments
template <typename... AV>
typo. see PR844
typo
I signed it! #cla
name 'tf_copts' is not defined.
name 'tf_copts' is not defined.
bazel version
This quarter is nearly over, how about do it in Q2:-)？@danmane
@dsmilkov @danmane any feedback on this? I'll rebase it if it's worth the effort.
@martinwicke rebased and ran `gulp vulcanize` again.
``` python
Related to #551, #763 ?
modified.
> ebe109b
Starting TensorBoard  on port 6006
Exception in thread Thread-2:
@danmane I was training an RNN on 4 GPUs. The weights was defined on CPU. Then I got a large graph. I am not sure if I am using RNN correctly in this multiple-GPU case. Do you know how to train RNN on 4 GPUs? I was using rnn_cell.BasicLSTMCell and rnn_cell.MultiRNNCell() to create an RNN.
File "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 530, in convert_to_tensor
activation=tf.nn.relu)
``` python
``` python
print("Exception hear")
``` python
However, if I change `GradientDescentOptimizer` to `AdagradOptimizer` (or `RMSProp`, `Adam`, ...), it fails in the variable-initialization (second-to-last) line, complaining that `v_input` must be fed:
At first I was under the impression, the variables returned by `tf.moving_average_variables()` would be shadow variables created, but that does not seem to be the case (they didn't save/restore correctly for me).
I guess I understand what @panmari says because I have found the same problem.
So seems to me that the second case was closed only by commit message special syntax like "Closing".
Maybe try a `bazel clean --expunge` first
lli@mobvoi-rhea-01:~/jdk1.8.0_65$ /home/lli/bazel/output/bazel clean --expunge
lli@mobvoi-rhea-01:~/jdk1.8.0_65$ cd
lli@mobvoi-rhea-01:~$ cd tensorflow/
lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel clean --expunge
Unrecognized option: --verbose_failure
My bad the correct option is --verbose_failures
> Unrecognized option: --verbose_failure
> lli@mobvoi-rhea-01:~/jdk1.8.0_65$ cd
> lli@mobvoi-rhea-01:~$ cd tensorflow/
> lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel clean
> lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c
> Unrecognized option: --verbose_failure
> lli@mobvoi-rhea-01:~/tensorflow$ /home/lli/bazel/output/bazel build -c
> error adding symbols: Bad value
Is it fixed because we have the similar problem? @fancyerii
Hello ibab.  Have you been able to make any progress on this?  Thanks!
``` python
``` python
This might turn out to be too difficult for me, but I'll give it a try over the next few weeks.
This strikes me as a very hacky solution.
What about xw_plus_b?
Up to now, it's not a problem since `unigrams` is always given.
e.code)
#### Versions:
@mcuadros, the images taged with `-gpu` are intended to be used with the [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) project, as you may well note the parent image they are built from, [`FROM nvidia/cuda:7.0-cudnn2-runtime`](https://github.com/tensorflow/tensorflow/blob/5abead8c434d5c99c0eb43385f833844eff55721/tensorflow/tools/docker/Dockerfile.gpu#L1).  Go check out [nvidia-docker wiki](https://github.com/NVIDIA/nvidia-docker/wiki) on how to get started and launch GPU enabeled containers. Cuda is already setup in the image, all your host needs is the nvidia driver, docker, and the nvidia-docker plugin.
@cancan101 , that's odd. Just did a fresh build of both cuda and tensorflow with [`Dockerfile.devel-gpu` ](https://github.com/tensorflow/tensorflow/blob/41671d980d1c0e517e588d217d0d7d63b430d03b/tensorflow/tools/docker/Dockerfile.devel-gpu) and then got the same error you got:
@ruffsl After reading the [code](https://github.com/tensorflow/tensorflow/blob/97f585d506cccc57dc98f234f4d5fcd824dd3c03/tensorflow/stream_executor/dso_loader.cc#L125-L176)  more thoroughly here is a workaround (don't ask me why):
So I turned to docker and it leads to the current problem.
The problem here is that it tries to dlopen `libcudnn.so` but it doesn't exist if you don't have the package `libcudnn4-dev` (which is the case for `nvidia/cuda:7.5-cudnn4-runtime`). Instead, `libcudnn.so.4` is present, which corresponds to the `SONAME`. It works for other CUDA libraries though, it seems we have a small inconsistency here.
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Thank you for reviewing, @ebrevdo !
I am running the code provided with the tutorial Convolutional Neural Networks https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html
Manu
On Mon, Jan 18, 2016 at 3:30 PM, Aymeric Damien notifications@github.com
in cifar10_eval
Manu or Carlos, could you elaborate a bit on how you solved this problem?
I used the normal learning rates (and parameters) from the standard CIFAR10 models. My way of successfully combating this problem was actually kind of working around it : Since i work with very suspicious and paradox data (difficult and human labeled), sometimes, due to contradicting information, there is a sequence of learning examples generating very high errors. So i simply reduced the batch size to not stack these errors too high, and in case of failure (see above) i simply switched back to an "OK" model before that point.
train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
(tensorflowGPU)administrator@zsp515-6a:/home$ python -m tensorflow.models.image.mnist.convolutional
File "/home/administrator/tensorflowGPU/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 30, in run
train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
os.makedirs(path, mode)
File "/home/administrator/tensorflowGPU/lib/python2.7/os.py", line 157, in makedirs
2) my google account is biruei.chiu@gmail.com. I signed CLA using it and designate my github account as brchiu@ms35.hinet.net in contact information.
With verbose_faillures :
ERROR: /Users/zeis/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command
I'm totally new to bazel, so I didn't easily spot what went wrong, but someone familiar with it might know what's going on …
Thank you, @danmane , @vrv , and @martinwicke .
``` python
Also just from a practicality standpoint, you should be able to compute gradients and then perform mathematical operations with them without having to worry about something unexpectedly becoming a non-`Tensor` and causing an exception to be raised. In some TF code I wrote recently I had to make the following function to avoid this bug:
``` python
``` python
2. There's a weird bug in `scatter_op_gpu.cu.cc` where I cannot use `TF_CALL_GPU_NUMBER_TYPES`.  I suspect it has to do with the `defined(__ANDROID__)` magic in `register_types.h` interacting badly with nvcc, but cannot confirm.
Hi, I made several changes based on comments.  PTAL.
Squashed and rebased.  PTAL.
@bellaj The glibc version is too low. and I failed in compling it on the machine. I have no idea, maybe it cannot install on an old system.
It's also a very low version 2.6.32-220.23.2
(tensorflow_env)sghosh@halaklin:~/anaconda/envs/tensorflow_env/lib/python2.7/site-packages/tensorflow$ python -m tensorflow.models.image.mnist.convolutional
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Aborted (core dumped)
No output to the terminal. No nothing. Running Chrome on Ubuntu 14.04 at 669790ac.
ERROR: /system/user/bioinf01/tom/sources/tensorflow/tensorflow/python/BUILD:71:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: crosstool_wrapper_driver_is_not_
However, my custom operations that worked in 0.5.0 fail with the same message. Would be very interested to find out why
Update: Probably related to this report: http://stackoverflow.com/questions/34689114/where-is-the-external-code-unsupported-eigen-cxx11-tensor-for-tensorflow-githu/34753456#34753456
ERROR: /system/user/bioinf01/tom/apps/tensorflow/tensorflow/core/BUILD:148:1: undeclared inclusion(s) in rule '//tensorflow/core:gpu_runtime':
Running (spawn):
@untom I got a similar "undeclared inclusion(s)" error with bazel 0.1.3, but downgrading to a precompiled 0.1.1 seemed to fix the problem (note, however, that I am not trying to build the pip package).
@untom +1, I got the same problem
@untom, @JianbangZ
`ERROR: /system/user/bioinf01/tom/sources/tensorflow/tensorflow/python/BUILD:71:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: crosstool_wrapper_driver_is_not_
ERROR: /data01/bioinf/tom/apps/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command
For what it's worth, I'm trying to use Python 3.5.1 (custom build, not the system installation). What I find really puzzling is the file bazel is trying to execute (`2to3`):
ERROR: /data01/bioinf/tom/apps/tensorflow/tensorflow/python/BUILD:247:1: output 'python3/tensorflow/python/ops/functional_ops.py' was not created.
bazel 0.1.1 and 0.1.3 give this error immediately
PATH=/home/zer0n/src/torch/install/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
PATH=/home/zer0n/src/torch/install/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jRkkvsB1iWA
> What matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out [the next tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts) where we do a lot better, and learn how to build more sophisticated models using TensorFlow!
rebased
[[Node: SparseToDense/_203 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/gpu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_474_SparseToDense", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"]()]]
Would it be acceptable to use PY_MAJOR_VERSION for the comparison instead of NUMPY_IMPORT_ARRAY_RETVAL, just to avoid adding macro black magic just for this? I think it will make the code clearer.
Since I have both Python 2 and 3 installed, is there a chance bazel is passing the wrong includes/libs to GCC? I tried cleaning and reconfiguring the build to no avail.
To add to the weirdness of this issue: something (maybe SWIG?) adds the following macros to pywrap_tensorflow.cc:
What is the status of this?
Unfortunately it turned out to be not completely trivial. Contraction performance characteristics are also tightly tied to execution order (e.g. packed rhs reuse while it is hot in cache). And any asynchronous continuation-based implementation shuffles execution order. So most time is taken by ensuring that there are no significant performance degradations on any of hundreds of different matrix configurations, thread counts and instruction sets (sse, avx, fma).
def gibbs_hvh(self, h0_sample):
""" A gibbs step starting from the hidden layer """
""" A gibbs step starting from the visible layer """
" One step of contrastive divergence, with Rao-Blackwellization "
import rbm
wo = init_weight([500,10])
bo = init_bias(10)
py_x = build_model(X, rbm_w, rbm_hb, wo, bo)
but here comes the error:
following code
Looks like the mac build is flacky...
dcgan.train(FLAGS)
File "/home/panmari/DCGAN-tensorflow/model.py", line 140, in train
File "/home/panmari/DCGAN-tensorflow/model.py", line 245, in save
@vrv is it? It could have the same cause, but the exception is raised in a completely different place.
``` Python
e.code)
name: GRID K520
e.code)
Caused by op u'ResizeBilinear', defined at:
So may I ask if TensorFlow supports Fast R-CNN (currently)? Or are there any possible specific tips to solve these problems?
@ExonRen I was thinking to that too, did you get any further with fast R-CNN on tensorflow?
- Additionally, _part three_, which is also a native Python loop (for each ROI) with TensorFlow `run()` inside, makes some contributions to the poor performance too.
@ExonRen Sounds great.
@ck196 Ah yes he did a good job. He implemented the RoIPooling directly in C++ as a new operation, while I just worked out some Python codes to do a similar task, due to the narrow schedule. So I believe his implementation should be much more efficient and helpful.
Change-Id: Ib2843c620bc4bd348aafb2d676b61fa35f1223af
Note that the extra logic here: https://github.com/kentonl/tensorflow/commit/04036d7e443da373cc151305ba81ffacafe85d13#diff-9d717423e6d3f4359151c45dfaa554b6R234 is required for shapeless placeholders to work correctly for bidirectional RNNs.
This is necessary because of this shape check in the `reverse_sequence` op: https://github.com/tensorflow/tensorflow/blob/d1b8333effdcb031e6e34a2835a2f1c877fdd79b/tensorflow/python/ops/array_ops.py#L1014. We can't promote the input to rank 2 there, because `tf.reverse_sequence` does not make assumptions about the exact rank of the input.
Memory: 2G
Swap space: 2G
Bazel: 0.1.1
SWIG: 2.0.11
TensorFlow:  f9514a917265f0e98c8fb44abc51158685f72ca6
``` diff
mutex_lock l(mu);
- Why does this break now? Was it just a warning previously?
As I installed it in a non-standard directory, I added the following options to the cmake commands when building TensorFlow:
Tensorflow version: 0.6.0
Running cifar10_train.py
name: GRID K520
Aborted (core dumped)
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
name: GRID K520
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Aborted (core dumped)
/cc @CDLuminate
You can install these arch AUR packages with `yaourt`, see arch wiki for detail.
`2` is a bit complicated ....
Dirty hacked packages come out very fast.
... some GET
I've had some strange java-related issues with Ubuntu that caused similar behavior. Doing a complete shutdown (not a restart) and then rebooting after waiting 10-15 seconds seems to work for me (no idea why).
Thnks skearnes. I solved this problem through cold starting.
name = "androidsdk",
api_level = 23,
path = "/home/kuntal/Android/Sdk"
path = "/home/kuntal/knowledge/IDE/android/android-ndk-r10e/",
api_level = 21
ERROR: /home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 13 argument(s) skipped).
WARNING: /home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/core/BUILD:649:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/core/kernels:where_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/core/BUILD:649:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/core/kernels:xent_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
**ERROR: ****/home/kuntal/.cache/bazel/_bazel_root/5da6062df6ce7596334ec4b5c79b36c3/external/androidsdk/BUILD:150:2: Executing genrule @androidsdk//:zipalign_runner failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped).
Actually, theano acts very fast, it support cudnn4 only several days after it comes out.
@fayeshine I think cuda 7.5 is also experimentally supported right now. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#using-a-different-cuda-sdk-and-cudnn-versions
Aborted (core dumped)
from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
#ifndef __CUDA_ARCH__
``` python
Throws the following exception:
``` python
import os
Bug: When repeatedly creating and closing sessions on a GPU or CPU (on 0.6.0 release and Jan 5 dev version), memory appears to leak as the python process continually grows in memory usage. Additionally, the time to execute continues to grow.
Hi folks. This name sigmoid_cross_entropy_with_logits struck me as strange. In addition to being a mouthful, we're calling it cross_entropy when it really is just binary cross_entropy calculated separately on every node.
There seem to be two solutions here:
Sophea
``` python
@koth  thanks for your information :+1:
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
e.code)
Cool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.
When I fetch the hessian, I get the error message
Motivated by [this StackOverflow question](http://stackoverflow.com/questions/34536340/how-to-use-tensorflow-optimizer-without-recomputing-activations-in-reinforcement)
The feature is still experimental and might yet change.
# Naive / Inefficient
I think the document of convolution is still quite confusing right now.
seems not easy to shard them without changing the NegTrainOp and python
caller code a lot?
Aborted (core dumped)
Aborted (core dumped)
Firstly, thanks for tensorflow!
specific names and paths of CUDA libraries
supports only primitive types, but is more performant
Thanks for trying this out @Mistobaan and writing a tutorial!  I'd add a step for installing GNU coreutils ("brew install coreutils") to the tutorial - most people probably don't have it installed.
@Mistobaan I followed your instructions, but "brew cask install cuda" defaults to CUDA 7.0, and @ville-k patch is using 7.5 by default.  I tried both.  With CUDA 7.0, I get a compile error as such:
@ville-k In testing your change I find that the newly added ALT_PATH doesn't really work.  If you have `libcudnn.6.5.dylib` located inside `/usr/local/cuda/` and not `/usr/local/cuda/lib/`, the symlink command at the end of your `cuda_config.sh` will end up silently creating a bad symlink like this
- /usr/local/cuda/cudnn.so.6.5  (ALT_PATH)
**Mac**
Thanks @Mistobaan.  I actually think you mean `brew update` though.  I did try "upgrade" before and that didn't work.  I didn't know that "update" is the thing to do to "upgrade brew" :-)
(lldb) bt
> libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: mutex lock failed: Invalid argument
(lldb) bt
@ville-k would you mind terribly rebasing?  On linux, updates to bazel have created a number of installation issues where fixes were rolled into the git in the last few weeks.
@elbamos I have a rebased/hacked up branch here: https://github.com/NathanHowell/tensorflow/tree/cuda_osx2 (EDIT: cuda_osx3 is broken)
@elbamos I started a rebase over the weekend, but ran into the Eigen issue also reported here: https://github.com/tensorflow/tensorflow/issues/883
@elbamos I just rebased and pointed this PR to temporarily use my fork of Eigen that builds on OSX (Eigen PR is pending)
Using @Mistobaan's gist as a starting point, I wrote updated/expanded build instructions for those less familiar with building tensorflow:
This is so frustrating. I don't understand why it's so hard to get this to work for macs.
On Apr 18, 2016, at 9:35 AM, Ville Kallioniemi <notifications@github.com<mailto:notifications@github.com>> wrote:
CuDnn version: 6.5
For  OpenCL you can follow https://github.com/tensorflow/tensorflow/issues/22
``` python
KeyboardInterrupt
nvidia-smi
![screenshot 2015-12-31 10 58 57](https://cloud.githubusercontent.com/assets/12500045/12065926/a9860536-afad-11e5-8b5d-12fdb343327d.png)
![screenshot 2015-12-31 11 01 15](https://cloud.githubusercontent.com/assets/12500045/12065944/da739de8-afad-11e5-937b-dcfe1fc3591d.png)
I noticed a similar bug as well, it also is a function of when you do ctrl-c, quite often I've had TensorFlow crash the driver. I presume there must be a NVIDIA driver bug as well, since a user process should never be able to crash a kernel driver...
2. killing the python process
Extracting data/train-images-idx3-ubyte.gz
MemoryError
Marcello
``` python
print("\nlog beam probs")
log beam probs
Hi tilneyyang,
My code is largely based on the code of @kyunghyuncho (you can find his code [here](https://github.com/nyu-dl/dl4mt-tutorial)), but he uses Theano in the implementation.
--data /Users/Translated/Downloads/data --size=256
``` python
u'train/translate.ckpt-15350' (Wrong)
Title is wrong spell.
``` python
``` python
I changed the code in the line notes above, but here is the fixed Python code again so going back up to read it isn't necessary. To test out Unicode letters in Python, make sure to explicitly make the string unicode with `u"This is my héader string!"` syntax:
``` python
while (out + "-" + str(i)) in existingAnchors and i < 100:
zhuotun@sunformoon:~$ python
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
zhuotun@sunformoon:~$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
[sudo] password for zhuotun:
Hey osdf,
Parth
conda install --channel https://anaconda.org/jjhelmus/tensorflow tensorflow
Granted, these aren't the safest of operations, but it's still a surprise that they throw an exception.
also, I think a few lines are over 80 chars. but otherwise, LGTM.
@vrv ok I will add the cuda part and do the edits !
I think there is a mistake:
https://github.com/Mistobaan/tensorflow/blob/7a262ee6467c909cae723e0de5fb87a2a7e9a664/tensorflow/core/kernels/training_ops.cc#L53
This line either should have `accum = ...` (instead of `+=`) or follow the respective line in the rmsprop implementation further down (https://github.com/Mistobaan/tensorflow/blob/7a262ee6467c909cae723e0de5fb87a2a7e9a664/tensorflow/core/kernels/training_ops.cc#L133).
FYI, there's a couple of bugs, namely the += shoulda been =
from william chen
> Hi, mistobaan, please let us know how you plan to proceed on this? I can
Twitter: @fabmilo
## Github: http://github.com/Mistobaan/
Welch)
Perfection must be reached by degrees; she requires the slow hand of time
The best way to predict the future is to invent it (Alan Kay)
@Mistobaan, any update on this? I'm also really interested in trying AdaDelta. Thanks!
e.code)
Can somebody tell me what's wrong?
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
I'm having the same issue, it doesn't seem to make sense...
``` python
However, in Grammar as a Foreign Language, attention is computed using the last hidden vector of the multi-layer LSTM (e.g. `cell_output`). Computing attention with `new_state` involves using every layer's state, including their cell state (which should probably be only used inside the LSTM layer). To match GaaFL, the attention call should be
``` python
``` python
I have this code:
e.code)
Merged upstream and handled the conflicts.
Error: Unknown option: -g
Variable
Tensor
This is not supposed to happen right?
rm -rf /tmp/wchan
12-29 13:09:44.526 3522-4714/? D/BatteryService: online:4, current avg:-882, charge type:2, power sharing:false, high voltage charger:false, capacity:280000, current_now:-432
12-29 13:09:47.396 3522-3691/? I/ActivityManager: Start proc com.skype.raider for broadcast com.skype.raider/com.skype.android.push.RegisterReceiver: pid=17959 uid=10126 gids={50126, 9997, 3003, 3002, 1028, 1015} abi=armeabi-v7a
Why say it is a small trick ?  Both definition should be equivalent right?
Hello Keveman,
code：
import os
import os
Thank  @pannous  and @wchan  very much.
This is also an issue for me.
Here is my code:
Use keras, they wrap this basic functionality in one line
e.code)
[[Node: Variable_1/read = Identity[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](Variable_1)]]
sungjinkim@fas.harvard.edu
sungjinkim@fas.harvard.edu
How is this possible in TensorFlow, I feel like it should be trivial, but I can't find anything in the API to do this =\
fyi, wrote a kernel for this:
This is a fundamental operation and so GPU support is rather important. I'm getting the error on Ubuntu 14.04 with CUDA 7.0.
$ echo $CUDA_SO
$ echo $DEVICES
also @ebrevdo, i've got a quick question which may save me a lot of time:
Or does using OpenCV in that way kind of defeat the purpose of TensorFlow.  I plan on using `SIFT`, possibly `SURF` (since i can GPU accel), colorspace + filter + segmentation + morphology and other algorithms.  There are only a few of these algorithms directly implemented in TensorFlow.  It would seem that I could compose some of the available API functions in TensorFlow to recreate most of the algorithms I could need, but doing so would take more time.  E.G., I don't know that I could simply/efficiently replicate SIFT in TensorFlow without spending quite a few hours.
Initialized!
Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Tensorflow Dockerfile: https://github.com/abhishekpatnia/Docker/blob/master/docker_images/Dockerfile.tensorflow
https://github.com/abhishekpatnia/Docker/blob/master/docker_images/Dockerfile.pycharmDE
https://github.com/abhishekpatnia/Docker/blob/master/bin/dockersh
G0 G0 pâturage d’infrastructures d’infrastructures Twin Twin Twin Twin Twin
expédiées expédiées expédiées m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0
$ virtualenv --system-site-packages /Users/marcotrombetti/Documents/tensorflow-python-env (io ho creato una cartella in Documents)
$ source /Users/marcotrombetti/Documents/tensorflow-python-env/bin/activate
which is not there...
@girving: Why did you close the question without a clear response?
``` python
Hey all, sorry I haven't been able to get to this until now. @yuanbyu from the todos in the code it looks like you might have been involved in writing control_flow_ops.py. I'm looking at the function GetRealValue , and I'm a little confused by this code (line 635-636 of control_flow_ops.py in 7003b76)  :
@yuanbyu @ebrevdo, any updates on this?
On Thu, Jun 16, 2016, 8:38 PM ebrevdo notifications@github.com wrote:
I have the same problem, @sukruozan @elzurdo
I added another commit to support the newly added `align_cornes` flag and adapted some other things necessary. Unfortunately, on my local machine one of the tests in `//tensorflow/python:image_ops_test` is now failing. Could this be due to changes in the GPU allocator?
@vrv in the test where I compare the cpu resizing with the gpu resizing, I get completely different results there even though the other tests pass
File "/home/panmari/.cache/bazel/_bazel_panmari/52e003c51984ee00b8d04ed6ebb99872/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/tensorflow/python/ops/image_ops_test.py", line 990, in testCompareNearestNeighbor
File "/home/panmari/.cache/bazel/_bazel_panmari/52e003c51984ee00b8d04ed6ebb99872/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/tensorflow/python/framework/test_util.py", line 435, in assertAllClose
@vrv sorry, it took some time until I got around fixing this. Seemed like `np.float32` was interpreted differently due to some change since 0.6, leading to the failing test. I generalized my code a bit to make an extension to other types more easy.
So I'm wondering about the --copt=-mfpu=neon
``` python
File "/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 566, in convert_to_tensor
@mrry, you must know better, but I never yet faced an expression where I had to feed `TensorShape` and tuple (well, sometimes list) of integers did do the job.
I mean: I understand that `TensorShape` is a convention for Python API, but I have not seen a case where it is what the final user might need. Do you have documentation on how the unknown rank is represented?
If you replace the mutex in the thread pool with a spinlock (I copied the source from the one here: https://github.com/mldbai/mldb/blob/master/arch/spinlock.h), does it change the shape of the graph?
@tridemax Do you mean some kind of higher-level partitioning?
Different requirements lead to different designs.
It would help if someone working on Tensorflow from Google could provide a position on what kinds of external use-cases are interesting vs the internal ones to help us agree on a starting point.
Hi, my problem is similar to @alantus  post in the beginning:
``` python
File "/home/linchaoz/.local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py", line 30, in run
panmari@pc:~$ tensorboard --logdir PycharmProjects/tensor_stuff/log_render_buffers/
Starting TensorBoard  on port 6006
Exception in thread Thread-2:
The workaround proposed by @ffmpbgrnn fixed it.
@ffmpbgrnn it does work. thanks.
@sherrym, haven't dug into this but still experiencing this bug on 97522f0acd53652baa57e42d06557ebb94bf0c4d and also @ffmpbgrnn's suggestion to change `kDefaultTotalBytesLimit` did allow me to not crash
@panmari Same problem here with the current master. As soon as the complexity of my graph goes beyond a certain threshold, tensorboard does not pick up any data. It shows the run under runs but there is no graph or summaries. Tensorboard does not show any signs of a complain in the logs. However, when I restart it, it gives a similar error message:
Exception in thread Thread-2:
@raingo 's  solution is also effective !
@raingo suggestion was the easiest fix
./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:36:52: fatal error: src/Tensor/TensorContractionThreadPool.h: No such file or directory
PATH=/home/kearnes/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/kearnes/bin:/usr/local/cuda/bin \
PATH=/home/kearnes/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/kearnes/bin:/usr/local/cuda/bin \
Fix citation to Bahdanau et al. in sequence to sequence documentation
added `os.path.isabs` checks.
surprising.
@jendap Can the bot add a label like "admin" or "maintaner" when the PR was opened by developers with write access to this repo?
Rekha-
gflags.FLAGS(['-minloglevel', '3'])
Thanks @davefairtex  I agree its painful for its a simple act of suppressing and silencing logs.Was providing quick suggestions hoping simplest logical way works with flags :-) However looking more though there are various flavors of logging, this one just hits the tensorflow/core/platform/default/logging.h.For this case used verbose logging.thanks.
Hi @fiannaca As stated in the #619 , logging has a different flow here and cannot be overridden by gflags/re2 atm. You can apply the patch for the #619 for your setup to avoid the logs. Another quick pull here https://github.com/tensorflow/tensorflow/pull/1229 to have conditional logging., in some time.thanks.
uname -a
> > >  python
except Exception, ex:
@se7en007  thank you so much! problem solved!
@wlsherica @ivarvanwooning Welcome.
$ docker run -it b.gcr.io/tensorflow.tensorflow
``` python
{'Mul:0': image_data})
@juesato I think that there is a bug somewhere in the tutorial. I tried to make it work with a small subset and different configurations but I never got a plausible translation. I am trying to debug it. Until now no errors on the data and the script until the seq2seq model creation. Continuing...
I have the following similar problem, suddenly my perplexity scores shot up from 3 digit numbers to 5+ digit numbers:
Some of the sampel outputs that I am getting are attached
After updating TensorFlow to the most recent source yesterday (I'm at b1cabed4e60015602dacd66ea39d419db50c3e1b), I've noticed that while GPU utilization frequently appears much higher in nividia-smi than in prior releases, my actual code is much slower. Some sequence to sequence models I was training began taking 3-4 times as long per step, despite GPU utilization hovering between 60 and 99%, which is much higher than I have observed in the past. As I have code for benchmarking fully connected feedforward networks on MNIST in various frameworks, I dusted that off and, again, slower. Previously, training a network with three hidden layers of 2,048 rectified linear units + dropout (input + hidden) took 1.78 seconds per epoch (averaged over 10 epochs)  when trained using vanilla SGD with momentum and a minibatch size of 256. That is now up to 65.2 seconds. This holds across different combinations of hidden layer sizes and minibatch sizes. On the other hand, convolutional net performance does not seem to be affected as when I run Soumith's convolutional net benchmarks, I get numbers close to what he originally reported using the same test setup.
So to summarize, I've been recompiling TensorFlow regularly (every few days since its release) and after the most recent compile noticed a quite substantial performance hit for vanilla fully connected feedforward and recurrent architectures, but not for convolutional networks. This is all with TensorFlow running on a Titan X with no other processes running and using the most recent versions of CUDA, cuDNN (well, I have v3 installed, not the release candidate for v4), cuBLAS, etc.
``` python
Stumbled upon some more errors in the documentation for AvgPool3D and MaxPool3D. Documentation says that these ops can be used with many tensor types, as can be seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/tf.nn.avg_pool3d.md) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/tf.nn.max_pool3d.md). But they are registered only for float32 as can be seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pooling_ops_3d.cc#L124). Will send out a PR soon.
This feels like a dead end to me, as cudnn has come into the picture. Is there a way around this?
types, which might come in handy at some point.
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/gen_docs_combined.py", line 23, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/__init__.py", line 23, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/__init__.py", line 50, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/framework_lib.py", line 62, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/ops.py", line 40, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/framework/versions.py", line 24, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/home/panmari/.cache/bazel/_bazel_panmari/f5b407cdd255b7813aa5a26beb5d6822/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/gen_docs_test.runfiles/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
@vrv this was directed at a bot, not me?
[u'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0',
u'module_a/RNN/cell_output/GRUCell/Gates/Linear/Matrix:0',
u'module_a/RNN/cell_output/GRUCell/Gates/Linear/Bias:0',
u'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Matrix:0',
u'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Bias:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Matrix:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Bias:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Matrix:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Bias:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Matrix:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Bias:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Matrix:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Bias:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0',
u'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0']
@petewarden  Is it compulsory to include the graph in the apk itself / can we program to configure the graph after installing the apk and download the graph later on runtime.
Edit: I mean methods such as resize_nearest_neighbor, not the reshape operator.
e.code)
Caused by op u'Skipgram', defined at:
Starting TensorBoard  on port 6006
The only difference here seems to be the pip packaging.
@danmane
On Apr 11, 2016 2:58 AM, "ultrons" notifications@github.com wrote:
@danmane , good to know that this functionality exists. I will create another issue to request this as a feature in future releases.
First, bazel require GLIBC 2.14, 3 issues came up, however, no solution or idea for the problem is proposed.
`Aborted`
Merge: 5a30c8f 00986d4
error:
pciBusID 0000:08:00.0
Python 2.7.11 :: Anaconda 2.4.1 (64-bit)
which python
``` python
AdaDelta (http://arxiv.org/abs/1212.5701) is a popular training algorithm for Neural Network. It is available in most other libraries I have used, like Torch or chainer.
In my small personal experience (and at least for the Neural Networks I have used), AdaDelta is often the optimizer that works best out of the box (RMSProp working quite well as well, while Adam, Adagrad and SGD typically being not as good)
Therefore, it would be nice if AdaDelta could be added to the set of available optimizers.
I tried to give it a shot in [this branch](https://github.com/tensorflow/tensorflow/compare/master...Mistobaan:master)
# Attention mask is a softmax of v^T * tanh(...).
impl can be found here for those who need it before its pushed upstream:
In my personal framework (not TF), creating a graph of equal size takes approx 2 secs (as opposed to TF almost 8 mins) and the step time is less 4 seconds (as opposed to TF 2 mins).... something really weird is going on.
@ebrevdo hehehe? Actually, kinda stuck... if my Tensor is in GPU memory, how can I get the value in CPU memory? can't find any API function available in tensor.h or in Eigen Tensor for that, i.e., for the sequence_length to do the dynamic compute.
thanks for the pointer! no, i need GPU for sure... i dont have google infrastructure, definitely need GPUs :p
Anyone see something like that before? I'm guessing this must be related to the Eigen::Tensor?
notice the “Tensor” reference and “GpuDevice” in the name mangling … I’m guessing this must be a template error, and the GpuDevice specialization failed
William Chan
William Chan
On Wed, Feb 17, 2016 at 7:37 PM, ebrevdo notifications@github.com wrote:
Hi @ebrevdo ,
Hi @ebrevdo ,
The error is:
I'm using 2.7, however when I commented out the `from __future__ import division` the original code worked fine. So it is some "v3" trickery.
``` python
This might be connected to #464, the error is similar and also appears with Momentum and Adagrad.
``` python
if n.type == "MatMul":
nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled,
``` python
@goodfeli Is there any plan to make a PR of your Net2Net tensorflow implementation?
@petewarden I would greatly appreciate it if you could provide us a tutorial on how to send an image to a tensorflow serving server and get a response with the results.
@syed-ahmed
@petewarden Hi Pete, so I loaded the newer inception model in the android demo with the input size of 299 and mean of 128. I got "correct recognition" however, I didn't divide the RGB values with 128 in the tensorflow_jni.cc. The division part looks like what we do in standardizing a random variable that is normally distributed? So since I didn't divide it with 128, does it mean that the recognition is still happening but since it's not standardized, it may fail for some cases?
Oren
@syed-ahmed thanks for your help! That got me a little bit farther, but there's a fun new error now:
got past `KeyError: "The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph."` and it's the key to my next problem.
@josefmonje Check from here, maybe this will be useful. https://github.com/eldor4do/Tensorflow-Examples/blob/master/retraining-example.py#L82
@petewarden Sure,  no problem!
@josefmonje welcome! and i merged your PR
ERROR: /home/erle/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@accessibility-developer-tools//': SSL peer shut down incorrectly and referenced by '//tensorflow/tensorboard/bower:bower'.
Also tried with bazel `0.1.1`. Same result.
Aborted (core dumped)
dmesg shows that an illegal memory access was performed:
=========     at 0x000002f0 in /opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:346:_ZNK5Eigen15TensorEvaluatorIKNS_18TensorCwiseUnaryOpINS_8internal13scalar_log_opIfEEKNS_9TensorMapINS_6TensorIfLm2ELi1ElEELi1EEEEENS_9GpuDeviceEE6packetILi1EEE6float4l
=========     Address 0x00000000 is out of bounds
=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:407:void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op<float>) (void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op<float>) : 0x1460)
=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bc036]
=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bb2f8]
Shouldn't we use both `pragma` once and `ifndef` gaurds ?
Still getting the above error on the latest El-Capitan released today
from io import BytesIO
import _io
@orcaman @Archivus I was having the same issue after installing tensorflow from bazel, on mac. I fixed it with
When I try to use tf.nn.elu.
There are three types:
Hi hardmaru!
@elbamos I have a new open PR for OSX Cuda support that uses Cuda 7.5: https://github.com/tensorflow/tensorflow/pull/664
+1 - Had this partical MacBook spec'ed because of the NVIDIA GeForce GT 750M 2048 MB (MacBook Pro (Retina, 15-inch, Mid 2014))  Oddly enough it seems on the latest models Apple stopped supplying NVIDIA graphics cards.
I use a linux machine for running TensorFlow, but I'm developing on MacBookPro (Early 2013) which has NVIDIA GeForce GT 650M. Though I have installed CUDA on my mac, TensorFlow wouldn't use GPU. Any support will help me quite a lot.
@elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?
I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X.
> @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?
> On Apr 9, 2016, at 00:19, elbamos notifications@github.com wrote:
> I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X.
> > @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?
> > On Apr 9, 2016, at 00:19, elbamos notifications@github.com wrote:
> > > @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?
@elbamos: this is most likely not true. Can you please run the CUDA benchmarks on OSX and Linux for something simple like repeated GEMM calls (to ensure proper burn in) and report back?  CUDA provides examples that can natively be built on OSX and Linux. If you are seeing more cpu cores being used you are most likely using CPU BLAS and might be experiencing performance gains due to lack of host <-> gpu transfers.
> @elbamos: this is most likely not true. Can you please run the CUDA benchmarks on OSX and Linux for something simple like repeated GEMM calls (to ensure proper burn in) and report back? CUDA provides examples that can natively be built on OSX and Linux. If you are seeing more cpu cores being used you are most likely using CPU BLAS and might be experiencing performance gains due to lack of host <-> gpu transfers.
If you are curious about the Metal API, you can watch some of the videos from WWDC (https://developer.apple.com/videos/play/wwdc2015/610/). Unfortunately, I think you might have to watch them on Safari, because for some reason the videos only stream on safari.
In fact, I highly recommend checking out this guys, Amund Tveit, https://github.com/atveit, work...
> If you are curious about the Metal API, you can watch some of the videos from WWDC (https://developer.apple.com/videos/play/wwdc2015/610/). Unfortunately, I think you might have to watch them on Safari, because for some reason the videos only stream on safari.
> In fact, I highly recommend checking out this guys, Amund Tveit, https://github.com/atveit, work...
Thank you @Vesnica !
Thanks, @Vesnica
^Hsaito, this didn't fix the problem for me either
Would there be any reason that this procedure would be different for python 2.7 instead of python3? I've gone through all the steps above, but am still getting this error:
This break code with v0.5.
We were able to get arctan implemented and working great with complex64 numbers. We're running into an issue with registering the scalar_arg_op with the complex64 data type. We can get arg to compile when we register it only with floats and double without errors, but when we call it in python it fails when passed a complex64 as expected. We think the error we are getting stems from:
I'm working with Ryan on this. We have it working utilizing the Eigen implementation of arg and are currently in the process of writing tests for it. We haven't been able to test it on a GPU enabled device but will likely be able to do that soon as well, should we wait until we do that to submit a pull request? Also, we were curious if we should include the code we created for implementing arctan through Eigen before we knew that Eigen had a method for computing the argument directly.
for bazel
showing error
![wmtbazelerror](https://cloud.githubusercontent.com/assets/10511526/11765697/768b05e2-a188-11e5-958b-31b4edf30860.png)
![error in language](https://cloud.githubusercontent.com/assets/10511526/11775069/afedea00-a261-11e5-8dd8-36f4ced1dcee.png)
for elm in tensor_list_list:
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
OS：ubuntu 15
Exception information:
@elbamos Thanks for trying that out! Looks like I rushed the pull request and missed a step. I was not passing the "--config=cuda" option to bazel and the build code that gives you the error you reported never got run.  I'll:
From nvidia-dmi
|    3     26078    C   python                                          37MiB |
``` python
And the full error:
$ cat /home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/testlogs/tensorflow/python/chain_crf_op_test/test.log
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/kernel_tests/chain_crf_op_test.py", line 1, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/__init__.py", line 23, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/__init__.py", line 50, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/framework/framework_lib.py", line 62, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/framework/ops.py", line 40, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/framework/versions.py", line 24, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/pywrap_tensorflow.py", line 26, in <module>
File "/home/jernite/.cache/bazel/_bazel_jernite/f9fe393f3882802b0a658bd50e054d61/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/python/chain_crf_op_test.runfiles/tensorflow/python/pywrap_tensorflow.py", line 22, in swig_import_helper
Yacine
Yacine
alex@ml1:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
alex@ml1:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
Thu Dec 10 12:13:53 PST 2015 : === Using tmpdir: /tmp/tmp.NqHERREKmr
/tmp/tmp.NqHERREKmr ~/tensorflow
This problem still appears for me for master and bazel 0.1.2
Strings also need to be encoded for the `wfile.write`s in [tensorboard_handler.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard_handler.py).
To dankolbman,
``` diff
import os
``` diff
-import urllib.parse
if compressed_histograms:
@@ -386,7 +386,7 @@ class TensorboardHandler(http.server.BaseHTTPRequestHandler):
The error is
Exception happened during processing of request from ('127.0.0.1', 53044)
To nikitakit,
``` diff
The Error:
Exception happened during processing of request from ('127.0.0.1', 62162)
self.fileobj.write(chr(flags.encode('latin-1')))
@kibtes
The erroneous line in the traceback seems suspect. On my computer, it is `self.fileobj.write(chr(flags).encode('latin-1'))` (note the different placement of parens). And this is inside the Python installation! Did you manually modify that file, by any chance?
@nikitakit
@nikitakit, yes I'm using Chromium 47.0
Started out with this error
ERROR: /tmp/gbowyer/.cache/bazel/_bazel_gbowyer/d132132edbbae685571ab9488dabc906/external/gemmlowp/BUILD:77:1: C++ compilation of rule '@gemmlowp//:eight_bit_int_gemm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
(cd /tmp/gbowyer/.cache/bazel/_bazel_gbowyer/d132132edbbae685571ab9488dabc906/tensorflow && \
(cd /tmp/gbowyer/.cache/bazel/_bazel_gbowyer/d132132edbbae685571ab9488dabc906/tensorflow && \
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ nvcc --version
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel build --verbose_failures -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --spawn_strategy=standalone
ERROR: /tmp/gbowyer/tensorflow/tensorflow/stream_executor/BUILD:5:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:stream_executor':
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel build --verbose_failures -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --spawn_strategy=standalone
ERROR: /tmp/gbowyer/tensorflow/tensorflow/core/BUILD:272:1: undeclared inclusion(s) in rule '//tensorflow/core:gpu_kernels':
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $
> I posted https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jRkkvsB1iWA.
> What version of bazel do you happen to be using?
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ nvcc --version
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ gcc --version
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel --version
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $ bazel version
(env)gbowyer@compute-10-3-61-179 ~/tensorflow $
@lberki has a fix for this.
@frankyjuang, did u try downgrading bazel to 0.1.1?
So applying the same change to third_party/gpus/crosstool/CROSSTOOL should work, that is
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
When running
When using rmsprop, I get this error:
When using adagrad or momentum, I get this error:
if n.type == "MatMul":
nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled,
Thank you for fixing this :-) However will this also fix the RMSProp error? It seems to be of a slightly different nature...
It is quite possible that I do not fully understand the issue, but I was not expecting RMSProp to be much more difficult to use than Adagrad. I will try to briefly state how I see things (and please excuse me if I write something obviously stupid).
Yes, for Adagrad it is enough to not upgrade the squared gradient sum (and keep track of the global total number of updates).
(d)[nani@nande cat]$ pip show tensorflow
Location: /home/nani/Desktop/cat/d/lib/python2.7/site-packages
(d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/
you mean building from source? I am still building bazel.
(d)[nani@nande cat]$ pip show tensorflow
Location: /home/nani/Desktop/cat/d/lib/python2.7/site-packages
(d)[nani@nande cat]$ ls d/lib/python2.7/site-packages/tensorflow/models/image/
I am experimenting with conv nets for sentiment analysis.
https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/mnist.py which leads nowhere. I'm not sure exactly where this mistake is, but it should definitely be fixed because otherwise people can't follow along with the tensorflow mechanics tutorial.
eddie7@albus:~/lab/tensorflow$ git pull
eddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
eddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -j 1 --spawn_strategy=standalone
eddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel clean
eddie7@albus:~/lab/tensorflow$ git pull
eddie7@albus:~/lab/tensorflow$ ./configure
eddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
it seems issue with bazel rather than tensorflow, as far as I think.
I have never used bazel before too, so can't tell anything
4- compile bazel
'//third_party:protoc' is referring to the protoc in bazel here:
How were you doing this step: "4- compile bazel"? If you were doing just "compile.sh compile", then the embedded tools might not have been created. If you do just "compile.sh" it will do all the steps.
eddie7@albus:~/lab/tensorflow$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
Bazel could be more explicit about it, though.
ERROR: /home/anish/Projects/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_checked_element_behavior//': Error cloning repository: https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.
https://hub.docker.com/r/nitnelave/tensorflow/builds/bbhn9ycakwkbmzyqupkzjbh/
The dependencies are installed from https://hub.docker.com/r/nitnelave/tensorflow-dependencies/builds/bujqarnnsp7hmbetck5a6ug/ , so Bazel 0.2.1
The following piece of code:
op: "Sub"
input: "ResizeBilinear"
type: DT_FLOAT
As a side note, the implementation of l-BFGS-b in scipy is a wrapper for the official FORTRAN implementation:
http://users.iems.northwestern.edu/~nocedal/lbfgsb.html
@yaroslavvb you have to be careful to use an equivalent stopping criterion which is not always easy to do.
I've cross-compiled tensorflow for `armv7l` and generated a wheel successfully however when deploying it into an embedded board with the same architecture (e.g.: Raspberry Pi 2), i get the following when executing https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1%20-%20Introduction/helloworld.py:
erle@erle-brain-2 ~/TensorFlow-Examples/examples/1 - Introduction $ python helloworld.py
Aborted
Digging a bit more:
erle@erle-brain-2 ~/TensorFlow-Examples/examples/1 - Introduction $ gdb -ex r --args python helloworld.py
This GDB was configured as "arm-linux-gnueabihf".
Type "apropos word" to search for commands related to "word"...
Starting program: /usr/bin/python helloworld.py
Program received signal SIGILL, Illegal instruction.
(gdb) bt
ii  libhogweed2:armhf                      2.7.1-5                                   armhf        low level cryptographic library (public-key cryptos)
ii  libpococrypto9                         1.3.6p1-5                                 armhf        C++ Portable Components (POCO) Crypto library
Aborted
From the use case perspective I'd definitely like to see TensorFlow on Pi, at least for the predict function.
For what it's worth, I'm attempting to natively compile TensorFlow on a Raspberry Pi 3 running Raspbian. For the most part, I'm using a modified version of [these instructions for building on a Jetson TK1](http://cudamusing.blogspot.com/2015/11/building-tensorflow-for-jetson-tk1.html), with the main changes being that I'm building Bazel 1.4 instead of 1.0, and I'm not building for Cuda.
Thanks @vmayoral for all the effort you put into this on both the TensorFlow and Bazel front- you got the momentum for this rolling in the first place. Let me know if the instructions/wheel file work for you!
Would you consider adding such a layer at some point?
@girving can this issue be closed?
``` python
Junli
@LeavesBreathe - Ok, that's what I'll do.  Thanks.
Aborted (core dumped)
Having met the following compilation errors:
ERROR: /Users/chaotan/Workspace/deep_learning/tensorflow/tensorflow1/google/protobuf/BUILD:64:1: C++ compilation of rule '//google/protobuf:protobuf' failed: osx_gcc_wrapper.sh failed: error executing command
(cd /private/var/tmp/_bazel_chaotan/58945d877128dff569ec8b54b7c14ab6/tensorflow1 && \
(cd /private/var/tmp/_bazel_chaotan/58945d877128dff569ec8b54b7c14ab6/tensorflow1 && \
Thus I think this issue is not related to compiler, but compiler features flags. The compiler seems to detect that is a lambda expression, but do not expect that macro, thus the error?
which can also be generalized to higher dimensions (not only 3D)
Hmmm I'm getting some very strange results. So just to be sure, the way I'm controlling threads is by invoking a session this way:
``` python
Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
yields the other common error:
Is there a way to fix this without using Bazel?
**ERROR:**  ~/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/external/six_archive/BUILD:1:1: Executing genrule @six_archive//:copy_six failed: bash failed: error executing command
(cd ~/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow && \
@craigcitro -- I've run the build again using the following command: "bazel build -s  --verbose_failures //tensorflow/tools/pip_package:build_pip_package"
Interesting, when I build without the <code>-j 1</code> I get a random error, either from <code>six_archive</code> or from <code>//google/protobuf:python_srcs_genrule</code>.
@craigcitro Thanks for the effort :)
in `<root>/bazel-tensorflow`, or, equivalently, in `/local.mnt/vol1/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow`? My suspicion is that `cp` silently fails for some reason. If that works, try
@severun : Does this work if you add the `--spawn_strategy=standalone --genrule_strategy=standalone` command line options to the Bazel command line?
@severun : do you also have ample disk space under `/local.mnt/vol1/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow`?
How about just with `--genrule_strategy=standalone` then? Also, do you have ample disk space under `/local.mnt/vol1/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow`?
Hehe :) The reason why your SSH connection closes is simple, but also rather surprising (now I wonder why Bazel even prints "exec env" - this is never something you'd want to paste into your shell):
This error message in your log looks very strange:
Something is very different with your Linux machine compared to the ones we test Bazel on :) Which Linux distribution are you using? Is there anything "special" about your machine or Linux setup?
If you are interested in any additional information about this case, please let me know!
Forgive me if Ive missed something obvious - new to Tensorflow / machine learning.
Totally get this is a vastly different prospect for a machine learning tool to inherit the prospect of images not being 'tightly packed' and convoluting what a tensor might be. Just throwing out a concern from a video nerd.
Thanks @ebrevdo, that was it.
I receive the following errors:
Storing complete log in /Users/kanitw/.pip/pip.log
I receive the following errors:
Storing complete log in /Users/kanitw/.pip/pip.log
reading data line 13300000
reading data line 13400000
reading data line 13500000
reading data line 13600000
reading data line 13700000
I haven't seen this before. @philwo Do you know what might be causing this?
Hi @roostapour,
reading data line 16600000
reading data line 16700000
reading data line 16800000
I think it is a memory limitation problem. I set up the TensorFlow on another VM machine with a bit more RAM and ran the translate example again. The error happened after several thousands more lines were processed.
Hamid
### 1- changed google/protobuf/BUILD
// [""] returns bazel error
srcs = glob(["jni/**/*.cc"]),
"-mfpu=neon",
linkopts = ["-llog -landroid -lm -ljnigraphics"],
name = "androidsdk",
api_level = 20,
path = "/hamidb/software/TADP/android-sdk-linux",
api_level=21)
Is this procedure even works for API 19?
Here I put some toy codes to re-generate errors.
( The loss calculation doesn't make sense, It's just for re-generating errors)
There are two problems.
!!! Training Time !!!
e.code)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 605, in _AggregatedGrads
e.code)
``` python
raise ValueError("Tensor rank is different "
@girving @ebrevdo @sjperkins @hugman
@tejaskhot `ravel_multi_index` above should do it
``` python
@tejaskhot Ah rats, the following line:
``` python
@ebrevdo Shiny :-)
Shyamal
import cifar10
File "/home/ooky/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py", line 11, in run
Hi ebrevdo,
-rw-rw-r--. 1 xxxx    6256 Oct  2 15:44 python.o
(cd /home/zhiyunlu/.cache/bazel/_bazel_zhiyunlu/d568262eb4464bf011ab3d998aff21ac/tensorflow && \
@pronobis - Did that work well for you?
I just realized that my test succeeds if I set `num_threads=1` in the `tf.train.batch` call. So the strange behavior seems to originate from there.
____Loading package: @local-jdk//
[<user>@atl4-05 tensorflow]$ ls /lvol/<user>/external/png_archive/
"pngmem.c",
"pngrio.c",
"pngrtran.c",
"pngrutil.c",
"pngwio.c",
"pngwtran.c",
"pngwutil.c",
genrule(
@ludimagister
It has been quite a long time since we have been waiting for the Bidirectional RNN. When will it be made available to public?
@shoaibahmed even though they haven't been officially made public yet, bidirectional RNNs are available in rnn.py, along with the other preliminary RNN architectures.
## Code end
pciBusID 0000:83:00.0
gradients/Mean_grad/floordiv_1: /job:localhost/replica:0/task:0/gpu:0
In _tensorflow/models/image/cifar10/cifar10.py_ it says:
'data_batch_%d.bin' % i)
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
For bazel build, I had the problem
This is what I see with verbose_failures
cc1plus: error: unrecognized command line option "-iquote"
cc1plus: error: unrecognized command line option "-iquote"
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
That is an old Cuda version. Am I correct in thinking that my issues with Bazel are independent of issues with importing tensorflow in python resulting in
@danmane Thanks danmane, seems a problem for my firefox in my virtual machine running ubuntu. On windows from chrome I can see the graph, so not an issue, closing.
@danmane I've been experiencing a similar problem. Is this because only the Chrome browser is compatible with Tensorboard?
@chenghuige see my question above
@danmane I'm using Firefox on Ubuntu linux. The only thing I'm seeing is the 'Events' and 'Histograms'. No 'Images' nor 'Graph'. I don't think I have an option to install Chrome browser on Ubuntu, so I have no idea if it does work on Chrome.
@jaelim I had similar problems until I realized that the summary writer did not actually write the file unless I called the summary writer's flush() function. Maybe explicitly calling the flush() function will sort it out for you too?
Second problem:
Hopefully this is not TMI:
Ata
@ebrevdo I just tested again the CIFAR-10 code on the same machine, and the issue is resolved. Thank you.
I'd like to write Rust bindings for TensorFlow, and I had a few questions.  First of all, is anyone already working on this, and if so, can I lend a hand?  If not, is this something the TensorFlow team would be interested in?  I assume that the TensorFlow team would not be willing to commit right now to supporting Rust, so I thought a separate open source project (with the option to fold into the main project later) would be the way to go.
I would be glad to work on a Rust interface for TensorFlow if nobody is currently working on a solution.
I just launched a project to provide the Rust bindings at https://github.com/google/tensorflow-rust.  The build glue is there, but most of the bindings have yet to be written.
Thanks! I looked it up and it still depends quite much on Python.
(1) SWIG, used in shogun (https://github.com/shogun-toolbox/shogun)
@thirdwing I'm also quite interested in R bindings. I think that mapping directly from the C++ API to R via either SWIG or Rcpp is a necessary but not sufficient condition to create really compelling R bindings. What we really need are idiomatic bindings that take advantage of R formulas, do automatic differentiation, etc. I think it's correct that it would take time and maintenance to do this properly but the result could be really stunning!
@girving Interested in your guidance on the various ways to pursue R bindings. Read your comment here providing the lay of the land for Rust (https://github.com/tensorflow/tensorflow/issues/388#issuecomment-161019498). We obviously can pursue SWIG bindings to the existing C++ classes and then pickup additional functionality from the C++ layer once more of the features from the python layer are moved there.
I've got an R package that has a binding to the TensorFlow C-interface here: https://github.com/jjallaire/TensorFlow
Awkward, but it worked.  Hoping that a better solution gets developed.
Anyways, I would expect that all ops defined in Tensorflow would be available for both CPU and GPU (and any future device). Me and I guess others don't have day to day access to a suitable GPU. Or is that not one of the design goals?
I guess you should ask this questions to @LeavesBreathe :)
Also, what version of numpy and cuda (in particular cuFFT if you know) are
``` python
@drufat unfortunately that's the case -- the current ops are GPU-only.
``` python
@alexhock -- I haven't tried your code myself but I believe you're hitting a common TensorFlow pitfall which is that you're also measuring the time TensorFlow spends constructing the graph and also the C++/Python conversion of the resulting tensor into a numpy array.
``` python
Thanks @rryan for implementing the batched form of the FFT and IFFT. Sorry I have been gone for a while, but I had other responsibilities I had to take care of.
Right now though, it doesn't seem like backprop is supported for complex numbers and I have inquired about it  #here:https://github.com/tensorflow/tensorflow/issues/2255
I have TensorFlow running on Arm 32bit ( Jetson TK1) and it seems to work fine.
http://cudamusing.blogspot.com/2015/11/building-tensorflow-for-jetson-tk1.html
``` python
When I build bazel it shows this ERROR
I try to run bazel fetch//
So could you please provide an alternative (traditional) build system (e.g. make, cmake) or setup.py,
@davidzchen  Actually I'm not sure whether I've encountered a BUG when bootstraping bazel, but anyway I'll try to describe what I've seen while bootstraping bazel, in bazel issue 670.
### Weird stuff
Strangely `--tea=False` is _incorrectly_ interpreted as True.  Having both tea and no tea is only possible in certain Infocom games.
C'mon. It's a boolean flag !
"Say something once.  Why say it again?"  -- _Talking Heads_
I have configured TensorFlow with GPU support using `./configure`. However, during compilation of `dna_encode_op_gpu.cu.cc`, Bazel doesn't find the CUDA headers:
./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:55:18: fatal error: cuda.h: No such file or directory
#include <cuda.h>
``` python
"-x", "cuda",
linkopts = [
"-Wl,-Bsymbolic",
"//third_party/eigen3"
hiro106@hiro106-virtual-machine:~$ python tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py
export PYTHONPATH
``` python
import scipy as sp
import scipy.sparse
``` python
Thus the interest in a differentiable Cholesky :P
https://github.com/GPflow/tensorflow/blob/master/tensorflow/core/user_ops/chol_grad.cc
@vrv we're aware it isn't yet up to TensorFlow main repo standards but would you like to discuss getting it into the main repo.
https://github.com/GPflow/tensorflow/blob/master/tensorflow/python/kernel_tests/cholesky_grad_op_test.py
- cholesky (gpu_cholesky*)
- cholesky_grad (gpu_cholgrad*)
### streams
I use cusolver for the cholesky op. This isn't yet wrapped in Stream.h so I'm linking/calling it raw. This adds a dependency on linking cusolver into the framework, and maybe wrapping it as you guys have wrapped cublas.
I found a small piece of wrong code:
results in jargon, unusable.
@girving I was still learning Tensorflow at the time I filed this so I guess I didn't phrase it properly.
As a reference, following is the TypeError with "::".
TypeError: in method 'PyRecordWriter_WriteRecord', argument 2 of type '::tensorflow::StringPiece'
The contrast is much better now. So the readability.
The math doesn't look right to me:
Segmentation fault: 11
and two bazel command were ran, but still go wrong.
> > >   File "/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/**init**.py", line 4, in <module>
> > >   File "/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/**init**.py", line 22, in <module>
> > >   File "/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py", line 35, in <module>
> > >   File "/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 11, in <module>
> > >   File "/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
> > >   File "/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
> > > ImportError: /home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: ELF file OS ABI invalid
uname -a output：
Un abrazo,
> Hmm :(. All the info online suggests some kind of glibc version skew. You
from third_party/eigen3/unsupported/Eigen/CXX11/Core:14,
from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:14,
ERROR: /home/local/ANT/arpgup/tensorflow/tensorflow/tensorflow.bzl:226:3: C++ compilation of rule '//tensorflow/core:kernels' failed: gcc failed: error executing command
(cd /home/local/ANT/arpgup/.cache/bazel/_bazel_arpgup/1d518f226ad034073e38f75799c6eef6/tensorflow && \
(cd /home/local/ANT/arpgup/.cache/bazel/_bazel_arpgup/1d518f226ad034073e38f75799c6eef6/tensorflow && \
Gathering variables.
(tensorflow)itay@ubuntu:~/tensorflow$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
IOError: [Errno 2] No such file or directory: '/tmp/pip-2LuRsH-build/setup.py'
IOError: [Errno 2] No such file or directory: '/tmp/pip-2LuRsH-build/setup.py'
Command python setup.py egg_info failed with error code 1 in /tmp/pip-2LuRsH-build
X is sparse,w_h, w_o is dense
possibly after transposition.
Either matrix can be transposed on the fly by setting the corresponding flag
``` python
@ebrevdo  thanks! 　I will have a try:)
but X actually should be sparse, converting to dense is not good..
seems there is a bug , that "sum" just work the same as "mean".
Thanks @ebrevdo , but I don't see it.
name: GRID K520
Creating 2 layers of 256 units.
reading data line 100000
reading data line 200000
reading data line 300000
reading data line 15000000
``` python
But then I get another error:
File "/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py", line 4, in <module>
File "/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
File "/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py", line 4, in <module>
File "/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 13, in <module>
1   cc_binary(
@mrry
ll -t libm.so.6
What matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out the next tutorial where we do a lot better, and learn how to build more sophisticated models using TensorFlow!
I have spent some time writing scripts to do stat in Caffe and this let me know Tensorboard is so amazing! It is actually one of the major reasons that makes me switch to tensorflow quickly.
More problems: It is extremely hard to make testing plots in tensorboard. Training plots are OK. However, if I want to make the plots for testing, there will be problems
But how to tell bazel ? I tried
/tmp/tmp.MWtBU7R88K /home/users/chenghuige/other/tensorflow
Exception:
os.makedirs(destsubdir)
Try sudo.
Tried sudo, a different error:
> Exception:
Possible Incorrect Documentation:
``` python
# tensorboard --logdir=./tenIrisSave/   # BAD!
# tensorboard --logdir=$(pwd)/tenIrisSave/  # Good!
e.code)
Caused by op u'gradients/Relu_grad/Relu/CheckNumerics', defined at:
...which was originally created as op u'Relu', defined at:
> be traced back to a weirdly wired model, learning rate issues, bad
``` python
``` python
/home/joao/Dropbox/data analysis/rossman/results/results1/nn_val.dat) and [here](https://dl.dropboxusercontent.com/u/15853805/nn_train.dat).
@vincentvanhoucke You were right. My problem was due to the loss function being RMS. If I use the loss functions you proposed everything works fine. Thank you for the help.
Thanx lightscalar
@lightscalar @Muaazbinsaeed Nice - I was stuck on this for awhile and convinced that my activations/weights were somehow exploding.
thanks @lightscalar
This seems much higher level than what I was thinking of, but still worth taking inspiration from [Tensorflow Playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,2&seed=0.82019&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification).
template<typename T>
Bojan
e.code)
``` python
File "/home/kearnes/git/tensorflow/bazel-bin/deaps/test.runfiles/tensorflow/__init__.py", line 8, in <module>
File "/home/kearnes/git/tensorflow/bazel-bin/deaps/test.runfiles/tensorflow/python/__init__.py", line 24, in <module>
@ebrevdo Any updates on this issue?
and is there a link to zoo?
File "anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py", line 270, in <module>
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py", line 11, in run
File "anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py", line 103, in main
test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')
File "anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py", line 39, in maybe_download
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py", line 98, in urlretrieve
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py", line 245, in retrieve
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py", line 213, in open
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py", line 351, in open_http
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py", line 1207, in getreply
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py", line 1132, in getresponse
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py", line 453, in begin
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py", line 409, in _read_status
File "/Users/kk/anaconda/envs/cinos/lib/python2.7/socket.py", line 480, in readline
"tensorflow.python.framework.errors.OutOfRangeError: Nan in summary histogram for: HistogramSummary"
"tensorflow.python.framework.errors.OutOfRangeError: Nan in summary
Is nan out of range? Why I am getting a OutOfRangeError when I increase
When the camera is connected, YUV_420_888 format is used, Then plane #0 is always Y, plane #1 is always U (Cb), and plane #2 is always V (Cr).
@girving I am still having the same problem: https://goo.gl/photos/RfPszQCKECafxocQA
- Vectors
- Data frames
- Different types:
- Tuples (also series with different types)
- Views (slices with different types)
I think that there is an error in the description of the rank of the tensors.
gg
If the intent is for TensorFlow to be a general purpose computational tool for machine learning there should be more support for special functions (i.e. the functions in scipy.special). In particular, functions occurring in probability distributions such as the gamma(ln) are quite central to probabilistic machine learning.
@ebrevdo hi, I saw you implemented erf, erfc, and lgamma in Eigen. Why not also implement tgamma from the same section of "cmath" header, just so that all four functions from "Error and gamma functions" section would be available? http://en.cppreference.com/w/cpp/header/cmath
@akuz I've been using a hack where you just define a series expansion within the TF graph. It's not pretty and I don't guarantee accuracy, but if you really can't wait for proper support it might be worth a try:
def gammaln(x):
# fast approximate gammaln from paul mineiro
Hello @girving
But this causes an error:
This is the error:
bug confirmed
- http://www.ankhor.com/
- https://www.dadisp.com/
- http://noflojs.org/
But in this case I am looking for something in the like of Flow Sheets (http://www.ankhor.com/). Again there are some project perusing things similar like: http://unisonweb.org/ and perhaps http://www.witheve.com/ depending on how it evolves.
protobuf is not happy. What can I do?
File "/Users/philipteare/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py", line 4, in <module>
File "/Users/philipteare/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 13, in <module>
File "/Users/philipteare/anaconda/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py", line 9, in <module>
I'm on a lower version. Anyone got tips on upgrading. Preferably on anaconda
evolu8 Ltd
LOGCAT:
11-18 12:53:54.234 1947-1969/org.tensorflow.demo D/mali_winsys: new_window_surface returns 0x3000
11-18 12:53:54.568 1947-1998/org.tensorflow.demo D/mali_winsys: new_window_surface returns 0x3000
11-18 12:53:54.867 128-128/? W/debuggerd: type=1400 audit(0.0:100): avc: denied { search } for name="org.tensorflow.demo" dev="mmcblk0p9" ino=97888 scontext=u:r:debuggerd:s0 tcontext=u:object_r:app_data_file:s0 tclass=dir permissive=0
e.code)
Edit: I'm guessing it was called 'logits' by (dodgy?) analogy to the corresponding argument in `sigmoid_cross_entropy_with_logits` ?
New to ubuntu
File "/home/pilotwarela0/Development/tensorflow/tensorflow/**init**.py", line 4, in <module>
File "/home/pilotwarela0/Development/tensorflow/tensorflow/python/**init**.py", line 15, in <module>
export PYTHONPATH
PYTHONPATH="${PYTHONPATH}:/home/pilotwarela0/Development/tensorflow"
export PYTHONPATH
File "/home/pilotwarela0/Development/tensorflow/tensorflow/**init**.py", line 4, in <module>
File "/home/pilotwarela0/Development/tensorflow/tensorflow/python/**init**.py", line 13, in <module>
> > >   File "/home/pilotwarela0/Development/tensorflow/tensorflow/**init**.py", line 4, in <module>
> > >   File "/home/pilotwarela0/Development/tensorflow/tensorflow/python/**init**.py", line 13, in <module>
PYTHONPATH="${PYTHONPATH}:/home/pilotwarela0/Development/tensorflow"
export PYTHONPATH
(cd /private/var/tmp/_bazel_astellato/e1414493f08fd994c717215d0d5d2bb5/tensorflow && \
(cd /private/var/tmp/_bazel_astellato/e1414493f08fd994c717215d0d5d2bb5/tensorflow && \
Nb : If you are using archlinux, you are now using python 3.5 and not 3.4, so you have to rename the package.
@Zenol, could you elaborate a bit more? What needs to be renamed? I seem to be facing this issue.
Sent with Mixmax
``` python
``` python
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:86] kernel driver does not appear to be running on this host (joao): /proc/driver/nvidia/version does not exist
Since #445 is now closed, I'm going to post progress and updates for running TensorFlow on Raspberry Pi here. I did some [very basic benchmarking on the Inception-v3 model](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/tree/master/benchmarks/inceptionv3) to explore whether the build is working properly. Interestingly, the Pi appears to have similar performance to a MacBook Pro when it comes to using a compiled C++ model, but _much_ worse when using the Python version (granted, the C++ version also runs slower on the Mac, which raises a separate set of questions).
I haven't gotten a good look at how Python binds to C++ in the code, but I'll do my best to figure out what's going on. My hope is that there is something screwy on the Pi that can be fixed which would lead to an order-of-magnitude time increase. I don't have much experience with C++ and its compile settings, so any ideas would be much appreciated!
FWIW here's the picture http://imgur.com/UbsYZYC (upside down and truncated salt/pepperpots, so 'hourglass' was close ;)
I'm also curious about the Python wrapper being slower than plain C++ since I thought Python offloaded all TF's heavy lifting to compiled code anyway. Is it possible it's running a different build of the TF core in one case?
Thanks @danbri- I'm definitely going to take a closer look at the Python and C++ Inception code to see if there are any obvious differences in implementation between the two. I ran your image through my RPi directly in Raspbian and got the same results as I did with smaller images. My gut tells me that the Pi can do better that the performance we're getting, but I'm not sure if I should be looking at compiler options first or dive into the code first.
> ImportError: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
`export LD_LIBRARY_PATH=/usr/lib/gcc/arm-linux-gnueabihf/4.8/:$LD_LIBRARY_PATH`
`export LD_LIBRARY_PATH=/usr/lib/arm-linux-gnueabihf/:$LD_LIBRARY_PATH`
I'm building custom classifiers one basic perceptron type model and a CNN and I can say the odroid runs 5x slower with the perceptron case (which is still plenty fast actually) and the CNN actually runs at about the same rate on both machines (really slow). When the CNN does run on the Odroid it eats between 4 and 7 of the 8 available cores and consumes ~1.9 GB of the 2 GB of RAM. I still have the swap partition turned on from the install and from what I've seen that's probably a good thing to have when running big CNNs like that. All in all pleasantly surprised by the performance so far.
@austinsteamboat Thanks for the info! I just finished putting the 0.8.0 pip release out for the Raspberry Pi, so I'm hoping to have a few spare minutes over the next few weeks to check out some of the speed issues. From your info, it sounds like the problem could be at the operating system level, as Raspbian is still a 32-bit OS. The Odroid is certainly a more powerful machine than the Pi, but if the ImageNet model only takes 4 cores on both, I don't see the Pi lagging behind that badly.
@NickQian Glad the TensorFlow binaries are working for you. Couple quick responses:
@indiangolfer It's on my to-do list! I've streamlined my compilation process a touch, so I'll try to get the Python 3 binaries out before the end of the weekend.
@indiangolfer Let me know if [this binary](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/tag/v0.8.0) works for you! I just updated the readme and guide to include Python 3 installation instructions as well.
Great work @samjabrahams.
Hi @indiangolfer, could you please tell how did you install binary on RPi2B?
I follow below steps on RPi2B with Raspbian Wheezy:
python
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
ImportError: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so)
Example of showing how to evaluate using auc instead of  precision is greatly appreciated.
aurora@aurora-Z170X-UD5:~/workspace/tensorflow/tensorflow$ bazel build --jobs 2 -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures
BTW, the truth that Bazel do not recongnize the global env-vars was about to KILL me.
@ebrevdo  the env vars
XDG_VTNR=7
LC_PAPER=zh_CN.UTF-8
LC_ADDRESS=zh_CN.UTF-8
XDG_GREETER_DATA_DIR=/var/lib/lightdm-data/aurora
SELINUX_INIT=YES
LC_MONETARY=zh_CN.UTF-8
LC_NUMERIC=zh_CN.UTF-8
USER=aurora
LC_TELEPHONE=zh_CN.UTF-8
LC_IDENTIFICATION=zh_CN.UTF-8
PWD=/home/aurora/Desktop
JOB=dbus
LC_MEASUREMENT=zh_CN.UTF-8
HOME=/home/aurora
LOGNAME=aurora
UPSTART_JOB=startxfce4
GLADE_CATALOG_PATH=:
TEXTDOMAINDIR=/usr/share/locale/
LC_NAME=zh_CN.UTF-8
XAUTHORITY=/home/aurora/.Xauthority
aurora@aurora-Z170X-UD5:~/workspace/tensorflow/tensorflow$ echo $PATH
I don't know how to use bazel . Is there anything wrong in my operation.
@SyncedSynapse
I'm using anaconda which contains numpy.
``` python
@khellan Thanks!
:+1:  danke
Changing this:
Floating point exception (core dumped)
``` python
leading to an obvious NameError:
which doesn't work because it's missing '--'  before arguments
sessi.run(init,init)
Initialized!
Am I running without GPUs or GPU??
there is undefined name "state" in the example of Variables section. I think it should be "var" but I am new to tensorflow. Please confirm and update the doc so that others don't get confused.
Sent with Mixmax
than tensorflow itself.
import mnist
--> 419                                               e.code)
Wow!  Tensorboard is so amazing.
Jetson  TK1 has a limited amount of memory ( CPU+GPU <2GB) so most of the examples will not run without modifications but aside from  Out Of Memory errors, TensorFlow runs just fine.
For the following code:
import scipy as sp
time elapsed scipy:      0:00:01.613825
**1.6 sec (Scipy)  vs. ~18 sec (tensorflow), that's a huge difference.**
- Is this expected behavior?
time elapsed scipy:      0:00:07.803832
time elapsed scipy:      0:00:00.913324
time elapsed scipy:      0:00:00.844610
@Yangqing do you guys happen to have example WORKSPACE and BUILD files for using TensorFlow in an Android app that is separately managed (i.e. the Android project is not located inside the TensorFlow project like your example is)?  bazel has a huge learning curve to incorporate a large dependency like TensorFlow and I'm not crazy about manually copying over the .h and .so files.
Hi renats,
| Error | Error meaning |
| Error | Error meaning | Count |
| E116 | unexpected indentation (comment) | 1 |
| E251 | unexpected spaces around keyword / parameter equals | 15 |
@evanthebouncy, see #654
@ebrevdo Can you provide a pointer to how to do beam search using the C++ API?
Just my 2 cents, I starred at this for much longer than I should have because of this confusion.
e.code)
model_name=FLAGS.model_one_scope)
model_name=FLAGS.model_two_scope)
Any thoughts on this? I looked through the embedding_rnn_seq2seq function and it looks like it builds from get_variables and variable_scope throughout. Even though there's a reuse_variables in, for example, the "RNN" scope, I would think that this would be in a different domain given that the top level scope is different ("en-10000-256-2-10000" vs "de-10000-256-2-10000")
And for the J_pq it looks like this:
Just in case anyone is using this code, theres a typo. It should be:
Yan Lecun''s web page is not available - cannot download data from there is there a mirror?
The port on the address 'yann.lecun.com' is not listening. Either someone stopped the webserver (unlikely) or it died because of overload (much more likely). After more research I have found that no known port is responding and it might very well have been the whole server machine which have gone down. The domain is owned by 'PERFECT PRIVACY LLC' and reverse IP domain check reports that only one domain is hosted at the same location.
Personally, to me this sounds like it is some ones private server and I think that the data located there used by TensorFlow should be copied or transferred over to a more public mirror.
- Damian
> The port on the address 'yann.lecun.com' is not listening. Either someone stopped the webserver (unlikely) or it died because of overload (much more likely). After more research I have found that no known port is responding and it might very well have been the whole server machine which have gone down. The domain is owned by 'PERFECT PRIVACY LLC' and reverse IP domain check reports that only one domain is hosted at the same location.
~Foorack
- Damian
> ~Foorack
@jeremija Actually wayback-machine should work! Tell me how it went if you decide to try it. :)
Sigurd
Can you please verify the code ?
(I know it's much dirty!)
You can see [here](https://github.com/rdipietro/miccai-2016-surgical-activity-rec/blob/master/train_and_eval.ipynb) for a more complex example, with a batch size > 1. But you'll need to either a) write not-so-clean code that's efficient (as they do in TensorFlow officially) or b) write clean code that is less efficient (this is what I do; I just wrap shorter sequences in time until all sequences are the same length, which simultaneously makes short sequences "count" just as much loss wise as long sequences).
bias_op_gpu.cu.cc
conv_ops_gpu_matmul.cu.cc
matmul_op_gpu.cu.cc
relu_op_gpu.cu.cc
xent_op_gpu.cu.cc
@ebrevdo What is the status on this issue?
indicies = tf.concat(1, [
``` python
@waleedka I adapted @ebrevdo's example to work with an additional dimension for the output neurons of an RNN. This should yield the last relevant output activations while preserving the shape information.
``` python
@danijar this is a working solution, but when I tried it, I got the following warning from tensorflow:
Is this implemented yet by any chance @ebrevdo?
``` python
@girving I don't know if this is a reasonable path or not (I may be missing the point) but I wonder if borrowing ideas from SSA https://en.wikipedia.org/wiki/Static_single_assignment_form for manipulating the computation graph in cases where there is a side effect and no return value is a way to achieve this?
@girving I agree that NumPy's vectorized indexing is great. The problem is that NumPy's implementation includes a hacks to make it "more intuitive" when slices and arrays are mixed, which work a lot of the time but then result in some very bizarre edge cases.
``` python
Docker has tricky GPU support as well as CPU resource allocation that can lead to poor outcomes.
@ebrevdo [To me](http://talk.cregox.com/t/my-social-unfair-undervaluation-and-why-the-financial-system-is-broken/7751?u=cregox), you're just saying you will probably look at this back again in the future, but can't predict when! ;P
I think what **cesarsalgado** mentioned is correct.
--verbose_failures doesn't change the output.
the error is
# which javac
It was an incomplete bazel installation.
Now I am getting thee errors:
@xubenben  Not yet.
bogon:~ ra$ python
`Unrecognized option: --data_dir`
PATH=.:/usr/local/jdk/bin:/usr/local/jdk/jre/bin:/home/share/TensorFlow/bazel/output:/home/xuezhisd/w
D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' '-frandom-seed=bazel-
bazel-
PATH=.:/usr/local/jdk/bin:/usr/local/jdk/jre/bin:/home/share/TensorFlow/bazel/output:/home/xuezhisd/w
D__TIMESTAMP__="redacted"' '-D__TIME__="redacted"' '-frandom-seed=bazel-
bazel-
This happened to me when I tried to use a different version of bazel than 0.1.0
I used bazel-master.
🍃  Building Bazel from scratch............
🍃  Building Bazel with Bazel.
[root@/home/share/backup/tensorflow]#uname -a
print "c_shape after dropout:", c.get_shape()  # prints TensorShape(None) which is strange
print "c2_shape:", c2.get_shape()  # prints TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(16)]) which is strange
bazel-out/local_darwin-opt/bin/tensorflow/python/pywrap_tensorflow.cc:3609:10: fatal error: 'numpy/arrayobject.h' file not found
Works great for first step. Waiting for next two to execute. Thanks ebrevdo!
> > >   File "/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/**init**.py", line 4, in <module>
> > >   File "/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py", line 35, in <module>
> > >   File "/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 11, in <module>
> > >   File "/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
> > >   File "/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
In the RNN example tutorial, we're compiling the python script with bazel before executing it, instead of executing the script directory as in the simple MNIST example. Is it possible to do the RNN example purely in python script instead of using bazel? I used a binary installation instead of cloning the repo, so many of the file structures are different. I tried to hack it so I can run it from script instead of compiling with bazel, but I'm getting this error:
Or is learning bazel the only way (which I don't mind but would like to avoid)?
sorry I don't code much at all, this is fairly challenging.
e.code)
I see that you're trying to apply a trick here to avoid overflow, but it should be
I see, you're assuming your targets to be 0/1, where as usually, people are using -1/+1 labels when speaking of the logistic loss.
It would be nice for Tensor Flow to have a native log-sum-exp, that'll make things faster, and help people have less numerical issues.
``` python
There is an error:
``` python
I am not a fan of Python but an following: https://github.com/Maratyszcza/PeachPy is an interesting approach used in http://www.yeppp.info/
File "/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py", line 4, in <module>
File "/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 22, in <module>
File "/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py", line 35, in <module>
File "/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 11, in <module>
File "/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
ImportError: dlopen(/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 2): Library not loaded: /usr/lib/libc++.1.dylib
Referenced from: /Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
pciBusID 0000:09:00.0
|    1     32740    C   python                                         209MiB |
|    3      9429    C   python                                         336MiB |
@nouiz, you are correct in your analysis. TensorFlow tries to grab all the GPU it sees from the system that passes its criteria. CUDA_VISIBLE_DEVICES is the solution I would suggest. Please let us know if that is not enough.
@zheng-xq For 1: AFAIK, the only responsibility of most job schedulers is to ensure that you will have access to the number of GPUs you have asked for on the machine which has been assigned to you. It leaves the actual CPU/GPU scheduling of the processes on that machine to the OS/Cuda driver. Also, hypothetically, if the scheduler was to attempt something of this sort, it would not be able to block the GPU without creating something like a context.
Please, support 3D convnets. Thanks for help.
@el3ment sure!, @daeyun I am wondering though, if I define the operation like in #2467 , if I use the function as a layer, will the backward pass do something? (since we are actually using the backward pass of the convolution as forwards pass for the "deconvolution" )
@rogertrullo I misunderstood, although for 2D, conv2d_transpose is actually `conv2d_backprop_input`, it looks like its 3d counterpart is incomplete and the backward pass of conv3d_backprop_input is not implemented. I haven't gotten around it either. (Edited)
As a newcomer to docker myself, I'm struggling quite a lot to get off the ground here, even after studying it and better understanding it quite well. It isn't as simple or easy as it may seem at first - but it could and should be!
Also what are the origins of the `b.gcr.io/tensorflow/tensorflow` base image?
I'm not clear on how it was built:
tensorflow                                gpu                    1ca458346ab2        16 hours ago        4.847 GB
cuda                                      7.0                    f8abb195d52b        21 hours ago        2.012 GB
Hemanths-MBP:tensorflow hemanthreganti$ (tensorflow)$ pip install --upgrade <$url_to_binary.whl>
Hemanths-MBP:tensorflow hemanthreganti$  pip install --upgrade <$url_to_binary.whl>
Hemanths-MBP:tensorflow hemanthreganti$ pip install --upgrade <$url_to_binary.whl>
Hemanths-MBP:tensorflow hemanthreganti$  source bin/activate
(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ pip install --upgrade <$url_to_binary.whl>
(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ (tensorflow)$ pip install --upgrade <$url_to_binary.whl>
(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ (tensorflow)$ python tensorflow/models/image/mnist/convolutional.py
(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ python tensorflow/models/image/mnist/convolutional.py
(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$
Hemanths-MBP:tensorflow hemanthreganti$ sudo pip install --upgrade virtualenv
Password:
The directory '/Users/hemanthreganti/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/hemanthreganti/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Thanks ebrevdo. I am unable to find any link on the page http://tensorflow.org/get_started/os_setup.md .Would you be able to provide me with the right link.
In order to use GPU in training you need to specify --config=cuda:
Would love to be wrong.
Hi. I am a newbie on TensorFlow, just like most of the others here.
-Taeksoo
calling `nn.moments` with an axis of dimension `None` produces this error:
I'm using this custom function right now:
``` python
divisor, name="variance")
ubuntu@slave1:/media/slave1temp/tensorflow_full$ python tensorflow/tensorflow/models/image/mnist/convolutional.py
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
e.code)
ubuntu@slave1:/media/slave1temp/tensorflow_full$
``` python
``` python
@ywatanabex ,  if it may help you, using Docker stats I saw memory peak around 1.9GB
Anything unusual about it taking 6 hours or is it normal since I am using CPU and not the recommended GPU for DNNs?
Storing complete log in /home/shubhanshu/.pip/pip.log
IOError: [Errno 13] Permission denied: '/home/shubhanshu/.pip/pip.log'
Unpacking /home/ayan/Desktop/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Running setup.py egg_info for package from file:///home/ayan/Desktop/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Storing complete log in /home/ayan/.pip/pip.log
That's kind of odd, since I got it to work in Python 2.7.3. (And it fixed [my version of the bug](https://github.com/tensorflow/tensorflow/issues/46) )
If we're in for the openess and effectiveness of the whole thing...
I noticed a large number of compile-time warnings while compiling TensorFlow on Ubuntu 15.04.
ong int>, 1>]':
:complex<float>, 1ul, 1, long int>, 1>]':
i don't know what is wrong.
Red Hat Enterprise Linux Server release 5.5 (Tikanga)
File "/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
The errors are   1)Unable to load cuBLAS DSO,2)Unable to load cuDNN DSO,3)Unable to load cuFFT DSO,4)Unable to load cuRAND DSO.
@mtanana  I tried your solution. It seems to be working, but there is some warmings, such as
I have this same problem trying to install Tensor Flow from source on a CentOS 6.7 machine. I've tried with both bazel 0.1.1 and 0.1.3. With 0.1.1, I get:
``` python
``` Python
$ caffe time -model alexnet_dummydata_nolrn.prototxt -gpu 0
fi
## Bazel
- Comment out `atexit "rm -fr ${DIR}"`
4. `export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'`
5. `export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'`
- Why the strange flags? Because otherwise, after building with the older `libc`, we'll get an error about `secure_getenv`.
- mkdir glibc
``` python
linkopts = [
I'm more looking at Tensorflow as a shared library to make it more code / platform agnostic to talk to (my platform of choice being PHP/HHVM). There's already bindings for Go here: https://github.com/chai2010/tensorflow which seems similar to your ideas @jimfleming
@jimfleming  have you gotten a shared library working?  I tried @aliasaila's suggestion and also got seg fault when trying to run my program.  When compiling with the cc_library option, I do get a .so file, but it is very small, and when compiling I have undefined references.  So  I know something isn't set up correctly. thanks
@nbenhaim Tensorflow:core has some dependencies set as libstatic in the Bazel build file (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD) which would explain why you're missing references (http://bazel.io/docs/be/c-cpp.html) :
@nbenhaim Initially I was following the label_image example and using tensorflow::ops::ReadFile to load the image file. Unfortunately I couldn't pinpoint the issue (the debug version of the shared lib was too big to load). To bypass the issue I added code to load and populate the input Tensor outside of TensorFlow, and that was successful.
Sure, I can do that. It'll be a good exercise in learning the contribution process anyways. Yah, it's the "can" that was unclear, "must" would be better.
Exception information:
[bazel release head (@125b349)]
bazel help <command>
bazel help info-keys
I'll cross post this issue to `bazel`.
$ bazel version
File "/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
U __isnanf@@GLIBC_2.2.5
How to fix the issue `__isnanf: symbol not found`
> tensorflow.python.platform.default._gfile.FileError: [Errno 2] No such file or directory: 'data/giga-fren.release2.ids40000.en'
@rameshdom  thanks.
🍃  Building Bazel from scratch............
🍃  Building Bazel with Bazel.
Exception:
shutil.move(old, new)
On  0, pannous notifications@github.com wrote:
On  0, pannous notifications@github.com wrote:
On  0, pannous notifications@github.com wrote:
This is the output of the uname -a
haejongs@sclasic02:~/Downloads$ uname -a
I have upgraded my system,here is the log when i  execute:  `uname -a`
Thanks @ebrevdo and @vrv :-)
Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,
Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,
Abhishek
@abhishekpatnia Thanks! let me give it a try. Not sure how this will affect performance :(
Is there being work done to support these ops on GPU? Would be happy to chip in if individual efforts are being assigned and tracked somewhere.
I also wonder if there are "finer grained issues"? Just to be able to follow the progress...
there is error
then installing the python wheel again.
compiling all deps with gulp vulcanize throws an error however:
[07:29:53] Starting 'vulcanize'...
[07:29:53] Finished 'vulcanize' after 6.08 ms
@danmane ive been testing the tensorboard release quite a bit and it seems to be all working well. events, images, graph and histogram all good! :)
Thanks @danmane that worked. :)
Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
(gdb) r
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Program received signal SIGABRT, Aborted.
(gdb) bt
name = "androidsdk",
api_level = 22,
api_level=21)
sangpil_kim@ ~/git/tensorflow $ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
Is this the right place to report typo?
third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorBase.h:703:5: note: the ABI of passing structure with complex float member has changed in GCC 4.4
shanker@blackhole ~/projects/tensorflow $
Exception:
File "/Users/sangpil_kim/tensorflow/lib/python2.7/site-packages/pip/basecommand.py", line 211, in main
File "/Users/sangpil_kim/tensorflow/lib/python2.7/site-packages/pip/commands/install.py", line 305, in run
File "/Users/sangpil_kim/tensorflow/lib/python2.7/site-packages/pip/wheel.py", line 705, in build
File "/Users/sangpil_kim/tensorflow/lib/python2.7/site-packages/pip/download.py", line 373, in request
I don't know why but it worked
This statement is a little confusing ...
File "/Users/alexryan/projects/machineLearning/tensorFlow/clone/tensorflow/tensorflow/python/**init**.py", line 13, in <module>
import mnist
"(tensorflow)unknownb8e85638de86:tensorflow jonathanmalkin$ pip install --upgrade <$url_to_binary.whl>"
Name: numpy
Name: protobuf
(tensorflow)yiliu@yi-2014:~$ protoc --version
Please, add support of cuda 7.5 and cudnn 7.0.
Does it mean it does NOT support Cudnn 7.0?
Shen
Upgrading ubuntu is definitely the best way to go. But if not...
I would not upgrade the system libc.so on the shared cluster. It should be backward compatible but there are regression here and there. You can still make it work if you want to. If you grep libc from newer package [http://packages.ubuntu.com/trusty/libc6](click on amd64). You can extract that deb. Then you can use LD_PRELOAD the files from content directory. See something like [https://rafalcieslak.wordpress.com/2013/04/02/dynamic-linker-tricks-using-ld_preload-to-cheat-inject-features-and-investigate-programs/] or [http://stackoverflow.com/questions/5223971/question-about-overriding-c-standard-library-functions-and-how-to-link-everythin].
ERROR: ld.so: object '/home/hpc/pr63so/ga93yih2/lib/libc.so.6' from LD_PRELOAD cannot be preloaded: ignored.
Name        : glibc
Arch        : i686
Segmentation fault
Add '-lrt' to link opts
change
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/setuptools/dist.py", line 257, in finalize_options
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 2029, in require
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 580, in resolve
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 825, in best_match
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 837, in obtain
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/setuptools/dist.py", line 272, in fetch_build_egg
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/setuptools/dist.py", line 225, in __init__
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/setuptools/dist.py", line 257, in finalize_options
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 2029, in require
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 579, in resolve
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 748, in __init__
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 777, in scan
File "/home/phong/.local/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/pkg_resources.py", line 1702, in find_distributions
(tensorflow)$ uname -a
2015-11-12 11:39 GMT+01:00 Babak Gh notifications@github.com:
manu @ korfmann . info
sungjinkim@fas.harvard.edu
sungjinkim@fas.harvard.edu
On Mon, Jan 25, 2016 at 8:31 PM, Ahn, Soohan notifications@github.com
How about using RICE?
> Interested, but is there a technical reason it needs to be SWIG? I can
scientific computing, but rubyists are great innovators and have great
innovations. I want ruby community to be able to participate.  Please feel
hi jtoy
> innovations. I want ruby community to be able to participate. Please feel
sungjinkim@fas.harvard.edu
[Linkedin] https://www.linkedin.com/in/jamessungjinkim
[Facebook] https://www.facebook.com/jamessungjin.kim
[alternative email] jamessungjin.kim@gmail.com
sungjinkim@fas.harvard.edu
[Linkedin] https://www.linkedin.com/in/jamessungjinkim
[Facebook] https://www.facebook.com/jamessungjin.kim
[alternative email] jamessungjin.kim@gmail.com
SciRuby. It is undergoing the selection process, and the results will be
sungjinkim@fas.harvard.edu
[Linkedin] https://www.linkedin.com/in/jamessungjinkim
[Facebook] https://www.facebook.com/jamessungjin.kim
[alternative email] jamessungjin.kim@gmail.com
It's amazing to know that you are employing Tensorflow for real life
Using sklearn is an exercise in Udacity's Deep Learning course. I think it
> sungjinkim@fas.harvard.edu
> [Linkedin] https://www.linkedin.com/in/jamessungjinkim
> [Facebook] https://www.facebook.com/jamessungjin.kim
> [alternative email] jamessungjin.kim@gmail.com
> It's amazing to know that you are employing Tensorflow for real life
> Using sklearn is an exercise in Udacity's Deep Learning course. I think it
> > sungjinkim@fas.harvard.edu
> > [Linkedin] https://www.linkedin.com/in/jamessungjinkim
> > [Facebook] https://www.facebook.com/jamessungjin.kim
> > [alternative email] jamessungjin.kim@gmail.com
sungjinkim@fas.harvard.edu
[Linkedin] https://www.linkedin.com/in/jamessungjinkim
[Facebook] https://www.facebook.com/jamessungjin.kim
[alternative email] jamessungjin.kim@gmail.com
@nethsix judging from https://summerofcode.withgoogle.com/organizations/5131000665341952/ it seems that the TensorFlow Ruby API project has not been accepted in GSoC? Am I right?
That's so sad =(
Hey Akshay,
mentors to track progress accurately, and ensure students stick to their
Tensorflow version, which may seem simple, but it acts a good foundation to
khor
> Hey Akshay,
> khor
> > Hey Akshay,
> > khor
@jtoy This seems like a wonderful opportunity and i would like to work on this.  I had applied to Gsoc this year for Supervised machine learning project in sciruby. I was close to being selected but the slots were limited so didn't make it.
It appears to me that this is a challenging task, but if this could be made a sponsored project and a member of sciruby or Tensor flow could mentor this. Then I would be more than happy to work on this.
As for this project, I think the sciruby community would be very welcoming to this idea. And if some mentors are free they could even provide some valuable insights for this project.
Hey Arafat, @jtoy,
Arafat, I was talking to Sameer (one of the SciRuby admin) about this
> As for this project, I think the sciruby community would be very welcoming
Do spread the word!
crowdai.org, is written in Ruby :-)
> people can get in touch by mailing me at sameer.deshmukh93@gmail.com.
Get in touch with @Arafatk
Is it a protobuf version problem ?
Sid
http://blaze.pydata.org/
I don't have any specific use cases in mind (maybe to efface c++ use?), but just wanted to make the team aware.  As a pydata user, I would love to see any  cooperation and synergy between all these amazing projects in the ecosystem.
So I did some digging in the documentation. It would be convenient to be able to write a new op with Numba.
Command python setup.py egg_info failed with error code 1 in /var/folders/gQ/gQ784XlzFBu-AAhgOvz4Ek+++TI/-Tmp-/pip-emt5Bl-build
Storing complete log in /Users/munafo/.pip/pip.log
(tensorflow) $ ls -l /var/folders/gQ/gQ784XlzFBu-AAhgOvz4Ek+++TI/-Tmp-/pip-emt5Bl-build/tensorflow/tools/pip_package/setup.py
-rw-r--r--  1 munafo  staff  2574 Nov  9 17:41 /var/folders/gQ/gQ784XlzFBu-AAhgOvz4Ek+++TI/-Tmp-/pip-emt5Bl-build/tensorflow/tools/pip_package/setup.py
Searching for pip
Writing /tmp/easy_install-LyZnJ8/pip-7.1.2/setup.cfg
Exception:
shutil.move(old, new)
@ne0shell
Exception:
shutil.move(old, new)
Exception:
shutil.move(old, new)
> > >   File "/home/jxie/anaconda/lib/python2.7/site-packages/tensorflow/**init**.py", line 4, in <module>
I just learned about TensorFlow and I want to install on Windows 7, 64.  I have both Python 2.7 and 3.5  on my PC.
I rather use PIP but when I try :
An error occurred trying to connect: Post
made bec
@aebk2015 is that with the docker? I forgot another step, you also need to create a docker vm after installing the docker toolbox, the commands are:
@jgharris7 run as admin
Also, any tips for setting up the files from https://github.com/anishathalye/neural-style Tensorflow/Python implementation so as to run on vdocker via Python commands or better yet ipython notebook commands? Thanks again for the docker install!
@iaroslav-ai , thanks for the update!
@anj1 Exactly. Thanks. You still need the tunnel, though. Since Docker usually needs to have the same operating system on the Docker host system as it's in the Docker container (well, parts that is) you need a Linux VM to run a Docker container that is based on Linux under Windows. Docker itself opens port 8888 between the VM and the container. You have to explicitly tell it to forward the port from the outside of the VM to the open Docker container port by using said parameter `-p 8888:8888`.
Thanks a lot @SeveQ  and @anj1. It worked. Now tensorflow is running on my Windows 10.
@sarahharun You're welcome!
docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow
@sarahharun  Thanks a lot, now I can run tensorflow at http://192.168.99.101:8888/notebooks/. But I still can not run at docker terminal or Windows CMD, is that normal?
**@sarahharun**  Hi i am having problem with running tensor flow in command prompt, i followed all your four point that u said to **@luling2010**, im still not able to get Linux shell to write python code, and execute tensorflow
@sarahharun hi supporse if i want to run following code, how do i do it in jupyter
Weird... what does the output of the `jupyter notebook` command at the command line say?
That's really weird... I'm sorry, I can't help you much more with this as I can't tell why your kernel isn't coming up... I don't see anything in the message log that might point towards where the problem is...
@SeveQ Thank you very much! As you said, use cmd _docker run -it -p 8888:8888 -p 6006:6006 b.gcr.io/tensorflow/tensorflow_ and it works now.
@sarahharun  Hi, I found a problem, when I try to restart my computer, I can not connect to http://192.168.99.101:8888   do you have the same problem and how to fix it.
I am using `docker  -p 8888:8888 run -it b.gcr.io/tensorflow/tensorflow` to run it.
ons: )
@helenahan2015 Hi , I use the command: docker run -it -p 8888:8888 -p 6006:6006 b.gcr.io/tensorflow/tensorflow
@iamdrink2009 Sorry for this very late reply. Here is your VM ip. But I'm not very familiar with docker so I don't know how to set either. I suggested you to use Chrome and I failed with IE or Edge to open tensorboard before.
@SeveQ Hii . I use the command: docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow but it says that port 0.0.0..0:8888 already allocated. what now should I do.plz help
@HWiese1980  hii.  plz help me
@HWiese1980 Thanx for quick reply . I choose that port as you said but now that it give me warning again
@Saleem44 you can safely ignore that warning. It just states that the notebook accepts incoming connection on all ports. That **could** be a security issue in case the host of the notebook session (the docker container in this case) was connected directly to the internet. That shouldn't be the case under normal circumstances.
@HWiese1980 then how can I use tensorflow because I can't be able to import tensorflow
@Hwiese1980 no its not open .Firefox can't establish a connection to the server at localhost:5776
@Saleem44 Huh... this is a tough one... I can't test it myself here because I'm on a metered connection for the time being and don't have the docker container on my PC anymore due to a complete Windows reinstall...
@hwiese1980 I stop notebook and rerun the command ...-p 8888:8888... , now it doesn't say that port is already allocated. But it still not open notebook on http://192.168.99.101:8888 or on localhost:8888
@HWiese1980 is there any solution
docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow
But today, when I run the command 'docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow', I got this error: docker: An error occurred trying to connect: Post http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.23/containers/create: open //./pipe/docker_engine: The system cannot find the file specified.._*
I run python:
2. Installed Sublime Text
Hi HWiese1980,
Wali
$ docker run -it -p 8888:8888 -p 6006:6006 b.gcr.io/tensorflow/tensorflow
_**The code part of the code**_
Gland I can help, this is an amazing project :)
Not really sure how this works. Do we need to write swig interface file specifically for Javascript or is it auto-generated when running some commands or is somebody already working on this (this would be awesome) ?
@tngan The slack channel is private, however I was able to join with the herokuapp link. :+1:
@nikhilk Thanks. Something like `new tf.Tensor()` instead of `tf.NewTensor()` might be a nice addition but I'm not planning on expanding it at the moment. I'm only interested in loading graphs created in python and I think I like the minimalism.
If anyone has experience with SWIG, I'd love to collaborate, as it seems like a huge amount of the python SWIG interfaces are custom overrides etc. and I'm keen not to reproduce their work. Additionally, would be great to get some clarity from the tensorflow team on what API's would be good to initially cover as I'm sure their roadmap has many changes on the way, and I wouldn't want to conflict. (cc @martinwicke ? )
Mon Nov 9 15:34:49 EST 2015 : === Using tmpdir: /tmp/tmp.itHpiLkuJI
/tmp/tmp.itHpiLkuJI ~/packages/tensorflow
Got this as an issue while trying to run TensorBoard on El Captain.
To answer @danmane
Sidenote: I'm behind corporate firewall, I use --insecure-registry extra args to get around certificate issues and using hyper-v for boot2docker...
Maad
@craigcitro  nvm it was a proxy/firewall issue... Thank you for the quick reply though :)
Exception information:
``` python
what's the timeline now @ebrevdo
Hi Ebrevdo,
@ebrevdo any progress? :)
@eleijonmarck @halilakin
@eleijonmarck ^
@sdgandhi @eleijonmarck and anyone else interested in chatting.
Using Matrix clients such as http://vector.im provide most all of the user experience goodness of Slack in my experience.
adding a Go port :smile:
name: GRID K520
/cc @lucamilanesio for the plugin
@markusdr, this is very strange. Could you post the completely steps you build the binary?
ls -lah /usr/local/cuda/lib64/libcublas.so ?
@vsrikarunyan, it is better to use CUDA Toolkit 7.0, as recommended. You can install an older CUDA Toolkit along with your newer toolkit. Just point TensorFlow "configure" and maybe LD_LIBRARY_PATH to the CUDA 7.0 when you run TensorFlow.
@nbenhaim, just what did you have to do to get it to work?
@markusdr, @jbencook, the NAN is quite troubling. I ran the same thing myself, and didn't have any problem.
Setting up Cuda include
Setting up Cuda bin
Setting up Cuda nvvm
@nbenhaim @markusdr
I can confirm that with @erikbern's install script and the latest TensorFlow master branch the `cifar10_multi_gpu_train.py` works as expected on the GPU:
Warp size:                                     32
|    0     60160    C   python                                        3819MiB |
|    1     60160    C   python                                        3783MiB |
|    2     60160    C   python                                        3783MiB |
|    3     60160    C   python                                        3783MiB |
@mhejrati according to a comment on https://news.ycombinator.com/item?id=10555692 it seems like you can't do it in AWS:
@erikbern @mhejrati I'm not so sure that specific property of Xen is a problem. P2P copies don't seem to be necessary as the cpu can still assign work to each GPU without GPUs needing to communicate to each other. It's still strange that all GPUs on the instance seem to be in this semi-utilized state but work proceeds without error.
JFYI, I'm getting the following error when trying to execute the models/image/mnist/convolutional.py example (Python 2.7.10 in PyEnv, on Xubuntu 14.04, 64bit):
File "/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/__init__.py", line 4, in <module>
File "/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 22, in <module>
File "/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py", line 35, in <module>
File "/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 11, in <module>
File "/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
I'm seeing what might be a related error:
jvanderdoes@ubuntu:~/Code/tensorflow/tensorflow/models/image/imagenet$ python ./classify_image.py
My apologies, I was simply missing cudnn. :/
-Minjie
@saudet /cc @ctn
TensorFlow-serving released today, it seems networking in gRPC has been proved to be a mature solution, great news!
It appears there were two parallel approaches here:  (a) SparkNet  and (b) gRpc using C++. Is that correct?  And both are also currently active?
@javadba sorry for the confusion, with "small networks" I mean the deep neural networks can not have too many model parameters, otherwise the communication time between machines will exceed the computation time, which means speedup is not high.
Pardon my ignorance but did anyone succeeded running Tensorflow on multiple machines?
``` python
``` python
``` python
I will be interested in expanding Tensor Flow with OpenCL. As we have already released OpenCL caffe. https://github.com/amd/OpenCL-caffe.  Hopefully it can get integrated in light way? Is anyone interested in working together on this?
@gujunli Nice to see AMD here. /cc @naibaf7 @lunochod
@gujunli Certainly would be interested in contributing. Please let me know when you plan to start.
Luke
Once we come up with a reasonable approach that targets heterogeneous programming models ( not only OpenCL / SYCL )  we will create a proposal.
Luke
tensor flow.
Junlu
Tensor flow now?
Junli
@hsaputra
Thanks @naibaf7. Yeah, I don't think there is a viable alternative for cuDNN for OpenCL right now.
We're launching a porting initiative for GEGL next week, but we're happy to also support you.
Luke
C++ adds interesting metaprogramming features that allows to replace most of the code generators used such as in clBLAS or other frameworks to generate code more adapted to X or Y hardware.
Also N4355 in c++17 could enter in the game soon or later
@bhack Yes I love multidimensional arrays. Also in our domain of interest, there is the SG14 in the C++ committee that tries to have all the people interested in these issues to converge into **the** standard.
@gujunli Does OpenCL Caffe run on Android? Sorry for asking this here but I didn't find anywhere else to ask it :) Would be great with a deep learning library that ran on Android devices _and_ could use the GPU but it seems like there are no at the moment. (Correct me if I'm wrong!)
@krikru
A real alternative to cudnn could be the extension of [OpenVx standard objects](https://www.khronos.org/registry/vx/specs/1.0/html/d0/da0/group__group__basic__objects.html) with support to Tensor, NdConvolution, NdPooling operators and (probably) some other operator that could be considered standardizable.
@naibaf7 Do the [OpenCL Caffe](https://github.com/BVLC/caffe/tree/opencl) branch and the [OpenCL Caffe](https://github.com/amd/OpenCL-caffe) implementation by AMD have anything more in common besides the name? Have you compared the two or do you know if there is any difference in performance? You write that the OpenCL branch is far from optimal performance. What does that mean and what would be necessary in order to improve it? It would be interesting to try it on Android.
We are going off topic
@bhack Yeah, sorry for hijacking this thread. I just didn't know where to ask the question.
@krikru
But before this event there is the full C++ F2F at the end of the month in Jacksonville, Florida.
[https://github.com/strin/mocha-profile](https://github.com/strin/mocha-profile)
@strin Have you tried the last sgemm version in the MALI SDK?
Tensorflow is latee ! ahah
Canned operations
@keryell Hi, I also have interest in implementing TensorFlow on FPGA, using high level programming languages like Xilinx C++ or OpenCL. I am with pleasure to contribute if you have some plan.
@benoitsteiner So don't you still have a tensorflow/tensorflow/stream_executor/opencl/ counterpart internally? What about "Canned operators"?
@bhack @benoitsteiner
@benoitsteiner  How Can I simply remove cuda implementation? because '#ifdef GOOGLE_CUDA' is so complicated. It sometimes means CUDA, sometimes means GPU.
@CNugteren
Did you also have a chance to look at the libDNN convolutions?
@naibaf7 I saw it, yes! :) I haven't looked at libDNN at all so far, but I am not sure what you mean exactly. I assume convolution is implemented using GEMM?
@CNugteren
(https://github.com/naibaf7/caffe/blob/master/src/caffe/greentea/libdnn.cpp).
To compile this code, you need a SYCL compiler. Currently, the only supported compiler is Codeplay's ComputeCpp, which is available via a Codeplay's evaluation program. ComputeCpp will be made available for free as a public open beta, later in 2016 and then released with a free version (the ComputeCpp Community Edition) in 2017. This will let anyone compile and develop TensorFlow on OpenCL devices, such as AMD or Intel GPUs and CPUs.
btw. shouldn't this issue have OpenCL label? :)
Luke
@naibaf7
@naibaf7
@robertwgh
The C++ tensor code in Eigen would not easily be portable to OpenCL C without SYCL, but there are other features that would work well on OpenCL C. Have a look at this spreadsheet: https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit#gid=0 and fill free put your name down on the features that should use normal OpenCL C (such as BLAS and convolutions).
File "/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 28, in <module>
File "/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
@nivwusquorum haha that's a terrible workaround, as it'll start issues with other libraries. Thanks a lot though. I'm installing CUDA 7.0
@nivwusquorum no.
@ebrevdo yes
it breaks !
(in torch7, I have no pbs with gpus)
CUDA_VERSION='7.5'
@ebrevdo @andorremus Could you share the method of installing different cuda(7.0 and 7.5) separately?
@emergix @fivejjs Does symbolic linking libcuda.7.5 to libcuda.7.0 or just providing libcuda.7.0 have side effects?
@andorremus I can find a lot of instructions for installing a single version cuda, but I can't find any information about installing 7.0 and 7.5 simultaneously. How do you make it?
@andorremus Thanks.
@andorremus Sorry to bother you. One more question, do I need to install driver when I install the second cuda version or I just need to install the libraries?
/cc @ducha-aiki
@wangg12
@bhack thanks for tagging!
https://github.com/aboulch/tensorflow
@esafak how dod you do that?
@esafak do you mind explaining what you did? Perhaps in a gist?
(tensorflow)tsainbur@txori:~$ pip freeze | grep tensorflow
``` python
File "/opt/conda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 24, in swig_import_helper
``` python
Tx!
Swift is a very popular and expressive language with a large community of pro developers.
I understand the sentiment, but Swift, C# and Java are not "every" language, and this is not "every project" either. This is open source and Github so there might be enough hands to man the maintanance of ports to the most popular languages for such an important piece of software. Python is quite popular in academia but a little less popular with the bulk of private sector developers.
Given that TensorFlow is designed to also run on mobile and that Apple is recommending [Swift](https://developer.apple.com/swift/) as a primary language for iOS development, I think having a Swift API for TensorFlow would be a good idea.
I don't think adding support for the most widely-used languages such as Java, C#, and Swift would be a bad idea. The [gRPC](http://www.grpc.io/) project already supports 10 languages, including Java, C#, and JavaScript (via Node.js).
I couldn't agree more with janerivi and davidzchen. Swift has many advanced features that make it one, if not the best choice for full API support for TensorFlow.
https://www.quora.com/In-terms-of-performance-speed-is-Swift-faster-than-Java
FYI, @zaphar has just contributed initial [C# rules to Bazel](https://github.com/bazelbuild/bazel/tree/master/tools/build_defs/dotnet). These rules are currently in an early state and currently only support Mono, but the plan is to add Microsoft .NET support once [Bazel supports Windows](https://github.com/bazelbuild/bazel/issues/276). If anyone is interested in helping with improving Bazel's C# rules, contributions are definitely welcome. :)
The problem is not to invoke C++ code, but to translate the Python part of TensorFlow to either C# or C++. It has about 60K lines of code. That's where TensorFlow "magic" resides on...
I'm currently working on porting TensorFlow to Node.js, and we got stuck on this. Maintaining a synced version of some thousand lines of code will get out of control as soon as something changes in the Master. The way I see it working, is to translate the Python part of TensorFlow to C++.
Saying that, this is a completely pie in the sky idea and I really haven't looked at any specifics yet! :)
Anyone tried IronPython approach?
Anyone tried IronPython approach?
using highly efficient code implemented in another language.
@ahmadia do you have any tips on how to install tensorflow using conda?
Hope native Tensorflow for Windows will be ready soon, might have to wait until bazel for Windows is stable, thrilled to try [Deep Learning course](https://www.udacity.com/course/deep-learning--ud730) by Google and Udacity!
@umarniz Thanks for the guide. However, I think, what most people here anticipate to see is TensorFlow running natively on Windows **because of GPU support**. That unfortunately doesn't work with Docker as far as I know nor any other solution using virtualization...
By the way, only to mention this... the Deep MNIST example, which is admittedly already not that simple anymore, takes several hours on my CPU whereas my GPU (GTX 980Ti) rushes through it in at most a few seconds. On a native Ubuntu that is. I've one set up on a USB stick. It works, however dual boot can be quite cumbersome and a huge demotivating factor. Even more so since a hibernated Windows on a UEFI system can be pretty picky when it comes to accessing data on NTFS partitions from Linux...
@SeveQ I didn't take it that way either :)
@umarniz Sounds good. I don't have the time right now to also dedicate myself to this, unfortunately. Other priorities like graduating... Windows support for TensorFlow would be a great help though in this matter. I'm graduating as MSc., systems engineer, specializing in, who would have thought, machine learning...
> On Jan 30, 2016, at 2:46 AM, datashinobi notifications@github.com wrote:
An update on Windows support for Bazel: the initial set of patches to get Bazel working on Windows has been merged (see bazelbuild/bazel#276), and @dslomov has been able to get Bazel to [bootstrap itself on Windows](https://twitter.com/mulambda/status/700735662354333696).
@Fhrozen https://github.com/Sabrewarrior/tensorflow/blob/test/tf_win_env.txt
Now i got this error on the cuda version.
> Please specify the Cudnn version you want to use. [Leave empty to use system default]: 4.0.7
Setting up Cuda include
Setting up Cuda
@Fhrozen AFAIK you can't use gcc as a compiler for CUDA on Windows. Only Visual C++ Compiler is supported by CUDA on Windows ([see here](http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/)).
@petewarden will you also be open sourcing the development of the iOS integration? i.e. will developers be able to contribute to the development?
I suspect that this approach is taken because a mobile phone doesn't have enough computing power to do learning efficiently, given that lots of models are built with multi-GPUs. But that remains to be seen because the TensorFlow public C++ API doesn't yet support the wide range of network training capabilities that the Python API does.
@StephenOman that pretty much answers my question
Please give my sincere thanks to :santa:
~: python
mercurial (3.5.2)
vboxapi (1.0)
mercurial (3.5.2)
vboxapi (1.0)
mercurial (3.5.2)
vboxapi (1.0)
Collecting protobuf==3.0.0a1
Installing collected packages: protobuf
gp@MacBook-Pro-Gregory ~> python
pip uninstall protobuf
brew uninstall protobuf
Exception:
shutil.move(old, new)
Name: numpy
Name: protobuf
Name: numpy
Name: protobuf
(tensorflow)root@dmlserver:/home/rzibello/Documents# python
- su
Password:
sh-3.2# whoami
root
I'd be interested in helping @alonsovidales !
Idk if I'm qualified but I'd like to do whatever to help @alonsovidales !
It seems like Bazel doesn't yet have C/C++ interop for Go so cgo may be the only option unless Bazel adds support for it soon.
Are you planning to port the graph building mechanism into Go? It seems like it might be a good idea to get the consensus of the TF dev team before embarking on something as ambitious as that if they're planning on supporting this in the public API at some point in the future.
"github.com/davecgh/go-spew/spew"
"github.com/golang/protobuf/proto"
tensor {
tensor {
tensor {
t.Fatal(err)
t.Fatal(err)
t.Fatal(err)
Here you have the Go port of the tuto example:
Rhodesian ridgeback : 0.007571198
You should look at the RNN tutorial: http://tensorflow.org/tutorials/recurrent/index.md .
2. @ebrevdo I understand the computational saving motivation. However, returning zeros is logically very different from returning the state at `sequence_length` (if provided). The former is just wrong. Again, please correct if I misread the code.
@ebrevdo what are the considerations surrounding this decision between stateful rnncell implementations vs. storing the state in the graph collections?
@ebrevdo:
I converted the VGG-16 caffemodel to TensorFlow https://github.com/ry/tensorflow-vgg16 which might be helpful for a generalized script
We are also interested in adapting TensorFlow for Java. @ravwojdyla  Have you, by any chance, started working on the Swig Interface for Java? If you have, we could join our efforts and collaborate on that
/cc @saudet
``` diff
And run Bazel like this, for example:
@saudet Is there a reason why you are using a `cc_binary` rule to build the shared library rather than `cc_library`? You can just have a `cc_library` rule with the name `tensorflow` and the build target will build a shared library called `libtensorflow.so`.
A few other bits of feedback:
@saudet Thanks! I was just checking to make sure that it wasn't an issue with Bazel. :) Feel free to let me know or open a bug if you run into any issues.
@saudet Thanks for the info on using Bazel. I too am new to it and did not realize it was capable of generating a `.so` in that manner.
@saudet I don't think you need to pass `-shared` yourself. `cc_library` should be building a `.so` by default. Does that work for you?
``` diff
+genrule(
+    outs = ["jni_md.h"],
+genrule(
+    outs = ["jni.h"],
+        ":jni.h",
+        ":jni_md.h",
``` diff
``` python
genrule(
outs = ["jni_md.h"],
@verdiyanto Sorry, I don't have CI yet, but uploading the API docs is easy enough, so I've at least done that. Enjoy!
Seems that we have here a quite complete Javacpp preset. Is it an acceptable solution for "the team"?
@saudet I'm trying to build a copy of the JavaCPP wrappers, but it seems that due to the rapid rate of change of the tensorflow source they are not compatible with either the 0.6.0 release or today's master branch. Would it be possible to update them with a pointer to the exact tensorflow commit/version they were tested with?
/cc @databricks
@kovasb I think I missed this first time through. Are you saying that all the nice auto-differentiation magic that we get from using TensorFlow through python is implemented within python, not within the c++ libs? So in practice, a Java API would either need to re-implement all this, or would be just another numerics library? I'm not familiar enough with the internals of TensorFlow or the python glue to understand exactly what heavy lifting is done where.
I was trying to follow JavaCpp path but at the end there will be lots of duplication code and will be hard to maintain consistencies when something change in the Python implementation.
protobuf==2.6.1
File "/home/panmari/PycharmProjects/tensor_stuff/stuff.py", line 150, in <module>
sudo pip install -Iv protobuf==3.0a3
``` python
@mgcdanny seems they require contributors to sign an agreement first.
If you require 2.7 for the 2.X line, then why restrict yourselves to 3.0 (which was [released](https://www.python.org/downloads/) about the time of 2.6)? If you target 3.3+, then you'll save yourselves a lot of headache, especially if you're using `u'あ'` unicode strings. (also see [`unicode_literals`](https://docs.python.org/3.3/howto/pyporting.html#from-future-import-unicode-literals)).
I want to add TensorFlow to [Kaggle Scripts](kaggle.com/scripts), where we only support Python 3. Do you have an estimated timeframe of getting to Python 3 support (a couple days? a week? a month?).
sudo pip install /tmp/tensorflow_pkg/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
